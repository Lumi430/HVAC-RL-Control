Using TensorFlow backend.
[2019-03-25 22:02:14,672] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='linear', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[19, 1], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-25 22:02:14,672] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-25 22:02:14.704564: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-25 22:02:29,478] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-25 22:02:29,478] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-25 22:02:29,487] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-25 22:02:29,491] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-25 22:02:29,496] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-25 22:02:29,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-25 22:02:29,504] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-25 22:02:29,504] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:29,504] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-25 22:02:29,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:29,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-25 22:02:30,505] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:30,508] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-25 22:02:30,598] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,598] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-25 22:02:30,695] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-25 22:02:30,696] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:02:30,697] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:02:30,697] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,698] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:02:30,698] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:02:30,699] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,699] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,699] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:02:30,700] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:30,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-25 22:02:30,711] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-25 22:02:30,721] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-25 22:02:30,722] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-25 22:02:30,743] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-25 22:02:31,508] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:31,509] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-25 22:02:31,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:31,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-25 22:02:32,510] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:32,511] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-25 22:02:32,568] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:32,568] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-25 22:02:33,512] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:33,513] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-25 22:02:33,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:33,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-25 22:02:34,514] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:34,515] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-25 22:02:34,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:34,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-25 22:02:35,516] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:35,517] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-25 22:02:35,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:35,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-25 22:02:36,518] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:36,518] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-25 22:02:36,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:36,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-25 22:02:37,519] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:37,520] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-25 22:02:37,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:37,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-25 22:02:38,521] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:38,523] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-25 22:02:38,591] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:38,592] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-25 22:02:39,524] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:39,526] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-25 22:02:39,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:39,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-25 22:02:40,527] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:40,528] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-25 22:02:40,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:40,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-25 22:02:41,529] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:41,530] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-25 22:02:41,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:41,598] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-25 22:02:42,531] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:42,532] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-25 22:02:42,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:42,619] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-25 22:02:43,533] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:43,534] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-25 22:02:43,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:43,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-25 22:02:44,535] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-25 22:02:44,539] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-25 22:02:44,630] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:02:44,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-25 22:02:53,164] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-25 22:02:53,165] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.6, 74.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 454548.7152198232, 454548.7152198232, 224251.0830980136]
[2019-03-25 22:02:53,165] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:02:53,167] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.1708695  0.08778402 0.2976641  0.16887252 0.27480975], sampled 0.420089909587688
[2019-03-25 22:04:20,329] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-25 22:04:20,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.25, 84.33333333333334, 1.0, 2.0, 0.2590481942263846, 1.0, 2.0, 0.2590481942263846, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 723965.4457340363, 723965.4457340363, 242454.1033161834]
[2019-03-25 22:04:20,335] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:04:20,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.15470493 0.11194473 0.20656602 0.22729725 0.29948702], sampled 0.3034094619533607
[2019-03-25 22:04:36,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-25 22:04:36,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.23333333333333, 67.0, 1.0, 2.0, 0.2201808577873775, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3711558324684803, 6.911200000000001, 6.9112, 168.912956510431, 626804.0033594691, 626804.0033594685, 200074.887059467]
[2019-03-25 22:04:36,927] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:04:36,930] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.10429876 0.06889977 0.41033596 0.13886803 0.2775974 ], sampled 0.9845653219086715
[2019-03-25 22:04:37,135] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-25 22:04:37,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.6374738, 86.93956755333335, 1.0, 1.0, 0.1915059281929403, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3352063313143905, 6.911200000000001, 6.9112, 168.912956510431, 576336.4572934138, 576336.4572934132, 197587.836289832]
[2019-03-25 22:04:37,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:04:37,138] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.15326507 0.13487726 0.20649275 0.14970598 0.35565895], sampled 0.1410261698970563
[2019-03-25 22:04:42,300] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3540.3853 3197034060.0972 618.0000
[2019-03-25 22:04:42,493] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3556.9108 3307496962.5732 844.0000
[2019-03-25 22:04:42,625] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3419.0037 3516148368.5228 1265.0000
[2019-03-25 22:04:42,736] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3474.4134 3367695203.7181 1117.0000
[2019-03-25 22:04:42,849] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3563.4729 3228758251.7795 718.0000
[2019-03-25 22:04:43,863] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3419.0037181143116, 3516148368.52278, 1265.0, 3556.9107992483437, 3307496962.5731506, 844.0, 3540.3853167342368, 3197034060.0971613, 618.0, 3474.413363608105, 3367695203.7180624, 1117.0, 3563.4728825941997, 3228758251.7795124, 718.0]
[2019-03-25 22:04:48,769] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.11178944 0.21010855 0.10974124 0.3552777  0.21308304], sum to 1.0000
[2019-03-25 22:04:48,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8587
[2019-03-25 22:04:48,868] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 64.33333333333334, 1.0, 2.0, 0.3069304491262063, 1.0, 1.0, 0.3069304491262063, 1.0, 2.0, 0.5301082412714696, 6.9112, 6.9112, 170.5573041426782, 1358417.135020579, 1358417.135020579, 313574.2246648471], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 44400.0000, 
sim time next is 45000.0000, 
raw observation next is [27.1, 64.0, 1.0, 2.0, 0.298402059867328, 1.0, 2.0, 0.298402059867328, 1.0, 2.0, 0.5140642323779935, 6.9112, 6.9112, 170.5573041426782, 1315648.576293245, 1315648.576293245, 309344.2581026423], 
processed observation next is [1.0, 0.5217391304347826, 0.4834123222748816, 0.64, 1.0, 1.0, 0.15470127694858793, 1.0, 1.0, 0.15470127694858793, 1.0, 1.0, 0.4073954053390164, 0.0, 0.0, 0.8375144448122397, 0.3654579378592347, 0.3654579378592347, 0.4617078479143915], 
reward next is 0.5383, 
noisyNet noise sample is [array([0.38947675], dtype=float32), -0.43079153]. 
=============================================
[2019-03-25 22:04:48,888] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[9.116178]
 [8.684473]
 [8.05683 ]
 [9.034284]
 [8.505142]], R is [[ 9.53590965]
 [ 9.97252941]
 [10.43848801]
 [10.8704052 ]
 [10.76170158]].
[2019-03-25 22:04:57,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5805103e-08 2.5257364e-07 1.5402390e-08 9.9999964e-01 7.1928717e-08], sum to 1.0000
[2019-03-25 22:04:57,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8250
[2019-03-25 22:04:57,135] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.3, 96.0, 1.0, 2.0, 0.1916752433690354, 1.0, 2.0, 0.1916752433690354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 575169.9499695229, 575169.9499695229, 239096.3972487678], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 159000.0000, 
sim time next is 159600.0000, 
raw observation next is [22.2, 96.0, 1.0, 2.0, 0.1929382536160823, 1.0, 2.0, 0.1929382536160823, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 580802.7132864109, 580802.7132864109, 239555.8180197669], 
processed observation next is [1.0, 0.8695652173913043, 0.2511848341232228, 0.96, 1.0, 1.0, 0.02763645013985818, 1.0, 1.0, 0.02763645013985818, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16133408702400304, 0.16133408702400304, 0.35754599704442824], 
reward next is 0.6425, 
noisyNet noise sample is [array([-0.55261123], dtype=float32), -0.15101713]. 
=============================================
[2019-03-25 22:05:02,576] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9999952e-01 6.2112568e-09 4.4168114e-11 2.1639562e-07 1.9007054e-07], sum to 1.0000
[2019-03-25 22:05:02,585] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3787
[2019-03-25 22:05:02,592] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.56666666666667, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5533419766938275, 6.9112, 6.9112, 168.912956510431, 486037.8653682877, 486037.8653682877, 159173.6880523762], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 234600.0000, 
sim time next is 235200.0000, 
raw observation next is [21.53333333333333, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5524804799748622, 6.9112, 6.9112, 168.912956510431, 485351.9326370611, 485351.9326370611, 159051.6139002992], 
processed observation next is [0.0, 0.7391304347826086, 0.21958925750394942, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4542444877742221, 0.0, 0.0, 0.8294399451523027, 0.13481998128807254, 0.13481998128807254, 0.23739046850790926], 
reward next is 0.7626, 
noisyNet noise sample is [array([-0.932766], dtype=float32), -0.11189414]. 
=============================================
[2019-03-25 22:05:06,854] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7896: loss 0.1246
[2019-03-25 22:05:06,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7897: learning rate 0.0010
[2019-03-25 22:05:06,981] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7929: loss 0.3211
[2019-03-25 22:05:06,984] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7931: learning rate 0.0010
[2019-03-25 22:05:07,039] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 7954: loss 0.3316
[2019-03-25 22:05:07,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 7955: learning rate 0.0010
[2019-03-25 22:05:07,050] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7958: loss 0.3830
[2019-03-25 22:05:07,053] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7959: learning rate 0.0010
[2019-03-25 22:05:07,095] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7974: loss 0.2532
[2019-03-25 22:05:07,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7974: learning rate 0.0010
[2019-03-25 22:05:07,103] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7976: loss 0.1644
[2019-03-25 22:05:07,105] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 7976: loss 0.1355
[2019-03-25 22:05:07,107] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7976: learning rate 0.0010
[2019-03-25 22:05:07,107] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 7977: learning rate 0.0010
[2019-03-25 22:05:07,110] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7978: loss 0.1199
[2019-03-25 22:05:07,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7978: learning rate 0.0010
[2019-03-25 22:05:07,128] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 7984: loss 0.0476
[2019-03-25 22:05:07,130] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 7985: learning rate 0.0010
[2019-03-25 22:05:07,139] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7987: loss 0.1023
[2019-03-25 22:05:07,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7987: learning rate 0.0010
[2019-03-25 22:05:07,189] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8005: loss 0.0250
[2019-03-25 22:05:07,191] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8005: learning rate 0.0010
[2019-03-25 22:05:07,249] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8030: loss 0.0090
[2019-03-25 22:05:07,250] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 8030: loss 0.0068
[2019-03-25 22:05:07,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8031: learning rate 0.0010
[2019-03-25 22:05:07,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 8032: learning rate 0.0010
[2019-03-25 22:05:07,280] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 8039: loss 0.0025
[2019-03-25 22:05:07,283] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 8040: learning rate 0.0010
[2019-03-25 22:05:07,289] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 8043: loss 0.0002
[2019-03-25 22:05:07,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 8043: learning rate 0.0010
[2019-03-25 22:05:07,301] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8047: loss 0.0028
[2019-03-25 22:05:07,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8047: learning rate 0.0010
[2019-03-25 22:05:07,867] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9999130e-01 6.1503375e-10 1.8930845e-11 8.7030712e-06 2.6496531e-08], sum to 1.0000
[2019-03-25 22:05:07,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6772
[2019-03-25 22:05:07,881] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.7, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.570181177021866, 6.911199999999999, 6.9112, 168.912956510431, 498164.0132880309, 498164.0132880316, 161629.17512897], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 307800.0000, 
sim time next is 308400.0000, 
raw observation next is [23.73333333333333, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5718141389207629, 6.9112, 6.9112, 168.912956510431, 499433.5896106908, 499433.5896106908, 161868.2158508139], 
processed observation next is [0.0, 0.5652173913043478, 0.3238546603475513, 0.76, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47782212063507673, 0.0, 0.0, 0.8294399451523027, 0.13873155266963633, 0.13873155266963633, 0.24159435201614016], 
reward next is 0.7584, 
noisyNet noise sample is [array([1.5133822], dtype=float32), -0.99907726]. 
=============================================
[2019-03-25 22:05:13,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9805629e-01 6.7914596e-05 1.8733883e-03 8.1198706e-08 2.2198935e-06], sum to 1.0000
[2019-03-25 22:05:13,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4428
[2019-03-25 22:05:13,828] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.63333333333334, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5745786471112607, 6.9112, 6.9112, 168.912956510431, 508704.5823002744, 508704.5823002744, 162037.177612994], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 390000.0000, 
sim time next is 390600.0000, 
raw observation next is [22.65, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5163464295754653, 6.9112, 6.9112, 168.912956510431, 457067.6935307083, 457067.6935307083, 154075.1917123573], 
processed observation next is [1.0, 0.5217391304347826, 0.2725118483412322, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41017857265300645, 0.0, 0.0, 0.8294399451523027, 0.12696324820297453, 0.12696324820297453, 0.2299629727050109], 
reward next is 0.7700, 
noisyNet noise sample is [array([0.9811052], dtype=float32), -0.41654953]. 
=============================================
[2019-03-25 22:05:15,069] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.2353780e-13 7.9832218e-13 1.6904809e-11 2.1301478e-11], sum to 1.0000
[2019-03-25 22:05:15,078] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0836
[2019-03-25 22:05:15,086] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.58333333333334, 81.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5205788187543078, 6.911200000000001, 6.9112, 168.912956510431, 460102.2410224691, 460102.2410224684, 154653.9767340993], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 409800.0000, 
sim time next is 410400.0000, 
raw observation next is [21.5, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5148549326625375, 6.911200000000001, 6.9112, 168.912956510431, 455468.6955350673, 455468.6955350667, 153895.2856867405], 
processed observation next is [1.0, 0.782608695652174, 0.21800947867298584, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4083596739787042, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12651908209307425, 0.12651908209307408, 0.22969445624886642], 
reward next is 0.7703, 
noisyNet noise sample is [array([-2.187915], dtype=float32), 0.5747241]. 
=============================================
[2019-03-25 22:05:15,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 6.0756144e-15 2.2309804e-14 6.1597232e-14 4.1925239e-13], sum to 1.0000
[2019-03-25 22:05:15,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1121
[2019-03-25 22:05:15,265] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.16666666666667, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4961838115208047, 6.911199999999999, 6.9112, 168.912956510431, 440242.1385189432, 440242.1385189438, 151482.0807204802], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 412800.0000, 
sim time next is 413400.0000, 
raw observation next is [21.08333333333333, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4939975460357163, 6.911200000000001, 6.9112, 168.912956510431, 438600.465825786, 438600.4658257854, 151198.0949430977], 
processed observation next is [1.0, 0.782608695652174, 0.1982622432859398, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.38292383662892227, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.121833462729385, 0.12183346272938483, 0.2256687984225339], 
reward next is 0.7743, 
noisyNet noise sample is [array([0.76530576], dtype=float32), -0.24826491]. 
=============================================
[2019-03-25 22:05:27,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 6.3541339e-23 4.8432061e-20 1.2964702e-19 8.5263038e-19], sum to 1.0000
[2019-03-25 22:05:27,735] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8549
[2019-03-25 22:05:27,831] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.96666666666667, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4703355401222933, 6.9112, 6.9112, 168.912956510431, 418568.5634198648, 418568.5634198648, 148316.3462172554], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 582000.0000, 
sim time next is 582600.0000, 
raw observation next is [22.83333333333333, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4693856173886547, 6.9112, 6.9112, 168.912956510431, 417912.0570407765, 417912.0570407765, 148195.4424505147], 
processed observation next is [1.0, 0.7391304347826086, 0.2812006319115322, 0.665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35290928949835937, 0.0, 0.0, 0.8294399451523027, 0.1160866825113268, 0.1160866825113268, 0.22118722753808165], 
reward next is 0.7788, 
noisyNet noise sample is [array([-1.4542651], dtype=float32), -0.41824508]. 
=============================================
[2019-03-25 22:05:28,414] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15855: loss 0.5000
[2019-03-25 22:05:28,417] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15856: learning rate 0.0010
[2019-03-25 22:05:28,605] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 15927: loss 1.4494
[2019-03-25 22:05:28,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 15927: learning rate 0.0010
[2019-03-25 22:05:28,618] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15930: loss 1.0847
[2019-03-25 22:05:28,620] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15930: learning rate 0.0010
[2019-03-25 22:05:28,638] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15936: loss 1.6017
[2019-03-25 22:05:28,645] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15938: learning rate 0.0010
[2019-03-25 22:05:28,708] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 15960: loss 1.4286
[2019-03-25 22:05:28,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 15961: learning rate 0.0010
[2019-03-25 22:05:28,714] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15961: loss 1.1096
[2019-03-25 22:05:28,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15963: learning rate 0.0010
[2019-03-25 22:05:28,731] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15969: loss 1.1828
[2019-03-25 22:05:28,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15970: learning rate 0.0010
[2019-03-25 22:05:28,759] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15978: loss 2.0320
[2019-03-25 22:05:28,764] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15980: learning rate 0.0010
[2019-03-25 22:05:28,794] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15991: loss 1.5248
[2019-03-25 22:05:28,795] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 15991: loss 1.7632
[2019-03-25 22:05:28,797] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15991: learning rate 0.0010
[2019-03-25 22:05:28,798] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 15992: learning rate 0.0010
[2019-03-25 22:05:28,820] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16000: loss 1.4515
[2019-03-25 22:05:28,822] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16000: learning rate 0.0010
[2019-03-25 22:05:28,845] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16008: loss 1.6243
[2019-03-25 22:05:28,851] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16011: learning rate 0.0010
[2019-03-25 22:05:28,899] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16030: loss 1.2130
[2019-03-25 22:05:28,903] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16033: learning rate 0.0010
[2019-03-25 22:05:28,974] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16064: loss 0.7638
[2019-03-25 22:05:28,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16065: learning rate 0.0010
[2019-03-25 22:05:29,054] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 16090: loss 0.3777
[2019-03-25 22:05:29,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 16090: learning rate 0.0010
[2019-03-25 22:05:29,100] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 16107: loss 0.7047
[2019-03-25 22:05:29,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 16108: learning rate 0.0010
[2019-03-25 22:05:43,732] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 9.8914678e-21 3.4864654e-18 1.8199806e-18 3.7488467e-17], sum to 1.0000
[2019-03-25 22:05:43,744] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9053
[2019-03-25 22:05:43,843] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.28333333333334, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5237705490984925, 6.911200000000001, 6.9112, 168.912956510431, 461782.0591718462, 461782.0591718456, 155115.6606750371], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 810600.0000, 
sim time next is 811200.0000, 
raw observation next is [23.46666666666667, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5246201065931594, 6.9112, 6.9112, 168.912956510431, 462428.4785197963, 462428.4785197963, 155230.9448256263], 
processed observation next is [0.0, 0.391304347826087, 0.31121642969984215, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4202684226745846, 0.0, 0.0, 0.8294399451523027, 0.12845235514438785, 0.12845235514438785, 0.23168797735168103], 
reward next is 0.7683, 
noisyNet noise sample is [array([-0.27692795], dtype=float32), 0.94830334]. 
=============================================
[2019-03-25 22:05:49,823] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23833: loss 0.0321
[2019-03-25 22:05:49,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23833: learning rate 0.0010
[2019-03-25 22:05:50,031] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23910: loss 0.1816
[2019-03-25 22:05:50,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23911: learning rate 0.0010
[2019-03-25 22:05:50,091] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23932: loss 0.1315
[2019-03-25 22:05:50,092] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23932: learning rate 0.0010
[2019-03-25 22:05:50,111] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 23938: loss 0.0518
[2019-03-25 22:05:50,115] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 23940: learning rate 0.0010
[2019-03-25 22:05:50,125] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 23945: loss 0.0638
[2019-03-25 22:05:50,127] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 23945: learning rate 0.0010
[2019-03-25 22:05:50,138] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23949: loss 0.0898
[2019-03-25 22:05:50,140] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23950: learning rate 0.0010
[2019-03-25 22:05:50,158] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23957: loss 0.0674
[2019-03-25 22:05:50,161] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23957: learning rate 0.0010
[2019-03-25 22:05:50,175] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23964: loss 0.0036
[2019-03-25 22:05:50,178] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23965: learning rate 0.0010
[2019-03-25 22:05:50,232] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23984: loss 0.0102
[2019-03-25 22:05:50,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23984: learning rate 0.0010
[2019-03-25 22:05:50,249] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23989: loss 0.0008
[2019-03-25 22:05:50,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23991: learning rate 0.0010
[2019-03-25 22:05:50,288] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 24000: loss 0.0007
[2019-03-25 22:05:50,294] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 24001: learning rate 0.0010
[2019-03-25 22:05:50,309] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24008: loss 0.0012
[2019-03-25 22:05:50,311] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24009: learning rate 0.0010
[2019-03-25 22:05:50,412] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24047: loss 0.0921
[2019-03-25 22:05:50,419] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24048: learning rate 0.0010
[2019-03-25 22:05:50,461] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 24064: loss 0.1193
[2019-03-25 22:05:50,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 24064: learning rate 0.0010
[2019-03-25 22:05:50,619] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 24123: loss 0.0012
[2019-03-25 22:05:50,623] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 24124: learning rate 0.0010
[2019-03-25 22:05:50,671] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 24140: loss 0.0019
[2019-03-25 22:05:50,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 24141: learning rate 0.0010
[2019-03-25 22:05:51,127] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 5.8257836e-25 5.8989862e-22 2.7067079e-19 5.6646524e-22], sum to 1.0000
[2019-03-25 22:05:51,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3723
[2019-03-25 22:05:51,239] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.46666666666667, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5518786083826267, 6.911200000000001, 6.9112, 168.912956510431, 483761.9296071489, 483761.9296071482, 158999.3046782568], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 912000.0000, 
sim time next is 912600.0000, 
raw observation next is [24.6, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5530211516716014, 6.911200000000001, 6.9112, 168.912956510431, 484645.9345619114, 484645.9345619107, 159161.4552112886], 
processed observation next is [0.0, 0.5652173913043478, 0.36492890995260674, 0.68, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45490384350195284, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13462387071164206, 0.13462387071164186, 0.23755441076311729], 
reward next is 0.7624, 
noisyNet noise sample is [array([0.5312537], dtype=float32), -1.0957906]. 
=============================================
[2019-03-25 22:05:51,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 4.2648648e-27 1.0273484e-23 4.0883024e-24 9.3583248e-24], sum to 1.0000
[2019-03-25 22:05:51,903] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5167
[2019-03-25 22:05:51,994] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.56666666666667, 69.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5667167291172013, 6.9112, 6.9112, 168.912956510431, 495460.0831677268, 495460.0831677268, 161124.4169133012], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 919200.0000, 
sim time next is 919800.0000, 
raw observation next is [24.5, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5657562835017578, 6.9112, 6.9112, 168.912956510431, 494670.7814844513, 494670.7814844513, 160985.9701345858], 
processed observation next is [0.0, 0.6521739130434783, 0.3601895734597157, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47043449207531435, 0.0, 0.0, 0.8294399451523027, 0.1374085504123476, 0.1374085504123476, 0.24027756736505343], 
reward next is 0.7597, 
noisyNet noise sample is [array([1.0886354], dtype=float32), 0.09649764]. 
=============================================
[2019-03-25 22:05:53,108] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-25 22:05:53,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:05:53,109] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:05:53,110] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:05:53,111] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:05:53,111] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:05:53,112] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:05:53,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:05:53,114] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:05:53,114] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:05:53,116] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:05:53,137] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-25 22:05:53,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-25 22:05:53,139] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-25 22:05:53,171] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-25 22:05:53,210] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-25 22:05:54,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:05:54,583] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.36666666666667, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.240684967809429, 6.9112, 168.9113672294965, 1771252.638229877, 1537506.861373997, 324414.359347345]
[2019-03-25 22:05:54,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:05:54,590] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.0284988e-27 2.0854100e-24 1.6334850e-21 5.3932590e-23], sampled 0.977790566740676
[2019-03-25 22:05:54,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1771252.638229877 W.
[2019-03-25 22:06:18,982] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:06:18,987] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.819199245, 92.45596550166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7341704899872311, 6.911199999999999, 6.9112, 168.912956510431, 624317.8356958063, 624317.8356958069, 188993.9442360441]
[2019-03-25 22:06:18,988] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:06:18,990] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 9.4269706e-25 1.8845951e-22 2.0960776e-20 2.2264823e-22], sampled 0.3270570637525452
[2019-03-25 22:07:04,131] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:07:04,133] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.43333333333333, 47.33333333333334, 1.0, 2.0, 0.6307897524433731, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.985494502694081, 6.9112, 168.9124538993333, 1763745.263652779, 1711038.343255369, 370401.2127050431]
[2019-03-25 22:07:04,133] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:07:04,136] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 9.2307733e-28 1.2750872e-24 1.3709643e-21 2.6047487e-23], sampled 0.10942000266106011
[2019-03-25 22:07:04,139] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1763745.263652779 W.
[2019-03-25 22:07:46,869] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:07:46,870] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.11129635333334, 78.93907732666668, 1.0, 2.0, 0.8738300709707696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1252149.792563961, 1252149.792563961, 267319.0457104846]
[2019-03-25 22:07:46,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:07:46,876] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.7363642e-27 3.9398348e-24 1.4500943e-21 3.2249741e-23], sampled 0.230938634838472
[2019-03-25 22:07:46,877] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1252149.792563961 W.
[2019-03-25 22:07:53,671] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:07:53,672] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.01666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8894662693230779, 6.9112, 6.9112, 168.912956510431, 735128.9784211871, 735128.9784211871, 221028.9141519526]
[2019-03-25 22:07:53,672] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:07:53,677] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 7.7066119e-25 1.2518196e-22 2.0948785e-20 1.2697153e-22], sampled 0.16284464656801256
[2019-03-25 22:07:57,533] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:07:57,533] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.06666666666666, 61.0, 1.0, 2.0, 0.7707841712152278, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979703602979, 6.9112, 168.9123160186087, 1974185.529528743, 1906945.800690769, 400738.8247622006]
[2019-03-25 22:07:57,533] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:07:57,538] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.3044469e-27 1.1488820e-24 1.0825628e-21 4.1502474e-23], sampled 0.44796769723915375
[2019-03-25 22:07:57,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1974185.529528743 W.
[2019-03-25 22:07:58,260] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.15977511], dtype=float32), -0.07478644]
[2019-03-25 22:07:58,261] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.21666666666667, 79.66666666666667, 1.0, 2.0, 0.6178992307732271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564820556, 863485.1368352989, 863485.1368352982, 204126.9410298977]
[2019-03-25 22:07:58,262] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:07:58,266] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.8154988e-28 2.4772781e-25 2.6199070e-22 1.7387999e-24], sampled 0.4366614833899959
[2019-03-25 22:08:01,814] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:08:01,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:08:01,870] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.5406 2937714873.5162 1381.0000
[2019-03-25 22:08:02,055] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:08:02,201] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-03-25 22:08:03,216] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 25000, evaluation results [25000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8062.540634327665, 2937714873.516233, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-03-25 22:08:07,376] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.1529880e-15 4.7427843e-17 1.4549138e-15 3.1995923e-18], sum to 1.0000
[2019-03-25 22:08:07,383] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9287
[2019-03-25 22:08:07,487] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.96666666666667, 94.33333333333334, 1.0, 2.0, 0.2534449571945373, 1.0, 1.0, 0.2534449571945373, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 772609.7618477545, 772609.7618477545, 250404.9107826754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 991200.0000, 
sim time next is 991800.0000, 
raw observation next is [21.95, 94.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8665726879253107, 6.911199999999999, 6.9112, 168.912956510431, 749414.6585855066, 749414.6585855072, 216317.030534912], 
processed observation next is [1.0, 0.4782608695652174, 0.2393364928909953, 0.945, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.8372837657625739, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20817073849597406, 0.20817073849597423, 0.32286123960434626], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.16674933], dtype=float32), -0.14432654]. 
=============================================
[2019-03-25 22:08:10,308] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.9968650e-26 2.2937312e-25 4.2767750e-19 6.0926593e-27], sum to 1.0000
[2019-03-25 22:08:10,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5232
[2019-03-25 22:08:10,320] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.33333333333333, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6644440148083247, 6.9112, 6.9112, 168.912956510431, 571882.005666602, 571882.005666602, 176520.7402666984], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [22.36666666666667, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6657713451220382, 6.911199999999999, 6.9112, 168.912956510431, 572803.730547473, 572803.7305474736, 176747.1466927583], 
processed observation next is [1.0, 0.0, 0.2590837282780413, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5924040794171197, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15911214737429805, 0.15911214737429824, 0.2638017114817288], 
reward next is 0.7362, 
noisyNet noise sample is [array([-0.12396965], dtype=float32), -1.0220715]. 
=============================================
[2019-03-25 22:08:19,461] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 4.9375619e-20 1.5554867e-21 7.9944443e-15 1.1580354e-22], sum to 1.0000
[2019-03-25 22:08:19,475] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7767
[2019-03-25 22:08:19,483] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1266650.68031387 W.
[2019-03-25 22:08:19,489] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.5, 69.66666666666666, 1.0, 2.0, 0.2806004325946569, 1.0, 1.0, 0.2806004325946569, 1.0, 2.0, 0.4910398635315986, 6.9112, 6.9112, 170.5573041426782, 1266650.68031387, 1266650.68031387, 305370.7840299557], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1162200.0000, 
sim time next is 1162800.0000, 
raw observation next is [25.7, 69.0, 1.0, 2.0, 0.3788498726122694, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6672216538916469, 6.911200000000001, 6.9112, 168.912956510431, 1151314.276391055, 1151314.276391055, 258409.469142172], 
processed observation next is [1.0, 0.4782608695652174, 0.4170616113744076, 0.69, 1.0, 1.0, 0.25162635254490284, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.5941727486483498, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3198095212197375, 0.3198095212197375, 0.3856857748390627], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2425265], dtype=float32), 0.35925126]. 
=============================================
[2019-03-25 22:08:21,561] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31814: loss 0.0424
[2019-03-25 22:08:21,565] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31814: learning rate 0.0010
[2019-03-25 22:08:21,715] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31872: loss 0.0533
[2019-03-25 22:08:21,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31872: learning rate 0.0010
[2019-03-25 22:08:21,766] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31890: loss 0.0187
[2019-03-25 22:08:21,769] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31890: learning rate 0.0010
[2019-03-25 22:08:21,838] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31914: loss 0.1138
[2019-03-25 22:08:21,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31915: learning rate 0.0010
[2019-03-25 22:08:21,846] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 31916: loss 0.4539
[2019-03-25 22:08:21,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 31916: learning rate 0.0010
[2019-03-25 22:08:21,874] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31928: loss 0.0060
[2019-03-25 22:08:21,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31929: learning rate 0.0010
[2019-03-25 22:08:21,994] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31972: loss 0.1172
[2019-03-25 22:08:21,996] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31972: learning rate 0.0010
[2019-03-25 22:08:22,008] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 31975: loss 0.1822
[2019-03-25 22:08:22,012] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 31975: learning rate 0.0010
[2019-03-25 22:08:22,070] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31998: loss 0.3200
[2019-03-25 22:08:22,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31999: learning rate 0.0010
[2019-03-25 22:08:22,178] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32037: loss 0.1324
[2019-03-25 22:08:22,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32038: learning rate 0.0010
[2019-03-25 22:08:22,193] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32043: loss 0.0460
[2019-03-25 22:08:22,194] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32043: learning rate 0.0010
[2019-03-25 22:08:22,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.9917987e-15 2.0530996e-19 1.9331498e-12 3.0859335e-22], sum to 1.0000
[2019-03-25 22:08:22,224] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1344
[2019-03-25 22:08:22,229] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 77.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6272215414152009, 6.9112, 6.9112, 168.912956510431, 543808.0363463858, 543808.0363463858, 170357.5373575532], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1198200.0000, 
sim time next is 1198800.0000, 
raw observation next is [24.2, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6357128978741639, 6.9112, 6.9112, 168.912956510431, 551037.1222666402, 551037.1222666402, 171723.8225340937], 
processed observation next is [1.0, 0.9130434782608695, 0.3459715639810427, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.555747436431907, 0.0, 0.0, 0.8294399451523027, 0.15306586729628893, 0.15306586729628893, 0.2563042127374533], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.4779506], dtype=float32), -1.4878068]. 
=============================================
[2019-03-25 22:08:22,245] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 32060: loss 0.0130
[2019-03-25 22:08:22,248] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 32061: learning rate 0.0010
[2019-03-25 22:08:22,313] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 32083: loss 0.1573
[2019-03-25 22:08:22,316] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 32084: learning rate 0.0010
[2019-03-25 22:08:22,349] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32097: loss 0.9522
[2019-03-25 22:08:22,350] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32098: loss 0.9898
[2019-03-25 22:08:22,351] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32098: learning rate 0.0010
[2019-03-25 22:08:22,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32099: learning rate 0.0010
[2019-03-25 22:08:22,368] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 32103: loss 0.0961
[2019-03-25 22:08:22,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 32103: learning rate 0.0010
[2019-03-25 22:08:34,444] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 5.6917527e-21 2.3638697e-19 2.7891695e-18 9.1022325e-21], sum to 1.0000
[2019-03-25 22:08:34,456] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5498
[2019-03-25 22:08:34,462] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.16666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5818379494945073, 6.9112, 6.9112, 168.912956510431, 509013.5723045908, 509013.5723045908, 163309.7230578374], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1366800.0000, 
sim time next is 1367400.0000, 
raw observation next is [21.18333333333333, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.582594302444558, 6.911199999999999, 6.9112, 168.912956510431, 509589.5633018786, 509589.5633018792, 163423.1889133349], 
processed observation next is [1.0, 0.8260869565217391, 0.20300157977883085, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49096866151775365, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14155265647274404, 0.14155265647274423, 0.24391520733333566], 
reward next is 0.7561, 
noisyNet noise sample is [array([1.4099923], dtype=float32), 0.63789505]. 
=============================================
[2019-03-25 22:08:37,153] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.0573449e-27 1.2477868e-21 7.7965448e-22 7.6159598e-24], sum to 1.0000
[2019-03-25 22:08:37,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9646
[2019-03-25 22:08:37,164] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.68333333333334, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5926626290741202, 6.9112, 6.9112, 168.912956510431, 516777.1966169406, 516777.1966169406, 164956.6042349226], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1408200.0000, 
sim time next is 1408800.0000, 
raw observation next is [21.76666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5928617941093202, 6.9112, 6.9112, 168.912956510431, 516816.3737775118, 516816.3737775118, 164989.219708446], 
processed observation next is [0.0, 0.30434782608695654, 0.23064770932069528, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5034899928162441, 0.0, 0.0, 0.8294399451523027, 0.14356010382708662, 0.14356010382708662, 0.2462525667290239], 
reward next is 0.7537, 
noisyNet noise sample is [array([2.249023], dtype=float32), -0.5176658]. 
=============================================
[2019-03-25 22:08:42,283] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 4.6166225e-22 6.5056960e-20 3.3359073e-18 3.0060867e-23], sum to 1.0000
[2019-03-25 22:08:42,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3040
[2019-03-25 22:08:42,296] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.06666666666667, 96.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.592544890942741, 6.9112, 6.9112, 168.912956510431, 516865.1005311398, 516865.1005311398, 164934.9354804044], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [21.0, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5885269733436594, 6.9112, 6.9112, 168.912956510431, 513609.3363222133, 513609.3363222133, 164328.9207851411], 
processed observation next is [0.0, 0.13043478260869565, 0.19431279620853087, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49820362602885293, 0.0, 0.0, 0.8294399451523027, 0.1426692600895037, 0.1426692600895037, 0.24526704594797177], 
reward next is 0.7547, 
noisyNet noise sample is [array([-1.1476712], dtype=float32), -0.18985209]. 
=============================================
[2019-03-25 22:08:43,036] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39826: loss 0.0188
[2019-03-25 22:08:43,039] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39827: learning rate 0.0010
[2019-03-25 22:08:43,127] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39860: loss 0.0490
[2019-03-25 22:08:43,131] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39861: learning rate 0.0010
[2019-03-25 22:08:43,142] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39862: loss 0.1170
[2019-03-25 22:08:43,143] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39862: learning rate 0.0010
[2019-03-25 22:08:43,215] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39889: loss 0.1696
[2019-03-25 22:08:43,218] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39889: learning rate 0.0010
[2019-03-25 22:08:43,264] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39909: loss 0.1198
[2019-03-25 22:08:43,266] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39910: learning rate 0.0010
[2019-03-25 22:08:43,342] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39937: loss 0.0867
[2019-03-25 22:08:43,343] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39937: loss 0.0852
[2019-03-25 22:08:43,348] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39939: learning rate 0.0010
[2019-03-25 22:08:43,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39937: learning rate 0.0010
[2019-03-25 22:08:43,362] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 39944: loss 0.0237
[2019-03-25 22:08:43,363] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 39944: learning rate 0.0010
[2019-03-25 22:08:43,417] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39961: loss 0.0201
[2019-03-25 22:08:43,419] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39961: learning rate 0.0010
[2019-03-25 22:08:43,552] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40014: loss 0.0010
[2019-03-25 22:08:43,558] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40015: learning rate 0.0010
[2019-03-25 22:08:43,667] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 40056: loss 0.0820
[2019-03-25 22:08:43,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 40056: learning rate 0.0010
[2019-03-25 22:08:43,702] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 40068: loss 0.0865
[2019-03-25 22:08:43,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 40070: learning rate 0.0010
[2019-03-25 22:08:43,755] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40088: loss 0.0831
[2019-03-25 22:08:43,757] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40089: learning rate 0.0010
[2019-03-25 22:08:43,816] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 40110: loss 0.0147
[2019-03-25 22:08:43,818] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 40110: learning rate 0.0010
[2019-03-25 22:08:43,899] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40146: loss 0.0002
[2019-03-25 22:08:43,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40146: learning rate 0.0010
[2019-03-25 22:08:43,961] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 40172: loss 0.0002
[2019-03-25 22:08:43,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 40173: learning rate 0.0010
[2019-03-25 22:08:44,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.4789728e-23 5.3630772e-24 7.7596759e-19 9.0368119e-28], sum to 1.0000
[2019-03-25 22:08:44,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1309
[2019-03-25 22:08:44,618] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.75, 51.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6554507042217965, 6.9112, 6.9112, 168.912956510431, 563479.9992026711, 563479.9992026711, 175008.051499821], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1518600.0000, 
sim time next is 1519200.0000, 
raw observation next is [29.8, 52.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6615785329274284, 6.911199999999999, 6.9112, 168.912956510431, 568262.6690425077, 568262.6690425084, 176039.38335872], 
processed observation next is [0.0, 0.6086956521739131, 0.6113744075829385, 0.52, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5872908938139371, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15785074140069658, 0.15785074140069677, 0.262745348296597], 
reward next is 0.7373, 
noisyNet noise sample is [array([-0.19866827], dtype=float32), 0.7830291]. 
=============================================
[2019-03-25 22:08:44,971] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 5.3414378e-23 1.3657639e-23 1.3770786e-17 4.7765882e-26], sum to 1.0000
[2019-03-25 22:08:44,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1471
[2019-03-25 22:08:45,077] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.65, 51.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6459623104416596, 6.911199999999999, 6.9112, 168.912956510431, 556260.8394629578, 556260.8394629584, 173429.3733664133], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1517400.0000, 
sim time next is 1518000.0000, 
raw observation next is [29.7, 51.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6499733060953979, 6.9112, 6.9112, 168.912956510431, 559244.8657543177, 559244.8657543177, 174094.2420053981], 
processed observation next is [0.0, 0.5652173913043478, 0.6066350710900474, 0.5166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5731381781651194, 0.0, 0.0, 0.8294399451523027, 0.15534579604286602, 0.15534579604286602, 0.2598421522468628], 
reward next is 0.7402, 
noisyNet noise sample is [array([2.240099], dtype=float32), -0.96385944]. 
=============================================
[2019-03-25 22:08:45,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[77.91655]
 [77.92709]
 [77.93324]
 [77.95061]
 [77.96945]], R is [[77.88430023]
 [77.84661102]
 [77.81077576]
 [77.7782135 ]
 [77.74485016]].
[2019-03-25 22:08:46,303] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.0300140e-29 6.5745769e-28 5.5589805e-25 4.9433552e-32], sum to 1.0000
[2019-03-25 22:08:46,318] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1467
[2019-03-25 22:08:46,414] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6336240653510443, 6.9112, 6.9112, 168.912956510431, 548685.4184535787, 548685.4184535787, 171392.9269148293], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1539600.0000, 
sim time next is 1540200.0000, 
raw observation next is [23.8, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.634950446089687, 6.911199999999999, 6.9112, 168.912956510431, 549739.9703249993, 549739.9703249999, 171608.1873798774], 
processed observation next is [0.0, 0.8260869565217391, 0.3270142180094788, 0.815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5548176171825451, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1527055473124998, 0.15270554731249997, 0.2561316229550409], 
reward next is 0.7439, 
noisyNet noise sample is [array([0.4453089], dtype=float32), 1.209323]. 
=============================================
[2019-03-25 22:08:51,374] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 6.4541543e-21 6.3038982e-21 9.0104553e-18 1.5074209e-23], sum to 1.0000
[2019-03-25 22:08:51,383] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1312
[2019-03-25 22:08:51,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1206429.114347075 W.
[2019-03-25 22:08:51,400] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 85.0, 1.0, 2.0, 0.409739697639157, 1.0, 2.0, 0.409739697639157, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1206429.114347075, 1206429.114347075, 281869.2496633731], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1604400.0000, 
sim time next is 1605000.0000, 
raw observation next is [24.08333333333334, 85.0, 1.0, 2.0, 0.8202726161826823, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1221645.96093472, 1221645.96093472, 259303.6777846015], 
processed observation next is [1.0, 0.5652173913043478, 0.34044233807267016, 0.85, 1.0, 1.0, 0.7834609833526293, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33934610025964446, 0.33934610025964446, 0.3870204146038828], 
reward next is 0.6130, 
noisyNet noise sample is [array([-0.37661558], dtype=float32), 1.10168]. 
=============================================
[2019-03-25 22:08:51,432] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.264168]
 [57.432972]
 [57.9933  ]
 [57.521816]
 [58.179405]], R is [[56.68461609]
 [56.69707108]
 [56.71218109]
 [56.78007507]
 [56.87596893]].
[2019-03-25 22:08:54,567] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.5662849e-22 9.3768046e-23 9.4722385e-19 9.5255842e-25], sum to 1.0000
[2019-03-25 22:08:54,579] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9150
[2019-03-25 22:08:54,584] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.33333333333334, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.730483758865942, 6.911200000000002, 6.9112, 168.912956510431, 619112.6441190926, 619112.6441190913, 188279.8820452777], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1657200.0000, 
sim time next is 1657800.0000, 
raw observation next is [23.35, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7388767237457171, 6.9112, 6.9112, 168.912956510431, 626098.9645301371, 626098.9645301371, 189861.3406753297], 
processed observation next is [1.0, 0.17391304347826086, 0.3056872037914693, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6815569801777037, 0.0, 0.0, 0.8294399451523027, 0.1739163790361492, 0.1739163790361492, 0.283375135336313], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.62208647], dtype=float32), 0.7598011]. 
=============================================
[2019-03-25 22:09:04,600] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47825: loss 0.7342
[2019-03-25 22:09:04,603] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47826: learning rate 0.0010
[2019-03-25 22:09:04,612] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47829: loss 1.9964
[2019-03-25 22:09:04,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47829: learning rate 0.0010
[2019-03-25 22:09:04,630] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47834: loss 1.0491
[2019-03-25 22:09:04,636] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47835: learning rate 0.0010
[2019-03-25 22:09:04,707] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47860: loss 1.7454
[2019-03-25 22:09:04,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47860: learning rate 0.0010
[2019-03-25 22:09:04,770] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47887: loss 0.5560
[2019-03-25 22:09:04,773] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47888: learning rate 0.0010
[2019-03-25 22:09:04,866] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47921: loss 1.5043
[2019-03-25 22:09:04,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47921: learning rate 0.0010
[2019-03-25 22:09:04,945] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 47955: loss 0.9747
[2019-03-25 22:09:04,947] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 47955: learning rate 0.0010
[2019-03-25 22:09:04,989] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47978: loss 0.0862
[2019-03-25 22:09:04,993] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47978: learning rate 0.0010
[2019-03-25 22:09:05,004] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 47982: loss 0.0283
[2019-03-25 22:09:05,006] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 47982: learning rate 0.0010
[2019-03-25 22:09:05,091] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 48012: loss 0.0607
[2019-03-25 22:09:05,094] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 48012: learning rate 0.0010
[2019-03-25 22:09:05,144] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48035: loss 0.3984
[2019-03-25 22:09:05,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48035: learning rate 0.0010
[2019-03-25 22:09:05,181] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48049: loss 0.0466
[2019-03-25 22:09:05,187] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48051: learning rate 0.0010
[2019-03-25 22:09:05,384] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 48123: loss 0.0177
[2019-03-25 22:09:05,388] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 48124: learning rate 0.0010
[2019-03-25 22:09:05,462] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 48148: loss 0.1294
[2019-03-25 22:09:05,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 48149: learning rate 0.0010
[2019-03-25 22:09:05,492] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 48157: loss 1.7489
[2019-03-25 22:09:05,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 48157: learning rate 0.0010
[2019-03-25 22:09:05,607] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48203: loss 0.1527
[2019-03-25 22:09:05,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48204: learning rate 0.0010
[2019-03-25 22:09:10,387] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-25 22:09:10,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:09:10,390] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:09:10,390] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:09:10,392] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:09:10,393] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:09:10,390] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:09:10,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:09:10,394] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:09:10,395] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:09:10,398] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:09:10,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-25 22:09:10,437] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-25 22:09:10,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-25 22:09:10,455] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-25 22:09:10,495] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-25 22:09:22,580] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:09:22,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.38356120333333, 85.41679961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4085280508357269, 6.9112, 6.9112, 168.912956510431, 368715.0062197731, 368715.0062197731, 141254.1669302938]
[2019-03-25 22:09:22,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:09:22,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 2.4798550e-21 1.0508576e-22 5.7873574e-16 8.4546154e-22], sampled 0.2791655379970607
[2019-03-25 22:09:26,932] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:09:26,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.91551947666667, 94.83203915666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6496111172277312, 6.911199999999999, 6.9112, 168.912956510431, 560853.7636734885, 560853.7636734891, 174022.2872695397]
[2019-03-25 22:09:26,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:09:26,938] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.9175202e-20 1.3342998e-22 4.6624420e-15 2.6455179e-21], sampled 0.8442415063947294
[2019-03-25 22:09:41,177] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:09:41,177] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.41422508666667, 91.34450113, 1.0, 2.0, 0.6774485798867228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 946739.6443970284, 946739.6443970278, 216049.906183625]
[2019-03-25 22:09:41,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:09:41,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 2.17611154e-20 8.42534900e-22 1.32232590e-15
 1.01796833e-19], sampled 0.8570837123310672
[2019-03-25 22:09:41,185] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 946739.6443970284 W.
[2019-03-25 22:10:34,800] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:10:34,805] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.76666666666667, 84.0, 1.0, 2.0, 0.6237279568047518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871633.8498768875, 871633.8498768875, 205249.0081515096]
[2019-03-25 22:10:34,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:10:34,809] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.2765631e-22 3.8668772e-23 1.5896712e-16 4.9014954e-19], sampled 0.49207324790202567
[2019-03-25 22:10:34,810] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 871633.8498768875 W.
[2019-03-25 22:10:46,038] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:10:46,041] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.83333333333333, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9238883129014762, 6.9112, 6.9112, 168.912956510431, 755756.2847229472, 755756.2847229472, 228770.4972575374]
[2019-03-25 22:10:46,041] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:10:46,046] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.9125319e-20 6.4199419e-23 2.3741144e-14 2.9258871e-21], sampled 0.6980273962719493
[2019-03-25 22:11:17,714] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:11:18,738] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.16288164], dtype=float32), 0.040140685]
[2019-03-25 22:11:18,738] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.88207674333334, 78.44975505000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.922645213580037, 6.9112, 6.9112, 168.912956510431, 758682.6150332016, 758682.6150332016, 228643.3172331596]
[2019-03-25 22:11:18,739] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:11:18,741] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 5.8708441e-19 1.2556032e-21 1.1292056e-13 2.9648663e-19], sampled 0.4199374252432785
[2019-03-25 22:11:18,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:11:18,869] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.7244 2937773304.3596 1381.0000
[2019-03-25 22:11:18,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.9543 3105642123.8214 2010.0000
[2019-03-25 22:11:19,061] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.2590 2989314505.7283 1566.0000
[2019-03-25 22:11:20,072] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 50000, evaluation results [50000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7346.9543209995, 3105642123.821427, 2010.0, 8061.724446284878, 2937773304.359631, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7925.258991697124, 2989314505.7282696, 1566.0]
[2019-03-25 22:11:31,280] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.4706330e-26 5.2235280e-27 5.2530409e-20 1.7058009e-30], sum to 1.0000
[2019-03-25 22:11:31,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1948
[2019-03-25 22:11:31,302] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.73333333333333, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.852301129097624, 6.9112, 6.9112, 168.912956510431, 706869.444719193, 706869.444719193, 212766.5103210473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2028000.0000, 
sim time next is 2028600.0000, 
raw observation next is [25.85, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8542764561691788, 6.9112, 6.9112, 168.912956510431, 708243.1851724099, 708243.1851724099, 213192.4285760463], 
processed observation next is [0.0, 0.4782608695652174, 0.4241706161137442, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8222883611819253, 0.0, 0.0, 0.8294399451523027, 0.19673421810344718, 0.19673421810344718, 0.3181976545911139], 
reward next is 0.6818, 
noisyNet noise sample is [array([0.50597703], dtype=float32), -1.2135004]. 
=============================================
[2019-03-25 22:11:31,502] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.0384512e-24 1.8909284e-26 6.0426158e-20 9.8307580e-35], sum to 1.0000
[2019-03-25 22:11:31,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8928
[2019-03-25 22:11:31,519] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.78333333333333, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8677380133114044, 6.9112, 6.9112, 168.912956510431, 717417.7760401964, 717417.7760401964, 216113.7136450761], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2033400.0000, 
sim time next is 2034000.0000, 
raw observation next is [26.9, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.87126737776848, 6.911200000000001, 6.9112, 168.912956510431, 720122.0626827726, 720122.062682772, 216897.3954042611], 
processed observation next is [0.0, 0.5652173913043478, 0.4739336492890995, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.843008997278634, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20003390630077017, 0.20003390630077, 0.3237274558272554], 
reward next is 0.6763, 
noisyNet noise sample is [array([0.4089676], dtype=float32), 0.0603779]. 
=============================================
[2019-03-25 22:11:31,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.65369 ]
 [70.668144]
 [70.66084 ]
 [70.66089 ]
 [70.64131 ]], R is [[70.63461304]
 [70.60571289]
 [70.57788086]
 [70.550354  ]
 [70.52352142]].
[2019-03-25 22:11:32,746] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00000000e+00 9.03073253e-31 6.08556247e-31 1.24660854e-26
 4.64457654e-33], sum to 1.0000
[2019-03-25 22:11:32,754] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3628
[2019-03-25 22:11:32,759] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.76666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8538194728831964, 6.9112, 6.9112, 168.912956510431, 707557.854038222, 707557.854038222, 213081.7444970296], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2046000.0000, 
sim time next is 2046600.0000, 
raw observation next is [26.7, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.850673364969116, 6.9112, 6.9112, 168.912956510431, 705354.5522846185, 705354.5522846185, 212403.8816028467], 
processed observation next is [0.0, 0.6956521739130435, 0.46445497630331756, 0.855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8178943475233122, 0.0, 0.0, 0.8294399451523027, 0.19593182007906068, 0.19593182007906068, 0.3170207188102189], 
reward next is 0.6830, 
noisyNet noise sample is [array([-0.09056508], dtype=float32), -0.18139528]. 
=============================================
[2019-03-25 22:11:34,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.6462289e-27 4.4643869e-30 2.7536694e-25 0.0000000e+00], sum to 1.0000
[2019-03-25 22:11:34,239] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3554
[2019-03-25 22:11:34,339] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.35, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7797969849593903, 6.911199999999999, 6.9112, 168.912956510431, 655788.7725820044, 655788.772582005, 197772.3839489263], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2079000.0000, 
sim time next is 2079600.0000, 
raw observation next is [24.33333333333334, 95.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7799407912317139, 6.9112, 6.9112, 168.912956510431, 655868.5273271379, 655868.5273271379, 197800.4411604291], 
processed observation next is [0.0, 0.043478260869565216, 0.35229067930489766, 0.9566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7316351112581876, 0.0, 0.0, 0.8294399451523027, 0.1821857020353161, 0.1821857020353161, 0.29522453904541657], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.6538295], dtype=float32), -0.9404698]. 
=============================================
[2019-03-25 22:11:35,640] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 55770: loss 0.0030
[2019-03-25 22:11:35,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 55774: learning rate 0.0010
[2019-03-25 22:11:35,756] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55810: loss 0.0017
[2019-03-25 22:11:35,762] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55810: learning rate 0.0010
[2019-03-25 22:11:35,818] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55835: loss 0.0073
[2019-03-25 22:11:35,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55835: learning rate 0.0010
[2019-03-25 22:11:35,845] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55847: loss 0.0104
[2019-03-25 22:11:35,850] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55847: learning rate 0.0010
[2019-03-25 22:11:35,939] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 55880: loss 0.0156
[2019-03-25 22:11:35,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 55880: learning rate 0.0010
[2019-03-25 22:11:36,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55926: loss 0.0249
[2019-03-25 22:11:36,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55930: learning rate 0.0010
[2019-03-25 22:11:36,141] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55952: loss 0.0017
[2019-03-25 22:11:36,144] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55952: learning rate 0.0010
[2019-03-25 22:11:36,342] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56026: loss 0.0281
[2019-03-25 22:11:36,343] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56026: loss 0.0236
[2019-03-25 22:11:36,348] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56029: learning rate 0.0010
[2019-03-25 22:11:36,349] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56029: learning rate 0.0010
[2019-03-25 22:11:36,366] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56036: loss 0.0554
[2019-03-25 22:11:36,376] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56038: learning rate 0.0010
[2019-03-25 22:11:36,389] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 56043: loss 0.0566
[2019-03-25 22:11:36,393] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 56045: learning rate 0.0010
[2019-03-25 22:11:36,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 5.4435934e-29 5.7032022e-30 1.8057154e-24 1.8556216e-33], sum to 1.0000
[2019-03-25 22:11:36,403] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 56048: loss 0.0633
[2019-03-25 22:11:36,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 56048: learning rate 0.0010
[2019-03-25 22:11:36,411] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4445
[2019-03-25 22:11:36,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56050: loss 0.0488
[2019-03-25 22:11:36,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56051: learning rate 0.0010
[2019-03-25 22:11:36,418] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.8, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9196237746110528, 6.911199999999999, 6.9112, 168.912956510431, 751459.3925727669, 751459.3925727676, 227721.0100798798], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2109600.0000, 
sim time next is 2110200.0000, 
raw observation next is [29.0, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9237697062124418, 6.911200000000001, 6.9112, 168.912956510431, 753992.2469766083, 753992.2469766077, 228666.8075620852], 
processed observation next is [0.0, 0.43478260869565216, 0.5734597156398105, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9070362270883435, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20944229082683563, 0.20944229082683546, 0.3412937426299779], 
reward next is 0.6587, 
noisyNet noise sample is [array([0.6814143], dtype=float32), -0.020679636]. 
=============================================
[2019-03-25 22:11:36,686] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 56146: loss 0.0011
[2019-03-25 22:11:36,687] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 56146: learning rate 0.0010
[2019-03-25 22:11:36,811] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56193: loss 0.0743
[2019-03-25 22:11:36,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56195: learning rate 0.0010
[2019-03-25 22:11:36,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56248: loss 0.1198
[2019-03-25 22:11:36,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56248: learning rate 0.0010
[2019-03-25 22:11:42,757] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 6.1258204e-20 1.4507154e-19 2.7110818e-17 2.3733894e-22], sum to 1.0000
[2019-03-25 22:11:42,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0860
[2019-03-25 22:11:42,775] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1547103.855250822 W.
[2019-03-25 22:11:42,780] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.042692586995122, 6.9112, 168.9123721477031, 1547103.855250822, 1453818.811136247, 311351.3012333542], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2190000.0000, 
sim time next is 2190600.0000, 
raw observation next is [28.6, 80.5, 1.0, 2.0, 0.3988134643494656, 1.0, 1.0, 0.3988134643494656, 1.0, 1.0, 0.6896843580280744, 6.9112, 6.9112, 170.5573041426782, 1672593.360768614, 1672593.360768614, 351826.6965220363], 
processed observation next is [1.0, 0.34782608695652173, 0.5545023696682465, 0.805, 1.0, 1.0, 0.2756788727101995, 1.0, 0.5, 0.2756788727101995, 1.0, 0.5, 0.6215662902781394, 0.0, 0.0, 0.8375144448122397, 0.46460926688017057, 0.46460926688017057, 0.5251144724209497], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.01063115], dtype=float32), 0.303232]. 
=============================================
[2019-03-25 22:11:45,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 9.8050786e-25 1.4891054e-20 2.1653064e-19 1.9901647e-22], sum to 1.0000
[2019-03-25 22:11:45,041] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9764
[2019-03-25 22:11:45,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1106798.720229462 W.
[2019-03-25 22:11:45,150] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.63333333333333, 68.66666666666667, 1.0, 2.0, 0.2639749002998902, 1.0, 2.0, 0.2639749002998902, 1.0, 1.0, 0.458437103987391, 6.9112, 6.9112, 170.5573041426782, 1106798.720229462, 1106798.720229462, 290382.2897431799], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2221800.0000, 
sim time next is 2222400.0000, 
raw observation next is [31.46666666666667, 69.33333333333334, 1.0, 2.0, 0.2673471982708085, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4642936703268953, 6.911199999999999, 6.9112, 168.912956510431, 747169.3948859458, 747169.3948859464, 212050.9258633342], 
processed observation next is [1.0, 0.7391304347826086, 0.6903633491311217, 0.6933333333333335, 1.0, 1.0, 0.11728578104916686, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.34669959795962835, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20754705413498495, 0.20754705413498512, 0.31649391919900627], 
reward next is 0.6835, 
noisyNet noise sample is [array([-0.45835283], dtype=float32), -1.3307248]. 
=============================================
[2019-03-25 22:11:56,945] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7679619e-16 5.3254071e-15 9.1917163e-17 2.3141419e-11 1.0000000e+00], sum to 1.0000
[2019-03-25 22:11:56,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5215
[2019-03-25 22:11:56,964] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.83333333333333, 62.66666666666667, 1.0, 2.0, 0.5703741646690633, 1.0, 2.0, 0.5703741646690633, 1.0, 2.0, 0.9905512984115384, 6.911199999999999, 6.9112, 170.5573041426782, 2392880.71522759, 2392880.715227591, 467222.2155619714], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2384400.0000, 
sim time next is 2385000.0000, 
raw observation next is [32.84999999999999, 62.5, 1.0, 2.0, 0.5678615667550437, 1.0, 2.0, 0.5678615667550437, 1.0, 2.0, 0.9861877467637139, 6.9112, 6.9112, 170.5573041426782, 2382329.609434088, 2382329.609434088, 465245.5721245038], 
processed observation next is [1.0, 0.6086956521739131, 0.7559241706161132, 0.625, 1.0, 1.0, 0.4793512852470406, 1.0, 1.0, 0.4793512852470406, 1.0, 1.0, 0.9831557887362364, 0.0, 0.0, 0.8375144448122397, 0.6617582248428023, 0.6617582248428023, 0.6943963763052295], 
reward next is 0.3056, 
noisyNet noise sample is [array([0.14740337], dtype=float32), 1.8096651]. 
=============================================
[2019-03-25 22:11:56,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[32.09774 ]
 [32.068718]
 [31.910978]
 [32.06966 ]
 [32.030827]], R is [[31.94602013]
 [31.92921448]
 [31.91842842]
 [31.96725273]
 [31.98711777]].
[2019-03-25 22:11:57,282] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 63811: loss 0.4098
[2019-03-25 22:11:57,283] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 63811: learning rate 0.0010
[2019-03-25 22:11:57,315] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63821: loss 0.0683
[2019-03-25 22:11:57,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63821: learning rate 0.0010
[2019-03-25 22:11:57,383] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63849: loss 0.0367
[2019-03-25 22:11:57,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63850: learning rate 0.0010
[2019-03-25 22:11:57,449] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63876: loss 0.0957
[2019-03-25 22:11:57,451] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63876: learning rate 0.0010
[2019-03-25 22:11:57,535] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63901: loss 0.0143
[2019-03-25 22:11:57,541] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63901: learning rate 0.0010
[2019-03-25 22:11:57,671] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 63955: loss 0.0091
[2019-03-25 22:11:57,673] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 63955: learning rate 0.0010
[2019-03-25 22:11:57,692] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 63962: loss 0.0948
[2019-03-25 22:11:57,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 63962: learning rate 0.0010
[2019-03-25 22:11:57,705] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63967: loss 0.1445
[2019-03-25 22:11:57,711] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63967: learning rate 0.0010
[2019-03-25 22:11:57,770] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63986: loss 0.0195
[2019-03-25 22:11:57,771] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63986: learning rate 0.0010
[2019-03-25 22:11:57,809] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64001: loss 0.0619
[2019-03-25 22:11:57,810] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64001: learning rate 0.0010
[2019-03-25 22:11:57,834] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64014: loss 0.1914
[2019-03-25 22:11:57,839] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64014: learning rate 0.0010
[2019-03-25 22:11:57,858] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 64024: loss 0.2531
[2019-03-25 22:11:57,862] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 64024: learning rate 0.0010
[2019-03-25 22:11:58,106] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64114: loss 0.0151
[2019-03-25 22:11:58,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64114: learning rate 0.0010
[2019-03-25 22:11:58,128] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 64121: loss 0.0034
[2019-03-25 22:11:58,131] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 64122: learning rate 0.0010
[2019-03-25 22:11:58,301] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64190: loss 0.0439
[2019-03-25 22:11:58,304] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64190: learning rate 0.0010
[2019-03-25 22:11:58,366] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 64209: loss 0.0107
[2019-03-25 22:11:58,368] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 64209: learning rate 0.0010
[2019-03-25 22:11:58,655] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1345145e-02 7.0565158e-11 6.6526910e-12 1.2695663e-06 9.8865360e-01], sum to 1.0000
[2019-03-25 22:11:58,668] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7116
[2019-03-25 22:11:58,675] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.65, 75.0, 1.0, 2.0, 0.1954405830974708, 1.0, 2.0, 0.1954405830974708, 1.0, 2.0, 0.3394156596518235, 6.9112, 6.9112, 170.5573041426782, 819337.0231931418, 819337.0231931418, 268596.174844872], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2406600.0000, 
sim time next is 2407200.0000, 
raw observation next is [30.56666666666667, 75.33333333333333, 1.0, 2.0, 0.1955985528769924, 1.0, 2.0, 0.1955985528769924, 1.0, 2.0, 0.3396900009174481, 6.911200000000001, 6.9112, 170.5573041426782, 819999.526171314, 819999.5261713134, 268638.8739792915], 
processed observation next is [1.0, 0.8695652173913043, 0.6477093206951029, 0.7533333333333333, 1.0, 1.0, 0.03084162997227999, 1.0, 1.0, 0.03084162997227999, 1.0, 1.0, 0.19474390355786353, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.22777764615869833, 0.22777764615869817, 0.40095354325267385], 
reward next is 0.5990, 
noisyNet noise sample is [array([-1.9507208], dtype=float32), 1.2111787]. 
=============================================
[2019-03-25 22:12:00,715] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.8777457e-10 7.7115662e-14 1.8566199e-18 6.3079111e-11 1.0000000e+00], sum to 1.0000
[2019-03-25 22:12:00,727] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3375
[2019-03-25 22:12:00,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.2748927196457008, 1.0, 2.0, 0.2748927196457008, 1.0, 2.0, 0.4713065974758501, 6.9112, 6.9112, 170.5573041426782, 1152599.754827846, 1152599.754827846, 293922.824624266], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2440800.0000, 
sim time next is 2441400.0000, 
raw observation next is [27.61666666666667, 84.16666666666667, 1.0, 2.0, 0.2693249885425958, 1.0, 2.0, 0.2693249885425958, 1.0, 2.0, 0.4619129324639969, 6.9112, 6.9112, 170.5573041426782, 1129242.479364362, 1129242.479364362, 291864.3506459107], 
processed observation next is [1.0, 0.2608695652173913, 0.5078988941548186, 0.8416666666666667, 1.0, 1.0, 0.11966866089469372, 1.0, 1.0, 0.11966866089469372, 1.0, 1.0, 0.3437962591024352, 0.0, 0.0, 0.8375144448122397, 0.31367846649010056, 0.31367846649010056, 0.43561843379986676], 
reward next is 0.5644, 
noisyNet noise sample is [array([0.12003258], dtype=float32), -0.59419346]. 
=============================================
[2019-03-25 22:12:02,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0146276e-10 3.1786375e-14 2.7754050e-17 1.6308720e-11 1.0000000e+00], sum to 1.0000
[2019-03-25 22:12:02,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2098
[2019-03-25 22:12:02,569] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.1, 86.0, 1.0, 2.0, 0.4572433982561608, 1.0, 2.0, 0.4572433982561608, 1.0, 2.0, 0.7848508553749357, 6.911200000000001, 6.9112, 170.5573041426782, 1917863.365523154, 1917863.365523153, 385324.616064568], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2473200.0000, 
sim time next is 2473800.0000, 
raw observation next is [27.2, 85.66666666666667, 1.0, 2.0, 0.4510493461944085, 1.0, 2.0, 0.4510493461944085, 1.0, 2.0, 0.7745931739634928, 6.911199999999999, 6.9112, 170.5573041426782, 1891860.073500847, 1891860.073500848, 381537.5717981997], 
processed observation next is [1.0, 0.6521739130434783, 0.4881516587677725, 0.8566666666666667, 1.0, 1.0, 0.33861367011374516, 1.0, 1.0, 0.33861367011374516, 1.0, 1.0, 0.7251136267847473, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5255166870835686, 0.5255166870835689, 0.5694590623853727], 
reward next is 0.4305, 
noisyNet noise sample is [array([0.9828136], dtype=float32), -0.55970466]. 
=============================================
[2019-03-25 22:12:04,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.4993024e-01 8.4359512e-11 1.1006783e-16 3.4027163e-11 3.5006979e-01], sum to 1.0000
[2019-03-25 22:12:04,193] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3507
[2019-03-25 22:12:04,198] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9514000937019702, 6.9112, 6.9112, 168.912956510431, 774467.0789518902, 774467.0789518902, 235246.3966781257], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [26.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9497210098183413, 6.9112, 6.9112, 168.912956510431, 773105.8638315027, 773105.8638315027, 234835.2264676486], 
processed observation next is [1.0, 0.9130434782608695, 0.4739336492890995, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9386841583150504, 0.0, 0.0, 0.8294399451523027, 0.2147516288420841, 0.2147516288420841, 0.35050033801141584], 
reward next is 0.6495, 
noisyNet noise sample is [array([0.12630175], dtype=float32), -1.7648714]. 
=============================================
[2019-03-25 22:12:06,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.5475254e-17 1.8306093e-22 6.0156437e-18 2.5619767e-18], sum to 1.0000
[2019-03-25 22:12:06,079] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4580
[2019-03-25 22:12:06,083] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666666, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9404658788038569, 6.911199999999999, 6.9112, 168.912956510431, 766350.0565213942, 766350.0565213948, 232618.3823423622], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2506200.0000, 
sim time next is 2506800.0000, 
raw observation next is [26.63333333333333, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9392497097696552, 6.9112, 6.9112, 168.912956510431, 765500.8184588561, 765500.8184588561, 232330.5433196622], 
processed observation next is [1.0, 0.0, 0.46129541864139006, 0.9433333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9259142802068964, 0.0, 0.0, 0.8294399451523027, 0.21263911623857112, 0.21263911623857112, 0.3467620049547197], 
reward next is 0.6532, 
noisyNet noise sample is [array([0.28609344], dtype=float32), -0.62754047]. 
=============================================
[2019-03-25 22:12:06,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.8301898e-15 1.4808944e-16 3.1920629e-15 6.1779615e-10], sum to 1.0000
[2019-03-25 22:12:06,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3050
[2019-03-25 22:12:06,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 948558.0227099655 W.
[2019-03-25 22:12:06,536] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.2262506926910325, 1.0, 2.0, 0.2262506926910325, 1.0, 2.0, 0.3898115782821743, 6.9112, 6.9112, 170.5573041426782, 948558.0227099655, 948558.0227099655, 277383.464800279], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2527200.0000, 
sim time next is 2527800.0000, 
raw observation next is [26.31666666666667, 95.66666666666667, 1.0, 2.0, 0.2373155956453147, 1.0, 2.0, 0.2373155956453147, 1.0, 2.0, 0.4087237113346375, 6.9112, 6.9112, 170.5573041426782, 994969.2470950843, 994969.2470950843, 280895.4949836272], 
processed observation next is [1.0, 0.2608695652173913, 0.4462875197472356, 0.9566666666666667, 1.0, 1.0, 0.08110312728351168, 1.0, 1.0, 0.08110312728351168, 1.0, 1.0, 0.27893135528614327, 0.0, 0.0, 0.8375144448122397, 0.2763803464153012, 0.2763803464153012, 0.4192470074382496], 
reward next is 0.5808, 
noisyNet noise sample is [array([-0.34816968], dtype=float32), -0.26834556]. 
=============================================
[2019-03-25 22:12:08,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.2157396e-07 4.0946526e-16 2.0268702e-22 1.7664142e-19 9.9999917e-01], sum to 1.0000
[2019-03-25 22:12:08,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7430
[2019-03-25 22:12:08,798] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.08333333333333, 69.66666666666667, 1.0, 2.0, 0.8895264955982162, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989144286689184, 6.9112, 168.9124924990002, 2140376.59690399, 2085080.389192937, 431790.2912529256], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2559000.0000, 
sim time next is 2559600.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5131457352727237, 1.0, 1.0, 0.5131457352727237, 1.0, 2.0, 0.8847330323429826, 6.911199999999999, 6.9112, 170.5573041426782, 2152575.903525292, 2152575.903525293, 423158.3393096201], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.7, 1.0, 1.0, 0.41342859671412485, 1.0, 0.5, 0.41342859671412485, 1.0, 1.0, 0.8594305272475397, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5979377509792477, 0.5979377509792481, 0.6315796109098807], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5964682], dtype=float32), 0.13950957]. 
=============================================
[2019-03-25 22:12:16,306] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 3.4721532e-28 4.4228145e-34 5.5667838e-27 9.9232949e-35], sum to 1.0000
[2019-03-25 22:12:16,319] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8193
[2019-03-25 22:12:16,324] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.16666666666667, 93.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6982616108689165, 6.9112, 6.9112, 168.912956510431, 597938.4926027788, 597938.4926027788, 182418.9276818118], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2663400.0000, 
sim time next is 2664000.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6942302914944608, 6.911200000000001, 6.9112, 168.912956510431, 594915.4175127856, 594915.4175127851, 181700.4679777462], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6271101115786106, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16525428264244046, 0.1652542826424403, 0.2711947283249943], 
reward next is 0.7288, 
noisyNet noise sample is [array([-1.2601824], dtype=float32), 0.12092872]. 
=============================================
[2019-03-25 22:12:16,337] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[85.47383 ]
 [85.45178 ]
 [85.409645]
 [85.37528 ]
 [85.320274]], R is [[85.40856934]
 [85.2822113 ]
 [85.15602875]
 [85.02993774]
 [84.9037323 ]].
[2019-03-25 22:12:18,648] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71771: loss 0.2216
[2019-03-25 22:12:18,651] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71771: learning rate 0.0010
[2019-03-25 22:12:18,696] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71788: loss 0.2393
[2019-03-25 22:12:18,701] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71789: learning rate 0.0010
[2019-03-25 22:12:18,789] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71827: loss 0.1724
[2019-03-25 22:12:18,792] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71827: learning rate 0.0010
[2019-03-25 22:12:18,812] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 71835: loss 0.1392
[2019-03-25 22:12:18,816] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 71836: learning rate 0.0010
[2019-03-25 22:12:18,893] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71860: loss 0.2034
[2019-03-25 22:12:18,897] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71861: learning rate 0.0010
[2019-03-25 22:12:18,996] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 71909: loss 0.2408
[2019-03-25 22:12:18,999] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 71909: learning rate 0.0010
[2019-03-25 22:12:19,012] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71914: loss 0.2417
[2019-03-25 22:12:19,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71916: learning rate 0.0010
[2019-03-25 22:12:19,029] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71919: loss 0.1865
[2019-03-25 22:12:19,030] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71920: learning rate 0.0010
[2019-03-25 22:12:19,192] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71981: loss 0.1226
[2019-03-25 22:12:19,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71982: learning rate 0.0010
[2019-03-25 22:12:19,245] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72000: loss 0.0266
[2019-03-25 22:12:19,250] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72001: learning rate 0.0010
[2019-03-25 22:12:19,375] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72046: loss 0.0267
[2019-03-25 22:12:19,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72047: learning rate 0.0010
[2019-03-25 22:12:19,408] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 72061: loss 0.0040
[2019-03-25 22:12:19,410] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 72061: learning rate 0.0010
[2019-03-25 22:12:19,682] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72164: loss 0.0007
[2019-03-25 22:12:19,685] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72165: learning rate 0.0010
[2019-03-25 22:12:19,753] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 72191: loss 0.0024
[2019-03-25 22:12:19,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 72191: learning rate 0.0010
[2019-03-25 22:12:19,763] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72197: loss 0.0152
[2019-03-25 22:12:19,766] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72197: learning rate 0.0010
[2019-03-25 22:12:20,203] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72356: loss 0.0007
[2019-03-25 22:12:20,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72356: learning rate 0.0010
[2019-03-25 22:12:24,906] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 9.4589945e-25 2.0570460e-31 9.9790005e-24 1.3373748e-26], sum to 1.0000
[2019-03-25 22:12:24,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2587
[2019-03-25 22:12:24,925] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6197591954724252, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169170.2589030449], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2782800.0000, 
sim time next is 2783400.0000, 
raw observation next is [22.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6643418855228287, 6.9112, 6.9112, 168.912956510431, 576176.0798346517, 576176.0798346517, 176463.9094860812], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5906608360034495, 0.0, 0.0, 0.8294399451523027, 0.16004891106518102, 0.16004891106518102, 0.2633789693822107], 
reward next is 0.7366, 
noisyNet noise sample is [array([-0.8431567], dtype=float32), -1.6638677]. 
=============================================
[2019-03-25 22:12:26,860] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 7.4712620e-20 8.2261102e-26 6.8852225e-17 1.1809546e-18], sum to 1.0000
[2019-03-25 22:12:26,873] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9775
[2019-03-25 22:12:26,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1096173.519215918 W.
[2019-03-25 22:12:26,892] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.33333333333333, 87.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.217333411316337, 6.9112, 168.9114738359504, 1096173.519215918, 878993.8472932654, 256622.569998173], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2812800.0000, 
sim time next is 2813400.0000, 
raw observation next is [24.5, 86.0, 1.0, 1.0, 0.248719160468891, 1.0, 1.0, 0.248719160468891, 1.0, 2.0, 0.4208366743588051, 6.911200000000001, 6.9112, 170.5573041426782, 1068030.440066829, 1068030.440066828, 286715.8845980715], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.86, 1.0, 0.5, 0.09484236201071204, 1.0, 0.5, 0.09484236201071204, 1.0, 1.0, 0.293703261413177, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.29667512224078585, 0.2966751222407855, 0.42793415611652463], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91784763], dtype=float32), -0.17105412]. 
=============================================
[2019-03-25 22:12:27,327] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-25 22:12:27,328] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:12:27,329] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:12:27,330] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:12:27,331] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:12:27,331] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:12:27,332] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:12:27,333] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:12:27,335] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:12:27,333] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:12:27,336] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:12:27,355] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-25 22:12:27,356] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-25 22:12:27,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-25 22:12:27,393] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-25 22:12:27,393] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-25 22:12:33,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:12:33,761] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.56666666666667, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8160492624047458, 6.9112, 6.9112, 168.912956510431, 725006.8517060048, 725006.8517060048, 204655.4510787747]
[2019-03-25 22:12:33,761] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:12:33,764] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 5.0487150e-21 3.2031908e-28 1.4633825e-19 7.9730620e-23], sampled 0.16186515329405915
[2019-03-25 22:12:40,583] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:12:40,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.79669162, 68.09009526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4343162093653928, 6.9112, 6.9112, 168.912956510431, 391266.7397798408, 391266.7397798408, 143959.1545028609]
[2019-03-25 22:12:40,590] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:12:40,592] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.1740438e-22 8.6685234e-30 8.1725210e-21 1.3065306e-24], sampled 0.530683929519783
[2019-03-25 22:13:00,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:13:00,194] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.28333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6721859043065344, 6.9112, 6.9112, 168.912956510431, 580398.1734074616, 580398.1734074616, 177830.9023215217]
[2019-03-25 22:13:00,195] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:13:00,198] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 4.3833196e-22 5.6665531e-30 9.3311046e-21 1.2924376e-24], sampled 0.6874237682226086
[2019-03-25 22:13:34,857] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:13:34,859] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.17567876, 96.22179192, 1.0, 2.0, 0.878276201402276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1227560.026979915, 1227560.026979915, 264029.7527301282]
[2019-03-25 22:13:34,862] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:13:34,864] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.9718620e-22 1.2805048e-28 1.5531573e-20 2.2619866e-20], sampled 0.4883088146638974
[2019-03-25 22:13:34,866] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1227560.026979915 W.
[2019-03-25 22:13:42,376] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:13:42,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9672327403804188, 6.9112, 6.9112, 168.912956510431, 781907.2612443268, 781907.2612443268, 238880.4764567941]
[2019-03-25 22:13:42,378] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:13:42,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.8712510e-24 1.7954002e-30 6.8208695e-22 1.3326669e-22], sampled 0.7733811855326194
[2019-03-25 22:13:47,976] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:13:47,976] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.23121133166666, 60.11584552166667, 1.0, 2.0, 0.6202919852485743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 925388.4339110205, 925388.4339110205, 211971.4916600031]
[2019-03-25 22:13:47,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:13:47,982] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 4.5775537e-21 1.3988008e-26 6.9846040e-19 6.3783624e-18], sampled 0.9308754587806237
[2019-03-25 22:13:47,984] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 925388.4339110205 W.
[2019-03-25 22:13:48,164] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:13:48,164] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025605003679191, 6.911200000000001, 6.9112, 168.9127977371928, 827968.4556724363, 827968.4556724357, 253806.6932567527]
[2019-03-25 22:13:48,167] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:13:48,170] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 2.4963876e-22 7.2457204e-31 8.4603621e-21 1.1455249e-25], sampled 0.9603434276799021
[2019-03-25 22:14:09,787] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:09,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.08333333333334, 80.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9978816223227387, 6.911200000000001, 6.9112, 168.9128771126892, 807997.1835060484, 807997.1835060478, 246709.9062192294]
[2019-03-25 22:14:09,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:14:09,794] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 8.7892292e-22 6.8097663e-30 3.2238595e-20 1.1017630e-24], sampled 0.09790012710757945
[2019-03-25 22:14:11,617] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:11,618] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.9, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8527451394451858, 6.911199999999999, 6.9112, 168.912956510431, 708266.7389208385, 708266.7389208392, 212896.6993846837]
[2019-03-25 22:14:11,620] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:14:11,624] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.3907157e-23 8.1456151e-32 6.1468587e-22 1.7700730e-25], sampled 0.5799379699890205
[2019-03-25 22:14:21,859] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:21,859] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.62803426666667, 73.79808846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9473110617278422, 6.911199999999999, 6.9112, 168.912956510431, 770754.9249970937, 770754.9249970943, 234226.6751473689]
[2019-03-25 22:14:21,859] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:14:21,864] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.6238284e-23 6.0547711e-29 1.5335349e-20 1.1563635e-20], sampled 0.8432614863866679
[2019-03-25 22:14:24,856] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:24,859] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.65, 78.5, 1.0, 2.0, 0.6327522412132346, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98858504845384, 6.9112, 168.9124212498625, 1769237.134468391, 1714337.691676312, 370794.6418829592]
[2019-03-25 22:14:24,860] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:14:24,864] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.1705691e-22 8.8632951e-31 4.9913807e-21 5.3822433e-23], sampled 0.7598399511896469
[2019-03-25 22:14:24,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1769237.134468391 W.
[2019-03-25 22:14:24,988] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:24,989] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.44749925, 96.24846971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8847722938940155, 6.911200000000001, 6.9112, 168.912956510431, 730686.8109104169, 730686.8109104162, 219934.0606309332]
[2019-03-25 22:14:24,990] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:14:24,992] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.2198897e-20 9.9905594e-28 1.0663624e-18 9.1647684e-22], sampled 0.15525819882477
[2019-03-25 22:14:29,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.20032303], dtype=float32), -0.09698589]
[2019-03-25 22:14:29,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.03333333333333, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9457558948566248, 6.9112, 6.9112, 168.912956510431, 769782.4558007751, 769782.4558007751, 233862.0180621698]
[2019-03-25 22:14:29,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:14:29,322] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.0927029e-21 1.4261698e-29 5.7223217e-20 2.3685663e-24], sampled 0.7190243106897125
[2019-03-25 22:14:35,419] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:14:35,505] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:14:35,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.6269 2989312438.8763 1566.0000
[2019-03-25 22:14:36,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.7244 2937773304.3596 1381.0000
[2019-03-25 22:14:36,109] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:14:37,126] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 75000, evaluation results [75000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8061.724446284878, 2937773304.359631, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7924.626933369283, 2989312438.876319, 1566.0]
[2019-03-25 22:14:49,979] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79795: loss 25.2498
[2019-03-25 22:14:49,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79795: learning rate 0.0010
[2019-03-25 22:14:50,108] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 79820: loss -16.8204
[2019-03-25 22:14:50,111] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 79821: learning rate 0.0010
[2019-03-25 22:14:50,207] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 79839: loss -43.2809
[2019-03-25 22:14:50,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 79839: learning rate 0.0010
[2019-03-25 22:14:50,307] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79853: loss 68.4261
[2019-03-25 22:14:50,310] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79853: learning rate 0.0010
[2019-03-25 22:14:50,313] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 79855: loss 9.3257
[2019-03-25 22:14:50,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 79855: learning rate 0.0010
[2019-03-25 22:14:50,470] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79875: loss -24.7187
[2019-03-25 22:14:50,472] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79875: learning rate 0.0010
[2019-03-25 22:14:50,632] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 79914: loss 0.0295
[2019-03-25 22:14:50,638] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79916: loss -6.0157
[2019-03-25 22:14:50,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 79914: learning rate 0.0010
[2019-03-25 22:14:50,719] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79917: learning rate 0.0010
[2019-03-25 22:14:50,848] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79952: loss 62.7379
[2019-03-25 22:14:50,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79952: learning rate 0.0010
[2019-03-25 22:14:50,941] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79962: loss -0.0894
[2019-03-25 22:14:50,944] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79963: learning rate 0.0010
[2019-03-25 22:14:51,171] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80038: loss 57.9748
[2019-03-25 22:14:51,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80038: learning rate 0.0010
[2019-03-25 22:14:51,517] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80141: loss 106.6441
[2019-03-25 22:14:51,519] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80143: learning rate 0.0010
[2019-03-25 22:14:51,520] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 80143: loss 30.5601
[2019-03-25 22:14:51,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 80143: learning rate 0.0010
[2019-03-25 22:14:51,773] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 80194: loss -78.1021
[2019-03-25 22:14:51,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 80194: learning rate 0.0010
[2019-03-25 22:14:51,960] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80238: loss 33.6003
[2019-03-25 22:14:51,961] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80239: learning rate 0.0010
[2019-03-25 22:14:52,326] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80359: loss 33.2477
[2019-03-25 22:14:52,328] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80360: learning rate 0.0010
[2019-03-25 22:14:53,608] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.5180472e-27 2.3284679e-31 2.0440643e-28 4.4060239e-27], sum to 1.0000
[2019-03-25 22:14:53,614] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3180
[2019-03-25 22:14:53,618] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5529982390478029, 6.9112, 6.9112, 168.912956510431, 485966.2386615312, 485966.2386615312, 159118.6082633739], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3016200.0000, 
sim time next is 3016800.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5541855337319791, 6.911200000000001, 6.9112, 168.912956510431, 487009.8538300132, 487009.8538300126, 159284.0033731791], 
processed observation next is [1.0, 0.9565217391304348, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45632382162436474, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13528051495278146, 0.1352805149527813, 0.23773731846743149], 
reward next is 0.7623, 
noisyNet noise sample is [array([-0.41896573], dtype=float32), 0.8094436]. 
=============================================
[2019-03-25 22:15:04,006] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 5.0350810e-20 1.4468894e-21 7.3579171e-23 3.2858731e-15], sum to 1.0000
[2019-03-25 22:15:04,013] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6345
[2019-03-25 22:15:04,017] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.5, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.825860298840093, 6.911199999999999, 6.9112, 168.912956510431, 688149.9741911981, 688149.9741911988, 207147.1965387255], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3180600.0000, 
sim time next is 3181200.0000, 
raw observation next is [25.33333333333333, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8234819374079129, 6.9112, 6.9112, 168.912956510431, 686684.7820207487, 686684.7820207487, 206656.1660879106], 
processed observation next is [1.0, 0.8260869565217391, 0.3996840442338071, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7847340700096497, 0.0, 0.0, 0.8294399451523027, 0.1907457727835413, 0.1907457727835413, 0.30844203893718], 
reward next is 0.6916, 
noisyNet noise sample is [array([1.0200719], dtype=float32), -0.5500941]. 
=============================================
[2019-03-25 22:15:04,882] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.3880814e-38 0.0000000e+00 0.0000000e+00 5.1804235e-36], sum to 1.0000
[2019-03-25 22:15:04,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5471
[2019-03-25 22:15:04,893] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.66666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8662471284308239, 6.9112, 6.9112, 168.912956510431, 715884.2964793191, 715884.2964793191, 215769.7896433326], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3190800.0000, 
sim time next is 3191400.0000, 
raw observation next is [25.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8569407053550923, 6.911200000000001, 6.9112, 168.912956510431, 709765.1329730997, 709765.1329730991, 213757.2578534998], 
processed observation next is [1.0, 0.9565217391304348, 0.40758293838862564, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8255374455549904, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19715698138141657, 0.19715698138141644, 0.3190406833634325], 
reward next is 0.6810, 
noisyNet noise sample is [array([1.4606999], dtype=float32), -0.36276418]. 
=============================================
[2019-03-25 22:15:08,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 5.3095432e-23 5.7414113e-27 1.2420920e-22 1.6476632e-32], sum to 1.0000
[2019-03-25 22:15:08,253] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2898
[2019-03-25 22:15:08,259] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.780960847360557, 6.9112, 6.9112, 168.912956510431, 656322.6936613321, 656322.6936613321, 197997.3647991244], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3222000.0000, 
sim time next is 3222600.0000, 
raw observation next is [26.16666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7838270871496524, 6.911199999999999, 6.9112, 168.912956510431, 658335.4629072178, 658335.4629072185, 198566.3620750686], 
processed observation next is [0.0, 0.30434782608695654, 0.4391785150078992, 0.8316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7363744965239664, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1828709619186716, 0.1828709619186718, 0.29636770458965467], 
reward next is 0.7036, 
noisyNet noise sample is [array([0.81815106], dtype=float32), 0.7300024]. 
=============================================
[2019-03-25 22:15:10,373] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 2.2380366e-23 1.3205424e-36 3.4519548e-31 4.2022499e-36], sum to 1.0000
[2019-03-25 22:15:10,382] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0859
[2019-03-25 22:15:10,387] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.33333333333333, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9740987419064622, 6.9112, 6.9112, 168.912956510431, 787916.7723366097, 787916.7723366097, 240621.1449860829], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3264600.0000, 
sim time next is 3265200.0000, 
raw observation next is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9583408097430927, 6.9112, 6.9112, 168.912956510431, 777834.200262403, 777834.200262403, 236840.3669144776], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9491961094427958, 0.0, 0.0, 0.8294399451523027, 0.21606505562844527, 0.21606505562844527, 0.3534930849469815], 
reward next is 0.6465, 
noisyNet noise sample is [array([-1.2699138], dtype=float32), -0.3964949]. 
=============================================
[2019-03-25 22:15:12,172] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 87740: loss 0.1284
[2019-03-25 22:15:12,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 87740: learning rate 0.0010
[2019-03-25 22:15:12,353] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 87809: loss 0.2217
[2019-03-25 22:15:12,356] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 87810: learning rate 0.0010
[2019-03-25 22:15:12,387] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 87820: loss 0.1342
[2019-03-25 22:15:12,388] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87820: loss 0.1605
[2019-03-25 22:15:12,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 87820: learning rate 0.0010
[2019-03-25 22:15:12,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87820: learning rate 0.0010
[2019-03-25 22:15:12,424] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 87830: loss 0.0786
[2019-03-25 22:15:12,425] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 87830: learning rate 0.0010
[2019-03-25 22:15:12,448] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87839: loss 0.0850
[2019-03-25 22:15:12,450] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87839: learning rate 0.0010
[2019-03-25 22:15:12,592] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87895: loss 0.0461
[2019-03-25 22:15:12,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87895: learning rate 0.0010
[2019-03-25 22:15:12,717] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 87940: loss 0.0365
[2019-03-25 22:15:12,718] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 87940: learning rate 0.0010
[2019-03-25 22:15:12,849] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87991: loss 0.0283
[2019-03-25 22:15:12,853] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87992: learning rate 0.0010
[2019-03-25 22:15:12,907] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 88012: loss 0.0388
[2019-03-25 22:15:12,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 88012: learning rate 0.0010
[2019-03-25 22:15:12,928] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88018: loss 0.0569
[2019-03-25 22:15:12,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88018: learning rate 0.0010
[2019-03-25 22:15:13,297] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 88153: loss 0.0060
[2019-03-25 22:15:13,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 88153: learning rate 0.0010
[2019-03-25 22:15:13,402] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 88193: loss 0.0215
[2019-03-25 22:15:13,404] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88193: loss 0.1038
[2019-03-25 22:15:13,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 88194: learning rate 0.0010
[2019-03-25 22:15:13,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88194: learning rate 0.0010
[2019-03-25 22:15:13,429] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88201: loss 0.0152
[2019-03-25 22:15:13,432] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88202: learning rate 0.0010
[2019-03-25 22:15:13,776] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88331: loss 0.0043
[2019-03-25 22:15:13,780] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88331: learning rate 0.0010
[2019-03-25 22:15:16,385] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.2172602e-12 1.8377793e-24 1.0813510e-18 1.7975095e-24], sum to 1.0000
[2019-03-25 22:15:16,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8185
[2019-03-25 22:15:16,403] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9471808280868261, 6.9112, 6.9112, 168.912956510431, 771720.5091685026, 771720.5091685026, 234247.261954663], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3353400.0000, 
sim time next is 3354000.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9449019081054465, 6.911199999999999, 6.9112, 168.912956510431, 770303.0270442418, 770303.0270442424, 233712.4933499254], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9328072050066419, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21397306306784494, 0.2139730630678451, 0.3488246169401872], 
reward next is 0.6512, 
noisyNet noise sample is [array([0.7808856], dtype=float32), 0.6711322]. 
=============================================
[2019-03-25 22:15:16,417] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.895622]
 [48.420887]
 [48.33229 ]
 [47.72066 ]
 [47.854168]], R is [[49.02255249]
 [49.18270111]
 [49.3400116 ]
 [49.49459839]
 [49.64593124]].
[2019-03-25 22:15:18,117] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.9707529e-18 6.7901564e-30 2.0199702e-23 8.8709893e-20], sum to 1.0000
[2019-03-25 22:15:18,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4764
[2019-03-25 22:15:18,146] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1337160.75720047 W.
[2019-03-25 22:15:18,237] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.33333333333334, 1.0, 2.0, 0.9566423349472709, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565102534, 1337160.75720047, 1337160.757200469, 285992.8939208431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3379200.0000, 
sim time next is 3379800.0000, 
raw observation next is [26.05, 93.66666666666667, 1.0, 2.0, 0.9263251736377331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104309, 1294758.662986451, 1294758.662986451, 277280.9337661878], 
processed observation next is [1.0, 0.08695652173913043, 0.43364928909952616, 0.9366666666666668, 1.0, 1.0, 0.9112351489611242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523021, 0.3596551841629031, 0.3596551841629031, 0.41385213994953407], 
reward next is 0.5861, 
noisyNet noise sample is [array([-0.13263753], dtype=float32), 2.3804462]. 
=============================================
[2019-03-25 22:15:20,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.00000000e+00 1.11384795e-26 4.03381497e-26 6.04732450e-26
 3.70958291e-25], sum to 1.0000
[2019-03-25 22:15:20,242] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1252
[2019-03-25 22:15:20,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1465377.138878739 W.
[2019-03-25 22:15:20,262] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.66666666666666, 85.66666666666667, 1.0, 2.0, 0.5241578182415242, 1.0, 1.0, 0.5241578182415242, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1465377.138878739, 1465377.138878739, 307936.355493026], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3397200.0000, 
sim time next is 3397800.0000, 
raw observation next is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.3697705989766054, 1.0, 2.0, 0.3697705989766054, 1.0, 1.0, 0.6421692454166575, 6.911199999999999, 6.9112, 170.5573041426782, 1550701.668689531, 1550701.668689532, 336786.2380062629], 
processed observation next is [1.0, 0.30434782608695654, 0.5655608214849924, 0.8483333333333333, 1.0, 1.0, 0.24068746864651255, 1.0, 1.0, 0.24068746864651255, 1.0, 0.5, 0.5636210309959238, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.43075046352486973, 0.43075046352487, 0.5026660268750193], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0850027], dtype=float32), 0.2160667]. 
=============================================
[2019-03-25 22:15:20,679] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.3923459e-22 3.3559702e-19 3.5350839e-23 2.6221853e-23], sum to 1.0000
[2019-03-25 22:15:20,687] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6530
[2019-03-25 22:15:20,699] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2576530.241629653 W.
[2019-03-25 22:15:20,705] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 69.0, 1.0, 2.0, 0.9211564894479448, 1.0, 2.0, 0.9211564894479448, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2576530.241629653, 2576530.241629652, 483173.3923082269], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3411000.0000, 
sim time next is 3411600.0000, 
raw observation next is [32.66666666666667, 68.33333333333333, 1.0, 2.0, 0.7103906691551503, 1.0, 2.0, 0.6757853740918378, 1.0, 1.0, 1.03, 7.005098551464871, 6.9112, 170.5573041426782, 2835612.347716574, 2768348.99575342, 524631.7409310304], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.6833333333333332, 1.0, 1.0, 0.6510730953676509, 1.0, 1.0, 0.6093799687853467, 1.0, 0.5, 1.0365853658536586, 0.009389855146487136, 0.0, 0.8375144448122397, 0.7876700965879373, 0.7689858321537278, 0.7830324491507916], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37070096], dtype=float32), 1.9374175]. 
=============================================
[2019-03-25 22:15:22,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 8.3087279e-21 1.6107175e-17 1.2266429e-22 2.0544738e-31], sum to 1.0000
[2019-03-25 22:15:22,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4499
[2019-03-25 22:15:22,512] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.33333333333333, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8737560659996109, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 217404.2832877164], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.873802262652742, 6.911200000000001, 6.9112, 168.912956510431, 721076.5801110404, 721076.5801110397, 217426.5726696299], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8461003203082219, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20029905003084456, 0.20029905003084436, 0.3245172726412387], 
reward next is 0.6755, 
noisyNet noise sample is [array([-0.22142327], dtype=float32), 1.3104084]. 
=============================================
[2019-03-25 22:15:22,537] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[31.082533]
 [31.261017]
 [31.2811  ]
 [31.281612]
 [31.55508 ]], R is [[31.49543762]
 [31.85599899]
 [32.21233368]
 [32.5643158 ]
 [32.91172028]].
[2019-03-25 22:15:29,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.6711554e-27 0.0000000e+00 4.5625087e-36 2.5839235e-25], sum to 1.0000
[2019-03-25 22:15:29,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4377
[2019-03-25 22:15:29,648] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8768247497306804, 6.911199999999999, 6.9112, 168.912956510431, 724179.5685848698, 724179.5685848705, 218130.771849735], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3541800.0000, 
sim time next is 3542400.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8748369664543195, 6.9112, 6.9112, 168.912956510431, 722537.2784442991, 722537.2784442991, 217681.846473759], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8473621542125847, 0.0, 0.0, 0.8294399451523027, 0.20070479956786086, 0.20070479956786086, 0.3248982783190433], 
reward next is 0.6751, 
noisyNet noise sample is [array([0.14650397], dtype=float32), 0.653287]. 
=============================================
[2019-03-25 22:15:33,845] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95795: loss 67.3030
[2019-03-25 22:15:33,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95796: learning rate 0.0010
[2019-03-25 22:15:33,872] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 95804: loss 29.3190
[2019-03-25 22:15:33,879] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 95806: learning rate 0.0010
[2019-03-25 22:15:33,944] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95825: loss -40.6913
[2019-03-25 22:15:33,945] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95825: learning rate 0.0010
[2019-03-25 22:15:34,042] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 95872: loss 2.3046
[2019-03-25 22:15:34,045] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 95872: learning rate 0.0010
[2019-03-25 22:15:34,100] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95897: loss -80.5183
[2019-03-25 22:15:34,102] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95897: learning rate 0.0010
[2019-03-25 22:15:34,129] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95907: loss -46.4685
[2019-03-25 22:15:34,132] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95907: learning rate 0.0010
[2019-03-25 22:15:34,164] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95917: loss -141.0350
[2019-03-25 22:15:34,166] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95918: learning rate 0.0010
[2019-03-25 22:15:34,267] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95958: loss -80.2435
[2019-03-25 22:15:34,268] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95958: learning rate 0.0010
[2019-03-25 22:15:34,298] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 95970: loss -19.9922
[2019-03-25 22:15:34,302] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 95972: learning rate 0.0010
[2019-03-25 22:15:34,552] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 96060: loss -41.0373
[2019-03-25 22:15:34,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 96060: learning rate 0.0010
[2019-03-25 22:15:34,586] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96072: loss -69.4100
[2019-03-25 22:15:34,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96072: learning rate 0.0010
[2019-03-25 22:15:34,616] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 96083: loss -150.9059
[2019-03-25 22:15:34,617] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 96083: learning rate 0.0010
[2019-03-25 22:15:34,642] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96092: loss -82.1073
[2019-03-25 22:15:34,644] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96093: learning rate 0.0010
[2019-03-25 22:15:34,779] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 96147: loss -29.4781
[2019-03-25 22:15:34,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 96149: learning rate 0.0010
[2019-03-25 22:15:35,049] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96248: loss -45.2954
[2019-03-25 22:15:35,052] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96248: learning rate 0.0010
[2019-03-25 22:15:35,115] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 96272: loss 28.0851
[2019-03-25 22:15:35,118] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 96273: learning rate 0.0010
[2019-03-25 22:15:37,095] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.9071663e-26 8.2774719e-36 6.0980445e-30 2.3312122e-14], sum to 1.0000
[2019-03-25 22:15:37,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5075
[2019-03-25 22:15:37,195] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8469899778485553, 6.9112, 6.9112, 168.912956510431, 703714.157063147, 703714.157063147, 211642.5986865934], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3627600.0000, 
sim time next is 3628200.0000, 
raw observation next is [28.0, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8379606813761249, 6.911200000000001, 6.9112, 168.912956510431, 697488.0883621316, 697488.0883621309, 209720.0919838609], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7483333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8023910748489328, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19374669121170324, 0.19374669121170304, 0.31301506266247897], 
reward next is 0.6870, 
noisyNet noise sample is [array([-1.115162], dtype=float32), -0.75306946]. 
=============================================
[2019-03-25 22:15:44,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 3.177857e-17 1.227266e-21 3.875319e-18 2.112663e-12], sum to 1.0000
[2019-03-25 22:15:44,124] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8837
[2019-03-25 22:15:44,132] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1139105.030208582 W.
[2019-03-25 22:15:44,139] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4050404828004037, 1.0, 2.0, 0.4050404828004037, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1139105.030208582, 1139105.030208582, 274246.1731383282], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3726600.0000, 
sim time next is 3727200.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.339551584166635, 1.0, 2.0, 0.339551584166635, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 955214.414052115, 955214.414052115, 258762.8988289151], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.74, 1.0, 1.0, 0.20427901706823492, 1.0, 1.0, 0.20427901706823492, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2653373372366986, 0.2653373372366986, 0.3862132818342016], 
reward next is 0.6138, 
noisyNet noise sample is [array([0.15643609], dtype=float32), 0.39316007]. 
=============================================
[2019-03-25 22:15:44,741] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 6.8791582e-17 1.9889904e-20 1.9321246e-20 5.0547314e-12], sum to 1.0000
[2019-03-25 22:15:44,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3574
[2019-03-25 22:15:44,751] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 79.0, 1.0, 2.0, 0.2870736702758963, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4857836236332352, 6.911200000000001, 6.9112, 168.912956510431, 821851.7093033842, 821851.7093033836, 218168.2459917111], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3733200.0000, 
sim time next is 3733800.0000, 
raw observation next is [26.0, 79.00000000000001, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.872214093478616, 6.9112, 6.9112, 168.912956510431, 738919.9568566037, 738919.9568566037, 217561.4809133394], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.7900000000000001, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8441635286324586, 0.0, 0.0, 0.8294399451523027, 0.20525554357127881, 0.20525554357127881, 0.32471862822886477], 
reward next is 0.6753, 
noisyNet noise sample is [array([-1.153336], dtype=float32), -0.302804]. 
=============================================
[2019-03-25 22:15:45,154] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-25 22:15:45,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:15:45,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:15:45,158] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:15:45,158] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:15:45,160] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:15:45,162] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:15:45,165] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:15:45,167] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:15:45,170] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:15:45,171] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:15:45,190] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-25 22:15:45,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-25 22:15:45,191] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-25 22:15:45,250] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-25 22:15:45,275] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-25 22:15:49,443] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:15:49,447] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.3, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1824415642970355, 6.9112, 6.9112, 170.5573041426782, 479022.8493820302, 479022.8493820302, 227119.7720694946]
[2019-03-25 22:15:49,449] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:15:49,451] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0804322e-03 3.5666804e-17 4.8520482e-25 3.3735522e-19 9.9491960e-01], sampled 0.7276333304856566
[2019-03-25 22:16:11,642] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:16:11,643] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.30267508, 86.89836809, 1.0, 2.0, 0.2891704622868722, 1.0, 2.0, 0.2891704622868722, 1.0, 2.0, 0.496301098784347, 6.9112, 6.9112, 171.5212843490159, 1212495.007589562, 1212495.007589562, 299676.2153708698]
[2019-03-25 22:16:11,644] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:16:11,647] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.8788609e-04 2.5385805e-12 8.1193104e-22 2.5312696e-16 9.9981219e-01], sampled 0.5194121191176776
[2019-03-25 22:16:15,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:16:15,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.66666666666667, 87.16666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2305032672202804, 6.911199999999999, 6.9112, 169.0403247858759, 591504.4098756107, 591504.4098756114, 243512.1886460782]
[2019-03-25 22:16:15,244] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:16:15,246] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0289829e-04 5.0259848e-17 9.8086541e-25 2.0449986e-19 9.9969709e-01], sampled 0.5299769116975825
[2019-03-25 22:16:34,591] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:16:34,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 90.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8797683732666718, 6.911200000000001, 6.9112, 168.912956510431, 728961.3664931458, 728961.3664931452, 218879.5929695981]
[2019-03-25 22:16:34,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:16:34,599] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9963295e-01 1.0374375e-15 1.5632568e-25 8.3090778e-19 3.6701022e-04], sampled 0.655478944284237
[2019-03-25 22:16:38,462] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:16:38,463] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.73094333333333, 51.59740991, 1.0, 2.0, 0.1922897308763248, 1.0, 2.0, 0.1922897308763248, 1.0, 2.0, 0.3339146923487184, 6.9112, 6.9112, 184.5923449428631, 806099.8459962833, 806099.8459962833, 271806.5400237326]
[2019-03-25 22:16:38,464] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:16:38,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0402014e-04 7.2777845e-17 1.2130748e-25 2.7994607e-19 9.9939597e-01], sampled 0.834233942527408
[2019-03-25 22:17:19,419] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:17:19,420] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.0, 74.0, 1.0, 2.0, 0.20163456622939, 1.0, 2.0, 0.20163456622939, 1.0, 2.0, 0.3501725599704443, 6.9112, 6.9112, 169.0403247858759, 845317.0111394114, 845317.0111394114, 270007.7398766867]
[2019-03-25 22:17:19,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:17:19,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3195868e-03 1.7142551e-16 6.8511608e-26 3.4089511e-19 9.9268037e-01], sampled 0.38420242966539153
[2019-03-25 22:17:19,898] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.23762032], dtype=float32), -0.12113148]
[2019-03-25 22:17:19,898] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.66666666666666, 76.66666666666667, 1.0, 2.0, 0.3731918319524908, 1.0, 2.0, 0.3731918319524908, 1.0, 2.0, 0.6481107956767364, 6.9112, 6.9112, 178.6582176852504, 1565007.909312133, 1565007.909312133, 340573.8081487788]
[2019-03-25 22:17:19,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:17:19,902] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2043233e-06 9.2325673e-18 2.8562447e-26 3.7876616e-20 9.9999475e-01], sampled 0.29676261098156476
[2019-03-25 22:17:53,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 5564.1839 3390946838.8858 269.0000
[2019-03-25 22:17:54,160] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6416.2372 3505815094.6835 137.0000
[2019-03-25 22:17:54,219] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 5094.9504 3676661412.2357 682.0000
[2019-03-25 22:17:54,335] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6022.7635 3516075726.8431 261.0000
[2019-03-25 22:17:54,340] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 4894.9969 3513147011.7618 102.0000
[2019-03-25 22:17:55,357] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 100000, evaluation results [100000.0, 5094.950449261831, 3676661412.235692, 682.0, 6416.237243050843, 3505815094.683455, 137.0, 4894.996918388983, 3513147011.7618303, 102.0, 6022.763541953797, 3516075726.843136, 261.0, 5564.183937544412, 3390946838.885829, 269.0]
[2019-03-25 22:18:01,297] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 5.0714009e-19 6.1189573e-34 3.6831304e-25 7.1720438e-25], sum to 1.0000
[2019-03-25 22:18:01,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2160
[2019-03-25 22:18:01,417] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.66666666666666, 67.5, 1.0, 1.0, 0.3025041641870322, 1.0, 1.0, 0.3025041641870322, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 845460.2924258512, 845460.2924258512, 250375.7063787913], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3837000.0000, 
sim time next is 3837600.0000, 
raw observation next is [33.0, 67.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.043403931962408, 6.9112, 168.9120924071501, 922627.0519574295, 828837.5123140934, 254813.0836035805], 
processed observation next is [0.0, 0.43478260869565216, 0.7630331753554502, 0.67, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.013220393196240821, 0.0, 0.8294357020096054, 0.2562852922103971, 0.23023264230947038, 0.38031803522922464], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.2839384], dtype=float32), 0.4251601]. 
=============================================
[2019-03-25 22:18:04,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 3.0173754e-26 0.0000000e+00 3.2269913e-31 6.9473556e-32], sum to 1.0000
[2019-03-25 22:18:04,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5970
[2019-03-25 22:18:04,528] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.66666666666667, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9222524612702238, 6.9112, 6.9112, 168.912956510431, 755208.0488084322, 755208.0488084322, 228416.1492213074], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3878400.0000, 
sim time next is 3879000.0000, 
raw observation next is [29.5, 74.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9265760925415825, 6.9112, 6.9112, 168.912956510431, 757747.8344441624, 757747.8344441624, 229402.564254031], 
processed observation next is [0.0, 0.9130434782608695, 0.5971563981042655, 0.745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9104586494409542, 0.0, 0.0, 0.8294399451523027, 0.2104855095678229, 0.2104855095678229, 0.3423918869463149], 
reward next is 0.6576, 
noisyNet noise sample is [array([0.57816917], dtype=float32), 0.4089511]. 
=============================================
[2019-03-25 22:18:04,548] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.757965]
 [70.56628 ]
 [70.17445 ]
 [70.695755]
 [70.378914]], R is [[70.73007202]
 [70.68185425]
 [70.6330719 ]
 [70.58917236]
 [70.5454483 ]].
[2019-03-25 22:18:04,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 5.4194401e-27 0.0000000e+00 7.0809290e-30 5.3544877e-33], sum to 1.0000
[2019-03-25 22:18:04,711] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0364
[2019-03-25 22:18:04,719] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.33333333333334, 63.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9613325959820412, 6.9112, 6.9112, 168.912956510431, 778638.2848594616, 778638.2848594616, 237496.199988455], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [32.16666666666666, 65.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9760979358675476, 6.9112, 6.9112, 168.912956510431, 788166.8405789012, 788166.8405789012, 241048.756746183], 
processed observation next is [0.0, 0.782608695652174, 0.7235387045813582, 0.6516666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9708511413018871, 0.0, 0.0, 0.8294399451523027, 0.21893523349413924, 0.21893523349413924, 0.35977426380027316], 
reward next is 0.6402, 
noisyNet noise sample is [array([0.95797014], dtype=float32), -1.4253756]. 
=============================================
[2019-03-25 22:18:05,306] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103683: loss 0.1657
[2019-03-25 22:18:05,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103685: learning rate 0.0010
[2019-03-25 22:18:05,502] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 103760: loss 0.2519
[2019-03-25 22:18:05,504] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 103760: learning rate 0.0010
[2019-03-25 22:18:05,513] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 103763: loss 0.3543
[2019-03-25 22:18:05,515] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 103763: learning rate 0.0010
[2019-03-25 22:18:05,763] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103854: loss 0.6021
[2019-03-25 22:18:05,766] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103854: learning rate 0.0010
[2019-03-25 22:18:05,769] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 103854: loss 0.3719
[2019-03-25 22:18:05,775] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 103856: learning rate 0.0010
[2019-03-25 22:18:05,823] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103873: loss 0.2295
[2019-03-25 22:18:05,827] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103873: learning rate 0.0010
[2019-03-25 22:18:05,886] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103895: loss 0.2861
[2019-03-25 22:18:05,890] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103895: learning rate 0.0010
[2019-03-25 22:18:06,051] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103956: loss 0.1022
[2019-03-25 22:18:06,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103956: learning rate 0.0010
[2019-03-25 22:18:06,193] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104011: loss 0.1911
[2019-03-25 22:18:06,196] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104011: learning rate 0.0010
[2019-03-25 22:18:06,302] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104052: loss 0.2766
[2019-03-25 22:18:06,303] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104052: learning rate 0.0010
[2019-03-25 22:18:06,364] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 104077: loss 0.3248
[2019-03-25 22:18:06,366] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 104078: learning rate 0.0010
[2019-03-25 22:18:06,426] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 104100: loss 0.6873
[2019-03-25 22:18:06,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 104100: learning rate 0.0010
[2019-03-25 22:18:06,450] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104109: loss 0.1896
[2019-03-25 22:18:06,453] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104109: learning rate 0.0010
[2019-03-25 22:18:06,780] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 104233: loss 0.1587
[2019-03-25 22:18:06,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 104233: learning rate 0.0010
[2019-03-25 22:18:06,913] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104280: loss 0.1862
[2019-03-25 22:18:06,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104281: learning rate 0.0010
[2019-03-25 22:18:06,941] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 104288: loss 0.1882
[2019-03-25 22:18:06,946] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 104288: learning rate 0.0010
[2019-03-25 22:18:10,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 9.2236514e-23 2.3117859e-30 1.6760922e-21 1.4551069e-24], sum to 1.0000
[2019-03-25 22:18:10,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4493
[2019-03-25 22:18:10,497] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 871505.963098749 W.
[2019-03-25 22:18:10,504] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 72.5, 1.0, 2.0, 0.3118194731363971, 1.0, 2.0, 0.3118194731363971, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 871505.963098749, 871505.963098749, 252217.7602993324], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3955800.0000, 
sim time next is 3956400.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.2095626493932127, 1.0, 2.0, 0.2095626493932127, 1.0, 1.0, 0.3639410185688376, 6.9112, 6.9112, 170.5573041426782, 878564.5948233723, 878564.5948233723, 272554.1143268483], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.75, 1.0, 1.0, 0.04766584264242493, 1.0, 1.0, 0.04766584264242493, 1.0, 0.5, 0.2243183153278507, 0.0, 0.0, 0.8375144448122397, 0.2440457207842701, 0.2440457207842701, 0.40679718556246014], 
reward next is 0.5932, 
noisyNet noise sample is [array([-2.0357327], dtype=float32), -0.12527797]. 
=============================================
[2019-03-25 22:18:15,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.6775480e-24 2.9648089e-27 3.3741984e-19 1.2108414e-21], sum to 1.0000
[2019-03-25 22:18:15,172] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0409
[2019-03-25 22:18:15,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2645784.572378939 W.
[2019-03-25 22:18:15,191] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666666, 63.5, 1.0, 2.0, 0.9458899858007356, 1.0, 2.0, 0.9458899858007356, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2645784.572378939, 2645784.572378939, 497147.5688766504], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4017000.0000, 
sim time next is 4017600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.7050225757045301, 1.0, 2.0, 0.6731013273665277, 1.0, 1.0, 1.03, 7.005098128184737, 6.9112, 170.5573041426782, 2824337.288730404, 2757074.239980028, 522920.1037733746], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.63, 1.0, 1.0, 0.6446055128970242, 1.0, 1.0, 0.6061461775500334, 1.0, 0.5, 1.0365853658536586, 0.009389812818473686, 0.0, 0.8375144448122397, 0.7845381357584456, 0.7658539555500078, 0.7804777668259322], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9539759], dtype=float32), -1.1689335]. 
=============================================
[2019-03-25 22:18:26,800] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111696: loss 68.2137
[2019-03-25 22:18:26,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111697: learning rate 0.0010
[2019-03-25 22:18:27,081] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 111797: loss 106.0829
[2019-03-25 22:18:27,084] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 111798: learning rate 0.0010
[2019-03-25 22:18:27,156] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 111828: loss 115.1758
[2019-03-25 22:18:27,157] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 111829: learning rate 0.0010
[2019-03-25 22:18:27,293] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 111874: loss 83.8107
[2019-03-25 22:18:27,296] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 111874: learning rate 0.0010
[2019-03-25 22:18:27,434] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111928: loss 123.1660
[2019-03-25 22:18:27,436] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111928: learning rate 0.0010
[2019-03-25 22:18:27,449] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 111934: loss 110.4927
[2019-03-25 22:18:27,450] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 111934: learning rate 0.0010
[2019-03-25 22:18:27,454] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111935: loss 109.6260
[2019-03-25 22:18:27,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111936: learning rate 0.0010
[2019-03-25 22:18:27,636] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 112004: loss 63.0855
[2019-03-25 22:18:27,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 112004: learning rate 0.0010
[2019-03-25 22:18:27,656] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112010: loss 85.7682
[2019-03-25 22:18:27,661] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112010: learning rate 0.0010
[2019-03-25 22:18:27,694] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112020: loss 37.4735
[2019-03-25 22:18:27,697] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112021: learning rate 0.0010
[2019-03-25 22:18:27,791] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112058: loss 48.4711
[2019-03-25 22:18:27,797] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 112059: loss 63.5988
[2019-03-25 22:18:27,799] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112059: learning rate 0.0010
[2019-03-25 22:18:27,800] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 112060: learning rate 0.0010
[2019-03-25 22:18:27,870] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112087: loss 43.3360
[2019-03-25 22:18:27,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112087: learning rate 0.0010
[2019-03-25 22:18:28,027] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 112147: loss 7.4825
[2019-03-25 22:18:28,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 112148: learning rate 0.0010
[2019-03-25 22:18:28,248] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112228: loss 7.0790
[2019-03-25 22:18:28,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112228: learning rate 0.0010
[2019-03-25 22:18:28,343] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 112256: loss 0.5196
[2019-03-25 22:18:28,345] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 112256: learning rate 0.0010
[2019-03-25 22:18:29,231] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.5101094e-22 9.6538189e-29 1.9518356e-14 5.3540017e-24], sum to 1.0000
[2019-03-25 22:18:29,242] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2016
[2019-03-25 22:18:29,254] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 884641.8806431611 W.
[2019-03-25 22:18:29,260] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 72.33333333333334, 1.0, 1.0, 0.6330324310091466, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9127920013108, 884641.8806431611, 884641.8806431611, 207060.2214498298], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4225800.0000, 
sim time next is 4226400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.203370717348803, 1.0, 1.0, 0.203370717348803, 1.0, 1.0, 0.353187680310916, 6.9112, 6.9112, 170.5573041426782, 852595.3933269118, 852595.3933269118, 270784.5759267983], 
processed observation next is [1.0, 0.9565217391304348, 0.7156398104265403, 0.71, 1.0, 1.0, 0.04020568355277468, 1.0, 0.5, 0.04020568355277468, 1.0, 0.5, 0.21120448818404391, 0.0, 0.0, 0.8375144448122397, 0.23683205370191995, 0.23683205370191995, 0.40415608347283327], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05808575], dtype=float32), -0.347051]. 
=============================================
[2019-03-25 22:18:31,009] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3981454e-03 1.8519619e-19 1.2097201e-17 9.9760193e-01 1.8655147e-12], sum to 1.0000
[2019-03-25 22:18:31,025] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0653
[2019-03-25 22:18:31,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.542222312989073, 1.0, 2.0, 0.542222312989073, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1515915.385777603, 1515915.385777604, 313896.9734180394], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4246800.0000, 
sim time next is 4247400.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.54062914209819, 1.0, 2.0, 0.54062914209819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1511458.145438796, 1511458.145438796, 313363.3517152366], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 0.44654113505806026, 1.0, 1.0, 0.44654113505806026, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41984948484411, 0.41984948484411, 0.46770649509736806], 
reward next is 0.5323, 
noisyNet noise sample is [array([-1.0100883], dtype=float32), 0.5000412]. 
=============================================
[2019-03-25 22:18:32,891] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.5123123e-36 0.0000000e+00 6.6092499e-29 0.0000000e+00], sum to 1.0000
[2019-03-25 22:18:32,901] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4330
[2019-03-25 22:18:32,910] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3861681.80472879 W.
[2019-03-25 22:18:32,918] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.16666666666667, 56.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 8.239120998655029, 6.9112, 170.5573041426782, 3861681.80472879, 2910437.977019561, 545666.2642245495], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4277400.0000, 
sim time next is 4278000.0000, 
raw observation next is [37.33333333333334, 56.00000000000001, 1.0, 2.0, 0.7973150325229624, 1.0, 2.0, 0.719247555775744, 1.0, 1.0, 1.03, 7.005105406610752, 6.9112, 170.5573041426782, 3018201.080445671, 2950932.817862673, 553829.8287148023], 
processed observation next is [1.0, 0.5217391304347826, 0.9684044233807272, 0.56, 1.0, 1.0, 0.7558012440035692, 1.0, 1.0, 0.6617440431033059, 1.0, 0.5, 1.0365853658536586, 0.009390540661075164, 0.0, 0.8375144448122397, 0.8383891890126863, 0.8197035605174092, 0.8266116846489586], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1268914], dtype=float32), -0.7805962]. 
=============================================
[2019-03-25 22:18:32,930] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[36.78427 ]
 [37.05045 ]
 [36.724884]
 [36.27356 ]
 [36.4122  ]], R is [[36.56533432]
 [36.19968033]
 [35.83768463]
 [35.47930908]
 [35.12451553]].
[2019-03-25 22:18:33,474] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.3586591e-25 6.6424467e-26 6.1218655e-11 1.2568616e-24], sum to 1.0000
[2019-03-25 22:18:33,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3454
[2019-03-25 22:18:33,490] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3161219.652661082 W.
[2019-03-25 22:18:33,495] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.0, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.262425136526153, 6.9112, 170.5573041426782, 3161219.652661082, 2909622.800442294, 551802.6466524772], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4276800.0000, 
sim time next is 4277400.0000, 
raw observation next is [37.16666666666667, 56.5, 1.0, 2.0, 0.9348938118315426, 1.0, 2.0, 0.7880369454300339, 1.0, 1.0, 1.03, 7.005116260628635, 6.9112, 170.5573041426782, 3307246.376648845, 3239970.338891324, 605796.2323018201], 
processed observation next is [1.0, 0.5217391304347826, 0.9605055292259086, 0.565, 1.0, 1.0, 0.9215588094355935, 1.0, 1.0, 0.7446228258193179, 1.0, 0.5, 1.0365853658536586, 0.009391626062863524, 0.0, 0.8375144448122397, 0.9186795490691236, 0.8999917608031456, 0.9041734810474926], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6986214], dtype=float32), -0.5535607]. 
=============================================
[2019-03-25 22:18:48,180] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119641: loss 0.0073
[2019-03-25 22:18:48,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119641: learning rate 0.0010
[2019-03-25 22:18:48,547] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119775: loss 0.2072
[2019-03-25 22:18:48,551] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119775: learning rate 0.0010
[2019-03-25 22:18:48,596] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 119794: loss 0.2561
[2019-03-25 22:18:48,599] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 119794: learning rate 0.0010
[2019-03-25 22:18:48,648] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 119812: loss 0.1454
[2019-03-25 22:18:48,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 119814: learning rate 0.0010
[2019-03-25 22:18:48,701] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.8299074e-32 0.0000000e+00 1.5016967e-33 9.4729103e-37], sum to 1.0000
[2019-03-25 22:18:48,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7914
[2019-03-25 22:18:48,715] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8528438861517773, 6.9112, 6.9112, 168.912956510431, 709499.2557531429, 709499.2557531429, 212953.4331631324], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8531097193103538, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 213011.9217008209], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.82086551135409, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19714457833561494, 0.19714457833561513, 0.3179282413445088], 
reward next is 0.6821, 
noisyNet noise sample is [array([-0.6300519], dtype=float32), -0.6304236]. 
=============================================
[2019-03-25 22:18:48,825] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119878: loss 0.0805
[2019-03-25 22:18:48,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119878: learning rate 0.0010
[2019-03-25 22:18:48,830] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119878: loss 0.0367
[2019-03-25 22:18:48,834] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119878: learning rate 0.0010
[2019-03-25 22:18:48,918] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119912: loss 0.3294
[2019-03-25 22:18:48,920] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119913: learning rate 0.0010
[2019-03-25 22:18:49,113] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 119989: loss 0.1180
[2019-03-25 22:18:49,118] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 119989: learning rate 0.0010
[2019-03-25 22:18:49,152] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 120006: loss 0.0306
[2019-03-25 22:18:49,154] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 120006: learning rate 0.0010
[2019-03-25 22:18:49,211] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120025: loss 0.0391
[2019-03-25 22:18:49,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120026: learning rate 0.0010
[2019-03-25 22:18:49,290] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120054: loss 0.0091
[2019-03-25 22:18:49,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120056: learning rate 0.0010
[2019-03-25 22:18:49,330] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120068: loss 0.0364
[2019-03-25 22:18:49,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120069: learning rate 0.0010
[2019-03-25 22:18:49,454] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120113: loss 0.0144
[2019-03-25 22:18:49,456] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120113: learning rate 0.0010
[2019-03-25 22:18:49,593] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 120166: loss 0.0106
[2019-03-25 22:18:49,597] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 120168: learning rate 0.0010
[2019-03-25 22:18:49,911] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 9.921563e-33 0.000000e+00 5.584335e-33 0.000000e+00], sum to 1.0000
[2019-03-25 22:18:49,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4912
[2019-03-25 22:18:49,932] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8152891572257049, 6.9112, 6.9112, 168.912956510431, 683393.7833797967, 683393.7833797967, 205015.5752762454], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4508400.0000, 
sim time next is 4509000.0000, 
raw observation next is [26.0, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8207608721418982, 6.911200000000001, 6.9112, 168.912956510431, 686984.0802107382, 686984.0802107377, 206145.9407921921], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7814156977340222, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1908289111696495, 0.19082891116964934, 0.3076805086450628], 
reward next is 0.6923, 
noisyNet noise sample is [array([1.2607635], dtype=float32), -1.1569945]. 
=============================================
[2019-03-25 22:18:49,952] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[63.128475]
 [63.318157]
 [63.49546 ]
 [63.709435]
 [63.795788]], R is [[62.9572258 ]
 [63.02166367]
 [63.08677673]
 [63.15135193]
 [63.21265793]].
[2019-03-25 22:18:49,993] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120314: loss 0.2317
[2019-03-25 22:18:49,998] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120314: learning rate 0.0010
[2019-03-25 22:18:50,260] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120412: loss 0.0690
[2019-03-25 22:18:50,261] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120414: learning rate 0.0010
[2019-03-25 22:18:51,133] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.5435792e-32 0.0000000e+00 3.4066812e-30 0.0000000e+00], sum to 1.0000
[2019-03-25 22:18:51,141] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9556
[2019-03-25 22:18:51,149] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8458092791192436, 6.9112, 6.9112, 168.912956510431, 703467.9188803635, 703467.9188803635, 211407.4317959672], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4527000.0000, 
sim time next is 4527600.0000, 
raw observation next is [28.0, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8532527179655012, 6.911200000000001, 6.9112, 168.912956510431, 708384.2538676553, 708384.2538676547, 212998.8884464223], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8210398999579283, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19677340385212647, 0.1967734038521263, 0.3179087887260034], 
reward next is 0.6821, 
noisyNet noise sample is [array([1.349239], dtype=float32), -0.35750505]. 
=============================================
[2019-03-25 22:18:58,514] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.1414567e-22 8.6102191e-16 6.4602956e-26 2.7134277e-19], sum to 1.0000
[2019-03-25 22:18:58,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9401
[2019-03-25 22:18:58,535] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3383243.574940484 W.
[2019-03-25 22:18:58,542] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.572006309304159, 6.9112, 170.5573041426782, 3383243.574940484, 2909881.135703846, 550011.3173476256], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4633200.0000, 
sim time next is 4633800.0000, 
raw observation next is [34.66666666666667, 61.16666666666666, 1.0, 2.0, 0.7477291823318206, 1.0, 2.0, 0.694454630680173, 1.0, 1.0, 1.03, 7.005101495859446, 6.9112, 170.5573041426782, 2914040.388987314, 2846774.927834556, 536829.0660191218], 
processed observation next is [1.0, 0.6521739130434783, 0.8420221169036337, 0.6116666666666666, 1.0, 1.0, 0.6960592558214707, 1.0, 1.0, 0.6318730490122566, 1.0, 0.5, 1.0365853658536586, 0.009390149585944575, 0.0, 0.8375144448122397, 0.8094556636075873, 0.7907708132873766, 0.8012374119688385], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5135595], dtype=float32), -0.5128576]. 
=============================================
[2019-03-25 22:19:02,590] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-25 22:19:02,594] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:19:02,595] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:19:02,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:19:02,597] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:19:02,598] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:19:02,600] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:19:02,603] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:19:02,598] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:19:02,604] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:19:02,605] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:19:02,625] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-25 22:19:02,627] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-25 22:19:02,662] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-25 22:19:02,662] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-25 22:19:02,700] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-25 22:20:04,707] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.56100017], dtype=float32), -0.14436504]
[2019-03-25 22:20:04,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.36594811, 60.50413756, 1.0, 2.0, 0.605349716593009, 0.0, 2.0, 0.0, 1.0, 2.0, 1.028498759313878, 6.911199999999999, 6.9112, 168.9128726486037, 1692555.892131281, 1692555.892131282, 366088.1974589813]
[2019-03-25 22:20:04,709] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:20:04,712] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.2333669e-25 3.4058080e-24 1.2030108e-27 2.4791360e-21], sampled 0.5917694812180306
[2019-03-25 22:20:04,714] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1692555.892131281 W.
[2019-03-25 22:20:09,080] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.56100017], dtype=float32), -0.14436504]
[2019-03-25 22:20:09,082] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 1.0, 0.5901009509026782, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129030488005, 824623.2670373523, 824623.2670373529, 198915.6181380441]
[2019-03-25 22:20:09,083] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:20:09,086] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 8.5346495e-23 1.3984714e-24 6.0639109e-26 2.6712744e-22], sampled 0.27451266353027926
[2019-03-25 22:20:39,595] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.56100017], dtype=float32), -0.14436504]
[2019-03-25 22:20:39,596] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.45479182666667, 82.23546844666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9701629830678018, 6.9112, 6.9112, 168.912956510431, 788205.062093003, 788205.062093003, 239818.0561495743]
[2019-03-25 22:20:39,601] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:20:39,604] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.7911908e-25 1.1106058e-27 1.4605762e-28 3.9166530e-24], sampled 0.09292547495558867
[2019-03-25 22:21:00,816] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.56100017], dtype=float32), -0.14436504]
[2019-03-25 22:21:00,819] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.55884317166667, 96.32033778666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6494012637442214, 6.9112, 6.9112, 168.912956510431, 564025.6955494323, 564025.6955494323, 173948.5634180288]
[2019-03-25 22:21:00,820] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:21:00,823] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.4800421e-23 8.9313727e-25 3.9081548e-27 3.2407505e-21], sampled 0.696233212402002
[2019-03-25 22:21:09,834] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4652 3319525033.0383 2143.0000
[2019-03-25 22:21:10,868] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:21:11,059] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7922.4239 2989573851.3048 1566.0000
[2019-03-25 22:21:11,137] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.6231 2937985494.9653 1381.0000
[2019-03-25 22:21:11,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3407 3185096716.9286 2464.0000
[2019-03-25 22:21:12,270] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 125000, evaluation results [125000.0, 7287.465185338501, 3319525033.038298, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8059.623113280159, 2937985494.9653387, 1381.0, 7031.340658675213, 3185096716.9285784, 2464.0, 7922.423932225668, 2989573851.3048186, 1566.0]
[2019-03-25 22:21:13,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.2182469e-18 1.5798447e-27 2.2317522e-24 9.9769707e-19], sum to 1.0000
[2019-03-25 22:21:13,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3030
[2019-03-25 22:21:13,726] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2208653.205499702 W.
[2019-03-25 22:21:13,731] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.83333333333334, 67.5, 1.0, 2.0, 0.9383060949985145, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.993576747564893, 6.9112, 168.9124665911481, 2208653.205499702, 2150212.475091744, 445368.5422807522], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4716600.0000, 
sim time next is 4717200.0000, 
raw observation next is [30.66666666666667, 69.0, 1.0, 2.0, 0.9615765202301173, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993790225121447, 6.9112, 168.9123983219866, 2241222.55298638, 2182630.398372871, 452168.9843376062], 
processed observation next is [1.0, 0.6086956521739131, 0.6524486571879939, 0.69, 1.0, 1.0, 0.9537066508796594, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008259022512144743, 0.0, 0.8294372041915008, 0.6225618202739945, 0.6062862217702419, 0.6748790811009048], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.25466385], dtype=float32), -1.2076278]. 
=============================================
[2019-03-25 22:21:14,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999988e-01 6.8326379e-15 1.9382929e-23 2.6802960e-23 1.1776096e-07], sum to 1.0000
[2019-03-25 22:21:14,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0031
[2019-03-25 22:21:14,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2744530.57878369 W.
[2019-03-25 22:21:14,852] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.66702494786016, 1.0, 2.0, 0.6541025134443426, 1.0, 1.0, 1.03, 7.005095132247872, 6.9112, 170.5573041426782, 2744530.57878369, 2677269.676144757, 511115.4246947106], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4723200.0000, 
sim time next is 4723800.0000, 
raw observation next is [31.83333333333334, 67.5, 1.0, 2.0, 0.5975313269727722, 1.0, 2.0, 0.5975313269727722, 1.0, 2.0, 1.03, 6.919872014632063, 6.9112, 170.5573041426782, 2506926.968562057, 2500714.851726109, 487084.896227794], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.675, 1.0, 1.0, 0.5150979843045448, 1.0, 1.0, 0.5150979843045448, 1.0, 1.0, 1.0365853658536586, 0.0008672014632063352, 0.0, 0.8375144448122397, 0.6963686023783491, 0.6946430143683636, 0.7269923824295433], 
reward next is 0.2296, 
noisyNet noise sample is [array([-0.93825704], dtype=float32), 0.6718561]. 
=============================================
[2019-03-25 22:21:19,635] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 127754: loss 0.7842
[2019-03-25 22:21:19,638] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 127755: learning rate 0.0010
[2019-03-25 22:21:19,676] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127767: loss 0.8312
[2019-03-25 22:21:19,677] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127767: loss -113.9908
[2019-03-25 22:21:19,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127768: learning rate 0.0010
[2019-03-25 22:21:19,683] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127768: learning rate 0.0010
[2019-03-25 22:21:19,784] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 127811: loss -245.7550
[2019-03-25 22:21:19,785] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 127811: learning rate 0.0010
[2019-03-25 22:21:19,962] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127873: loss 1.1080
[2019-03-25 22:21:19,965] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127874: learning rate 0.0010
[2019-03-25 22:21:20,069] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127916: loss 1.3166
[2019-03-25 22:21:20,072] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127916: learning rate 0.0010
[2019-03-25 22:21:20,099] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127926: loss -314.1375
[2019-03-25 22:21:20,103] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127926: learning rate 0.0010
[2019-03-25 22:21:20,279] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127992: loss -223.6822
[2019-03-25 22:21:20,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127993: learning rate 0.0010
[2019-03-25 22:21:20,314] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 128000: loss 2.1838
[2019-03-25 22:21:20,318] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 128000: learning rate 0.0010
[2019-03-25 22:21:20,421] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 128041: loss 2.8310
[2019-03-25 22:21:20,422] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 128041: learning rate 0.0010
[2019-03-25 22:21:20,438] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128047: loss -342.4432
[2019-03-25 22:21:20,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128047: learning rate 0.0010
[2019-03-25 22:21:20,486] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128064: loss -246.7895
[2019-03-25 22:21:20,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128064: learning rate 0.0010
[2019-03-25 22:21:20,553] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128086: loss -293.9160
[2019-03-25 22:21:20,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128090: learning rate 0.0010
[2019-03-25 22:21:20,735] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128155: loss -349.8645
[2019-03-25 22:21:20,736] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128156: learning rate 0.0010
[2019-03-25 22:21:21,068] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128284: loss -123.3664
[2019-03-25 22:21:21,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128285: learning rate 0.0010
[2019-03-25 22:21:21,084] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128289: loss -207.9137
[2019-03-25 22:21:21,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128289: learning rate 0.0010
[2019-03-25 22:21:31,981] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2861062e-05 7.4437172e-19 2.4055361e-28 1.5780234e-27 9.9998713e-01], sum to 1.0000
[2019-03-25 22:21:31,990] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2799
[2019-03-25 22:21:31,998] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.4822400559019792, 1.0, 1.0, 0.4822400559019792, 1.0, 2.0, 0.8270390104156218, 6.9112, 6.9112, 170.5573041426782, 2022808.416952966, 2022808.416952966, 401259.048462202], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [30.0, 67.33333333333334, 1.0, 2.0, 0.4607867224931087, 1.0, 2.0, 0.4607867224931087, 1.0, 2.0, 0.7896585110011406, 6.9112, 6.9112, 170.5573041426782, 1932738.902846278, 1932738.902846278, 387335.9930710163], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6733333333333335, 1.0, 1.0, 0.3503454487868779, 1.0, 1.0, 0.3503454487868779, 1.0, 1.0, 0.7434859890257811, 0.0, 0.0, 0.8375144448122397, 0.5368719174572995, 0.5368719174572995, 0.5781134224940542], 
reward next is 0.4219, 
noisyNet noise sample is [array([-2.10134], dtype=float32), -0.10406446]. 
=============================================
[2019-03-25 22:21:37,813] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 9.7294946e-37 0.0000000e+00 0.0000000e+00 2.0775117e-37], sum to 1.0000
[2019-03-25 22:21:37,822] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6433
[2019-03-25 22:21:37,827] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8763365716800123, 6.9112, 6.9112, 168.912956510431, 722333.8987893036, 722333.8987893036, 217967.3698234989], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5051400.0000, 
sim time next is 5052000.0000, 
raw observation next is [31.0, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8779639522613574, 6.911199999999999, 6.9112, 168.912956510431, 723566.5849451869, 723566.5849451876, 218331.1913994753], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.6300000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8511755515382408, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20099071804032967, 0.20099071804032986, 0.32586744984996313], 
reward next is 0.6741, 
noisyNet noise sample is [array([0.0711281], dtype=float32), -0.6917748]. 
=============================================
[2019-03-25 22:21:37,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[66.8446  ]
 [66.791214]
 [66.72386 ]
 [66.728134]
 [66.72286 ]], R is [[66.90238953]
 [66.90804291]
 [66.9143219 ]
 [66.92168427]
 [66.93029785]].
[2019-03-25 22:21:41,073] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135718: loss 5.6709
[2019-03-25 22:21:41,076] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135719: learning rate 0.0010
[2019-03-25 22:21:41,154] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135749: loss 7.7814
[2019-03-25 22:21:41,156] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135750: learning rate 0.0010
[2019-03-25 22:21:41,173] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135754: loss 6.6341
[2019-03-25 22:21:41,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135754: learning rate 0.0010
[2019-03-25 22:21:41,256] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135783: loss 7.0270
[2019-03-25 22:21:41,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135786: learning rate 0.0010
[2019-03-25 22:21:41,317] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 135808: loss 8.7977
[2019-03-25 22:21:41,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 135808: learning rate 0.0010
[2019-03-25 22:21:41,430] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135848: loss 9.9408
[2019-03-25 22:21:41,432] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135848: learning rate 0.0010
[2019-03-25 22:21:41,877] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136008: loss 5.9696
[2019-03-25 22:21:41,883] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136009: learning rate 0.0010
[2019-03-25 22:21:41,926] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136028: loss 5.0889
[2019-03-25 22:21:41,929] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136029: learning rate 0.0010
[2019-03-25 22:21:41,956] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136037: loss 4.6981
[2019-03-25 22:21:41,960] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136037: learning rate 0.0010
[2019-03-25 22:21:41,994] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136054: loss 5.7413
[2019-03-25 22:21:41,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136054: learning rate 0.0010
[2019-03-25 22:21:42,026] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 136064: loss 6.4962
[2019-03-25 22:21:42,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 136064: learning rate 0.0010
[2019-03-25 22:21:42,094] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136091: loss 6.3031
[2019-03-25 22:21:42,095] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136091: learning rate 0.0010
[2019-03-25 22:21:42,200] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136129: loss 6.4852
[2019-03-25 22:21:42,207] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136133: learning rate 0.0010
[2019-03-25 22:21:42,214] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136134: loss 7.5723
[2019-03-25 22:21:42,219] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136136: learning rate 0.0010
[2019-03-25 22:21:42,341] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.3345441e-22 7.7400198e-33 2.7750219e-26 7.0487144e-10], sum to 1.0000
[2019-03-25 22:21:42,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8032
[2019-03-25 22:21:42,361] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8568016392164056, 6.911200000000001, 6.9112, 168.912956510431, 712394.8416375022, 712394.8416375016, 213814.1485409866], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5101200.0000, 
sim time next is 5101800.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.854232274770525, 6.911200000000001, 6.9112, 168.912956510431, 710540.4599146695, 710540.4599146689, 213255.6872539773], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8222344814274694, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19737234997629707, 0.19737234997629693, 0.3182920705283243], 
reward next is 0.6817, 
noisyNet noise sample is [array([0.77580947], dtype=float32), -1.3400111]. 
=============================================
[2019-03-25 22:21:42,665] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136300: loss 5.5037
[2019-03-25 22:21:42,667] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136300: learning rate 0.0010
[2019-03-25 22:21:42,678] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136304: loss 5.8854
[2019-03-25 22:21:42,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136304: learning rate 0.0010
[2019-03-25 22:21:42,873] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 4.9413998e-30 4.4115493e-38 2.8287543e-31 5.9584062e-27], sum to 1.0000
[2019-03-25 22:21:42,877] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9825
[2019-03-25 22:21:42,887] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8697083488618673, 6.9112, 6.9112, 168.912956510431, 719738.71351649, 719738.71351649, 216579.0932432204], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5124000.0000, 
sim time next is 5124600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8757498500277652, 6.9112, 6.9112, 168.912956510431, 723918.9855987724, 723918.9855987724, 217910.3570406907], 
processed observation next is [0.0, 0.30434782608695654, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8484754268631283, 0.0, 0.0, 0.8294399451523027, 0.20108860711077012, 0.20108860711077012, 0.32523933886670253], 
reward next is 0.6748, 
noisyNet noise sample is [array([-0.769466], dtype=float32), -1.0483581]. 
=============================================
[2019-03-25 22:21:56,839] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.4173374e-33 1.8573175e-37 0.0000000e+00 2.3058526e-35], sum to 1.0000
[2019-03-25 22:21:56,848] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8162
[2019-03-25 22:21:56,855] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2031935.012569896 W.
[2019-03-25 22:21:56,860] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.21666666666667, 52.0, 1.0, 2.0, 0.4844137862272285, 1.0, 2.0, 0.4844137862272285, 1.0, 2.0, 0.8412665485896204, 6.9112, 6.9112, 170.5573041426782, 2031935.012569896, 2031935.012569896, 404536.401216493], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5316600.0000, 
sim time next is 5317200.0000, 
raw observation next is [36.2, 52.0, 1.0, 2.0, 0.5306088834222425, 1.0, 2.0, 0.5306088834222425, 1.0, 2.0, 0.9214921554652717, 6.911200000000001, 6.9112, 170.5573041426782, 2225905.55829815, 2225905.558298149, 436959.4994932861], 
processed observation next is [1.0, 0.5652173913043478, 0.9146919431279622, 0.52, 1.0, 1.0, 0.4344685342436657, 1.0, 1.0, 0.4344685342436657, 1.0, 1.0, 0.9042587261771606, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6183070995272639, 0.6183070995272637, 0.6521783574526657], 
reward next is 0.3478, 
noisyNet noise sample is [array([-1.3111658], dtype=float32), -1.0491909]. 
=============================================
[2019-03-25 22:22:02,705] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143780: loss -25.7486
[2019-03-25 22:22:02,708] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143781: learning rate 0.0010
[2019-03-25 22:22:02,753] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143798: loss -28.2560
[2019-03-25 22:22:02,754] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143798: learning rate 0.0010
[2019-03-25 22:22:02,792] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143811: loss -64.5430
[2019-03-25 22:22:02,799] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143813: learning rate 0.0010
[2019-03-25 22:22:02,810] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143817: loss -30.3974
[2019-03-25 22:22:02,813] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143817: learning rate 0.0010
[2019-03-25 22:22:02,880] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143842: loss -4.6346
[2019-03-25 22:22:02,885] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143843: learning rate 0.0010
[2019-03-25 22:22:02,906] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 143852: loss -55.6118
[2019-03-25 22:22:02,908] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 143853: learning rate 0.0010
[2019-03-25 22:22:03,183] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 143955: loss -54.7351
[2019-03-25 22:22:03,187] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 143957: learning rate 0.0010
[2019-03-25 22:22:03,284] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143991: loss -60.5657
[2019-03-25 22:22:03,287] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143991: learning rate 0.0010
[2019-03-25 22:22:03,326] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144008: loss -48.8717
[2019-03-25 22:22:03,332] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144009: learning rate 0.0010
[2019-03-25 22:22:03,423] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144045: loss -19.2640
[2019-03-25 22:22:03,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144045: learning rate 0.0010
[2019-03-25 22:22:03,481] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 144063: loss -17.9539
[2019-03-25 22:22:03,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 144063: learning rate 0.0010
[2019-03-25 22:22:03,604] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144109: loss -26.8089
[2019-03-25 22:22:03,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144109: learning rate 0.0010
[2019-03-25 22:22:03,660] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144130: loss -17.9187
[2019-03-25 22:22:03,662] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144130: learning rate 0.0010
[2019-03-25 22:22:03,793] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144178: loss -8.1624
[2019-03-25 22:22:03,797] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144178: learning rate 0.0010
[2019-03-25 22:22:03,851] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144201: loss -35.6198
[2019-03-25 22:22:03,855] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144201: learning rate 0.0010
[2019-03-25 22:22:03,963] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144239: loss -10.3322
[2019-03-25 22:22:03,965] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144239: learning rate 0.0010
[2019-03-25 22:22:09,501] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 5.8596618e-25 6.8689231e-27 4.3820344e-36 7.2072779e-35], sum to 1.0000
[2019-03-25 22:22:09,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3299
[2019-03-25 22:22:09,519] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2926850.472069062 W.
[2019-03-25 22:22:09,523] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.90000000000001, 56.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 6.935630165655905, 6.9112, 170.5573041426782, 2926850.472069062, 2909350.150654136, 553513.9389412191], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5487600.0000, 
sim time next is 5488200.0000, 
raw observation next is [36.0, 55.0, 1.0, 2.0, 0.9858356399617352, 1.0, 2.0, 0.9858356399617352, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2757641.384970357, 2757641.384970357, 520493.9191745874], 
processed observation next is [1.0, 0.5217391304347826, 0.9052132701421801, 0.55, 1.0, 1.0, 0.9829345059779941, 1.0, 1.0, 0.9829345059779941, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7660114958250992, 0.7660114958250992, 0.7768565957829663], 
reward next is 0.2231, 
noisyNet noise sample is [array([0.8821512], dtype=float32), 0.18129468]. 
=============================================
[2019-03-25 22:22:19,380] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-25 22:22:19,383] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:22:19,383] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:22:19,384] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:22:19,384] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:22:19,385] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:22:19,387] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:22:19,386] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:22:19,392] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:22:19,395] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:22:19,395] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:22:19,406] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-25 22:22:19,407] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-25 22:22:19,445] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-25 22:22:19,445] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-25 22:22:19,460] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-25 22:22:23,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.42832044], dtype=float32), -0.08321629]
[2019-03-25 22:22:23,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.72518557, 77.63853715666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6330385194444282, 6.911200000000001, 6.9112, 168.912956510431, 547452.754403397, 547452.7544033964, 171306.552588746]
[2019-03-25 22:22:23,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:22:23,818] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 7.3673891e-21 3.3566146e-24 1.4235312e-25 1.2475131e-10], sampled 0.5407627759733649
[2019-03-25 22:22:24,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.42832044], dtype=float32), -0.08321629]
[2019-03-25 22:22:24,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.7, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4822160984484265, 6.911199999999999, 6.9112, 168.912956510431, 430118.4523829383, 430118.4523829389, 149660.2207369458]
[2019-03-25 22:22:24,303] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:22:24,305] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999988e-01 2.7924147e-19 1.3299351e-21 4.2112618e-24 6.3921441e-08], sampled 0.3658857609230163
[2019-03-25 22:22:44,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.42832044], dtype=float32), -0.08321629]
[2019-03-25 22:22:44,204] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.40304463, 88.97978700499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8114700374409546, 6.9112, 6.9112, 168.912956510431, 677298.1806478035, 677298.1806478035, 204143.7469201553]
[2019-03-25 22:22:44,205] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:22:44,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9999976e-01 4.3532474e-19 7.6038310e-22 1.1596082e-23 1.9751550e-07], sampled 0.006591816257189853
[2019-03-25 22:22:58,178] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.42832044], dtype=float32), -0.08321629]
[2019-03-25 22:22:58,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.46666666666667, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7061870409448671, 6.911199999999999, 6.9112, 168.912956510431, 604275.1853533658, 604275.1853533664, 183844.2317804411]
[2019-03-25 22:22:58,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:22:58,181] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999988e-01 1.8909276e-19 5.2267101e-22 1.9821692e-24 8.8417345e-08], sampled 0.3913819812111414
[2019-03-25 22:24:28,014] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.3375 3320022098.3022 2142.0000
[2019-03-25 22:24:28,079] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7027.8654 3185688303.3076 2464.0000
[2019-03-25 22:24:28,117] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7917.5875 2990619472.8252 1566.0000
[2019-03-25 22:24:28,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8054.6425 2938960483.7872 1379.0000
[2019-03-25 22:24:28,594] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7345.1215 3106279041.4627 2012.0000
[2019-03-25 22:24:29,611] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 150000, evaluation results [150000.0, 7286.337455378656, 3320022098.302194, 2142.0, 7345.121503747565, 3106279041.4627304, 2012.0, 8054.64250268971, 2938960483.7871904, 1379.0, 7027.865432931943, 3185688303.3075843, 2464.0, 7917.587500754839, 2990619472.8251824, 1566.0]
[2019-03-25 22:24:30,852] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 1.6421013e-20 4.2466939e-29 2.9556794e-23 1.7441002e-25], sum to 1.0000
[2019-03-25 22:24:30,860] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1558
[2019-03-25 22:24:30,864] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8595861484186654, 6.911200000000001, 6.9112, 168.912956510431, 711945.8785060223, 711945.8785060217, 214342.3324466258], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5641200.0000, 
sim time next is 5641800.0000, 
raw observation next is [27.66666666666667, 80.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8624667635603621, 6.911199999999999, 6.9112, 168.912956510431, 713925.4316630303, 713925.431663031, 214968.0862270939], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012641, 0.8033333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8322765409272708, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983126199063973, 0.1983126199063975, 0.32084788989118496], 
reward next is 0.6792, 
noisyNet noise sample is [array([0.43336573], dtype=float32), -0.8066493]. 
=============================================
[2019-03-25 22:24:34,307] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 151749: loss 0.0448
[2019-03-25 22:24:34,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 151750: learning rate 0.0010
[2019-03-25 22:24:34,342] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151763: loss 0.0181
[2019-03-25 22:24:34,344] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151763: learning rate 0.0010
[2019-03-25 22:24:34,416] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 151788: loss 0.0001
[2019-03-25 22:24:34,420] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151789: loss 0.0058
[2019-03-25 22:24:34,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 151788: learning rate 0.0010
[2019-03-25 22:24:34,423] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151789: learning rate 0.0010
[2019-03-25 22:24:34,475] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151812: loss 0.0014
[2019-03-25 22:24:34,484] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151813: learning rate 0.0010
[2019-03-25 22:24:34,524] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151829: loss 0.0131
[2019-03-25 22:24:34,525] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151829: learning rate 0.0010
[2019-03-25 22:24:34,779] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151923: loss 0.4219
[2019-03-25 22:24:34,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151924: learning rate 0.0010
[2019-03-25 22:24:35,002] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152007: loss 0.0032
[2019-03-25 22:24:35,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152008: learning rate 0.0010
[2019-03-25 22:24:35,069] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152028: loss 0.0191
[2019-03-25 22:24:35,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152029: learning rate 0.0010
[2019-03-25 22:24:35,138] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152057: loss 0.0445
[2019-03-25 22:24:35,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152057: learning rate 0.0010
[2019-03-25 22:24:35,335] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 152130: loss 0.0617
[2019-03-25 22:24:35,340] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 152130: learning rate 0.0010
[2019-03-25 22:24:35,369] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152142: loss 0.0840
[2019-03-25 22:24:35,373] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152143: learning rate 0.0010
[2019-03-25 22:24:35,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.1461497e-33 0.0000000e+00 4.0331635e-36 0.0000000e+00], sum to 1.0000
[2019-03-25 22:24:35,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7766
[2019-03-25 22:24:35,393] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152151: loss 0.0462
[2019-03-25 22:24:35,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152151: learning rate 0.0010
[2019-03-25 22:24:35,400] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.16666666666667, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9037745753233549, 6.911199999999999, 6.9112, 168.912956510431, 743904.9314034265, 743904.931403427, 224225.2271815274], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5694000.0000, 
sim time next is 5694600.0000, 
raw observation next is [27.08333333333333, 85.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8990415909767324, 6.9112, 6.9112, 168.912956510431, 740647.1620163413, 740647.1620163413, 223149.492732468], 
processed observation next is [0.0, 0.9130434782608695, 0.4826224328593995, 0.8583333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8768799889960152, 0.0, 0.0, 0.8294399451523027, 0.20573532278231704, 0.20573532278231704, 0.33305894437681793], 
reward next is 0.6669, 
noisyNet noise sample is [array([0.37882373], dtype=float32), -1.2520734]. 
=============================================
[2019-03-25 22:24:35,423] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152160: loss 0.0274
[2019-03-25 22:24:35,435] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152163: learning rate 0.0010
[2019-03-25 22:24:35,463] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152173: loss 0.0183
[2019-03-25 22:24:35,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152173: learning rate 0.0010
[2019-03-25 22:24:35,727] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152270: loss 0.0095
[2019-03-25 22:24:35,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152270: learning rate 0.0010
[2019-03-25 22:24:37,137] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.1358536e-22 9.2534690e-30 1.2731569e-25 8.5848171e-20], sum to 1.0000
[2019-03-25 22:24:37,149] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7356
[2019-03-25 22:24:37,155] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.2, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8783091434009641, 6.9112, 6.9112, 168.912956510431, 724981.4719438948, 724981.4719438948, 218451.2925150674], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5736600.0000, 
sim time next is 5737200.0000, 
raw observation next is [30.36666666666667, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782294730007763, 6.911199999999999, 6.9112, 168.912956510431, 724818.5745895375, 724818.574589538, 218429.6948492588], 
processed observation next is [0.0, 0.391304347826087, 0.6382306477093209, 0.6566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8514993573180198, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2013384929415382, 0.20133849294153836, 0.32601446992426686], 
reward next is 0.6740, 
noisyNet noise sample is [array([0.08657966], dtype=float32), -0.18042567]. 
=============================================
[2019-03-25 22:24:37,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:24:37,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9703
[2019-03-25 22:24:37,546] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.96666666666667, 79.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8799335825403639, 6.9112, 6.9112, 168.912956510431, 727048.3299479038, 727048.3299479038, 218845.8504966047], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5728200.0000, 
sim time next is 5728800.0000, 
raw observation next is [28.13333333333333, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8818578333331311, 6.9112, 6.9112, 168.912956510431, 728287.2904269149, 728287.2904269149, 219270.409454086], 
processed observation next is [0.0, 0.30434782608695654, 0.532385466034755, 0.7833333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8559241869916232, 0.0, 0.0, 0.8294399451523027, 0.20230202511858747, 0.20230202511858747, 0.3272692678419194], 
reward next is 0.6727, 
noisyNet noise sample is [array([-0.24191897], dtype=float32), 0.84934485]. 
=============================================
[2019-03-25 22:24:39,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.5655327e-36 0.0000000e+00 0.0000000e+00 3.8621569e-32], sum to 1.0000
[2019-03-25 22:24:39,018] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7683
[2019-03-25 22:24:39,024] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [33.6, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9122483108759081, 6.9112, 6.9112, 168.912956510431, 746003.939925248, 746003.939925248, 226005.6511884767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5750400.0000, 
sim time next is 5751000.0000, 
raw observation next is [33.4, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9007810488526233, 6.911200000000001, 6.9112, 168.912956510431, 738434.2367303645, 738434.2367303639, 223405.6904881031], 
processed observation next is [0.0, 0.5652173913043478, 0.7819905213270142, 0.53, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8790012790885651, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20512062131399014, 0.20512062131398998, 0.3334413290867211], 
reward next is 0.6666, 
noisyNet noise sample is [array([-0.88586336], dtype=float32), -0.0057882485]. 
=============================================
[2019-03-25 22:24:39,073] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.29783 ]
 [72.178246]
 [72.08382 ]
 [71.96965 ]
 [71.92363 ]], R is [[72.35449982]
 [72.29364014]
 [72.22820282]
 [72.16513824]
 [72.10506439]].
[2019-03-25 22:24:39,628] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.534489e-35], sum to 1.0000
[2019-03-25 22:24:39,637] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9558
[2019-03-25 22:24:39,642] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9170145122050126, 6.911200000000001, 6.9112, 168.912956510431, 748916.7481963548, 748916.7481963541, 227085.0587062076], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5756400.0000, 
sim time next is 5757000.0000, 
raw observation next is [33.81666666666667, 53.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.956263738654057, 6.9112, 6.9112, 168.912956510431, 779711.722389029, 779711.722389029, 236504.9448028618], 
processed observation next is [0.0, 0.6521739130434783, 0.8017377567140602, 0.5383333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9466630959195815, 0.0, 0.0, 0.8294399451523027, 0.21658658955250806, 0.21658658955250806, 0.3529924549296445], 
reward next is 0.6470, 
noisyNet noise sample is [array([-0.5457832], dtype=float32), -0.16494939]. 
=============================================
[2019-03-25 22:24:39,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.73051]
 [66.61938]
 [66.56593]
 [66.48593]
 [66.40539]], R is [[66.77107239]
 [66.76442719]
 [66.7617569 ]
 [66.7624588 ]
 [66.76678467]].
[2019-03-25 22:24:41,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.1891925e-31 0.0000000e+00 0.0000000e+00 2.2426036e-20], sum to 1.0000
[2019-03-25 22:24:41,498] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6835
[2019-03-25 22:24:41,503] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.45, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9271069970877306, 6.911199999999999, 6.9112, 168.912956510431, 759377.2549444822, 759377.2549444829, 229582.2883027122], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5782200.0000, 
sim time next is 5782800.0000, 
raw observation next is [27.4, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9235889782649536, 6.911200000000001, 6.9112, 168.912956510431, 756771.0843011299, 756771.0843011293, 228754.5781649618], 
processed observation next is [0.0, 0.9565217391304348, 0.4976303317535545, 0.8633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9068158271523824, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102141900836472, 0.21021419008364703, 0.34142474352979374], 
reward next is 0.6586, 
noisyNet noise sample is [array([-0.10788862], dtype=float32), 0.22221382]. 
=============================================
[2019-03-25 22:24:51,030] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 3.2406038e-20 1.9310782e-20 1.5369109e-24 8.9021244e-09], sum to 1.0000
[2019-03-25 22:24:51,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9794
[2019-03-25 22:24:51,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2430676.664598926 W.
[2019-03-25 22:24:51,057] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.6, 72.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.118216556491692, 6.9112, 168.9116906190567, 2430676.664598926, 2283813.095117708, 475633.5158866348], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5913000.0000, 
sim time next is 5913600.0000, 
raw observation next is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.570434052275985, 6.9112, 168.9090845385719, 2751781.786365552, 2284109.181245875, 474636.7224680542], 
processed observation next is [1.0, 0.43478260869565216, 0.7045813586097948, 0.7133333333333333, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0659234052275985, 0.0, 0.8294209319978781, 0.7643838295459866, 0.6344747725682987, 0.7084130186090362], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.66188294], dtype=float32), 1.4446939]. 
=============================================
[2019-03-25 22:24:55,995] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159739: loss 0.2097
[2019-03-25 22:24:55,997] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159739: learning rate 0.0010
[2019-03-25 22:24:56,098] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 159774: loss 0.1303
[2019-03-25 22:24:56,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 159774: learning rate 0.0010
[2019-03-25 22:24:56,233] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159823: loss 0.0464
[2019-03-25 22:24:56,235] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159824: learning rate 0.0010
[2019-03-25 22:24:56,287] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159842: loss 0.7322
[2019-03-25 22:24:56,288] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159843: learning rate 0.0010
[2019-03-25 22:24:56,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159846: loss 0.4380
[2019-03-25 22:24:56,302] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159846: learning rate 0.0010
[2019-03-25 22:24:56,325] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159853: loss 1.4038
[2019-03-25 22:24:56,327] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159855: learning rate 0.0010
[2019-03-25 22:24:56,647] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159974: loss -84.6528
[2019-03-25 22:24:56,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159974: learning rate 0.0010
[2019-03-25 22:24:56,720] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 159997: loss -259.1493
[2019-03-25 22:24:56,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 159997: learning rate 0.0010
[2019-03-25 22:24:56,739] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 160001: loss -118.8009
[2019-03-25 22:24:56,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 160001: learning rate 0.0010
[2019-03-25 22:24:56,768] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160013: loss 0.5044
[2019-03-25 22:24:56,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160013: learning rate 0.0010
[2019-03-25 22:24:56,875] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160057: loss -152.3129
[2019-03-25 22:24:56,876] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160057: learning rate 0.0010
[2019-03-25 22:24:57,049] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 160119: loss -81.4827
[2019-03-25 22:24:57,051] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 160119: learning rate 0.0010
[2019-03-25 22:24:57,088] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160132: loss -148.3776
[2019-03-25 22:24:57,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160132: learning rate 0.0010
[2019-03-25 22:24:57,105] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160137: loss 17.0430
[2019-03-25 22:24:57,109] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160138: learning rate 0.0010
[2019-03-25 22:24:57,295] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160206: loss -19.3291
[2019-03-25 22:24:57,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160206: learning rate 0.0010
[2019-03-25 22:24:57,355] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 160227: loss -47.4591
[2019-03-25 22:24:57,357] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 160228: learning rate 0.0010
[2019-03-25 22:24:58,278] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 4.3124508e-29 9.3239532e-36 1.0446542e-30 0.0000000e+00], sum to 1.0000
[2019-03-25 22:24:58,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2233
[2019-03-25 22:24:58,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2230446.555717343 W.
[2019-03-25 22:24:58,307] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5316903962541238, 1.0, 1.0, 0.5316903962541238, 1.0, 2.0, 0.9231374796717744, 6.9112, 6.9112, 170.5573041426782, 2230446.555717343, 2230446.555717343, 437707.2376521191], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6015600.0000, 
sim time next is 6016200.0000, 
raw observation next is [29.5, 77.66666666666667, 1.0, 2.0, 0.7275772261659772, 1.0, 2.0, 0.7275772261659772, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2034612.457285279, 2034612.457285278, 386156.7826887922], 
processed observation next is [1.0, 0.6521739130434783, 0.5971563981042655, 0.7766666666666667, 1.0, 1.0, 0.6717797905614183, 1.0, 1.0, 0.6717797905614183, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5651701270236886, 0.5651701270236883, 0.5763534069981974], 
reward next is 0.4236, 
noisyNet noise sample is [array([0.6503211], dtype=float32), 1.0499384]. 
=============================================
[2019-03-25 22:25:02,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 8.6810634e-23 8.9623535e-27 3.1314209e-23 8.6054225e-24], sum to 1.0000
[2019-03-25 22:25:02,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0158
[2019-03-25 22:25:02,852] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2093763.614292392 W.
[2019-03-25 22:25:02,858] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.26666666666667, 76.83333333333333, 1.0, 2.0, 0.4991393488925903, 1.0, 2.0, 0.4991393488925903, 1.0, 2.0, 0.8668399811213652, 6.911199999999999, 6.9112, 170.5573041426782, 2093763.614292392, 2093763.614292393, 414550.7642974391], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6079800.0000, 
sim time next is 6080400.0000, 
raw observation next is [29.4, 76.0, 1.0, 2.0, 0.4617780933441426, 1.0, 2.0, 0.4617780933441426, 1.0, 2.0, 0.8015473691155917, 6.9112, 6.9112, 170.5573041426782, 1936900.89992359, 1936900.89992359, 389653.7267369974], 
processed observation next is [1.0, 0.391304347826087, 0.5924170616113744, 0.76, 1.0, 1.0, 0.35153987149896704, 1.0, 1.0, 0.35153987149896704, 1.0, 1.0, 0.7579845964824289, 0.0, 0.0, 0.8375144448122397, 0.5380280277565528, 0.5380280277565528, 0.5815727264731305], 
reward next is 0.4184, 
noisyNet noise sample is [array([0.24795747], dtype=float32), 0.56220204]. 
=============================================
[2019-03-25 22:25:15,186] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:25:15,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7471
[2019-03-25 22:25:15,205] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.7, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9286211985285018, 6.9112, 6.9112, 168.912956510431, 757324.6055087395, 757324.6055087395, 229795.527402057], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6256800.0000, 
sim time next is 6257400.0000, 
raw observation next is [29.85, 73.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9287762302052359, 6.911199999999999, 6.9112, 168.912956510431, 757373.5847509764, 757373.5847509771, 229828.9957631355], 
processed observation next is [0.0, 0.43478260869565216, 0.613744075829384, 0.7316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9131417441527266, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21038155131971567, 0.21038155131971586, 0.34302835188527686], 
reward next is 0.6570, 
noisyNet noise sample is [array([-0.31802043], dtype=float32), -1.1109768]. 
=============================================
[2019-03-25 22:25:15,511] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:25:15,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5036
[2019-03-25 22:25:15,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:25:15,531] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.78333333333333, 64.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8919129182855818, 6.9112, 6.9112, 168.912956510431, 733504.0449582271, 733504.0449582271, 221453.2596050955], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6267000.0000, 
sim time next is 6267600.0000, 
raw observation next is [30.8, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8870142020210794, 6.9112, 6.9112, 168.912956510431, 730278.8104418882, 730278.8104418882, 220361.5431941051], 
processed observation next is [0.0, 0.5652173913043478, 0.6587677725118484, 0.64, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8622124414891212, 0.0, 0.0, 0.8294399451523027, 0.2028552251227467, 0.2028552251227467, 0.3288978256628434], 
reward next is 0.6711, 
noisyNet noise sample is [array([-1.627951], dtype=float32), 0.48161292]. 
=============================================
[2019-03-25 22:25:15,538] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4388
[2019-03-25 22:25:15,544] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.86666666666667, 62.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8745130685315566, 6.911200000000001, 6.9112, 168.912956510431, 722141.0243194934, 722141.0243194929, 217604.1159265221], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6270000.0000, 
sim time next is 6270600.0000, 
raw observation next is [30.88333333333333, 62.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8702814790296923, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 216669.4402958438], 
processed observation next is [0.0, 0.5652173913043478, 0.6627172195892573, 0.6233333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8418066817435271, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19975613079391805, 0.1997561307939182, 0.3233872243221549], 
reward next is 0.6766, 
noisyNet noise sample is [array([-0.13007317], dtype=float32), 0.90047395]. 
=============================================
[2019-03-25 22:25:17,447] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 167661: loss 0.0538
[2019-03-25 22:25:17,452] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 167661: learning rate 0.0010
[2019-03-25 22:25:17,599] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167717: loss 0.0007
[2019-03-25 22:25:17,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167717: learning rate 0.0010
[2019-03-25 22:25:17,637] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 167731: loss 0.0139
[2019-03-25 22:25:17,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 167732: learning rate 0.0010
[2019-03-25 22:25:17,806] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167793: loss 0.0222
[2019-03-25 22:25:17,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167795: learning rate 0.0010
[2019-03-25 22:25:17,914] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 167834: loss 0.0380
[2019-03-25 22:25:17,917] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 167834: learning rate 0.0010
[2019-03-25 22:25:17,949] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167844: loss 0.0375
[2019-03-25 22:25:17,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167848: learning rate 0.0010
[2019-03-25 22:25:18,212] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167943: loss 0.0449
[2019-03-25 22:25:18,214] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167944: learning rate 0.0010
[2019-03-25 22:25:18,293] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167971: loss 0.0186
[2019-03-25 22:25:18,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167972: learning rate 0.0010
[2019-03-25 22:25:18,436] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168025: loss 0.0004
[2019-03-25 22:25:18,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168026: learning rate 0.0010
[2019-03-25 22:25:18,480] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168042: loss 0.0168
[2019-03-25 22:25:18,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168042: learning rate 0.0010
[2019-03-25 22:25:18,738] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 168142: loss 0.0057
[2019-03-25 22:25:18,739] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 168142: learning rate 0.0010
[2019-03-25 22:25:18,777] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 168153: loss 0.0057
[2019-03-25 22:25:18,783] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 168155: learning rate 0.0010
[2019-03-25 22:25:18,811] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168167: loss 0.0005
[2019-03-25 22:25:18,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168169: learning rate 0.0010
[2019-03-25 22:25:18,837] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168177: loss 0.0022
[2019-03-25 22:25:18,839] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168178: learning rate 0.0010
[2019-03-25 22:25:19,077] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168263: loss 0.0091
[2019-03-25 22:25:19,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168263: learning rate 0.0010
[2019-03-25 22:25:19,145] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168289: loss 0.0020
[2019-03-25 22:25:19,149] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168289: learning rate 0.0010
[2019-03-25 22:25:28,589] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 9.6861917e-17 5.1141054e-25 9.2147629e-21 1.9359601e-21], sum to 1.0000
[2019-03-25 22:25:28,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1478
[2019-03-25 22:25:28,611] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2708518.563820357 W.
[2019-03-25 22:25:28,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.88333333333333, 69.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.486645905554379, 6.9112, 168.9097576313952, 2708518.563820357, 2300285.164384248, 475604.2529376298], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6436200.0000, 
sim time next is 6436800.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 0.5901727719346271, 1.0, 1.0, 0.5901727719346271, 1.0, 2.0, 1.015193821423776, 6.9112, 6.9112, 170.5573041426782, 2476023.756243272, 2476023.756243272, 480980.7619484395], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.69, 1.0, 1.0, 0.5062322553429243, 1.0, 0.5, 0.5062322553429243, 1.0, 1.0, 1.0185290505167999, 0.0, 0.0, 0.8375144448122397, 0.6877843767342423, 0.6877843767342423, 0.7178817342514022], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1503432], dtype=float32), -0.5537954]. 
=============================================
[2019-03-25 22:25:32,711] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.0608321e-14 4.1116689e-24 3.4841809e-24 1.8190014e-26 1.0000000e+00], sum to 1.0000
[2019-03-25 22:25:32,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8475
[2019-03-25 22:25:32,723] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.75, 89.33333333333333, 1.0, 2.0, 0.2405224963258531, 1.0, 1.0, 0.2405224963258531, 1.0, 2.0, 0.411327379201882, 6.911200000000001, 6.9112, 170.5573041426782, 1008420.819344199, 1008420.819344199, 281734.2415151398], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6504600.0000, 
sim time next is 6505200.0000, 
raw observation next is [26.8, 89.0, 1.0, 2.0, 0.2406856631432269, 1.0, 2.0, 0.2406856631432269, 1.0, 2.0, 0.4118849486407998, 6.9112, 6.9112, 170.5573041426782, 1009105.238638196, 1009105.238638196, 281808.3538971551], 
processed observation next is [1.0, 0.30434782608695654, 0.4691943127962086, 0.89, 1.0, 1.0, 0.08516344957015287, 1.0, 1.0, 0.08516344957015287, 1.0, 1.0, 0.2827865227326826, 0.0, 0.0, 0.8375144448122397, 0.2803070107328322, 0.2803070107328322, 0.42060948342858967], 
reward next is 0.5794, 
noisyNet noise sample is [array([-1.295618], dtype=float32), -1.0507642]. 
=============================================
[2019-03-25 22:25:37,231] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-25 22:25:37,232] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:25:37,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:25:37,237] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:25:37,237] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:25:37,238] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:25:37,240] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:25:37,240] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:25:37,242] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:25:37,243] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:25:37,247] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:25:37,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-25 22:25:37,278] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-25 22:25:37,279] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-25 22:25:37,279] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-25 22:25:37,335] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-25 22:25:58,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:25:58,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 75.0, 1.0, 2.0, 0.7965335827019898, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.967676542018853, 6.9112, 168.9125736105433, 2010222.043314975, 1970155.731458656, 408243.1940950128]
[2019-03-25 22:25:58,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:25:58,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 6.7367581e-35 0.0000000e+00 3.3697683e-37 0.0000000e+00], sampled 0.09946373718721435
[2019-03-25 22:25:58,838] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2010222.043314975 W.
[2019-03-25 22:26:39,342] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:26:39,345] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.75, 61.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.039814982375161, 6.9112, 168.9120914006813, 920079.9466071185, 828836.5188416279, 254812.5716145325]
[2019-03-25 22:26:39,346] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:26:39,349] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.0316062e-29 3.0015436e-34 3.7870226e-32 2.7217764e-24], sampled 0.09036255099245072
[2019-03-25 22:26:39,350] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 920079.9466071185 W.
[2019-03-25 22:27:00,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:27:00,212] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8643883745542392, 6.911199999999998, 6.9112, 168.912956510431, 716261.1662004351, 716261.1662004364, 215420.861177088]
[2019-03-25 22:27:00,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:27:00,216] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.6015567e-36 0.0000000e+00 0.0000000e+00 3.3820498e-32], sampled 0.38475163433695203
[2019-03-25 22:27:24,374] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:27:24,374] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.83458511166667, 52.44873477666667, 1.0, 2.0, 0.8484311926568462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1227469.824100784, 1227469.824100784, 262087.8714600225]
[2019-03-25 22:27:24,375] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:27:24,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 8.4542177e-32 0.0000000e+00 2.0241638e-33 7.1359443e-31], sampled 0.7174708194137686
[2019-03-25 22:27:24,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1227469.824100784 W.
[2019-03-25 22:27:25,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:27:25,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.13333333333333, 71.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025086246600319, 6.9112, 6.9112, 168.9127663399539, 848459.8081876563, 848459.8081876563, 254684.1179458869]
[2019-03-25 22:27:25,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:27:25,224] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 9.1568757e-36 0.0000000e+00 6.4646970e-37 3.7115014e-36], sampled 0.6629474279752067
[2019-03-25 22:27:30,417] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:27:30,418] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.23172657, 58.84666182333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7472658381894314, 6.9112, 6.9112, 168.912956510431, 634560.4718236413, 634560.4718236413, 191477.4010354879]
[2019-03-25 22:27:30,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:27:30,421] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 5.4998499e-35 0.0000000e+00 1.2785124e-37 1.0344187e-31], sampled 0.4912174377044697
[2019-03-25 22:27:45,275] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3407 3185096716.9286 2464.0000
[2019-03-25 22:27:46,437] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8126 2937782196.4717 1381.0000
[2019-03-25 22:27:46,539] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.6705 3105724935.9881 2010.0000
[2019-03-25 22:27:46,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4105955], dtype=float32), -0.124929726]
[2019-03-25 22:27:46,613] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.1, 79.0, 1.0, 2.0, 0.7247249412934091, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97958524063154, 6.9112, 168.9124944079125, 1909728.302945617, 1861213.59232743, 391038.8537614286]
[2019-03-25 22:27:46,614] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:27:46,615] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 8.4603409e-26 1.6878436e-32 3.8195871e-28 7.2376669e-20], sampled 0.604703748006088
[2019-03-25 22:27:46,615] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1909728.302945617 W.
[2019-03-25 22:27:46,619] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0488 2989273624.9387 1566.0000
[2019-03-25 22:27:46,774] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.9996 3319521307.1094 2143.0000
[2019-03-25 22:27:47,788] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 175000, evaluation results [175000.0, 7288.999644938805, 3319521307.1093755, 2143.0, 7347.670539669374, 3105724935.988064, 2010.0, 8061.812589515823, 2937782196.4716883, 1381.0, 7031.340658675213, 3185096716.9285784, 2464.0, 7926.048825221757, 2989273624.9386597, 1566.0]
[2019-03-25 22:27:49,771] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175748: loss 153.6536
[2019-03-25 22:27:49,772] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175748: learning rate 0.0010
[2019-03-25 22:27:49,814] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 175759: loss 27.2168
[2019-03-25 22:27:49,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 175762: learning rate 0.0010
[2019-03-25 22:27:49,948] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 175815: loss -15.6200
[2019-03-25 22:27:49,950] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 175815: learning rate 0.0010
[2019-03-25 22:27:49,998] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175832: loss 35.0631
[2019-03-25 22:27:49,999] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175832: learning rate 0.0010
[2019-03-25 22:27:50,011] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175836: loss 66.6434
[2019-03-25 22:27:50,013] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175836: learning rate 0.0010
[2019-03-25 22:27:50,279] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175932: loss -123.3358
[2019-03-25 22:27:50,281] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175932: learning rate 0.0010
[2019-03-25 22:27:50,299] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175940: loss 47.5845
[2019-03-25 22:27:50,300] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175940: loss 92.1962
[2019-03-25 22:27:50,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175940: learning rate 0.0010
[2019-03-25 22:27:50,304] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175941: learning rate 0.0010
[2019-03-25 22:27:50,383] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 175971: loss 0.3983
[2019-03-25 22:27:50,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 175971: learning rate 0.0010
[2019-03-25 22:27:50,672] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176073: loss 6.1378
[2019-03-25 22:27:50,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176073: learning rate 0.0010
[2019-03-25 22:27:50,688] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176078: loss 154.0811
[2019-03-25 22:27:50,689] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176078: learning rate 0.0010
[2019-03-25 22:27:50,746] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 176100: loss -5.1274
[2019-03-25 22:27:50,749] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 176101: learning rate 0.0010
[2019-03-25 22:27:50,853] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 176141: loss 93.3014
[2019-03-25 22:27:50,856] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 176142: learning rate 0.0010
[2019-03-25 22:27:51,001] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176194: loss 81.0268
[2019-03-25 22:27:51,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176194: learning rate 0.0010
[2019-03-25 22:27:51,003] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176194: loss -30.7100
[2019-03-25 22:27:51,007] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176195: learning rate 0.0010
[2019-03-25 22:27:51,082] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176225: loss 26.1802
[2019-03-25 22:27:51,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176225: learning rate 0.0010
[2019-03-25 22:27:51,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.1646112e-24 2.9390580e-33 1.8504177e-27 1.5586510e-38], sum to 1.0000
[2019-03-25 22:27:51,184] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0169
[2019-03-25 22:27:51,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1639698.639253685 W.
[2019-03-25 22:27:51,200] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5864640552585569, 1.0, 2.0, 0.5864640552585569, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1639698.639253685, 1639698.639253685, 329306.9676378978], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6611400.0000, 
sim time next is 6612000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5105860353088557, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8754119303053186, 6.911199999999999, 6.9112, 168.9129565104276, 1427418.775320223, 1427418.775320223, 311838.6118833389], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.410344620854043, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.8480633296406324, -8.881784197001253e-17, 0.0, 0.8294399451522859, 0.3965052153667286, 0.3965052153667286, 0.4654307640049834], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3304434], dtype=float32), 0.20747505]. 
=============================================
[2019-03-25 22:27:51,213] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[44.688282]
 [44.350895]
 [43.503593]
 [42.866688]
 [42.375835]], R is [[44.92629623]
 [44.98553085]
 [44.99007797]
 [44.54017639]
 [44.09477615]].
[2019-03-25 22:27:51,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.5566893e-24 4.6966310e-28 9.7942105e-23 6.5008576e-09], sum to 1.0000
[2019-03-25 22:27:51,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6940
[2019-03-25 22:27:51,303] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1578325.503551984 W.
[2019-03-25 22:27:51,309] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5645250945644604, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9677727966059083, 6.911200000000001, 6.9112, 168.912956510431, 1578325.503551984, 1578325.503551984, 342728.479011309], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6613200.0000, 
sim time next is 6613800.0000, 
raw observation next is [31.08333333333334, 65.0, 1.0, 2.0, 0.3760612149245838, 1.0, 1.0, 0.3760612149245838, 1.0, 2.0, 0.6480699519744418, 6.911199999999999, 6.9112, 170.5573041426782, 1577101.944732203, 1577101.944732204, 339380.3013342903], 
processed observation next is [1.0, 0.5652173913043478, 0.6721958925750398, 0.65, 1.0, 1.0, 0.24826652400552263, 1.0, 0.5, 0.24826652400552263, 1.0, 1.0, 0.5708170146029778, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.438083873536723, 0.43808387353672334, 0.5065377631855079], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2135767], dtype=float32), -0.34726307]. 
=============================================
[2019-03-25 22:27:55,042] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 4.2914093e-23 6.4850102e-19 6.5950088e-25 2.4858711e-12], sum to 1.0000
[2019-03-25 22:27:55,055] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3764
[2019-03-25 22:27:55,064] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.6, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9411501699791425, 6.911199999999999, 6.9112, 168.912956510431, 789186.6917665504, 789186.691766551, 233607.4225641418], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6670800.0000, 
sim time next is 6671400.0000, 
raw observation next is [24.68333333333334, 94.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9171684191052355, 6.911200000000001, 6.9112, 168.912956510431, 769005.48115764, 769005.4811576394, 227829.0194969481], 
processed observation next is [1.0, 0.21739130434782608, 0.3688783570300162, 0.9483333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8989858769576041, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2136126336549, 0.21361263365489983, 0.3400433126820121], 
reward next is 0.6600, 
noisyNet noise sample is [array([0.09804142], dtype=float32), -0.43700728]. 
=============================================
[2019-03-25 22:27:57,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3381400e-07 4.9965883e-25 4.1313949e-26 1.0318858e-28 9.9999964e-01], sum to 1.0000
[2019-03-25 22:27:57,537] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9159
[2019-03-25 22:27:57,548] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.03333333333333, 79.0, 1.0, 2.0, 0.44966874377912, 1.0, 2.0, 0.44966874377912, 1.0, 2.0, 0.7703211986233588, 6.9112, 6.9112, 170.5573041426782, 1886064.243135231, 1886064.243135231, 380376.9588368291], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6688200.0000, 
sim time next is 6688800.0000, 
raw observation next is [28.2, 78.0, 1.0, 2.0, 0.4479467651220622, 1.0, 2.0, 0.4479467651220622, 1.0, 2.0, 0.767453278116317, 6.9112, 6.9112, 170.5573041426782, 1878835.344056895, 1878835.344056895, 379335.0620077208], 
processed observation next is [1.0, 0.43478260869565216, 0.5355450236966824, 0.78, 1.0, 1.0, 0.33487562062899057, 1.0, 1.0, 0.33487562062899057, 1.0, 1.0, 0.7164064367272159, 0.0, 0.0, 0.8375144448122397, 0.5218987066824708, 0.5218987066824708, 0.5661717343398818], 
reward next is 0.4338, 
noisyNet noise sample is [array([1.0994793], dtype=float32), 0.5834069]. 
=============================================
[2019-03-25 22:28:00,552] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9998271e-01 1.7631676e-26 3.9703892e-34 1.3518214e-33 1.7311821e-05], sum to 1.0000
[2019-03-25 22:28:00,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7427
[2019-03-25 22:28:00,578] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.96666666666667, 69.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6622187381874435, 6.9112, 6.9112, 168.912956510431, 570601.8994316269, 570601.8994316269, 176140.4796939967], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6730800.0000, 
sim time next is 6731400.0000, 
raw observation next is [25.83333333333334, 69.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6576475977302179, 6.911200000000001, 6.9112, 168.912956510431, 567287.1956160339, 567287.1956160332, 175366.4391452997], 
processed observation next is [1.0, 0.9130434782608695, 0.42338072669826254, 0.6983333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5824970704027047, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1575797765600094, 0.1575797765600092, 0.2617409539482085], 
reward next is 0.7383, 
noisyNet noise sample is [array([-1.6585816], dtype=float32), 0.3400681]. 
=============================================
[2019-03-25 22:28:02,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 7.5572123e-31 4.6206944e-36 3.8779031e-35 1.9733904e-21], sum to 1.0000
[2019-03-25 22:28:02,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8932
[2019-03-25 22:28:02,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1207694.280834182 W.
[2019-03-25 22:28:02,498] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.26666666666667, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.027017220349919, 7.350329652845409, 6.9112, 168.9110050301053, 1207694.280834182, 896164.1906314968, 255624.339964151], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6769200.0000, 
sim time next is 6769800.0000, 
raw observation next is [24.5, 72.0, 1.0, 1.0, 0.2707855711528535, 1.0, 1.0, 0.2707855711528535, 1.0, 2.0, 0.4780844065406722, 6.911199999999999, 6.9112, 170.5573041426782, 1239179.873902534, 1239179.873902534, 303215.1985437134], 
processed observation next is [1.0, 0.34782608695652173, 0.3601895734597157, 0.72, 1.0, 0.5, 0.12142839897934157, 1.0, 0.5, 0.12142839897934157, 1.0, 1.0, 0.36351756895203924, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.34421663163959276, 0.34421663163959276, 0.45255999782643797], 
reward next is 0.5474, 
noisyNet noise sample is [array([1.2744608], dtype=float32), 0.1157603]. 
=============================================
[2019-03-25 22:28:09,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:28:09,976] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6630
[2019-03-25 22:28:09,982] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.46666666666667, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6479101735698509, 6.9112, 6.9112, 168.912956510431, 560082.745709959, 560082.745709959, 173735.1253011544], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6859200.0000, 
sim time next is 6859800.0000, 
raw observation next is [26.65, 63.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6427502520352889, 6.9112, 6.9112, 168.912956510431, 556058.4220116308, 556058.4220116308, 172882.0287250923], 
processed observation next is [0.0, 0.391304347826087, 0.462085308056872, 0.635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5643295756527913, 0.0, 0.0, 0.8294399451523027, 0.15446067278100856, 0.15446067278100856, 0.25803287869416763], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.30614334], dtype=float32), 2.4432065]. 
=============================================
[2019-03-25 22:28:11,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 183650: loss 0.0109
[2019-03-25 22:28:11,193] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 183651: learning rate 0.0010
[2019-03-25 22:28:11,439] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 183739: loss 0.0379
[2019-03-25 22:28:11,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 183739: learning rate 0.0010
[2019-03-25 22:28:11,472] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183755: loss 0.0164
[2019-03-25 22:28:11,476] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183755: learning rate 0.0010
[2019-03-25 22:28:11,513] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183767: loss 0.0514
[2019-03-25 22:28:11,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183767: learning rate 0.0010
[2019-03-25 22:28:11,580] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183790: loss 0.0803
[2019-03-25 22:28:11,583] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183790: learning rate 0.0010
[2019-03-25 22:28:11,840] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183880: loss 0.1142
[2019-03-25 22:28:11,843] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183881: learning rate 0.0010
[2019-03-25 22:28:11,972] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183937: loss 0.0761
[2019-03-25 22:28:11,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183937: learning rate 0.0010
[2019-03-25 22:28:12,013] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183949: loss 0.0529
[2019-03-25 22:28:12,015] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183949: learning rate 0.0010
[2019-03-25 22:28:12,178] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184011: loss 0.0423
[2019-03-25 22:28:12,180] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184011: learning rate 0.0010
[2019-03-25 22:28:12,327] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184066: loss 0.0113
[2019-03-25 22:28:12,332] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184067: learning rate 0.0010
[2019-03-25 22:28:12,501] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184131: loss 0.0031
[2019-03-25 22:28:12,503] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184132: learning rate 0.0010
[2019-03-25 22:28:12,608] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 184172: loss 0.0045
[2019-03-25 22:28:12,610] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 184172: learning rate 0.0010
[2019-03-25 22:28:12,705] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 184204: loss 0.0186
[2019-03-25 22:28:12,708] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 184204: learning rate 0.0010
[2019-03-25 22:28:12,785] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184236: loss 0.0162
[2019-03-25 22:28:12,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184236: loss 0.0184
[2019-03-25 22:28:12,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184236: learning rate 0.0010
[2019-03-25 22:28:12,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184236: learning rate 0.0010
[2019-03-25 22:28:12,882] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184270: loss 0.0393
[2019-03-25 22:28:12,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184270: learning rate 0.0010
[2019-03-25 22:28:22,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 3.5790272e-34 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-25 22:28:22,139] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7666
[2019-03-25 22:28:22,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1849313.284558905 W.
[2019-03-25 22:28:22,153] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.98333333333333, 45.33333333333334, 1.0, 2.0, 0.6464904024580478, 1.0, 2.0, 0.6464904024580478, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1849313.284558905, 1849313.284558905, 357145.2070698243], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7049400.0000, 
sim time next is 7050000.0000, 
raw observation next is [31.76666666666667, 46.66666666666667, 1.0, 2.0, 0.4387457701304791, 1.0, 2.0, 0.4387457701304791, 1.0, 1.0, 0.7349232150763001, 6.911199999999999, 6.9112, 170.5573041426782, 1857689.932711332, 1857689.932711332, 372507.0051374907], 
processed observation next is [1.0, 0.6086956521739131, 0.7045813586097948, 0.46666666666666673, 1.0, 1.0, 0.3237900844945531, 1.0, 1.0, 0.3237900844945531, 1.0, 0.5, 0.6767356281418294, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5160249813087033, 0.5160249813087033, 0.555980604682822], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.671455], dtype=float32), -0.22032766]. 
=============================================
[2019-03-25 22:28:22,167] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[43.990192]
 [44.24508 ]
 [44.247562]
 [44.150368]
 [44.39625 ]], R is [[43.37159729]
 [43.40482712]
 [43.42165375]
 [43.43898392]
 [43.45635605]].
[2019-03-25 22:28:22,351] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 2.957129e-34 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-25 22:28:22,359] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0707
[2019-03-25 22:28:22,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1912435.890258724 W.
[2019-03-25 22:28:22,376] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.76666666666667, 53.00000000000001, 1.0, 2.0, 0.7163131766658407, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.949684162933739, 6.9112, 168.9127264103073, 1912435.890258724, 1885133.937742747, 391704.6077136731], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7039200.0000, 
sim time next is 7039800.0000, 
raw observation next is [30.85, 52.5, 1.0, 2.0, 0.7118817850217003, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.946696538789347, 6.9112, 168.912713284244, 1914285.381563859, 1889102.951511605, 391789.8923460796], 
processed observation next is [1.0, 0.4782608695652174, 0.661137440758294, 0.525, 1.0, 1.0, 0.6528696205080726, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0035496538789346666, 0.0, 0.8294387508003751, 0.5317459393232942, 0.5247508198643347, 0.5847610333523576], 
reward next is 0.2378, 
noisyNet noise sample is [array([-0.31380907], dtype=float32), 0.990124]. 
=============================================
[2019-03-25 22:28:23,686] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:28:23,699] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6771
[2019-03-25 22:28:23,706] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.36666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8135595105414178, 6.9112, 6.9112, 168.912956510431, 680145.4825121628, 680145.4825121628, 204609.5615068625], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7069200.0000, 
sim time next is 7069800.0000, 
raw observation next is [26.23333333333333, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8149788596171398, 6.9112, 6.9112, 168.912956510431, 681209.1636801178, 681209.1636801178, 204904.1993555647], 
processed observation next is [1.0, 0.8260869565217391, 0.44233807266982617, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7743644629477313, 0.0, 0.0, 0.8294399451523027, 0.1892247676889216, 0.1892247676889216, 0.3058271632172607], 
reward next is 0.6942, 
noisyNet noise sample is [array([1.0059308], dtype=float32), -0.38682958]. 
=============================================
[2019-03-25 22:28:30,724] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:28:30,734] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1083
[2019-03-25 22:28:30,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.792536714897852, 6.9112, 6.9112, 168.912956510431, 665093.9799112281, 665093.9799112281, 200321.4608375816], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7154400.0000, 
sim time next is 7155000.0000, 
raw observation next is [26.1, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7920637447699438, 6.9112, 6.9112, 168.912956510431, 664910.0225106572, 664910.0225106572, 200229.5199108536], 
processed observation next is [1.0, 0.8260869565217391, 0.4360189573459717, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7464192009389558, 0.0, 0.0, 0.8294399451523027, 0.18469722847518255, 0.18469722847518255, 0.2988500297176919], 
reward next is 0.7011, 
noisyNet noise sample is [array([0.24359974], dtype=float32), -0.24169633]. 
=============================================
[2019-03-25 22:28:30,767] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[65.902214]
 [65.68705 ]
 [65.65883 ]
 [65.64256 ]
 [65.71052 ]], R is [[65.80706024]
 [65.84999847]
 [65.89229584]
 [65.9339447 ]
 [65.97479248]].
[2019-03-25 22:28:32,601] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.800059e-33], sum to 1.0000
[2019-03-25 22:28:32,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1479
[2019-03-25 22:28:32,621] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.8, 90.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8921674630142803, 6.9112, 6.9112, 168.912956510431, 742825.5061838657, 742825.5061838657, 221830.7946948011], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7186200.0000, 
sim time next is 7186800.0000, 
raw observation next is [25.8, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8397320912091728, 6.9112, 6.9112, 168.912956510431, 698931.3315803, 698931.3315803, 210102.3237514393], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.9033333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8045513307428936, 0.0, 0.0, 0.8294399451523027, 0.1941475921056389, 0.1941475921056389, 0.3135855578379691], 
reward next is 0.6864, 
noisyNet noise sample is [array([0.8549061], dtype=float32), -0.45978272]. 
=============================================
[2019-03-25 22:28:32,940] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191698: loss -190.8873
[2019-03-25 22:28:32,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191700: learning rate 0.0010
[2019-03-25 22:28:33,016] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191726: loss -115.2210
[2019-03-25 22:28:33,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191726: learning rate 0.0010
[2019-03-25 22:28:33,019] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 191726: loss -194.5855
[2019-03-25 22:28:33,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 191727: learning rate 0.0010
[2019-03-25 22:28:33,209] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191796: loss -32.1069
[2019-03-25 22:28:33,211] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191796: learning rate 0.0010
[2019-03-25 22:28:33,288] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191827: loss -105.6286
[2019-03-25 22:28:33,290] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191827: learning rate 0.0010
[2019-03-25 22:28:33,357] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191852: loss -93.9017
[2019-03-25 22:28:33,362] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191855: learning rate 0.0010
[2019-03-25 22:28:33,533] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 191918: loss -36.8667
[2019-03-25 22:28:33,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 191918: learning rate 0.0010
[2019-03-25 22:28:33,699] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191981: loss -34.6082
[2019-03-25 22:28:33,704] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191982: learning rate 0.0010
[2019-03-25 22:28:33,816] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192019: loss -45.8037
[2019-03-25 22:28:33,819] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192019: learning rate 0.0010
[2019-03-25 22:28:33,958] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192074: loss 55.3392
[2019-03-25 22:28:33,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192075: learning rate 0.0010
[2019-03-25 22:28:34,046] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192104: loss -72.4168
[2019-03-25 22:28:34,054] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192104: learning rate 0.0010
[2019-03-25 22:28:34,153] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 192145: loss -103.2627
[2019-03-25 22:28:34,157] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 192148: learning rate 0.0010
[2019-03-25 22:28:34,193] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192161: loss -112.0636
[2019-03-25 22:28:34,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192161: learning rate 0.0010
[2019-03-25 22:28:34,273] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192184: loss 29.2957
[2019-03-25 22:28:34,276] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192184: learning rate 0.0010
[2019-03-25 22:28:34,277] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 192185: loss -61.8275
[2019-03-25 22:28:34,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 192186: learning rate 0.0010
[2019-03-25 22:28:34,390] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192230: loss -44.4045
[2019-03-25 22:28:34,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192231: learning rate 0.0010
[2019-03-25 22:28:39,002] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.000000e+00 1.873803e-33 4.738832e-36 0.000000e+00 9.933328e-28], sum to 1.0000
[2019-03-25 22:28:39,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9432
[2019-03-25 22:28:39,017] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.6, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5914506125478923, 6.911199999999999, 6.9112, 168.912956510431, 517660.6663926761, 517660.6663926768, 164729.966021408], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7272000.0000, 
sim time next is 7272600.0000, 
raw observation next is [21.56666666666667, 90.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.648388465579265, 6.911199999999999, 6.9112, 168.912956510431, 567524.7436648917, 567524.7436648923, 173688.4029409012], 
processed observation next is [1.0, 0.17391304347826086, 0.22116903633491333, 0.9016666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.571205445828372, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15764576212913659, 0.15764576212913675, 0.25923642229985255], 
reward next is 0.7408, 
noisyNet noise sample is [array([0.69960433], dtype=float32), 0.86883825]. 
=============================================
[2019-03-25 22:28:51,968] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.6927306e-37 0.0000000e+00 0.0000000e+00 3.3307736e-31], sum to 1.0000
[2019-03-25 22:28:51,982] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9397
[2019-03-25 22:28:51,991] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.3, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5717251426385489, 6.911199999999999, 6.9112, 168.912956510431, 500191.566891972, 500191.5668919726, 161835.496191045], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7443000.0000, 
sim time next is 7443600.0000, 
raw observation next is [21.3, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5749170745651705, 6.911200000000001, 6.9112, 168.912956510431, 502784.2935226561, 502784.2935226554, 162302.5834866247], 
processed observation next is [0.0, 0.13043478260869565, 0.2085308056872039, 0.9333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48160618849411035, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13966230375629335, 0.13966230375629315, 0.2422426619203354], 
reward next is 0.7578, 
noisyNet noise sample is [array([1.5256163], dtype=float32), -1.2289674]. 
=============================================
[2019-03-25 22:28:54,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5139747e-36], sum to 1.0000
[2019-03-25 22:28:54,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9621
[2019-03-25 22:28:54,190] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.33333333333334, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6751206602903538, 6.9112, 6.9112, 168.912956510431, 579066.6838532434, 579066.6838532434, 178352.9320076877], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7474800.0000, 
sim time next is 7475400.0000, 
raw observation next is [24.45, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6770866613615205, 6.911199999999999, 6.9112, 168.912956510431, 580439.909540628, 580439.9095406285, 178692.8597619729], 
processed observation next is [0.0, 0.5217391304347826, 0.3578199052132702, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6062032455628299, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16123330820572998, 0.16123330820573015, 0.2667057608387655], 
reward next is 0.7333, 
noisyNet noise sample is [array([0.21302998], dtype=float32), -0.9970724]. 
=============================================
[2019-03-25 22:28:54,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 199662: loss 0.0313
[2019-03-25 22:28:54,497] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 199663: learning rate 0.0010
[2019-03-25 22:28:54,572] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 199689: loss 0.0325
[2019-03-25 22:28:54,574] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 199689: learning rate 0.0010
[2019-03-25 22:28:54,596] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 199697: loss 0.0389
[2019-03-25 22:28:54,598] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 199697: learning rate 0.0010
[2019-03-25 22:28:54,606] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199699: loss 0.0089
[2019-03-25 22:28:54,610] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199700: learning rate 0.0010
[2019-03-25 22:28:54,971] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199834: loss 0.0349
[2019-03-25 22:28:54,973] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199834: learning rate 0.0010
[2019-03-25 22:28:54,988] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199840: loss 0.0558
[2019-03-25 22:28:54,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199840: learning rate 0.0010
[2019-03-25 22:28:55,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:28:55,186] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199910: loss 0.0013
[2019-03-25 22:28:55,187] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199910: learning rate 0.0010
[2019-03-25 22:28:55,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5339
[2019-03-25 22:28:55,197] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.26666666666667, 75.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.714429658418378, 6.911200000000001, 6.9112, 168.912956510431, 607589.8976980147, 607589.8976980142, 185325.761062553], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7488600.0000, 
sim time next is 7489200.0000, 
raw observation next is [26.33333333333334, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7163393069130126, 6.911199999999999, 6.9112, 168.912956510431, 608953.8282641704, 608953.828264171, 185673.8458508311], 
processed observation next is [0.0, 0.6956521739130435, 0.44707740916271754, 0.7566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.654072325503674, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16915384118449178, 0.16915384118449195, 0.27712514306094194], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.2880637], dtype=float32), 0.85334456]. 
=============================================
[2019-03-25 22:28:55,240] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 199927: loss 0.0196
[2019-03-25 22:28:55,246] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 199927: learning rate 0.0010
[2019-03-25 22:28:55,432] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-25 22:28:55,433] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:28:55,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:28:55,435] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:28:55,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:28:55,439] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:28:55,441] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:28:55,438] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:28:55,443] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:28:55,444] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:28:55,446] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:28:55,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-25 22:28:55,462] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-25 22:28:55,462] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-25 22:28:55,514] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-25 22:28:55,516] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-25 22:29:09,718] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:29:09,719] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5485539170343042, 6.911199999999999, 6.9112, 168.912956510431, 481905.6715583483, 481905.6715583489, 158507.7481476913]
[2019-03-25 22:29:09,720] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:29:09,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5544407920311394
[2019-03-25 22:29:14,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:29:14,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.05, 88.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5386378788703737, 6.911199999999999, 6.9112, 168.912956510431, 474421.9454683191, 474421.9454683198, 157112.1644520098]
[2019-03-25 22:29:14,123] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:29:14,126] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3340317336310423
[2019-03-25 22:29:26,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:29:26,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.93333333333333, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.705436420502138, 6.9112, 6.9112, 168.912956510431, 606077.090537336, 606077.090537336, 183708.0101483172]
[2019-03-25 22:29:26,658] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:29:26,661] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6184998859428602
[2019-03-25 22:29:50,182] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:29:50,183] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9588471799516414, 6.9112, 6.9112, 168.912956510431, 778786.4500342685, 778786.4500342685, 236993.0781790181]
[2019-03-25 22:29:50,183] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:29:50,187] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5577284951468635
[2019-03-25 22:30:11,999] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:11,999] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.33333333333334, 77.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.746256089327783, 6.9112, 168.9086595192302, 2046563.186868673, 1454160.727247422, 311352.2580886176]
[2019-03-25 22:30:12,000] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:30:12,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.9857134e-37], sampled 0.12337823299664785
[2019-03-25 22:30:12,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2046563.186868673 W.
[2019-03-25 22:30:17,633] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:17,633] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.16731943, 60.30915109, 1.0, 2.0, 0.7541062005615132, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053922.422880543, 1053922.422880543, 232958.1457041246]
[2019-03-25 22:30:17,633] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:30:17,636] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5387987118416672
[2019-03-25 22:30:17,637] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1053922.422880543 W.
[2019-03-25 22:30:28,277] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:28,278] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.04774629, 61.52271906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9600015316393024, 6.9112, 6.9112, 168.912956510431, 780261.0960318404, 780261.0960318404, 237305.5596943602]
[2019-03-25 22:30:28,280] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:30:28,283] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5247375651441668
[2019-03-25 22:30:35,467] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:35,468] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.55926987833333, 83.10389702166665, 1.0, 2.0, 0.7208467471988811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510424, 1007417.686147374, 1007417.686147374, 225424.9946814333]
[2019-03-25 22:30:35,470] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:30:35,473] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2585975015199925
[2019-03-25 22:30:35,474] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1007417.686147374 W.
[2019-03-25 22:30:42,856] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:42,858] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.30420247, 82.69926964333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.893719791676406, 6.9112, 6.9112, 168.912956510431, 737692.0145793329, 737692.0145793329, 221972.7508209225]
[2019-03-25 22:30:42,859] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:30:42,863] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8566779991442892
[2019-03-25 22:30:55,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.43135494], dtype=float32), -0.079170056]
[2019-03-25 22:30:55,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.29596674166667, 81.08154262833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5909564658505986, 6.9112, 6.9112, 168.912956510431, 518869.691760284, 518869.691760284, 164609.1557236374]
[2019-03-25 22:30:55,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:30:55,669] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6403639497584694
[2019-03-25 22:31:04,204] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.4079 3105587199.2298 2010.0000
[2019-03-25 22:31:04,291] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3555 3185024827.1607 2464.0000
[2019-03-25 22:31:04,408] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.9996 3319521307.1094 2143.0000
[2019-03-25 22:31:04,635] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-03-25 22:31:04,696] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8126 2937782196.4717 1381.0000
[2019-03-25 22:31:05,710] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 200000, evaluation results [200000.0, 7288.999644938805, 3319521307.1093755, 2143.0, 7348.407911634859, 3105587199.229759, 2010.0, 8061.812589515823, 2937782196.4716883, 1381.0, 7031.3555153855805, 3185024827.1606627, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-03-25 22:31:05,850] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200061: loss 0.1389
[2019-03-25 22:31:05,854] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200062: learning rate 0.0010
[2019-03-25 22:31:05,984] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200109: loss 0.1869
[2019-03-25 22:31:05,987] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200109: learning rate 0.0010
[2019-03-25 22:31:06,207] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 200189: loss 0.0368
[2019-03-25 22:31:06,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 200189: learning rate 0.0010
[2019-03-25 22:31:06,236] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200202: loss 0.0375
[2019-03-25 22:31:06,240] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200202: learning rate 0.0010
[2019-03-25 22:31:06,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 200223: loss 0.0199
[2019-03-25 22:31:06,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 200223: learning rate 0.0010
[2019-03-25 22:31:06,332] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200235: loss 0.0553
[2019-03-25 22:31:06,336] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200235: learning rate 0.0010
[2019-03-25 22:31:06,357] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 200243: loss 0.0301
[2019-03-25 22:31:06,359] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 200243: learning rate 0.0010
[2019-03-25 22:31:06,464] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200282: loss 0.0090
[2019-03-25 22:31:06,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200283: learning rate 0.0010
[2019-03-25 22:31:09,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:31:09,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3292
[2019-03-25 22:31:09,949] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.96666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7171058207905371, 6.9112, 6.9112, 168.912956510431, 609759.0852984507, 609759.0852984507, 185816.0943794932], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7548600.0000, 
sim time next is 7549200.0000, 
raw observation next is [25.2, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7219642709495262, 6.9112, 6.9112, 168.912956510431, 613192.212126483, 613192.212126483, 186706.4133173114], 
processed observation next is [0.0, 0.391304347826087, 0.3933649289099526, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6609320377433247, 0.0, 0.0, 0.8294399451523027, 0.17033117003513415, 0.17033117003513415, 0.2786662885333006], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.32317182], dtype=float32), 0.22019124]. 
=============================================
[2019-03-25 22:31:14,042] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:31:14,051] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6383
[2019-03-25 22:31:14,058] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8082231615802212, 6.911199999999999, 6.9112, 168.912956510431, 686057.8155082215, 686057.8155082221, 203683.2498301097], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7626600.0000, 
sim time next is 7627200.0000, 
raw observation next is [24.1, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8394530987082038, 6.911199999999999, 6.9112, 168.912956510431, 712060.7058499899, 712060.7058499905, 210321.1508443904], 
processed observation next is [1.0, 0.2608695652173913, 0.3412322274881518, 0.9266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8042110959856145, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1977946405138861, 0.19779464051388623, 0.31391216543938866], 
reward next is 0.6861, 
noisyNet noise sample is [array([0.70051223], dtype=float32), -1.4824597]. 
=============================================
[2019-03-25 22:31:19,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.8245497e-38 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-25 22:31:19,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0354
[2019-03-25 22:31:19,313] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.63333333333334, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8099627052557451, 6.9112, 6.9112, 168.912956510431, 678268.8769390162, 678268.8769390162, 203884.8534992929], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7683600.0000, 
sim time next is 7684200.0000, 
raw observation next is [25.6, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8095672999425473, 6.911200000000001, 6.9112, 168.912956510431, 678243.1676352267, 678243.1676352262, 203809.49037551], 
processed observation next is [1.0, 0.9565217391304348, 0.4123222748815167, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7677649999299356, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1884008798986741, 0.18840087989867393, 0.3041932692171791], 
reward next is 0.6958, 
noisyNet noise sample is [array([-0.24369842], dtype=float32), 0.5231401]. 
=============================================
[2019-03-25 22:31:22,398] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 5.3891078e-37 0.0000000e+00 1.8297366e-37 0.0000000e+00], sum to 1.0000
[2019-03-25 22:31:22,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0830
[2019-03-25 22:31:22,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2063988.94042931 W.
[2019-03-25 22:31:22,421] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.7, 67.0, 1.0, 2.0, 0.4920480957511802, 1.0, 2.0, 0.4920480957511802, 1.0, 2.0, 0.8507168307266341, 6.911200000000001, 6.9112, 170.5573041426782, 2063988.94042931, 2063988.940429309, 409015.6098288079], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7728000.0000, 
sim time next is 7728600.0000, 
raw observation next is [30.9, 66.0, 1.0, 2.0, 0.8796749769587966, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991446263548728, 6.9112, 168.9124788237336, 2126587.478240841, 2069658.178012936, 429023.8435890404], 
processed observation next is [1.0, 0.43478260869565216, 0.6635071090047393, 0.66, 1.0, 1.0, 0.8550300927214417, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008024626354872754, 0.0, 0.8294375994919325, 0.5907187439557892, 0.5749050494480378, 0.6403340949090155], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34021753], dtype=float32), 0.5630811]. 
=============================================
[2019-03-25 22:31:26,290] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207626: loss 12.4324
[2019-03-25 22:31:26,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207627: learning rate 0.0010
[2019-03-25 22:31:26,441] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 207681: loss -34.5308
[2019-03-25 22:31:26,446] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 207683: learning rate 0.0010
[2019-03-25 22:31:26,475] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207694: loss 9.1017
[2019-03-25 22:31:26,483] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207697: learning rate 0.0010
[2019-03-25 22:31:26,595] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 207737: loss 21.3910
[2019-03-25 22:31:26,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 207737: learning rate 0.0010
[2019-03-25 22:31:26,756] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207796: loss -8.6748
[2019-03-25 22:31:26,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207797: learning rate 0.0010
[2019-03-25 22:31:26,877] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207841: loss -44.9438
[2019-03-25 22:31:26,880] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207842: learning rate 0.0010
[2019-03-25 22:31:27,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207890: loss -9.2397
[2019-03-25 22:31:27,011] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207890: learning rate 0.0010
[2019-03-25 22:31:27,305] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207996: loss -20.1004
[2019-03-25 22:31:27,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207996: learning rate 0.0010
[2019-03-25 22:31:27,560] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208090: loss -3.2799
[2019-03-25 22:31:27,563] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208091: learning rate 0.0010
[2019-03-25 22:31:27,636] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208119: loss 41.2724
[2019-03-25 22:31:27,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208119: learning rate 0.0010
[2019-03-25 22:31:27,694] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208138: loss 12.3614
[2019-03-25 22:31:27,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208138: learning rate 0.0010
[2019-03-25 22:31:27,747] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208159: loss -4.2001
[2019-03-25 22:31:27,750] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208160: learning rate 0.0010
[2019-03-25 22:31:27,833] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 208189: loss -29.9107
[2019-03-25 22:31:27,834] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 208189: learning rate 0.0010
[2019-03-25 22:31:27,849] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208194: loss -43.8563
[2019-03-25 22:31:27,851] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208195: learning rate 0.0010
[2019-03-25 22:31:27,875] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208205: loss -4.9208
[2019-03-25 22:31:27,877] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208205: learning rate 0.0010
[2019-03-25 22:31:28,028] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 208260: loss 10.4317
[2019-03-25 22:31:28,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 208262: learning rate 0.0010
[2019-03-25 22:31:29,954] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.6097322e-35 0.0000000e+00 2.8369672e-31 0.0000000e+00], sum to 1.0000
[2019-03-25 22:31:29,966] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8018
[2019-03-25 22:31:29,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2095241.743520436 W.
[2019-03-25 22:31:29,982] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.65, 69.0, 1.0, 2.0, 0.4994913808133883, 1.0, 2.0, 0.4994913808133883, 1.0, 1.0, 0.866804055740252, 6.911199999999999, 6.9112, 170.5573041426782, 2095241.743520436, 2095241.743520436, 414677.6996958386], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7828200.0000, 
sim time next is 7828800.0000, 
raw observation next is [30.76666666666667, 69.33333333333333, 1.0, 2.0, 0.7768948351144206, 1.0, 2.0, 0.7768948351144206, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2172665.129340841, 2172665.129340841, 408780.4800512033], 
processed observation next is [1.0, 0.6086956521739131, 0.6571879936808849, 0.6933333333333332, 1.0, 1.0, 0.7311985965233982, 1.0, 1.0, 0.7311985965233982, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6035180914835669, 0.6035180914835669, 0.6101201194794079], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0984133], dtype=float32), 0.98664314]. 
=============================================
[2019-03-25 22:31:31,082] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 1.698912e-36 0.000000e+00], sum to 1.0000
[2019-03-25 22:31:31,095] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6596
[2019-03-25 22:31:31,104] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666666, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8574594391772781, 6.911200000000001, 6.9112, 168.912956510431, 709834.1086543112, 709834.1086543106, 213859.896097115], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7864800.0000, 
sim time next is 7865400.0000, 
raw observation next is [26.13333333333334, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.858336941581765, 6.9112, 6.9112, 168.912956510431, 710428.8250831377, 710428.8250831377, 214049.4649979922], 
processed observation next is [1.0, 0.0, 0.43759873617693557, 0.9066666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.827240172660689, 0.0, 0.0, 0.8294399451523027, 0.19734134030087158, 0.19734134030087158, 0.31947681342983913], 
reward next is 0.6805, 
noisyNet noise sample is [array([0.6341723], dtype=float32), -1.8554715]. 
=============================================
[2019-03-25 22:31:37,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:37,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:37,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-25 22:31:38,087] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-25 22:31:38,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-25 22:31:38,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,249] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-25 22:31:38,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-25 22:31:38,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,519] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-25 22:31:38,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-25 22:31:38,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,748] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-25 22:31:38,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-25 22:31:38,953] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,955] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-25 22:31:38,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-25 22:31:38,991] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:38,993] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:38,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-25 22:31:39,022] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:39,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:39,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-25 22:31:39,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:39,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:39,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:39,043] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:39,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:31:39,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:31:39,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-25 22:31:39,064] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-25 22:31:39,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-25 22:31:49,483] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:31:49,493] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3923
[2019-03-25 22:31:49,498] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.43333333333333, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5333812128326848, 6.9112, 6.9112, 168.912956510431, 470432.0629111052, 470432.0629111052, 156381.8617128285], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 202800.0000, 
sim time next is 203400.0000, 
raw observation next is [20.45, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5339402548122759, 6.911199999999999, 6.9112, 168.912956510431, 470853.9445166664, 470853.944516667, 156459.3389555488], 
processed observation next is [0.0, 0.34782608695652173, 0.16824644549763035, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.43163445708814135, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13079276236574067, 0.13079276236574083, 0.23352140142619224], 
reward next is 0.7665, 
noisyNet noise sample is [array([1.5596756], dtype=float32), -0.73922104]. 
=============================================
[2019-03-25 22:31:49,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:31:49,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9175
[2019-03-25 22:31:49,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1044949.823108759 W.
[2019-03-25 22:31:49,840] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.8, 96.0, 1.0, 1.0, 0.3525551209131221, 0.0, 2.0, 0.0, 1.0, 2.0, 0.610816466658128, 6.9112, 6.9112, 168.9126839948336, 1044949.823108759, 1044949.823108759, 244484.7728320357], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 133800.0000, 
sim time next is 134400.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.2616865399416018, 1.0, 1.0, 0.2616865399416018, 1.0, 2.0, 0.4504806780138481, 6.9112, 6.9112, 170.5573041426782, 1152421.929517836, 1152421.929517836, 294689.3392936546], 
processed observation next is [1.0, 0.5652173913043478, 0.2796208530805688, 0.96, 1.0, 1.0, 0.1104657107730142, 1.0, 0.5, 0.1104657107730142, 1.0, 1.0, 0.3298544853827416, 0.0, 0.0, 0.8375144448122397, 0.32011720264384336, 0.32011720264384336, 0.43983483476664864], 
reward next is 0.5602, 
noisyNet noise sample is [array([0.29447606], dtype=float32), -1.2328054]. 
=============================================
[2019-03-25 22:31:56,909] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:31:56,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4518
[2019-03-25 22:31:56,928] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.73333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5586565362406493, 6.9112, 6.9112, 168.912956510431, 489982.389484485, 489982.389484485, 159939.2539728316], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 231600.0000, 
sim time next is 232200.0000, 
raw observation next is [21.7, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5567567869276613, 6.9112, 6.9112, 168.912956510431, 488466.0768189292, 488466.0768189292, 159668.019764009], 
processed observation next is [0.0, 0.6956521739130435, 0.2274881516587678, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45945949625324545, 0.0, 0.0, 0.8294399451523027, 0.13568502133859145, 0.13568502133859145, 0.2383104772597149], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.3945542], dtype=float32), 0.6964388]. 
=============================================
[2019-03-25 22:32:00,174] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:32:00,182] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1563
[2019-03-25 22:32:00,187] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.93333333333334, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5069975922100349, 6.911200000000001, 6.9112, 168.912956510431, 448295.1869766206, 448295.1869766201, 152902.7001232971], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 271200.0000, 
sim time next is 271800.0000, 
raw observation next is [19.85, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5034233413533191, 6.911199999999999, 6.9112, 168.912956510431, 445380.6546891804, 445380.6546891811, 152441.8142005409], 
processed observation next is [0.0, 0.13043478260869565, 0.1398104265402845, 0.945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.39441870896746234, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12371684852477233, 0.12371684852477252, 0.22752509582170283], 
reward next is 0.7725, 
noisyNet noise sample is [array([-0.41736358], dtype=float32), -1.3157666]. 
=============================================
[2019-03-25 22:32:02,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:32:02,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0896
[2019-03-25 22:32:02,432] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333333, 76.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5649937430337325, 6.911199999999999, 6.9112, 168.912956510431, 494246.5703175248, 494246.5703175254, 160871.2954307767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 303000.0000, 
sim time next is 303600.0000, 
raw observation next is [23.46666666666667, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5648061209981845, 6.9112, 6.9112, 168.912956510431, 494021.2431159222, 494021.2431159222, 160846.0630321472], 
processed observation next is [0.0, 0.5217391304347826, 0.31121642969984215, 0.7666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4692757573148591, 0.0, 0.0, 0.8294399451523027, 0.13722812308775617, 0.13722812308775617, 0.24006875079424958], 
reward next is 0.7599, 
noisyNet noise sample is [array([0.6371432], dtype=float32), -0.77422637]. 
=============================================
[2019-03-25 22:32:04,760] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:32:04,771] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9023
[2019-03-25 22:32:04,775] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.83333333333333, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7763012103910513, 6.911200000000001, 6.9112, 168.912956510431, 689691.8147762476, 689691.8147762469, 196548.1948081011], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 381000.0000, 
sim time next is 381600.0000, 
raw observation next is [21.9, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7464218581299481, 6.9112, 6.9112, 168.912956510431, 663157.3421255347, 663157.3421255347, 190732.6173234151], 
processed observation next is [1.0, 0.43478260869565216, 0.23696682464454974, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6907583635731074, 0.0, 0.0, 0.8294399451523027, 0.18421037281264852, 0.18421037281264852, 0.2846755482439031], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.18536532], dtype=float32), 1.2445356]. 
=============================================
[2019-03-25 22:32:13,889] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-25 22:32:13,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:32:13,896] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:32:13,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:32:13,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:32:13,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:32:13,900] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:32:13,900] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:32:13,901] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:32:13,903] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:32:13,905] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:32:13,918] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 8.2653830e-37 0.0000000e+00 1.5145408e-36 9.2442274e-37], sum to 1.0000
[2019-03-25 22:32:13,921] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4120
[2019-03-25 22:32:13,926] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-25 22:32:13,929] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4277012506744383, 6.911199999999999, 6.9112, 168.912956510431, 383785.0787003994, 383785.0787004, 143394.2169278245], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 543000.0000, 
sim time next is 543600.0000, 
raw observation next is [20.0, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4283114621820312, 6.9112, 6.9112, 168.912956510431, 384136.3550020184, 384136.3550020184, 143473.0469965022], 
processed observation next is [1.0, 0.30434782608695654, 0.1469194312796209, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.30281885631955024, 0.0, 0.0, 0.8294399451523027, 0.10670454305611622, 0.10670454305611622, 0.2141388761141824], 
reward next is 0.7859, 
noisyNet noise sample is [array([-0.39472103], dtype=float32), -0.569375]. 
=============================================
[2019-03-25 22:32:13,949] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-25 22:32:13,971] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-25 22:32:13,972] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-25 22:32:14,013] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-25 22:32:48,763] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:32:48,767] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333334, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.951588428636511, 6.911200000000001, 6.9112, 168.912956510431, 773142.9587436481, 773142.9587436474, 235219.266174265]
[2019-03-25 22:32:48,767] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:32:48,770] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.10148997497348955
[2019-03-25 22:32:51,074] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:32:51,074] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.86006866, 59.93285613, 1.0, 2.0, 0.958514226450926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349146.1526283, 1349146.1526283, 287976.8546972359]
[2019-03-25 22:32:51,075] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:32:51,078] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8648976278951077
[2019-03-25 22:32:51,080] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1349146.1526283 W.
[2019-03-25 22:32:53,166] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:32:53,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9620272366444005, 6.911200000000001, 6.9112, 168.912956510431, 780607.6991719429, 780607.6991719423, 237741.3456451185]
[2019-03-25 22:32:53,168] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:32:53,171] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6549322398911587
[2019-03-25 22:33:08,109] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:33:08,113] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.04646046000001, 70.12598843666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.101400045401649, 6.9112, 168.9117602994729, 963787.0702702716, 828853.5674633767, 254812.7224542832]
[2019-03-25 22:33:08,113] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:33:08,116] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.19589552905359542
[2019-03-25 22:33:08,118] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 963787.0702702716 W.
[2019-03-25 22:33:22,064] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:33:22,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.10546582666667, 61.41759399833333, 1.0, 2.0, 0.7143932469085351, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976182192255, 6.9112, 168.9123160318229, 1895270.234498251, 1828033.003855511, 387861.9148744188]
[2019-03-25 22:33:22,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:33:22,070] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.541629790081226
[2019-03-25 22:33:22,071] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1895270.234498251 W.
[2019-03-25 22:33:45,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:33:45,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.51666666666667, 55.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.968821940952147, 6.9112, 168.9124931349316, 1494661.798537604, 1453782.9225745, 311359.015295704]
[2019-03-25 22:33:45,501] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:33:45,503] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.21279897648340051
[2019-03-25 22:33:45,504] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1494661.798537604 W.
[2019-03-25 22:33:55,451] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.4575369], dtype=float32), 0.04310132]
[2019-03-25 22:33:55,453] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.83333333333333, 56.0, 1.0, 2.0, 0.9158804440358738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1280150.879846955, 1280150.879846956, 274348.0712270936]
[2019-03-25 22:33:55,455] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:33:55,459] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9802782801864788
[2019-03-25 22:33:55,460] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1280150.879846955 W.
[2019-03-25 22:34:22,149] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:34:22,261] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:34:23,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:34:23,162] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.5406 2937714873.5162 1381.0000
[2019-03-25 22:34:23,213] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-03-25 22:34:24,227] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 225000, evaluation results [225000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8062.540634327665, 2937714873.516233, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-03-25 22:34:26,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:34:26,555] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0464
[2019-03-25 22:34:26,562] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7126174397305693, 6.9112, 6.9112, 168.912956510431, 636514.9852693421, 636514.9852693421, 184215.8515572498], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 549000.0000, 
sim time next is 549600.0000, 
raw observation next is [21.2, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6715993600777949, 6.911200000000001, 6.9112, 168.912956510431, 599861.8445047349, 599861.8445047343, 177017.3708834142], 
processed observation next is [1.0, 0.34782608695652173, 0.20379146919431282, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.599511414729018, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16662829014020414, 0.16662829014020397, 0.2642050311692749], 
reward next is 0.7358, 
noisyNet noise sample is [array([-0.953736], dtype=float32), -0.7565401]. 
=============================================
[2019-03-25 22:34:27,989] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:34:28,004] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7969
[2019-03-25 22:34:28,014] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.15, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4338024576197501, 6.9112, 6.9112, 168.912956510431, 389299.1388566919, 389299.1388566919, 144032.7715663778], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 513000.0000, 
sim time next is 513600.0000, 
raw observation next is [19.06666666666666, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4322391700487274, 6.9112, 6.9112, 168.912956510431, 387957.2324333489, 387957.2324333489, 143862.6373526909], 
processed observation next is [1.0, 0.9565217391304348, 0.10268562401263795, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.30760874396186266, 0.0, 0.0, 0.8294399451523027, 0.10776589789815247, 0.10776589789815247, 0.2147203542577476], 
reward next is 0.7853, 
noisyNet noise sample is [array([1.2693537], dtype=float32), -2.633927]. 
=============================================
[2019-03-25 22:34:52,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 3.839217e-37 0.000000e+00 0.000000e+00 1.595258e-26], sum to 1.0000
[2019-03-25 22:34:52,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8018
[2019-03-25 22:34:52,426] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5514349938918485, 6.911199999999999, 6.9112, 168.912956510431, 484073.4764388891, 484073.4764388897, 158917.4662043549], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 856800.0000, 
sim time next is 857400.0000, 
raw observation next is [21.76666666666667, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.552932055187101, 6.911199999999999, 6.9112, 168.912956510431, 485300.6216864148, 485300.6216864154, 159128.1264218148], 
processed observation next is [0.0, 0.9565217391304348, 0.23064770932069528, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4547951892525621, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13480572824622633, 0.1348057282462265, 0.23750466630121614], 
reward next is 0.7625, 
noisyNet noise sample is [array([-0.06059482], dtype=float32), -0.079207025]. 
=============================================
[2019-03-25 22:34:56,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7878513e-34], sum to 1.0000
[2019-03-25 22:34:56,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9358
[2019-03-25 22:34:56,931] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1061376.521502173 W.
[2019-03-25 22:34:56,941] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [21.86666666666667, 95.0, 1.0, 2.0, 0.2338201199667136, 1.0, 1.0, 0.2338201199667136, 1.0, 2.0, 0.4106901621640853, 6.911199999999999, 6.9112, 170.5573041426782, 1061376.521502173, 1061376.521502174, 288594.1225984267], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 994200.0000, 
sim time next is 994800.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.2378467887370626, 0.0, 1.0, 0.0, 1.0, 2.0, 0.4208638630125117, 6.9112, 6.9112, 168.912956510431, 727990.5163805939, 727990.5163805939, 209805.1462860757], 
processed observation next is [1.0, 0.5217391304347826, 0.23380726698262277, 0.95, 1.0, 1.0, 0.08174311896031639, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.2937364183079411, 0.0, 0.0, 0.8294399451523027, 0.2022195878834983, 0.2022195878834983, 0.31314200938220255], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.3113086], dtype=float32), -0.9879596]. 
=============================================
[2019-03-25 22:34:59,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6781036e-33], sum to 1.0000
[2019-03-25 22:34:59,235] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1186
[2019-03-25 22:34:59,241] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6057602650681589, 6.9112, 6.9112, 168.912956510431, 526165.6061189548, 526165.6061189548, 166982.0330916396], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 952200.0000, 
sim time next is 952800.0000, 
raw observation next is [21.8, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6061521549460634, 6.9112, 6.9112, 168.912956510431, 526394.0971710646, 526394.0971710646, 167043.9888886398], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.9466666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5196977499342237, 0.0, 0.0, 0.8294399451523027, 0.14622058254751794, 0.14622058254751794, 0.24931938640095494], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.19125737], dtype=float32), 0.63696754]. 
=============================================
[2019-03-25 22:35:15,297] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:35:15,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2634
[2019-03-25 22:35:15,318] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 58.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.587065809031344, 6.911200000000001, 6.9112, 168.912956510431, 507561.5152911974, 507561.5152911968, 164184.0450386821], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1185600.0000, 
sim time next is 1186200.0000, 
raw observation next is [27.15, 59.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5785809147139042, 6.911200000000001, 6.9112, 168.912956510431, 501528.0278186602, 501528.0278186596, 162917.9359061411], 
processed observation next is [1.0, 0.7391304347826086, 0.485781990521327, 0.595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4860742862364685, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13931334106073895, 0.13931334106073878, 0.24316109836737476], 
reward next is 0.7568, 
noisyNet noise sample is [array([1.7090074], dtype=float32), 0.35677972]. 
=============================================
[2019-03-25 22:35:17,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:35:17,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0908
[2019-03-25 22:35:17,757] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.2, 78.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.771443836888667, 6.9112, 6.9112, 168.912956510431, 643790.0398299666, 643790.0398299666, 195994.3172173816], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1273200.0000, 
sim time next is 1273800.0000, 
raw observation next is [27.05, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.781878265189495, 6.9112, 6.9112, 168.912956510431, 653284.1257916833, 653284.1257916833, 198096.3885282711], 
processed observation next is [1.0, 0.7391304347826086, 0.4810426540284361, 0.7933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7339978843774329, 0.0, 0.0, 0.8294399451523027, 0.18146781271991202, 0.18146781271991202, 0.295666251534733], 
reward next is 0.7043, 
noisyNet noise sample is [array([1.3162565], dtype=float32), -1.5857537]. 
=============================================
[2019-03-25 22:35:18,503] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:35:18,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0147
[2019-03-25 22:35:18,521] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.85, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6614428018380085, 6.911199999999999, 6.9112, 168.912956510431, 575654.3116356224, 575654.311635623, 175941.1934023973], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1222200.0000, 
sim time next is 1222800.0000, 
raw observation next is [21.83333333333334, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6341472266113797, 6.9112, 6.9112, 168.912956510431, 551870.001663338, 551870.001663338, 171436.8748184002], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5538380812333898, 0.0, 0.0, 0.8294399451523027, 0.15329722268426058, 0.15329722268426058, 0.2558759325647764], 
reward next is 0.7441, 
noisyNet noise sample is [array([-0.38910168], dtype=float32), -1.7439476]. 
=============================================
[2019-03-25 22:35:18,586] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:35:18,595] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8617
[2019-03-25 22:35:18,611] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1607117.717010979 W.
[2019-03-25 22:35:18,617] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.01666666666667, 74.33333333333333, 1.0, 2.0, 0.3832131373469783, 1.0, 2.0, 0.3832131373469783, 1.0, 2.0, 0.6402270672249396, 6.911200000000001, 6.9112, 170.5573041426782, 1607117.717010979, 1607117.717010978, 340309.2763872124], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1246200.0000, 
sim time next is 1246800.0000, 
raw observation next is [27.23333333333334, 73.66666666666667, 1.0, 2.0, 0.5721169958686178, 1.0, 2.0, 0.5721169958686178, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1599555.667957389, 1599555.667957389, 324167.4908035718], 
processed observation next is [1.0, 0.43478260869565216, 0.4897314375987366, 0.7366666666666667, 1.0, 1.0, 0.484478308275443, 1.0, 1.0, 0.484478308275443, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4443210188770525, 0.4443210188770525, 0.4838320758262266], 
reward next is 0.5162, 
noisyNet noise sample is [array([0.08769047], dtype=float32), 1.3830014]. 
=============================================
[2019-03-25 22:35:20,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.3817052e-38], sum to 1.0000
[2019-03-25 22:35:20,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9946
[2019-03-25 22:35:20,238] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1370562.661934166 W.
[2019-03-25 22:35:20,246] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.8, 75.0, 1.0, 2.0, 0.9587489803684313, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1370562.661934166, 1370562.661934166, 291253.8341848298], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1245600.0000, 
sim time next is 1246200.0000, 
raw observation next is [27.01666666666667, 74.33333333333333, 1.0, 2.0, 0.5753729696330515, 1.0, 1.0, 0.5753729696330515, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1608665.727757643, 1608665.727757643, 325319.4823890631], 
processed observation next is [1.0, 0.43478260869565216, 0.4794628751974725, 0.7433333333333333, 1.0, 1.0, 0.48840116823259216, 1.0, 0.5, 0.48840116823259216, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.44685159104378974, 0.44685159104378974, 0.485551466252333], 
reward next is 0.5144, 
noisyNet noise sample is [array([0.42941996], dtype=float32), 1.8000884]. 
=============================================
[2019-03-25 22:35:22,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.9875549e-36 1.5653671e-37], sum to 1.0000
[2019-03-25 22:35:22,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9687
[2019-03-25 22:35:22,599] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.91666666666667, 90.50000000000001, 1.0, 1.0, 0.2797609276440182, 1.0, 1.0, 0.2797609276440182, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 860812.83773003, 860812.83773003, 256269.7581318509], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1343400.0000, 
sim time next is 1344000.0000, 
raw observation next is [21.83333333333334, 90.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.174824804072199, 6.9112, 168.911562747649, 1082365.444529628, 895342.4784631403, 256508.8837356624], 
processed observation next is [1.0, 0.5652173913043478, 0.23380726698262277, 0.9, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.02636248040721991, 0.0, 0.8294331011389756, 0.30065706792489666, 0.24870624401753896, 0.3828490802024812], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0170627], dtype=float32), 0.79956186]. 
=============================================
[2019-03-25 22:35:22,613] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[44.375248]
 [46.18525 ]
 [46.092236]
 [45.74215 ]
 [45.756546]], R is [[43.89891052]
 [44.07743073]
 [44.25394058]
 [43.81140137]
 [43.3732872 ]].
[2019-03-25 22:35:23,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:35:23,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6677
[2019-03-25 22:35:23,072] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.45, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7984139630918087, 6.9112, 6.9112, 168.912956510431, 668955.3979834877, 668955.3979834877, 201501.4179064965], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1276200.0000, 
sim time next is 1276800.0000, 
raw observation next is [26.3, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7943486685773753, 6.9112, 6.9112, 168.912956510431, 665988.6470284841, 665988.6470284841, 200677.8981890676], 
processed observation next is [1.0, 0.782608695652174, 0.4454976303317536, 0.8266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.749205693387043, 0.0, 0.0, 0.8294399451523027, 0.18499684639680114, 0.18499684639680114, 0.2995192510284591], 
reward next is 0.7005, 
noisyNet noise sample is [array([0.98908067], dtype=float32), 0.6415532]. 
=============================================
[2019-03-25 22:35:26,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 9.1943767e-33 0.0000000e+00 5.2999852e-33 1.9211263e-31], sum to 1.0000
[2019-03-25 22:35:26,492] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8985
[2019-03-25 22:35:26,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1217652.912852239 W.
[2019-03-25 22:35:26,510] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.2797371116381218, 1.0, 1.0, 0.2797371116381218, 1.0, 1.0, 0.4777454322298076, 6.9112, 6.9112, 170.5573041426782, 1217652.912852239, 1217652.912852239, 299895.5223744906], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1331400.0000, 
sim time next is 1332000.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.2807654808101761, 1.0, 2.0, 0.2807654808101761, 1.0, 2.0, 0.4786358216530059, 6.9112, 6.9112, 170.5573041426782, 1218910.330002694, 1218910.330002694, 299911.9953484714], 
processed observation next is [1.0, 0.43478260869565216, 0.28909952606635075, 0.95, 1.0, 1.0, 0.13345238651828445, 1.0, 1.0, 0.13345238651828445, 1.0, 1.0, 0.3641900264061047, 0.0, 0.0, 0.8375144448122397, 0.3385862027785261, 0.3385862027785261, 0.44762984380368864], 
reward next is 0.5524, 
noisyNet noise sample is [array([0.7205402], dtype=float32), -1.4056801]. 
=============================================
[2019-03-25 22:35:26,524] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.311337]
 [60.312496]
 [60.744537]
 [61.167706]
 [61.516422]], R is [[59.70651627]
 [59.66184616]
 [59.68608093]
 [59.73266602]
 [59.13534164]].
[2019-03-25 22:35:29,219] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3997434e-31], sum to 1.0000
[2019-03-25 22:35:29,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2247
[2019-03-25 22:35:29,235] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5776469380334789, 6.911200000000001, 6.9112, 168.912956510431, 506622.848828721, 506622.8488287205, 162661.2416470822], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1362600.0000, 
sim time next is 1363200.0000, 
raw observation next is [21.03333333333333, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5755125883103029, 6.911199999999999, 6.9112, 168.912956510431, 504488.8921073263, 504488.8921073269, 162357.9977312883], 
processed observation next is [1.0, 0.782608695652174, 0.19589257503949445, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4823324247686621, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14013580336314618, 0.14013580336314635, 0.24232536974819152], 
reward next is 0.7577, 
noisyNet noise sample is [array([-0.89841014], dtype=float32), 2.474557]. 
=============================================
[2019-03-25 22:35:31,745] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-25 22:35:31,748] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:35:31,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:35:31,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:35:31,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:35:31,751] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:35:31,754] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:35:31,755] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:35:31,755] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:35:31,758] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:35:31,758] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:35:31,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-25 22:35:31,798] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-25 22:35:31,818] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-25 22:35:31,819] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-25 22:35:31,857] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-25 22:35:32,966] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:35:32,966] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.38333333333333, 79.0, 1.0, 2.0, 0.6812984744907018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564463541, 1066521.709309476, 1066521.709309476, 230755.5608010496]
[2019-03-25 22:35:32,966] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:35:32,968] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4237867219603725
[2019-03-25 22:35:32,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1066521.709309476 W.
[2019-03-25 22:35:42,333] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:35:42,333] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.73685096, 72.204306225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4401877363235027, 6.911199999999999, 6.9112, 168.912956510431, 395655.6600821475, 395655.6600821481, 144664.2916064444]
[2019-03-25 22:35:42,334] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:35:42,337] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9138007634542816
[2019-03-25 22:36:00,562] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:36:00,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.56666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8544730189630297, 6.911199999999999, 6.9112, 168.912956510431, 707853.5785466613, 707853.5785466619, 213217.4852395443]
[2019-03-25 22:36:00,564] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:36:00,568] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.823253685166047
[2019-03-25 22:36:53,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:36:53,501] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.63333333333333, 82.33333333333334, 1.0, 1.0, 0.6091565375293876, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128716867289, 851262.7380008275, 851262.7380008275, 202463.3072520455]
[2019-03-25 22:36:53,502] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:36:53,506] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8852206387729453
[2019-03-25 22:36:57,179] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:36:57,180] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.722428565, 85.900178675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9081203854926745, 6.9112, 6.9112, 168.912956510431, 747433.6975054626, 747433.6975054626, 225238.7279266208]
[2019-03-25 22:36:57,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:36:57,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6738506925858879
[2019-03-25 22:36:59,969] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:36:59,971] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.05, 65.5, 1.0, 2.0, 0.9437222098221358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1319090.24475488, 1319090.24475488, 282250.5741564582]
[2019-03-25 22:36:59,971] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:36:59,974] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.36164648100815
[2019-03-25 22:36:59,975] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1319090.24475488 W.
[2019-03-25 22:37:05,715] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:37:05,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.06666666666666, 88.0, 1.0, 2.0, 0.6114973781291423, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.944427761844446, 6.9112, 168.9121178453645, 1709758.586588416, 1686185.785765908, 366829.3002256238]
[2019-03-25 22:37:05,720] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:37:05,722] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8161057135721508
[2019-03-25 22:37:05,722] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1709758.586588416 W.
[2019-03-25 22:37:24,754] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:37:24,756] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.13333333333334, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7387171953835353, 6.9112, 6.9112, 168.912956510431, 631573.347661859, 631573.347661859, 189873.7295067454]
[2019-03-25 22:37:24,758] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:37:24,762] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.0672491619031309
[2019-03-25 22:37:33,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:37:33,719] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.03333333333333, 63.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7190885532548859, 6.9112, 6.9112, 168.912956510431, 614067.6716358176, 614067.6716358176, 186198.0011179119]
[2019-03-25 22:37:33,720] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:37:33,724] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8097663610002066
[2019-03-25 22:37:38,092] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.46904945], dtype=float32), 0.089219615]
[2019-03-25 22:37:38,095] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.39813405, 66.72916321999999, 1.0, 2.0, 0.8592919411870755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1201010.862392078, 1201010.862392077, 258984.7008720835]
[2019-03-25 22:37:38,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:37:38,099] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7020743336246498
[2019-03-25 22:37:38,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1201010.862392078 W.
[2019-03-25 22:37:40,383] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:37:40,989] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-03-25 22:37:41,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:37:41,418] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:37:41,425] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.5406 2937714873.5162 1381.0000
[2019-03-25 22:37:42,442] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 250000, evaluation results [250000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8062.540634327665, 2937714873.516233, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-03-25 22:37:45,619] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:37:45,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0279
[2019-03-25 22:37:45,637] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.93333333333334, 69.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7583953894823373, 6.9112, 6.9112, 168.912956510431, 639298.9911641382, 639298.9911641382, 193567.4800280523], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1440600.0000, 
sim time next is 1441200.0000, 
raw observation next is [27.86666666666667, 69.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7541232141879118, 6.911199999999999, 6.9112, 168.912956510431, 636194.6363583707, 636194.6363583714, 192745.4522873811], 
processed observation next is [0.0, 0.6956521739130435, 0.519747235387046, 0.6966666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7001502612047704, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17672073232176963, 0.17672073232176982, 0.2876797795334046], 
reward next is 0.7123, 
noisyNet noise sample is [array([-0.17428303], dtype=float32), -2.5122886]. 
=============================================
[2019-03-25 22:37:46,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:37:46,754] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0929
[2019-03-25 22:37:46,765] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.9, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6708862680430631, 6.911200000000001, 6.9112, 168.912956510431, 576963.7118294627, 576963.711829462, 177621.3762884925], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [22.85, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6699196475890312, 6.911200000000001, 6.9112, 168.912956510431, 576206.3941894145, 576206.3941894138, 177455.5214123992], 
processed observation next is [0.0, 0.8695652173913043, 0.28199052132701435, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5974629848646722, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1600573317192818, 0.1600573317192816, 0.26485898718268536], 
reward next is 0.7351, 
noisyNet noise sample is [array([-0.9378077], dtype=float32), -0.20730168]. 
=============================================
[2019-03-25 22:37:53,860] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.7530222e-33 0.0000000e+00 0.0000000e+00 1.3035201e-22], sum to 1.0000
[2019-03-25 22:37:53,870] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5455
[2019-03-25 22:37:53,877] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.13333333333333, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7161820286826319, 6.911199999999999, 6.9112, 168.912956510431, 610666.0307506204, 610666.030750621, 185658.3080911199], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1633200.0000, 
sim time next is 1633800.0000, 
raw observation next is [23.11666666666667, 96.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7161626697412077, 6.9112, 6.9112, 168.912956510431, 610619.7769257235, 610619.7769257235, 185654.5808683418], 
processed observation next is [1.0, 0.9130434782608695, 0.29462875197472377, 0.9683333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6538569143185459, 0.0, 0.0, 0.8294399451523027, 0.16961660470158987, 0.16961660470158987, 0.27709638935573405], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.24165373], dtype=float32), -1.3729736]. 
=============================================
[2019-03-25 22:37:54,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.454974e-36], sum to 1.0000
[2019-03-25 22:37:54,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5679
[2019-03-25 22:37:54,496] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.6, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5534538288291851, 6.911199999999999, 6.9112, 168.912956510431, 484786.7941328553, 484786.7941328558, 159228.1254359836], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1572600.0000, 
sim time next is 1573200.0000, 
raw observation next is [21.6, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5521739471404001, 6.911200000000001, 6.9112, 168.912956510431, 483763.8062439788, 483763.8062439781, 159047.3305818507], 
processed observation next is [1.0, 0.21739130434782608, 0.22274881516587688, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4538706672443904, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13437883506777187, 0.13437883506777168, 0.23738407549529955], 
reward next is 0.7626, 
noisyNet noise sample is [array([-1.7323083], dtype=float32), -1.3159388]. 
=============================================
[2019-03-25 22:37:55,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.8282394e-30], sum to 1.0000
[2019-03-25 22:37:55,162] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7505
[2019-03-25 22:37:55,174] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1140031.637307943 W.
[2019-03-25 22:37:55,179] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.63333333333334, 85.0, 1.0, 2.0, 0.7562596063971289, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1140031.637307943, 1140031.637307943, 244420.7728314528], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1592400.0000, 
sim time next is 1593000.0000, 
raw observation next is [23.65, 85.0, 1.0, 2.0, 0.3760776947545195, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6599326824072093, 6.911199999999999, 6.9112, 168.912956510431, 1136474.684841009, 1136474.68484101, 256492.9564770478], 
processed observation next is [1.0, 0.43478260869565216, 0.31990521327014215, 0.85, 1.0, 1.0, 0.24828637922231267, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.585283759033182, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31568741245583587, 0.3156874124558361, 0.3828253081746982], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4648044], dtype=float32), -1.5291104]. 
=============================================
[2019-03-25 22:37:55,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.752842]
 [64.45592 ]
 [64.46017 ]
 [64.47485 ]
 [64.406815]], R is [[63.44245911]
 [62.8080368 ]
 [62.17995834]
 [61.55815887]
 [60.94257736]].
[2019-03-25 22:38:06,785] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:38:06,797] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2279
[2019-03-25 22:38:06,803] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.45, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6061101091475193, 6.911199999999999, 6.9112, 168.912956510431, 528242.5919976203, 528242.5919976209, 167004.7363960108], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1809000.0000, 
sim time next is 1809600.0000, 
raw observation next is [21.46666666666667, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6092347289718154, 6.9112, 6.9112, 168.912956510431, 530656.300366194, 530656.300366194, 167492.4760990699], 
processed observation next is [1.0, 0.9565217391304348, 0.21642969984202226, 0.9533333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5234569865509944, 0.0, 0.0, 0.8294399451523027, 0.14740452787949834, 0.14740452787949834, 0.24998877029711924], 
reward next is 0.7500, 
noisyNet noise sample is [array([0.17772342], dtype=float32), -0.7005268]. 
=============================================
[2019-03-25 22:38:14,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.5784779e-33 0.0000000e+00 2.0104772e-37 9.3739004e-32], sum to 1.0000
[2019-03-25 22:38:14,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7268
[2019-03-25 22:38:14,873] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.65, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7640737573668032, 6.9112, 6.9112, 168.912956510431, 645650.7652913006, 645650.7652913006, 194703.5863013199], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1894200.0000, 
sim time next is 1894800.0000, 
raw observation next is [24.6, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7639935194344392, 6.911200000000001, 6.9112, 168.912956510431, 645650.2503282651, 645650.2503282645, 194688.818911223], 
processed observation next is [1.0, 0.9565217391304348, 0.36492890995260674, 0.9066666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7121872188224868, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1793472917578514, 0.17934729175785127, 0.29058032673316864], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.8874463], dtype=float32), 1.1098799]. 
=============================================
[2019-03-25 22:38:21,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 9.4758017e-37 0.0000000e+00 5.4682845e-34 0.0000000e+00], sum to 1.0000
[2019-03-25 22:38:21,422] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6214
[2019-03-25 22:38:21,435] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1282476.802348142 W.
[2019-03-25 22:38:21,439] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.93333333333334, 76.33333333333334, 1.0, 2.0, 0.3058496180330434, 1.0, 1.0, 0.3058496180330434, 1.0, 2.0, 0.5118069081273778, 6.9112, 6.9112, 170.5573041426782, 1282476.802348142, 1282476.802348142, 304877.943143988], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [27.0, 76.0, 1.0, 2.0, 0.2891347837305261, 1.0, 2.0, 0.2891347837305261, 1.0, 2.0, 0.4839148861210366, 6.911200000000001, 6.9112, 170.5573041426782, 1212349.175336965, 1212349.175336964, 298246.443602197], 
processed observation next is [1.0, 0.5217391304347826, 0.4786729857819906, 0.76, 1.0, 1.0, 0.14353588401268202, 1.0, 1.0, 0.14353588401268202, 1.0, 1.0, 0.37062790990370315, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.33676365981582357, 0.33676365981582335, 0.4451439456749209], 
reward next is 0.5549, 
noisyNet noise sample is [array([-0.4662973], dtype=float32), 0.090687186]. 
=============================================
[2019-03-25 22:38:28,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:38:28,545] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0809
[2019-03-25 22:38:28,551] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.4, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8694440902921632, 6.9112, 6.9112, 168.912956510431, 718328.0743165503, 718328.0743165503, 216478.0319856968], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2039400.0000, 
sim time next is 2040000.0000, 
raw observation next is [27.33333333333334, 82.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8687568840859526, 6.9112, 6.9112, 168.912956510431, 718177.7489100752, 718177.7489100752, 216338.8384583104], 
processed observation next is [0.0, 0.6086956521739131, 0.4944707740916275, 0.8266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8399474196170152, 0.0, 0.0, 0.8294399451523027, 0.19949381914168754, 0.19949381914168754, 0.32289378874374686], 
reward next is 0.6771, 
noisyNet noise sample is [array([-1.0965586], dtype=float32), 0.46701577]. 
=============================================
[2019-03-25 22:38:28,573] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[73.6476  ]
 [73.60868 ]
 [73.56956 ]
 [73.537476]
 [73.47919 ]], R is [[73.64497375]
 [73.5854187 ]
 [73.52605438]
 [73.46647644]
 [73.40690613]].
[2019-03-25 22:38:37,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.4125245e-38 0.0000000e+00 3.1980141e-28], sum to 1.0000
[2019-03-25 22:38:37,566] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5593
[2019-03-25 22:38:37,572] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.1, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9269005619068587, 6.9112, 6.9112, 168.912956510431, 757032.0214540289, 757032.0214540289, 229435.5294847163], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2237400.0000, 
sim time next is 2238000.0000, 
raw observation next is [28.0, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9232128850403781, 6.911200000000001, 6.9112, 168.912956510431, 754578.278169787, 754578.2781697864, 228581.6261004704], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.8333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9063571768785098, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2096050772693853, 0.20960507726938513, 0.3411666061201051], 
reward next is 0.6588, 
noisyNet noise sample is [array([0.02206434], dtype=float32), 2.2008955]. 
=============================================
[2019-03-25 22:38:37,585] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[40.55971 ]
 [40.865543]
 [40.63758 ]
 [40.72159 ]
 [40.74743 ]], R is [[40.71524429]
 [40.96565247]
 [41.21239853]
 [41.45572281]
 [41.69561005]].
[2019-03-25 22:38:39,944] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.3332750e-25 9.0275041e-29 8.5372168e-24 3.2989015e-35], sum to 1.0000
[2019-03-25 22:38:39,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0631
[2019-03-25 22:38:39,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2204809.953556988 W.
[2019-03-25 22:38:39,974] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 73.5, 1.0, 2.0, 0.9355600947918281, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999860206582634, 6.9112, 168.9124290349641, 2204809.953556988, 2141911.547976008, 444338.6265685046], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2199000.0000, 
sim time next is 2199600.0000, 
raw observation next is [30.2, 73.0, 1.0, 2.0, 0.5297890372912697, 1.0, 1.0, 0.5297890372912697, 1.0, 2.0, 0.9200683538630305, 6.9112, 6.9112, 170.5573041426782, 2222463.243824692, 2222463.243824692, 436356.9150213614], 
processed observation next is [1.0, 0.4782608695652174, 0.6303317535545023, 0.73, 1.0, 1.0, 0.43348076782080686, 1.0, 0.5, 0.43348076782080686, 1.0, 1.0, 0.9025223827597934, 0.0, 0.0, 0.8375144448122397, 0.6173509010624144, 0.6173509010624144, 0.6512789776438229], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4460912], dtype=float32), 0.3822849]. 
=============================================
[2019-03-25 22:38:49,959] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-25 22:38:49,962] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:38:49,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:38:49,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:38:49,967] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:38:49,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:38:49,969] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:38:49,969] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:38:49,970] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:38:49,970] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:38:49,973] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:38:49,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-25 22:38:49,987] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-25 22:38:50,023] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-25 22:38:50,039] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-25 22:38:50,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-25 22:38:55,005] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:38:55,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.43333333333334, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.518907481043315, 6.911200000000001, 6.9112, 168.912956510431, 457874.7427059273, 457874.7427059267, 154466.7698189131]
[2019-03-25 22:38:55,009] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:38:55,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 3.0320931e-37 4.7455820e-35 2.1931037e-35 7.9971831e-24], sampled 0.685987669454481
[2019-03-25 22:38:56,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:38:56,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.4, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4152845506242471, 6.911199999999999, 6.9112, 168.912956510431, 373844.8694813706, 373844.8694813711, 142018.9499762677]
[2019-03-25 22:38:56,362] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:38:56,367] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9891016e-30], sampled 0.8929220031498344
[2019-03-25 22:39:20,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:39:20,376] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.78226667, 91.62612149333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6794331018772061, 6.911199999999999, 6.9112, 168.912956510431, 585495.184747902, 585495.1847479026, 179090.9247626756]
[2019-03-25 22:39:20,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:39:20,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.9665049e-38 9.4691118e-38 3.5441845e-38 1.4902358e-28], sampled 0.7653708932426637
[2019-03-25 22:39:22,458] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:39:22,459] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8672310950082062, 6.9112, 6.9112, 168.912956510431, 722310.7278117696, 722310.7278117696, 216171.1886202169]
[2019-03-25 22:39:22,461] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:39:22,463] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 5.5592615e-38 0.0000000e+00 2.6005686e-38 7.2119364e-32], sampled 0.80777339458343
[2019-03-25 22:39:27,506] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:39:27,507] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.43333333333334, 74.66666666666667, 1.0, 2.0, 0.7113151682700708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 994090.6155838153, 994090.6155838153, 223304.7527097594]
[2019-03-25 22:39:27,508] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:39:27,511] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.7310799e-35 5.8105700e-36 3.9672823e-32 7.3834638e-26], sampled 0.5296596255374963
[2019-03-25 22:39:27,512] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 994090.6155838153 W.
[2019-03-25 22:40:08,741] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:40:08,742] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.03333333333334, 79.33333333333334, 1.0, 2.0, 0.6377707563481522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891266.3115426314, 891266.3115426314, 207993.4758676096]
[2019-03-25 22:40:08,743] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:40:08,744] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7196973e-38 1.2213751e-31], sampled 0.5405502703000087
[2019-03-25 22:40:08,746] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 891266.3115426314 W.
[2019-03-25 22:40:37,047] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.470449], dtype=float32), 0.08641025]
[2019-03-25 22:40:37,047] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.66666666666666, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9416488888253864, 6.911200000000001, 6.9112, 168.912956510431, 769785.5485108382, 769785.5485108376, 233020.9136104479]
[2019-03-25 22:40:37,048] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:40:37,052] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 7.2121106e-37 1.0035629e-35 9.4301911e-36 8.2339196e-28], sampled 0.21275271628764847
[2019-03-25 22:40:59,283] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7032.0616 3185036340.5509 2464.0000
[2019-03-25 22:40:59,376] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7288.9996 3319521307.1094 2143.0000
[2019-03-25 22:40:59,413] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.3569 2989248936.3710 1566.0000
[2019-03-25 22:40:59,508] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.4079 3105587199.2298 2010.0000
[2019-03-25 22:40:59,700] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8126 2937782196.4717 1381.0000
[2019-03-25 22:41:00,716] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 275000, evaluation results [275000.0, 7288.999644938805, 3319521307.1093755, 2143.0, 7348.407911634859, 3105587199.229759, 2010.0, 8061.812589515823, 2937782196.4716883, 1381.0, 7032.061550876785, 3185036340.5509343, 2464.0, 7925.35686221895, 2989248936.3710446, 1566.0]
[2019-03-25 22:41:01,611] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.2102870e-28 2.2260957e-26 1.7124532e-20 2.3058978e-19], sum to 1.0000
[2019-03-25 22:41:01,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0525
[2019-03-25 22:41:01,636] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1128688.835601956 W.
[2019-03-25 22:41:01,643] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.1, 73.5, 1.0, 2.0, 0.8075749051276166, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1128688.835601956, 1128688.835601956, 245777.8724052707], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2359800.0000, 
sim time next is 2360400.0000, 
raw observation next is [29.23333333333333, 73.0, 1.0, 2.0, 0.4093029648819454, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6956349128567573, 6.911199999999999, 6.9112, 168.912956510431, 1144114.404157693, 1144114.404157693, 261381.8373785969], 
processed observation next is [1.0, 0.30434782608695654, 0.5845181674565559, 0.73, 1.0, 1.0, 0.28831682515897034, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.62882306445946, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3178095567104703, 0.3178095567104703, 0.3901221453411894], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.44020572], dtype=float32), -0.44705528]. 
=============================================
[2019-03-25 22:41:02,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 7.6835314e-27 7.1625364e-23 6.3137827e-23 1.7788348e-22], sum to 1.0000
[2019-03-25 22:41:02,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6685
[2019-03-25 22:41:02,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1130275.323438445 W.
[2019-03-25 22:41:02,067] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.1, 73.5, 1.0, 2.0, 0.4043567890255748, 1.0, 1.0, 0.4043567890255748, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1130275.323438445, 1130275.323438445, 273228.0974565301], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2359800.0000, 
sim time next is 2360400.0000, 
raw observation next is [29.23333333333333, 73.0, 1.0, 2.0, 0.409422980946407, 1.0, 2.0, 0.409422980946407, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1144444.11679378, 1144444.11679378, 274524.9467969319], 
processed observation next is [1.0, 0.30434782608695654, 0.5845181674565559, 0.73, 1.0, 1.0, 0.2884614228269964, 1.0, 1.0, 0.2884614228269964, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.31790114355382776, 0.31790114355382776, 0.4097387265625849], 
reward next is 0.5903, 
noisyNet noise sample is [array([0.5768514], dtype=float32), -1.5446595]. 
=============================================
[2019-03-25 22:41:02,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.0347919e-27 4.7965045e-28 4.1434272e-24 1.0421800e-27], sum to 1.0000
[2019-03-25 22:41:02,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9648
[2019-03-25 22:41:02,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2107477.492310442 W.
[2019-03-25 22:41:02,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.6, 65.33333333333334, 1.0, 2.0, 0.8660217979043424, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.000686574630299, 6.9112, 168.9124243319912, 2107477.492310442, 2043992.836393371, 425003.389916285], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2370000.0000, 
sim time next is 2370600.0000, 
raw observation next is [31.75, 65.0, 1.0, 2.0, 0.8891679403949264, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.999021964357471, 6.9112, 168.912361405778, 2139874.727603374, 2077571.022995133, 431333.297150485], 
processed observation next is [1.0, 0.43478260869565216, 0.7037914691943128, 0.65, 1.0, 1.0, 0.8664673980661763, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008782196435747115, 0.0, 0.8294370229160164, 0.5944096465564928, 0.5771030619430925, 0.6437810405231119], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3487055], dtype=float32), -0.32010445]. 
=============================================
[2019-03-25 22:41:10,206] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.6963639e-32 6.4407071e-34 5.0113877e-31 3.7153581e-34], sum to 1.0000
[2019-03-25 22:41:10,218] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5586
[2019-03-25 22:41:10,226] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1994882.28020787 W.
[2019-03-25 22:41:10,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.4, 89.0, 1.0, 2.0, 0.7133829342162711, 1.0, 2.0, 0.7133829342162711, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1994882.28020787, 1994882.28020787, 379900.2444185597], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2469600.0000, 
sim time next is 2470200.0000, 
raw observation next is [26.51666666666667, 88.5, 1.0, 2.0, 0.7447695318363368, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.978440899556907, 6.9112, 168.9125614834055, 1937779.064528846, 1890076.167690302, 395626.2815464232], 
processed observation next is [1.0, 0.6086956521739131, 0.45576619273301755, 0.885, 1.0, 1.0, 0.6924934118510081, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.006724089955690715, 0.0, 0.8294380053887633, 0.5382719623691239, 0.5250211576917505, 0.5904869873827212], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37186238], dtype=float32), -0.9321777]. 
=============================================
[2019-03-25 22:41:15,576] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.2514994e-30 0.0000000e+00 9.1588011e-35 2.0362480e-38], sum to 1.0000
[2019-03-25 22:41:15,586] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8742
[2019-03-25 22:41:15,598] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1887375.749899192 W.
[2019-03-25 22:41:15,608] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.51666666666667, 79.33333333333334, 1.0, 2.0, 0.7087518132674632, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990305017990815, 6.9112, 168.9124850952179, 1887375.749899192, 1831256.084175403, 387143.3718461615], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2545800.0000, 
sim time next is 2546400.0000, 
raw observation next is [28.63333333333333, 78.66666666666667, 1.0, 2.0, 0.6581073370696143, 1.0, 1.0, 0.6581073370696143, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1840178.544447829, 1840178.544447829, 356705.3908785657], 
processed observation next is [1.0, 0.4782608695652174, 0.55608214849921, 0.7866666666666667, 1.0, 1.0, 0.5880811289995354, 1.0, 0.5, 0.5880811289995354, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5111607067910636, 0.5111607067910636, 0.5323961057889041], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.00267077], dtype=float32), 1.0147856]. 
=============================================
[2019-03-25 22:41:20,278] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:41:20,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2797
[2019-03-25 22:41:20,293] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.66666666666667, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7602401672126845, 6.9112, 6.9112, 168.912956510431, 642721.1019697809, 642721.1019697809, 193956.6741180435], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2658000.0000, 
sim time next is 2658600.0000, 
raw observation next is [24.5, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7502171422137102, 6.911199999999999, 6.9112, 168.912956510431, 635592.2341938505, 635592.2341938512, 192028.8711566466], 
processed observation next is [0.0, 0.782608695652174, 0.3601895734597157, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6953867587972075, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1765533983871807, 0.1765533983871809, 0.2866102554576815], 
reward next is 0.7134, 
noisyNet noise sample is [array([1.5249082], dtype=float32), -0.45740363]. 
=============================================
[2019-03-25 22:41:20,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:41:20,336] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4228
[2019-03-25 22:41:20,342] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.86666666666667, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7344352422590769, 6.911200000000001, 6.9112, 168.912956510431, 624853.7127108527, 624853.7127108522, 189046.4910974956], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2610600.0000, 
sim time next is 2611200.0000, 
raw observation next is [23.83333333333333, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7365009780088506, 6.9112, 6.9112, 168.912956510431, 626709.2718968336, 626709.2718968336, 189437.4354920388], 
processed observation next is [0.0, 0.21739130434782608, 0.32859399684044216, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.678659729279086, 0.0, 0.0, 0.8294399451523027, 0.17408590886023154, 0.17408590886023154, 0.28274244103289375], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.38118073], dtype=float32), -1.374224]. 
=============================================
[2019-03-25 22:41:22,115] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:41:22,125] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1475
[2019-03-25 22:41:22,133] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8281140504418588, 6.911200000000001, 6.9112, 168.912956510431, 690030.6117701493, 690030.6117701486, 207627.4180448015], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2644800.0000, 
sim time next is 2645400.0000, 
raw observation next is [27.0, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8194394957447412, 6.911199999999999, 6.9112, 168.912956510431, 683973.2053727263, 683973.2053727269, 205818.3636418799], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7798042631033428, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18999255704797954, 0.18999255704797968, 0.3071915875251939], 
reward next is 0.6928, 
noisyNet noise sample is [array([-0.02168941], dtype=float32), -0.96609664]. 
=============================================
[2019-03-25 22:41:36,816] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:41:36,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0590
[2019-03-25 22:41:36,839] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7055632674951066, 6.911200000000001, 6.9112, 168.912956510431, 602438.267845867, 602438.2678458664, 183727.2489241073], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2835600.0000, 
sim time next is 2836200.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7072190966109699, 6.9112, 6.9112, 168.912956510431, 603852.4830929274, 603852.4830929274, 184027.0201129947], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6429501178182561, 0.0, 0.0, 0.8294399451523027, 0.1677368008591465, 0.1677368008591465, 0.27466719419849955], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.26851368], dtype=float32), -0.7473599]. 
=============================================
[2019-03-25 22:41:40,112] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:41:40,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6919
[2019-03-25 22:41:40,130] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 900560.6969882754 W.
[2019-03-25 22:41:40,138] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5880114023256069, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564925879, 900560.6969882754, 900560.6969882754, 208131.0893150237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2883600.0000, 
sim time next is 2884200.0000, 
raw observation next is [22.05, 93.66666666666667, 1.0, 2.0, 0.5291138164130661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104265, 814546.6251159222, 814546.6251159215, 197293.8722913457], 
processed observation next is [1.0, 0.391304347826087, 0.24407582938388633, 0.9366666666666668, 1.0, 1.0, 0.43266724869044104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451522806, 0.2262629514210895, 0.2262629514210893, 0.29446846610648614], 
reward next is 0.7055, 
noisyNet noise sample is [array([-0.6209623], dtype=float32), -1.38374]. 
=============================================
[2019-03-25 22:41:46,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7350656e-36], sum to 1.0000
[2019-03-25 22:41:46,992] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0576
[2019-03-25 22:41:46,996] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.2625741142172455, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4713324871700151, 6.911200000000001, 6.9112, 168.912956510431, 822530.3867721275, 822530.3867721268, 218543.2966981874], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2976000.0000, 
sim time next is 2976600.0000, 
raw observation next is [22.0, 88.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8934166348888698, 6.9112, 6.9112, 168.912956510431, 779975.7503229211, 779975.7503229211, 222292.7742723563], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.88, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8700202864498411, 0.0, 0.0, 0.8294399451523027, 0.21665993064525585, 0.21665993064525585, 0.3317802601079945], 
reward next is 0.6682, 
noisyNet noise sample is [array([0.7845603], dtype=float32), 0.6362817]. 
=============================================
[2019-03-25 22:41:49,989] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.1553243e-31], sum to 1.0000
[2019-03-25 22:41:50,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7623
[2019-03-25 22:41:50,014] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1390040.233085932 W.
[2019-03-25 22:41:50,022] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 97.0, 1.0, 1.0, 0.4812082315585673, 1.0, 1.0, 0.4812082315585673, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1390040.233085932, 1390040.233085932, 299821.3721388111], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3072600.0000, 
sim time next is 3073200.0000, 
raw observation next is [23.0, 98.0, 1.0, 2.0, 0.4378659455938813, 1.0, 2.0, 0.4378659455938813, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1259133.19592178, 1259133.19592178, 286293.7595596633], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.98, 1.0, 1.0, 0.32273005493238716, 1.0, 1.0, 0.32273005493238716, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.34975922108938334, 0.34975922108938334, 0.4273041187457661], 
reward next is 0.5727, 
noisyNet noise sample is [array([-1.2305114], dtype=float32), -0.2889791]. 
=============================================
[2019-03-25 22:41:50,698] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.318008e-36], sum to 1.0000
[2019-03-25 22:41:50,708] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9886
[2019-03-25 22:41:50,715] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.66666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5512771855035629, 6.911199999999999, 6.9112, 168.912956510431, 483656.3705015852, 483656.3705015858, 158903.8397830266], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3037200.0000, 
sim time next is 3037800.0000, 
raw observation next is [20.83333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.552942236682034, 6.911200000000001, 6.9112, 168.912956510431, 484879.9304059562, 484879.9304059555, 159142.0454749248], 
processed observation next is [1.0, 0.13043478260869565, 0.1864139020537123, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45480760570979756, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13468886955721007, 0.13468886955720988, 0.23752544100735043], 
reward next is 0.7625, 
noisyNet noise sample is [array([0.5802659], dtype=float32), -1.1807519]. 
=============================================
[2019-03-25 22:41:53,794] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.372981e-38], sum to 1.0000
[2019-03-25 22:41:53,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0143
[2019-03-25 22:41:53,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1208121.458065267 W.
[2019-03-25 22:41:53,815] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8371445149898817, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1208121.458065267, 1208121.458065267, 258641.0551640905], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3076800.0000, 
sim time next is 3077400.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.287514363244554, 1.0, 1.0, 0.287514363244554, 1.0, 1.0, 0.4826084374570529, 6.9112, 6.9112, 170.5573041426782, 1220630.78560721, 1220630.78560721, 299213.7526930163], 
processed observation next is [1.0, 0.6086956521739131, 0.28909952606635075, 1.0, 1.0, 1.0, 0.14158357017416143, 1.0, 0.5, 0.14158357017416143, 1.0, 0.5, 0.36903467982567423, 0.0, 0.0, 0.8375144448122397, 0.33906410711311386, 0.33906410711311386, 0.44658769058659153], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8900265], dtype=float32), -1.201289]. 
=============================================
[2019-03-25 22:42:08,157] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-25 22:42:08,159] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:42:08,160] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:42:08,161] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:42:08,161] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:42:08,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:42:08,166] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:42:08,166] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:42:08,168] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:42:08,168] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:42:08,170] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:42:08,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-25 22:42:08,211] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-25 22:42:08,234] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-25 22:42:08,235] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-25 22:42:08,235] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-25 22:42:38,776] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.3765425], dtype=float32), 0.17088456]
[2019-03-25 22:42:38,778] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.01666666666667, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6576780361723361, 6.9112, 6.9112, 168.912956510431, 566654.7085138375, 566654.7085138375, 175375.7398445395]
[2019-03-25 22:42:38,781] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:42:38,783] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9672698358656223
[2019-03-25 22:42:45,754] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3765425], dtype=float32), 0.17088456]
[2019-03-25 22:42:45,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.691851764304225, 6.911199999999999, 6.9112, 168.912956510431, 594274.9700631507, 594274.9700631513, 181277.2610228638]
[2019-03-25 22:42:45,756] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:42:45,762] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.12417922775928358
[2019-03-25 22:42:45,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.3765425], dtype=float32), 0.17088456]
[2019-03-25 22:42:45,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.7, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6930689259680296, 6.911200000000001, 6.9112, 168.912956510431, 592676.683676867, 592676.6836768665, 181492.3533810487]
[2019-03-25 22:42:45,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:42:45,969] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5929092074373864
[2019-03-25 22:43:29,717] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.3765425], dtype=float32), 0.17088456]
[2019-03-25 22:43:29,718] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.96666666666667, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.017705585496632, 6.911200000000001, 6.9112, 168.9127698479155, 822536.1011887118, 822536.1011887112, 251779.3072983574]
[2019-03-25 22:43:29,718] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:43:29,723] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28330247334615377
[2019-03-25 22:44:06,977] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.3765425], dtype=float32), 0.17088456]
[2019-03-25 22:44:06,979] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.524563115, 74.60002601833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6073389845334872, 6.911200000000001, 6.9112, 168.912956510431, 542318.0350378545, 542318.0350378539, 166654.9989710286]
[2019-03-25 22:44:06,980] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:44:06,983] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5636480778105291
[2019-03-25 22:44:17,507] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7926.0737 2989256945.2154 1566.0000
[2019-03-25 22:44:17,562] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3407 3185096716.9286 2464.0000
[2019-03-25 22:44:17,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:44:17,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.8126 2937782196.4717 1381.0000
[2019-03-25 22:44:18,055] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.3938 3105661283.1468 2010.0000
[2019-03-25 22:44:19,069] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 300000, evaluation results [300000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7348.393758113934, 3105661283.146778, 2010.0, 8061.812589515823, 2937782196.4716883, 1381.0, 7031.340658675213, 3185096716.9285784, 2464.0, 7926.073720331059, 2989256945.215427, 1566.0]
[2019-03-25 22:44:19,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:44:19,849] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3845
[2019-03-25 22:44:19,859] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9152427338091621, 6.9112, 6.9112, 168.912956510431, 750365.2480657027, 750365.2480657027, 226794.1834983413], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3366600.0000, 
sim time next is 3367200.0000, 
raw observation next is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9147903679163981, 6.9112, 6.9112, 168.912956510431, 749994.1639529181, 749994.1639529181, 226687.4330919687], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8960858145321927, 0.0, 0.0, 0.8294399451523027, 0.20833171220914393, 0.20833171220914393, 0.3383394523760727], 
reward next is 0.6617, 
noisyNet noise sample is [array([-0.8904271], dtype=float32), -1.1088967]. 
=============================================
[2019-03-25 22:44:20,834] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:44:20,839] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3958
[2019-03-25 22:44:20,846] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.5, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7498930419853365, 6.9112, 6.9112, 168.912956510431, 635719.2119125779, 635719.2119125779, 191971.4805337337], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3306600.0000, 
sim time next is 3307200.0000, 
raw observation next is [25.66666666666666, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7591708688685964, 6.911200000000001, 6.9112, 168.912956510431, 642078.7491417435, 642078.7491417429, 193751.7932392929], 
processed observation next is [0.0, 0.2608695652173913, 0.4154818325434437, 0.8366666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7063059376446297, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17835520809492875, 0.1783552080949286, 0.2891817809541685], 
reward next is 0.7108, 
noisyNet noise sample is [array([-0.04424678], dtype=float32), -0.77515256]. 
=============================================
[2019-03-25 22:44:21,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:44:21,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6000
[2019-03-25 22:44:21,717] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.83333333333334, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8311661188167208, 6.911199999999999, 6.9112, 168.912956510431, 692074.0541447345, 692074.0541447351, 208265.8809711502], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3315000.0000, 
sim time next is 3315600.0000, 
raw observation next is [29.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8349564553017255, 6.9112, 6.9112, 168.912956510431, 694769.66157284, 694769.66157284, 209066.2444170878], 
processed observation next is [0.0, 0.391304347826087, 0.5734597156398105, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7987273845142994, 0.0, 0.0, 0.8294399451523027, 0.1929915726591222, 0.1929915726591222, 0.3120391707717728], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.045177], dtype=float32), -0.19681165]. 
=============================================
[2019-03-25 22:44:21,937] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:44:21,944] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2933
[2019-03-25 22:44:21,952] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 79.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.943669416738019, 6.9112, 168.9126051323898, 851844.7780556623, 828809.9042286007, 254812.1589651237], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3345600.0000, 
sim time next is 3346200.0000, 
raw observation next is [30.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989962542932815, 6.9112, 168.9122994559342, 884699.3601948858, 828822.7187962156, 254812.1602551757], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007876254293281537, 0.0, 0.8294367187139225, 0.24574982227635717, 0.23022853299894877, 0.38031665709727713], 
reward next is 0.2259, 
noisyNet noise sample is [array([-0.4054895], dtype=float32), -0.32295358]. 
=============================================
[2019-03-25 22:44:26,457] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 9.0366598e-31 0.0000000e+00 1.4138968e-34 0.0000000e+00], sum to 1.0000
[2019-03-25 22:44:26,466] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7833
[2019-03-25 22:44:26,473] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1340514.97189479 W.
[2019-03-25 22:44:26,479] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.1, 93.33333333333334, 1.0, 2.0, 0.3196821180168924, 1.0, 1.0, 0.3196821180168924, 1.0, 2.0, 0.5469742083411818, 6.911199999999999, 6.9112, 170.5573041426782, 1340514.97189479, 1340514.971894791, 312007.8842339672], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3379200.0000, 
sim time next is 3379800.0000, 
raw observation next is [26.05, 93.66666666666667, 1.0, 2.0, 0.3088375587367946, 1.0, 2.0, 0.3088375587367946, 1.0, 2.0, 0.5286160781054352, 6.911199999999999, 6.9112, 170.5573041426782, 1295013.287373115, 1295013.287373115, 307377.5628203267], 
processed observation next is [1.0, 0.08695652173913043, 0.43364928909952616, 0.9366666666666668, 1.0, 1.0, 0.16727416715276455, 1.0, 1.0, 0.16727416715276455, 1.0, 1.0, 0.42514155866516484, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3597259131591986, 0.3597259131591986, 0.45877248182138314], 
reward next is 0.5412, 
noisyNet noise sample is [array([0.24224035], dtype=float32), 0.0494892]. 
=============================================
[2019-03-25 22:44:35,531] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 4.3830565e-35 0.0000000e+00 1.6598464e-38 2.5796815e-38], sum to 1.0000
[2019-03-25 22:44:35,538] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2643
[2019-03-25 22:44:35,546] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8748369664543195, 6.9112, 6.9112, 168.912956510431, 722537.2784442991, 722537.2784442991, 217681.846473759], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3542400.0000, 
sim time next is 3543000.0000, 
raw observation next is [28.0, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8694651027416562, 6.9112, 6.9112, 168.912956510431, 718676.0227941895, 718676.0227941895, 216494.4169506093], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8408111009044589, 0.0, 0.0, 0.8294399451523027, 0.19963222855394153, 0.19963222855394153, 0.3231259954486706], 
reward next is 0.6769, 
noisyNet noise sample is [array([0.02826935], dtype=float32), -1.444003]. 
=============================================
[2019-03-25 22:44:35,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.050407]
 [55.89193 ]
 [55.924366]
 [55.949043]
 [55.989178]], R is [[53.17822647]
 [53.32154846]
 [53.46276474]
 [53.60200882]
 [53.73985291]].
[2019-03-25 22:44:39,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.7811750e-29 1.3386826e-32 2.1968973e-27 9.7321970e-31], sum to 1.0000
[2019-03-25 22:44:39,701] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4197
[2019-03-25 22:44:39,710] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333333, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.016802652405404, 6.9112, 6.9112, 168.9128567654146, 851922.6564413669, 851922.6564413669, 252829.310514856], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3563400.0000, 
sim time next is 3564000.0000, 
raw observation next is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.934903256739456, 6.9112, 168.9126516030736, 879548.2006837219, 862732.3249042644, 256336.6097317266], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0023703256739455725, 0.0, 0.8294384479175907, 0.2443189446343672, 0.23964786802896235, 0.3825919548234726], 
reward next is 0.4989, 
noisyNet noise sample is [array([-0.4929566], dtype=float32), 0.25970852]. 
=============================================
[2019-03-25 22:44:39,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[30.791626]
 [30.407785]
 [29.31407 ]
 [27.927805]
 [27.72188 ]], R is [[31.18629837]
 [31.49707794]
 [31.80930901]
 [32.12839508]
 [32.40700531]].
[2019-03-25 22:44:44,335] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 4.886084e-38 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-25 22:44:44,344] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8175
[2019-03-25 22:44:44,361] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9463725717472855, 6.911199999999999, 6.9112, 168.912956510431, 769683.1551019042, 769683.1551019049, 233982.7372660607], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [29.0, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9411743403124173, 6.911200000000001, 6.9112, 168.912956510431, 766515.270748058, 766515.2707480573, 232770.2981082987], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9282613906248992, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21292090854112722, 0.21292090854112702, 0.34741835538552046], 
reward next is 0.6526, 
noisyNet noise sample is [array([1.086086], dtype=float32), 0.7846685]. 
=============================================
[2019-03-25 22:45:04,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.4004922e-35 2.0954151e-34 2.4208516e-33 9.3056083e-38], sum to 1.0000
[2019-03-25 22:45:04,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4969
[2019-03-25 22:45:04,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 915806.9458128207 W.
[2019-03-25 22:45:04,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 75.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.033794160147954, 6.9112, 168.9121352334478, 915806.9458128207, 828834.8521103037, 254812.5342138469], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3963600.0000, 
sim time next is 3964200.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 1.0, 0.3127663253588777, 1.0, 1.0, 0.3127663253588777, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 874153.4041233822, 874153.4041233822, 252402.2659829911], 
processed observation next is [0.0, 0.9130434782608695, 0.6761453396524489, 0.7433333333333333, 1.0, 0.5, 0.1720076209143105, 1.0, 0.5, 0.1720076209143105, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.24282039003427283, 0.24282039003427283, 0.3767197999746136], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.53127056], dtype=float32), 1.4944724]. 
=============================================
[2019-03-25 22:45:10,721] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.4130687e-21 1.4500906e-26 1.6179723e-09 3.0946498e-34], sum to 1.0000
[2019-03-25 22:45:10,732] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5366
[2019-03-25 22:45:10,741] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1273896.022662809 W.
[2019-03-25 22:45:10,747] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.4557040533518043, 0.0, 2.0, 0.0, 1.0, 2.0, 0.791407237038754, 6.911199999999999, 6.9112, 168.912956510431, 1273896.022662809, 1273896.02266281, 285283.6108147764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3994800.0000, 
sim time next is 3995400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.9021881975391465, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1261001.491075976, 1261001.491075976, 270543.7126969425], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.8821544548664415, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3502781919655489, 0.3502781919655489, 0.40379658611483954], 
reward next is 0.5962, 
noisyNet noise sample is [array([-2.5725398], dtype=float32), -0.28492138]. 
=============================================
[2019-03-25 22:45:14,480] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.572728e-38 0.000000e+00], sum to 1.0000
[2019-03-25 22:45:14,491] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8276
[2019-03-25 22:45:14,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 3167785.248012776 W.
[2019-03-25 22:45:14,507] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.83333333333334, 67.0, 1.0, 2.0, 0.8685175621061992, 1.0, 2.0, 0.754848820567362, 1.0, 2.0, 1.03, 7.005111023372939, 6.9112, 170.5573041426782, 3167785.248012776, 3100512.961914549, 579842.4557347631], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [34.66666666666667, 67.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.889350739777005, 6.9112, 170.5573041426782, 3610835.160351059, 2910145.996820624, 548034.8397221836], 
processed observation next is [1.0, 0.6956521739130435, 0.8420221169036337, 0.67, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09781507397770053, 0.0, 0.8375144448122397, 1.003009766764183, 0.8083738880057288, 0.8179624473465428], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7196864], dtype=float32), -0.9861323]. 
=============================================
[2019-03-25 22:45:17,465] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.00000000e+00 2.19453472e-20 1.21820319e-21 1.45390965e-11
 3.73590136e-23], sum to 1.0000
[2019-03-25 22:45:17,475] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2921
[2019-03-25 22:45:17,483] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1206318.510462655 W.
[2019-03-25 22:45:17,489] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.4315436344483379, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7488479172492444, 6.911199999999999, 6.9112, 168.912956510431, 1206318.510462655, 1206318.510462656, 273614.327405342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4161000.0000, 
sim time next is 4161600.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.8422175690988795, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1177133.210844192, 1177133.210844192, 254540.2624695754], 
processed observation next is [1.0, 0.17391304347826086, 0.5260663507109005, 0.89, 1.0, 1.0, 0.8099006856613006, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32698144745672, 0.32698144745672, 0.37991083950682897], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.12424569], dtype=float32), -0.30835655]. 
=============================================
[2019-03-25 22:45:17,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4527646e-07 7.6716882e-21 1.3295152e-26 9.9999940e-01 0.0000000e+00], sum to 1.0000
[2019-03-25 22:45:17,948] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9349
[2019-03-25 22:45:17,952] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.83333333333333, 71.0, 1.0, 2.0, 0.8937779751824754, 1.0, 2.0, 0.8937779751824754, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2499874.278962036, 2499874.278962036, 468121.839409286], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4103400.0000, 
sim time next is 4104000.0000, 
raw observation next is [33.0, 71.0, 1.0, 2.0, 0.9445704593884453, 1.0, 2.0, 0.9445704593884453, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2642089.772626481, 2642089.772626481, 496404.3481420618], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.71, 1.0, 1.0, 0.9332174209499341, 1.0, 1.0, 0.9332174209499341, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7339138257295781, 0.7339138257295781, 0.740902012152331], 
reward next is 0.2591, 
noisyNet noise sample is [array([-0.47866127], dtype=float32), 0.5995193]. 
=============================================
[2019-03-25 22:45:17,979] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[22.482477]
 [22.503534]
 [21.896347]
 [21.360794]
 [21.048447]], R is [[22.18445206]
 [22.26391983]
 [22.34313011]
 [22.38918114]
 [22.44863701]].
[2019-03-25 22:45:18,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.8199179e-17 1.2555368e-20 8.7583690e-11 2.5802687e-18], sum to 1.0000
[2019-03-25 22:45:18,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3298
[2019-03-25 22:45:18,301] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9946034185591279, 6.9112, 6.9112, 168.912956510431, 801462.3948753437, 801462.3948753437, 245650.9102559812], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [28.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.005687862224942, 6.911200000000001, 6.9112, 168.9128271770838, 811118.6161196782, 811118.6161196776, 248547.4347535466], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0069364173474904, 8.881784197001253e-17, 0.0, 0.829439310066359, 0.22531072669991062, 0.22531072669991042, 0.3709663205276815], 
reward next is 0.6290, 
noisyNet noise sample is [array([-2.6867008], dtype=float32), -0.44119424]. 
=============================================
[2019-03-25 22:45:19,140] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.0579530e-20 4.9083422e-27 6.2769555e-12 5.9259656e-28], sum to 1.0000
[2019-03-25 22:45:19,148] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8459
[2019-03-25 22:45:19,153] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.922458296665548, 6.9112, 168.9127186634017, 836791.0447736374, 828804.0328849908, 254811.9306266711], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4143600.0000, 
sim time next is 4144200.0000, 
raw observation next is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.962813450864135, 6.9112, 168.9124740357907, 865431.4573705362, 828815.2035187959, 254811.9356868308], 
processed observation next is [1.0, 1.0, 0.5734597156398105, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005161345086413504, 0.0, 0.8294375759809409, 0.24039762704737117, 0.23022644542188775, 0.380316321920643], 
reward next is 0.3616, 
noisyNet noise sample is [array([-1.7228383], dtype=float32), -0.44730443]. 
=============================================
[2019-03-25 22:45:20,125] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.6146847e-18 4.3065524e-20 9.2728985e-16 3.5022549e-29], sum to 1.0000
[2019-03-25 22:45:20,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2246
[2019-03-25 22:45:20,141] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 894882.8118537148 W.
[2019-03-25 22:45:20,147] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 76.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.004311336613021, 6.9112, 168.9122771807279, 894882.8118537148, 828826.6906793991, 254812.4440673252], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4133400.0000, 
sim time next is 4134000.0000, 
raw observation next is [30.33333333333334, 78.0, 1.0, 1.0, 0.2013944232872693, 1.0, 1.0, 0.2013944232872693, 1.0, 2.0, 0.349755511096465, 6.911199999999999, 6.9112, 170.5573041426782, 844306.872291464, 844306.8722914647, 270230.8475169392], 
processed observation next is [1.0, 0.8695652173913043, 0.6366508688783573, 0.78, 1.0, 0.5, 0.037824606370203975, 1.0, 0.5, 0.037824606370203975, 1.0, 1.0, 0.2070189159712988, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2345296867476289, 0.23452968674762908, 0.4033296231596108], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.44310096], dtype=float32), -1.1066749]. 
=============================================
[2019-03-25 22:45:20,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[18.467014]
 [17.753244]
 [17.564182]
 [17.164978]
 [17.558046]], R is [[18.47533798]
 [18.29058456]
 [18.10767937]
 [17.92660332]
 [17.74733734]].
[2019-03-25 22:45:22,850] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9999988e-01 2.0738961e-19 1.6572123e-14 7.0466776e-08 6.5486831e-24], sum to 1.0000
[2019-03-25 22:45:22,860] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2714
[2019-03-25 22:45:22,866] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1107736.140559789 W.
[2019-03-25 22:45:22,871] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 86.5, 1.0, 2.0, 0.264198362499621, 1.0, 2.0, 0.264198362499621, 1.0, 1.0, 0.4588251839282447, 6.9112, 6.9112, 170.5573041426782, 1107736.140559789, 1107736.140559789, 290464.2935286884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4170600.0000, 
sim time next is 4171200.0000, 
raw observation next is [29.33333333333334, 85.66666666666667, 1.0, 2.0, 0.8196780425539479, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1145613.637609731, 1145613.637609731, 248803.3406672745], 
processed observation next is [1.0, 0.2608695652173913, 0.5892575039494474, 0.8566666666666667, 1.0, 1.0, 0.7827446295830698, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3182260104471475, 0.3182260104471475, 0.3713482696526485], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5555618], dtype=float32), -1.5809863]. 
=============================================
[2019-03-25 22:45:26,465] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-25 22:45:26,468] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:45:26,470] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:45:26,471] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:45:26,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:45:26,473] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:45:26,472] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:45:26,473] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:45:26,474] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:45:26,473] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:45:26,479] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:45:26,495] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-25 22:45:26,496] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-25 22:45:26,496] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-25 22:45:26,531] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-25 22:45:26,567] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-25 22:45:43,119] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:45:43,121] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.36666666666667, 93.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5425150387514026, 6.9112, 6.9112, 168.912956510431, 478156.972780908, 478156.972780908, 157626.754984747]
[2019-03-25 22:45:43,121] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:45:43,124] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 3.4581264e-29 4.0145197e-33 7.9590309e-21 3.7389922e-32], sampled 0.9033423982117076
[2019-03-25 22:46:09,257] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:09,259] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.40018895833333, 85.51529978833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5846109981579157, 6.911200000000001, 6.9112, 168.912956510431, 512869.9409943574, 512869.9409943568, 163679.9686084534]
[2019-03-25 22:46:09,260] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:46:09,264] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 4.3314913e-30 1.5766610e-33 8.6115770e-21 6.1224595e-34], sampled 0.5278084783706419
[2019-03-25 22:46:10,099] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:10,100] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.33333333333334, 75.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.9316396058215, 6.9112, 168.9126414869824, 843879.2060175647, 829378.6732144632, 254846.0313562987]
[2019-03-25 22:46:10,100] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:46:10,103] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 4.3743131e-31 3.3037517e-34 2.3077675e-23 7.0264159e-37], sampled 0.9739865567933025
[2019-03-25 22:46:22,354] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:22,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.16666666666666, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8668855947703665, 6.9112, 6.9112, 168.912956510431, 717572.6380960967, 717572.6380960967, 215952.7860381386]
[2019-03-25 22:46:22,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:46:22,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.1554564e-33 3.1686039e-35 2.9726116e-27 1.1372118e-35], sampled 0.39052689205692814
[2019-03-25 22:46:24,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:24,617] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.46666666666667, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.939847170401467, 6.911200000000001, 6.9112, 168.912956510431, 768840.7909451592, 768840.7909451586, 232607.5649640441]
[2019-03-25 22:46:24,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:46:24,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.9442931e-29 8.5450390e-31 7.5865612e-23 1.3999884e-30], sampled 0.2520680447262289
[2019-03-25 22:46:25,359] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:25,360] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.77938646, 67.09598423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7358917321655202, 6.9112, 6.9112, 168.912956510431, 628678.7119894107, 628678.7119894107, 189337.3494346389]
[2019-03-25 22:46:25,360] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:46:25,364] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.9204935e-32 3.4058798e-35 1.1524973e-27 2.3076025e-36], sampled 0.21419998553851838
[2019-03-25 22:46:33,000] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:33,000] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.84591635666667, 79.13994237, 1.0, 2.0, 0.5882583527697042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104181, 822047.373303645, 822047.3733036458, 198578.5580829493]
[2019-03-25 22:46:33,002] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:46:33,004] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.3615714e-34 1.4510593e-33 4.0879481e-22 2.7034004e-38], sampled 0.8795064746255263
[2019-03-25 22:46:59,645] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:46:59,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.73333333333333, 59.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9688129566471918, 6.9112, 6.9112, 168.912956510431, 788583.1719508063, 788583.1719508063, 239554.9999701288]
[2019-03-25 22:46:59,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:46:59,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 9.0114132e-34 7.4468404e-37 2.3417106e-26 0.0000000e+00], sampled 0.8772766147578579
[2019-03-25 22:47:00,019] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:47:00,022] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.47738507333334, 49.88997691500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.007395120711082, 6.9112, 6.9112, 168.9128821161397, 811380.7259266677, 811380.7259266677, 248925.5661264654]
[2019-03-25 22:47:00,025] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:47:00,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 6.4494519e-29 3.3064926e-31 8.9786055e-20 8.6705187e-35], sampled 0.9692703816589873
[2019-03-25 22:47:23,316] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39459512], dtype=float32), 0.11018282]
[2019-03-25 22:47:23,317] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.55711988, 81.93601132666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6423558830831557, 6.9112, 6.9112, 168.912956510431, 557109.1543462904, 557109.1543462904, 172800.7748041172]
[2019-03-25 22:47:23,318] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:47:23,321] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.2426166e-29 2.6597646e-29 2.8629510e-23 4.8209039e-27], sampled 0.6161010984563936
[2019-03-25 22:47:35,107] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:47:35,425] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7349.1146 3105598293.1538 2010.0000
[2019-03-25 22:47:35,624] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7924.5381 2989430626.8423 1566.0000
[2019-03-25 22:47:35,773] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.2826 3185062665.2130 2464.0000
[2019-03-25 22:47:35,840] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-03-25 22:47:36,856] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 325000, evaluation results [325000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7349.114573366201, 3105598293.153783, 2010.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7031.282625179972, 3185062665.2130423, 2464.0, 7924.538052060808, 2989430626.842283, 1566.0]
[2019-03-25 22:47:47,665] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 7.801494e-27 8.052972e-36 7.695699e-21 0.000000e+00], sum to 1.0000
[2019-03-25 22:47:47,673] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4898
[2019-03-25 22:47:47,678] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8914596642414144, 6.911199999999999, 6.9112, 168.912956510431, 733315.7759126496, 733315.7759126503, 221356.29693643], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4392000.0000, 
sim time next is 4392600.0000, 
raw observation next is [31.0, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9056472812497036, 6.911199999999999, 6.9112, 168.912956510431, 743665.8485963726, 743665.8485963732, 224590.8492573557], 
processed observation next is [1.0, 0.8695652173913043, 0.6682464454976303, 0.6566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.884935708841102, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20657384683232574, 0.2065738468323259, 0.33521022277217266], 
reward next is 0.6648, 
noisyNet noise sample is [array([-1.5520332], dtype=float32), 0.9827333]. 
=============================================
[2019-03-25 22:47:51,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.0168265e-28 1.9226766e-35 5.3366979e-21 2.5717338e-37], sum to 1.0000
[2019-03-25 22:47:51,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7337
[2019-03-25 22:47:51,646] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.920918584421718, 6.9112, 168.9127071221563, 835698.2948231729, 828803.6067260893, 254811.926671996], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4425600.0000, 
sim time next is 4426200.0000, 
raw observation next is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.935048907071021, 6.9112, 168.9126179309013, 845726.7196606703, 828807.518082411, 254811.9278067688], 
processed observation next is [0.0, 0.21739130434782608, 0.5734597156398105, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0023848907071021054, 0.0, 0.8294382825718094, 0.23492408879463064, 0.2302243105784475, 0.3803163101593564], 
reward next is 0.5004, 
noisyNet noise sample is [array([-1.5057122], dtype=float32), 0.67409104]. 
=============================================
[2019-03-25 22:48:03,047] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.3643510e-33 1.0498661e-33 2.1604281e-36 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:03,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3686
[2019-03-25 22:48:03,070] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2950704.344206705 W.
[2019-03-25 22:48:03,075] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666667, 64.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.85058523710522, 6.9112, 168.907756217859, 2950704.344206705, 2284292.643187483, 473947.4108149706], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4625400.0000, 
sim time next is 4626000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6397648900842624, 1.0, 1.0, 0.6397648900842624, 1.0, 2.0, 1.03, 7.002329843128023, 6.9112, 170.5573041426782, 2684307.112912563, 2619027.099354948, 502848.0357666226], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5659817952822438, 1.0, 0.5, 0.5659817952822438, 1.0, 1.0, 1.0365853658536586, 0.009112984312802297, 0.0, 0.8375144448122397, 0.7456408646979342, 0.7275075275985966, 0.7505194563680934], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.51064473], dtype=float32), 0.06420269]. 
=============================================
[2019-03-25 22:48:03,095] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[28.484615]
 [29.176468]
 [29.039827]
 [28.559757]
 [27.942255]], R is [[28.28510475]
 [28.00225449]
 [27.72223282]
 [27.44501114]
 [27.17056084]].
[2019-03-25 22:48:10,993] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.7638022e-20 1.5187650e-20 4.5299186e-15 7.1949218e-26], sum to 1.0000
[2019-03-25 22:48:11,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7628
[2019-03-25 22:48:11,013] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1255684.566006308 W.
[2019-03-25 22:48:11,018] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.2994638502689379, 1.0, 2.0, 0.2994638502689379, 1.0, 1.0, 0.5179432355619166, 6.9112, 6.9112, 170.5573041426782, 1255684.566006308, 1255684.566006308, 304015.2966851695], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4693800.0000, 
sim time next is 4694400.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5079444843004224, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8747745518242763, 6.911200000000001, 6.9112, 168.912956510431, 1420028.989437727, 1420028.989437726, 311135.0460573349], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4071620292776173, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.8472860388100929, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3944524970660353, 0.394452497066035, 0.4643806657572163], 
reward next is 0.5356, 
noisyNet noise sample is [array([0.2426625], dtype=float32), 0.23611943]. 
=============================================
[2019-03-25 22:48:12,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 6.8210861e-27 2.4678329e-28 1.0123212e-21 2.9805265e-34], sum to 1.0000
[2019-03-25 22:48:12,541] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5600
[2019-03-25 22:48:12,549] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2226959.390835219 W.
[2019-03-25 22:48:12,554] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 70.83333333333333, 1.0, 2.0, 0.9513857302852109, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.99089821641077, 6.9112, 168.9124820908899, 2226959.390835219, 2170418.891916147, 449268.9723140724], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4783800.0000, 
sim time next is 4784400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.9442285030681543, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986759456412224, 6.9112, 168.912442030748, 2216942.141966355, 2163337.826301716, 447324.1265109101], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.7, 1.0, 1.0, 0.9328054253833185, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007555945641222373, 0.0, 0.8294374188215293, 0.6158172616573209, 0.6009271739726988, 0.6676479500162837], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0398602], dtype=float32), 0.41795903]. 
=============================================
[2019-03-25 22:48:23,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 2.0403286e-32 5.7168945e-33 2.1206895e-19 7.6411725e-36], sum to 1.0000
[2019-03-25 22:48:23,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9690
[2019-03-25 22:48:23,614] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 936508.2514518601 W.
[2019-03-25 22:48:23,623] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666666, 82.33333333333334, 1.0, 2.0, 0.3350653178187609, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5693893354299391, 6.9112, 6.9112, 168.912956510431, 936508.2514518601, 936508.2514518601, 232428.9770206418], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4866000.0000, 
sim time next is 4866600.0000, 
raw observation next is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.3341818148168005, 1.0, 1.0, 0.3341818148168005, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 934033.8129871718, 934033.8129871718, 256817.7235798303], 
processed observation next is [1.0, 0.30434782608695654, 0.5181674565560824, 0.8066666666666668, 1.0, 1.0, 0.1978094154419283, 1.0, 0.5, 0.1978094154419283, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.25945383694088103, 0.25945383694088103, 0.38331003519377654], 
reward next is 0.6167, 
noisyNet noise sample is [array([1.1012594], dtype=float32), -0.7357536]. 
=============================================
[2019-03-25 22:48:33,254] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5752165e-20 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:33,263] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6095
[2019-03-25 22:48:33,270] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8333827131185261, 6.911199999999999, 6.9112, 168.912956510431, 693679.6201163708, 693679.6201163714, 208734.3486673482], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5029800.0000, 
sim time next is 5030400.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8354866880870464, 6.911199999999999, 6.9112, 168.912956510431, 695163.3212670541, 695163.3212670546, 209178.9506297195], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7993740098622517, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19310092257418168, 0.19310092257418185, 0.31220738899958134], 
reward next is 0.6878, 
noisyNet noise sample is [array([-1.9708974], dtype=float32), -0.15596561]. 
=============================================
[2019-03-25 22:48:35,054] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3394635e-33 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:35,062] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3629
[2019-03-25 22:48:35,072] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.5, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8148843037402328, 6.9112, 6.9112, 168.912956510431, 681416.4922489817, 681416.4922489817, 204891.3629048023], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5023800.0000, 
sim time next is 5024400.0000, 
raw observation next is [25.33333333333333, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8068551399197265, 6.9112, 6.9112, 168.912956510431, 676195.8533694813, 676195.8533694813, 203249.8872447761], 
processed observation next is [0.0, 0.13043478260869565, 0.3996840442338071, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7644574877069834, 0.0, 0.0, 0.8294399451523027, 0.1878321814915226, 0.1878321814915226, 0.3033580406638449], 
reward next is 0.6966, 
noisyNet noise sample is [array([-0.61518043], dtype=float32), -0.5562625]. 
=============================================
[2019-03-25 22:48:36,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3367018e-27 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:36,575] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8618
[2019-03-25 22:48:36,584] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333333, 84.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8568108960277343, 6.9112, 6.9112, 168.912956510431, 710784.0142643537, 710784.0142643537, 213765.6612260253], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5035800.0000, 
sim time next is 5036400.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8589067363317153, 6.9112, 6.9112, 168.912956510431, 712074.1252579205, 712074.1252579205, 214214.7095895658], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.82793504430697, 0.0, 0.0, 0.8294399451523027, 0.19779836812720014, 0.19779836812720014, 0.3197234471486057], 
reward next is 0.6803, 
noisyNet noise sample is [array([-0.31905222], dtype=float32), 0.50924116]. 
=============================================
[2019-03-25 22:48:36,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.072213e-21 0.000000e+00], sum to 1.0000
[2019-03-25 22:48:36,830] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6066
[2019-03-25 22:48:36,834] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.66666666666667, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8778176539773705, 6.9112, 6.9112, 168.912956510431, 725322.8092902369, 725322.8092902369, 218366.9560968455], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5128800.0000, 
sim time next is 5129400.0000, 
raw observation next is [29.83333333333333, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8740858604737449, 6.911200000000001, 6.9112, 168.912956510431, 722969.9774023629, 722969.9774023623, 217549.7710056056], 
processed observation next is [0.0, 0.34782608695652173, 0.6129541864139019, 0.6733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.846446171309445, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20082499372287857, 0.2008249937228784, 0.32470115075463524], 
reward next is 0.6753, 
noisyNet noise sample is [array([0.15394744], dtype=float32), 0.41354746]. 
=============================================
[2019-03-25 22:48:37,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.5359056e-28 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:37,867] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2219
[2019-03-25 22:48:37,874] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.5, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9030987149704129, 6.911200000000001, 6.9112, 168.912956510431, 740496.5441567816, 740496.544156781, 223951.5616734846], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5056200.0000, 
sim time next is 5056800.0000, 
raw observation next is [31.66666666666666, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9132556991737921, 6.911199999999999, 6.9112, 168.912956510431, 747116.4357692245, 747116.4357692251, 226255.6375148273], 
processed observation next is [0.0, 0.5217391304347826, 0.6998420221169034, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8942142672851123, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20753234326922904, 0.2075323432692292, 0.33769498136541387], 
reward next is 0.6623, 
noisyNet noise sample is [array([1.7604712], dtype=float32), 0.7288173]. 
=============================================
[2019-03-25 22:48:41,765] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2635317e-35 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:41,784] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8100
[2019-03-25 22:48:41,792] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7995927774533711, 6.9112, 6.9112, 168.912956510431, 671654.9019920448, 671654.9019920448, 201781.2239054657], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5114400.0000, 
sim time next is 5115000.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7998798772523904, 6.9112, 6.9112, 168.912956510431, 671896.143399213, 671896.143399213, 201840.364077364], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7559510698199882, 0.0, 0.0, 0.8294399451523027, 0.1866378176108925, 0.1866378176108925, 0.30125427474233435], 
reward next is 0.6987, 
noisyNet noise sample is [array([-2.203037], dtype=float32), 0.92160314]. 
=============================================
[2019-03-25 22:48:41,814] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.19156 ]
 [73.116196]
 [72.99532 ]
 [72.89771 ]
 [72.80271 ]], R is [[73.10433197]
 [73.0721283 ]
 [73.03979492]
 [73.00730896]
 [72.97480774]].
[2019-03-25 22:48:44,233] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.5245503e-30 4.6517815e-38 3.1275124e-26 0.0000000e+00], sum to 1.0000
[2019-03-25 22:48:44,251] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6112
[2019-03-25 22:48:44,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2557893.495067439 W.
[2019-03-25 22:48:44,268] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 68.5, 1.0, 2.0, 0.9145003264141336, 1.0, 2.0, 0.9145003264141336, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2557893.495067439, 2557893.495067439, 479465.3685476836], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5225400.0000, 
sim time next is 5226000.0000, 
raw observation next is [31.66666666666666, 68.0, 1.0, 2.0, 0.5998523262571496, 1.0, 2.0, 0.5998523262571496, 1.0, 1.0, 1.03, 6.924403286009412, 6.9112, 170.5573041426782, 2516674.466550206, 2507216.415706328, 487924.7390073623], 
processed observation next is [1.0, 0.4782608695652174, 0.6998420221169034, 0.68, 1.0, 1.0, 0.5178943689845176, 1.0, 1.0, 0.5178943689845176, 1.0, 0.5, 1.0365853658536586, 0.0013203286009411564, 0.0, 0.8375144448122397, 0.6990762407083905, 0.6964490043628689, 0.7282458791154661], 
reward next is 0.2057, 
noisyNet noise sample is [array([-2.1976929], dtype=float32), -1.8683091]. 
=============================================
[2019-03-25 22:48:44,283] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[44.859566]
 [44.46342 ]
 [44.79055 ]
 [44.053146]
 [43.856785]], R is [[44.65881348]
 [44.49660873]
 [44.33755493]
 [44.07533264]
 [43.92038345]].
[2019-03-25 22:48:44,357] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-25 22:48:44,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:48:44,364] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:48:44,366] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:48:44,365] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:48:44,370] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:48:44,366] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:48:44,371] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:48:44,368] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:48:44,371] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:48:44,374] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:48:44,391] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-25 22:48:44,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-25 22:48:44,426] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-25 22:48:44,427] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-25 22:48:44,465] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-25 22:49:08,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:49:08,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 71.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5146793225023394, 6.911199999999999, 6.9112, 168.912956510431, 456827.1140208881, 456827.1140208887, 153802.1630951404]
[2019-03-25 22:49:08,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:49:08,531] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8151501232450238
[2019-03-25 22:49:15,189] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:49:15,189] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.2935136, 83.22477437500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6480461434522282, 6.9112, 6.9112, 168.912956510431, 566661.5076732046, 566661.5076732046, 173647.4149627673]
[2019-03-25 22:49:15,193] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:49:15,196] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7429350e-34 1.0932944e-37], sampled 0.205821710474005
[2019-03-25 22:49:27,014] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:49:27,015] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.54972948666667, 87.94456775666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6620381613668815, 6.911200000000001, 6.9112, 168.912956510431, 571426.7632215112, 571426.7632215106, 176102.8194460901]
[2019-03-25 22:49:27,017] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:49:27,022] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7036847e-33 0.0000000e+00], sampled 0.12700411592164018
[2019-03-25 22:49:35,174] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:49:35,175] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.78719751333334, 96.10649188000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6690803806910955, 6.9112, 6.9112, 168.912956510431, 573501.9084356179, 573501.9084356179, 177315.4208865817]
[2019-03-25 22:49:35,176] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:49:35,179] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.8055496e-32 3.8405102e-35], sampled 0.4022192329386667
[2019-03-25 22:49:35,519] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:49:35,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.86666666666667, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004946637029282, 6.911200000000001, 6.9112, 168.9128521936853, 809231.8369004052, 809231.8369004047, 248280.8921104002]
[2019-03-25 22:49:35,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:49:35,526] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.6964393e-36 0.0000000e+00], sampled 0.5636348456003245
[2019-03-25 22:50:02,501] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:50:02,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.27450064, 74.727588645, 1.0, 2.0, 0.8199490404379145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1234083.831061452, 1234083.831061453, 260880.0472474552]
[2019-03-25 22:50:02,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:50:02,505] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1843327e-27 0.0000000e+00], sampled 0.4668815007063948
[2019-03-25 22:50:02,506] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1234083.831061452 W.
[2019-03-25 22:50:16,082] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:50:16,082] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 83.83333333333333, 1.0, 2.0, 0.8218278061106886, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005982891553151, 6.9112, 168.9123159942768, 2045622.715185811, 1978380.724724527, 413294.2172145923]
[2019-03-25 22:50:16,084] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:50:16,089] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 3.617038e-31 0.000000e+00], sampled 0.3737703047225299
[2019-03-25 22:50:16,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2045622.715185811 W.
[2019-03-25 22:50:19,843] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:50:19,845] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.38824155833333, 73.30023486, 1.0, 2.0, 0.8529890903936531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1192196.579689263, 1192196.579689263, 257343.5142680616]
[2019-03-25 22:50:19,848] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:50:19,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 1.5154555e-34 0.0000000e+00 2.7433657e-23 3.6974829e-36], sampled 0.5288649641325173
[2019-03-25 22:50:19,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1192196.579689263 W.
[2019-03-25 22:50:33,672] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39438388], dtype=float32), 0.15096964]
[2019-03-25 22:50:33,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.03053714, 91.9066411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8527541081960992, 6.911200000000001, 6.9112, 168.912956510431, 710150.9125015006, 710150.9125014999, 212954.5578376667]
[2019-03-25 22:50:33,678] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:50:33,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 2.052256e-32 0.000000e+00], sampled 0.6488659352620494
[2019-03-25 22:50:52,550] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.3569 2989248936.3710 1566.0000
[2019-03-25 22:50:53,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.7511 3319453916.2368 2143.0000
[2019-03-25 22:50:53,610] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8062.3661 2937831822.1909 1381.0000
[2019-03-25 22:50:53,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.3407 3185096716.9286 2464.0000
[2019-03-25 22:50:53,778] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.7831 3105820370.8268 2010.0000
[2019-03-25 22:50:54,796] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 350000, evaluation results [350000.0, 7289.751126213786, 3319453916.236812, 2143.0, 7348.783114152705, 3105820370.826825, 2010.0, 8062.366084066912, 2937831822.1909375, 1381.0, 7031.340658675213, 3185096716.9285784, 2464.0, 7925.35686221895, 2989248936.3710446, 1566.0]
[2019-03-25 22:50:55,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:50:55,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1686
[2019-03-25 22:50:55,566] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9272235978710114, 6.9112, 6.9112, 168.912956510431, 757327.4045932451, 757327.4045932451, 229514.1502672735], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5158800.0000, 
sim time next is 5159400.0000, 
raw observation next is [31.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9412008810683581, 6.9112, 6.9112, 168.912956510431, 769182.6136442873, 769182.6136442873, 232901.2700750477], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9282937574004367, 0.0, 0.0, 0.8294399451523027, 0.21366183712341316, 0.21366183712341316, 0.347613835932907], 
reward next is 0.6524, 
noisyNet noise sample is [array([0.23874414], dtype=float32), 1.07111]. 
=============================================
[2019-03-25 22:50:56,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:50:56,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6499
[2019-03-25 22:50:56,496] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8840592366142372, 6.9112, 6.9112, 168.912956510431, 729545.6058289391, 729545.6058289391, 219751.2202989896], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5169000.0000, 
sim time next is 5169600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8822651661991745, 6.911200000000001, 6.9112, 168.912956510431, 728400.2327483637, 728400.232748363, 219354.8886893599], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8564209343892372, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2023333979856566, 0.20233339798565642, 0.327395356252776], 
reward next is 0.6726, 
noisyNet noise sample is [array([-1.1198854], dtype=float32), 1.3488855]. 
=============================================
[2019-03-25 22:51:02,265] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9947041e-01 2.6696900e-12 1.6505818e-11 2.6173716e-14 5.2957510e-04], sum to 1.0000
[2019-03-25 22:51:02,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1397
[2019-03-25 22:51:02,297] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2831253.999624919 W.
[2019-03-25 22:51:02,303] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.06666666666666, 52.5, 1.0, 2.0, 1.012121797323297, 1.0, 2.0, 1.012121797323297, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2831253.999624919, 2831253.999624919, 536358.7952125545], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5309400.0000, 
sim time next is 5310000.0000, 
raw observation next is [36.4, 51.0, 1.0, 2.0, 0.7279892335483166, 1.0, 2.0, 0.6845846562884209, 1.0, 1.0, 1.03, 7.005099939184792, 6.9112, 170.5573041426782, 2872576.809078163, 2805312.463034781, 530315.7023952781], 
processed observation next is [1.0, 0.4782608695652174, 0.924170616113744, 0.51, 1.0, 1.0, 0.6722761849979717, 1.0, 1.0, 0.6199815136005071, 1.0, 0.5, 1.0365853658536586, 0.009389993918479167, 0.0, 0.8375144448122397, 0.797938002521712, 0.7792534619541058, 0.7915159737242957], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29736567], dtype=float32), 1.4338797]. 
=============================================
[2019-03-25 22:51:02,320] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[24.2807  ]
 [24.344353]
 [24.935534]
 [24.940607]
 [25.099255]], R is [[23.85976601]
 [23.82063293]
 [23.79682541]
 [23.8163929 ]
 [23.5782299 ]].
[2019-03-25 22:51:04,955] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.1289008e-31 5.5172264e-28 3.7368989e-38 6.7477812e-38], sum to 1.0000
[2019-03-25 22:51:04,963] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0697
[2019-03-25 22:51:04,970] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3321296.881701742 W.
[2019-03-25 22:51:04,975] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.485630353435785, 6.9112, 170.5573041426782, 3321296.881701742, 2909809.053202862, 550520.6831875474], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5323200.0000, 
sim time next is 5323800.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 0.8484203789676689, 1.0, 2.0, 0.7448002289980971, 1.0, 1.0, 1.03, 7.005109437884374, 6.9112, 170.5573041426782, 3125562.736455986, 3058291.586107711, 572306.8801649959], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 0.8173739505634564, 1.0, 1.0, 0.6925303963832494, 1.0, 0.5, 1.0365853658536586, 0.009390943788437411, 0.0, 0.8375144448122397, 0.8682118712377739, 0.8495254405854752, 0.8541893733805909], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6307589], dtype=float32), 0.7043252]. 
=============================================
[2019-03-25 22:51:07,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 6.2688072e-16 2.5945051e-15 3.5656870e-26 3.4282996e-09], sum to 1.0000
[2019-03-25 22:51:07,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7567
[2019-03-25 22:51:07,717] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3374298.669182693 W.
[2019-03-25 22:51:07,723] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.55953389591921, 6.9112, 170.5573041426782, 3374298.669182693, 2909870.726999032, 550082.7449380731], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5324400.0000, 
sim time next is 5325000.0000, 
raw observation next is [36.08333333333334, 53.0, 1.0, 2.0, 0.9101517773270099, 1.0, 2.0, 0.7756659281777674, 1.0, 1.0, 1.03, 7.005114308282229, 6.9112, 170.5573041426782, 3255259.816281796, 3187985.177069422, 595927.5729133863], 
processed observation next is [1.0, 0.6521739130434783, 0.9091627172195897, 0.53, 1.0, 1.0, 0.8917491293096504, 1.0, 1.0, 0.7297179857563462, 1.0, 0.5, 1.0365853658536586, 0.00939143082822289, 0.0, 0.8375144448122397, 0.9042388378560544, 0.8855514380748394, 0.8894441386766959], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9721998], dtype=float32), 0.97004575]. 
=============================================
[2019-03-25 22:51:07,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[-6.128525 ]
 [-6.014719 ]
 [-6.37146  ]
 [-6.300206 ]
 [-6.6803217]], R is [[-6.19875336]
 [-6.13676596]
 [-6.07539845]
 [-6.01464462]
 [-5.95449829]].
[2019-03-25 22:51:28,855] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:51:28,870] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4231
[2019-03-25 22:51:28,880] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.66666666666667, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8511840965991883, 6.911199999999999, 6.9112, 168.912956510431, 707460.1998255189, 707460.1998255195, 212568.8109650466], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5628000.0000, 
sim time next is 5628600.0000, 
raw observation next is [25.65, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8489647129758837, 6.9112, 6.9112, 168.912956510431, 706001.12753763, 706001.12753763, 212094.009958967], 
processed observation next is [0.0, 0.13043478260869565, 0.41469194312796204, 0.915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.815810625580346, 0.0, 0.0, 0.8294399451523027, 0.19611142431600834, 0.19611142431600834, 0.3165582238193538], 
reward next is 0.6834, 
noisyNet noise sample is [array([-1.1279678], dtype=float32), -1.899151]. 
=============================================
[2019-03-25 22:51:31,215] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:51:31,230] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9360
[2019-03-25 22:51:31,238] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9149885720557194, 6.911199999999999, 6.9112, 168.912956510431, 750890.1393398107, 750890.1393398113, 226765.2079439591], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5691600.0000, 
sim time next is 5692200.0000, 
raw observation next is [27.41666666666667, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9141222436340499, 6.9112, 6.9112, 168.912956510431, 750568.1525656576, 750568.1525656576, 226577.0506439368], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690366, 0.8516666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8952710288220121, 0.0, 0.0, 0.8294399451523027, 0.20849115349046046, 0.20849115349046046, 0.338174702453637], 
reward next is 0.6618, 
noisyNet noise sample is [array([0.8653552], dtype=float32), 2.112981]. 
=============================================
[2019-03-25 22:51:33,735] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:51:33,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4721
[2019-03-25 22:51:33,756] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.910182883816196, 6.911199999999999, 6.9112, 168.912956510431, 747908.3730064328, 747908.3730064334, 225673.8274378209], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5692800.0000, 
sim time next is 5693400.0000, 
raw observation next is [27.25, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9069162683035442, 6.9112, 6.9112, 168.912956510431, 745850.2126275769, 745850.2126275769, 224933.5901464879], 
processed observation next is [0.0, 0.9130434782608695, 0.490521327014218, 0.855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8864832540287123, 0.0, 0.0, 0.8294399451523027, 0.20718061461877135, 0.20718061461877135, 0.3357217763380416], 
reward next is 0.6643, 
noisyNet noise sample is [array([0.6163597], dtype=float32), -1.8791208]. 
=============================================
[2019-03-25 22:51:34,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:51:34,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9658
[2019-03-25 22:51:34,750] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.6, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8738569901895517, 6.911200000000001, 6.9112, 168.912956510431, 723325.4809899459, 723325.4809899452, 217517.0607452387], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5698800.0000, 
sim time next is 5699400.0000, 
raw observation next is [26.58333333333334, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.873066827939338, 6.911199999999999, 6.9112, 168.912956510431, 722999.0630069259, 722999.0630069264, 217350.229204448], 
processed observation next is [0.0, 1.0, 0.45892575039494504, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8452034487065097, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2008330730574794, 0.20083307305747958, 0.32440332717081793], 
reward next is 0.6756, 
noisyNet noise sample is [array([-1.0124211], dtype=float32), 2.3089938]. 
=============================================
[2019-03-25 22:51:45,835] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9995315e-01 2.7501400e-17 1.0463600e-15 5.6849995e-28 4.6844067e-05], sum to 1.0000
[2019-03-25 22:51:45,846] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2981
[2019-03-25 22:51:45,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2399159.187081685 W.
[2019-03-25 22:51:45,863] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.05, 77.0, 1.0, 2.0, 0.5718692846464873, 1.0, 2.0, 0.5718692846464873, 1.0, 2.0, 0.993147827368592, 6.9112, 6.9112, 170.5573041426782, 2399159.187081685, 2399159.187081685, 468402.4397798836], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5931000.0000, 
sim time next is 5931600.0000, 
raw observation next is [30.1, 77.33333333333334, 1.0, 2.0, 1.027799757863122, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005994872317389, 6.9112, 168.912275243621, 2333913.457694378, 2266662.983923231, 471966.1229034478], 
processed observation next is [1.0, 0.6521739130434783, 0.6255924170616115, 0.7733333333333334, 1.0, 1.0, 1.033493684172436, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479487231738925, 0.0, 0.8294365998203803, 0.6483092938039939, 0.6296286066453419, 0.7044270491096235], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3501835], dtype=float32), -0.39554635]. 
=============================================
[2019-03-25 22:51:51,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999917e-01 5.6532353e-27 1.2502968e-21 1.0159749e-31 7.8664448e-07], sum to 1.0000
[2019-03-25 22:51:51,451] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3617
[2019-03-25 22:51:51,461] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2197884.436431714 W.
[2019-03-25 22:51:51,468] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.86666666666667, 75.83333333333333, 1.0, 2.0, 0.7859034339417998, 1.0, 1.0, 0.7859034339417998, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2197884.436431714, 2197884.436431714, 413069.3540034724], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5928600.0000, 
sim time next is 5929200.0000, 
raw observation next is [29.9, 76.0, 1.0, 2.0, 0.7943749821758279, 1.0, 2.0, 0.7943749821758279, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2221599.499557013, 2221599.499557013, 417144.6333463556], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.76, 1.0, 1.0, 0.7522590146696722, 1.0, 1.0, 0.7522590146696722, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6171109720991703, 0.6171109720991703, 0.6226039303676949], 
reward next is 0.3774, 
noisyNet noise sample is [array([1.0400686], dtype=float32), 2.1929145]. 
=============================================
[2019-03-25 22:51:52,570] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 1.8163901e-38 0.0000000e+00 0.0000000e+00 1.4641396e-22], sum to 1.0000
[2019-03-25 22:51:52,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5399
[2019-03-25 22:51:52,588] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.68333333333334, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.958746036473569, 6.911199999999999, 6.9112, 168.912956510431, 778481.0365817486, 778481.0365817493, 236956.8736005086], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5953800.0000, 
sim time next is 5954400.0000, 
raw observation next is [27.6, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9578920912473413, 6.9112, 6.9112, 168.912956510431, 777920.7148114762, 777920.7148114762, 236752.7639716079], 
processed observation next is [1.0, 0.9565217391304348, 0.5071090047393366, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9486488917650503, 0.0, 0.0, 0.8294399451523027, 0.21608908744763228, 0.21608908744763228, 0.35336233428598196], 
reward next is 0.6466, 
noisyNet noise sample is [array([0.81460744], dtype=float32), 0.4059783]. 
=============================================
[2019-03-25 22:51:53,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2959296e-01 1.2902276e-13 3.4589810e-15 9.9290398e-10 8.7040704e-01], sum to 1.0000
[2019-03-25 22:51:53,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3053
[2019-03-25 22:51:53,477] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.85, 78.33333333333334, 1.0, 2.0, 0.5690464606835497, 1.0, 2.0, 0.5690464606835497, 1.0, 2.0, 0.9882455156671188, 6.911199999999999, 6.9112, 170.5573041426782, 2387305.301419808, 2387305.301419809, 466176.5669345544], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5994600.0000, 
sim time next is 5995200.0000, 
raw observation next is [30.0, 77.66666666666667, 1.0, 2.0, 0.5802286518014494, 1.0, 2.0, 0.5802286518014494, 1.0, 2.0, 1.007665283631799, 6.9112, 6.9112, 170.5573041426782, 2434263.340087826, 2434263.340087826, 475058.4018910726], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.7766666666666667, 1.0, 1.0, 0.4942513877125897, 1.0, 1.0, 0.4942513877125897, 1.0, 1.0, 1.0093479068680475, 0.0, 0.0, 0.8375144448122397, 0.6761842611355072, 0.6761842611355072, 0.7090423908821979], 
reward next is 0.2910, 
noisyNet noise sample is [array([1.6407455], dtype=float32), -0.8425849]. 
=============================================
[2019-03-25 22:52:02,353] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-25 22:52:02,356] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:52:02,358] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:52:02,358] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:52:02,359] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:52:02,361] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:52:02,363] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:52:02,365] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:52:02,361] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:52:02,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:52:02,369] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:52:02,394] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-25 22:52:02,396] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-25 22:52:02,422] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-25 22:52:02,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-25 22:52:02,464] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-25 22:52:22,508] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:22,509] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.75, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5717941707784141, 6.9112, 6.9112, 168.912956510431, 508963.6290880655, 508963.6290880655, 161493.1876657699]
[2019-03-25 22:52:22,510] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:52:22,512] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7080516e-38 6.2694587e-29], sampled 0.35233039404040667
[2019-03-25 22:52:32,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:32,071] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5803609604421316, 6.9112, 6.9112, 168.912956510431, 507877.6350391586, 507877.6350391586, 163088.8029776613]
[2019-03-25 22:52:32,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:52:32,079] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.3819358e-35 1.5305192e-35 3.6890651e-32 5.1893540e-17], sampled 0.7957553604371737
[2019-03-25 22:52:45,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:45,983] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.3, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9152322892918993, 6.911199999999999, 6.9112, 168.912956510431, 750759.8169143841, 750759.8169143847, 226808.8945002586]
[2019-03-25 22:52:45,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:52:45,988] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5916220e-35 2.0312049e-31], sampled 0.7446522347216487
[2019-03-25 22:52:46,700] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:46,701] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.4, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8030803345870094, 6.911200000000001, 6.9112, 168.912956510431, 676143.9389056627, 676143.9389056622, 202531.8830187356]
[2019-03-25 22:52:46,702] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:52:46,704] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4737105e-31], sampled 0.6119753694819937
[2019-03-25 22:52:48,590] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:48,591] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.21164057333333, 82.79694832166666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5034168058421173, 6.9112, 6.9112, 168.912956510431, 446125.1257069691, 446125.1257069691, 152407.4694223842]
[2019-03-25 22:52:48,592] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:52:48,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.00000000e+00 1.37231807e-38 0.00000000e+00 4.34754411e-32
 1.02062375e-27], sampled 0.3455250313053304
[2019-03-25 22:52:57,887] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:52:57,889] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.54244577, 94.18979507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8655151372684451, 6.9112, 6.9112, 168.912956510431, 719988.4024117339, 719988.4024117339, 215761.1052885282]
[2019-03-25 22:52:57,890] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:52:57,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.705522e-26], sampled 0.7325225487797323
[2019-03-25 22:53:14,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:53:14,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.10202132666667, 85.04964319666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.928745527244396, 6.9112, 168.9126441070804, 844847.7468442488, 832400.3689680132, 255021.8273893642]
[2019-03-25 22:53:14,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:53:14,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 3.3995114e-32 1.4757183e-30 1.2004827e-23 5.4821222e-15], sampled 0.2958220537045465
[2019-03-25 22:53:20,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:53:20,424] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.37834132, 86.31332223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.918778547745781, 6.911200000000001, 6.9112, 168.912956510431, 753718.4693965663, 753718.4693965656, 227649.7726088318]
[2019-03-25 22:53:20,424] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:53:20,428] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.5819261e-38 7.1131475e-37 7.3548373e-36 4.8521752e-20], sampled 0.3331355531896524
[2019-03-25 22:54:02,014] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.39530993], dtype=float32), 0.041509688]
[2019-03-25 22:54:02,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.86687778, 57.87084898666667, 1.0, 2.0, 0.5953139772201944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 908274.2819185837, 908274.2819185843, 209228.9030302984]
[2019-03-25 22:54:02,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:54:02,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 3.63568554e-31 9.90590980e-25 1.11437663e-23
 1.13687175e-08], sampled 0.142242390335525
[2019-03-25 22:54:02,022] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 908274.2819185837 W.
[2019-03-25 22:54:11,072] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7329.4819 3326373372.5909 1976.0000
[2019-03-25 22:54:12,019] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8080.6469 2939670093.2142 1319.0000
[2019-03-25 22:54:12,020] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7049.8741 3187271944.6598 2418.0000
[2019-03-25 22:54:12,062] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7351.1796 3109081956.0994 1998.0000
[2019-03-25 22:54:12,091] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7947.9448 2989643717.4849 1480.0000
[2019-03-25 22:54:13,107] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 375000, evaluation results [375000.0, 7329.481947959019, 3326373372.590946, 1976.0, 7351.179637537147, 3109081956.0993896, 1998.0, 8080.646853693275, 2939670093.214154, 1319.0, 7049.874054476393, 3187271944.659761, 2418.0, 7947.94477436156, 2989643717.4849305, 1480.0]
[2019-03-25 22:54:16,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9999857e-01 1.4491461e-29 2.4518841e-30 1.5091518e-29 1.3905162e-06], sum to 1.0000
[2019-03-25 22:54:16,107] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6303
[2019-03-25 22:54:16,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2369847.413811213 W.
[2019-03-25 22:54:16,118] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.3, 70.5, 1.0, 2.0, 0.5648890794784422, 1.0, 2.0, 0.5648890794784422, 1.0, 1.0, 0.980882432512595, 6.9112, 6.9112, 170.5573041426782, 2369847.413811213, 2369847.413811213, 462887.9577384117], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6187800.0000, 
sim time next is 6188400.0000, 
raw observation next is [30.2, 71.0, 1.0, 2.0, 0.8257608599833262, 1.0, 2.0, 0.8257608599833262, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2309456.339578586, 2309456.339578586, 432605.4823045952], 
processed observation next is [1.0, 0.6521739130434783, 0.6303317535545023, 0.71, 1.0, 1.0, 0.7900733252811158, 1.0, 1.0, 0.7900733252811158, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6415156498829406, 0.6415156498829406, 0.6456798243352166], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04091837], dtype=float32), -1.3893831]. 
=============================================
[2019-03-25 22:54:24,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:54:24,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0370
[2019-03-25 22:54:24,849] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9097988088267673, 6.911200000000001, 6.9112, 168.912956510431, 747387.7525456371, 747387.7525456365, 225575.3221918042], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6314400.0000, 
sim time next is 6315000.0000, 
raw observation next is [27.26666666666667, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9083927436552462, 6.9112, 6.9112, 168.912956510431, 746274.5013215193, 746274.5013215193, 225247.3228605063], 
processed observation next is [0.0, 0.08695652173913043, 0.4913112164297, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8882838337259099, 0.0, 0.0, 0.8294399451523027, 0.20729847258931092, 0.20729847258931092, 0.3361900341201587], 
reward next is 0.6638, 
noisyNet noise sample is [array([-1.177429], dtype=float32), -0.18420133]. 
=============================================
[2019-03-25 22:54:24,867] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.28876 ]
 [63.311783]
 [63.574493]
 [63.880665]
 [64.16453 ]], R is [[63.70126343]
 [63.72756958]
 [63.75390625]
 [63.78038025]
 [63.80691528]].
[2019-03-25 22:54:25,908] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:54:25,921] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7464
[2019-03-25 22:54:25,926] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666666, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9209489992327021, 6.9112, 6.9112, 168.912956510431, 752494.217547921, 752494.217547921, 228033.0289417249], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6262800.0000, 
sim time next is 6263400.0000, 
raw observation next is [30.68333333333333, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9180854919714755, 6.911200000000001, 6.9112, 168.912956510431, 750647.4480241525, 750647.448024152, 227376.6256403496], 
processed observation next is [0.0, 0.4782608695652174, 0.6532385466034754, 0.6733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9001042585017992, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20851318000670904, 0.20851318000670888, 0.3393680979706711], 
reward next is 0.6606, 
noisyNet noise sample is [array([0.9036146], dtype=float32), 1.5752019]. 
=============================================
[2019-03-25 22:54:31,109] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:54:31,116] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6982
[2019-03-25 22:54:31,122] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.7, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8880686814403609, 6.911199999999999, 6.9112, 168.912956510431, 732384.012359855, 732384.0123598556, 220650.0593303106], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6325800.0000, 
sim time next is 6326400.0000, 
raw observation next is [26.8, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8884010013057568, 6.9112, 6.9112, 168.912956510431, 732628.1155994342, 732628.1155994342, 220725.0700548234], 
processed observation next is [0.0, 0.21739130434782608, 0.4691943127962086, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8639036601289718, 0.0, 0.0, 0.8294399451523027, 0.20350780988873174, 0.20350780988873174, 0.3294404030669006], 
reward next is 0.6706, 
noisyNet noise sample is [array([-0.22116815], dtype=float32), -0.03462865]. 
=============================================
[2019-03-25 22:54:33,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9999869e-01 5.2757720e-18 1.6432879e-15 7.2858190e-23 1.3622642e-06], sum to 1.0000
[2019-03-25 22:54:33,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8832
[2019-03-25 22:54:33,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2277781.086187609 W.
[2019-03-25 22:54:33,485] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.3, 72.33333333333334, 1.0, 2.0, 0.8144454827576759, 1.0, 2.0, 0.8144454827576759, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2277781.086187609, 2277781.086187609, 426958.9790262613], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6430200.0000, 
sim time next is 6430800.0000, 
raw observation next is [29.4, 71.66666666666667, 1.0, 2.0, 0.5460587057316156, 1.0, 2.0, 0.5460587057316156, 1.0, 1.0, 0.9387529793244233, 6.9112, 6.9112, 170.5573041426782, 2290776.999994835, 2290776.999994835, 446544.2466560999], 
processed observation next is [1.0, 0.43478260869565216, 0.5924170616113744, 0.7166666666666667, 1.0, 1.0, 0.4530827779898983, 1.0, 1.0, 0.4530827779898983, 1.0, 0.5, 0.9253085113712477, 0.0, 0.0, 0.8375144448122397, 0.6363269444430097, 0.6363269444430097, 0.6664839502329849], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5419466], dtype=float32), 1.1372414]. 
=============================================
[2019-03-25 22:54:33,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.8388479e-33 0.0000000e+00 5.0707856e-25], sum to 1.0000
[2019-03-25 22:54:33,674] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7220
[2019-03-25 22:54:33,687] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2129415.095383148 W.
[2019-03-25 22:54:33,693] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.507629986207242, 1.0, 2.0, 0.507629986207242, 1.0, 1.0, 0.8691328664254708, 6.9112, 6.9112, 170.5573041426782, 2129415.095383148, 2129415.095383148, 418158.5403507242], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6451200.0000, 
sim time next is 6451800.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.741709198807658, 1.0, 2.0, 0.741709198807658, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2074169.664888792, 2074169.664888792, 392484.8696560567], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.6888062636236842, 1.0, 1.0, 0.6888062636236842, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5761582402468867, 0.5761582402468867, 0.5857983129194876], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82148474], dtype=float32), 1.6106143]. 
=============================================
[2019-03-25 22:54:46,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 6.1809526e-37 4.8214656e-38 0.0000000e+00 9.6820808e-19], sum to 1.0000
[2019-03-25 22:54:46,095] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3650
[2019-03-25 22:54:46,104] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.1, 72.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8072915147501732, 6.9112, 6.9112, 168.912956510431, 675412.4353496789, 675412.4353496789, 203313.7146841624], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6551400.0000, 
sim time next is 6552000.0000, 
raw observation next is [28.0, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8066614532903812, 6.9112, 6.9112, 168.912956510431, 675018.9527168088, 675018.9527168088, 203186.0015319792], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7642212845004648, 0.0, 0.0, 0.8294399451523027, 0.187505264643558, 0.187505264643558, 0.3032626888537003], 
reward next is 0.6967, 
noisyNet noise sample is [array([0.15463282], dtype=float32), -0.7655148]. 
=============================================
[2019-03-25 22:54:46,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[56.872086]
 [57.068874]
 [57.024258]
 [57.31272 ]
 [57.34838 ]], R is [[56.88218689]
 [57.0099144 ]
 [57.13566971]
 [57.25881577]
 [57.37943268]].
[2019-03-25 22:54:48,680] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.20306957e-16 1.08103645e-35 4.93245141e-31 0.00000000e+00
 1.00000000e+00], sum to 1.0000
[2019-03-25 22:54:48,690] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9907
[2019-03-25 22:54:48,699] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.9, 85.5, 1.0, 2.0, 0.1713328416499347, 1.0, 2.0, 0.1713328416499347, 1.0, 2.0, 0.2910058036958589, 6.911199999999999, 6.9112, 170.5573041426782, 718237.3516221666, 718237.3516221672, 262221.8634783282], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6640200.0000, 
sim time next is 6640800.0000, 
raw observation next is [26.86666666666667, 85.66666666666666, 1.0, 2.0, 0.1711631857577353, 1.0, 2.0, 0.1711631857577353, 1.0, 2.0, 0.2906795991504086, 6.911199999999999, 6.9112, 170.5573041426782, 717525.906256131, 717525.9062561317, 262180.8768177669], 
processed observation next is [1.0, 0.8695652173913043, 0.4723538704581361, 0.8566666666666666, 1.0, 1.0, 0.0014014286237774391, 1.0, 1.0, 0.0014014286237774391, 1.0, 1.0, 0.13497512091513242, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19931275173781418, 0.19931275173781438, 0.3913147415190551], 
reward next is 0.6087, 
noisyNet noise sample is [array([0.08699393], dtype=float32), 0.9686481]. 
=============================================
[2019-03-25 22:54:53,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.3756927e-35 6.9395228e-30 2.7158807e-35 4.6552017e-18], sum to 1.0000
[2019-03-25 22:54:53,174] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3558
[2019-03-25 22:54:53,184] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1908939.792975723 W.
[2019-03-25 22:54:53,190] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 61.33333333333334, 1.0, 2.0, 0.6826766953739574, 1.0, 2.0, 0.6826766953739574, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1908939.792975723, 1908939.792975723, 366785.4783357954], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6702000.0000, 
sim time next is 6702600.0000, 
raw observation next is [30.15, 60.66666666666666, 1.0, 2.0, 0.4545424933641209, 1.0, 2.0, 0.4545424933641209, 1.0, 1.0, 0.767204059409159, 6.9112, 6.9112, 170.5573041426782, 1906524.60075731, 1906524.60075731, 381460.2689227751], 
processed observation next is [1.0, 0.5652173913043478, 0.6279620853080569, 0.6066666666666666, 1.0, 1.0, 0.3428222811615914, 1.0, 1.0, 0.3428222811615914, 1.0, 0.5, 0.716102511474584, 0.0, 0.0, 0.8375144448122397, 0.5295901668770305, 0.5295901668770305, 0.5693436849593658], 
reward next is 0.4307, 
noisyNet noise sample is [array([-0.4182393], dtype=float32), 1.0021007]. 
=============================================
[2019-03-25 22:54:54,091] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.4460043e-33 4.3421553e-33 1.6394359e-34 1.6493663e-26], sum to 1.0000
[2019-03-25 22:54:54,101] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9720
[2019-03-25 22:54:54,111] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.86666666666667, 95.0, 1.0, 1.0, 0.3071818886112656, 1.0, 1.0, 0.3071818886112656, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 858539.164418605, 858539.164418605, 251269.0823835713], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6664800.0000, 
sim time next is 6665400.0000, 
raw observation next is [24.85, 95.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.362916599808242, 6.9112, 168.9095362493682, 1178535.975707175, 858079.1618702178, 256190.426643348], 
processed observation next is [1.0, 0.13043478260869565, 0.37677725118483424, 0.95, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.045171659980824244, 0.0, 0.8294231501046819, 0.3273711043631042, 0.23835532274172716, 0.3823737711094746], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.25679973], dtype=float32), 0.97356147]. 
=============================================
[2019-03-25 22:55:13,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 4.2140273e-31 2.7013998e-31 2.0790206e-27 6.2364829e-25], sum to 1.0000
[2019-03-25 22:55:13,148] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9833
[2019-03-25 22:55:13,159] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1210198.976179915 W.
[2019-03-25 22:55:13,164] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.65, 83.5, 1.0, 2.0, 0.4316998597248159, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7197728919379816, 6.911200000000001, 6.9112, 168.9129560345683, 1210198.976179915, 1210198.976179915, 269227.1224886249], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7007400.0000, 
sim time next is 7008000.0000, 
raw observation next is [25.6, 83.66666666666667, 1.0, 2.0, 0.3990225691834868, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6700434884582753, 6.911200000000001, 6.9112, 168.9129565103122, 1129946.916874103, 1129946.916874102, 256849.1388198844], 
processed observation next is [1.0, 0.08695652173913043, 0.4123222748815167, 0.8366666666666667, 1.0, 1.0, 0.2759308062451648, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5976140103149699, 8.881784197001253e-17, 0.0, 0.8294399451517193, 0.31387414357613974, 0.31387414357613946, 0.38335692361176776], 
reward next is 0.6166, 
noisyNet noise sample is [array([-0.6425638], dtype=float32), 0.3767983]. 
=============================================
[2019-03-25 22:55:13,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[45.85871 ]
 [47.547607]
 [51.0957  ]
 [53.482265]
 [53.15913 ]], R is [[45.01003265]
 [44.55993271]
 [44.58946991]
 [44.14357376]
 [44.40864944]].
[2019-03-25 22:55:18,231] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:55:18,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0372
[2019-03-25 22:55:18,256] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.35, 57.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7281511819202381, 6.9112, 6.9112, 168.912956510431, 619755.1127983677, 619755.1127983677, 187868.5668175328], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6978600.0000, 
sim time next is 6979200.0000, 
raw observation next is [29.26666666666667, 57.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7224466746469522, 6.911200000000001, 6.9112, 168.912956510431, 615679.3197189064, 615679.3197189058, 186812.2211448768], 
processed observation next is [0.0, 0.782608695652174, 0.5860979462875199, 0.5733333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6615203349353076, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17102203325525178, 0.17102203325525162, 0.2788242106639952], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.212885], dtype=float32), -0.5535994]. 
=============================================
[2019-03-25 22:55:20,607] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-25 22:55:20,610] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:55:20,611] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:55:20,612] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:55:20,612] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:55:20,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:55:20,613] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:55:20,618] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:55:20,615] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:55:20,621] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:55:20,622] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:55:20,646] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-25 22:55:20,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-25 22:55:20,685] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-25 22:55:20,686] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-25 22:55:20,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-25 22:55:23,349] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:55:23,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.01558429666667, 88.98738011333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6725600722018052, 6.9112, 6.9112, 168.912956510431, 575998.3811228581, 575998.3811228581, 177910.9186819944]
[2019-03-25 22:55:23,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:55:23,357] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6335731e-38], sampled 0.8453265922037755
[2019-03-25 22:55:26,956] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:55:26,958] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.99727775333334, 84.55511278666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6654232924796668, 6.9112, 6.9112, 168.912956510431, 572086.343057364, 572086.343057364, 176689.4253551313]
[2019-03-25 22:55:26,959] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:55:26,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.738145e-36], sampled 0.2993623557844919
[2019-03-25 22:55:35,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:55:35,055] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.15, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6663053776057556, 6.911199999999999, 6.9112, 168.912956510431, 575708.1911456663, 575708.191145667, 176822.1607972532]
[2019-03-25 22:55:35,057] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 22:55:35,059] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000e+00 0.0000e+00 0.0000e+00 0.0000e+00 2.4994e-36], sampled 0.6319640401552639
[2019-03-25 22:55:55,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:55:55,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.15, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.948589828622511, 6.9112, 6.9112, 168.912956510431, 771358.7945473588, 771358.7945473588, 234517.7685864094]
[2019-03-25 22:55:55,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:55:55,845] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9568750243254276
[2019-03-25 22:56:51,552] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:56:51,554] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.6, 52.0, 1.0, 2.0, 0.9001798884775598, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984473289566687, 6.9112, 168.9124592124111, 2155288.297046525, 2103305.856598299, 434886.8008657243]
[2019-03-25 22:56:51,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:56:51,561] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.000000e+00 3.056520e-34 5.841894e-33 2.775451e-37 1.787258e-19], sampled 0.7144422121671308
[2019-03-25 22:56:51,562] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2155288.297046525 W.
[2019-03-25 22:56:58,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:56:58,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.50728838, 82.850868485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9863024552872889, 6.9112, 6.9112, 168.9129474736822, 798636.8098519417, 798636.8098519417, 243748.8001587866]
[2019-03-25 22:56:58,938] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 22:56:58,940] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7326788e-35], sampled 0.27356995044627996
[2019-03-25 22:57:09,412] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.5284685], dtype=float32), -0.00035420782]
[2019-03-25 22:57:09,412] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.10635881833333, 82.845011145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6306379656381985, 6.9112, 6.9112, 168.912956510431, 550132.1293672876, 550132.1293672876, 170845.485711818]
[2019-03-25 22:57:09,413] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:57:09,416] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0150061e-26], sampled 0.9190118346285757
[2019-03-25 22:57:29,314] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8736 3185454724.3924 2460.0000
[2019-03-25 22:57:29,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7284.4702 3320972420.5437 2133.0000
[2019-03-25 22:57:29,972] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.4784 2938930604.8718 1375.0000
[2019-03-25 22:57:30,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.7527 3106468321.3457 2015.0000
[2019-03-25 22:57:30,232] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7922.6316 2990297208.5063 1564.0000
[2019-03-25 22:57:31,246] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 400000, evaluation results [400000.0, 7284.470166677882, 3320972420.543695, 2133.0, 7347.752650029007, 3106468321.3457327, 2015.0, 8059.478399448296, 2938930604.871814, 1375.0, 7029.87363625732, 3185454724.392364, 2460.0, 7922.631637529471, 2990297208.5063324, 1564.0]
[2019-03-25 22:57:35,454] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.7371685e-30], sum to 1.0000
[2019-03-25 22:57:35,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9548
[2019-03-25 22:57:35,472] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.75, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8083786106972812, 6.911200000000001, 6.9112, 168.912956510431, 677849.5942588504, 677849.5942588499, 203575.4560092241], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7075800.0000, 
sim time next is 7076400.0000, 
raw observation next is [25.7, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8057129599745759, 6.9112, 6.9112, 168.912956510431, 675852.2249214473, 675852.2249214473, 203026.5135721018], 
processed observation next is [1.0, 0.9130434782608695, 0.4170616113744076, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7630645853348486, 0.0, 0.0, 0.8294399451523027, 0.1877367291448465, 0.1877367291448465, 0.30302464712254], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.90401256], dtype=float32), 0.6062909]. 
=============================================
[2019-03-25 22:57:43,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.9999154e-01 3.0919482e-17 1.3324428e-21 2.0459005e-16 8.4347130e-06], sum to 1.0000
[2019-03-25 22:57:43,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1653
[2019-03-25 22:57:43,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2020972.473593946 W.
[2019-03-25 22:57:43,642] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 81.5, 1.0, 2.0, 0.8042149877180078, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002846279175987, 6.9112, 168.9123348004423, 2020972.473593946, 1955955.688230112, 408995.4694178933], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7205400.0000, 
sim time next is 7206000.0000, 
raw observation next is [29.0, 80.66666666666667, 1.0, 2.0, 0.7062177501383353, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000372714199125, 6.9112, 168.9123509734166, 1883829.664987638, 1820567.699235808, 386216.2136994871], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.8066666666666668, 1.0, 1.0, 0.6460454820943798, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00891727141991252, 0.0, 0.829436971688346, 0.5232860180521216, 0.5057132497877245, 0.5764421099992345], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0321832], dtype=float32), -0.9439934]. 
=============================================
[2019-03-25 22:57:43,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[47.26531]
 [46.81015]
 [45.66331]
 [46.45842]
 [46.37   ]], R is [[47.27822495]
 [46.80544281]
 [46.33738708]
 [45.87401199]
 [45.83487701]].
[2019-03-25 22:58:00,470] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:58:00,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5125
[2019-03-25 22:58:00,486] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.38333333333333, 91.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.564998760545832, 6.9112, 6.9112, 168.912956510431, 494844.2001520861, 494844.2001520861, 160856.7662470466], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7438200.0000, 
sim time next is 7438800.0000, 
raw observation next is [21.36666666666667, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5648620509011754, 6.9112, 6.9112, 168.912956510431, 494711.8894483713, 494711.8894483713, 160837.6228105339], 
processed observation next is [0.0, 0.08695652173913043, 0.21169036334913136, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4693439645136285, 0.0, 0.0, 0.8294399451523027, 0.13741996929121425, 0.13741996929121425, 0.24005615344855807], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.7571599], dtype=float32), 0.71743774]. 
=============================================
[2019-03-25 22:58:08,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:58:08,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8043
[2019-03-25 22:58:08,027] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.93333333333334, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8019077474803196, 6.911199999999999, 6.9112, 168.912956510431, 671630.1126504984, 671630.1126504991, 202215.3950593901], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7600200.0000, 
sim time next is 7600800.0000, 
raw observation next is [24.86666666666667, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7985040156437883, 6.9112, 6.9112, 168.912956510431, 669289.3830200105, 669289.3830200105, 201525.7808804005], 
processed observation next is [0.0, 1.0, 0.3775671406003162, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7542731898094978, 0.0, 0.0, 0.8294399451523027, 0.1859137175055585, 0.1859137175055585, 0.3007847475826873], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.70653754], dtype=float32), -0.06567107]. 
=============================================
[2019-03-25 22:58:15,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.8059452e-32 1.1898014e-22 1.0475731e-34 9.4156714e-13], sum to 1.0000
[2019-03-25 22:58:15,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3904
[2019-03-25 22:58:15,424] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1759196.76279506 W.
[2019-03-25 22:58:15,431] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.85, 63.0, 1.0, 2.0, 0.4194462592629226, 1.0, 2.0, 0.4194462592629226, 1.0, 2.0, 0.7091693456224031, 6.9112, 6.9112, 170.5573041426782, 1759196.76279506, 1759196.76279506, 361008.8011741494], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7644600.0000, 
sim time next is 7645200.0000, 
raw observation next is [29.93333333333334, 62.66666666666666, 1.0, 2.0, 0.6019973682858848, 1.0, 2.0, 0.6019973682858848, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1683162.435269583, 1683162.435269583, 334980.8976575581], 
processed observation next is [1.0, 0.4782608695652174, 0.6176935229067935, 0.6266666666666666, 1.0, 1.0, 0.5204787569709455, 1.0, 1.0, 0.5204787569709455, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4675451209082175, 0.4675451209082175, 0.4999714890411315], 
reward next is 0.5000, 
noisyNet noise sample is [array([0.16659081], dtype=float32), -1.2277455]. 
=============================================
[2019-03-25 22:58:21,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.4678217e-34 2.0810329e-30 0.0000000e+00 2.5406737e-17], sum to 1.0000
[2019-03-25 22:58:21,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3282
[2019-03-25 22:58:21,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1889129.864255304 W.
[2019-03-25 22:58:21,806] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.65, 62.0, 1.0, 2.0, 0.7100053150850835, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98945802742444, 6.9112, 168.912490411611, 1889129.864255304, 1833611.079366062, 387449.3983632487], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7734600.0000, 
sim time next is 7735200.0000, 
raw observation next is [31.7, 61.66666666666667, 1.0, 2.0, 0.533505173898011, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9120866634695584, 6.9112, 6.9112, 168.9128927894449, 1491537.632773917, 1491537.632773917, 324069.0741639223], 
processed observation next is [1.0, 0.5217391304347826, 0.7014218009478673, 0.6166666666666667, 1.0, 1.0, 0.43795804084097706, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8927886139872663, 0.0, 0.0, 0.8294396322530894, 0.4143160091038658, 0.4143160091038658, 0.483685185319287], 
reward next is 0.5163, 
noisyNet noise sample is [array([-0.9709127], dtype=float32), -1.0647929]. 
=============================================
[2019-03-25 22:58:23,991] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 22:58:24,003] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3172
[2019-03-25 22:58:24,014] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.86666666666667, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.891555935402901, 6.9112, 6.9112, 168.912956510431, 730748.9337854399, 730748.9337854399, 221270.7115490037], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7841400.0000, 
sim time next is 7842000.0000, 
raw observation next is [28.73333333333333, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.897361027526644, 6.9112, 6.9112, 168.912956510431, 735205.1416437647, 735205.1416437647, 222595.5396381101], 
processed observation next is [1.0, 0.782608695652174, 0.560821484992101, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.874830521373956, 0.0, 0.0, 0.8294399451523027, 0.2042236504566013, 0.2042236504566013, 0.33223214871359713], 
reward next is 0.6678, 
noisyNet noise sample is [array([0.9154593], dtype=float32), -0.58787614]. 
=============================================
[2019-03-25 22:58:24,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[61.99553 ]
 [61.598576]
 [61.088577]
 [60.04253 ]
 [57.1159  ]], R is [[61.54083252]
 [61.59517288]
 [61.65282822]
 [61.71640778]
 [61.786129  ]].
[2019-03-25 22:58:32,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:32,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:32,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-25 22:58:33,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:33,202] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:33,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-25 22:58:33,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:33,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:33,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-25 22:58:33,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:33,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:33,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-25 22:58:34,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:34,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:34,357] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-25 22:58:34,975] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:34,977] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:35,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-25 22:58:35,454] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:35,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:35,483] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-25 22:58:35,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:35,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:35,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-25 22:58:36,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,225] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-25 22:58:36,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,265] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-25 22:58:36,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,293] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,309] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-25 22:58:36,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,326] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,332] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,332] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-25 22:58:36,367] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-25 22:58:36,369] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-25 22:58:36,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-25 22:58:36,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 22:58:36,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:36,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-25 22:58:37,063] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-25 22:58:37,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 22:58:37,066] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 22:58:37,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:37,067] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 22:58:37,067] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:37,066] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 22:58:37,067] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:37,067] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 22:58:37,068] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:37,068] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 22:58:37,072] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-25 22:58:37,089] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-25 22:58:37,110] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-25 22:58:37,126] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-25 22:58:37,143] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-25 22:58:51,109] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43155152], dtype=float32), 0.11407205]
[2019-03-25 22:58:51,111] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.7, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5299700969417286, 6.9112, 6.9112, 168.912956510431, 466636.1393664596, 466636.1393664596, 155955.8001237741]
[2019-03-25 22:58:51,113] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 22:58:51,117] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.21295676552186216
[2019-03-25 22:59:01,029] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.43155152], dtype=float32), 0.11407205]
[2019-03-25 22:59:01,030] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 51.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5024871976999293, 6.9112, 6.9112, 168.912956510431, 445576.2249908865, 445576.2249908865, 152277.9744048876]
[2019-03-25 22:59:01,032] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 22:59:01,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8976420387655794
[2019-03-25 22:59:44,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.43155152], dtype=float32), 0.11407205]
[2019-03-25 22:59:44,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.371753725, 91.977341225, 1.0, 2.0, 0.8613363203051749, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.997567648347722, 6.9112, 168.9124427801869, 2100919.434751989, 2039647.438821695, 423883.00081143]
[2019-03-25 22:59:44,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 22:59:44,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 3.5372024e-30 4.3324307e-31 4.6170858e-27 9.3502842e-19], sampled 0.34010620592354834
[2019-03-25 22:59:44,221] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2100919.434751989 W.
[2019-03-25 23:00:07,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.43155152], dtype=float32), 0.11407205]
[2019-03-25 23:00:07,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.65, 71.5, 1.0, 2.0, 0.5800811284644196, 1.0, 2.0, 0.5800811284644196, 1.0, 1.0, 1.007409084382091, 6.9112, 6.9112, 170.5573041426782, 2433643.825098601, 2433643.825098601, 474939.6894263934]
[2019-03-25 23:00:07,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:00:07,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4411288e-04 1.8631877e-22 7.9997441e-26 4.9594987e-21 9.9985588e-01], sampled 0.7156382304867238
[2019-03-25 23:00:15,199] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.43155152], dtype=float32), 0.11407205]
[2019-03-25 23:00:15,202] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.66666666666667, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9113952932322951, 6.9112, 6.9112, 168.912956510431, 748802.3683038806, 748802.3683038806, 225954.5863988744]
[2019-03-25 23:00:15,205] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:00:15,209] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.1152336e-38], sampled 0.6979081941910475
[2019-03-25 23:00:45,574] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7982.9523 3002807613.2890 1372.0000
[2019-03-25 23:00:46,075] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7065.7643 3198398366.3475 2339.0000
[2019-03-25 23:00:46,260] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7352.7168 3336517692.7568 1871.0000
[2019-03-25 23:00:46,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8086.4601 2951511693.1425 1260.0000
[2019-03-25 23:00:46,464] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.0950 3119533085.1105 1973.0000
[2019-03-25 23:00:47,475] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 425000, evaluation results [425000.0, 7352.716766930887, 3336517692.756784, 1871.0, 7346.094999943021, 3119533085.110509, 1973.0, 8086.46013979538, 2951511693.142518, 1260.0, 7065.764292184005, 3198398366.3474774, 2339.0, 7982.952257460077, 3002807613.289004, 1372.0]
[2019-03-25 23:00:49,426] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 7.4750302e-26 1.4912868e-22 4.5094652e-25 7.5955047e-09], sum to 1.0000
[2019-03-25 23:00:49,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3415
[2019-03-25 23:00:49,448] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1225362.345628849 W.
[2019-03-25 23:00:49,455] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.58333333333334, 96.0, 1.0, 2.0, 0.4146365203572692, 1.0, 1.0, 0.4146365203572692, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1225362.345628849, 1225362.345628849, 283714.4036740095], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [22.56666666666667, 96.0, 1.0, 2.0, 0.3907730653384505, 1.0, 2.0, 0.3907730653384505, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1153641.138634086, 1153641.138634086, 277177.8233978099], 
processed observation next is [1.0, 0.6956521739130435, 0.26856240126382325, 0.96, 1.0, 1.0, 0.26599164498608496, 1.0, 1.0, 0.26599164498608496, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3204558718428016, 0.3204558718428016, 0.4136982438773282], 
reward next is 0.5863, 
noisyNet noise sample is [array([-0.17797633], dtype=float32), -2.2645655]. 
=============================================
[2019-03-25 23:00:50,800] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 4.3569976e-29 4.2864267e-27 9.8990524e-29 1.2605214e-15], sum to 1.0000
[2019-03-25 23:00:50,809] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2380
[2019-03-25 23:00:50,820] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 983404.825950508 W.
[2019-03-25 23:00:50,827] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.3, 63.33333333333333, 1.0, 2.0, 0.6559929134903002, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983404.825950508, 983404.8259505074, 220108.7774722858], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 46200.0000, 
sim time next is 46800.0000, 
raw observation next is [27.4, 63.0, 1.0, 2.0, 0.2314332824387589, 1.0, 1.0, 0.2314332824387589, 1.0, 1.0, 0.4029445662819964, 6.911199999999999, 6.9112, 170.5573041426782, 1036555.670958187, 1036555.670958187, 286223.8820599313], 
processed observation next is [1.0, 0.5652173913043478, 0.4976303317535545, 0.63, 1.0, 1.0, 0.07401600293826373, 1.0, 0.5, 0.07401600293826373, 1.0, 0.5, 0.27188361741706873, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2879321308217186, 0.2879321308217186, 0.4271998239700467], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6577204], dtype=float32), 0.9769322]. 
=============================================
[2019-03-25 23:00:51,390] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:00:51,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3054
[2019-03-25 23:00:51,405] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.43333333333333, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6696423903677016, 6.9112, 6.9112, 168.912956510431, 576234.0410331846, 576234.0410331846, 177407.0225305206], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 153600.0000, 
sim time next is 154200.0000, 
raw observation next is [22.41666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.674188679731674, 6.9112, 6.9112, 168.912956510431, 580258.5197760034, 580258.5197760034, 178187.2174370212], 
processed observation next is [1.0, 0.782608695652174, 0.26145339652448685, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6026691216239926, 0.0, 0.0, 0.8294399451523027, 0.16118292216000096, 0.16118292216000096, 0.2659510708015242], 
reward next is 0.7340, 
noisyNet noise sample is [array([-1.4456517], dtype=float32), -1.4437857]. 
=============================================
[2019-03-25 23:00:53,852] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 8.8424943e-38 6.1469314e-36 2.2989567e-36 3.5451031e-16], sum to 1.0000
[2019-03-25 23:00:53,861] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9889
[2019-03-25 23:00:53,868] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.3, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6161956072154033, 6.911200000000001, 6.9112, 168.912956510431, 536190.0994677328, 536190.0994677322, 168584.9125472638], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 88800.0000, 
sim time next is 89400.0000, 
raw observation next is [22.3, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6156589282971588, 6.9112, 6.9112, 168.912956510431, 535723.0073958393, 535723.0073958393, 168500.9612810219], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5312913759721449, 0.0, 0.0, 0.8294399451523027, 0.14881194649884424, 0.14881194649884424, 0.25149397206122676], 
reward next is 0.7485, 
noisyNet noise sample is [array([-1.7167209], dtype=float32), 1.1812297]. 
=============================================
[2019-03-25 23:00:54,400] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.8375304e-31 2.4814093e-34 7.5388995e-31 7.0937421e-23], sum to 1.0000
[2019-03-25 23:00:54,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0105
[2019-03-25 23:00:54,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.46666666666667, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.689899378281825, 6.911200000000001, 6.9112, 168.912956510431, 598953.3325073369, 598953.3325073362, 180877.2989491654], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 96000.0000, 
sim time next is 96600.0000, 
raw observation next is [22.48333333333333, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6786472115061702, 6.911200000000001, 6.9112, 168.912956510431, 588945.2340165173, 588945.2340165167, 178912.3162125599], 
processed observation next is [1.0, 0.08695652173913043, 0.26461295418641384, 0.8983333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6081063554953295, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16359589833792146, 0.16359589833792132, 0.26703330777994017], 
reward next is 0.7330, 
noisyNet noise sample is [array([-0.08678058], dtype=float32), -0.18443286]. 
=============================================
[2019-03-25 23:00:56,845] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9245387e-01 7.1767456e-29 2.6106320e-22 3.2900138e-24 7.5461590e-03], sum to 1.0000
[2019-03-25 23:00:56,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8005
[2019-03-25 23:00:56,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1340751.666985507 W.
[2019-03-25 23:00:56,878] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.8, 95.83333333333333, 1.0, 2.0, 0.3083669592257877, 1.0, 2.0, 0.3083669592257877, 1.0, 2.0, 0.5262107727586552, 6.9112, 6.9112, 170.5573041426782, 1340751.666985507, 1340751.666985507, 311407.3594387497], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 129000.0000, 
sim time next is 129600.0000, 
raw observation next is [22.8, 96.0, 1.0, 2.0, 0.4648313658026255, 1.0, 2.0, 0.4648313658026255, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1355767.618080031, 1355767.618080031, 296323.3085883817], 
processed observation next is [1.0, 0.5217391304347826, 0.2796208530805688, 0.96, 1.0, 1.0, 0.35521851301521146, 1.0, 1.0, 0.35521851301521146, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.37660211613334194, 0.37660211613334194, 0.44227359490803236], 
reward next is 0.5577, 
noisyNet noise sample is [array([-0.5522692], dtype=float32), 0.8446751]. 
=============================================
[2019-03-25 23:00:58,227] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5382927e-12 3.6346676e-34 7.5732828e-32 2.3441512e-33 1.0000000e+00], sum to 1.0000
[2019-03-25 23:00:58,239] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0830
[2019-03-25 23:00:58,247] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.56666666666667, 96.0, 1.0, 2.0, 0.2625296753291544, 1.0, 2.0, 0.2625296753291544, 1.0, 2.0, 0.4512720283723512, 6.9112, 6.9112, 170.5573041426782, 1153641.138551824, 1153641.138551824, 294708.0145559297], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 145200.0000, 
sim time next is 145800.0000, 
raw observation next is [22.55, 96.0, 1.0, 2.0, 0.257887504336004, 1.0, 2.0, 0.257887504336004, 1.0, 2.0, 0.4436791260332955, 6.9112, 6.9112, 170.5573041426782, 1134692.36716184, 1134692.36716184, 293181.8527355819], 
processed observation next is [1.0, 0.6956521739130435, 0.26777251184834133, 0.96, 1.0, 1.0, 0.10588855944096866, 1.0, 1.0, 0.10588855944096866, 1.0, 1.0, 0.32155990979670185, 0.0, 0.0, 0.8375144448122397, 0.3151923242116222, 0.3151923242116222, 0.43758485482922677], 
reward next is 0.5624, 
noisyNet noise sample is [array([1.8961637], dtype=float32), -0.36554107]. 
=============================================
[2019-03-25 23:01:06,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:01:06,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3386
[2019-03-25 23:01:06,604] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5195163530727073, 6.9112, 6.9112, 168.912956510431, 458364.2539521999, 458364.2539521999, 154547.689182507], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 263400.0000, 
sim time next is 264000.0000, 
raw observation next is [20.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5197626676427656, 6.9112, 6.9112, 168.912956510431, 458581.6140000548, 458581.6140000548, 154579.7086593983], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.414344716637519, 0.0, 0.0, 0.8294399451523027, 0.12738378166668188, 0.12738378166668188, 0.23071598307372881], 
reward next is 0.7693, 
noisyNet noise sample is [array([-1.4572849], dtype=float32), 0.21565348]. 
=============================================
[2019-03-25 23:01:06,627] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[69.44186]
 [69.74712]
 [70.01292]
 [70.34921]
 [70.73035]], R is [[69.24664307]
 [69.32350922]
 [69.39961243]
 [69.47488403]
 [69.54924774]].
[2019-03-25 23:01:15,588] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:01:15,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5328
[2019-03-25 23:01:15,600] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.56666666666667, 72.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9297970135771522, 6.9112, 6.9112, 168.912956510431, 824134.3303615414, 824134.3303615414, 230331.8170237257], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 388200.0000, 
sim time next is 388800.0000, 
raw observation next is [22.6, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.915698545076073, 6.9112, 6.9112, 168.912956510431, 811252.6944798813, 811252.6944798813, 226997.1860188281], 
processed observation next is [1.0, 0.5217391304347826, 0.27014218009478685, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8971933476537474, 0.0, 0.0, 0.8294399451523027, 0.22534797068885593, 0.22534797068885593, 0.33880177017735535], 
reward next is 0.6612, 
noisyNet noise sample is [array([-0.42951754], dtype=float32), -0.8331591]. 
=============================================
[2019-03-25 23:01:27,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 4.1350755e-31 6.0285898e-29 4.9762016e-24 3.8150797e-17], sum to 1.0000
[2019-03-25 23:01:27,627] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7723
[2019-03-25 23:01:27,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1060844.73650728 W.
[2019-03-25 23:01:27,643] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.13333333333333, 56.00000000000001, 1.0, 1.0, 0.2178061727131792, 1.0, 1.0, 0.2178061727131792, 1.0, 2.0, 0.3988376614288162, 6.911199999999999, 6.9112, 170.5573041426782, 1060844.73650728, 1060844.736507281, 289092.470166685], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 559200.0000, 
sim time next is 559800.0000, 
raw observation next is [24.35, 55.5, 1.0, 2.0, 0.2085402218081464, 1.0, 2.0, 0.2085402218081464, 1.0, 2.0, 0.3816603239646121, 6.9112, 6.9112, 170.5573041426782, 1014564.134731876, 1014564.134731876, 286010.9211770418], 
processed observation next is [1.0, 0.4782608695652174, 0.35308056872037924, 0.555, 1.0, 1.0, 0.046434002178489635, 1.0, 1.0, 0.046434002178489635, 1.0, 1.0, 0.24592722434708789, 0.0, 0.0, 0.8375144448122397, 0.28182337075885444, 0.28182337075885444, 0.4268819719060325], 
reward next is 0.5731, 
noisyNet noise sample is [array([1.0125209], dtype=float32), 0.41778898]. 
=============================================
[2019-03-25 23:01:29,626] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.466686e-37 0.000000e+00], sum to 1.0000
[2019-03-25 23:01:29,635] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8463
[2019-03-25 23:01:29,645] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4618806883060043, 6.9112, 6.9112, 168.912956510431, 412422.9783335476, 412422.9783335476, 147260.786302112], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 586800.0000, 
sim time next is 587400.0000, 
raw observation next is [21.75, 70.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.460378501094824, 6.911199999999999, 6.9112, 168.912956510431, 411276.6103205053, 411276.6103205059, 147077.5452764042], 
processed observation next is [1.0, 0.8260869565217391, 0.2298578199052133, 0.705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3419250013351512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11424350286680703, 0.1142435028668072, 0.21951872429314062], 
reward next is 0.7805, 
noisyNet noise sample is [array([0.5514732], dtype=float32), -0.16998363]. 
=============================================
[2019-03-25 23:01:31,580] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 6.5150093e-35 8.9488313e-33 1.8590602e-23 1.3084531e-28], sum to 1.0000
[2019-03-25 23:01:31,595] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2697
[2019-03-25 23:01:31,601] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.53333333333333, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3853782928041453, 6.911200000000001, 6.9112, 168.912956510431, 348394.4774936022, 348394.4774936016, 138979.2846418638], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 609600.0000, 
sim time next is 610200.0000, 
raw observation next is [17.45, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3833713056572097, 6.911199999999999, 6.9112, 168.912956510431, 346676.3030224599, 346676.3030224605, 138783.7969249846], 
processed observation next is [1.0, 0.043478260869565216, 0.026066350710900507, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.24801378738684113, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09629897306179441, 0.09629897306179458, 0.2071399954104248], 
reward next is 0.7929, 
noisyNet noise sample is [array([1.0211129], dtype=float32), 1.2536267]. 
=============================================
[2019-03-25 23:01:32,353] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.293474e-35], sum to 1.0000
[2019-03-25 23:01:32,368] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2098
[2019-03-25 23:01:32,374] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.45, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4505314518753213, 6.911200000000001, 6.9112, 168.912956510431, 403544.4234412894, 403544.4234412888, 145900.9327319407], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 678600.0000, 
sim time next is 679200.0000, 
raw observation next is [20.33333333333334, 78.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4529885652260691, 6.9112, 6.9112, 168.912956510431, 405761.9823307951, 405761.9823307951, 146172.2652426373], 
processed observation next is [1.0, 0.8695652173913043, 0.16271721958925783, 0.7833333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3329128844220355, 0.0, 0.0, 0.8294399451523027, 0.1127116617585542, 0.1127116617585542, 0.21816756006363777], 
reward next is 0.7818, 
noisyNet noise sample is [array([0.04279784], dtype=float32), -0.59619325]. 
=============================================
[2019-03-25 23:01:38,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.5307596e-37 0.0000000e+00], sum to 1.0000
[2019-03-25 23:01:38,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2633
[2019-03-25 23:01:38,121] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.6, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3983844539923, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 140280.5688195635], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [17.6, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3970219876606945, 6.9112, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073982, 140145.6533955668], 
processed observation next is [1.0, 0.17391304347826086, 0.0331753554502371, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2646609605618226, 0.0, 0.0, 0.8294399451523027, 0.09950699402983283, 0.09950699402983283, 0.20917261700830866], 
reward next is 0.7908, 
noisyNet noise sample is [array([1.8908666], dtype=float32), 0.48634693]. 
=============================================
[2019-03-25 23:01:40,896] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:01:40,904] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0292
[2019-03-25 23:01:40,913] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.2, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4734875880820239, 6.911199999999999, 6.9112, 168.912956510431, 422468.2373203978, 422468.2373203984, 148620.2586538315], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 768600.0000, 
sim time next is 769200.0000, 
raw observation next is [20.13333333333333, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4743530103712, 6.911200000000001, 6.9112, 168.912956510431, 423251.0919288568, 423251.0919288562, 148720.9539194463], 
processed observation next is [1.0, 0.9130434782608695, 0.15323854660347538, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35896708581853654, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11756974775801578, 0.11756974775801561, 0.22197157301409895], 
reward next is 0.7780, 
noisyNet noise sample is [array([0.4455453], dtype=float32), 1.3478945]. 
=============================================
[2019-03-25 23:01:48,915] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 3.9066443e-37 1.5236038e-36 7.8489021e-33 7.9805336e-19], sum to 1.0000
[2019-03-25 23:01:48,932] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6556
[2019-03-25 23:01:48,939] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9741662873234019, 6.9112, 6.9112, 168.912956510431, 846897.5477797518, 846897.5477797518, 241942.3728970416], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 981600.0000, 
sim time next is 982200.0000, 
raw observation next is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.018355990692283, 6.9112, 6.9112, 168.9127931844733, 885330.1506273457, 885330.1506273457, 253398.4018770298], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0223853545027841, 0.0, 0.0, 0.8294391431470793, 0.24592504184092934, 0.24592504184092934, 0.3782065699657161], 
reward next is 0.6218, 
noisyNet noise sample is [array([2.0768573], dtype=float32), 2.370083]. 
=============================================
[2019-03-25 23:01:54,964] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-25 23:01:54,966] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:01:54,969] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:01:54,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:01:54,970] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:01:54,970] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:01:54,970] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:01:54,971] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:01:54,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:01:54,972] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:01:54,973] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:01:54,989] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-25 23:01:54,990] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-25 23:01:55,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-25 23:01:55,029] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-25 23:01:55,045] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-25 23:01:59,778] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:01:59,779] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.83333333333333, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4710831330748151, 6.911200000000001, 6.9112, 168.912956510431, 421014.5600767238, 421014.5600767232, 148295.473960247]
[2019-03-25 23:01:59,780] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:01:59,783] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9487718822816661
[2019-03-25 23:02:02,858] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:02:02,859] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.54137871333333, 90.91463202666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5477035111993143, 6.9112, 6.9112, 168.912956510431, 482207.4871013163, 482207.4871013163, 158355.6374373784]
[2019-03-25 23:02:02,861] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:02:02,864] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8801474327730318
[2019-03-25 23:02:05,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:02:05,323] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.03333333333333, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5758288610620361, 6.911199999999999, 6.9112, 168.912956510431, 505907.1673273044, 505907.167327305, 162369.7021193509]
[2019-03-25 23:02:05,324] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:02:05,327] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.0463127949234865
[2019-03-25 23:02:29,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:02:29,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.93333333333334, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.824396863199923, 6.9112, 6.9112, 168.912956510431, 690143.3343921842, 690143.3343921842, 206919.5504094661]
[2019-03-25 23:02:29,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:02:29,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28728748287812855
[2019-03-25 23:02:36,265] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:02:36,267] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.16666666666667, 72.0, 1.0, 2.0, 0.75513389567293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1086042.837593264, 1086042.837593264, 237386.4360748446]
[2019-03-25 23:02:36,270] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:02:36,272] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8386716e-35 3.8931518e-29], sampled 0.8978390593208879
[2019-03-25 23:02:36,273] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1086042.837593264 W.
[2019-03-25 23:02:48,104] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:02:48,105] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.66666666666666, 60.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9720908390165155, 6.911200000000001, 6.9112, 168.912956510431, 787916.5391960784, 787916.5391960779, 240204.7637275251]
[2019-03-25 23:02:48,107] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:02:48,111] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7174193040885745
[2019-03-25 23:03:00,573] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:03:00,575] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.09951841500001, 62.96213914, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.313751637992656, 6.9112, 168.9109735532127, 1739531.15465394, 1453950.520523064, 311359.7120107348]
[2019-03-25 23:03:00,578] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:03:00,581] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5596431e-32], sampled 0.6047795726678549
[2019-03-25 23:03:00,581] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1739531.15465394 W.
[2019-03-25 23:03:20,181] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.38087222], dtype=float32), 0.23556052]
[2019-03-25 23:03:20,182] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.47810767, 82.38285563666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8284023799294783, 6.911199999999999, 6.9112, 168.912956510431, 694109.3490213675, 694109.3490213681, 207787.721529713]
[2019-03-25 23:03:20,184] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:03:20,186] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6012322987453202
[2019-03-25 23:04:04,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.6231 2937985494.9653 1381.0000
[2019-03-25 23:04:04,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.2169 3105854149.7565 2010.0000
[2019-03-25 23:04:04,723] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.1561 2989509937.5631 1566.0000
[2019-03-25 23:04:04,849] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.7170 3319727200.6801 2143.0000
[2019-03-25 23:04:04,983] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.1639 3185281672.3323 2464.0000
[2019-03-25 23:04:05,994] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 450000, evaluation results [450000.0, 7286.717049302781, 3319727200.680101, 2143.0, 7346.216919389658, 3105854149.756524, 2010.0, 8059.623113280159, 2937985494.9653387, 1381.0, 7029.16391117388, 3185281672.3322916, 2464.0, 7923.156098041422, 2989509937.563126, 1566.0]
[2019-03-25 23:04:06,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:04:06,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4862
[2019-03-25 23:04:06,413] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.2, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6050104961785576, 6.9112, 6.9112, 168.912956510431, 525660.1717307077, 525660.1717307077, 166864.5974072701], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 942000.0000, 
sim time next is 942600.0000, 
raw observation next is [22.15, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.606394878875037, 6.911200000000001, 6.9112, 168.912956510431, 526792.0208482681, 526792.0208482675, 167078.4774285926], 
processed observation next is [0.0, 0.9130434782608695, 0.24881516587677724, 0.915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5199937547256548, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1463311169022967, 0.14633111690229653, 0.2493708618337203], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.2827628], dtype=float32), 0.0062840576]. 
=============================================
[2019-03-25 23:04:16,997] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 9.985332e-37 5.879793e-32 4.595249e-18], sum to 1.0000
[2019-03-25 23:04:17,006] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1356
[2019-03-25 23:04:17,017] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1510361.087594324 W.
[2019-03-25 23:04:17,022] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 59.33333333333334, 1.0, 2.0, 0.9995461820108865, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1510361.087594324, 1510361.087594323, 315744.7929211081], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [27.6, 59.0, 1.0, 2.0, 0.4987096728341657, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8779455595800422, 6.911199999999999, 6.9112, 168.912956510431, 1514832.996002601, 1514832.996002601, 318136.6006481743], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.59, 1.0, 1.0, 0.39603575040260924, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.8511531214390757, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4207869433340558, 0.4207869433340558, 0.47483074723608104], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.62119037], dtype=float32), 0.018152427]. 
=============================================
[2019-03-25 23:04:17,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 7.3237610e-36 1.0445347e-30 1.0625866e-32], sum to 1.0000
[2019-03-25 23:04:17,210] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4392
[2019-03-25 23:04:17,220] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 960980.9160049107 W.
[2019-03-25 23:04:17,226] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.61666666666667, 68.83333333333333, 1.0, 2.0, 0.3163269685361757, 1.0, 1.0, 0.3163269685361757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 960980.9160049107, 960980.9160049107, 262660.5513839462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1087800.0000, 
sim time next is 1088400.0000, 
raw observation next is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.6281623996219932, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 961985.3896602662, 961985.3896602662, 216463.5069301341], 
processed observation next is [1.0, 0.6086956521739131, 0.4139020537124806, 0.6866666666666668, 1.0, 1.0, 0.5520028911108351, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2672181637945184, 0.2672181637945184, 0.3230798610897524], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.836383], dtype=float32), -0.9839045]. 
=============================================
[2019-03-25 23:04:20,291] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9999774e-01 2.7406022e-35 3.8822331e-34 3.1811721e-25 2.2265890e-06], sum to 1.0000
[2019-03-25 23:04:20,305] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9427
[2019-03-25 23:04:20,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1918400.665217826 W.
[2019-03-25 23:04:20,324] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 73.0, 1.0, 2.0, 0.6860570742918907, 1.0, 1.0, 0.6860570742918907, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1918400.665217826, 1918400.665217826, 368204.7238321679], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1260000.0000, 
sim time next is 1260600.0000, 
raw observation next is [28.46666666666667, 73.16666666666667, 1.0, 2.0, 0.4551052855747983, 1.0, 2.0, 0.4551052855747983, 1.0, 1.0, 0.7738755583950762, 6.911199999999999, 6.9112, 170.5573041426782, 1908887.26883075, 1908887.26883075, 382774.4977759798], 
processed observation next is [1.0, 0.6086956521739131, 0.5481832543443919, 0.7316666666666667, 1.0, 1.0, 0.3435003440660221, 1.0, 1.0, 0.3435003440660221, 1.0, 0.5, 0.7242384858476537, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5302464635640972, 0.5302464635640972, 0.5713052205611638], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3760322], dtype=float32), 1.3235257]. 
=============================================
[2019-03-25 23:04:27,640] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 4.5145850e-29 5.7199922e-33 2.2425672e-22 2.3470843e-12], sum to 1.0000
[2019-03-25 23:04:27,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6407
[2019-03-25 23:04:27,660] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.85, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7540769345657085, 6.911200000000001, 6.9112, 168.912956510431, 648402.5254040464, 648402.5254040457, 192820.2255686259], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1236600.0000, 
sim time next is 1237200.0000, 
raw observation next is [24.0, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7579519613984728, 6.9112, 6.9112, 168.912956510431, 651293.4722701488, 651293.4722701488, 193575.1622069073], 
processed observation next is [1.0, 0.30434782608695654, 0.3364928909952607, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7048194651200886, 0.0, 0.0, 0.8294399451523027, 0.18091485340837468, 0.18091485340837468, 0.2889181525476228], 
reward next is 0.7111, 
noisyNet noise sample is [array([0.5018631], dtype=float32), 0.01829451]. 
=============================================
[2019-03-25 23:04:36,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 7.6613193e-34 5.4355371e-31 1.5455786e-22], sum to 1.0000
[2019-03-25 23:04:36,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0480
[2019-03-25 23:04:36,721] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 980255.7795487546 W.
[2019-03-25 23:04:36,726] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.83333333333334, 91.83333333333333, 1.0, 2.0, 0.3087902234314681, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5586212391072852, 6.911200000000001, 6.9112, 168.912956510431, 980255.7795487546, 980255.779548754, 235005.1049160375], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1356600.0000, 
sim time next is 1357200.0000, 
raw observation next is [20.8, 92.0, 1.0, 2.0, 0.2082512917853164, 1.0, 1.0, 0.2082512917853164, 1.0, 2.0, 0.3748334644696423, 6.9112, 6.9112, 170.5573041426782, 983054.366886994, 983054.366886994, 284116.3446591651], 
processed observation next is [1.0, 0.7391304347826086, 0.1848341232227489, 0.92, 1.0, 1.0, 0.046085893717248666, 1.0, 0.5, 0.046085893717248666, 1.0, 1.0, 0.23760178593858813, 0.0, 0.0, 0.8375144448122397, 0.2730706574686094, 0.2730706574686094, 0.4240542457599479], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.47225967], dtype=float32), -0.5413179]. 
=============================================
[2019-03-25 23:04:38,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.3462543e-36], sum to 1.0000
[2019-03-25 23:04:38,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2442
[2019-03-25 23:04:38,609] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.43333333333333, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5651255639683076, 6.911200000000001, 6.9112, 168.912956510431, 496114.8118131742, 496114.8118131736, 160841.8258340693], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1384800.0000, 
sim time next is 1385400.0000, 
raw observation next is [20.41666666666666, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5631221719929254, 6.911200000000001, 6.9112, 168.912956510431, 494435.7706362947, 494435.7706362941, 160554.5392920695], 
processed observation next is [0.0, 0.0, 0.16666666666666644, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4672221609669821, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13734326962119298, 0.1373432696211928, 0.23963364073443208], 
reward next is 0.7604, 
noisyNet noise sample is [array([-2.7333608], dtype=float32), 0.66620964]. 
=============================================
[2019-03-25 23:04:49,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:04:49,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9325
[2019-03-25 23:04:49,871] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.63333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7341690437217397, 6.911199999999999, 6.9112, 168.912956510431, 620794.4020437129, 620794.4020437135, 188954.1219282224], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1668000.0000, 
sim time next is 1668600.0000, 
raw observation next is [23.65, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7331289988278178, 6.9112, 6.9112, 168.912956510431, 619786.8178648985, 619786.8178648985, 188756.5064170978], 
processed observation next is [1.0, 0.30434782608695654, 0.31990521327014215, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6745475595461193, 0.0, 0.0, 0.8294399451523027, 0.1721630049624718, 0.1721630049624718, 0.28172612898074295], 
reward next is 0.7183, 
noisyNet noise sample is [array([-1.5603089], dtype=float32), 1.1918113]. 
=============================================
[2019-03-25 23:04:54,461] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0205744e-04 8.3056893e-37 2.4817103e-30 2.2930889e-30 9.9989796e-01], sum to 1.0000
[2019-03-25 23:04:54,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5909
[2019-03-25 23:04:54,477] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.51666666666667, 94.0, 1.0, 2.0, 0.1732012000555767, 1.0, 2.0, 0.1732012000555767, 1.0, 2.0, 0.2893501741899162, 6.9112, 6.9112, 170.5573041426782, 726072.2709296795, 726072.2709296795, 262428.1282714974], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1738200.0000, 
sim time next is 1738800.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.1722790050203591, 1.0, 2.0, 0.1722790050203591, 1.0, 2.0, 0.2877267863337363, 6.9112, 6.9112, 170.5573041426782, 722205.0594627606, 722205.0594627606, 262211.6198705562], 
processed observation next is [1.0, 0.13043478260869565, 0.3601895734597157, 0.94, 1.0, 1.0, 0.0027457891811555135, 1.0, 1.0, 0.0027457891811555135, 1.0, 1.0, 0.13137412967528814, 0.0, 0.0, 0.8375144448122397, 0.2006125165174335, 0.2006125165174335, 0.39136062667247196], 
reward next is 0.6086, 
noisyNet noise sample is [array([-0.98965096], dtype=float32), -1.3370317]. 
=============================================
[2019-03-25 23:04:58,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.5749178e-33 7.2323550e-31 9.7323657e-27 6.3674987e-17], sum to 1.0000
[2019-03-25 23:04:58,313] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5472
[2019-03-25 23:04:58,323] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1084663.237477211 W.
[2019-03-25 23:04:58,332] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 87.33333333333334, 1.0, 2.0, 0.233176807262317, 1.0, 1.0, 0.233176807262317, 1.0, 2.0, 0.4159638186042339, 6.9112, 6.9112, 170.5573041426782, 1084663.237477211, 1084663.237477211, 291037.3003240552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1788000.0000, 
sim time next is 1788600.0000, 
raw observation next is [22.16666666666667, 85.66666666666666, 1.0, 2.0, 0.6892200423081866, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1080299.969210541, 1080299.969210541, 232771.3001493508], 
processed observation next is [1.0, 0.6956521739130435, 0.24960505529225935, 0.8566666666666666, 1.0, 1.0, 0.6255663160339597, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3000833247807059, 0.3000833247807059, 0.3474198509691803], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05521244], dtype=float32), -1.0710558]. 
=============================================
[2019-03-25 23:04:59,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 2.0192723e-35 2.8400336e-37 9.0042845e-30 1.3323944e-19], sum to 1.0000
[2019-03-25 23:04:59,464] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8085
[2019-03-25 23:04:59,480] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1040452.098804224 W.
[2019-03-25 23:04:59,491] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.56666666666667, 84.33333333333334, 1.0, 2.0, 0.6643804999954055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1040452.098804224, 1040452.098804224, 226848.7206161644], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1777200.0000, 
sim time next is 1777800.0000, 
raw observation next is [22.48333333333333, 84.16666666666666, 1.0, 2.0, 0.3284972370761594, 1.0, 1.0, 0.3284972370761594, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1020209.915321499, 1020209.915321499, 267655.5316606466], 
processed observation next is [1.0, 0.5652173913043478, 0.26461295418641384, 0.8416666666666666, 1.0, 1.0, 0.19096052659778243, 1.0, 0.5, 0.19096052659778243, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2833916431448608, 0.2833916431448608, 0.3994858681502188], 
reward next is 0.6005, 
noisyNet noise sample is [array([-0.21267514], dtype=float32), 1.4811285]. 
=============================================
[2019-03-25 23:05:07,276] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.00000000e+00 4.70191594e-22 6.79466623e-22 1.04238255e-20
 4.24844437e-10], sum to 1.0000
[2019-03-25 23:05:07,290] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5363
[2019-03-25 23:05:07,301] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 999600.2955273252 W.
[2019-03-25 23:05:07,307] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.022550604882141, 7.057125236646197, 6.9112, 168.912115332711, 999600.2955273252, 896076.4258375159, 254301.3527687115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1785600.0000, 
sim time next is 1786200.0000, 
raw observation next is [21.23333333333333, 92.33333333333334, 1.0, 1.0, 0.678733112836882, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128376919114, 1072570.912405413, 1072570.912405413, 231101.4482896911], 
processed observation next is [1.0, 0.6956521739130435, 0.2053712480252764, 0.9233333333333335, 1.0, 0.5, 0.6129314612492553, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439361698976, 0.2979363645570592, 0.2979363645570592, 0.34492753476073296], 
reward next is 0.6551, 
noisyNet noise sample is [array([-0.18536599], dtype=float32), -0.5900601]. 
=============================================
[2019-03-25 23:05:13,455] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-25 23:05:13,457] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:05:13,458] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:05:13,459] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:05:13,460] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:05:13,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:05:13,462] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:05:13,462] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:05:13,465] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:05:13,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:05:13,467] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:05:13,487] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-25 23:05:13,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-25 23:05:13,487] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-25 23:05:13,545] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-25 23:05:13,546] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-25 23:05:17,444] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.25717062], dtype=float32), 0.23031527]
[2019-03-25 23:05:17,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.73333333333333, 72.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4829068651142936, 6.9112, 6.9112, 168.912956510431, 430784.1473548727, 430784.1473548727, 149739.7215569806]
[2019-03-25 23:05:17,448] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:05:17,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7354187979301977
[2019-03-25 23:05:47,263] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25717062], dtype=float32), 0.23031527]
[2019-03-25 23:05:47,265] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.13333333333333, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.874182463827609, 6.911200000000001, 6.9112, 168.912956510431, 722126.8266303927, 722126.8266303921, 217538.9623321339]
[2019-03-25 23:05:47,265] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:05:47,268] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.815320501127687
[2019-03-25 23:06:08,849] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25717062], dtype=float32), 0.23031527]
[2019-03-25 23:06:08,850] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8908995656480173, 6.911200000000001, 6.9112, 168.912956510431, 734470.1798115391, 734470.1798115385, 221290.2452436224]
[2019-03-25 23:06:08,850] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:06:08,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8818547325268237
[2019-03-25 23:06:52,108] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.25717062], dtype=float32), 0.23031527]
[2019-03-25 23:06:52,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.23333333333333, 86.66666666666667, 1.0, 2.0, 0.8121715441410081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1135116.658233617, 1135116.658233617, 246922.8459571875]
[2019-03-25 23:06:52,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:06:52,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 9.620974e-32 6.384906e-27], sampled 0.028154753247628506
[2019-03-25 23:06:52,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1135116.658233617 W.
[2019-03-25 23:07:22,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.25717062], dtype=float32), 0.23031527]
[2019-03-25 23:07:22,294] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.11666666666667, 90.83333333333334, 1.0, 2.0, 0.8381606666026218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1171459.915470558, 1171459.915470558, 253491.8299411257]
[2019-03-25 23:07:22,296] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:07:22,298] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 6.7096014e-34 2.7423674e-26 8.8893473e-28], sampled 0.3692082591762529
[2019-03-25 23:07:22,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1171459.915470558 W.
[2019-03-25 23:07:22,429] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.6231 2937985494.9653 1381.0000
[2019-03-25 23:07:22,569] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.2169 3105854149.7565 2010.0000
[2019-03-25 23:07:22,654] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.1561 2989509937.5631 1566.0000
[2019-03-25 23:07:22,839] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.7170 3319727200.6801 2143.0000
[2019-03-25 23:07:22,888] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.1639 3185281672.3323 2464.0000
[2019-03-25 23:07:23,905] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 475000, evaluation results [475000.0, 7286.717049302781, 3319727200.680101, 2143.0, 7346.216919389658, 3105854149.756524, 2010.0, 8059.623113280159, 2937985494.9653387, 1381.0, 7029.16391117388, 3185281672.3322916, 2464.0, 7923.156098041422, 2989509937.563126, 1566.0]
[2019-03-25 23:07:33,463] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:07:33,477] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8727
[2019-03-25 23:07:33,486] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.15, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.948589828622511, 6.9112, 6.9112, 168.912956510431, 771358.7945473588, 771358.7945473588, 234517.7685864094], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2143800.0000, 
sim time next is 2144400.0000, 
raw observation next is [27.96666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9455798447975541, 6.9112, 6.9112, 168.912956510431, 769584.5904216487, 769584.5904216487, 233816.3206763892], 
processed observation next is [0.0, 0.8260869565217391, 0.524486571879937, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9336339570701879, 0.0, 0.0, 0.8294399451523027, 0.21377349733934686, 0.21377349733934686, 0.3489795830990884], 
reward next is 0.6510, 
noisyNet noise sample is [array([0.24243726], dtype=float32), -1.0657817]. 
=============================================
[2019-03-25 23:07:34,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:07:34,235] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8154
[2019-03-25 23:07:34,242] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8516253164748689, 6.911200000000001, 6.9112, 168.912956510431, 705692.6776508042, 705692.6776508036, 212598.0261295171], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2017200.0000, 
sim time next is 2017800.0000, 
raw observation next is [25.6, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8523311569362108, 6.9112, 6.9112, 168.912956510431, 706239.9046428276, 706239.9046428276, 212751.7625875982], 
processed observation next is [0.0, 0.34782608695652173, 0.4123222748815167, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8199160450441594, 0.0, 0.0, 0.8294399451523027, 0.19617775128967432, 0.19617775128967432, 0.31753994416059433], 
reward next is 0.6825, 
noisyNet noise sample is [array([-0.29832593], dtype=float32), -1.0455939]. 
=============================================
[2019-03-25 23:07:39,474] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:07:39,484] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7284
[2019-03-25 23:07:39,492] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.83333333333334, 97.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7648054150691853, 6.9112, 6.9112, 168.912956510431, 644983.9256269227, 644983.9256269227, 194827.171626894], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2091000.0000, 
sim time next is 2091600.0000, 
raw observation next is [23.8, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.763919515928722, 6.911199999999999, 6.9112, 168.912956510431, 644345.0983357441, 644345.0983357447, 194654.8579162625], 
processed observation next is [0.0, 0.21739130434782608, 0.3270142180094788, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.712096970644783, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1789847495377067, 0.17898474953770685, 0.2905296386809888], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.8678465], dtype=float32), -0.9836789]. 
=============================================
[2019-03-25 23:07:40,675] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.3950136e-36 0.0000000e+00], sum to 1.0000
[2019-03-25 23:07:40,687] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9379
[2019-03-25 23:07:40,695] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8814265702370766, 6.911200000000001, 6.9112, 168.912956510431, 725770.9342080891, 725770.9342080886, 219091.6871357754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2106000.0000, 
sim time next is 2106600.0000, 
raw observation next is [27.71666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8873188043160366, 6.911200000000001, 6.9112, 168.912956510431, 729739.0070281653, 729739.0070281646, 220400.4309560196], 
processed observation next is [0.0, 0.391304347826087, 0.5126382306477094, 0.8316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8625839077024835, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2027052797300459, 0.2027052797300457, 0.32895586709853675], 
reward next is 0.6710, 
noisyNet noise sample is [array([0.65790904], dtype=float32), 0.65136486]. 
=============================================
[2019-03-25 23:07:55,848] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9566638e-01 1.5540417e-22 3.6555961e-22 9.4815335e-22 4.3335673e-03], sum to 1.0000
[2019-03-25 23:07:55,860] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4299
[2019-03-25 23:07:55,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1236959.494654373 W.
[2019-03-25 23:07:55,877] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.2950007478256998, 1.0, 2.0, 0.2950007478256998, 1.0, 2.0, 0.5060850614389036, 6.9112, 6.9112, 170.5573041426782, 1236959.494654373, 1236959.494654373, 301787.1562971495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2439600.0000, 
sim time next is 2440200.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.8492591943811846, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1186980.504968642, 1186980.504968642, 256361.0437605455], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.8183845715435959, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3297168069357339, 0.3297168069357339, 0.38262842352320225], 
reward next is 0.6174, 
noisyNet noise sample is [array([-0.8050502], dtype=float32), 1.4940224]. 
=============================================
[2019-03-25 23:08:04,668] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 7.8541478e-32 4.7625095e-32 9.4199370e-30 9.6008607e-29], sum to 1.0000
[2019-03-25 23:08:04,675] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4150
[2019-03-25 23:08:04,690] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1044831.489760792 W.
[2019-03-25 23:08:04,698] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.7, 85.5, 1.0, 2.0, 0.3738040836874718, 1.0, 2.0, 0.3738040836874718, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1044831.489760792, 1044831.489760792, 265731.6569166079], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2446200.0000, 
sim time next is 2446800.0000, 
raw observation next is [27.7, 85.66666666666666, 1.0, 2.0, 0.3685640780310871, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6314965485662094, 6.9112, 6.9112, 168.912956510431, 1030182.767745729, 1030182.767745729, 245443.0814141497], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.8566666666666666, 1.0, 1.0, 0.23923382895311698, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.5506055470319626, 0.0, 0.0, 0.8294399451523027, 0.28616187992936915, 0.28616187992936915, 0.3663329573345518], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2085869], dtype=float32), 1.7272953]. 
=============================================
[2019-03-25 23:08:12,163] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9650973e-01 3.9087805e-23 6.4187709e-25 3.2682316e-20 3.4902103e-03], sum to 1.0000
[2019-03-25 23:08:12,170] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8192
[2019-03-25 23:08:12,183] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1930616.424884671 W.
[2019-03-25 23:08:12,198] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.33333333333334, 87.33333333333334, 1.0, 2.0, 0.6904217330313539, 1.0, 1.0, 0.6904217330313539, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1930616.424884671, 1930616.424884671, 370050.351794499], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2539200.0000, 
sim time next is 2539800.0000, 
raw observation next is [27.45, 86.5, 1.0, 2.0, 0.7060382954320624, 1.0, 2.0, 0.7060382954320624, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1974325.031554698, 1974325.031554699, 376719.085107394], 
processed observation next is [1.0, 0.391304347826087, 0.5, 0.865, 1.0, 1.0, 0.6458292716048945, 1.0, 1.0, 0.6458292716048945, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5484236198763051, 0.5484236198763053, 0.5622672912050657], 
reward next is 0.4377, 
noisyNet noise sample is [array([-0.18596971], dtype=float32), -1.420446]. 
=============================================
[2019-03-25 23:08:13,554] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7197690e-11 2.9158079e-33 1.3722536e-33 1.5391310e-36 1.0000000e+00], sum to 1.0000
[2019-03-25 23:08:13,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7876
[2019-03-25 23:08:13,570] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.18333333333334, 75.5, 1.0, 2.0, 0.4884942115024324, 1.0, 2.0, 0.4884942115024324, 1.0, 2.0, 0.8443859443992037, 6.9112, 6.9112, 170.5573041426782, 2049067.242549483, 2049067.242549483, 406583.0979392874], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2566200.0000, 
sim time next is 2566800.0000, 
raw observation next is [29.1, 76.0, 1.0, 2.0, 0.4954340758711984, 1.0, 2.0, 0.4954340758711984, 1.0, 2.0, 0.8564377596849543, 6.9112, 6.9112, 170.5573041426782, 2078205.852403416, 2078205.852403416, 411294.115709705], 
processed observation next is [1.0, 0.7391304347826086, 0.5781990521327015, 0.76, 1.0, 1.0, 0.3920892480375885, 1.0, 1.0, 0.3920892480375885, 1.0, 1.0, 0.8249240971767734, 0.0, 0.0, 0.8375144448122397, 0.5772794034453933, 0.5772794034453933, 0.613871814492097], 
reward next is 0.3861, 
noisyNet noise sample is [array([0.6179666], dtype=float32), -0.11191712]. 
=============================================
[2019-03-25 23:08:18,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:08:18,465] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5648
[2019-03-25 23:08:18,470] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.797083787628996, 6.911199999999999, 6.9112, 168.912956510431, 668329.9348324661, 668329.9348324666, 201239.3108978028], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2634000.0000, 
sim time next is 2634600.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7991707638203084, 6.9112, 6.9112, 168.912956510431, 669724.8584926792, 669724.8584926792, 201660.2070176964], 
processed observation next is [0.0, 0.4782608695652174, 0.470774091627172, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7550862973418394, 0.0, 0.0, 0.8294399451523027, 0.1860346829146331, 0.1860346829146331, 0.3009853836085021], 
reward next is 0.6990, 
noisyNet noise sample is [array([0.6740302], dtype=float32), 1.1732334]. 
=============================================
[2019-03-25 23:08:18,585] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:08:18,596] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1543
[2019-03-25 23:08:18,602] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7977753240464865, 6.9112, 6.9112, 168.912956510431, 668923.8227857094, 668923.8227857094, 201381.5698934319], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2704800.0000, 
sim time next is 2705400.0000, 
raw observation next is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7986908004284822, 6.9112, 6.9112, 168.912956510431, 669691.6689820278, 669691.6689820278, 201569.7295550462], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7545009761322952, 0.0, 0.0, 0.8294399451523027, 0.18602546360611882, 0.18602546360611882, 0.3008503426194719], 
reward next is 0.6991, 
noisyNet noise sample is [array([0.9981751], dtype=float32), 0.42121035]. 
=============================================
[2019-03-25 23:08:19,150] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:08:19,160] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7017
[2019-03-25 23:08:19,167] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7353812175648464, 6.9112, 6.9112, 168.912956510431, 624868.7322546768, 624868.7322546768, 189218.166768819], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2713200.0000, 
sim time next is 2713800.0000, 
raw observation next is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7345178396523152, 6.911200000000001, 6.9112, 168.912956510431, 624140.126048276, 624140.1260482754, 189055.2802516758], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.676241267868677, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1733722572356322, 0.17337225723563204, 0.2821720600771281], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.25959638], dtype=float32), 0.7119131]. 
=============================================
[2019-03-25 23:08:21,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4397852e-22], sum to 1.0000
[2019-03-25 23:08:21,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2110
[2019-03-25 23:08:21,191] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6890766424773449, 6.9112, 6.9112, 168.912956510431, 590841.7069712145, 590841.7069712145, 180788.0224066814], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2668800.0000, 
sim time next is 2669400.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6888870330516692, 6.911199999999999, 6.9112, 168.912956510431, 590679.0946986069, 590679.0946986075, 180754.5861443292], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.620593942745938, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1640775263051686, 0.16407752630516872, 0.2697829643945212], 
reward next is 0.7302, 
noisyNet noise sample is [array([-0.5954616], dtype=float32), 0.7378141]. 
=============================================
[2019-03-25 23:08:21,438] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.8423516e-37], sum to 1.0000
[2019-03-25 23:08:21,448] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2686
[2019-03-25 23:08:21,454] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6890766424773449, 6.9112, 6.9112, 168.912956510431, 590841.7069712145, 590841.7069712145, 180788.0224066814], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2668800.0000, 
sim time next is 2669400.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6888870330516692, 6.911199999999999, 6.9112, 168.912956510431, 590679.0946986069, 590679.0946986075, 180754.5861443292], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.620593942745938, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1640775263051686, 0.16407752630516872, 0.2697829643945212], 
reward next is 0.7302, 
noisyNet noise sample is [array([0.04766355], dtype=float32), -1.2876749]. 
=============================================
[2019-03-25 23:08:30,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8151277e-01 5.1178569e-19 1.2998734e-28 1.6765306e-20 3.1848723e-01], sum to 1.0000
[2019-03-25 23:08:30,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1539
[2019-03-25 23:08:30,920] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 972122.0522153301 W.
[2019-03-25 23:08:30,929] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.0, 84.0, 1.0, 2.0, 0.2190432139963722, 1.0, 2.0, 0.2190432139963722, 1.0, 2.0, 0.3790622083693861, 6.9112, 6.9112, 170.5573041426782, 972122.0522153301, 972122.0522153301, 281160.3554302144], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2808600.0000, 
sim time next is 2809200.0000, 
raw observation next is [24.0, 85.0, 1.0, 2.0, 0.2892583456883259, 1.0, 2.0, 0.2892583456883259, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 856955.9651991971, 856955.9651991971, 254308.8850301539], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.85, 1.0, 1.0, 0.14368475384135654, 1.0, 1.0, 0.14368475384135654, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23804332366644362, 0.23804332366644362, 0.37956550004500583], 
reward next is 0.6204, 
noisyNet noise sample is [array([0.935861], dtype=float32), 1.4559654]. 
=============================================
[2019-03-25 23:08:31,356] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-25 23:08:31,357] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:08:31,359] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:08:31,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:08:31,360] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:08:31,359] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:08:31,362] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:08:31,363] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:08:31,361] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:08:31,365] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:08:31,367] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:08:31,377] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-25 23:08:31,396] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-25 23:08:31,415] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-25 23:08:31,417] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-25 23:08:31,418] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-25 23:08:55,562] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:08:55,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [17.43333333333333, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4028238893696768, 6.911200000000001, 6.9112, 168.912956510431, 363772.5724731752, 363772.5724731746, 140674.1953343927]
[2019-03-25 23:08:55,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:08:55,567] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7613440252753888
[2019-03-25 23:09:05,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:09:05,157] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.31818907, 87.60108703, 1.0, 2.0, 0.7341341263260489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564736048, 1025996.402299829, 1025996.40229983, 228391.3741541399]
[2019-03-25 23:09:05,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:09:05,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 6.9589128e-34 0.0000000e+00 9.6524734e-33 2.5334177e-15], sampled 0.9407204656070048
[2019-03-25 23:09:05,164] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1025996.402299829 W.
[2019-03-25 23:09:32,915] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:09:32,917] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8890227391041086, 6.9112, 6.9112, 168.912956510431, 733981.0445219798, 733981.0445219798, 220898.7521455966]
[2019-03-25 23:09:32,918] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:09:32,919] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.11508167133840908
[2019-03-25 23:09:47,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:09:47,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.40000000000001, 50.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8800743845777325, 6.9112, 6.9112, 168.912956510431, 728536.6129624887, 728536.6129624887, 218925.9559199413]
[2019-03-25 23:09:47,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:09:47,047] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6067184368877975
[2019-03-25 23:09:52,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:09:52,818] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.63333333333333, 78.33333333333334, 1.0, 1.0, 0.5949628883287658, 1.0, 1.0, 0.5949628883287658, 1.0, 2.0, 1.03, 6.914890567830213, 6.9112, 169.0403247858759, 2496162.763358036, 2493542.573262449, 485854.0450879448]
[2019-03-25 23:09:52,819] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:09:52,823] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 3.0351574e-31 1.6453540e-37 2.7952556e-30 4.5234238e-16], sampled 0.31959809037027886
[2019-03-25 23:09:52,824] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2496162.763358036 W.
[2019-03-25 23:10:18,183] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.38885403], dtype=float32), 0.2945821]
[2019-03-25 23:10:18,186] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.53333333333333, 76.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9058387945270674, 6.9112, 6.9112, 168.912956510431, 746132.1238156273, 746132.1238156273, 224727.8735834011]
[2019-03-25 23:10:18,187] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:10:18,193] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9563704e-32], sampled 0.005603433082854159
[2019-03-25 23:10:41,552] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7426.0040 3328061158.0928 1742.0000
[2019-03-25 23:10:41,713] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7194.9476 3190850249.7622 2036.0000
[2019-03-25 23:10:41,780] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7426.2298 3118223978.3942 1820.0000
[2019-03-25 23:10:41,897] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8048.5988 2998745154.9467 1233.0000
[2019-03-25 23:10:42,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8133.5077 2951180561.1198 1153.0000
[2019-03-25 23:10:43,149] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 500000, evaluation results [500000.0, 7426.003998182957, 3328061158.092836, 1742.0, 7426.2298145986415, 3118223978.3941903, 1820.0, 8133.507737151363, 2951180561.119808, 1153.0, 7194.947583256188, 3190850249.762172, 2036.0, 8048.5987637743365, 2998745154.9466543, 1233.0]
[2019-03-25 23:10:44,978] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.6116596e-30 1.4261361e-38 7.4016136e-29 6.7161200e-11], sum to 1.0000
[2019-03-25 23:10:44,989] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6136
[2019-03-25 23:10:45,007] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1058189.700993781 W.
[2019-03-25 23:10:45,013] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.5, 91.5, 1.0, 2.0, 0.3470019871635073, 0.0, 2.0, 0.0, 1.0, 1.0, 0.612511215051963, 6.911199999999999, 6.9112, 168.912956510431, 1058189.700993781, 1058189.700993782, 245705.257687432], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2907000.0000, 
sim time next is 2907600.0000, 
raw observation next is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.2359428303574267, 1.0, 1.0, 0.2359428303574267, 1.0, 2.0, 0.4129993120902691, 6.9112, 6.9112, 170.5573041426782, 1065390.80364178, 1065390.80364178, 288705.9256476399], 
processed observation next is [1.0, 0.6521739130434783, 0.2575039494470777, 0.9233333333333335, 1.0, 1.0, 0.07944919320171889, 1.0, 0.5, 0.07944919320171889, 1.0, 1.0, 0.28414550254910864, 0.0, 0.0, 0.8375144448122397, 0.29594188990049447, 0.29594188990049447, 0.43090436663826853], 
reward next is 0.5691, 
noisyNet noise sample is [array([1.9140164], dtype=float32), 0.8973913]. 
=============================================
[2019-03-25 23:10:46,147] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.2600179e-28 0.0000000e+00 1.4443387e-25 1.7832169e-22], sum to 1.0000
[2019-03-25 23:10:46,168] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5596
[2019-03-25 23:10:46,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 915265.8554169796 W.
[2019-03-25 23:10:46,184] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.940953558185163, 6.9112, 168.9126236998573, 915265.8554169796, 894157.6987916379, 256532.3476297408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2971200.0000, 
sim time next is 2971800.0000, 
raw observation next is [22.0, 91.0, 1.0, 1.0, 0.5950292973172586, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129322838348, 924388.4206073722, 924388.4206073716, 210924.7069731395], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.91, 1.0, 0.5, 0.5120834907436851, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294398261886243, 0.2567745612798256, 0.25677456127982545, 0.31481299548229774], 
reward next is 0.6852, 
noisyNet noise sample is [array([2.5090232], dtype=float32), 0.15524966]. 
=============================================
[2019-03-25 23:10:47,439] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.9451242e-36 0.0000000e+00 3.3604812e-33 1.9547212e-27], sum to 1.0000
[2019-03-25 23:10:47,450] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0967
[2019-03-25 23:10:47,455] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5480108965071602, 6.911199999999999, 6.9112, 168.912956510431, 481582.4135163953, 481582.4135163959, 158428.004513142], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2941800.0000, 
sim time next is 2942400.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.548280013458453, 6.9112, 6.9112, 168.912956510431, 481818.9860315085, 481818.9860315085, 158465.0975638478], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44912196763225976, 0.0, 0.0, 0.8294399451523027, 0.13383860723097457, 0.13383860723097457, 0.23651507099081762], 
reward next is 0.7635, 
noisyNet noise sample is [array([0.51370275], dtype=float32), 0.6702489]. 
=============================================
[2019-03-25 23:10:50,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 3.3848160e-37 0.0000000e+00 8.8479312e-37 5.5785805e-35], sum to 1.0000
[2019-03-25 23:10:50,097] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8017
[2019-03-25 23:10:50,104] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5502619847414228, 6.9112, 6.9112, 168.912956510431, 483561.1139139077, 483561.1139139077, 158738.8843844109], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3025800.0000, 
sim time next is 3026400.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5487717060076643, 6.911199999999999, 6.9112, 168.912956510431, 482251.184449583, 482251.1844495836, 158532.9202644997], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4497215926922736, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13395866234710638, 0.13395866234710654, 0.23661629890223834], 
reward next is 0.7634, 
noisyNet noise sample is [array([-0.16104834], dtype=float32), -0.37193793]. 
=============================================
[2019-03-25 23:10:55,883] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4326262e-36 3.0915184e-37], sum to 1.0000
[2019-03-25 23:10:55,889] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6516
[2019-03-25 23:10:55,897] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.550442402474032, 6.9112, 6.9112, 168.912956510431, 483719.6979696542, 483719.6979696542, 158763.8596881012], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3019800.0000, 
sim time next is 3020400.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5554504683553412, 6.9112, 6.9112, 168.912956510431, 488121.714566459, 488121.714566459, 159460.6323573001], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45786642482358675, 0.0, 0.0, 0.8294399451523027, 0.13558936515734973, 0.13558936515734973, 0.23800094381686585], 
reward next is 0.7620, 
noisyNet noise sample is [array([-0.6205585], dtype=float32), 1.3337111]. 
=============================================
[2019-03-25 23:10:58,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:10:58,306] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4694
[2019-03-25 23:10:58,310] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5508399091689428, 6.911199999999999, 6.9112, 168.912956510431, 484069.1000402491, 484069.1000402498, 158818.9176736157], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3025200.0000, 
sim time next is 3025800.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5502619847414228, 6.9112, 6.9112, 168.912956510431, 483561.1139139077, 483561.1139139077, 158738.8843844109], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45153900578222295, 0.0, 0.0, 0.8294399451523027, 0.13432253164275215, 0.13432253164275215, 0.23692370803643417], 
reward next is 0.7631, 
noisyNet noise sample is [array([-1.65475], dtype=float32), -1.179448]. 
=============================================
[2019-03-25 23:11:05,176] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:11:05,185] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9851
[2019-03-25 23:11:05,190] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333333, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.911694584933789, 6.9112, 6.9112, 168.912956510431, 746565.3666367391, 746565.3666367391, 225920.2766871348], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3237000.0000, 
sim time next is 3237600.0000, 
raw observation next is [30.66666666666667, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9236445586492635, 6.9112, 6.9112, 168.912956510431, 754338.4574006329, 754338.4574006329, 228657.7050479987], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879939, 0.69, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9068836081088578, 0.0, 0.0, 0.8294399451523027, 0.2095384603890647, 0.2095384603890647, 0.34128015678805773], 
reward next is 0.6587, 
noisyNet noise sample is [array([-1.0530795], dtype=float32), -0.10584728]. 
=============================================
[2019-03-25 23:11:09,571] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:11:09,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6456
[2019-03-25 23:11:09,589] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.16666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7452493770422063, 6.9112, 6.9112, 168.912956510431, 632396.6356465198, 632396.6356465198, 191086.0165868445], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3297000.0000, 
sim time next is 3297600.0000, 
raw observation next is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7338571928123899, 6.9112, 6.9112, 168.912956510431, 624174.6248391629, 624174.6248391629, 188935.8480736673], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6754356009907193, 0.0, 0.0, 0.8294399451523027, 0.1733818402331008, 0.1733818402331008, 0.2819938030950258], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.7439593], dtype=float32), -0.49597988]. 
=============================================
[2019-03-25 23:11:10,277] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:11:10,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3528
[2019-03-25 23:11:10,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8147727513925705, 6.911200000000001, 6.9112, 168.912956510431, 680869.0270612417, 680869.0270612411, 204856.6839043165], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3183600.0000, 
sim time next is 3184200.0000, 
raw observation next is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8142927745571881, 6.911199999999999, 6.9112, 168.912956510431, 680493.1619666036, 680493.1619666043, 204756.6192453514], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7735277738502292, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18902587832405657, 0.18902587832405676, 0.3056068943960469], 
reward next is 0.6944, 
noisyNet noise sample is [array([0.20913708], dtype=float32), -0.4284831]. 
=============================================
[2019-03-25 23:11:11,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6160309e-37 0.0000000e+00], sum to 1.0000
[2019-03-25 23:11:11,303] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5742
[2019-03-25 23:11:11,310] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7991920013838036, 6.9112, 6.9112, 168.912956510431, 669582.8372130482, 669582.8372130482, 201660.8707739403], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3203400.0000, 
sim time next is 3204000.0000, 
raw observation next is [25.0, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7954500253434464, 6.911199999999999, 6.9112, 168.912956510431, 666848.7250758626, 666848.7250758633, 200901.8556278109], 
processed observation next is [0.0, 0.08695652173913043, 0.38388625592417064, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7505488113944468, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1852357569655174, 0.18523575696551758, 0.29985351586240433], 
reward next is 0.7001, 
noisyNet noise sample is [array([0.6217755], dtype=float32), 0.8168342]. 
=============================================
[2019-03-25 23:11:11,324] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.15165]
 [77.24443]
 [77.39568]
 [77.49618]
 [77.57734]], R is [[76.868927  ]
 [76.79924774]
 [76.72916412]
 [76.65883636]
 [76.58840179]].
[2019-03-25 23:11:28,232] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.98263299e-01 4.08519129e-31 1.11680975e-35 2.07131472e-33
 1.73677236e-03], sum to 1.0000
[2019-03-25 23:11:28,245] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4148
[2019-03-25 23:11:28,253] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 978291.5620029061 W.
[2019-03-25 23:11:28,261] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.83333333333334, 79.66666666666667, 1.0, 2.0, 0.3500092842108438, 1.0, 2.0, 0.3500092842108438, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 978291.5620029061, 978291.5620029061, 260258.7656023116], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3554400.0000, 
sim time next is 3555000.0000, 
raw observation next is [26.75, 80.0, 1.0, 2.0, 0.2304718161230914, 1.0, 2.0, 0.2304718161230914, 1.0, 1.0, 0.3862274455979214, 6.9112, 6.9112, 170.5573041426782, 966263.0930054747, 966263.0930054747, 277929.6378154776], 
processed observation next is [1.0, 0.13043478260869565, 0.4668246445497631, 0.8, 1.0, 1.0, 0.0728576097868571, 1.0, 1.0, 0.0728576097868571, 1.0, 0.5, 0.25149688487551386, 0.0, 0.0, 0.8375144448122397, 0.26840641472374294, 0.26840641472374294, 0.4148203549484741], 
reward next is 0.5852, 
noisyNet noise sample is [array([-1.8173718], dtype=float32), 0.7159165]. 
=============================================
[2019-03-25 23:11:28,276] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[42.060402]
 [42.152245]
 [42.963337]
 [42.642277]
 [42.852917]], R is [[42.25815582]
 [42.4471283 ]
 [42.62667465]
 [42.77981567]
 [42.92691803]].
[2019-03-25 23:11:29,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.0773065e-28 2.3622301e-33 6.0365134e-34 1.5070555e-27], sum to 1.0000
[2019-03-25 23:11:29,559] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5126
[2019-03-25 23:11:29,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.84769205989259, 6.911200000000001, 6.9112, 168.912956510431, 704297.670122568, 704297.6701225673, 211795.9650960776], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3544800.0000, 
sim time next is 3545400.0000, 
raw observation next is [28.0, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8385516569940723, 6.911199999999999, 6.9112, 168.912956510431, 697980.1569303558, 697980.1569303565, 209847.7852246131], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.7483333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8031117768220394, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19388337692509883, 0.19388337692509902, 0.3132056495889748], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.380573], dtype=float32), 0.25546145]. 
=============================================
[2019-03-25 23:11:34,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.8555893e-18 4.2277277e-32 2.1349792e-38 2.5512246e-29 1.0000000e+00], sum to 1.0000
[2019-03-25 23:11:34,502] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7661
[2019-03-25 23:11:34,512] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.318048218580434, 1.0, 2.0, 0.318048218580434, 1.0, 2.0, 0.5365058016924554, 6.911199999999999, 6.9112, 170.5573041426782, 1333659.3206053, 1333659.320605301, 310449.0489764144], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3637800.0000, 
sim time next is 3638400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.2984173628282316, 1.0, 2.0, 0.2984173628282316, 1.0, 2.0, 0.5025851265059486, 6.9112, 6.9112, 170.5573041426782, 1251293.969274351, 1251293.969274351, 302221.0225616197], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.15471971425088146, 1.0, 1.0, 0.15471971425088146, 1.0, 1.0, 0.3933964957389616, 0.0, 0.0, 0.8375144448122397, 0.3475816581317641, 0.3475816581317641, 0.45107615307704435], 
reward next is 0.5489, 
noisyNet noise sample is [array([-0.08932623], dtype=float32), -0.99323845]. 
=============================================
[2019-03-25 23:11:36,423] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 3.3247388e-34 0.0000000e+00 0.0000000e+00 1.4642745e-36], sum to 1.0000
[2019-03-25 23:11:36,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7666
[2019-03-25 23:11:36,439] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.16666666666666, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8118389912407364, 6.911200000000001, 6.9112, 168.912956510431, 678580.0330346901, 678580.0330346895, 204246.1234860654], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3549000.0000, 
sim time next is 3549600.0000, 
raw observation next is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8093225861878696, 6.9112, 6.9112, 168.912956510431, 677191.7475802012, 677191.7475802012, 203738.4366903196], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7674665685217921, 0.0, 0.0, 0.8294399451523027, 0.1881088187722781, 0.1881088187722781, 0.3040872189407755], 
reward next is 0.6959, 
noisyNet noise sample is [array([0.19733162], dtype=float32), -1.582653]. 
=============================================
[2019-03-25 23:11:50,634] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-25 23:11:50,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:11:50,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:11:50,637] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:11:50,638] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:11:50,640] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:11:50,641] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:11:50,643] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:11:50,644] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:11:50,645] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:11:50,645] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:11:50,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-25 23:11:50,690] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-25 23:11:50,691] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-25 23:11:50,710] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-25 23:11:50,732] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-25 23:12:30,233] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:12:30,235] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.53333333333333, 65.66666666666667, 1.0, 1.0, 0.6146515375744642, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128936046062, 934078.0349588544, 934078.0349588551, 212797.7118814239]
[2019-03-25 23:12:30,237] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:12:30,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 4.1342235e-31 1.7781423e-30 1.0721244e-28 3.9083689e-13], sampled 0.29702415053426556
[2019-03-25 23:12:30,241] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 934078.0349588544 W.
[2019-03-25 23:12:52,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:12:52,577] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.66666666666667, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9337430808239012, 6.911199999999999, 6.9112, 168.912956510431, 762325.4236424618, 762325.4236424625, 231062.8882719383]
[2019-03-25 23:12:52,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:12:52,582] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9185586e-34], sampled 0.2903286285244062
[2019-03-25 23:12:57,209] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:12:57,210] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.924200105, 59.4498918, 1.0, 2.0, 0.4499020035632162, 1.0, 2.0, 0.4499020035632162, 1.0, 2.0, 0.7813309953231622, 6.9112, 6.9112, 184.5923449428631, 1886917.144071138, 1886917.144071138, 386352.8977995312]
[2019-03-25 23:12:57,212] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:12:57,214] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3995725e-02 2.9766748e-26 5.0032688e-31 1.9970312e-30 9.7600424e-01], sampled 0.6664670115958935
[2019-03-25 23:13:12,495] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:13:12,497] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.21121243666667, 72.36129881333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9396074517356914, 6.911199999999999, 6.9112, 168.912956510431, 787545.3766789029, 787545.3766789036, 233222.6510573063]
[2019-03-25 23:13:12,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:13:12,502] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.819844e-33], sampled 0.9302126769083284
[2019-03-25 23:13:20,763] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:13:20,763] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.9, 59.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.046743217987938, 6.9112, 168.9120579300501, 924996.9490790435, 828838.4367460667, 254812.2603027982]
[2019-03-25 23:13:20,765] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:13:20,769] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 1.6406354e-36 9.2655604e-35 0.0000000e+00 6.1726464e-11], sampled 0.4666992649005558
[2019-03-25 23:13:20,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 924996.9490790435 W.
[2019-03-25 23:13:29,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:13:29,801] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.7013784734207615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 980197.2892790865, 980197.2892790865, 221144.1413560637]
[2019-03-25 23:13:29,801] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:13:29,805] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9817562e-01 7.1481150e-28 1.4605820e-30 6.8349477e-28 1.8243897e-03], sampled 0.6727647597206318
[2019-03-25 23:13:29,807] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 980197.2892790865 W.
[2019-03-25 23:13:47,701] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.52799875], dtype=float32), 0.25770313]
[2019-03-25 23:13:47,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7858397885880762, 6.9112, 6.9112, 168.912956510431, 660374.3298207412, 660374.3298207412, 198979.9049527768]
[2019-03-25 23:13:47,703] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:13:47,704] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.9661642e-36 9.6482989e-36 3.8216865e-35 1.3651632e-18], sampled 0.35725157707771804
[2019-03-25 23:14:00,186] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7552.4687 3137912836.6423 1526.0000
[2019-03-25 23:14:00,302] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7480.3184 3356773588.7179 1473.0000
[2019-03-25 23:14:00,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8055.3500 3030989587.9258 1060.0000
[2019-03-25 23:14:00,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8188.2348 2984265860.0762 920.0000
[2019-03-25 23:14:00,696] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7304.7188 3218385616.1103 1695.0000
[2019-03-25 23:14:01,711] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 525000, evaluation results [525000.0, 7480.318394750977, 3356773588.7179017, 1473.0, 7552.468703419245, 3137912836.6422925, 1526.0, 8188.234791240913, 2984265860.0761714, 920.0, 7304.718767977563, 3218385616.1103196, 1695.0, 8055.349979504198, 3030989587.925768, 1060.0]
[2019-03-25 23:14:02,821] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 7.2865233e-38 0.0000000e+00 3.0886650e-30 2.3767665e-24], sum to 1.0000
[2019-03-25 23:14:02,830] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2163
[2019-03-25 23:14:02,834] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 68.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.922994794479168, 6.9112, 168.9127040011421, 837171.8020519811, 828804.1814118023, 254812.2951430564], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3835800.0000, 
sim time next is 3836400.0000, 
raw observation next is [32.33333333333334, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977568215271402, 6.9112, 168.9124015182628, 875903.029229082, 828819.287799993, 254812.5594803671], 
processed observation next is [0.0, 0.391304347826087, 0.7314375987361774, 0.68, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006636821527140224, 0.0, 0.8294372198866804, 0.24330639700807835, 0.23022757994444248, 0.3803172529557718], 
reward next is 0.2878, 
noisyNet noise sample is [array([0.98853964], dtype=float32), -0.65660787]. 
=============================================
[2019-03-25 23:14:08,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.00000000e+00 4.42807660e-36 1.43576655e-36 1.78047948e-35
 1.31134702e-27], sum to 1.0000
[2019-03-25 23:14:08,319] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3511
[2019-03-25 23:14:08,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1001738.98692927 W.
[2019-03-25 23:14:08,332] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 79.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.154875946259902, 6.9112, 168.9115028233929, 1001738.98692927, 828868.3717336464, 254813.3248481862], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3970800.0000, 
sim time next is 3971400.0000, 
raw observation next is [30.83333333333334, 79.83333333333334, 1.0, 1.0, 0.3308875607099534, 1.0, 1.0, 0.3308875607099534, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 924822.4455142403, 924822.4455142403, 256139.0206493987], 
processed observation next is [0.0, 1.0, 0.6603475513428123, 0.7983333333333335, 1.0, 0.5, 0.19384043459030528, 1.0, 0.5, 0.19384043459030528, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2568951237539556, 0.2568951237539556, 0.3822970457453712], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1389338], dtype=float32), 0.26167277]. 
=============================================
[2019-03-25 23:14:11,552] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.2462201e-32 1.6157560e-34 1.7746696e-26], sum to 1.0000
[2019-03-25 23:14:11,561] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3474
[2019-03-25 23:14:11,570] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9733075776660621, 6.9112, 6.9112, 168.912956510431, 788553.3433048675, 788553.3433048675, 240491.4001559084], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3882600.0000, 
sim time next is 3883200.0000, 
raw observation next is [29.0, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9823848908041476, 6.9112, 6.9112, 168.912956510431, 794261.9989311914, 794261.9989311914, 242689.9879767903], 
processed observation next is [0.0, 0.9565217391304348, 0.5734597156398105, 0.8233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9785181595172531, 0.0, 0.0, 0.8294399451523027, 0.22062833303644205, 0.22062833303644205, 0.3622238626519258], 
reward next is 0.6378, 
noisyNet noise sample is [array([0.17540044], dtype=float32), -1.7206526]. 
=============================================
[2019-03-25 23:14:15,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.6618189e-28 6.4743019e-28 4.7724332e-27 7.0022273e-22], sum to 1.0000
[2019-03-25 23:14:15,595] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7331
[2019-03-25 23:14:15,603] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9320647248243821, 6.9112, 6.9112, 168.912956510431, 761081.2257639385, 761081.2257639385, 230665.1970330832], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4051200.0000, 
sim time next is 4051800.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9322752585081284, 6.9112, 6.9112, 168.912956510431, 761253.21176342, 761253.21176342, 230715.7950116245], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.917408851839181, 0.0, 0.0, 0.8294399451523027, 0.2114592254898389, 0.2114592254898389, 0.3443519328531709], 
reward next is 0.6556, 
noisyNet noise sample is [array([-0.0075739], dtype=float32), 0.8613847]. 
=============================================
[2019-03-25 23:14:17,034] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:14:17,046] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5987
[2019-03-25 23:14:17,051] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9320647248243816, 6.9112, 6.9112, 168.912956510431, 761081.2257639385, 761081.2257639385, 230665.1970330831], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4051200.0000, 
sim time next is 4051800.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9322752585081282, 6.9112, 6.9112, 168.912956510431, 761253.21176342, 761253.21176342, 230715.7950116244], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9174088518391808, 0.0, 0.0, 0.8294399451523027, 0.2114592254898389, 0.2114592254898389, 0.34435193285317073], 
reward next is 0.6556, 
noisyNet noise sample is [array([1.1458502], dtype=float32), -0.85946155]. 
=============================================
[2019-03-25 23:14:20,606] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.9648187e-28 2.4867721e-27 8.7174535e-26 1.0727108e-18], sum to 1.0000
[2019-03-25 23:14:20,619] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5170
[2019-03-25 23:14:20,630] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2810030.631499434 W.
[2019-03-25 23:14:20,634] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.16666666666666, 62.5, 1.0, 2.0, 1.004543353073735, 1.0, 2.0, 1.004543353073735, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2810030.631499434, 2810030.631499433, 531738.4017655618], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4018200.0000, 
sim time next is 4018800.0000, 
raw observation next is [33.33333333333334, 62.00000000000001, 1.0, 2.0, 0.5434401489538108, 1.0, 2.0, 0.5434401489538108, 1.0, 1.0, 0.943775820329252, 6.9112, 6.9112, 170.5573041426782, 2279781.843448465, 2279781.843448465, 446486.4857917544], 
processed observation next is [1.0, 0.5217391304347826, 0.7788309636650873, 0.6200000000000001, 1.0, 1.0, 0.44992789030579605, 1.0, 1.0, 0.44992789030579605, 1.0, 0.5, 0.931433927230795, 0.0, 0.0, 0.8375144448122397, 0.6332727342912403, 0.6332727342912403, 0.6663977399876931], 
reward next is 0.3336, 
noisyNet noise sample is [array([0.0027811], dtype=float32), 0.7509823]. 
=============================================
[2019-03-25 23:14:20,686] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0089284e-14 3.8548838e-18 8.6148256e-22 2.0506810e-19 1.0000000e+00], sum to 1.0000
[2019-03-25 23:14:20,694] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8580
[2019-03-25 23:14:20,701] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666666, 61.0, 1.0, 2.0, 0.5825189418562509, 1.0, 1.0, 0.5825189418562509, 1.0, 2.0, 1.011642760046494, 6.911199999999999, 6.9112, 170.5573041426782, 2443881.308738862, 2443881.308738862, 476899.2817009516], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4020000.0000, 
sim time next is 4020600.0000, 
raw observation next is [33.83333333333334, 60.5, 1.0, 2.0, 0.6065346772955361, 1.0, 2.0, 0.6065346772955361, 1.0, 2.0, 1.03, 6.937449392442856, 6.9112, 170.5573041426782, 2544738.75304667, 2525935.245482836, 490360.6393168592], 
processed observation next is [1.0, 0.5217391304347826, 0.8025276461295423, 0.605, 1.0, 1.0, 0.5259453943319712, 1.0, 1.0, 0.5259453943319712, 1.0, 1.0, 1.0365853658536586, 0.002624939244285596, 0.0, 0.8375144448122397, 0.7068718758462972, 0.7016486793007878, 0.7318815512191928], 
reward next is 0.1369, 
noisyNet noise sample is [array([0.29725564], dtype=float32), -1.7680689]. 
=============================================
[2019-03-25 23:14:25,074] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0931926e-30 4.1007544e-30], sum to 1.0000
[2019-03-25 23:14:25,084] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5647
[2019-03-25 23:14:25,093] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1179161.336412473 W.
[2019-03-25 23:14:25,098] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 88.16666666666667, 1.0, 2.0, 0.2812241216728094, 1.0, 2.0, 0.2812241216728094, 1.0, 2.0, 0.4834103227122038, 6.9112, 6.9112, 170.5573041426782, 1179161.336412473, 1179161.336412473, 296444.0412730573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4075800.0000, 
sim time next is 4076400.0000, 
raw observation next is [27.13333333333334, 88.33333333333334, 1.0, 2.0, 0.7566986459081482, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1057547.37223523, 1057547.372235231, 233559.7817038045], 
processed observation next is [1.0, 0.17391304347826086, 0.4849921011058455, 0.8833333333333334, 1.0, 1.0, 0.706865838443552, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29376315895423055, 0.2937631589542309, 0.34859668911015596], 
reward next is 0.6514, 
noisyNet noise sample is [array([1.9779544], dtype=float32), 0.49403065]. 
=============================================
[2019-03-25 23:14:33,873] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1414596e-32 0.0000000e+00 0.0000000e+00 4.3627819e-38 1.0000000e+00], sum to 1.0000
[2019-03-25 23:14:33,886] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7721
[2019-03-25 23:14:33,892] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 82.5, 1.0, 2.0, 0.4126651964402074, 1.0, 2.0, 0.4126651964402074, 1.0, 2.0, 0.7166629757507865, 6.911200000000001, 6.9112, 170.5573041426782, 1730733.384770215, 1730733.384770215, 360013.0501132194], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4263000.0000, 
sim time next is 4263600.0000, 
raw observation next is [31.66666666666667, 81.0, 1.0, 2.0, 0.5970502266952128, 1.0, 2.0, 0.5970502266952128, 1.0, 2.0, 1.03, 6.918932770196893, 6.9112, 170.5573041426782, 2504906.501801873, 2499367.203960858, 486913.4876479125], 
processed observation next is [1.0, 0.34782608695652173, 0.6998420221169038, 0.81, 1.0, 1.0, 0.514518345415919, 1.0, 1.0, 0.514518345415919, 1.0, 1.0, 1.0365853658536586, 0.0007732770196892779, 0.0, 0.8375144448122397, 0.6958073616116314, 0.694268667766905, 0.7267365487282277], 
reward next is 0.2346, 
noisyNet noise sample is [array([0.91776687], dtype=float32), 1.3203273]. 
=============================================
[2019-03-25 23:14:34,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.3862887e-38], sum to 1.0000
[2019-03-25 23:14:34,762] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8652
[2019-03-25 23:14:34,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 885312.4325484029 W.
[2019-03-25 23:14:34,785] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.16666666666667, 59.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990826384818758, 6.9112, 168.9122887987933, 885312.4325484029, 828822.9579326616, 254812.5909925696], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4215000.0000, 
sim time next is 4215600.0000, 
raw observation next is [34.0, 60.0, 1.0, 1.0, 0.3001815752844129, 1.0, 1.0, 0.3001815752844129, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 838966.4157020889, 838966.4157020889, 249923.8578591307], 
processed observation next is [1.0, 0.8260869565217391, 0.8104265402843602, 0.6, 1.0, 0.5, 0.1568452714270035, 1.0, 0.5, 0.1568452714270035, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23304622658391358, 0.23304622658391358, 0.37302068337183686], 
reward next is 0.6270, 
noisyNet noise sample is [array([-0.0015938], dtype=float32), -0.4261815]. 
=============================================
[2019-03-25 23:14:38,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.3537735e-35 4.4637649e-34 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-25 23:14:38,548] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7215
[2019-03-25 23:14:38,559] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666666, 72.33333333333333, 1.0, 2.0, 0.8126911703436356, 1.0, 2.0, 0.7269356246860804, 1.0, 2.0, 1.03, 7.005106619432897, 6.9112, 170.5573041426782, 3050502.163007095, 2983233.031630259, 559287.0410992127], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4268400.0000, 
sim time next is 4269000.0000, 
raw observation next is [33.83333333333334, 71.66666666666667, 1.0, 2.0, 0.835449493390626, 1.0, 2.0, 0.7383147862095756, 1.0, 2.0, 1.03, 7.005108414654035, 6.9112, 170.5573041426782, 3098312.741766052, 3031042.324399291, 567526.338423751], 
processed observation next is [1.0, 0.391304347826087, 0.8025276461295423, 0.7166666666666667, 1.0, 1.0, 0.8017463775790675, 1.0, 1.0, 0.6847166098910549, 1.0, 1.0, 1.0365853658536586, 0.009390841465403543, 0.0, 0.8375144448122397, 0.8606424282683477, 0.8419562012220253, 0.8470542364533596], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9767481], dtype=float32), 0.86883396]. 
=============================================
[2019-03-25 23:14:38,590] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[27.268309]
 [27.3708  ]
 [27.40169 ]
 [27.37302 ]
 [27.380232]], R is [[27.0488205 ]
 [26.77833176]
 [26.51054955]
 [26.24544525]
 [25.98299026]].
[2019-03-25 23:14:40,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0293217e-32 0.0000000e+00 8.9783359e-34 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-25 23:14:40,032] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8706
[2019-03-25 23:14:40,037] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.5, 55.5, 1.0, 2.0, 0.6624640488104386, 1.0, 2.0, 0.6518220639194819, 1.0, 2.0, 1.03, 7.005094772667835, 6.9112, 170.5573041426782, 2734951.630137101, 2667690.985079975, 509737.4828936192], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [37.66666666666666, 55.0, 1.0, 2.0, 0.7548039066809973, 1.0, 2.0, 0.6979919928547613, 1.0, 2.0, 1.03, 7.005102053791004, 6.9112, 170.5573041426782, 2928901.117114819, 2861635.256292991, 539199.9962735024], 
processed observation next is [1.0, 0.5217391304347826, 0.9842022116903629, 0.55, 1.0, 1.0, 0.7045830200975871, 1.0, 1.0, 0.6361349311503148, 1.0, 1.0, 1.0365853658536586, 0.009390205379100357, 0.0, 0.8375144448122397, 0.8135836436430053, 0.7948986823036086, 0.8047761138410483], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05930377], dtype=float32), 0.7198585]. 
=============================================
[2019-03-25 23:14:45,657] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6682787e-09 2.4436335e-30 1.1239666e-21 4.9712673e-32 1.0000000e+00], sum to 1.0000
[2019-03-25 23:14:45,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5511
[2019-03-25 23:14:45,675] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.33333333333334, 58.00000000000001, 1.0, 2.0, 0.9995055440856473, 1.0, 2.0, 0.8203428115570861, 1.0, 2.0, 1.03, 7.005121359782438, 6.9112, 170.5573041426782, 3443015.092709164, 3375735.402220353, 632641.5840765194], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4360800.0000, 
sim time next is 4361400.0000, 
raw observation next is [36.5, 57.0, 1.0, 2.0, 0.9941633512114745, 1.0, 2.0, 0.8176717151199997, 1.0, 2.0, 1.03, 7.00512093813489, 6.9112, 170.5573041426782, 3431788.976093384, 3364509.587647864, 630362.6344985653], 
processed observation next is [1.0, 0.4782608695652174, 0.9289099526066351, 0.57, 1.0, 1.0, 0.9929678930258728, 1.0, 1.0, 0.7803273676144574, 1.0, 1.0, 1.0365853658536586, 0.009392093813488956, 0.0, 0.8375144448122397, 0.9532747155814956, 0.9345859965688511, 0.9408397529829333], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38504675], dtype=float32), -1.3951961]. 
=============================================
[2019-03-25 23:14:49,364] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0531507e-02 2.4135187e-32 7.6320888e-24 2.9512991e-26 9.6946847e-01], sum to 1.0000
[2019-03-25 23:14:49,379] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6558
[2019-03-25 23:14:49,385] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6315719913750588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 882600.1162009859, 882600.1162009866, 206775.9656635359], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4442400.0000, 
sim time next is 4443000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.2141554308884647, 1.0, 1.0, 0.2141554308884647, 1.0, 1.0, 0.3719171611700404, 6.911199999999999, 6.9112, 170.5573041426782, 897827.3035348506, 897827.3035348513, 273900.4896022023], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.05319931432345143, 1.0, 0.5, 0.05319931432345143, 1.0, 0.5, 0.23404531850004928, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.24939647320412517, 0.24939647320412536, 0.40880670089880944], 
reward next is 0.5912, 
noisyNet noise sample is [array([-0.97921497], dtype=float32), -0.35023052]. 
=============================================
[2019-03-25 23:14:49,399] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[48.1526  ]
 [47.62441 ]
 [47.660145]
 [47.326324]
 [46.946426]], R is [[47.90878677]
 [48.12107849]
 [48.33187103]
 [48.51047516]
 [48.68703461]].
[2019-03-25 23:14:49,874] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 9.6705547e-36 1.8568522e-27], sum to 1.0000
[2019-03-25 23:14:49,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8700
[2019-03-25 23:14:49,889] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9466050124177916, 6.911200000000001, 6.9112, 168.912956510431, 771862.5247446579, 771862.5247446572, 234136.0186587247], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4547400.0000, 
sim time next is 4548000.0000, 
raw observation next is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9263311211305537, 6.911199999999999, 6.9112, 168.912956510431, 755121.7445071979, 755121.7445071986, 229232.542985943], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9101599038177483, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2097560401408883, 0.2097560401408885, 0.3421381238596164], 
reward next is 0.6579, 
noisyNet noise sample is [array([3.1315548], dtype=float32), 2.0824742]. 
=============================================
[2019-03-25 23:14:49,905] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[55.755512]
 [55.678017]
 [55.48743 ]
 [55.34739 ]
 [55.203457]], R is [[56.04502106]
 [56.13511276]
 [56.23419571]
 [56.33439636]
 [56.43536377]].
[2019-03-25 23:14:59,193] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:14:59,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4370
[2019-03-25 23:14:59,209] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9263311211305537, 6.911199999999999, 6.9112, 168.912956510431, 755121.7445071979, 755121.7445071986, 229232.542985943], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4548000.0000, 
sim time next is 4548600.0000, 
raw observation next is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9241478934675355, 6.911200000000001, 6.9112, 168.912956510431, 753282.8700948237, 753282.8700948231, 228709.3982364458], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.90749743105797, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20924524169300657, 0.2092452416930064, 0.34135731080066534], 
reward next is 0.6586, 
noisyNet noise sample is [array([-0.05998086], dtype=float32), -0.001778273]. 
=============================================
[2019-03-25 23:14:59,995] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:15:00,003] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1315
[2019-03-25 23:15:00,012] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.82100626558556, 6.911200000000001, 6.9112, 168.912956510431, 686120.0951298968, 686120.0951298961, 206171.2837380119], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4664400.0000, 
sim time next is 4665000.0000, 
raw observation next is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8213183361479504, 6.911200000000001, 6.9112, 168.912956510431, 686377.8396673821, 686377.8396673815, 206237.2415886644], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7820955318877444, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19066051101871725, 0.1906605110187171, 0.30781677849054384], 
reward next is 0.6922, 
noisyNet noise sample is [array([-0.43449455], dtype=float32), 0.9495866]. 
=============================================
[2019-03-25 23:15:00,026] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.042446]
 [60.1874  ]
 [60.34716 ]
 [60.52142 ]
 [60.6969  ]], R is [[59.98625183]
 [60.0786705 ]
 [60.17004013]
 [60.26025391]
 [60.34938812]].
[2019-03-25 23:15:00,859] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9607496e-33], sum to 1.0000
[2019-03-25 23:15:00,871] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3699
[2019-03-25 23:15:00,878] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9425550480486283, 6.911199999999999, 6.9112, 168.912956510431, 769650.4593724336, 769650.4593724343, 233200.8164048297], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4576200.0000, 
sim time next is 4576800.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9402137125182877, 6.911200000000001, 6.9112, 168.912956510431, 767737.7933325525, 767737.793332552, 232632.32568753], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9270898933149849, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21326049814793127, 0.2132604981479311, 0.3472124263992985], 
reward next is 0.6528, 
noisyNet noise sample is [array([-0.26200712], dtype=float32), 0.83855194]. 
=============================================
[2019-03-25 23:15:07,120] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 3.7740766e-37 1.5168948e-35 1.1896801e-35 1.2543563e-17], sum to 1.0000
[2019-03-25 23:15:07,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0639
[2019-03-25 23:15:07,138] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.75, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8045296407145317, 6.9112, 6.9112, 168.912956510431, 675281.3501446685, 675281.3501446685, 202790.3333543635], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4660200.0000, 
sim time next is 4660800.0000, 
raw observation next is [24.83333333333333, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8087106111199616, 6.9112, 6.9112, 168.912956510431, 678020.8288486514, 678020.8288486514, 203642.301133703], 
processed observation next is [1.0, 0.9565217391304348, 0.3759873617693521, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7667202574633678, 0.0, 0.0, 0.8294399451523027, 0.1883391191246254, 0.1883391191246254, 0.30394373303537764], 
reward next is 0.6961, 
noisyNet noise sample is [array([1.3553419], dtype=float32), -0.43663144]. 
=============================================
[2019-03-25 23:15:07,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.1755802e-02 9.5987305e-20 1.5309002e-23 1.5117385e-22 9.3824422e-01], sum to 1.0000
[2019-03-25 23:15:07,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3612
[2019-03-25 23:15:07,835] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.9845104471968117, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00132465481812, 6.9112, 168.9123271110938, 2273321.775289014, 2209384.481552281, 458750.1490036234], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4699200.0000, 
sim time next is 4699800.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.5436863382676222, 1.0, 1.0, 0.5436863382676222, 1.0, 2.0, 0.9442033697512925, 6.911199999999999, 6.9112, 170.5573041426782, 2280815.572898317, 2280815.572898318, 446670.6337547653], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.75, 1.0, 1.0, 0.4502245039368942, 1.0, 0.5, 0.4502245039368942, 1.0, 1.0, 0.9319553289649907, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6335598813606437, 0.6335598813606439, 0.6666725876936795], 
reward next is 0.3333, 
noisyNet noise sample is [array([-1.162814], dtype=float32), -0.0735518]. 
=============================================
[2019-03-25 23:15:09,268] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-25 23:15:09,271] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:15:09,272] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:15:09,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:15:09,274] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:15:09,274] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:15:09,275] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:15:09,277] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:15:09,277] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:15:09,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:15:09,279] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:15:09,306] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-25 23:15:09,306] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-25 23:15:09,345] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-25 23:15:09,346] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-25 23:15:09,366] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-25 23:15:13,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:15:13,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.85945578, 87.00282171500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4643130848418585, 6.911200000000001, 6.9112, 168.912956510431, 415203.8774983846, 415203.877498384, 147498.7984871738]
[2019-03-25 23:15:13,675] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:15:13,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.14797606663391427
[2019-03-25 23:15:31,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:15:31,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.78333333333333, 36.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5291836797017422, 6.911199999999999, 6.9112, 168.912956510431, 476832.4755097572, 476832.4755097579, 155185.4687072539]
[2019-03-25 23:15:31,400] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:15:31,402] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2692012519855803
[2019-03-25 23:15:51,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:15:51,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8078538318308207, 6.911199999999999, 6.9112, 168.912956510431, 679744.3908315484, 679744.3908315491, 203514.8726854725]
[2019-03-25 23:15:51,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:15:51,563] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5891621025675983
[2019-03-25 23:15:54,619] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:15:54,619] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.638223755, 89.72200713500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7067625773976163, 6.911200000000001, 6.9112, 168.912956510431, 604504.9235514172, 604504.9235514165, 183947.7163622339]
[2019-03-25 23:15:54,619] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:15:54,620] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7080453811234206
[2019-03-25 23:15:57,585] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:15:57,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.81034191, 95.07967445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5128900987157221, 6.911199999999999, 6.9112, 168.912956510431, 453838.9551178614, 453838.9551178621, 153638.4945838187]
[2019-03-25 23:15:57,587] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:15:57,589] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7192983386307523
[2019-03-25 23:16:01,051] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:16:01,052] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333334, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8386030420796837, 6.9112, 6.9112, 168.912956510431, 696310.3400400049, 696310.3400400049, 209807.531328028]
[2019-03-25 23:16:01,053] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:16:01,056] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6655189831751389
[2019-03-25 23:16:08,555] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:16:08,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.8621371234218504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 1204989.758999974, 1204989.758999973, 259731.2881968285]
[2019-03-25 23:16:08,560] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:16:08,563] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 1.1402624e-33 3.0029896e-32 6.8192095e-28 5.6904533e-20], sampled 0.17412769147421836
[2019-03-25 23:16:08,563] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1204989.758999974 W.
[2019-03-25 23:16:34,240] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:16:34,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.44915396, 56.34781365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8957468185482157, 6.9112, 6.9112, 168.912956510431, 737417.414863517, 737417.414863517, 222367.7891689141]
[2019-03-25 23:16:34,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:16:34,243] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5664538348122335
[2019-03-25 23:16:35,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:16:35,344] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.60000000000001, 61.66666666666667, 1.0, 2.0, 0.6140313855964235, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.952468615046874, 6.9112, 168.9126252355042, 1716849.447717812, 1687572.129266729, 367194.3291379897]
[2019-03-25 23:16:35,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:16:35,353] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0931989e-37 1.4072914e-23], sampled 0.44910033214820144
[2019-03-25 23:16:35,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1716849.447717812 W.
[2019-03-25 23:17:05,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:17:05,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.40000000000001, 73.5, 1.0, 2.0, 0.8829044756675694, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986706683963, 6.9112, 168.9123159659693, 2131107.772918255, 2063863.075893399, 429376.3734437163]
[2019-03-25 23:17:05,129] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:17:05,133] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 8.1023624e-37 6.5695293e-38 4.7264725e-33 1.2491263e-19], sampled 0.40942298377120834
[2019-03-25 23:17:05,136] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2131107.772918255 W.
[2019-03-25 23:17:09,681] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.5405701], dtype=float32), 0.25388163]
[2019-03-25 23:17:09,682] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.849128485, 75.70457160666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.71508807959021, 6.9112, 6.9112, 168.912956510431, 611504.495160901, 611504.495160901, 185465.101097362]
[2019-03-25 23:17:09,682] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:17:09,685] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3550216224526682
[2019-03-25 23:17:17,445] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.5737 3106562930.8887 2022.0000
[2019-03-25 23:17:18,155] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.6889 3319746086.7390 2143.0000
[2019-03-25 23:17:18,234] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7028.4335 3185344572.7566 2464.0000
[2019-03-25 23:17:18,529] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.7901 2938315371.3977 1378.0000
[2019-03-25 23:17:18,613] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.5298 2989488929.6114 1565.0000
[2019-03-25 23:17:19,628] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 550000, evaluation results [550000.0, 7286.68886113443, 3319746086.7390456, 2143.0, 7347.573670903205, 3106562930.888707, 2022.0, 8060.790148210282, 2938315371.3977036, 1378.0, 7028.433469164596, 3185344572.756556, 2464.0, 7923.529750336246, 2989488929.611378, 1565.0]
[2019-03-25 23:17:26,248] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8725204e-01 4.1854491e-21 5.5076828e-18 2.0011979e-26 7.1274787e-01], sum to 1.0000
[2019-03-25 23:17:26,256] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2750
[2019-03-25 23:17:26,268] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2299398.720125258 W.
[2019-03-25 23:17:26,274] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5481119995067998, 1.0, 2.0, 0.5481119995067998, 1.0, 2.0, 0.94998396203869, 6.911199999999999, 6.9112, 170.5573041426782, 2299398.720125258, 2299398.720125259, 449630.3664006292], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4791000.0000, 
sim time next is 4791600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.488505021064688, 1.0, 2.0, 0.488505021064688, 1.0, 2.0, 0.8460855245881957, 6.9112, 6.9112, 170.5573041426782, 2049112.628355818, 2049112.628355818, 406886.3533702439], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3837409892345639, 1.0, 1.0, 0.3837409892345639, 1.0, 1.0, 0.8122994202295071, 0.0, 0.0, 0.8375144448122397, 0.5691979523210606, 0.5691979523210606, 0.6072930647317073], 
reward next is 0.3927, 
noisyNet noise sample is [array([0.61584723], dtype=float32), -0.62068665]. 
=============================================
[2019-03-25 23:17:37,496] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:17:37,505] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7995
[2019-03-25 23:17:37,512] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666666, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8666421691131913, 6.911199999999999, 6.9112, 168.912956510431, 716029.1678607106, 716029.1678607112, 215851.7565001187], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5049600.0000, 
sim time next is 5050200.0000, 
raw observation next is [30.83333333333334, 63.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8707271189811538, 6.9112, 6.9112, 168.912956510431, 718741.8941740311, 718741.8941740311, 216742.6666820238], 
processed observation next is [0.0, 0.43478260869565216, 0.6603475513428123, 0.635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8423501450989681, 0.0, 0.0, 0.8294399451523027, 0.19965052615945308, 0.19965052615945308, 0.3234965174358564], 
reward next is 0.6765, 
noisyNet noise sample is [array([-0.48221645], dtype=float32), -1.4284074]. 
=============================================
[2019-03-25 23:17:42,486] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:17:42,499] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9245
[2019-03-25 23:17:42,504] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.5, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8866222869409646, 6.9112, 6.9112, 168.912956510431, 729520.7607885478, 729520.7607885478, 220255.0436527824], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5067000.0000, 
sim time next is 5067600.0000, 
raw observation next is [31.33333333333333, 61.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8847760566848739, 6.9112, 6.9112, 168.912956510431, 728275.8748730033, 728275.8748730033, 219844.247040162], 
processed observation next is [0.0, 0.6521739130434783, 0.6840442338072668, 0.6166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8594829959571633, 0.0, 0.0, 0.8294399451523027, 0.20229885413138982, 0.20229885413138982, 0.3281257418509881], 
reward next is 0.6719, 
noisyNet noise sample is [array([1.4704417], dtype=float32), 0.9729384]. 
=============================================
[2019-03-25 23:17:43,811] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6952863e-38], sum to 1.0000
[2019-03-25 23:17:43,822] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6634
[2019-03-25 23:17:43,830] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7926477907446777, 6.911199999999999, 6.9112, 168.912956510431, 665815.2861503358, 665815.2861503364, 200357.3155517029], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5017200.0000, 
sim time next is 5017800.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7919700897199681, 6.911200000000001, 6.9112, 168.912956510431, 665248.7122957862, 665248.7122957857, 200219.1338052814], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7463049874633756, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18479130897105173, 0.18479130897105156, 0.29883452806758415], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.82678664], dtype=float32), -1.0024633]. 
=============================================
[2019-03-25 23:17:53,773] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:17:53,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2256
[2019-03-25 23:17:53,790] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8824089782422425, 6.911199999999999, 6.9112, 168.912956510431, 728791.3441453838, 728791.3441453844, 219397.5507936243], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5172000.0000, 
sim time next is 5172600.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8830968301573193, 6.911199999999999, 6.9112, 168.912956510431, 729360.9876476685, 729360.9876476692, 219554.2276396884], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8574351587284381, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20260027434657457, 0.20260027434657477, 0.3276928770741618], 
reward next is 0.6723, 
noisyNet noise sample is [array([-0.2667013], dtype=float32), 0.09092539]. 
=============================================
[2019-03-25 23:18:09,685] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.6964617e-15 6.7979189e-26 1.7240211e-21 5.8289830e-33 1.0000000e+00], sum to 1.0000
[2019-03-25 23:18:09,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2632
[2019-03-25 23:18:09,709] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.98333333333333, 72.16666666666667, 1.0, 2.0, 0.4538163578551913, 1.0, 2.0, 0.4538163578551913, 1.0, 2.0, 0.7881289342315774, 6.9112, 6.9112, 170.5573041426782, 1903476.205951727, 1903476.205951727, 384680.2338412761], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5386200.0000, 
sim time next is 5386800.0000, 
raw observation next is [32.16666666666667, 71.33333333333334, 1.0, 2.0, 0.568305699706479, 1.0, 2.0, 0.568305699706479, 1.0, 2.0, 0.9869590588233456, 6.9112, 6.9112, 170.5573041426782, 2384194.642213326, 2384194.642213326, 465595.4064907383], 
processed observation next is [1.0, 0.34782608695652173, 0.7235387045813588, 0.7133333333333334, 1.0, 1.0, 0.47988638518852894, 1.0, 1.0, 0.47988638518852894, 1.0, 1.0, 0.9840964131992017, 0.0, 0.0, 0.8375144448122397, 0.6622762895037017, 0.6622762895037017, 0.6949185171503557], 
reward next is 0.3051, 
noisyNet noise sample is [array([0.9745502], dtype=float32), 1.3630136]. 
=============================================
[2019-03-25 23:18:14,119] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2790477e-24], sum to 1.0000
[2019-03-25 23:18:14,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6425
[2019-03-25 23:18:14,137] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9622317087754293, 6.9112, 6.9112, 168.912956510431, 782370.686174924, 782370.686174924, 237873.2432887002], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5524200.0000, 
sim time next is 5524800.0000, 
raw observation next is [27.86666666666667, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9546174338318874, 6.9112, 6.9112, 168.912956510431, 776888.0394928299, 776888.0394928299, 236027.10248622], 
processed observation next is [1.0, 0.9565217391304348, 0.519747235387046, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9446554071120578, 0.0, 0.0, 0.8294399451523027, 0.21580223319245276, 0.21580223319245276, 0.3522792574421194], 
reward next is 0.6477, 
noisyNet noise sample is [array([-0.35595888], dtype=float32), 1.3533002]. 
=============================================
[2019-03-25 23:18:17,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 9.0701667e-22 3.6383457e-18 7.9712012e-32 2.3147743e-10], sum to 1.0000
[2019-03-25 23:18:17,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5485
[2019-03-25 23:18:17,718] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2812684.59601527 W.
[2019-03-25 23:18:17,724] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.33333333333334, 47.66666666666666, 1.0, 2.0, 1.005491038261819, 1.0, 2.0, 1.005491038261819, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2812684.59601527, 2812684.59601527, 532311.766780961], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5496600.0000, 
sim time next is 5497200.0000, 
raw observation next is [36.2, 48.0, 1.0, 2.0, 0.712265390522252, 1.0, 2.0, 0.6767227347753886, 1.0, 1.0, 1.03, 7.005098699290533, 6.9112, 170.5573041426782, 2839550.006943228, 2772286.549086539, 525230.7641377436], 
processed observation next is [1.0, 0.6521739130434783, 0.9146919431279622, 0.48, 1.0, 1.0, 0.6533317958099422, 1.0, 1.0, 0.6105093190064922, 1.0, 0.5, 1.0365853658536586, 0.00938986992905333, 0.0, 0.8375144448122397, 0.7887638908175634, 0.770079596968483, 0.7839265136384234], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9639429], dtype=float32), 0.1253575]. 
=============================================
[2019-03-25 23:18:26,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:18:26,722] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8150
[2019-03-25 23:18:26,732] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.18333333333334, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9462242342603077, 6.911199999999999, 6.9112, 168.912956510431, 771574.7132101951, 771574.7132101957, 234044.0881316839], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5608200.0000, 
sim time next is 5608800.0000, 
raw observation next is [27.1, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.940325605447072, 6.9112, 6.9112, 168.912956510431, 767477.3544108379, 767477.3544108379, 232643.0270882838], 
processed observation next is [1.0, 0.9565217391304348, 0.4834123222748816, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9272263481061852, 0.0, 0.0, 0.8294399451523027, 0.21318815400301053, 0.21318815400301053, 0.34722839863922955], 
reward next is 0.6528, 
noisyNet noise sample is [array([-0.40485415], dtype=float32), -0.17799969]. 
=============================================
[2019-03-25 23:18:27,170] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-25 23:18:27,173] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:18:27,174] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:18:27,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:18:27,176] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:18:27,177] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:18:27,177] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:18:27,174] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:18:27,180] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:18:27,180] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:18:27,180] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:18:27,202] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-25 23:18:27,202] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-25 23:18:27,245] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-25 23:18:27,263] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-25 23:18:27,281] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-25 23:18:29,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:18:29,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.68533607666667, 85.23911025166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7978212348808295, 6.911199999999999, 6.9112, 168.912956510431, 672316.3232438866, 672316.3232438873, 201458.2684016914]
[2019-03-25 23:18:29,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:18:29,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3397045846686131
[2019-03-25 23:19:57,606] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:19:57,607] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.95, 67.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.026683761234423, 6.9112, 168.9120736037022, 910760.6106057201, 828832.8839795428, 254812.0111365749]
[2019-03-25 23:19:57,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:19:57,613] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7222312293408194
[2019-03-25 23:19:57,616] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 910760.6106057201 W.
[2019-03-25 23:20:12,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:20:12,386] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.06666666666667, 90.33333333333334, 1.0, 2.0, 0.607345095934616, 0.0, 2.0, 0.0, 1.0, 1.0, 1.025574969582594, 6.911200000000001, 6.9112, 168.912310198648, 1698139.434517809, 1698139.434517808, 365891.9920051275]
[2019-03-25 23:20:12,388] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:20:12,391] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.1644287e-35 4.1819226e-37 7.2402830e-22], sampled 0.18421558287755113
[2019-03-25 23:20:12,393] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1698139.434517809 W.
[2019-03-25 23:20:12,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:20:12,967] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.29700256333333, 63.85962017666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9552954763764349, 6.9112, 6.9112, 168.912956510431, 818626.9497492898, 818626.9497492898, 237326.1074143153]
[2019-03-25 23:20:12,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:20:12,975] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3967989246844412
[2019-03-25 23:20:16,881] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:20:16,883] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.28022544833334, 83.86774487333335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6060626050446645, 6.911199999999999, 6.9112, 168.912956510431, 527822.5173449778, 527822.5173449784, 167005.0992319539]
[2019-03-25 23:20:16,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:20:16,886] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.39037477326383896
[2019-03-25 23:20:33,722] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.50658053], dtype=float32), 0.30195373]
[2019-03-25 23:20:33,725] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.36223138333333, 77.29852527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9082155405373621, 6.911200000000001, 6.9112, 168.912956510431, 763281.9688783541, 763281.9688783534, 225754.0650683194]
[2019-03-25 23:20:33,726] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:20:33,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.741746047971286
[2019-03-25 23:20:36,193] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.1561 2989509937.5631 1566.0000
[2019-03-25 23:20:36,803] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.1639 3185281672.3323 2464.0000
[2019-03-25 23:20:36,945] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8059.7378 2938022170.9601 1381.0000
[2019-03-25 23:20:37,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.7088 3106163224.7300 2014.0000
[2019-03-25 23:20:37,120] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4826 3319658082.8562 2143.0000
[2019-03-25 23:20:38,138] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 575000, evaluation results [575000.0, 7287.48261098085, 3319658082.856155, 2143.0, 7348.708814693959, 3106163224.7300324, 2014.0, 8059.737782444199, 2938022170.9600596, 1381.0, 7029.16391117388, 3185281672.3322916, 2464.0, 7923.156098041422, 2989509937.563126, 1566.0]
[2019-03-25 23:20:47,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9989631e-08 2.1023509e-32 1.9545265e-28 4.0329166e-29 1.0000000e+00], sum to 1.0000
[2019-03-25 23:20:47,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1500
[2019-03-25 23:20:47,577] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.95, 94.5, 1.0, 2.0, 0.3198144821540113, 1.0, 2.0, 0.3198144821540113, 1.0, 2.0, 0.5482906725752902, 6.9112, 6.9112, 170.5573041426782, 1341070.358281401, 1341070.358281401, 312183.9976154029], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5884200.0000, 
sim time next is 5884800.0000, 
raw observation next is [25.93333333333333, 94.66666666666666, 1.0, 2.0, 0.286005517339525, 1.0, 2.0, 0.286005517339525, 1.0, 2.0, 0.4895334198528673, 6.911200000000001, 6.9112, 170.5573041426782, 1199220.746927168, 1199220.746927167, 298106.7651744994], 
processed observation next is [1.0, 0.08695652173913043, 0.42812006319115314, 0.9466666666666665, 1.0, 1.0, 0.13976568354159638, 1.0, 1.0, 0.13976568354159638, 1.0, 1.0, 0.37747978030837476, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.33311687414643554, 0.3331168741464353, 0.4449354704097006], 
reward next is 0.5551, 
noisyNet noise sample is [array([0.25130397], dtype=float32), 1.3544024]. 
=============================================
[2019-03-25 23:20:50,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2530911e-07 3.6535781e-22 2.0648298e-21 2.6544801e-30 9.9999905e-01], sum to 1.0000
[2019-03-25 23:20:50,134] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5579
[2019-03-25 23:20:50,142] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.43333333333333, 71.66666666666666, 1.0, 2.0, 0.5861530923820731, 1.0, 2.0, 0.5861530923820731, 1.0, 2.0, 1.017954077677903, 6.9112, 6.9112, 170.5573041426782, 2459142.910091972, 2459142.910091972, 479834.1580479696], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5917800.0000, 
sim time next is 5918400.0000, 
raw observation next is [31.3, 72.0, 1.0, 2.0, 0.5924257428893814, 1.0, 2.0, 0.5924257428893814, 1.0, 2.0, 1.028847597211876, 6.911200000000001, 6.9112, 170.5573041426782, 2485485.320645286, 2485485.320645285, 484942.6930322262], 
processed observation next is [1.0, 0.5217391304347826, 0.6824644549763034, 0.72, 1.0, 1.0, 0.5089466781799776, 1.0, 1.0, 0.5089466781799776, 1.0, 1.0, 1.0351799965998487, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.690412589068135, 0.6904125890681346, 0.7237950642272033], 
reward next is 0.2762, 
noisyNet noise sample is [array([-0.9276684], dtype=float32), -1.2440621]. 
=============================================
[2019-03-25 23:20:59,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5109744e-36 2.8132823e-29], sum to 1.0000
[2019-03-25 23:20:59,702] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0170
[2019-03-25 23:20:59,707] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.33333333333334, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.898811836460819, 6.9112, 6.9112, 168.912956510431, 739024.9802409161, 739024.9802409161, 223039.8930952314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6052800.0000, 
sim time next is 6053400.0000, 
raw observation next is [26.3, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8977064042387554, 6.9112, 6.9112, 168.912956510431, 738242.634801634, 738242.634801634, 222788.9017257076], 
processed observation next is [1.0, 0.043478260869565216, 0.4454976303317536, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8752517124862869, 0.0, 0.0, 0.8294399451523027, 0.20506739855600944, 0.20506739855600944, 0.3325207488443397], 
reward next is 0.6675, 
noisyNet noise sample is [array([-0.44753984], dtype=float32), -0.019984892]. 
=============================================
[2019-03-25 23:21:16,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.4893595e-37 0.0000000e+00 1.8606783e-30 1.3254261e-31], sum to 1.0000
[2019-03-25 23:21:16,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0686
[2019-03-25 23:21:16,838] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 991146.4209394319 W.
[2019-03-25 23:21:16,842] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 88.33333333333334, 1.0, 2.0, 0.3546047256509527, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6099765716082409, 6.9112, 6.9112, 168.912956510431, 991146.4209394319, 991146.4209394319, 240403.4703808894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6160800.0000, 
sim time next is 6161400.0000, 
raw observation next is [27.6, 88.0, 1.0, 2.0, 0.7225302687746756, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1009771.606379549, 1009771.60637955, 225784.8979793441], 
processed observation next is [1.0, 0.30434782608695654, 0.5071090047393366, 0.88, 1.0, 1.0, 0.6656991190056333, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2804921128832081, 0.2804921128832083, 0.33699238504379714], 
reward next is 0.6630, 
noisyNet noise sample is [array([0.92555743], dtype=float32), -0.13688605]. 
=============================================
[2019-03-25 23:21:22,285] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:21:22,295] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3233
[2019-03-25 23:21:22,305] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666667, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8912235277635934, 6.9112, 6.9112, 168.912956510431, 733278.3325510502, 733278.3325510502, 221308.3766753096], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6239400.0000, 
sim time next is 6240000.0000, 
raw observation next is [26.73333333333334, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8944886974673109, 6.9112, 6.9112, 168.912956510431, 735579.0213199387, 735579.0213199387, 222044.7849633827], 
processed observation next is [0.0, 0.21739130434782608, 0.4660347551342816, 0.8966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8713276798381838, 0.0, 0.0, 0.8294399451523027, 0.2043275059222052, 0.2043275059222052, 0.33141012681101895], 
reward next is 0.6686, 
noisyNet noise sample is [array([-1.0093073], dtype=float32), 1.4938273]. 
=============================================
[2019-03-25 23:21:22,326] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[69.27849 ]
 [69.278206]
 [69.20428 ]
 [69.19135 ]
 [69.373   ]], R is [[69.24311066]
 [69.22036743]
 [69.19858551]
 [69.17675018]
 [69.15484619]].
[2019-03-25 23:21:23,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:21:23,021] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5880
[2019-03-25 23:21:23,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.65, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.861662507040636, 6.9112, 6.9112, 168.912956510431, 714076.7523697877, 714076.7523697877, 214816.7233459754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6285000.0000, 
sim time next is 6285600.0000, 
raw observation next is [29.5, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.860983073531182, 6.9112, 6.9112, 168.912956510431, 713191.2149102789, 713191.2149102789, 214655.1089467912], 
processed observation next is [0.0, 0.782608695652174, 0.5971563981042655, 0.69, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8304671628429047, 0.0, 0.0, 0.8294399451523027, 0.1981086708084108, 0.1981086708084108, 0.3203807596220764], 
reward next is 0.6796, 
noisyNet noise sample is [array([0.18397054], dtype=float32), -1.3663579]. 
=============================================
[2019-03-25 23:21:30,318] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:21:30,329] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8314
[2019-03-25 23:21:30,336] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.8, 63.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8741798179526276, 6.9112, 6.9112, 168.912956510431, 722904.6237354833, 722904.6237354833, 217566.1321039395], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6358800.0000, 
sim time next is 6359400.0000, 
raw observation next is [30.85, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8848310658209533, 6.9112, 6.9112, 168.912956510431, 730064.704837345, 730064.704837345, 219923.0973340563], 
processed observation next is [0.0, 0.6086956521739131, 0.661137440758294, 0.64, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8595500802694552, 0.0, 0.0, 0.8294399451523027, 0.20279575134370695, 0.20279575134370695, 0.32824342885680047], 
reward next is 0.6718, 
noisyNet noise sample is [array([1.9722148], dtype=float32), 0.917736]. 
=============================================
[2019-03-25 23:21:35,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.7452248e-04 9.7810352e-23 3.3926927e-28 8.8604641e-30 9.9932551e-01], sum to 1.0000
[2019-03-25 23:21:35,900] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9202
[2019-03-25 23:21:35,910] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.63333333333334, 83.33333333333334, 1.0, 2.0, 0.3895043984245498, 1.0, 2.0, 0.3895043984245498, 1.0, 1.0, 0.6682705068285483, 6.9112, 6.9112, 170.5573041426782, 1633522.090469009, 1633522.090469009, 346042.9442543485], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6510000.0000, 
sim time next is 6510600.0000, 
raw observation next is [27.85, 81.5, 1.0, 2.0, 0.4156548133592592, 1.0, 2.0, 0.4156548133592592, 1.0, 2.0, 0.7135220480418268, 6.911200000000001, 6.9112, 170.5573041426782, 1743282.149889631, 1743282.14988963, 360499.8430753129], 
processed observation next is [1.0, 0.34782608695652173, 0.5189573459715641, 0.815, 1.0, 1.0, 0.29596965464970987, 1.0, 1.0, 0.29596965464970987, 1.0, 1.0, 0.6506366439534472, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4842450416360086, 0.48424504163600834, 0.5380594672765864], 
reward next is 0.4619, 
noisyNet noise sample is [array([1.460616], dtype=float32), 1.8343563]. 
=============================================
[2019-03-25 23:21:45,710] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-25 23:21:45,712] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:21:45,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:21:45,712] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:21:45,713] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:21:45,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:21:45,716] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:21:45,716] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:21:45,718] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:21:45,718] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:21:45,719] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:21:45,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-25 23:21:45,753] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-25 23:21:45,753] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-25 23:21:45,816] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-25 23:21:45,816] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-25 23:21:49,040] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:21:49,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 45.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9132422014303683, 6.9112, 6.9112, 168.912956510431, 817278.2580905819, 817278.2580905819, 225784.6364496436]
[2019-03-25 23:21:49,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:21:49,045] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08722224260678435
[2019-03-25 23:22:02,364] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:22:02,366] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.6, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5336743677256599, 6.9112, 6.9112, 168.912956510431, 470074.1763350264, 470074.1763350264, 156443.6656393121]
[2019-03-25 23:22:02,368] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:22:02,371] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.48034684758777957
[2019-03-25 23:22:09,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:22:09,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.65, 73.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4421934462595747, 6.911200000000001, 6.9112, 168.912956510431, 396858.4627842649, 396858.4627842643, 144929.1659857649]
[2019-03-25 23:22:09,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:22:09,061] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7785037945350419
[2019-03-25 23:22:21,215] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:22:21,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.90117282333333, 76.38354840000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9914468580669821, 6.9112, 6.9112, 168.912956510431, 797767.0531076927, 797767.0531076927, 244777.1865629478]
[2019-03-25 23:22:21,218] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:22:21,221] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.33339579795279684
[2019-03-25 23:22:47,971] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:22:47,972] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.71468167, 72.15040627666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.171130337917019, 6.9112, 168.9110362022631, 1638282.652296684, 1453881.220881219, 311360.7675626648]
[2019-03-25 23:22:47,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:22:47,978] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.6686035e-36 3.6560411e-19], sampled 0.8931650577605088
[2019-03-25 23:22:47,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1638282.652296684 W.
[2019-03-25 23:23:07,141] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:23:07,142] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.6, 70.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9538058424629252, 6.9112, 6.9112, 168.912956510431, 775605.5913504985, 775605.5913504985, 235796.7867300964]
[2019-03-25 23:23:07,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:23:07,146] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6253896153293336
[2019-03-25 23:23:20,758] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:23:20,758] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.25, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9177291673869691, 6.9112, 6.9112, 168.912956510431, 752833.9174064738, 752833.9174064738, 227400.146678038]
[2019-03-25 23:23:20,759] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:23:20,763] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6641338703766394
[2019-03-25 23:23:50,076] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.44367704], dtype=float32), 0.33676878]
[2019-03-25 23:23:50,076] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.36487074, 77.14373704, 1.0, 1.0, 0.6640272055310356, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912896978796, 927974.9753173746, 927974.9753173746, 213268.9491032611]
[2019-03-25 23:23:50,078] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:23:50,079] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5413193139000327
[2019-03-25 23:23:50,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 927974.9753173746 W.
[2019-03-25 23:23:54,594] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7345.4038 3105903310.7154 2013.0000
[2019-03-25 23:23:54,932] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.1561 2989509937.5631 1566.0000
[2019-03-25 23:23:55,235] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.1639 3185281672.3323 2464.0000
[2019-03-25 23:23:55,393] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.0942 2938004382.9996 1380.0000
[2019-03-25 23:23:55,400] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7286.7170 3319727200.6801 2143.0000
[2019-03-25 23:23:56,416] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 600000, evaluation results [600000.0, 7286.717049302781, 3319727200.680101, 2143.0, 7345.403808582658, 3105903310.715366, 2013.0, 8060.094240316087, 2938004382.99956, 1380.0, 7029.16391117388, 3185281672.3322916, 2464.0, 7923.156098041422, 2989509937.563126, 1566.0]
[2019-03-25 23:23:58,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:23:58,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6182
[2019-03-25 23:23:58,778] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.86666666666667, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7446736185275165, 6.911200000000001, 6.9112, 168.912956510431, 631364.9630658949, 631364.9630658942, 190969.8323775963], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6721800.0000, 
sim time next is 6722400.0000, 
raw observation next is [27.73333333333333, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7345186739990257, 6.911199999999999, 6.9112, 168.912956510431, 623713.7802010493, 623713.78020105, 189051.2324148545], 
processed observation next is [1.0, 0.8260869565217391, 0.513428120063191, 0.67, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6762422853646656, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1732538278336248, 0.173253827833625, 0.2821660185296336], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.8193258], dtype=float32), 0.32521656]. 
=============================================
[2019-03-25 23:24:00,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6195700e-09 1.8338220e-17 3.9373487e-20 5.6519622e-14 1.0000000e+00], sum to 1.0000
[2019-03-25 23:24:00,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6709
[2019-03-25 23:24:00,397] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.36666666666667, 83.0, 1.0, 2.0, 0.3776511910677621, 1.0, 2.0, 0.3776511910677621, 1.0, 2.0, 0.6452387816787375, 6.911199999999999, 6.9112, 170.5573041426782, 1583774.813828605, 1583774.813828605, 339463.8148859557], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6685800.0000, 
sim time next is 6686400.0000, 
raw observation next is [27.53333333333333, 82.0, 1.0, 2.0, 0.429751775203065, 1.0, 2.0, 0.429751775203065, 1.0, 2.0, 0.7351930243249079, 6.9112, 6.9112, 170.5573041426782, 1802455.411251612, 1802455.411251612, 368262.0411034808], 
processed observation next is [1.0, 0.391304347826087, 0.5039494470774091, 0.82, 1.0, 1.0, 0.3129539460277892, 1.0, 1.0, 0.3129539460277892, 1.0, 1.0, 0.6770646638108633, 0.0, 0.0, 0.8375144448122397, 0.5006820586810034, 0.5006820586810034, 0.5496448374678817], 
reward next is 0.4504, 
noisyNet noise sample is [array([1.5742897], dtype=float32), 0.5798354]. 
=============================================
[2019-03-25 23:24:01,401] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:01,414] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3836
[2019-03-25 23:24:01,419] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.86666666666667, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8670108921429508, 6.911199999999999, 6.9112, 168.912956510431, 717216.0075543323, 717216.0075543328, 215965.0039755738], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6640800.0000, 
sim time next is 6641400.0000, 
raw observation next is [26.83333333333334, 85.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8651335019936082, 6.911199999999999, 6.9112, 168.912956510431, 715755.7319229668, 715755.7319229674, 215549.0134907344], 
processed observation next is [1.0, 0.8695652173913043, 0.4707740916271725, 0.8583333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8355286609678149, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19882103664526854, 0.1988210366452687, 0.3217149455085588], 
reward next is 0.6783, 
noisyNet noise sample is [array([0.5135043], dtype=float32), 0.56287974]. 
=============================================
[2019-03-25 23:24:02,103] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.8216757e-37 4.8655178e-36], sum to 1.0000
[2019-03-25 23:24:02,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6680
[2019-03-25 23:24:02,122] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.63333333333333, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8645532379141074, 6.9112, 6.9112, 168.912956510431, 715836.9067466144, 715836.9067466144, 215438.7971400282], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6645000.0000, 
sim time next is 6645600.0000, 
raw observation next is [26.6, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8629345557906268, 6.9112, 6.9112, 168.912956510431, 714591.1487356551, 714591.1487356551, 215081.5039907834], 
processed observation next is [1.0, 0.9565217391304348, 0.4597156398104266, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8328470192568619, 0.0, 0.0, 0.8294399451523027, 0.19849754131545977, 0.19849754131545977, 0.32101717013549763], 
reward next is 0.6790, 
noisyNet noise sample is [array([-1.625837], dtype=float32), -0.24904864]. 
=============================================
[2019-03-25 23:24:03,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 6.3242685e-38 0.0000000e+00 5.0079097e-31 3.9457727e-33], sum to 1.0000
[2019-03-25 23:24:03,429] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8289
[2019-03-25 23:24:03,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 922959.6230848305 W.
[2019-03-25 23:24:03,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.76666666666667, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997470987739056, 6.9112, 168.912479419856, 922959.6230848305, 861756.1880326141, 256308.3618983541], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6667800.0000, 
sim time next is 6668400.0000, 
raw observation next is [24.73333333333333, 95.0, 1.0, 1.0, 0.2957040450925275, 1.0, 1.0, 0.2957040450925275, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 826447.5100411143, 826447.5100411143, 249041.5252024657], 
processed observation next is [1.0, 0.17391304347826086, 0.3712480252764612, 0.95, 1.0, 0.5, 0.15145065673798494, 1.0, 0.5, 0.15145065673798494, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22956875278919842, 0.22956875278919842, 0.37170376895890406], 
reward next is 0.6283, 
noisyNet noise sample is [array([-0.13754322], dtype=float32), -0.22055764]. 
=============================================
[2019-03-25 23:24:08,874] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.139057e-33 0.000000e+00], sum to 1.0000
[2019-03-25 23:24:08,882] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2730
[2019-03-25 23:24:08,887] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.86666666666667, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5924056211607474, 6.911199999999998, 6.9112, 168.912956510431, 517358.6362171899, 517358.6362171912, 164900.6910644447], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6819600.0000, 
sim time next is 6820200.0000, 
raw observation next is [24.75, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.593330117820155, 6.911200000000001, 6.9112, 168.912956510431, 518073.9918309319, 518073.9918309313, 165041.5420354382], 
processed observation next is [1.0, 0.9565217391304348, 0.3720379146919432, 0.695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5040611192928719, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14390944217525886, 0.1439094421752587, 0.2463306597543854], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.91518253], dtype=float32), 1.0438262]. 
=============================================
[2019-03-25 23:24:12,243] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:12,256] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5298
[2019-03-25 23:24:12,262] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.73333333333333, 46.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5787263528030271, 6.911199999999999, 6.9112, 168.912956510431, 506663.5070713871, 506663.5070713878, 162843.6980307276], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6867600.0000, 
sim time next is 6868200.0000, 
raw observation next is [28.81666666666666, 45.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5701935287502709, 6.911199999999999, 6.9112, 168.912956510431, 499939.8225847081, 499939.8225847087, 161585.5184025455], 
processed observation next is [0.0, 0.4782608695652174, 0.5647709320695099, 0.4516666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47584576676862295, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1388721729401967, 0.13887217294019685, 0.24117241552618732], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.17415887], dtype=float32), 1.8982255]. 
=============================================
[2019-03-25 23:24:13,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:13,584] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3576
[2019-03-25 23:24:13,590] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.26666666666667, 54.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6237201490527859, 6.9112, 6.9112, 168.912956510431, 540343.179096837, 540343.179096837, 169805.9335433972], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6891000.0000, 
sim time next is 6891600.0000, 
raw observation next is [28.13333333333333, 55.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6314711414209127, 6.911200000000001, 6.9112, 168.912956510431, 546862.1566660232, 546862.1566660225, 171045.8195766312], 
processed observation next is [0.0, 0.782608695652174, 0.532385466034755, 0.5566666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5505745627084301, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1519061546294509, 0.1519061546294507, 0.2552922680248227], 
reward next is 0.7447, 
noisyNet noise sample is [array([-0.43157235], dtype=float32), -0.72564906]. 
=============================================
[2019-03-25 23:24:14,178] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:14,187] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9145
[2019-03-25 23:24:14,199] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.53333333333333, 67.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6654346560773291, 6.9112, 6.9112, 168.912956510431, 573201.4065560342, 573201.4065560342, 176686.7298879921], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6900000.0000, 
sim time next is 6900600.0000, 
raw observation next is [26.46666666666667, 68.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6700624627798735, 6.9112, 6.9112, 168.912956510431, 576728.5518387468, 576728.5518387468, 177478.4164412583], 
processed observation next is [0.0, 0.8695652173913043, 0.45339652448657203, 0.6816666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.597637149731553, 0.0, 0.0, 0.8294399451523027, 0.160202375510763, 0.160202375510763, 0.2648931588675497], 
reward next is 0.7351, 
noisyNet noise sample is [array([1.1493304], dtype=float32), 0.6312545]. 
=============================================
[2019-03-25 23:24:15,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.0044631e-35 1.1548459e-37], sum to 1.0000
[2019-03-25 23:24:15,243] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6094
[2019-03-25 23:24:15,249] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.51666666666667, 71.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5939431043597502, 6.9112, 6.9112, 168.912956510431, 518444.9361335237, 518444.9361335237, 165137.3317178406], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6821400.0000, 
sim time next is 6822000.0000, 
raw observation next is [24.4, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5938943874753239, 6.9112, 6.9112, 168.912956510431, 518330.183853464, 518330.183853464, 165131.5816437622], 
processed observation next is [1.0, 1.0, 0.3554502369668246, 0.72, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5047492530186876, 0.0, 0.0, 0.8294399451523027, 0.14398060662596224, 0.14398060662596224, 0.24646504722949583], 
reward next is 0.7535, 
noisyNet noise sample is [array([-0.08271001], dtype=float32), 0.23382351]. 
=============================================
[2019-03-25 23:24:15,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.23898]
 [75.27024]
 [75.30551]
 [75.35142]
 [75.39719]], R is [[75.30882263]
 [75.30925751]
 [75.30971527]
 [75.31028748]
 [75.31106567]].
[2019-03-25 23:24:18,342] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:18,352] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6090
[2019-03-25 23:24:18,364] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.15, 40.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.531712063051468, 6.911200000000001, 6.9112, 168.912956510431, 469016.0463452879, 469016.0463452873, 156156.8075187496], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6870600.0000, 
sim time next is 6871200.0000, 
raw observation next is [29.23333333333333, 39.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5226275519331918, 6.911199999999999, 6.9112, 168.912956510431, 461700.470572328, 461700.4705723287, 154929.9336206887], 
processed observation next is [0.0, 0.5217391304347826, 0.5845181674565559, 0.3933333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.417838477967307, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12825013071453556, 0.12825013071453575, 0.23123870689655032], 
reward next is 0.7688, 
noisyNet noise sample is [array([0.13825947], dtype=float32), 0.98371726]. 
=============================================
[2019-03-25 23:24:22,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:22,053] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4571
[2019-03-25 23:24:22,057] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.45, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7251047251483533, 6.911199999999999, 6.9112, 168.912956510431, 617743.96967423, 617743.9696742307, 187304.6690742101], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6921000.0000, 
sim time next is 6921600.0000, 
raw observation next is [24.4, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7291034623056525, 6.911200000000001, 6.9112, 168.912956510431, 621184.5767911518, 621184.5767911512, 188051.1548915208], 
processed observation next is [0.0, 0.08695652173913043, 0.3554502369668246, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6696383686654299, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1725512713308755, 0.17255127133087533, 0.28067336550973254], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.13370796], dtype=float32), -0.5887992]. 
=============================================
[2019-03-25 23:24:24,427] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:24,441] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1181
[2019-03-25 23:24:24,453] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.36666666666667, 77.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7962383469569067, 6.9112, 6.9112, 168.912956510431, 665641.8819275071, 665641.8819275071, 201019.5272838558], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7064400.0000, 
sim time next is 7065000.0000, 
raw observation next is [27.25, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8006337205971151, 6.911199999999999, 6.9112, 168.912956510431, 669405.035718534, 669405.0357185346, 201924.8931605724], 
processed observation next is [1.0, 0.782608695652174, 0.490521327014218, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7568703909720915, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18594584325514835, 0.1859458432551485, 0.30138043755309313], 
reward next is 0.6986, 
noisyNet noise sample is [array([0.01552732], dtype=float32), -0.24131505]. 
=============================================
[2019-03-25 23:24:24,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.59774 ]
 [70.155685]
 [70.776825]
 [70.69188 ]
 [70.08581 ]], R is [[69.15652466]
 [69.16493225]
 [69.17427063]
 [69.18534851]
 [69.19801331]].
[2019-03-25 23:24:29,297] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:29,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8501
[2019-03-25 23:24:29,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9681215566337289, 6.9112, 6.9112, 168.912956510431, 818313.0422038824, 818313.0422038824, 240429.2813549631], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7018800.0000, 
sim time next is 7019400.0000, 
raw observation next is [25.75, 82.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9572886742723222, 6.9112, 6.9112, 168.912956510431, 809343.0356509637, 809343.0356509637, 237722.1601400507], 
processed observation next is [1.0, 0.21739130434782608, 0.41943127962085314, 0.8216666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9479130174052709, 0.0, 0.0, 0.8294399451523027, 0.22481750990304547, 0.22481750990304547, 0.35480919423888163], 
reward next is 0.6452, 
noisyNet noise sample is [array([-1.1829429], dtype=float32), -0.64751816]. 
=============================================
[2019-03-25 23:24:30,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:30,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0508
[2019-03-25 23:24:30,822] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8004789200378967, 6.911199999999999, 6.9112, 168.912956510431, 671933.7636823797, 671933.7636823804, 201954.0351218085], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7077600.0000, 
sim time next is 7078200.0000, 
raw observation next is [25.56666666666667, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8006568106823921, 6.9112, 6.9112, 168.912956510431, 672256.6350135135, 672256.6350135135, 201994.4002602639], 
processed observation next is [1.0, 0.9565217391304348, 0.41074249605055313, 0.8716666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7568985496126733, 0.0, 0.0, 0.8294399451523027, 0.1867379541704204, 0.1867379541704204, 0.3014841794929312], 
reward next is 0.6985, 
noisyNet noise sample is [array([0.24376072], dtype=float32), -0.91213775]. 
=============================================
[2019-03-25 23:24:33,230] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.9563267e-35 1.1554804e-37 5.6457587e-28 3.9444484e-33], sum to 1.0000
[2019-03-25 23:24:33,245] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0721
[2019-03-25 23:24:33,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1812806.129018048 W.
[2019-03-25 23:24:33,263] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333333, 75.0, 1.0, 2.0, 0.648326350432734, 1.0, 1.0, 0.648326350432734, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1812806.129018048, 1812806.129018048, 352787.6947824777], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7209600.0000, 
sim time next is 7210200.0000, 
raw observation next is [29.41666666666667, 74.0, 1.0, 2.0, 0.4285643349809005, 1.0, 2.0, 0.4285643349809005, 1.0, 1.0, 0.7393573041086634, 6.9112, 6.9112, 170.5573041426782, 1797470.898410252, 1797470.898410252, 368519.0292878287], 
processed observation next is [1.0, 0.43478260869565216, 0.5932069510268565, 0.74, 1.0, 1.0, 0.31152329515771143, 1.0, 1.0, 0.31152329515771143, 1.0, 0.5, 0.682143053791053, 0.0, 0.0, 0.8375144448122397, 0.49929747178062556, 0.49929747178062556, 0.5500284019221324], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6537019], dtype=float32), -0.58947474]. 
=============================================
[2019-03-25 23:24:48,507] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.154799e-28], sum to 1.0000
[2019-03-25 23:24:48,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0139
[2019-03-25 23:24:48,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 985228.6982386729 W.
[2019-03-25 23:24:48,537] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.23333333333333, 80.33333333333334, 1.0, 2.0, 0.2141137231086334, 1.0, 2.0, 0.2141137231086334, 1.0, 2.0, 0.3793797304026378, 6.911200000000001, 6.9112, 170.5573041426782, 985228.6982386729, 985228.6982386723, 283595.5367539106], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7288800.0000, 
sim time next is 7289400.0000, 
raw observation next is [23.46666666666667, 79.16666666666666, 1.0, 2.0, 0.324752736485934, 0.0, 1.0, 0.0, 1.0, 2.0, 0.57914234432851, 6.9112, 6.9112, 168.912956510431, 1006548.633052919, 1006548.633052919, 238848.9765559926], 
processed observation next is [1.0, 0.34782608695652173, 0.31121642969984215, 0.7916666666666665, 1.0, 1.0, 0.18644908010353495, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.48675895649818285, 0.0, 0.0, 0.8294399451523027, 0.2795968425146997, 0.2795968425146997, 0.35649100978506354], 
reward next is 0.6435, 
noisyNet noise sample is [array([1.355112], dtype=float32), 1.862206]. 
=============================================
[2019-03-25 23:24:51,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:24:51,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7440
[2019-03-25 23:24:51,436] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.64032889247132, 6.911199999999999, 6.9112, 168.912956510431, 552622.1308613934, 552622.130861394, 172498.3302604602], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7467600.0000, 
sim time next is 7468200.0000, 
raw observation next is [23.2, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6439119328830941, 6.9112, 6.9112, 168.912956510431, 555495.465452606, 555495.465452606, 173086.2417643512], 
processed observation next is [0.0, 0.43478260869565216, 0.29857819905213273, 0.885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5657462596135294, 0.0, 0.0, 0.8294399451523027, 0.1543042959590572, 0.1543042959590572, 0.25833767427515103], 
reward next is 0.7417, 
noisyNet noise sample is [array([-1.574102], dtype=float32), -0.941392]. 
=============================================
[2019-03-25 23:25:03,952] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-25 23:25:03,954] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:25:03,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:25:03,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:25:03,957] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:25:03,960] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:25:03,961] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:25:03,962] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:25:03,964] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:25:03,963] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:25:03,969] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:25:03,980] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-25 23:25:04,002] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-25 23:25:04,003] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-25 23:25:04,044] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-25 23:25:04,046] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-25 23:25:28,114] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:25:28,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.73333333333333, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4887645659992922, 6.9112, 6.9112, 168.912956510431, 434561.4207263609, 434561.4207263609, 150527.7828915938]
[2019-03-25 23:25:28,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:25:28,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5198818364090126
[2019-03-25 23:25:58,924] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:25:58,927] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9539350761731993, 6.9112, 6.9112, 168.912956510431, 776091.9979960889, 776091.9979960889, 235847.3457188155]
[2019-03-25 23:25:58,927] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:25:58,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9063303135014543
[2019-03-25 23:26:02,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:26:02,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.52894013166667, 82.64654261333334, 1.0, 2.0, 0.9156772620264915, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1318066.256067898, 1318066.256067898, 279954.357436551]
[2019-03-25 23:26:02,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:26:02,506] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9998784e-01 0.0000000e+00 6.0439582e-32 7.1789508e-36 1.2193002e-05], sampled 0.5501907727098458
[2019-03-25 23:26:02,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1318066.256067898 W.
[2019-03-25 23:26:36,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:26:36,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.96666666666667, 92.0, 1.0, 2.0, 0.2141376215172716, 1.0, 1.0, 0.2141376215172716, 1.0, 1.0, 0.3718862321819272, 6.9112, 6.9112, 178.6582176852504, 897735.570277616, 897735.570277616, 275903.317556859]
[2019-03-25 23:26:36,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:26:36,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4388656e-04 0.0000000e+00 1.6749592e-31 3.9248989e-37 9.9975616e-01], sampled 0.2105775833022816
[2019-03-25 23:27:09,643] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:27:09,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.96666666666667, 85.0, 1.0, 1.0, 0.594256094239679, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129106578402, 830432.0482869939, 830432.0482869939, 199677.504315275]
[2019-03-25 23:27:09,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:27:09,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5456482e-31], sampled 0.724914226724276
[2019-03-25 23:27:10,853] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.5760389], dtype=float32), 0.30225366]
[2019-03-25 23:27:10,854] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.76666666666667, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.018806844861138, 6.9112, 168.912113999363, 905170.3219970322, 828830.7034825876, 254811.9789865413]
[2019-03-25 23:27:10,855] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:27:10,859] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.1331020e-35 9.8236583e-38 2.2133044e-12], sampled 0.053542715135758034
[2019-03-25 23:27:10,860] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 905170.3219970322 W.
[2019-03-25 23:27:13,055] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7387.6608 3121674944.5675 1904.0000
[2019-03-25 23:27:13,268] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7359.6166 3336046436.6682 1913.0000
[2019-03-25 23:27:13,399] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8118.6056 2957498565.2780 1191.0000
[2019-03-25 23:27:13,636] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7998.1875 3005624779.0539 1339.0000
[2019-03-25 23:27:13,750] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7089.3822 3200129155.6486 2282.0000
[2019-03-25 23:27:14,765] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 625000, evaluation results [625000.0, 7359.616565770167, 3336046436.6681585, 1913.0, 7387.660751148103, 3121674944.5675006, 1904.0, 8118.60556083884, 2957498565.278024, 1191.0, 7089.382189928514, 3200129155.6486, 2282.0, 7998.187467985295, 3005624779.0538774, 1339.0]
[2019-03-25 23:27:15,841] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8261134e-24], sum to 1.0000
[2019-03-25 23:27:15,849] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6039
[2019-03-25 23:27:15,858] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.6, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6974641634247479, 6.911200000000001, 6.9112, 168.912956510431, 595490.0639916944, 595490.0639916938, 182271.551105773], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7513800.0000, 
sim time next is 7514400.0000, 
raw observation next is [23.6, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6983152611337674, 6.9112, 6.9112, 168.912956510431, 596095.3343462404, 596095.3343462404, 182423.1438186928], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6320917818704479, 0.0, 0.0, 0.8294399451523027, 0.1655820373184001, 0.1655820373184001, 0.27227334898312355], 
reward next is 0.7277, 
noisyNet noise sample is [array([-0.58693737], dtype=float32), -0.11647568]. 
=============================================
[2019-03-25 23:27:16,957] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:16,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1919
[2019-03-25 23:27:16,973] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.13333333333333, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.662068899433607, 6.911200000000001, 6.9112, 168.912956510431, 569319.488506724, 569319.4885067233, 176120.4032566017], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7532400.0000, 
sim time next is 7533000.0000, 
raw observation next is [23.1, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.65857000802433, 6.9112, 6.9112, 168.912956510431, 566659.9541613702, 566659.9541613702, 175529.2477167357], 
processed observation next is [0.0, 0.17391304347826086, 0.2938388625592418, 0.905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5836219610052804, 0.0, 0.0, 0.8294399451523027, 0.15740554282260283, 0.15740554282260283, 0.2619839518160234], 
reward next is 0.7380, 
noisyNet noise sample is [array([-1.4226055], dtype=float32), 2.2384813]. 
=============================================
[2019-03-25 23:27:17,003] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.33893]
 [67.27939]
 [67.22241]
 [67.10747]
 [67.09566]], R is [[67.44494629]
 [67.50763702]
 [67.56880951]
 [67.62864685]
 [67.68740082]].
[2019-03-25 23:27:18,317] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.326883e-31], sum to 1.0000
[2019-03-25 23:27:18,326] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9376
[2019-03-25 23:27:18,334] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.45, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8097062700220291, 6.9112, 6.9112, 168.912956510431, 689128.3889712683, 689128.3889712683, 204010.1372897763], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7617000.0000, 
sim time next is 7617600.0000, 
raw observation next is [23.4, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7868778933582802, 6.9112, 6.9112, 168.912956510431, 670110.6153167004, 670110.6153167004, 199313.7309912771], 
processed observation next is [1.0, 0.17391304347826086, 0.30805687203791465, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7400949919003416, 0.0, 0.0, 0.8294399451523027, 0.18614183758797234, 0.18614183758797234, 0.29748318058399564], 
reward next is 0.7025, 
noisyNet noise sample is [array([-0.4770236], dtype=float32), -0.16784531]. 
=============================================
[2019-03-25 23:27:20,626] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:20,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7057
[2019-03-25 23:27:20,644] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.58333333333333, 72.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7598057776849124, 6.9112, 6.9112, 168.912956510431, 640800.7608636237, 640800.7608636237, 193847.970964477], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7582200.0000, 
sim time next is 7582800.0000, 
raw observation next is [27.46666666666667, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7630229628015457, 6.9112, 6.9112, 168.912956510431, 642940.3347757942, 642940.3347757942, 194467.9550102612], 
processed observation next is [0.0, 0.782608695652174, 0.500789889415482, 0.7333333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7110036131726167, 0.0, 0.0, 0.8294399451523027, 0.1785945374377206, 0.1785945374377206, 0.29025067911979285], 
reward next is 0.7097, 
noisyNet noise sample is [array([-1.3418975], dtype=float32), -0.1786269]. 
=============================================
[2019-03-25 23:27:23,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:23,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3280
[2019-03-25 23:27:23,237] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.55, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.824563634351169, 6.9112, 6.9112, 168.912956510431, 700896.0771408944, 700896.0771408944, 207136.571764071], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7615800.0000, 
sim time next is 7616400.0000, 
raw observation next is [23.5, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8193275301093341, 6.9112, 6.9112, 168.912956510431, 696882.8024645454, 696882.8024645454, 206029.2961838245], 
processed observation next is [1.0, 0.13043478260869565, 0.31279620853080575, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7796677196455294, 0.0, 0.0, 0.8294399451523027, 0.1935785562401515, 0.1935785562401515, 0.30750641221466346], 
reward next is 0.6925, 
noisyNet noise sample is [array([-1.8819615], dtype=float32), 1.73431]. 
=============================================
[2019-03-25 23:27:24,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.2031198e-36 0.0000000e+00], sum to 1.0000
[2019-03-25 23:27:24,683] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0747
[2019-03-25 23:27:24,690] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.06666666666667, 89.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.930940096428887, 6.9112, 6.9112, 168.912956505949, 780099.5744693845, 780099.5744693845, 231115.5739961275], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7632600.0000, 
sim time next is 7633200.0000, 
raw observation next is [25.33333333333334, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 8.031827925496547, 6.9112, 168.9069797090855, 1661268.775221504, 866284.8864737287, 256420.7086854397], 
processed observation next is [1.0, 0.34782608695652173, 0.3996840442338076, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.11206279254965468, 0.0, 0.8294105963215048, 0.46146354867264, 0.24063469068714685, 0.38271747564991], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4495826], dtype=float32), -0.25446165]. 
=============================================
[2019-03-25 23:27:24,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 7.1727717e-28 8.5128661e-30 7.3589928e-28 3.8119582e-10], sum to 1.0000
[2019-03-25 23:27:24,730] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3906
[2019-03-25 23:27:24,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1646019.630386318 W.
[2019-03-25 23:27:24,739] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333333, 63.66666666666667, 1.0, 2.0, 0.3924820816308972, 1.0, 1.0, 0.3924820816308972, 1.0, 2.0, 0.6620157603865747, 6.9112, 6.9112, 170.5573041426782, 1646019.630386318, 1646019.630386318, 346024.3935749719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7643400.0000, 
sim time next is 7644000.0000, 
raw observation next is [29.76666666666667, 63.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.32977010492747, 6.9112, 168.9049816389876, 2460752.974934835, 1454418.033323993, 310831.1219396546], 
processed observation next is [1.0, 0.4782608695652174, 0.6097946287519749, 0.6333333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.14185701049274693, 0.0, 0.8294007848826037, 0.6835424930374542, 0.4040050092566647, 0.46392704767112625], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41604093], dtype=float32), 1.3603218]. 
=============================================
[2019-03-25 23:27:24,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.330265]
 [51.03115 ]
 [51.72468 ]
 [52.29752 ]
 [52.686565]], R is [[48.26549911]
 [47.78284454]
 [47.30501556]
 [46.8319664 ]
 [46.91136551]].
[2019-03-25 23:27:31,188] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:31,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4757
[2019-03-25 23:27:31,202] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.86666666666667, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8915474933968395, 6.9112, 6.9112, 168.912956510431, 730748.9337854399, 730748.9337854399, 221269.0645423006], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7841400.0000, 
sim time next is 7842000.0000, 
raw observation next is [28.73333333333333, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8973585750905235, 6.9112, 6.9112, 168.912956510431, 735205.1416437647, 735205.1416437647, 222595.0583489997], 
processed observation next is [1.0, 0.782608695652174, 0.560821484992101, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8748275305981994, 0.0, 0.0, 0.8294399451523027, 0.2042236504566013, 0.2042236504566013, 0.3322314303716414], 
reward next is 0.6678, 
noisyNet noise sample is [array([-0.6930219], dtype=float32), -0.6411308]. 
=============================================
[2019-03-25 23:27:31,220] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[54.90177 ]
 [54.469154]
 [53.6739  ]
 [52.913666]
 [51.563534]], R is [[55.30691147]
 [55.4235878 ]
 [55.54296875]
 [55.66767502]
 [55.79797745]].
[2019-03-25 23:27:38,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:38,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:38,158] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-25 23:27:39,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:39,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:39,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-25 23:27:39,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:39,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:39,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-25 23:27:40,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:40,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:41,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-25 23:27:41,293] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 3.822436e-32 0.000000e+00], sum to 1.0000
[2019-03-25 23:27:41,299] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1466
[2019-03-25 23:27:41,307] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 924937.6674999255 W.
[2019-03-25 23:27:41,312] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.1, 83.33333333333334, 1.0, 2.0, 0.2206191784219074, 1.0, 2.0, 0.2206191784219074, 1.0, 1.0, 0.3744561810398752, 6.9112, 6.9112, 170.5573041426782, 924937.6674999255, 924937.6674999255, 275287.9779920511], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7888800.0000, 
sim time next is 7889400.0000, 
raw observation next is [27.15, 83.0, 1.0, 2.0, 0.3368171122372107, 1.0, 2.0, 0.3368171122372107, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 941402.6679116269, 941402.6679116269, 257379.33722988], 
processed observation next is [1.0, 0.30434782608695654, 0.485781990521327, 0.83, 1.0, 1.0, 0.20098447257495267, 1.0, 1.0, 0.20098447257495267, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.261500741086563, 0.261500741086563, 0.38414826452220896], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3260838], dtype=float32), -1.2328994]. 
=============================================
[2019-03-25 23:27:41,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:41,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:41,514] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-25 23:27:42,308] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:42,309] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:42,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-25 23:27:43,058] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:43,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5718
[2019-03-25 23:27:43,073] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.36666666666667, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8524679563712179, 6.9112, 6.9112, 168.912956510431, 704078.1035583878, 704078.1035583878, 212704.1432172155], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7928400.0000, 
sim time next is 7929000.0000, 
raw observation next is [29.15, 72.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8571835610718936, 6.911199999999999, 6.9112, 168.912956510431, 707513.5521628319, 707513.5521628325, 213726.0074720009], 
processed observation next is [1.0, 0.782608695652174, 0.5805687203791469, 0.725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8258336110632849, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1965315422674533, 0.19653154226745345, 0.3189940410029864], 
reward next is 0.6810, 
noisyNet noise sample is [array([-0.21437791], dtype=float32), 0.24189572]. 
=============================================
[2019-03-25 23:27:43,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[54.86931 ]
 [54.339474]
 [52.983154]
 [51.98701 ]
 [51.0994  ]], R is [[55.02216721]
 [55.15447617]
 [55.28802872]
 [55.42459869]
 [55.56383896]].
[2019-03-25 23:27:43,405] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:43,406] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:43,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-25 23:27:45,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,214] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,242] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-25 23:27:45,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-25 23:27:45,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,483] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-25 23:27:45,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-25 23:27:45,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-25 23:27:45,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:45,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-25 23:27:45,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:45,978] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:46,003] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-25 23:27:46,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:46,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:46,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-25 23:27:46,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:27:46,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:27:46,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-25 23:27:48,010] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.0606478e-34 1.0585939e-31 7.8311375e-33 9.2714592e-10], sum to 1.0000
[2019-03-25 23:27:48,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0736
[2019-03-25 23:27:48,033] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1138752.783025442 W.
[2019-03-25 23:27:48,043] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.33333333333334, 70.33333333333334, 1.0, 2.0, 0.3726435258179618, 1.0, 1.0, 0.3726435258179618, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1138752.783025442, 1138752.783025442, 276753.1631030747], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 37200.0000, 
sim time next is 37800.0000, 
raw observation next is [25.55, 69.5, 1.0, 2.0, 0.3745430817365958, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6607084223253041, 6.911200000000001, 6.9112, 168.912956510431, 1141097.771645191, 1141097.77164519, 256891.4589745661], 
processed observation next is [1.0, 0.43478260869565216, 0.40995260663507116, 0.695, 1.0, 1.0, 0.2464374478754166, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.5862297833235416, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31697160323477525, 0.31697160323477497, 0.38342008802174044], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3136606], dtype=float32), 1.7849804]. 
=============================================
[2019-03-25 23:27:51,624] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:51,635] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0685
[2019-03-25 23:27:51,645] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.16666666666666, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5268240884278288, 6.911199999999999, 6.9112, 168.912956510431, 465186.8719295451, 465186.8719295457, 155490.0048223267], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 197400.0000, 
sim time next is 198000.0000, 
raw observation next is [20.2, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5263266128528146, 6.9112, 6.9112, 168.912956510431, 464688.8910107907, 464688.8910107907, 155426.7510460624], 
processed observation next is [0.0, 0.30434782608695654, 0.15639810426540288, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.42234952786928603, 0.0, 0.0, 0.8294399451523027, 0.12908024750299743, 0.12908024750299743, 0.23198022544188415], 
reward next is 0.7680, 
noisyNet noise sample is [array([2.1021206], dtype=float32), -0.4658991]. 
=============================================
[2019-03-25 23:27:51,670] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[85.91197 ]
 [85.87649 ]
 [85.82052 ]
 [85.782875]
 [85.75132 ]], R is [[85.947258  ]
 [85.85571289]
 [85.76509857]
 [85.67562866]
 [85.58748627]].
[2019-03-25 23:27:59,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:27:59,539] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1390
[2019-03-25 23:27:59,550] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.1, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5255771019246871, 6.9112, 6.9112, 168.912956510431, 464202.8945721302, 464202.8945721302, 155321.0468366248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 196200.0000, 
sim time next is 196800.0000, 
raw observation next is [20.13333333333333, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5267576516340658, 6.911200000000001, 6.9112, 168.912956510431, 465186.9534925816, 465186.953492581, 155478.8960139393], 
processed observation next is [0.0, 0.2608695652173913, 0.15323854660347538, 0.9433333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4228751849195925, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1292185981923838, 0.1292185981923836, 0.2320580537521482], 
reward next is 0.7679, 
noisyNet noise sample is [array([-1.2761155], dtype=float32), -0.14490214]. 
=============================================
[2019-03-25 23:28:01,804] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8929034e-36], sum to 1.0000
[2019-03-25 23:28:01,814] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5013
[2019-03-25 23:28:01,820] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.6, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.915698545076073, 6.9112, 6.9112, 168.912956510431, 811252.6944798813, 811252.6944798813, 226997.1860188281], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 388800.0000, 
sim time next is 389400.0000, 
raw observation next is [22.61666666666667, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7814792506979564, 6.911199999999999, 6.9112, 168.912956510431, 692076.9286697815, 692076.9286697821, 197710.0033208228], 
processed observation next is [1.0, 0.5217391304347826, 0.2709320695102688, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7335112813389713, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19224359129716154, 0.1922435912971617, 0.2950895571952579], 
reward next is 0.7049, 
noisyNet noise sample is [array([0.11910073], dtype=float32), -1.048499]. 
=============================================
[2019-03-25 23:28:05,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7715667e-25], sum to 1.0000
[2019-03-25 23:28:05,900] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7495
[2019-03-25 23:28:05,907] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4904172050922767, 6.911199999999999, 6.9112, 168.912956510431, 435708.9779443158, 435708.9779443164, 150745.4900627287], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 414000.0000, 
sim time next is 414600.0000, 
raw observation next is [20.91666666666667, 80.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4849326958005425, 6.911200000000001, 6.9112, 168.912956510431, 431159.913625452, 431159.9136254514, 150064.5095285366], 
processed observation next is [1.0, 0.8260869565217391, 0.19036334913112193, 0.8083333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3718691412201737, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11976664267373667, 0.11976664267373649, 0.22397687989333823], 
reward next is 0.7760, 
noisyNet noise sample is [array([0.67789066], dtype=float32), 1.9735438]. 
=============================================
[2019-03-25 23:28:06,543] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8053935e-35], sum to 1.0000
[2019-03-25 23:28:06,555] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1488
[2019-03-25 23:28:06,568] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.23333333333333, 80.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4858138824922472, 6.9112, 6.9112, 168.912956510431, 434520.2358408173, 434520.2358408173, 150013.8929918826], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 457800.0000, 
sim time next is 458400.0000, 
raw observation next is [20.26666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4489485295041875, 6.9112, 6.9112, 168.912956510431, 401497.8336004322, 401497.8336004322, 145770.2321429765], 
processed observation next is [1.0, 0.30434782608695654, 0.15955766192733034, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3279860115904726, 0.0, 0.0, 0.8294399451523027, 0.11152717600012006, 0.11152717600012006, 0.21756751066115898], 
reward next is 0.7824, 
noisyNet noise sample is [array([-0.11789796], dtype=float32), -0.73405606]. 
=============================================
[2019-03-25 23:28:07,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:28:07,429] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2300
[2019-03-25 23:28:07,438] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.38333333333334, 81.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5429932576251315, 6.911199999999999, 6.9112, 168.912956510431, 476756.8328659078, 476756.8328659085, 157752.8308953748], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 292200.0000, 
sim time next is 292800.0000, 
raw observation next is [22.46666666666667, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5431438567688968, 6.9112, 6.9112, 168.912956510431, 476816.5432192273, 476816.5432192273, 157775.5693664029], 
processed observation next is [0.0, 0.391304347826087, 0.2638230647709322, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4428583619132887, 0.0, 0.0, 0.8294399451523027, 0.1324490397831187, 0.1324490397831187, 0.23548592442746702], 
reward next is 0.7645, 
noisyNet noise sample is [array([0.8831212], dtype=float32), 0.8884468]. 
=============================================
[2019-03-25 23:28:16,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5231439e-37 2.6795268e-25], sum to 1.0000
[2019-03-25 23:28:16,418] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5523
[2019-03-25 23:28:16,427] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4455460729709971, 6.9112, 6.9112, 168.912956510431, 399087.1499936568, 399087.1499936568, 145352.2817433645], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [20.86666666666667, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4437357851621822, 6.9112, 6.9112, 168.912956510431, 397648.4020922547, 397648.4020922547, 145141.60649443], 
processed observation next is [1.0, 0.8695652173913043, 0.18799368088467638, 0.7366666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.32162900629534413, 0.0, 0.0, 0.8294399451523027, 0.11045788947007075, 0.11045788947007075, 0.21662926342452238], 
reward next is 0.7834, 
noisyNet noise sample is [array([-0.21168283], dtype=float32), 0.26915258]. 
=============================================
[2019-03-25 23:28:16,448] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.25779 ]
 [73.39766 ]
 [73.710686]
 [73.19569 ]
 [73.1974  ]], R is [[73.1497879 ]
 [73.20134735]
 [73.25198364]
 [73.30141449]
 [73.34939575]].
[2019-03-25 23:28:20,028] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-25 23:28:20,030] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:28:20,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:28:20,032] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:28:20,034] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:28:20,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:28:20,035] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:28:20,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:28:20,040] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:28:20,040] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:28:20,040] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:28:20,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-25 23:28:20,075] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-25 23:28:20,117] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-25 23:28:20,144] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-25 23:28:20,167] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-25 23:29:08,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:08,060] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.8527411, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4722208564826478, 6.911199999999999, 6.9112, 168.912956510431, 421508.5943558082, 421508.5943558089, 148461.3393554893]
[2019-03-25 23:29:08,062] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:29:08,067] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.073232e-23], sampled 0.3744464380154293
[2019-03-25 23:29:13,076] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:13,077] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.25, 58.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9896867998984402, 6.911200000000001, 6.9112, 168.9129092994872, 800378.2058937941, 800378.2058937935, 244556.5044694144]
[2019-03-25 23:29:13,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:29:13,082] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8225420621113795
[2019-03-25 23:29:16,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:16,102] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.7, 64.0, 1.0, 2.0, 0.7112049518070835, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.995357074570641, 6.9112, 168.9123862085816, 1890808.605152779, 1831104.881568125, 387502.7715547181]
[2019-03-25 23:29:16,104] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:29:16,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5919480e-37 2.5499384e-08], sampled 0.708162408751807
[2019-03-25 23:29:16,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1890808.605152779 W.
[2019-03-25 23:29:25,096] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:25,097] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.73333333333333, 69.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8982250730879242, 6.9112, 6.9112, 168.912956510431, 740558.1463655104, 740558.1463655104, 222982.6473525325]
[2019-03-25 23:29:25,097] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:29:25,101] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9863112674055067
[2019-03-25 23:29:34,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:34,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.61686643, 80.93137067666666, 1.0, 2.0, 0.487379651143759, 1.0, 2.0, 0.487379651143759, 1.0, 2.0, 0.8350829703151939, 6.911200000000001, 6.9112, 184.5923449428631, 2044239.281839016, 2044239.281839015, 408657.843051303]
[2019-03-25 23:29:34,764] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:29:34,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6642518e-17 0.0000000e+00 2.1521509e-38 1.3318225e-38 1.0000000e+00], sampled 0.15317740242895483
[2019-03-25 23:29:37,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:37,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.35, 79.0, 1.0, 2.0, 0.9173503864208028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1282206.698759779, 1282206.698759779, 274759.7707401625]
[2019-03-25 23:29:37,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:29:37,990] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.7384971e-01 0.0000000e+00 0.0000000e+00 1.1725076e-36 2.6150325e-02], sampled 0.9942427446335284
[2019-03-25 23:29:51,632] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.65153074], dtype=float32), 0.21773787]
[2019-03-25 23:29:51,632] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.80962164, 88.54499237499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9294632183690987, 6.9112, 6.9112, 168.912956510431, 761269.6018588615, 761269.6018588615, 230144.5433456439]
[2019-03-25 23:29:51,634] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:29:51,638] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3782436e-27], sampled 0.02942547665311923
[2019-03-25 23:30:28,998] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8134.5123 2989883227.9064 1023.0000
[2019-03-25 23:30:29,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7207.9016 3225915848.8584 1961.0000
[2019-03-25 23:30:29,312] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7991.3387 3035418294.3490 1182.0000
[2019-03-25 23:30:29,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7405.8249 3358255440.6958 1683.0000
[2019-03-25 23:30:29,623] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7494.8514 3145179453.9952 1656.0000
[2019-03-25 23:30:30,639] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 650000, evaluation results [650000.0, 7405.824867151293, 3358255440.6957955, 1683.0, 7494.851433673296, 3145179453.995195, 1656.0, 8134.512255869253, 2989883227.9063897, 1023.0, 7207.901568359049, 3225915848.858367, 1961.0, 7991.338671642057, 3035418294.3490305, 1182.0]
[2019-03-25 23:30:31,976] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9999893e-01 1.3835438e-31 1.2115056e-28 7.2021946e-25 1.0539814e-06], sum to 1.0000
[2019-03-25 23:30:31,981] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2861
[2019-03-25 23:30:31,992] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1027814.441741859 W.
[2019-03-25 23:30:32,000] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.7, 53.0, 1.0, 2.0, 0.3139738995085048, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5771231545987503, 6.9112, 6.9112, 168.912956510431, 1027814.441741859, 1027814.441741859, 238535.1347675458], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 658200.0000, 
sim time next is 658800.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.279834860644303, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5148588267803188, 6.911200000000001, 6.9112, 168.912956510431, 918044.9533558163, 918044.9533558157, 226286.2699712138], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.53, 1.0, 1.0, 0.1323311574027747, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.40836442290282776, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25501248704328233, 0.25501248704328217, 0.33774070144957286], 
reward next is 0.6623, 
noisyNet noise sample is [array([-1.2966846], dtype=float32), -0.30496046]. 
=============================================
[2019-03-25 23:30:46,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:30:46,356] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2827
[2019-03-25 23:30:46,367] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.1, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4576077645500669, 6.911200000000001, 6.9112, 168.912956510431, 409016.1123429117, 409016.1123429111, 146750.3036664], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 673200.0000, 
sim time next is 673800.0000, 
raw observation next is [21.88333333333333, 69.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4559722842627044, 6.9112, 6.9112, 168.912956510431, 407621.5253254219, 407621.5253254219, 146561.972748505], 
processed observation next is [1.0, 0.8260869565217391, 0.2361769352290678, 0.6916666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.33655156617402976, 0.0, 0.0, 0.8294399451523027, 0.11322820147928386, 0.11322820147928386, 0.21874921305747017], 
reward next is 0.7813, 
noisyNet noise sample is [array([-0.79381716], dtype=float32), -0.03957581]. 
=============================================
[2019-03-25 23:30:46,977] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:30:46,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9156
[2019-03-25 23:30:46,995] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.9, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4443082454123581, 6.9112, 6.9112, 168.912956510431, 398089.9218956504, 398089.9218956504, 145209.1723755067], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [19.8, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4430855671391422, 6.9112, 6.9112, 168.912956510431, 397035.174288188, 397035.174288188, 145073.2281093803], 
processed observation next is [1.0, 0.9130434782608695, 0.13744075829383895, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.32083605748675875, 0.0, 0.0, 0.8294399451523027, 0.11028754841338555, 0.11028754841338555, 0.21652720613340343], 
reward next is 0.7835, 
noisyNet noise sample is [array([-1.4618613], dtype=float32), -0.78136885]. 
=============================================
[2019-03-25 23:30:50,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3396925e-07 0.0000000e+00 4.2177948e-36 0.0000000e+00 9.9999976e-01], sum to 1.0000
[2019-03-25 23:30:50,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1041
[2019-03-25 23:30:50,664] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.06666666666666, 56.0, 1.0, 2.0, 0.1827351320053474, 1.0, 2.0, 0.1827351320053474, 1.0, 2.0, 0.3326966802023258, 6.9112, 6.9112, 170.5573041426782, 880113.4016069553, 880113.4016069553, 277840.434331299], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 733200.0000, 
sim time next is 733800.0000, 
raw observation next is [25.13333333333333, 55.5, 1.0, 2.0, 0.1809639941568561, 1.0, 2.0, 0.1809639941568561, 1.0, 2.0, 0.329597320383417, 6.9112, 6.9112, 170.5573041426782, 872194.9997801242, 872194.9997801242, 277379.1679383902], 
processed observation next is [1.0, 0.4782608695652174, 0.3902053712480251, 0.555, 1.0, 1.0, 0.013209631514284444, 1.0, 1.0, 0.013209631514284444, 1.0, 1.0, 0.18243575656514266, 0.0, 0.0, 0.8375144448122397, 0.24227638882781227, 0.24227638882781227, 0.4139987581170003], 
reward next is 0.5860, 
noisyNet noise sample is [array([-0.4286548], dtype=float32), 0.21088944]. 
=============================================
[2019-03-25 23:30:56,125] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7760440e-37 3.7972788e-17], sum to 1.0000
[2019-03-25 23:30:56,140] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4238
[2019-03-25 23:30:56,145] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6017668032100495, 6.9112, 6.9112, 168.912956510431, 522945.0502357254, 522945.0502357254, 166366.4461114486], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 949200.0000, 
sim time next is 949800.0000, 
raw observation next is [21.8, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6041245652711931, 6.9112, 6.9112, 168.912956510431, 524994.6937871921, 524994.6937871921, 166727.050845814], 
processed observation next is [0.0, 1.0, 0.23222748815165886, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5172250795990159, 0.0, 0.0, 0.8294399451523027, 0.14583185938533114, 0.14583185938533114, 0.24884634454599103], 
reward next is 0.7512, 
noisyNet noise sample is [array([-0.95326334], dtype=float32), -0.42470554]. 
=============================================
[2019-03-25 23:30:59,995] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:31:00,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4752
[2019-03-25 23:31:00,020] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.3, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5124202793199643, 6.911200000000001, 6.9112, 168.912956510431, 452744.3967277965, 452744.3967277958, 153607.0061820941], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 885000.0000, 
sim time next is 885600.0000, 
raw observation next is [21.4, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.516507440692165, 6.911200000000001, 6.9112, 168.912956510431, 456214.7829135215, 456214.7829135209, 154138.0885939306], 
processed observation next is [0.0, 0.2608695652173913, 0.21327014218009477, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41037492767337197, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12672632858708932, 0.12672632858708913, 0.2300568486476576], 
reward next is 0.7699, 
noisyNet noise sample is [array([-1.3784598], dtype=float32), -0.22198349]. 
=============================================
[2019-03-25 23:31:01,228] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.2702783e-38 0.0000000e+00], sum to 1.0000
[2019-03-25 23:31:01,239] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7936
[2019-03-25 23:31:01,244] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.538230740217674, 6.9112, 6.9112, 168.912956510431, 473492.4332114068, 473492.4332114068, 157076.8164104944], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1060800.0000, 
sim time next is 1061400.0000, 
raw observation next is [20.55, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.541087859008112, 6.9112, 6.9112, 168.912956510431, 475780.7580911931, 475780.7580911931, 157471.4177834782], 
processed observation next is [1.0, 0.2608695652173913, 0.17298578199052142, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44035104757086824, 0.0, 0.0, 0.8294399451523027, 0.1321613216919981, 0.1321613216919981, 0.2350319668410122], 
reward next is 0.7650, 
noisyNet noise sample is [array([-1.5258532], dtype=float32), 0.11752945]. 
=============================================
[2019-03-25 23:31:02,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:31:02,977] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6271
[2019-03-25 23:31:02,984] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5563245382366822, 6.911200000000001, 6.9112, 168.912956510431, 487224.2454706425, 487224.2454706419, 159631.441449538], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 914400.0000, 
sim time next is 915000.0000, 
raw observation next is [24.95, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5575027921940405, 6.9112, 6.9112, 168.912956510431, 488137.1331692123, 488137.1331692123, 159799.9025760866], 
processed observation next is [0.0, 0.6086956521739131, 0.3815165876777251, 0.665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.46036925877322016, 0.0, 0.0, 0.8294399451523027, 0.13559364810255897, 0.13559364810255897, 0.23850731727774122], 
reward next is 0.7615, 
noisyNet noise sample is [array([0.09234022], dtype=float32), -0.4096639]. 
=============================================
[2019-03-25 23:31:02,996] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.69565 ]
 [80.63236 ]
 [80.633514]
 [80.63459 ]
 [80.649925]], R is [[80.66880798]
 [80.62386322]
 [80.57955933]
 [80.53593445]
 [80.4930191 ]].
[2019-03-25 23:31:07,727] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.0778647e-37 0.0000000e+00 1.9575380e-31 6.1850298e-23], sum to 1.0000
[2019-03-25 23:31:07,738] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3083
[2019-03-25 23:31:07,746] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 84.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5621026913370676, 6.911199999999999, 6.9112, 168.912956510431, 494175.8738208034, 494175.873820804, 160389.9686812671], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1150800.0000, 
sim time next is 1151400.0000, 
raw observation next is [22.05, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5681748813181543, 6.9112, 6.9112, 168.912956510431, 499226.080001701, 499226.080001701, 161264.0259759963], 
processed observation next is [1.0, 0.30434782608695654, 0.24407582938388633, 0.8366666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4733840016075052, 0.0, 0.0, 0.8294399451523027, 0.13867391111158361, 0.13867391111158361, 0.24069257608357658], 
reward next is 0.7593, 
noisyNet noise sample is [array([0.5525905], dtype=float32), 0.7544283]. 
=============================================
[2019-03-25 23:31:08,459] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 3.777521e-36 2.236451e-29], sum to 1.0000
[2019-03-25 23:31:08,467] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0692
[2019-03-25 23:31:08,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 899858.9527263594 W.
[2019-03-25 23:31:08,485] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.96666666666667, 93.66666666666667, 1.0, 1.0, 0.2958161179540738, 1.0, 1.0, 0.2958161179540738, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 899858.9527263594, 899858.9527263594, 258414.8089535659], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 985200.0000, 
sim time next is 985800.0000, 
raw observation next is [21.98333333333333, 93.83333333333334, 1.0, 2.0, 0.1991907344021396, 1.0, 2.0, 0.1991907344021396, 1.0, 1.0, 0.3500531052612074, 6.9112, 6.9112, 170.5573041426782, 904864.1202816038, 904864.1202816038, 277748.5791174788], 
processed observation next is [1.0, 0.391304347826087, 0.24091627172195884, 0.9383333333333335, 1.0, 1.0, 0.035169559520650116, 1.0, 1.0, 0.035169559520650116, 1.0, 0.5, 0.20738183568439927, 0.0, 0.0, 0.8375144448122397, 0.2513511445226677, 0.2513511445226677, 0.41455011808578923], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5816151], dtype=float32), -0.560031]. 
=============================================
[2019-03-25 23:31:35,110] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:31:35,121] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2616
[2019-03-25 23:31:35,132] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.06666666666667, 96.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5767239985345034, 6.9112, 6.9112, 168.912956510431, 504130.242019396, 504130.242019396, 162571.1722910131], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1489200.0000, 
sim time next is 1489800.0000, 
raw observation next is [21.28333333333333, 95.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5834120949782879, 6.9112, 6.9112, 168.912956510431, 509261.1217800895, 509261.1217800895, 163568.1984000275], 
processed observation next is [0.0, 0.21739130434782608, 0.20774091627172192, 0.9566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49196596948571697, 0.0, 0.0, 0.8294399451523027, 0.14146142271669154, 0.14146142271669154, 0.24413163940302612], 
reward next is 0.7559, 
noisyNet noise sample is [array([-0.34621543], dtype=float32), -0.00624549]. 
=============================================
[2019-03-25 23:31:35,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:31:35,231] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1351
[2019-03-25 23:31:35,239] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.66666666666667, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.588969477184166, 6.9112, 6.9112, 168.912956510431, 515053.3206021808, 515053.3206021808, 164370.4563737181], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1567200.0000, 
sim time next is 1567800.0000, 
raw observation next is [21.65, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5952950874291942, 6.9112, 6.9112, 168.912956510431, 520673.3528020088, 520673.3528020088, 165316.3104553195], 
processed observation next is [1.0, 0.13043478260869565, 0.22511848341232227, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5064574236941393, 0.0, 0.0, 0.8294399451523027, 0.14463148688944688, 0.14463148688944688, 0.2467407618736112], 
reward next is 0.7533, 
noisyNet noise sample is [array([-1.2400429], dtype=float32), 1.2735552]. 
=============================================
[2019-03-25 23:31:38,214] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-25 23:31:38,217] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:31:38,218] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:31:38,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:31:38,221] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:31:38,222] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:31:38,224] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:31:38,222] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:31:38,227] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:31:38,228] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:31:38,228] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:31:38,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-25 23:31:38,268] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-25 23:31:38,291] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-25 23:31:38,312] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-25 23:31:38,312] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-25 23:31:44,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:31:44,705] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.41666666666666, 65.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.41905117530339, 6.9112, 6.9112, 168.912956510431, 377092.2104903346, 377092.2104903346, 142413.1678241215]
[2019-03-25 23:31:44,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:31:44,707] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7951439837586631
[2019-03-25 23:31:52,139] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:31:52,141] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.23104073166667, 87.78225653666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.414824535602306, 6.911200000000001, 6.9112, 168.912956510431, 373704.8441272785, 373704.8441272779, 141947.6927316504]
[2019-03-25 23:31:52,143] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:31:52,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.05747538002346597
[2019-03-25 23:33:26,410] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:33:26,410] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 77.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.001720764203979, 6.9112, 168.9068805393628, 2227906.462328975, 1454281.328459571, 311351.6248739538]
[2019-03-25 23:33:26,411] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:33:26,413] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4406236e-38], sampled 0.4681424620864295
[2019-03-25 23:33:26,414] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2227906.462328975 W.
[2019-03-25 23:33:36,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:33:36,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.63676575833333, 87.57789325833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7054645777248173, 6.911199999999999, 6.9112, 168.912956510431, 604907.577665095, 604907.5776650956, 183714.469365212]
[2019-03-25 23:33:36,783] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:33:36,787] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6856533355689449
[2019-03-25 23:33:37,303] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:33:37,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.7, 57.0, 1.0, 2.0, 0.6011034368782531, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.926992416011114, 6.9112, 168.9127397689554, 1680673.90764485, 1669470.238569615, 364799.2704422888]
[2019-03-25 23:33:37,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:33:37,312] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2427394e-36], sampled 0.35945629216744146
[2019-03-25 23:33:37,314] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1680673.90764485 W.
[2019-03-25 23:33:38,647] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:33:38,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.85, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6446803375579799, 6.9112, 6.9112, 168.912956510431, 557398.0143201961, 557398.0143201961, 173202.0081119766]
[2019-03-25 23:33:38,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:33:38,653] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.0036929185204450654
[2019-03-25 23:33:39,995] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7266731], dtype=float32), 0.18896969]
[2019-03-25 23:33:39,996] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7095169990406084, 6.9112, 6.9112, 168.912956510431, 607594.011278722, 607594.011278722, 184449.0949098317]
[2019-03-25 23:33:39,996] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:33:39,999] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07382288070283038
[2019-03-25 23:33:47,591] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.6389 3105817113.6086 2011.0000
[2019-03-25 23:33:47,657] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.2577 2937954570.8307 1381.0000
[2019-03-25 23:33:47,802] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8919 3185219386.8708 2464.0000
[2019-03-25 23:33:47,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8870 2989446328.4271 1566.0000
[2019-03-25 23:33:47,964] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7287.4826 3319658082.8562 2143.0000
[2019-03-25 23:33:48,982] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 675000, evaluation results [675000.0, 7287.48261098085, 3319658082.856155, 2143.0, 7346.638914708317, 3105817113.6086044, 2011.0, 8060.2577192199915, 2937954570.8307285, 1381.0, 7029.891904325195, 3185219386.870774, 2464.0, 7923.886968186372, 2989446328.4270563, 1566.0]
[2019-03-25 23:34:01,961] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:34:01,971] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7829
[2019-03-25 23:34:01,978] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666666, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8627615904083696, 6.911200000000001, 6.9112, 168.912956510431, 713059.179542403, 713059.1795424023, 214995.2031482915], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1708800.0000, 
sim time next is 1709400.0000, 
raw observation next is [27.53333333333333, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8602788589767054, 6.9112, 6.9112, 168.912956510431, 711304.1724018055, 711304.1724018055, 214454.3738518324], 
processed observation next is [1.0, 0.782608695652174, 0.5039494470774091, 0.8133333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8296083646057383, 0.0, 0.0, 0.8294399451523027, 0.19758449233383488, 0.19758449233383488, 0.32008115500273493], 
reward next is 0.6799, 
noisyNet noise sample is [array([1.4847785], dtype=float32), 0.59017664]. 
=============================================
[2019-03-25 23:34:06,334] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:34:06,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1155
[2019-03-25 23:34:06,352] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 97.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7168031751313353, 6.9112, 6.9112, 168.912956510431, 611053.8734260148, 611053.8734260148, 185771.6297033195], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1635000.0000, 
sim time next is 1635600.0000, 
raw observation next is [23.1, 97.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7167055578951139, 6.911200000000001, 6.9112, 168.912956510431, 610836.2120430194, 610836.2120430188, 185752.8692853509], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.9733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6545189730428217, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16967672556750538, 0.1696767255675052, 0.27724308848559837], 
reward next is 0.7228, 
noisyNet noise sample is [array([0.10746656], dtype=float32), -0.4795809]. 
=============================================
[2019-03-25 23:34:09,232] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 5.5036517e-34 9.3218808e-38 7.5004862e-32 2.0136278e-11], sum to 1.0000
[2019-03-25 23:34:09,243] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8052
[2019-03-25 23:34:09,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1906328.020806197 W.
[2019-03-25 23:34:09,261] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.9, 74.0, 1.0, 2.0, 0.6817435013622294, 1.0, 2.0, 0.6817435013622294, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1906328.020806197, 1906328.020806197, 366402.5280322376], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1702800.0000, 
sim time next is 1703400.0000, 
raw observation next is [28.78333333333333, 74.66666666666667, 1.0, 2.0, 0.2225570571565132, 1.0, 2.0, 0.2225570571565132, 1.0, 1.0, 0.3800661581274602, 6.9112, 6.9112, 170.5573041426782, 933065.6862250619, 933065.6862250619, 276025.3524021173], 
processed observation next is [1.0, 0.7391304347826086, 0.5631911532385465, 0.7466666666666667, 1.0, 1.0, 0.06332175561025685, 1.0, 1.0, 0.06332175561025685, 1.0, 0.5, 0.24398311966763436, 0.0, 0.0, 0.8375144448122397, 0.259184912840295, 0.259184912840295, 0.4119781379136079], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9894352], dtype=float32), -2.2884398]. 
=============================================
[2019-03-25 23:34:19,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7963794e-38 2.2612332e-30], sum to 1.0000
[2019-03-25 23:34:19,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0493
[2019-03-25 23:34:19,720] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6475901185513991, 6.9112, 6.9112, 168.912956510431, 561563.3696320932, 561563.3696320932, 173662.1869986197], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1825200.0000, 
sim time next is 1825800.0000, 
raw observation next is [21.9, 95.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6440205533506007, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 173076.5359018207], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.9516666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5658787235982935, 0.0, 0.0, 0.8294399451523027, 0.15509223345444534, 0.15509223345444534, 0.2583231879131652], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.62796074], dtype=float32), -1.4523736]. 
=============================================
[2019-03-25 23:34:20,269] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 6.0967092e-33 1.4183961e-35 2.6326886e-31 5.5459859e-10], sum to 1.0000
[2019-03-25 23:34:20,276] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8509
[2019-03-25 23:34:20,289] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1668469.64428898 W.
[2019-03-25 23:34:20,295] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.26666666666667, 75.33333333333334, 1.0, 2.0, 0.5967464542190666, 1.0, 1.0, 0.5967464542190666, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1668469.64428898, 1668469.64428898, 333041.152102429], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1952400.0000, 
sim time next is 1953000.0000, 
raw observation next is [27.1, 76.0, 1.0, 2.0, 0.3964854807977419, 1.0, 2.0, 0.3964854807977419, 1.0, 1.0, 0.6657755603440407, 6.9112, 6.9112, 170.5573041426782, 1662822.399988581, 1662822.399988581, 347707.5997048828], 
processed observation next is [1.0, 0.6086956521739131, 0.4834123222748816, 0.76, 1.0, 1.0, 0.2728740732502914, 1.0, 1.0, 0.2728740732502914, 1.0, 0.5, 0.592409219931757, 0.0, 0.0, 0.8375144448122397, 0.4618951111079392, 0.4618951111079392, 0.5189665667237057], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2142215], dtype=float32), 1.7425689]. 
=============================================
[2019-03-25 23:34:20,310] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[60.21858 ]
 [60.596886]
 [60.453327]
 [60.023838]
 [61.05304 ]], R is [[60.04837799]
 [59.44789505]
 [58.85341644]
 [58.26488113]
 [57.87501144]].
[2019-03-25 23:34:23,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.5210467e-37], sum to 1.0000
[2019-03-25 23:34:23,990] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4273
[2019-03-25 23:34:23,996] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.26666666666667, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7758482107682502, 6.911199999999999, 6.9112, 168.912956510431, 653402.7227129347, 653402.7227129353, 196999.6780047884], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1905600.0000, 
sim time next is 1906200.0000, 
raw observation next is [24.25, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7776168581730661, 6.9112, 6.9112, 168.912956510431, 654836.5348453631, 654836.5348453631, 197352.1181651906], 
processed observation next is [1.0, 0.043478260869565216, 0.3483412322274882, 0.955, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7288010465525195, 0.0, 0.0, 0.8294399451523027, 0.1818990374570453, 0.1818990374570453, 0.29455540024655313], 
reward next is 0.7054, 
noisyNet noise sample is [array([0.4905894], dtype=float32), -1.2219535]. 
=============================================
[2019-03-25 23:34:26,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:34:26,257] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0956
[2019-03-25 23:34:26,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1506373.16801152 W.
[2019-03-25 23:34:26,275] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.05, 80.33333333333334, 1.0, 2.0, 0.5315090417492212, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8929251920734828, 6.9112, 6.9112, 168.912956510431, 1506373.16801152, 1506373.16801152, 321213.2263891071], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1936200.0000, 
sim time next is 1936800.0000, 
raw observation next is [26.1, 80.0, 1.0, 2.0, 0.3215703098692732, 1.0, 1.0, 0.3215703098692732, 1.0, 2.0, 0.5363554497952511, 6.9112, 6.9112, 170.5573041426782, 1348437.659085979, 1348437.659085979, 311254.6203281507], 
processed observation next is [1.0, 0.43478260869565216, 0.4360189573459717, 0.8, 1.0, 1.0, 0.18261483116779903, 1.0, 0.5, 0.18261483116779903, 1.0, 1.0, 0.4345798168234769, 0.0, 0.0, 0.8375144448122397, 0.37456601641277193, 0.37456601641277193, 0.4645591348181354], 
reward next is 0.5354, 
noisyNet noise sample is [array([1.1114345], dtype=float32), 0.036902588]. 
=============================================
[2019-03-25 23:34:30,982] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:34:30,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7372
[2019-03-25 23:34:30,999] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.85, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6942889808284939, 6.9112, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784811, 181711.0044411647], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1974600.0000, 
sim time next is 1975200.0000, 
raw observation next is [22.9, 95.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.697362965094965, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 182258.7634777839], 
processed observation next is [1.0, 0.8695652173913043, 0.2843601895734597, 0.9566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6309304452377621, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16599520968387887, 0.16599520968387868, 0.27202800519072223], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.4333582], dtype=float32), -0.6417293]. 
=============================================
[2019-03-25 23:34:40,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.904266e-36], sum to 1.0000
[2019-03-25 23:34:40,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4722
[2019-03-25 23:34:40,897] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 74.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9567269129255369, 6.9112, 6.9112, 168.912956510431, 775305.1262493634, 775305.1262493634, 236379.4684820188], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2116200.0000, 
sim time next is 2116800.0000, 
raw observation next is [30.0, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.960123966394992, 6.9112, 6.9112, 168.912956510431, 777717.9206643502, 777717.9206643502, 237200.1098656891], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9513706907255999, 0.0, 0.0, 0.8294399451523027, 0.21603275574009725, 0.21603275574009725, 0.3540300147249091], 
reward next is 0.6460, 
noisyNet noise sample is [array([0.78464323], dtype=float32), -0.79023314]. 
=============================================
[2019-03-25 23:34:42,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.7505809e-38 0.0000000e+00 8.7906328e-38 3.5766525e-19], sum to 1.0000
[2019-03-25 23:34:42,918] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1034
[2019-03-25 23:34:42,921] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.01666666666667, 80.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9650372535983688, 6.9112, 6.9112, 168.912956510431, 782474.6424401292, 782474.6424401292, 238458.8314692526], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2326200.0000, 
sim time next is 2326800.0000, 
raw observation next is [28.93333333333334, 80.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9604415821880583, 6.9112, 6.9112, 168.912956510431, 779410.8681605118, 779410.8681605118, 237353.2333212982], 
processed observation next is [1.0, 0.9565217391304348, 0.5703001579778835, 0.8033333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9517580270586077, 0.0, 0.0, 0.8294399451523027, 0.2165030189334755, 0.2165030189334755, 0.3542585571959675], 
reward next is 0.6457, 
noisyNet noise sample is [array([0.48689812], dtype=float32), 2.8551006]. 
=============================================
[2019-03-25 23:34:50,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 5.2429615e-31 2.0129282e-34 3.5939899e-29 3.5941219e-17], sum to 1.0000
[2019-03-25 23:34:50,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3240
[2019-03-25 23:34:50,275] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.85, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9503345718521153, 6.9112, 6.9112, 168.912956510431, 772890.7641782783, 772890.7641782783, 234950.2765682981], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [28.8, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9471154729714286, 6.911200000000001, 6.9112, 168.912956510431, 770861.905343396, 770861.9053433953, 234192.2096578433], 
processed observation next is [1.0, 0.043478260869565216, 0.5639810426540285, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9355066743554008, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2141283070398322, 0.21412830703983202, 0.3495406114296169], 
reward next is 0.6505, 
noisyNet noise sample is [array([-0.16721448], dtype=float32), 0.18669672]. 
=============================================
[2019-03-25 23:34:51,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.9373987e-10 7.4747378e-28 1.6218798e-29 8.4119913e-29 1.0000000e+00], sum to 1.0000
[2019-03-25 23:34:51,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4233
[2019-03-25 23:34:51,855] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.3, 80.0, 1.0, 1.0, 0.1914940532746285, 1.0, 1.0, 0.1914940532746285, 1.0, 2.0, 0.3325618424869025, 6.9112, 6.9112, 170.5573041426782, 802785.9664841766, 802785.9664841766, 267539.6152444007], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2415600.0000, 
sim time next is 2416200.0000, 
raw observation next is [29.26666666666667, 80.0, 1.0, 2.0, 0.1907195777747854, 1.0, 2.0, 0.1907195777747854, 1.0, 2.0, 0.3312168346666365, 6.9112, 6.9112, 170.5573041426782, 799537.981500744, 799537.981500744, 267334.6511494467], 
processed observation next is [1.0, 1.0, 0.5860979462875199, 0.8, 1.0, 1.0, 0.024963346716608905, 1.0, 1.0, 0.024963346716608905, 1.0, 1.0, 0.18441077398370304, 0.0, 0.0, 0.8375144448122397, 0.22209388375020667, 0.22209388375020667, 0.3990069420140996], 
reward next is 0.6010, 
noisyNet noise sample is [array([-0.486282], dtype=float32), 0.87929255]. 
=============================================
[2019-03-25 23:34:56,455] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-25 23:34:56,458] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:34:56,459] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:34:56,460] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:34:56,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:34:56,465] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:34:56,465] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:34:56,467] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:34:56,468] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:34:56,466] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:34:56,472] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:34:56,495] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-25 23:34:56,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-25 23:34:56,519] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-25 23:34:56,519] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-25 23:34:56,539] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-25 23:35:26,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:35:26,979] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.41817563333333, 93.5322913, 1.0, 2.0, 0.6590285770823913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 920986.3859525837, 920986.385952583, 212247.9295247093]
[2019-03-25 23:35:26,980] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:35:26,984] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 2.2821726e-35 4.8902872e-35 4.0437713e-13], sampled 0.0015919486050494092
[2019-03-25 23:35:26,984] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 920986.3859525837 W.
[2019-03-25 23:35:38,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:35:38,844] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.95, 68.5, 1.0, 2.0, 0.6168526874560561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862022.0482386906, 862022.0482386912, 203918.7669976269]
[2019-03-25 23:35:38,845] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:35:38,847] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 5.118268e-38 4.423090e-09], sampled 0.15682907978127536
[2019-03-25 23:35:51,968] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:35:51,969] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.91666666666666, 59.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9735452070067568, 6.911199999999999, 6.9112, 168.912956510431, 790324.0605237822, 790324.0605237827, 240633.5470265553]
[2019-03-25 23:35:51,973] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:35:51,975] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4496261867363326
[2019-03-25 23:36:20,558] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:36:20,560] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9473884646493791, 6.911200000000001, 6.9112, 168.912956510431, 769927.7497968861, 769927.7497968855, 234201.7782844596]
[2019-03-25 23:36:20,562] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:36:20,564] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.238643060191693
[2019-03-25 23:36:25,340] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:36:25,342] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 78.16666666666666, 1.0, 2.0, 0.7011560177597613, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992723838647949, 6.9112, 168.9124046272878, 1876746.474838442, 1818910.846832459, 385402.3632469054]
[2019-03-25 23:36:25,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:36:25,349] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.818982e-17], sampled 0.7991811400141526
[2019-03-25 23:36:25,351] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1876746.474838442 W.
[2019-03-25 23:36:30,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:36:30,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.26666666666667, 77.66666666666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.02758716855448, 6.9112, 168.9121522706876, 911401.8038793907, 828833.1339030309, 254812.2272775755]
[2019-03-25 23:36:30,586] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:36:30,589] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 7.4531249e-36 1.8231256e-36 1.4933837e-12], sampled 0.04520034539953954
[2019-03-25 23:36:30,590] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 911401.8038793907 W.
[2019-03-25 23:37:05,239] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.6248235], dtype=float32), 0.21847437]
[2019-03-25 23:37:05,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.33639499666667, 78.39879234666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9097481972259753, 6.9112, 6.9112, 168.912956510431, 748493.9367890082, 748493.9367890082, 225609.889094876]
[2019-03-25 23:37:05,240] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:37:05,243] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2831138586249923
[2019-03-25 23:37:05,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8081.3744 2942918009.7538 1311.0000
[2019-03-25 23:37:05,556] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.9277 3109005081.8749 1998.0000
[2019-03-25 23:37:05,773] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7324.7588 3324355999.9647 2033.0000
[2019-03-25 23:37:05,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7964.1573 2993410257.4299 1460.0000
[2019-03-25 23:37:06,171] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7049.5183 3187682188.8692 2403.0000
[2019-03-25 23:37:07,190] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 700000, evaluation results [700000.0, 7324.758802996227, 3324355999.9646907, 2033.0, 7348.927719936029, 3109005081.8749413, 1998.0, 8081.374350758465, 2942918009.753812, 1311.0, 7049.518327627775, 3187682188.869244, 2403.0, 7964.157250207836, 2993410257.429923, 1460.0]
[2019-03-25 23:37:26,688] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:26,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5433
[2019-03-25 23:37:26,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.56666666666667, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7846540688456162, 6.911200000000001, 6.9112, 168.912956510431, 660557.0414491901, 660557.0414491895, 198762.9533697872], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2596800.0000, 
sim time next is 2597400.0000, 
raw observation next is [24.5, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7788739039784744, 6.911200000000001, 6.9112, 168.912956510431, 656448.3001028345, 656448.3001028338, 197613.5985023036], 
processed observation next is [0.0, 0.043478260869565216, 0.3601895734597157, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.730334029242042, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18234675002856512, 0.18234675002856493, 0.29494566940642325], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.9643551], dtype=float32), 1.5022317]. 
=============================================
[2019-03-25 23:37:26,714] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:26,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9674
[2019-03-25 23:37:26,734] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.26666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7578648494674003, 6.9112, 6.9112, 168.912956510431, 641486.3640677447, 641486.3640677447, 193503.8734463314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2599800.0000, 
sim time next is 2600400.0000, 
raw observation next is [24.23333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.755175520433319, 6.9112, 6.9112, 168.912956510431, 639556.9271264095, 639556.9271264095, 192985.3232757728], 
processed observation next is [0.0, 0.08695652173913043, 0.3475513428120062, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7014335615040477, 0.0, 0.0, 0.8294399451523027, 0.17765470197955818, 0.17765470197955818, 0.28803779593398926], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.5841974], dtype=float32), -0.4452672]. 
=============================================
[2019-03-25 23:37:30,229] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:30,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2637
[2019-03-25 23:37:30,250] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.83333333333333, 89.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7175190974757433, 6.911199999999999, 6.9112, 168.912956510431, 612542.7685328014, 612542.768532802, 185907.8451721902], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2661000.0000, 
sim time next is 2661600.0000, 
raw observation next is [23.66666666666666, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7117951321115529, 6.9112, 6.9112, 168.912956510431, 608200.7687486969, 608200.7687486969, 184861.0006985416], 
processed observation next is [0.0, 0.8260869565217391, 0.3206951026856238, 0.9066666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6485306489165278, 0.0, 0.0, 0.8294399451523027, 0.16894465798574915, 0.16894465798574915, 0.27591194134110686], 
reward next is 0.7241, 
noisyNet noise sample is [array([-2.0481372], dtype=float32), -0.6856791]. 
=============================================
[2019-03-25 23:37:38,971] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:38,979] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5616
[2019-03-25 23:37:38,984] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6050518029746351, 6.9112, 6.9112, 168.912956510431, 526324.1206999633, 526324.1206999633, 166860.607992694], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6047955851145521, 6.911199999999999, 6.9112, 168.912956510431, 526098.6374142431, 526098.6374142438, 166821.3323748149], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.518043396481161, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1461385103928453, 0.1461385103928455, 0.24898706324599237], 
reward next is 0.7510, 
noisyNet noise sample is [array([-0.7944329], dtype=float32), 0.06272841]. 
=============================================
[2019-03-25 23:37:40,382] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:40,392] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0526
[2019-03-25 23:37:40,397] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5487992700970943, 6.911199999999999, 6.9112, 168.912956510431, 482275.4127980136, 482275.4127980142, 158536.7243250376], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2944800.0000, 
sim time next is 2945400.0000, 
raw observation next is [20.16666666666667, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6471098702167476, 6.911200000000001, 6.9112, 168.912956510431, 568563.913527716, 568563.9135277154, 173412.4455945982], 
processed observation next is [1.0, 0.08695652173913043, 0.15481832543443946, 0.9900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5696461831911556, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15793442042436553, 0.15793442042436537, 0.2588245456635794], 
reward next is 0.7412, 
noisyNet noise sample is [array([-0.93630755], dtype=float32), -1.0331796]. 
=============================================
[2019-03-25 23:37:40,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8396584e-01 2.0405146e-28 4.9514808e-31 1.4126902e-28 4.1603416e-01], sum to 1.0000
[2019-03-25 23:37:40,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0040
[2019-03-25 23:37:40,949] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1228466.356836531 W.
[2019-03-25 23:37:40,955] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.2873715697594688, 1.0, 2.0, 0.2873715697594688, 1.0, 2.0, 0.4846943574870307, 6.911199999999999, 6.9112, 170.5573041426782, 1228466.356836531, 1228466.356836532, 300197.7611684147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2818200.0000, 
sim time next is 2818800.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.8386022136597922, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217806.696836881, 1217806.696836881, 260082.0209197442], 
processed observation next is [1.0, 0.6521739130434783, 0.38388625592417064, 0.83, 1.0, 1.0, 0.8055448357346894, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33827963801024474, 0.33827963801024474, 0.3881821207757376], 
reward next is 0.6118, 
noisyNet noise sample is [array([-0.81615907], dtype=float32), -1.1038235]. 
=============================================
[2019-03-25 23:37:45,797] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:37:45,811] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0768
[2019-03-25 23:37:45,817] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.613967211579247, 6.9112, 6.9112, 168.912956510431, 532415.1094333314, 532415.1094333314, 168265.2263575591], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2864400.0000, 
sim time next is 2865000.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.617209826550234, 6.9112, 6.9112, 168.912956510431, 535227.7222545915, 535227.7222545915, 168772.3729905666], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5331827153051635, 0.0, 0.0, 0.8294399451523027, 0.1486743672929421, 0.1486743672929421, 0.2518990641650248], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.44812834], dtype=float32), 0.26284996]. 
=============================================
[2019-03-25 23:37:45,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.95166]
 [68.98861]
 [69.02333]
 [69.05347]
 [69.06669]], R is [[68.97807312]
 [69.03714752]
 [69.09828949]
 [69.15870667]
 [69.21294403]].
[2019-03-25 23:37:56,035] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1884956e-21], sum to 1.0000
[2019-03-25 23:37:56,046] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3106
[2019-03-25 23:37:56,055] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8285666183601225, 6.9112, 6.9112, 168.912956510431, 691139.5004237609, 691139.5004237609, 207744.1321511632], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3193200.0000, 
sim time next is 3193800.0000, 
raw observation next is [25.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8230015566668616, 6.911200000000001, 6.9112, 168.912956510431, 687413.9141745166, 687413.914174516, 206584.3154195187], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7841482398376362, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19094830949292127, 0.1909483094929211, 0.30833479913361], 
reward next is 0.6917, 
noisyNet noise sample is [array([0.9204313], dtype=float32), -0.19163166]. 
=============================================
[2019-03-25 23:38:04,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:38:04,951] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4583
[2019-03-25 23:38:04,955] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7954981849985898, 6.9112, 6.9112, 168.912956510431, 668212.2365948637, 668212.2365948637, 200940.1512230568], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3289200.0000, 
sim time next is 3289800.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.797222637764873, 6.911199999999999, 6.9112, 168.912956510431, 669662.7503428259, 669662.7503428266, 201293.8330249709], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7527105338596013, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18601743065078497, 0.18601743065078516, 0.3004385567536879], 
reward next is 0.6996, 
noisyNet noise sample is [array([1.3110089], dtype=float32), -1.0551319]. 
=============================================
[2019-03-25 23:38:06,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:38:06,093] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0598
[2019-03-25 23:38:06,102] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 81.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9152900337421918, 6.911199999999999, 6.9112, 168.912956510431, 750558.1911082439, 750558.1911082446, 226811.7999020952], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3357000.0000, 
sim time next is 3357600.0000, 
raw observation next is [28.0, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9057903180414518, 6.9112, 6.9112, 168.912956510431, 744168.8889322086, 744168.8889322086, 224639.688920603], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8851101439529899, 0.0, 0.0, 0.8294399451523027, 0.20671358025894684, 0.20671358025894684, 0.33528311779194475], 
reward next is 0.6647, 
noisyNet noise sample is [array([0.9248759], dtype=float32), 1.0790889]. 
=============================================
[2019-03-25 23:38:06,267] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:38:06,279] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3664
[2019-03-25 23:38:06,284] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333333, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.808051234673425, 6.9112, 6.9112, 168.912956510431, 676689.9161190981, 676689.9161190981, 203487.0290809524], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3283800.0000, 
sim time next is 3284400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8055752662493155, 6.911200000000001, 6.9112, 168.912956510431, 674903.1520813071, 674903.1520813066, 202979.164001731], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7628966661577018, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1874730978003631, 0.18747309780036292, 0.30295397612198655], 
reward next is 0.6970, 
noisyNet noise sample is [array([0.2591695], dtype=float32), 0.5211733]. 
=============================================
[2019-03-25 23:38:11,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 4.8776071e-32 1.7059107e-31 4.2393414e-31 1.1588714e-13], sum to 1.0000
[2019-03-25 23:38:11,619] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5929
[2019-03-25 23:38:11,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1061857.631593524 W.
[2019-03-25 23:38:11,634] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.2532616172012676, 1.0, 1.0, 0.2532616172012676, 1.0, 1.0, 0.4327154667735781, 6.9112, 6.9112, 170.5573041426782, 1061857.631593524, 1061857.631593524, 285992.3384106411], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3385200.0000, 
sim time next is 3385800.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.3863736996576196, 1.0, 2.0, 0.3863736996576196, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1079982.910145968, 1079982.910145968, 268747.2280342581], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.26069120440677057, 1.0, 1.0, 0.26069120440677057, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.29999525281832445, 0.29999525281832445, 0.40111526572277334], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.54271287], dtype=float32), -1.5558704]. 
=============================================
[2019-03-25 23:38:12,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:38:12,939] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8572
[2019-03-25 23:38:12,944] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333334, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9495169523919962, 6.911200000000001, 6.9112, 168.912956510431, 770257.1991235091, 770257.1991235085, 234651.248429115], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3238800.0000, 
sim time next is 3239400.0000, 
raw observation next is [31.66666666666667, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9653832347896923, 6.911199999999999, 6.9112, 168.912956510431, 780349.7669519929, 780349.7669519936, 238417.235710149], 
processed observation next is [0.0, 0.4782608695652174, 0.6998420221169038, 0.675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9577844326703564, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21676382415333134, 0.21676382415333154, 0.35584662046290894], 
reward next is 0.6442, 
noisyNet noise sample is [array([1.2560586], dtype=float32), -1.789687]. 
=============================================
[2019-03-25 23:38:14,742] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-25 23:38:14,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:38:14,745] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:38:14,746] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:38:14,747] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:38:14,748] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:38:14,749] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:38:14,751] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:38:14,751] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:38:14,751] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:38:14,754] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:38:14,781] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-25 23:38:14,781] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-25 23:38:14,800] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-25 23:38:14,802] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-25 23:38:14,877] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-25 23:39:05,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.6693816], dtype=float32), 0.27953586]
[2019-03-25 23:39:05,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.26990442666666, 95.79917517666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7491881623125506, 6.9112, 6.9112, 168.912956510431, 644497.4857017257, 644497.4857017257, 191874.9008281421]
[2019-03-25 23:39:05,026] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:39:05,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6826788613582787
[2019-03-25 23:40:23,577] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.1026 2937918135.3263 1379.0000
[2019-03-25 23:40:23,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7923.8870 2989446328.4271 1566.0000
[2019-03-25 23:40:24,029] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.4393 3319753607.8411 2143.0000
[2019-03-25 23:40:24,093] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7029.8919 3185219386.8708 2464.0000
[2019-03-25 23:40:24,188] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7345.8771 3105849150.9122 2014.0000
[2019-03-25 23:40:25,204] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 725000, evaluation results [725000.0, 7289.439322444665, 3319753607.8411336, 2143.0, 7345.877103801266, 3105849150.912202, 2014.0, 8061.102600938669, 2937918135.326255, 1379.0, 7029.891904325195, 3185219386.870774, 2464.0, 7923.886968186372, 2989446328.4270563, 1566.0]
[2019-03-25 23:40:25,521] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7415326e-37 5.4715871e-32], sum to 1.0000
[2019-03-25 23:40:25,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6876
[2019-03-25 23:40:25,537] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 869383.0805872089 W.
[2019-03-25 23:40:25,545] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.311058999686291, 0.0, 2.0, 0.0, 1.0, 1.0, 0.519834435823606, 6.9112, 6.9112, 168.9129564943082, 869383.0805872089, 869383.0805872089, 223348.6095691902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3475200.0000, 
sim time next is 3475800.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.640295466292132, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510427, 894796.0074213075, 894796.0074213082, 208482.5353841589], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5666210437254602, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439945152283, 0.24855444650591874, 0.24855444650591893, 0.3111679632599387], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.42674083], dtype=float32), 1.643598]. 
=============================================
[2019-03-25 23:40:29,145] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:29,152] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7107
[2019-03-25 23:40:29,159] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 75.0, 1.0, 1.0, 0.3049317197438886, 1.0, 1.0, 0.3049317197438886, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 852247.6942427276, 852247.6942427276, 250847.9539993946], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3338400.0000, 
sim time next is 3339000.0000, 
raw observation next is [31.0, 75.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.000262087060708, 6.9112, 168.9123024366176, 892009.0349462451, 828825.5697682594, 254812.5235104037], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008906208706070817, 0.0, 0.8294367333504427, 0.24778028748506808, 0.23022932493562762, 0.3803171992692593], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1587881], dtype=float32), -0.9409512]. 
=============================================
[2019-03-25 23:40:29,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[52.471336]
 [54.511337]
 [54.264538]
 [55.23517 ]
 [54.994354]], R is [[52.4105835 ]
 [51.88647842]
 [51.36761475]
 [50.85393906]
 [50.34540176]].
[2019-03-25 23:40:34,211] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:34,221] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0300
[2019-03-25 23:40:34,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1978706.987117442 W.
[2019-03-25 23:40:34,237] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.4717359204049669, 1.0, 2.0, 0.4717359204049669, 1.0, 2.0, 0.8114181311827205, 6.9112, 6.9112, 170.5573041426782, 1978706.987117442, 1978706.987117442, 394822.4676696233], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3573000.0000, 
sim time next is 3573600.0000, 
raw observation next is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.814860576753555, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.986738949868495, 6.9112, 168.9125067631412, 2035871.571083885, 1982281.782885606, 412248.6830461105], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590839, 0.7133333333333333, 1.0, 1.0, 0.7769404539199458, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007553894986849485, 0.0, 0.8294377366872147, 0.5655198808566347, 0.550633828579335, 0.6152965418598664], 
reward next is 0.0070, 
noisyNet noise sample is [array([1.2825943], dtype=float32), 0.630673]. 
=============================================
[2019-03-25 23:40:37,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:37,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7954
[2019-03-25 23:40:37,918] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1285865.526709122 W.
[2019-03-25 23:40:37,925] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 76.5, 1.0, 2.0, 0.9199664980704151, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1285865.526709122, 1285865.526709121, 275486.8139239228], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3483000.0000, 
sim time next is 3483600.0000, 
raw observation next is [28.0, 75.66666666666666, 1.0, 2.0, 0.421343800557975, 1.0, 1.0, 0.421343800557975, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1177784.222052561, 1177784.222052561, 277630.5252209124], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.7566666666666666, 1.0, 1.0, 0.3028238560939458, 1.0, 0.5, 0.3028238560939458, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3271622839034891, 0.3271622839034891, 0.4143739182401678], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07854141], dtype=float32), 0.2790125]. 
=============================================
[2019-03-25 23:40:39,273] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:39,283] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2260
[2019-03-25 23:40:39,292] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1180448.615088503 W.
[2019-03-25 23:40:39,300] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.8445883631324715, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1180448.615088503, 1180448.615088504, 255149.2534841007], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3571800.0000, 
sim time next is 3572400.0000, 
raw observation next is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 0.6647025009145504, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.981798311815146, 6.9112, 168.91253859888, 1825736.249300431, 1775651.501008566, 378056.5673140006], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.7266666666666667, 1.0, 1.0, 0.5960271095356029, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007059831181514565, 0.0, 0.8294378930152665, 0.5071489581390086, 0.493236528057935, 0.5642635333044785], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9974562], dtype=float32), 0.8652783]. 
=============================================
[2019-03-25 23:40:40,415] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:40,426] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9501
[2019-03-25 23:40:40,432] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.33333333333334, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.967104272034321, 6.9112, 6.9112, 168.912956510431, 778172.6324962269, 778172.6324962269, 238646.5738765247], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3606000.0000, 
sim time next is 3606600.0000, 
raw observation next is [32.16666666666666, 66.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9679727235819006, 6.9112, 6.9112, 168.912956510431, 778871.6812137432, 778871.6812137432, 238862.5473932847], 
processed observation next is [1.0, 0.7391304347826086, 0.7235387045813582, 0.6633333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9609423458315859, 0.0, 0.0, 0.8294399451523027, 0.21635324478159532, 0.21635324478159532, 0.35651126476609657], 
reward next is 0.6435, 
noisyNet noise sample is [array([0.24194208], dtype=float32), 0.96095854]. 
=============================================
[2019-03-25 23:40:42,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:42,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3532
[2019-03-25 23:40:42,109] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8559264539161655, 6.9112, 6.9112, 168.912956510431, 709844.1747993702, 709844.1747993702, 213563.5925473469], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3627000.0000, 
sim time next is 3627600.0000, 
raw observation next is [28.0, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8469899778485553, 6.9112, 6.9112, 168.912956510431, 703714.157063147, 703714.157063147, 211642.5986865934], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8134024120104332, 0.0, 0.0, 0.8294399451523027, 0.19547615473976307, 0.19547615473976307, 0.31588447565163197], 
reward next is 0.6841, 
noisyNet noise sample is [array([0.8241568], dtype=float32), -1.3407727]. 
=============================================
[2019-03-25 23:40:44,628] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:44,636] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1579
[2019-03-25 23:40:44,646] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 919719.0136046803 W.
[2019-03-25 23:40:44,654] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 83.0, 1.0, 2.0, 0.3290610391159068, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5483295552197013, 6.911199999999999, 6.9112, 168.912956510431, 919719.0136046803, 919719.0136046809, 229036.2605671049], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3559200.0000, 
sim time next is 3559800.0000, 
raw observation next is [26.08333333333334, 83.5, 1.0, 2.0, 0.6458875547417883, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 902614.1241319691, 902614.1241319691, 209594.8014550746], 
processed observation next is [1.0, 0.17391304347826086, 0.43522906793049004, 0.835, 1.0, 1.0, 0.5733584996889015, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2507261455922136, 0.2507261455922136, 0.3128280618732457], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2764793], dtype=float32), 0.22687475]. 
=============================================
[2019-03-25 23:40:44,658] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 7.163446e-36 0.000000e+00 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-25 23:40:44,670] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8601
[2019-03-25 23:40:44,682] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2268829.100132929 W.
[2019-03-25 23:40:44,689] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 67.66666666666667, 1.0, 2.0, 0.5408316702143448, 1.0, 2.0, 0.5408316702143448, 1.0, 1.0, 0.9392457553959006, 6.911199999999999, 6.9112, 170.5573041426782, 2268829.100132929, 2268829.10013293, 444530.398773444], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [31.5, 66.5, 1.0, 2.0, 0.5482030657068413, 1.0, 2.0, 0.5482030657068413, 1.0, 2.0, 0.9520474316086266, 6.911199999999999, 6.9112, 170.5573041426782, 2299781.105796539, 2299781.105796539, 450079.7817652578], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.665, 1.0, 1.0, 0.45566634422511, 1.0, 1.0, 0.45566634422511, 1.0, 1.0, 0.9415212580593006, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6388280849434831, 0.6388280849434831, 0.671760868306355], 
reward next is 0.3282, 
noisyNet noise sample is [array([-0.65789807], dtype=float32), -0.4880219]. 
=============================================
[2019-03-25 23:40:45,836] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.5108458e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-25 23:40:45,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0926
[2019-03-25 23:40:45,853] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2098609.487337652 W.
[2019-03-25 23:40:45,865] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.5002934423792494, 1.0, 2.0, 0.5002934423792494, 1.0, 1.0, 0.8632126337910608, 6.9112, 6.9112, 170.5573041426782, 2098609.487337652, 2098609.487337652, 414330.4543626638], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3575400.0000, 
sim time next is 3576000.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7748621066668048, 1.0, 2.0, 0.7748621066668048, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2166974.648026465, 2166974.648026465, 407816.0206537014], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783573, 0.6866666666666668, 1.0, 1.0, 0.7287495261045841, 1.0, 1.0, 0.7287495261045841, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6019374022295736, 0.6019374022295736, 0.6086806278413454], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2553177], dtype=float32), -0.8779721]. 
=============================================
[2019-03-25 23:40:45,874] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[28.13902 ]
 [28.556799]
 [28.490326]
 [28.020948]
 [27.584167]], R is [[27.56681442]
 [27.67274284]
 [27.80235291]
 [27.95287514]
 [28.10197258]].
[2019-03-25 23:40:54,973] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:54,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5285
[2019-03-25 23:40:54,989] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8839738275599778, 6.9112, 6.9112, 168.912956510431, 728850.0638113859, 728850.0638113859, 219708.5861128222], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3810000.0000, 
sim time next is 3810600.0000, 
raw observation next is [27.5, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8907479136854184, 6.9112, 6.9112, 168.912956510431, 733535.10461071, 733535.10461071, 221224.4322780986], 
processed observation next is [0.0, 0.08695652173913043, 0.5023696682464456, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8667657483968517, 0.0, 0.0, 0.8294399451523027, 0.2037597512807528, 0.2037597512807528, 0.3301857198180576], 
reward next is 0.6698, 
noisyNet noise sample is [array([-1.1528792], dtype=float32), -0.98848146]. 
=============================================
[2019-03-25 23:40:59,215] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:40:59,221] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1805
[2019-03-25 23:40:59,229] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.5, 59.5, 1.0, 2.0, 0.6062361349569605, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564403891, 847180.0083410656, 847180.008341065, 201914.2350427894], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [34.33333333333333, 60.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.070099203163904, 6.9112, 168.9119432254172, 941572.7896911071, 828844.9023208882, 254813.1002642349], 
processed observation next is [0.0, 0.6956521739130435, 0.8262243285939966, 0.6066666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.015889920316390426, 0.0, 0.829434969459001, 0.26154799713641863, 0.23023469508913563, 0.380318060095873], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.698497], dtype=float32), 0.513689]. 
=============================================
[2019-03-25 23:40:59,260] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[37.71966 ]
 [36.8528  ]
 [37.37799 ]
 [37.895836]
 [38.53495 ]], R is [[38.01360321]
 [37.63346863]
 [37.25713348]
 [36.88456345]
 [36.51571655]].
[2019-03-25 23:41:14,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9920088e-01 5.7333553e-16 1.1716986e-19 2.3383903e-14 7.9909345e-04], sum to 1.0000
[2019-03-25 23:41:14,239] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2680
[2019-03-25 23:41:14,249] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1057547.670253809 W.
[2019-03-25 23:41:14,261] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.13333333333334, 88.33333333333334, 1.0, 2.0, 0.7566988590409346, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1057547.670253809, 1057547.670253808, 233559.8315296807], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4076400.0000, 
sim time next is 4077000.0000, 
raw observation next is [27.1, 88.5, 1.0, 2.0, 0.3863745808386766, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6597223549780072, 6.911200000000001, 6.9112, 168.912956510431, 1079990.66925834, 1079990.669258339, 252242.2768350622], 
processed observation next is [1.0, 0.17391304347826086, 0.4834123222748816, 0.885, 1.0, 1.0, 0.2606922660706947, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.5850272621683015, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2999974081273167, 0.2999974081273164, 0.37648101020158536], 
reward next is 0.6235, 
noisyNet noise sample is [array([0.1308498], dtype=float32), 1.2986139]. 
=============================================
[2019-03-25 23:41:14,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[15.369045 ]
 [15.7367115]
 [17.232565 ]
 [17.210022 ]
 [17.444708 ]], R is [[14.98823643]
 [14.83835411]
 [14.68997097]
 [15.16811085]
 [15.58530426]].
[2019-03-25 23:41:15,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 7.1096589e-34 1.3649726e-30 2.4491984e-35 4.3118061e-23], sum to 1.0000
[2019-03-25 23:41:15,231] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8121
[2019-03-25 23:41:15,241] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.5, 73.0, 1.0, 2.0, 0.6076251190521497, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564479231, 849121.8093320351, 849121.8093320351, 202175.0397596971], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3961800.0000, 
sim time next is 3962400.0000, 
raw observation next is [31.33333333333333, 73.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.058370125380732, 6.9112, 168.9119994409108, 933248.615953064, 828841.6553817426, 254812.645330567], 
processed observation next is [0.0, 0.8695652173913043, 0.6840442338072668, 0.7366666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.01471701253807316, 0.0, 0.8294352455028101, 0.25923572665362893, 0.23023379316159517, 0.38031738109039853], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.15001939], dtype=float32), 1.9242932]. 
=============================================
[2019-03-25 23:41:17,207] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9967682e-01 4.2617054e-31 3.5452329e-36 2.7773430e-34 3.2316954e-04], sum to 1.0000
[2019-03-25 23:41:17,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8226
[2019-03-25 23:41:17,224] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1261301.064201128 W.
[2019-03-25 23:41:17,229] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.3008025219230241, 1.0, 2.0, 0.3008025219230241, 1.0, 1.0, 0.5223945036661966, 6.9112, 6.9112, 170.5573041426782, 1261301.064201128, 1261301.064201128, 304777.8319677942], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3995400.0000, 
sim time next is 3996000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.4491765359343685, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7800710979676646, 6.911200000000001, 6.9112, 168.912956510431, 1255637.910697657, 1255637.910697657, 282093.1665474669], 
processed observation next is [1.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.3363572722100826, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.7317940219117861, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34878830852712694, 0.34878830852712694, 0.42103457693651775], 
reward next is 0.5790, 
noisyNet noise sample is [array([-1.7599858], dtype=float32), 2.0252838]. 
=============================================
[2019-03-25 23:41:17,243] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[41.2793  ]
 [41.031704]
 [41.91004 ]
 [42.45858 ]
 [43.051464]], R is [[41.3493309 ]
 [40.93583679]
 [40.52647781]
 [40.65934753]
 [40.81822205]].
[2019-03-25 23:41:32,761] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-25 23:41:32,762] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:41:32,764] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:41:32,764] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:41:32,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:41:32,765] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:41:32,767] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:41:32,767] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:41:32,768] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:41:32,765] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:41:32,769] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:41:32,794] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-25 23:41:32,794] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-25 23:41:32,815] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-25 23:41:32,857] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-25 23:41:32,879] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-25 23:41:41,116] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8003727], dtype=float32), 0.22716649]
[2019-03-25 23:41:41,117] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.8, 55.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.00651180790382, 6.96320591483695, 6.9112, 168.9125841341066, 932943.0209317689, 896048.3163772257, 249254.6219845569]
[2019-03-25 23:41:41,120] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:41:41,122] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.41274739989142095
[2019-03-25 23:41:41,122] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 932943.0209317689 W.
[2019-03-25 23:41:56,973] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8003727], dtype=float32), 0.22716649]
[2019-03-25 23:41:56,975] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.33333333333334, 50.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5214619250407735, 6.911200000000001, 6.9112, 168.912956510431, 462164.5731393087, 462164.573139308, 154712.3330562984]
[2019-03-25 23:41:56,977] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:41:56,980] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5909546498578377
[2019-03-25 23:42:03,629] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8003727], dtype=float32), 0.22716649]
[2019-03-25 23:42:03,631] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.3, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7869938373015668, 6.911200000000001, 6.9112, 168.912956510431, 681521.2726619978, 681521.2726619971, 199311.6410344141]
[2019-03-25 23:42:03,632] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:42:03,634] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7174433250181177
[2019-03-25 23:42:29,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8003727], dtype=float32), 0.22716649]
[2019-03-25 23:42:29,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.702297215, 67.12625637333333, 1.0, 2.0, 0.3179053160017328, 1.0, 1.0, 0.3179053160017328, 1.0, 1.0, 0.548212151230072, 6.9112, 6.9112, 171.5212843490159, 1333055.061837572, 1333055.061837572, 311891.5386003992]
[2019-03-25 23:42:29,610] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:42:29,614] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0949313e-14 1.5102927e-31 1.4587309e-31 1.8795321e-31 1.0000000e+00], sampled 0.8585240935893401
[2019-03-25 23:42:49,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8003727], dtype=float32), 0.22716649]
[2019-03-25 23:42:49,162] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 63.0, 1.0, 2.0, 0.6413511404702912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896271.9066966529, 896271.9066966529, 208703.1581387743]
[2019-03-25 23:42:49,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:42:49,168] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6656083e-01 2.1023291e-31 4.7892215e-24 8.0148009e-26 3.3343920e-01], sampled 0.5935092331895612
[2019-03-25 23:42:49,171] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 896271.9066966529 W.
[2019-03-25 23:43:40,732] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7312.8284 3229225632.4614 1754.0000
[2019-03-25 23:43:41,152] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7531.2972 3165681743.0327 1600.0000
[2019-03-25 23:43:41,397] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7437.3711 3355289639.0239 1606.0000
[2019-03-25 23:43:42,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8137.1333 2993928085.9150 1027.0000
[2019-03-25 23:43:42,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8033.4763 3034013692.7852 1117.0000
[2019-03-25 23:43:43,031] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 750000, evaluation results [750000.0, 7437.371062011613, 3355289639.023945, 1606.0, 7531.297227598296, 3165681743.0327125, 1600.0, 8137.133277956718, 2993928085.914971, 1027.0, 7312.828370616224, 3229225632.461421, 1754.0, 8033.476275259032, 3034013692.7852473, 1117.0]
[2019-03-25 23:43:43,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.000000e+00 5.857687e-35 1.236768e-30 2.016031e-36 9.843651e-12], sum to 1.0000
[2019-03-25 23:43:43,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9080
[2019-03-25 23:43:43,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 889105.8326580323 W.
[2019-03-25 23:43:43,787] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.66666666666666, 72.33333333333334, 1.0, 2.0, 0.3181127057792087, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5524565684591115, 6.911200000000001, 6.9112, 168.912956510431, 889105.8326580323, 889105.8326580317, 227979.9878760299], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4219800.0000, 
sim time next is 4220400.0000, 
raw observation next is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.3177093852981412, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5517561341636186, 6.911199999999999, 6.9112, 168.912956510431, 887978.1049949473, 887978.1049949479, 227840.8748551435], 
processed observation next is [1.0, 0.8695652173913043, 0.7314375987361774, 0.7366666666666667, 1.0, 1.0, 0.17796311481703758, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45336113922392507, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2466605847208187, 0.24666058472081887, 0.34006100724648286], 
reward next is 0.6599, 
noisyNet noise sample is [array([0.6671325], dtype=float32), 0.5306198]. 
=============================================
[2019-03-25 23:43:45,515] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.018449e-37], sum to 1.0000
[2019-03-25 23:43:45,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.2525846e-32 1.9123896e-30 7.2257624e-24 8.2567397e-24], sum to 1.0000
[2019-03-25 23:43:45,526] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4827
[2019-03-25 23:43:45,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7134
[2019-03-25 23:43:45,536] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2463788.606134657 W.
[2019-03-25 23:43:45,536] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1533981.374638286 W.
[2019-03-25 23:43:45,541] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.5872593343706856, 1.0, 2.0, 0.5872593343706856, 1.0, 1.0, 1.019875254172307, 6.911200000000001, 6.9112, 170.5573041426782, 2463788.606134657, 2463788.606134657, 480732.9625473178], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4365000.0000, 
sim time next is 4365600.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.6302476484910446, 1.0, 2.0, 0.6302476484910446, 1.0, 2.0, 1.03, 6.983747130008717, 6.9112, 170.5573041426782, 2644332.685063196, 2592364.22480938, 499208.6779131209], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.5545152391458369, 1.0, 1.0, 0.5545152391458369, 1.0, 1.0, 1.0365853658536586, 0.007254713000871682, 0.0, 0.8375144448122397, 0.7345368569619989, 0.7201011735581611, 0.7450875789748073], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05467233], dtype=float32), 0.9726621]. 
=============================================
[2019-03-25 23:43:45,542] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.83333333333333, 79.0, 1.0, 2.0, 0.5486758308358569, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9528684682919095, 6.9112, 6.9112, 168.912956510431, 1533981.374638286, 1533981.374638286, 335809.8314791002], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4258200.0000, 
sim time next is 4258800.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.3526053708364942, 1.0, 1.0, 0.3526053708364942, 1.0, 2.0, 0.6123589207649743, 6.911199999999999, 6.9112, 170.5573041426782, 1478666.441094599, 1478666.4410946, 328205.1078785243], 
processed observation next is [1.0, 0.30434782608695654, 0.6208530805687204, 0.79, 1.0, 1.0, 0.22000647088734238, 1.0, 0.5, 0.22000647088734238, 1.0, 1.0, 0.5272669765426516, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.41074067808183307, 0.4107406780818333, 0.4898583699679467], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.34037173], dtype=float32), -0.30017602]. 
=============================================
[2019-03-25 23:43:52,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8903127e-35], sum to 1.0000
[2019-03-25 23:43:52,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0044
[2019-03-25 23:43:52,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2777709.18836798 W.
[2019-03-25 23:43:52,562] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.66666666666667, 56.33333333333334, 1.0, 2.0, 0.9930017587797695, 1.0, 2.0, 0.9930017587797695, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2777709.18836798, 2777709.18836798, 524773.2443394313], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4375200.0000, 
sim time next is 4375800.0000, 
raw observation next is [35.0, 56.5, 1.0, 2.0, 0.7149210498490031, 1.0, 2.0, 0.6780505644387642, 1.0, 1.0, 1.03, 7.005098908696367, 6.9112, 170.5573041426782, 2845127.967200633, 2777864.359338027, 526083.7991322607], 
processed observation next is [1.0, 0.6521739130434783, 0.8578199052132701, 0.565, 1.0, 1.0, 0.6565313853602447, 1.0, 1.0, 0.6121091137816436, 1.0, 0.5, 1.0365853658536586, 0.00938989086963673, 0.0, 0.8375144448122397, 0.7903133242223981, 0.7716289887050075, 0.785199700197404], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.43096083], dtype=float32), 1.443392]. 
=============================================
[2019-03-25 23:43:52,625] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:43:52,638] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4888
[2019-03-25 23:43:52,648] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2463788.606134657 W.
[2019-03-25 23:43:52,654] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.5872593343706856, 1.0, 2.0, 0.5872593343706856, 1.0, 1.0, 1.019875254172307, 6.911200000000001, 6.9112, 170.5573041426782, 2463788.606134657, 2463788.606134657, 480732.9625472316], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4365000.0000, 
sim time next is 4365600.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.6302476484910446, 1.0, 2.0, 0.6302476484910446, 1.0, 2.0, 1.03, 6.983747130008717, 6.9112, 170.5573041426782, 2644332.685063196, 2592364.22480938, 499208.6779130939], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.5545152391458369, 1.0, 1.0, 0.5545152391458369, 1.0, 1.0, 1.0365853658536586, 0.007254713000871682, 0.0, 0.8375144448122397, 0.7345368569619989, 0.7201011735581611, 0.745087578974767], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34588397], dtype=float32), -1.8896519]. 
=============================================
[2019-03-25 23:43:59,311] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:43:59,318] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4583
[2019-03-25 23:43:59,327] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 869918.1644536766 W.
[2019-03-25 23:43:59,337] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.969135371902951, 6.9112, 168.9123978689826, 869918.1644536766, 828816.9535775301, 254811.9296600056], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4427400.0000, 
sim time next is 4428000.0000, 
raw observation next is [29.0, 84.0, 1.0, 1.0, 0.1970211072897448, 1.0, 1.0, 0.1970211072897448, 1.0, 2.0, 0.3421605074864658, 6.911199999999999, 6.9112, 170.5573041426782, 825965.5372039336, 825965.5372039343, 269025.2375949198], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 0.5, 0.03255555095149976, 1.0, 0.5, 0.03255555095149976, 1.0, 1.0, 0.19775671644690954, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2294348714455371, 0.2294348714455373, 0.40153020536555195], 
reward next is 0.5985, 
noisyNet noise sample is [array([-0.8024683], dtype=float32), 0.42650267]. 
=============================================
[2019-03-25 23:43:59,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.32221 ]
 [55.983475]
 [54.97324 ]
 [53.953663]
 [52.26976 ]], R is [[56.06139374]
 [55.83078766]
 [55.70427704]
 [55.6688652 ]
 [55.70537186]].
[2019-03-25 23:44:01,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00000000e+00 0.00000000e+00 1.04939245e-35 0.00000000e+00
 4.15550897e-22], sum to 1.0000
[2019-03-25 23:44:01,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2352
[2019-03-25 23:44:01,309] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.939463209392752, 6.9112, 6.9112, 168.912956510431, 767124.691844724, 767124.691844724, 232450.4101015317], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4578000.0000, 
sim time next is 4578600.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.939161562455254, 6.9112, 6.9112, 168.912956510431, 766878.2878554862, 766878.2878554862, 232377.3368674941], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9258067834820168, 0.0, 0.0, 0.8294399451523027, 0.21302174662652393, 0.21302174662652393, 0.34683184607088674], 
reward next is 0.6532, 
noisyNet noise sample is [array([0.0542562], dtype=float32), -0.24470598]. 
=============================================
[2019-03-25 23:44:05,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:44:05,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5007
[2019-03-25 23:44:05,568] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8638822793109396, 6.911199999999999, 6.9112, 168.912956510431, 715944.6793896905, 715944.6793896911, 215311.5030879379], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8644345927456685, 6.9112, 6.9112, 168.912956510431, 716328.7655173802, 716328.7655173802, 215432.1401250885], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8346763326166688, 0.0, 0.0, 0.8294399451523027, 0.19898021264371674, 0.19898021264371674, 0.32154050764938585], 
reward next is 0.6785, 
noisyNet noise sample is [array([-0.22652945], dtype=float32), 0.0052244766]. 
=============================================
[2019-03-25 23:44:06,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:44:06,368] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1968
[2019-03-25 23:44:06,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.33333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.891033251196414, 6.911200000000001, 6.9112, 168.912956510431, 733800.9636913787, 733800.9636913781, 221291.2454771031], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [28.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9012478509621881, 6.9112, 6.9112, 168.912956510431, 740444.2144029543, 740444.2144029543, 223581.8732186009], 
processed observation next is [0.0, 0.43478260869565216, 0.5497630331753555, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8795705499538877, 0.0, 0.0, 0.8294399451523027, 0.2056789484452651, 0.2056789484452651, 0.3337042883859715], 
reward next is 0.6663, 
noisyNet noise sample is [array([-0.44950065], dtype=float32), -2.0277462]. 
=============================================
[2019-03-25 23:44:08,023] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.2742680e-35 0.0000000e+00 1.4513077e-34 1.3413908e-26], sum to 1.0000
[2019-03-25 23:44:08,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3097
[2019-03-25 23:44:08,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1659525.649651962 W.
[2019-03-25 23:44:08,055] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.16666666666666, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.201053449143846, 6.9112, 168.9112134315935, 1659525.649651962, 1453895.75884064, 311352.1952657702], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4593000.0000, 
sim time next is 4593600.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.5386981323787489, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9294558819533584, 6.911200000000001, 6.9112, 168.9127204992929, 1506066.055019339, 1506066.055019338, 328727.2895645245], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.94, 1.0, 1.0, 0.4442146173237938, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9139705877479979, 8.881784197001253e-17, 0.0, 0.8294387862295682, 0.4183516819498164, 0.4183516819498161, 0.4906377456186933], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.615585], dtype=float32), -0.14266486]. 
=============================================
[2019-03-25 23:44:28,765] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.510861e-24 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-25 23:44:28,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0622
[2019-03-25 23:44:28,779] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.490278912580032, 1.0, 1.0, 0.490278912580032, 1.0, 2.0, 0.8457175656829581, 6.9112, 6.9112, 170.5573041426782, 2056560.64303493, 2056560.64303493, 407474.7556224531], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4875600.0000, 
sim time next is 4876200.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.5051161971573125, 1.0, 2.0, 0.5051161971573125, 1.0, 2.0, 0.8731373374096658, 6.9112, 6.9112, 170.5573041426782, 2118859.780242047, 2118859.780242047, 417956.6187844708], 
processed observation next is [1.0, 0.43478260869565216, 0.6445497630331753, 0.68, 1.0, 1.0, 0.40375445440640056, 1.0, 1.0, 0.40375445440640056, 1.0, 1.0, 0.8452894358654461, 0.0, 0.0, 0.8375144448122397, 0.5885721611783463, 0.5885721611783463, 0.6238158489320459], 
reward next is 0.3762, 
noisyNet noise sample is [array([-0.09842925], dtype=float32), 0.12888636]. 
=============================================
[2019-03-25 23:44:35,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:44:35,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0071
[2019-03-25 23:44:35,362] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8549327959628396, 6.911200000000001, 6.9112, 168.912956510431, 708355.0793879715, 708355.079387971, 213322.7986113017], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8560261496107959, 6.911200000000001, 6.9112, 168.912956510431, 709357.6101144654, 709357.6101144648, 213567.0910885875], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8244221336717024, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19704378058735147, 0.19704378058735134, 0.31875685237102613], 
reward next is 0.6812, 
noisyNet noise sample is [array([0.4741024], dtype=float32), -0.6321462]. 
=============================================
[2019-03-25 23:44:40,365] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:44:40,377] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1335
[2019-03-25 23:44:40,386] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7954946860160879, 6.9112, 6.9112, 168.912956510431, 668042.163974603, 668042.163974603, 200935.9528456593], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5015400.0000, 
sim time next is 5016000.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7944326922455417, 6.9112, 6.9112, 168.912956510431, 667270.4197075511, 667270.4197075511, 200721.0770295423], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7493081612750507, 0.0, 0.0, 0.8294399451523027, 0.18535289436320865, 0.18535289436320865, 0.2995836970590184], 
reward next is 0.7004, 
noisyNet noise sample is [array([1.77085], dtype=float32), 0.1794291]. 
=============================================
[2019-03-25 23:44:40,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.974274]
 [63.75043 ]
 [63.25913 ]
 [62.833687]
 [62.285095]], R is [[64.29524231]
 [64.35238647]
 [64.40821838]
 [64.46178436]
 [64.51302338]].
[2019-03-25 23:44:46,204] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.4088681e-34 8.7263218e-37 2.4676981e-38 3.6445077e-21], sum to 1.0000
[2019-03-25 23:44:46,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7473
[2019-03-25 23:44:46,222] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.5, 81.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9415768170507317, 6.9112, 6.9112, 168.912956510431, 768191.4446982025, 768191.4446982025, 232932.3130443644], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5263800.0000, 
sim time next is 5264400.0000, 
raw observation next is [28.5, 81.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9440991302000216, 6.9112, 6.9112, 168.912956510431, 769719.4302636108, 769719.4302636108, 233520.4555049938], 
processed observation next is [1.0, 0.9565217391304348, 0.5497630331753555, 0.8166666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9318282075610018, 0.0, 0.0, 0.8294399451523027, 0.213810952851003, 0.213810952851003, 0.3485379932910355], 
reward next is 0.6515, 
noisyNet noise sample is [array([0.72911483], dtype=float32), -0.09297672]. 
=============================================
[2019-03-25 23:44:47,480] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:44:47,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8406
[2019-03-25 23:44:47,495] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.83333333333333, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8740858604737449, 6.911200000000001, 6.9112, 168.912956510431, 722969.9774023629, 722969.9774023623, 217549.7710056056], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5129400.0000, 
sim time next is 5130000.0000, 
raw observation next is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.869639040554741, 6.9112, 6.9112, 168.912956510431, 720056.3069346979, 720056.3069346979, 216576.2353083571], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8410232201887085, 0.0, 0.0, 0.8294399451523027, 0.20001564081519385, 0.20001564081519385, 0.32324811240053297], 
reward next is 0.6768, 
noisyNet noise sample is [array([-0.3742599], dtype=float32), -0.32021773]. 
=============================================
[2019-03-25 23:44:47,513] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[67.37968 ]
 [67.38842 ]
 [67.39472 ]
 [67.417854]
 [67.453255]], R is [[67.4684906 ]
 [67.46910858]
 [67.46849823]
 [67.46679688]
 [67.46399689]].
[2019-03-25 23:44:50,496] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-25 23:44:50,497] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:44:50,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:44:50,500] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:44:50,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:44:50,503] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:44:50,504] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:44:50,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:44:50,507] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:44:50,508] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:44:50,510] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:44:50,540] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-25 23:44:50,560] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-25 23:44:50,584] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-25 23:44:50,603] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-25 23:44:50,603] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-25 23:45:03,066] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:45:03,067] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [14.99987687, 87.76412164499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2879870438029727, 6.9112, 6.9112, 168.912956510431, 261087.8708055963, 261087.8708055963, 90445.19960965804]
[2019-03-25 23:45:03,069] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:45:03,072] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.873388567905107
[2019-03-25 23:45:46,820] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:45:46,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.6, 63.66666666666667, 1.0, 2.0, 0.6217960700401316, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.954394287926109, 6.9112, 168.9126068277228, 1738577.512813556, 1707934.061764157, 369173.4338122724]
[2019-03-25 23:45:46,823] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:45:46,825] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 1.363376e-36 0.000000e+00 2.924687e-14], sampled 0.8855868752282793
[2019-03-25 23:45:46,826] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1738577.512813556 W.
[2019-03-25 23:46:02,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:46:02,419] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.73266782, 74.12204655666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8655537730225535, 6.9112, 6.9112, 168.912956510431, 721634.1499754613, 721634.1499754613, 215817.333299216]
[2019-03-25 23:46:02,420] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:46:02,422] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3381115950557192
[2019-03-25 23:46:30,051] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:46:30,052] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.707875495, 83.679168655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.950418039577503, 6.9112, 6.9112, 168.912956510431, 778744.9346427799, 778744.9346427799, 235242.2787750885]
[2019-03-25 23:46:30,052] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:46:30,057] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.401898e-29], sampled 0.7760041573128649
[2019-03-25 23:46:30,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:46:30,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.76666666666667, 90.66666666666667, 1.0, 2.0, 0.6748126625519317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 943054.2939460956, 943054.293946095, 215502.7000531154]
[2019-03-25 23:46:30,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:46:30,713] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9909902e-01 3.4079893e-38 2.5461111e-34 2.2495940e-34 9.0101489e-04], sampled 0.06926450221565261
[2019-03-25 23:46:30,714] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 943054.2939460956 W.
[2019-03-25 23:46:41,460] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9063638], dtype=float32), 0.28576407]
[2019-03-25 23:46:41,461] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.73333333333333, 76.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5995758359824853, 6.911200000000001, 6.9112, 168.912956510431, 523223.6843403783, 523223.6843403777, 165991.8550477106]
[2019-03-25 23:46:41,463] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:46:41,467] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.30289406867126023
[2019-03-25 23:46:59,185] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8109.7702 2956873120.7738 1204.0000
[2019-03-25 23:46:59,404] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8015.2382 3002975039.0899 1288.0000
[2019-03-25 23:46:59,646] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7380.3943 3333147405.8819 1864.0000
[2019-03-25 23:46:59,760] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7130.8867 3201155152.9212 2180.0000
[2019-03-25 23:46:59,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7391.8147 3129424902.9433 1859.0000
[2019-03-25 23:47:00,915] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 775000, evaluation results [775000.0, 7380.394329393362, 3333147405.8818717, 1864.0, 7391.814737673455, 3129424902.943257, 1859.0, 8109.770239856541, 2956873120.7737703, 1204.0, 7130.8867453666, 3201155152.9212375, 2180.0, 8015.238168804679, 3002975039.089937, 1288.0]
[2019-03-25 23:47:02,495] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.5715941e-24 2.3377502e-24 5.9492516e-20 2.3232991e-09], sum to 1.0000
[2019-03-25 23:47:02,509] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8472
[2019-03-25 23:47:02,518] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1302531.755363433 W.
[2019-03-25 23:47:02,526] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.96666666666667, 86.66666666666667, 1.0, 2.0, 0.9318829637231246, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1302531.755363433, 1302531.755363433, 278862.8269599694], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5293200.0000, 
sim time next is 5293800.0000, 
raw observation next is [29.15, 86.0, 1.0, 2.0, 0.4541910738308326, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7887796919607996, 6.911199999999999, 6.9112, 168.912956510431, 1269664.041313606, 1269664.041313607, 284548.7428830778], 
processed observation next is [1.0, 0.2608695652173913, 0.5805687203791469, 0.86, 1.0, 1.0, 0.3423988841335333, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.7424142584887798, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3526844559204461, 0.3526844559204464, 0.42469961624339975], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2597443], dtype=float32), -1.5153321]. 
=============================================
[2019-03-25 23:47:09,032] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:47:09,045] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8591
[2019-03-25 23:47:09,053] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.85, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9294281896781135, 6.9112, 6.9112, 168.912956510431, 759148.1943160454, 759148.1943160454, 230042.6996525798], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5254200.0000, 
sim time next is 5254800.0000, 
raw observation next is [28.83333333333334, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.927896303768089, 6.9112, 6.9112, 168.912956510431, 757795.7564669673, 757795.7564669673, 229671.3448889962], 
processed observation next is [1.0, 0.8260869565217391, 0.5655608214849924, 0.7866666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9120686631318158, 0.0, 0.0, 0.8294399451523027, 0.21049882124082425, 0.21049882124082425, 0.34279305207312866], 
reward next is 0.6572, 
noisyNet noise sample is [array([-1.2455702], dtype=float32), 0.25111273]. 
=============================================
[2019-03-25 23:47:21,747] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4864479e-36], sum to 1.0000
[2019-03-25 23:47:21,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3910
[2019-03-25 23:47:21,762] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.6, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.907565507899717, 6.911199999999999, 6.9112, 168.912956510431, 746200.2141612999, 746200.2141613004, 225078.1193429208], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5612400.0000, 
sim time next is 5613000.0000, 
raw observation next is [26.55, 90.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9024531599732729, 6.911199999999999, 6.9112, 168.912956510431, 742653.0972125567, 742653.0972125573, 223910.8836318431], 
processed observation next is [1.0, 1.0, 0.4573459715639811, 0.9016666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8810404389917961, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.206292527003488, 0.20629252700348816, 0.33419534870424344], 
reward next is 0.6658, 
noisyNet noise sample is [array([0.25654173], dtype=float32), -1.1024987]. 
=============================================
[2019-03-25 23:47:21,771] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[54.051224]
 [53.96144 ]
 [53.980247]
 [53.988518]
 [54.013992]], R is [[54.19549179]
 [54.31760025]
 [54.43688202]
 [54.55321884]
 [54.66625595]].
[2019-03-25 23:47:27,883] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:47:27,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4788
[2019-03-25 23:47:27,906] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8673978693527591, 6.911199999999999, 6.9112, 168.912956510431, 717436.249416491, 717436.2494164916, 216048.1181191715], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5642400.0000, 
sim time next is 5643000.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8731877003528481, 6.911200000000001, 6.9112, 168.912956510431, 721609.7460489165, 721609.7460489159, 217325.7820328417], 
processed observation next is [0.0, 0.30434782608695654, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8453508540888391, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20044715168025456, 0.20044715168025443, 0.3243668388549876], 
reward next is 0.6756, 
noisyNet noise sample is [array([-0.28564304], dtype=float32), 0.05356762]. 
=============================================
[2019-03-25 23:47:27,920] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.33842 ]
 [69.3552  ]
 [69.35211 ]
 [69.249954]
 [69.263405]], R is [[69.30060577]
 [69.28514099]
 [69.27144623]
 [69.25881958]
 [69.24667358]].
[2019-03-25 23:47:45,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:47:45,549] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4392
[2019-03-25 23:47:45,557] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 69.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9626306414585646, 6.911200000000001, 6.9112, 168.912956510431, 780680.4913811622, 780680.4913811616, 237869.4130058261], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5766600.0000, 
sim time next is 5767200.0000, 
raw observation next is [30.7, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9609302768663373, 6.9112, 6.9112, 168.912956510431, 779819.2208083083, 779819.2208083083, 237474.8180284696], 
processed observation next is [0.0, 0.782608695652174, 0.6540284360189573, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9523539961784601, 0.0, 0.0, 0.8294399451523027, 0.2166164502245301, 0.2166164502245301, 0.35444002690816356], 
reward next is 0.6456, 
noisyNet noise sample is [array([-0.3177868], dtype=float32), -0.48129892]. 
=============================================
[2019-03-25 23:47:51,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.2236917e-18 7.3050698e-20 3.5237868e-16 1.2119965e-10], sum to 1.0000
[2019-03-25 23:47:51,870] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1995
[2019-03-25 23:47:51,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2555989.087177876 W.
[2019-03-25 23:47:51,883] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.76666666666667, 67.66666666666667, 1.0, 2.0, 0.6092134376922413, 1.0, 2.0, 0.6092134376922413, 1.0, 2.0, 1.03, 6.942679279695081, 6.9112, 170.5573041426782, 2555989.087177876, 2533439.198637062, 491344.4222085889], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6004200.0000, 
sim time next is 6004800.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.5905165653477575, 1.0, 2.0, 0.5905165653477575, 1.0, 2.0, 1.025531987196737, 6.9112, 6.9112, 170.5573041426782, 2477467.544533773, 2477467.544533773, 483382.8466931668], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.5066464642744066, 1.0, 1.0, 0.5066464642744066, 1.0, 1.0, 1.0311365697521182, 0.0, 0.0, 0.8375144448122397, 0.6881854290371592, 0.6881854290371592, 0.7214669353629356], 
reward next is 0.2785, 
noisyNet noise sample is [array([-0.09134576], dtype=float32), 0.8379594]. 
=============================================
[2019-03-25 23:47:53,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:47:53,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6382
[2019-03-25 23:47:53,590] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.3, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8960684967581632, 6.9112, 6.9112, 168.912956510431, 737561.3314175841, 737561.3314175841, 222436.3902671261], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5878800.0000, 
sim time next is 5879400.0000, 
raw observation next is [26.25, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8941928493726952, 6.9112, 6.9112, 168.912956510431, 736170.9527711275, 736170.9527711275, 222009.407441859], 
processed observation next is [1.0, 0.043478260869565216, 0.4431279620853081, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8709668894788966, 0.0, 0.0, 0.8294399451523027, 0.20449193132531318, 0.20449193132531318, 0.33135732454008804], 
reward next is 0.6686, 
noisyNet noise sample is [array([-0.34308407], dtype=float32), -1.3678204]. 
=============================================
[2019-03-25 23:47:56,591] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 2.2897983e-24 1.4296076e-28 1.0457669e-22 5.6038436e-09], sum to 1.0000
[2019-03-25 23:47:56,599] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2258
[2019-03-25 23:47:56,610] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2439976.515668294 W.
[2019-03-25 23:47:56,615] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.8723836658348298, 1.0, 2.0, 0.8723836658348298, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2439976.515668294, 2439976.515668294, 456643.6872177352], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6088800.0000, 
sim time next is 6089400.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8676842727260184, 1.0, 2.0, 0.8676842727260184, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2426819.984026392, 2426819.984026392, 454162.694802056], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.65, 1.0, 1.0, 0.8405834611156848, 1.0, 1.0, 0.8405834611156848, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6741166622295534, 0.6741166622295534, 0.6778547683612777], 
reward next is 0.3221, 
noisyNet noise sample is [array([-0.39638075], dtype=float32), 0.030951764]. 
=============================================
[2019-03-25 23:47:57,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.00000000e+00 7.31840042e-28 1.22806528e-31 1.03986135e-26
 1.37479302e-13], sum to 1.0000
[2019-03-25 23:47:57,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8292
[2019-03-25 23:47:57,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2201813.837321199 W.
[2019-03-25 23:47:57,298] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 74.0, 1.0, 2.0, 0.5248713584768201, 1.0, 1.0, 0.5248713584768201, 1.0, 2.0, 0.9115279720635668, 6.9112, 6.9112, 170.5573041426782, 2201813.837321199, 2201813.837321199, 432771.1555473583], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5923800.0000, 
sim time next is 5924400.0000, 
raw observation next is [29.96666666666667, 74.33333333333334, 1.0, 2.0, 0.946479503044781, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.001968351045525, 6.9112, 168.9124162684917, 2220092.631252163, 2155698.644969776, 447417.6854025037], 
processed observation next is [1.0, 0.5652173913043478, 0.6192733017377569, 0.7433333333333334, 1.0, 1.0, 0.9355174735479289, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009076835104552483, 0.0, 0.829437292317057, 0.6166923975700453, 0.598805179158271, 0.6677875901529906], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.73586667], dtype=float32), 0.42270243]. 
=============================================
[2019-03-25 23:48:08,479] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-25 23:48:08,484] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:48:08,485] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:48:08,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:48:08,486] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:48:08,487] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:48:08,485] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:48:08,489] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:48:08,490] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:48:08,491] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:48:08,494] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:48:08,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-25 23:48:08,512] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-25 23:48:08,531] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-25 23:48:08,579] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-25 23:48:08,599] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-25 23:48:21,057] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:48:21,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [14.90294046, 89.66084892333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2906521402907237, 6.9112, 6.9112, 168.912956510431, 263504.335885458, 263504.335885458, 91748.03010464311]
[2019-03-25 23:48:21,060] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:48:21,063] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7274088690488609
[2019-03-25 23:48:29,787] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:48:29,787] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.51666666666667, 38.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5399710074116079, 6.9112, 6.9112, 168.912956510431, 486045.0861686604, 486045.0861686604, 156662.779144702]
[2019-03-25 23:48:29,789] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:48:29,791] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6495535010201463
[2019-03-25 23:49:02,833] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:49:02,833] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.022185219216001, 6.9112, 6.9112, 168.9127439409431, 822510.2318775085, 822510.2318775085, 252745.8834339217]
[2019-03-25 23:49:02,836] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:49:02,840] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7322910629145375
[2019-03-25 23:49:45,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:49:45,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 86.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.917328252854515, 6.9112, 168.9126903057376, 833150.1996241065, 828802.612982681, 254812.2646001444]
[2019-03-25 23:49:45,553] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:49:45,557] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.095519e-21], sampled 0.42816773810772135
[2019-03-25 23:50:00,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:50:00,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.31541608, 89.15051064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7933531404521191, 6.9112, 6.9112, 168.912956510431, 668762.8851855668, 668762.8851855668, 200546.452385238]
[2019-03-25 23:50:00,236] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:50:00,237] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32273272369107464
[2019-03-25 23:50:09,350] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:50:09,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [16.90043696833333, 86.07018483666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.392865257666934, 6.911199999999999, 6.9112, 168.912956510431, 356147.3643074508, 356147.3643074515, 127524.8798001804]
[2019-03-25 23:50:09,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:50:09,359] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9246534062383297
[2019-03-25 23:50:09,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.97361445], dtype=float32), 0.26662403]
[2019-03-25 23:50:09,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.83333333333334, 82.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.001779296953915, 6.9112, 6.9112, 168.912855101498, 811058.5290447765, 811058.5290447765, 247709.7947008502]
[2019-03-25 23:50:09,408] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:50:09,411] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9967606252928564
[2019-03-25 23:50:17,598] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7380.7409 3350430131.8545 1782.0000
[2019-03-25 23:50:18,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7169.8475 3220961163.5428 2044.0000
[2019-03-25 23:50:18,419] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8129.9819 2975308174.0086 1083.0000
[2019-03-25 23:50:18,489] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7446.3568 3144829252.2703 1748.0000
[2019-03-25 23:50:18,512] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8005.5661 3022153435.4783 1235.0000
[2019-03-25 23:50:19,527] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 800000, evaluation results [800000.0, 7380.74093944069, 3350430131.854495, 1782.0, 7446.356777786755, 3144829252.2703137, 1748.0, 8129.981924998245, 2975308174.00865, 1083.0, 7169.847464092869, 3220961163.5427856, 2044.0, 8005.566078615674, 3022153435.478338, 1235.0]
[2019-03-25 23:50:28,383] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:50:28,396] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0951
[2019-03-25 23:50:28,401] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.43333333333333, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8812605262877738, 6.9112, 6.9112, 168.912956510431, 728715.8697678183, 728715.8697678183, 219167.535571856], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6385200.0000, 
sim time next is 6385800.0000, 
raw observation next is [27.41666666666666, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8791309314283596, 6.9112, 6.9112, 168.912956510431, 727127.3935614872, 727127.3935614872, 218690.0367116684], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690361, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.852598696863853, 0.0, 0.0, 0.8294399451523027, 0.20197983154485755, 0.20197983154485755, 0.32640303986816177], 
reward next is 0.6736, 
noisyNet noise sample is [array([-0.9305328], dtype=float32), 0.07217163]. 
=============================================
[2019-03-25 23:50:32,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.2355811e-32 8.5138404e-30 2.5943428e-33 7.8378927e-14], sum to 1.0000
[2019-03-25 23:50:32,568] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4353
[2019-03-25 23:50:32,575] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.1, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8683389548757793, 6.9112, 6.9112, 168.912956510431, 720017.545643684, 720017.545643684, 216319.5623210992], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6397200.0000, 
sim time next is 6397800.0000, 
raw observation next is [27.08333333333334, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8676283839921125, 6.9112, 6.9112, 168.912956510431, 719500.3439099116, 719500.3439099116, 216162.8071005862], 
processed observation next is [1.0, 0.043478260869565216, 0.4826224328594, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.838571199990381, 0.0, 0.0, 0.8294399451523027, 0.19986120664164211, 0.19986120664164211, 0.3226310553740092], 
reward next is 0.6774, 
noisyNet noise sample is [array([1.2359871], dtype=float32), 0.7594261]. 
=============================================
[2019-03-25 23:50:35,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:50:35,439] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3116
[2019-03-25 23:50:35,451] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.7, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8880686814403609, 6.911199999999999, 6.9112, 168.912956510431, 732384.012359855, 732384.0123598556, 220650.0593303106], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6325800.0000, 
sim time next is 6326400.0000, 
raw observation next is [26.8, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8884010013057568, 6.9112, 6.9112, 168.912956510431, 732628.1155994342, 732628.1155994342, 220725.0700548234], 
processed observation next is [0.0, 0.21739130434782608, 0.4691943127962086, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8639036601289718, 0.0, 0.0, 0.8294399451523027, 0.20350780988873174, 0.20350780988873174, 0.3294404030669006], 
reward next is 0.6706, 
noisyNet noise sample is [array([1.0665976], dtype=float32), -0.07974713]. 
=============================================
[2019-03-25 23:50:39,753] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 4.7373344e-35 0.0000000e+00 2.2622811e-34 4.5108906e-26], sum to 1.0000
[2019-03-25 23:50:39,762] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9642
[2019-03-25 23:50:39,770] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 977466.9894317052 W.
[2019-03-25 23:50:39,782] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.31666666666667, 91.0, 1.0, 2.0, 0.34971440629393, 1.0, 2.0, 0.34971440629393, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 977466.9894317052, 977466.9894317052, 260200.5436972433], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6490200.0000, 
sim time next is 6490800.0000, 
raw observation next is [26.3, 91.0, 1.0, 2.0, 0.3412239092330728, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5793389464532631, 6.9112, 6.9112, 168.912956510431, 953729.2561366854, 953729.2561366854, 234541.1198772311], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.91, 1.0, 1.0, 0.2062938665458708, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.4869987151869062, 0.0, 0.0, 0.8294399451523027, 0.2649247933713015, 0.2649247933713015, 0.3500613729510912], 
reward next is 0.6499, 
noisyNet noise sample is [array([-2.1313934], dtype=float32), 1.6302768]. 
=============================================
[2019-03-25 23:50:56,265] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 9.3420865e-26 1.5763944e-28 1.6221220e-28 1.4510548e-09], sum to 1.0000
[2019-03-25 23:50:56,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9299
[2019-03-25 23:50:56,290] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1756026.941895909 W.
[2019-03-25 23:50:56,296] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.7, 66.0, 1.0, 2.0, 0.6280366438130477, 1.0, 2.0, 0.6280366438130477, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1756026.941895909, 1756026.941895909, 344830.4578112678], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6712200.0000, 
sim time next is 6712800.0000, 
raw observation next is [29.66666666666667, 66.33333333333333, 1.0, 2.0, 0.4316016749382132, 1.0, 2.0, 0.4316016749382132, 1.0, 1.0, 0.7333384011827748, 6.911200000000001, 6.9112, 170.5573041426782, 1810220.773812509, 1810220.773812508, 368558.6357244126], 
processed observation next is [1.0, 0.6956521739130435, 0.6050552922590839, 0.6633333333333333, 1.0, 1.0, 0.3151827408894135, 1.0, 1.0, 0.3151827408894135, 1.0, 0.5, 0.6748029282716766, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5028391038368081, 0.5028391038368077, 0.550087516006586], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24907355], dtype=float32), -1.4289323]. 
=============================================
[2019-03-25 23:51:03,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 7.6120033e-33 1.0534389e-36 2.3246292e-35 3.0101058e-21], sum to 1.0000
[2019-03-25 23:51:03,116] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2236
[2019-03-25 23:51:03,126] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1922228.284701584 W.
[2019-03-25 23:51:03,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 63.0, 1.0, 2.0, 0.6874246780009627, 1.0, 2.0, 0.6874246780009627, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1922228.284701584, 1922228.284701584, 368777.5230644612], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6706800.0000, 
sim time next is 6707400.0000, 
raw observation next is [29.96666666666667, 63.33333333333333, 1.0, 2.0, 0.6655693837385629, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.967403665084362, 6.9112, 168.9126282124153, 1826949.268593482, 1787076.531698288, 378630.4917366682], 
processed observation next is [1.0, 0.6521739130434783, 0.6192733017377569, 0.6333333333333333, 1.0, 1.0, 0.5970715466729674, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005620366508436181, 0.0, 0.8294383330587501, 0.5074859079426339, 0.49641014769396885, 0.5651201369204003], 
reward next is 0.1539, 
noisyNet noise sample is [array([0.5773248], dtype=float32), -0.8058536]. 
=============================================
[2019-03-25 23:51:06,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2351838e-01 3.2828708e-26 1.4963129e-28 9.0541776e-32 4.7648165e-01], sum to 1.0000
[2019-03-25 23:51:06,960] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7689
[2019-03-25 23:51:06,967] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 64.0, 1.0, 2.0, 0.3093069066536653, 1.0, 2.0, 0.3093069066536653, 1.0, 2.0, 0.5446795511813329, 6.911200000000001, 6.9112, 170.5573041426782, 1409865.696545881, 1409865.696545881, 319056.4795904416], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6773400.0000, 
sim time next is 6774000.0000, 
raw observation next is [26.13333333333333, 62.66666666666667, 1.0, 2.0, 0.3053686631748694, 1.0, 2.0, 0.3053686631748694, 1.0, 2.0, 0.5376880069785458, 6.9112, 6.9112, 170.5573041426782, 1391676.307739862, 1391676.307739862, 317261.8562433732], 
processed observation next is [1.0, 0.391304347826087, 0.43759873617693507, 0.6266666666666667, 1.0, 1.0, 0.16309477490948118, 1.0, 1.0, 0.16309477490948118, 1.0, 1.0, 0.43620488655920214, 0.0, 0.0, 0.8375144448122397, 0.3865767521499617, 0.3865767521499617, 0.4735251585721988], 
reward next is 0.5265, 
noisyNet noise sample is [array([-0.11091092], dtype=float32), -2.052513]. 
=============================================
[2019-03-25 23:51:06,982] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[54.36725 ]
 [54.6707  ]
 [55.030388]
 [55.990913]
 [56.750454]], R is [[54.20418549]
 [54.18593979]
 [53.64408112]
 [53.10763931]
 [52.57656479]].
[2019-03-25 23:51:13,924] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:51:13,932] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8054
[2019-03-25 23:51:13,939] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.85, 59.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.766446222411515, 6.9112, 6.9112, 168.912956510431, 646686.5410719265, 646686.5410719265, 195155.4968547634], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6951000.0000, 
sim time next is 6951600.0000, 
raw observation next is [30.0, 59.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7693573858689023, 6.9112, 6.9112, 168.912956510431, 648770.8987828647, 648770.8987828647, 195724.3011084419], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.59, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7187285193523199, 0.0, 0.0, 0.8294399451523027, 0.18021413855079574, 0.18021413855079574, 0.29212582254991326], 
reward next is 0.7079, 
noisyNet noise sample is [array([0.5513755], dtype=float32), -2.859281]. 
=============================================
[2019-03-25 23:51:14,680] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:51:14,689] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7291
[2019-03-25 23:51:14,694] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.9, 30.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4693677071879472, 6.9112, 6.9112, 168.912956510431, 419707.3587315264, 419707.3587315264, 148081.1735941965], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6876000.0000, 
sim time next is 6876600.0000, 
raw observation next is [29.86666666666667, 31.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4661693020018569, 6.911199999999999, 6.9112, 168.912956510431, 416885.9759888996, 416885.9759889003, 147709.884189609], 
processed observation next is [0.0, 0.6086956521739131, 0.6145339652448659, 0.3116666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.34898695366080107, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11580165999691655, 0.11580165999691674, 0.22046251371583434], 
reward next is 0.7795, 
noisyNet noise sample is [array([0.7168776], dtype=float32), -1.0314986]. 
=============================================
[2019-03-25 23:51:15,318] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:51:15,329] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4180
[2019-03-25 23:51:15,335] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.73333333333333, 75.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7813816511652258, 6.911200000000001, 6.9112, 168.912956510431, 652996.3637531785, 652996.3637531778, 197999.7814613417], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7062600.0000, 
sim time next is 7063200.0000, 
raw observation next is [27.6, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7869203573786151, 6.9112, 6.9112, 168.912956510431, 657716.0137445178, 657716.0137445178, 199118.7355560781], 
processed observation next is [1.0, 0.782608695652174, 0.5071090047393366, 0.76, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7401467772909939, 0.0, 0.0, 0.8294399451523027, 0.1826988927068105, 0.1826988927068105, 0.2971921426210121], 
reward next is 0.7028, 
noisyNet noise sample is [array([-0.24963513], dtype=float32), -1.4459352]. 
=============================================
[2019-03-25 23:51:25,648] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:51:25,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9122
[2019-03-25 23:51:25,663] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.53333333333333, 66.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.71328937122013, 6.9112, 6.9112, 168.912956510431, 608711.9609456295, 608711.9609456295, 185130.9002054096], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6990000.0000, 
sim time next is 6990600.0000, 
raw observation next is [27.41666666666666, 67.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7184911613807711, 6.9112, 6.9112, 168.912956510431, 612532.4056348385, 612532.4056348385, 186082.628253749], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690361, 0.6783333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6566965382692329, 0.0, 0.0, 0.8294399451523027, 0.17014789045412182, 0.17014789045412182, 0.27773526605037163], 
reward next is 0.7223, 
noisyNet noise sample is [array([-1.2742722], dtype=float32), -1.2690928]. 
=============================================
[2019-03-25 23:51:26,783] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:51:26,792] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8695
[2019-03-25 23:51:26,803] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1772782.25094562 W.
[2019-03-25 23:51:26,809] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.7, 47.0, 1.0, 2.0, 0.4195418115873028, 1.0, 2.0, 0.4195418115873028, 1.0, 2.0, 0.701775820828254, 6.9112, 6.9112, 170.5573041426782, 1772782.25094562, 1772782.25094562, 360990.3611791487], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7045200.0000, 
sim time next is 7045800.0000, 
raw observation next is [31.78333333333333, 46.50000000000001, 1.0, 2.0, 0.3657528737630069, 1.0, 2.0, 0.3657528737630069, 1.0, 2.0, 0.6134788416638997, 6.911199999999999, 6.9112, 170.5573041426782, 1551377.228568302, 1551377.228568303, 333460.5633430263], 
processed observation next is [1.0, 0.5652173913043478, 0.7053712480252764, 0.4650000000000001, 1.0, 1.0, 0.23584683585904448, 1.0, 1.0, 0.23584683585904448, 1.0, 1.0, 0.5286327337364629, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.43093811904675056, 0.43093811904675083, 0.4977023333478005], 
reward next is 0.5023, 
noisyNet noise sample is [array([0.5251375], dtype=float32), 2.3247762]. 
=============================================
[2019-03-25 23:51:27,135] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-25 23:51:27,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:51:27,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:51:27,142] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:51:27,142] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:51:27,143] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:51:27,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:51:27,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:51:27,147] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:51:27,149] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:51:27,145] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:51:27,171] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-25 23:51:27,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-25 23:51:27,212] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-25 23:51:27,232] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-25 23:51:27,233] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-25 23:51:28,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:51:28,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.65, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.803511706507405, 6.9112, 6.9112, 168.912956510431, 676043.7724652012, 676043.7724652012, 202612.3051926476]
[2019-03-25 23:51:28,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:51:28,903] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4796329653965957
[2019-03-25 23:51:45,757] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:51:45,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.36959339, 92.01397524000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5305893837634722, 6.911200000000001, 6.9112, 168.912956510431, 468561.0954832707, 468561.0954832701, 155986.5522978892]
[2019-03-25 23:51:45,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:51:45,760] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3062537721318619
[2019-03-25 23:51:57,700] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:51:57,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.88847053333333, 95.07992857666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5289238314668726, 6.911199999999999, 6.9112, 168.912956510431, 469436.8756415831, 469436.8756415838, 155660.0821204227]
[2019-03-25 23:51:57,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:51:57,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4237673033349437
[2019-03-25 23:52:26,766] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:52:26,767] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.97334727833333, 65.87854262, 1.0, 2.0, 0.8879529390782028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1241093.035256032, 1241093.035256032, 266643.3802686236]
[2019-03-25 23:52:26,768] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:52:26,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.99999881e-01 0.00000000e+00 1.09635246e-35 1.69143493e-38
 1.54366944e-07], sampled 0.5054822419938532
[2019-03-25 23:52:26,774] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1241093.035256032 W.
[2019-03-25 23:52:59,326] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:52:59,327] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.14904612333333, 75.48245347, 1.0, 2.0, 0.9648428246016946, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1348630.391635833, 1348630.391635833, 288402.2965847631]
[2019-03-25 23:52:59,327] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:52:59,329] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.3078048e-35 5.6788457e-37 8.8576516e-12], sampled 0.46556911478715446
[2019-03-25 23:52:59,330] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1348630.391635833 W.
[2019-03-25 23:53:23,145] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:53:23,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.60124815666667, 83.00763221666668, 1.0, 2.0, 0.665728751683585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104297, 930353.9196524754, 930353.9196524747, 213618.8348893649]
[2019-03-25 23:53:23,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:53:23,149] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8543552e-25], sampled 0.8575871078865687
[2019-03-25 23:53:23,150] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 930353.9196524754 W.
[2019-03-25 23:53:26,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:53:26,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.1, 59.0, 1.0, 2.0, 0.9461144409580279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1322436.07479215, 1322436.07479215, 282944.5549791544]
[2019-03-25 23:53:26,548] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:53:26,549] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 5.7738662e-37 0.0000000e+00 1.2746974e-10], sampled 0.6896660143899933
[2019-03-25 23:53:26,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1322436.07479215 W.
[2019-03-25 23:53:31,260] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:53:31,263] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.53333333333333, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7864921132620545, 6.911200000000001, 6.9112, 168.912956510431, 659137.7586540052, 659137.7586540047, 199074.0130069624]
[2019-03-25 23:53:31,264] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:53:31,267] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.34642251673455016
[2019-03-25 23:53:34,827] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9458323], dtype=float32), 0.2859094]
[2019-03-25 23:53:34,827] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.86666666666667, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8848227263768965, 6.911199999999999, 6.9112, 168.912956510431, 729516.6458953219, 729516.6458953226, 219900.858466957]
[2019-03-25 23:53:34,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:53:34,830] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5184479279789979
[2019-03-25 23:53:34,900] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7046.9407 3198704070.8835 2380.0000
[2019-03-25 23:53:36,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8071.7169 2956827232.4744 1297.0000
[2019-03-25 23:53:36,291] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7365.8373 3119367498.2406 1966.0000
[2019-03-25 23:53:36,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7309.6472 3334303011.1388 2011.0000
[2019-03-25 23:53:36,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7968.6787 3003378109.9396 1428.0000
[2019-03-25 23:53:37,606] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 825000, evaluation results [825000.0, 7309.647176537197, 3334303011.1388044, 2011.0, 7365.837294501899, 3119367498.240572, 1966.0, 8071.716949633909, 2956827232.4743757, 1297.0, 7046.940698526813, 3198704070.8835163, 2380.0, 7968.67873988088, 3003378109.9395876, 1428.0]
[2019-03-25 23:53:38,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.3151576e-37 0.0000000e+00], sum to 1.0000
[2019-03-25 23:53:38,026] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9115
[2019-03-25 23:53:38,033] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2460597.451371354 W.
[2019-03-25 23:53:38,039] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.7, 47.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 8.239611235216401, 6.9112, 168.9054685046537, 2460597.451371354, 1518218.578756851, 321178.5607497963], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7045200.0000, 
sim time next is 7045800.0000, 
raw observation next is [31.78333333333333, 46.50000000000001, 1.0, 2.0, 0.5996855799597001, 0.0, 2.0, 0.0, 1.0, 1.0, 1.026356246703052, 6.911199999999999, 6.9112, 168.9118748555719, 1746122.087310169, 1746122.08731017, 369363.2531192626], 
processed observation next is [1.0, 0.5652173913043478, 0.7053712480252764, 0.4650000000000001, 1.0, 1.0, 0.5176934698309639, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0321417642720148, -8.881784197001253e-17, 0.0, 0.8294346337317609, 0.48503391314171357, 0.4850339131417139, 0.5512884374914366], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06974105], dtype=float32), -1.5359529]. 
=============================================
[2019-03-25 23:53:49,634] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:53:49,647] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7439
[2019-03-25 23:53:49,653] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.66666666666666, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6011192772246988, 6.911199999999999, 6.9112, 168.912956510431, 524635.574303105, 524635.5743031057, 166225.1636294071], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7368000.0000, 
sim time next is 7368600.0000, 
raw observation next is [21.48333333333333, 90.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5907297488199177, 6.9112, 6.9112, 168.912956510431, 516408.1460469738, 516408.1460469738, 164637.5764666589], 
processed observation next is [1.0, 0.2608695652173913, 0.21721958925750387, 0.9083333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5008899375852655, 0.0, 0.0, 0.8294399451523027, 0.1434467072352705, 0.1434467072352705, 0.24572772606964016], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.5407893], dtype=float32), -2.2536843]. 
=============================================
[2019-03-25 23:53:54,535] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:53:54,549] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6418
[2019-03-25 23:53:54,560] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.33333333333333, 91.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5655745769539314, 6.9112, 6.9112, 168.912956510431, 495301.1078864357, 495301.1078864357, 160940.1260174946], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7440000.0000, 
sim time next is 7440600.0000, 
raw observation next is [21.31666666666667, 91.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5664952791411441, 6.9112, 6.9112, 168.912956510431, 496089.2785674792, 496089.2785674792, 161072.0681978588], 
processed observation next is [0.0, 0.08695652173913043, 0.20932069510268583, 0.9183333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47133570626968785, 0.0, 0.0, 0.8294399451523027, 0.13780257737985535, 0.13780257737985535, 0.2404060719371027], 
reward next is 0.7596, 
noisyNet noise sample is [array([-0.81134355], dtype=float32), 0.25470555]. 
=============================================
[2019-03-25 23:53:55,763] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.982361e-28 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-25 23:53:55,775] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9699
[2019-03-25 23:53:55,779] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.61666666666667, 90.16666666666666, 1.0, 2.0, 0.2008648956202331, 1.0, 2.0, 0.2008648956202331, 1.0, 2.0, 0.363070677597448, 6.9112, 6.9112, 170.5573041426782, 955051.0328112654, 955051.0328112654, 282376.7776479535], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7401000.0000, 
sim time next is 7401600.0000, 
raw observation next is [20.6, 90.0, 1.0, 2.0, 0.2059989012344695, 1.0, 2.0, 0.2059989012344695, 1.0, 2.0, 0.3725145104288997, 6.911199999999999, 6.9112, 170.5573041426782, 980221.7459084073, 980221.745908408, 284022.412856411], 
processed observation next is [1.0, 0.6956521739130435, 0.17535545023696694, 0.9, 1.0, 1.0, 0.04337217016201145, 1.0, 1.0, 0.04337217016201145, 1.0, 1.0, 0.23477379320597525, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2722838183078909, 0.2722838183078911, 0.4239140490394194], 
reward next is 0.5761, 
noisyNet noise sample is [array([-0.28557578], dtype=float32), -0.555787]. 
=============================================
[2019-03-25 23:54:06,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2422791e-26], sum to 1.0000
[2019-03-25 23:54:06,278] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2715
[2019-03-25 23:54:06,283] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.13333333333333, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5621493163991612, 6.911200000000001, 6.9112, 168.912956510431, 493494.2795078528, 493494.2795078522, 160419.3269604613], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7424400.0000, 
sim time next is 7425000.0000, 
raw observation next is [21.1, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.564589765348383, 6.9112, 6.9112, 168.912956510431, 495319.0347154685, 495319.0347154685, 160775.2990680395], 
processed observation next is [1.0, 0.9565217391304348, 0.1990521327014219, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4690119089614427, 0.0, 0.0, 0.8294399451523027, 0.1375886207542968, 0.1375886207542968, 0.23996313293737237], 
reward next is 0.7600, 
noisyNet noise sample is [array([0.5431639], dtype=float32), 0.68274]. 
=============================================
[2019-03-25 23:54:06,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.81918 ]
 [78.94502 ]
 [79.028824]
 [78.98716 ]
 [79.200325]], R is [[78.95072937]
 [78.92179108]
 [78.89377594]
 [78.86637115]
 [78.83917999]].
[2019-03-25 23:54:10,630] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:54:10,640] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5573
[2019-03-25 23:54:10,646] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.33333333333334, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6751206602903538, 6.9112, 6.9112, 168.912956510431, 579066.6838532434, 579066.6838532434, 178352.9320076877], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7474800.0000, 
sim time next is 7475400.0000, 
raw observation next is [24.45, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6770866613615205, 6.911199999999999, 6.9112, 168.912956510431, 580439.909540628, 580439.9095406285, 178692.8597619729], 
processed observation next is [0.0, 0.5217391304347826, 0.3578199052132702, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6062032455628299, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16123330820572998, 0.16123330820573015, 0.2667057608387655], 
reward next is 0.7333, 
noisyNet noise sample is [array([0.22740111], dtype=float32), -0.19961633]. 
=============================================
[2019-03-25 23:54:21,674] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:54:21,680] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2070
[2019-03-25 23:54:21,686] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.56666666666666, 74.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8455492052922383, 6.9112, 6.9112, 168.912956510431, 699763.2198669618, 699763.2198669618, 211239.7626259549], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7756800.0000, 
sim time next is 7757400.0000, 
raw observation next is [28.38333333333333, 75.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8508045477029827, 6.9112, 6.9112, 168.912956510431, 703989.6573216347, 703989.6573216347, 212383.8087346606], 
processed observation next is [1.0, 0.782608695652174, 0.5442338072669825, 0.7583333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.818054326467052, 0.0, 0.0, 0.8294399451523027, 0.19555268258934297, 0.19555268258934297, 0.31699075930546355], 
reward next is 0.6830, 
noisyNet noise sample is [array([-0.27385283], dtype=float32), 2.1575055]. 
=============================================
[2019-03-25 23:54:22,177] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:54:22,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1525
[2019-03-25 23:54:22,194] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.05, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9109495312271028, 6.9112, 6.9112, 168.912956510431, 744888.8867115246, 744888.8867115246, 225698.39985499], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7845000.0000, 
sim time next is 7845600.0000, 
raw observation next is [27.9, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9167216046231488, 6.9112, 6.9112, 168.912956510431, 749302.0141029314, 749302.0141029314, 227043.9162624625], 
processed observation next is [1.0, 0.8260869565217391, 0.5213270142180094, 0.8433333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8984409812477424, 0.0, 0.0, 0.8294399451523027, 0.20813944836192538, 0.20813944836192538, 0.3388715168096455], 
reward next is 0.6611, 
noisyNet noise sample is [array([0.0702412], dtype=float32), 0.6671895]. 
=============================================
[2019-03-25 23:54:30,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:30,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:30,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-25 23:54:32,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.6037661e-38 0.0000000e+00 3.2924394e-35 2.3216048e-22], sum to 1.0000
[2019-03-25 23:54:32,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:32,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:32,417] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0908
[2019-03-25 23:54:32,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2211306.674334531 W.
[2019-03-25 23:54:32,438] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.3, 68.5, 1.0, 2.0, 0.7906979104573999, 1.0, 1.0, 0.7906979104573999, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2211306.674334531, 2211306.674334531, 415364.8145632812], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7914600.0000, 
sim time next is 7915200.0000, 
raw observation next is [30.33333333333334, 68.0, 1.0, 2.0, 0.9612056129113954, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.987716306452471, 6.9112, 168.9125011183457, 2240703.422198464, 2186420.26726172, 452270.4719092184], 
processed observation next is [1.0, 0.6086956521739131, 0.6366508688783573, 0.68, 1.0, 1.0, 0.9532597745920426, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007651630645247121, 0.0, 0.8294377089686844, 0.6224176172773511, 0.6073389631282555, 0.6750305550883857], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5006565], dtype=float32), 0.73905283]. 
=============================================
[2019-03-25 23:54:32,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-25 23:54:32,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:32,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:32,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-25 23:54:34,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:34,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:34,228] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-25 23:54:35,230] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:35,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:35,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-25 23:54:35,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:35,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:35,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-25 23:54:36,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:36,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:36,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-25 23:54:37,296] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:54:37,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7950
[2019-03-25 23:54:37,310] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.83333333333334, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6275063890429085, 6.911200000000001, 6.9112, 168.912956510431, 544875.285667902, 544875.2856679014, 170391.4248127447], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 78000.0000, 
sim time next is 78600.0000, 
raw observation next is [22.76666666666667, 86.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6265973673674123, 6.911199999999999, 6.9112, 168.912956510431, 544213.7421216262, 544213.7421216267, 170244.5118804202], 
processed observation next is [1.0, 0.9130434782608695, 0.2780410742496052, 0.8666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5446309358139174, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15117048392267393, 0.1511704839226741, 0.25409628638868687], 
reward next is 0.7459, 
noisyNet noise sample is [array([0.41125333], dtype=float32), 0.609567]. 
=============================================
[2019-03-25 23:54:40,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:40,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:40,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-25 23:54:40,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:40,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:40,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-25 23:54:41,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:41,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:41,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-25 23:54:41,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:41,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:41,884] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-25 23:54:42,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:42,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-25 23:54:42,388] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:42,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:42,396] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-25 23:54:42,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-25 23:54:42,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:42,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,506] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-25 23:54:42,597] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:54:42,598] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2974
[2019-03-25 23:54:42,603] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.86666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5334769656670676, 6.9112, 6.9112, 168.912956510431, 472958.204721603, 472958.204721603, 156290.1658542408], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 9600.0000, 
sim time next is 10200.0000, 
raw observation next is [20.93333333333333, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.527950953401446, 6.9112, 6.9112, 168.912956510431, 467794.6389357135, 467794.6389357135, 155569.1096896605], 
processed observation next is [1.0, 0.08695652173913043, 0.19115323854660338, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4243304309773732, 0.0, 0.0, 0.8294399451523027, 0.12994295525992042, 0.12994295525992042, 0.232192701029344], 
reward next is 0.7678, 
noisyNet noise sample is [array([-1.0314897], dtype=float32), -1.1676325]. 
=============================================
[2019-03-25 23:54:42,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-25 23:54:42,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,711] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-25 23:54:42,925] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-25 23:54:42,926] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:54:42,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,927] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:54:42,928] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:54:42,930] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,931] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:54:42,931] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,929] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:54:42,931] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,935] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:54:42,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-25 23:54:42,954] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-25 23:54:42,971] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-25 23:54:42,996] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-25 23:54:43,013] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-25 23:55:04,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9398335], dtype=float32), 0.2445332]
[2019-03-25 23:55:04,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.28333333333333, 48.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3852144958079234, 6.9112, 6.9112, 168.912956510431, 348821.9296253172, 348821.9296253172, 138899.7253738708]
[2019-03-25 23:55:04,903] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:55:04,904] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5183454070626944
[2019-03-25 23:55:09,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9398335], dtype=float32), 0.2445332]
[2019-03-25 23:55:09,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.2, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7332658550327492, 6.911200000000001, 6.9112, 168.912956510431, 622443.8504055929, 622443.8504055924, 188813.1891316459]
[2019-03-25 23:55:09,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:55:09,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.20650876172363097
[2019-03-25 23:55:14,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.9398335], dtype=float32), 0.2445332]
[2019-03-25 23:55:14,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.73206369333333, 96.33450855000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6224778603612607, 6.9112, 6.9112, 168.912956510431, 541808.2634240912, 541808.2634240912, 169571.0586257146]
[2019-03-25 23:55:14,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:55:14,020] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5880644472673112
[2019-03-25 23:55:40,572] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9398335], dtype=float32), 0.2445332]
[2019-03-25 23:55:40,573] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.63333333333333, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.923006742657933, 6.9112, 168.9126577196105, 863589.5384890229, 855213.4437072983, 256095.8073571085]
[2019-03-25 23:55:40,575] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:55:40,578] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8869621688472259
[2019-03-25 23:56:51,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7310.9736 3326159592.2170 2065.0000
[2019-03-25 23:56:51,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9398335], dtype=float32), 0.2445332]
[2019-03-25 23:56:51,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.17078584333333, 93.04944754333333, 1.0, 2.0, 0.7343514177944086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026300.226881347, 1026300.226881347, 228439.530347141]
[2019-03-25 23:56:51,210] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-25 23:56:51,213] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9816185e-01 5.7672806e-36 5.1896176e-35 7.1236823e-35 1.8381018e-03], sampled 0.9284483186767212
[2019-03-25 23:56:51,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1026300.226881347 W.
[2019-03-25 23:56:52,033] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7948.7173 2997043267.0942 1478.0000
[2019-03-25 23:56:52,510] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7346.8576 3112025291.8686 2008.0000
[2019-03-25 23:56:52,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7031.2092 3191245186.3217 2436.0000
[2019-03-25 23:56:52,573] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8074.7072 2947394066.5830 1329.0000
[2019-03-25 23:56:53,588] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 850000, evaluation results [850000.0, 7310.973630982416, 3326159592.216966, 2065.0, 7346.8575937301675, 3112025291.868648, 2008.0, 8074.707166915708, 2947394066.582958, 1329.0, 7031.209240854114, 3191245186.3216567, 2436.0, 7948.7172524365915, 2997043267.0941734, 1478.0]
[2019-03-25 23:56:59,242] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:56:59,257] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9348
[2019-03-25 23:56:59,266] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.75, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5486437934608952, 6.9112, 6.9112, 168.912956510431, 482675.1279515905, 482675.1279515905, 158497.7582798681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 210600.0000, 
sim time next is 211200.0000, 
raw observation next is [20.76666666666667, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.547414837579121, 6.9112, 6.9112, 168.912956510431, 481606.5066932103, 481606.5066932103, 158328.0242059373], 
processed observation next is [0.0, 0.43478260869565216, 0.18325434439178534, 0.9233333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.448066875096489, 0.0, 0.0, 0.8294399451523027, 0.13377958519255842, 0.13377958519255842, 0.23631048388945866], 
reward next is 0.7637, 
noisyNet noise sample is [array([0.3956962], dtype=float32), 0.25720274]. 
=============================================
[2019-03-25 23:57:02,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.677924e-26], sum to 1.0000
[2019-03-25 23:57:02,202] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9605
[2019-03-25 23:57:02,208] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.23333333333333, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4715793411151544, 6.911199999999999, 6.9112, 168.912956510431, 418694.5448070042, 418694.5448070049, 148512.8051709988], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 355200.0000, 
sim time next is 355800.0000, 
raw observation next is [20.21666666666667, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4685890997958579, 6.9112, 6.9112, 168.912956510431, 416197.0379386503, 416197.0379386503, 148156.9433790827], 
processed observation next is [1.0, 0.08695652173913043, 0.15718799368088482, 0.8783333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3519379265803145, 0.0, 0.0, 0.8294399451523027, 0.11561028831629175, 0.11561028831629175, 0.22112976623743685], 
reward next is 0.7789, 
noisyNet noise sample is [array([-0.2729556], dtype=float32), -0.6436206]. 
=============================================
[2019-03-25 23:57:02,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 2.8825795e-37 0.0000000e+00 0.0000000e+00 5.1280981e-08], sum to 1.0000
[2019-03-25 23:57:02,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6787
[2019-03-25 23:57:02,513] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.4, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6189474282681109, 6.9112, 6.9112, 168.912956510431, 538335.8262998111, 538335.8262998111, 169020.8816922606], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [21.3, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6126902527597909, 6.9112, 6.9112, 168.912956510431, 533454.9806005908, 533454.9806005908, 168032.1687860991], 
processed observation next is [1.0, 0.9130434782608695, 0.2085308056872039, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5276710399509645, 0.0, 0.0, 0.8294399451523027, 0.14818193905571966, 0.14818193905571966, 0.25079428177029717], 
reward next is 0.7492, 
noisyNet noise sample is [array([0.3022149], dtype=float32), -1.3538803]. 
=============================================
[2019-03-25 23:57:02,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[76.57011]
 [76.48745]
 [76.33149]
 [76.28443]
 [76.12801]], R is [[76.65602875]
 [76.6371994 ]
 [76.61704254]
 [76.59568024]
 [76.57324982]].
[2019-03-25 23:57:13,800] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:57:13,809] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1624
[2019-03-25 23:57:13,813] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.25, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4979225551252273, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 151742.0601607622], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [20.4, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5063997459574248, 6.911199999999999, 6.9112, 168.912956510431, 448052.7956005751, 448052.7956005757, 152814.9784660993], 
processed observation next is [0.0, 0.2608695652173913, 0.16587677725118483, 0.9066666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.39804847067978627, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12445910988904864, 0.1244591098890488, 0.2280820574120885], 
reward next is 0.7719, 
noisyNet noise sample is [array([-0.9734589], dtype=float32), 0.30838808]. 
=============================================
[2019-03-25 23:57:13,827] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[73.96527]
 [74.05451]
 [74.04022]
 [74.12262]
 [74.17527]], R is [[73.86615753]
 [73.90101624]
 [73.9361496 ]
 [73.97160339]
 [74.00727081]].
[2019-03-25 23:57:16,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:57:16,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9882
[2019-03-25 23:57:16,861] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.63333333333333, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4279613055184083, 6.911200000000001, 6.9112, 168.912956510431, 383387.5142021794, 383387.5142021788, 143468.7176683537], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 444000.0000, 
sim time next is 444600.0000, 
raw observation next is [19.65, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4261953090792314, 6.9112, 6.9112, 168.912956510431, 381818.5767483634, 381818.5767483634, 143283.5604677909], 
processed observation next is [1.0, 0.13043478260869565, 0.13033175355450236, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.30023818180394074, 0.0, 0.0, 0.8294399451523027, 0.10606071576343427, 0.10606071576343427, 0.2138560603996879], 
reward next is 0.7861, 
noisyNet noise sample is [array([1.0435065], dtype=float32), 0.30744985]. 
=============================================
[2019-03-25 23:57:24,693] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.1517706e-28], sum to 1.0000
[2019-03-25 23:57:24,702] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7285
[2019-03-25 23:57:24,709] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.4, 56.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7202564811192492, 6.911200000000001, 6.9112, 168.912956510431, 642331.2885568935, 642331.2885568929, 185679.9257392984], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 565200.0000, 
sim time next is 565800.0000, 
raw observation next is [24.31666666666667, 56.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8056972620803401, 6.9112, 6.9112, 168.912956510431, 718684.7162382543, 718684.7162382543, 202302.1549749257], 
processed observation next is [1.0, 0.5652173913043478, 0.3515007898894157, 0.565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7630454415613902, 0.0, 0.0, 0.8294399451523027, 0.1996346433995151, 0.1996346433995151, 0.3019435148879488], 
reward next is 0.6981, 
noisyNet noise sample is [array([-1.3813068], dtype=float32), -0.96501666]. 
=============================================
[2019-03-25 23:57:41,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7829564e-30], sum to 1.0000
[2019-03-25 23:57:41,078] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2455
[2019-03-25 23:57:41,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.56666666666667, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.447870054624677, 6.9112, 6.9112, 168.912956510431, 401142.3041803128, 401142.3041803128, 145608.8725846529], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [20.45, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4505314518753344, 6.911200000000001, 6.9112, 168.912956510431, 403544.4234412894, 403544.4234412888, 145900.9327319429], 
processed observation next is [1.0, 0.8695652173913043, 0.16824644549763035, 0.775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3299164047260175, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11209567317813594, 0.11209567317813578, 0.21776258616707897], 
reward next is 0.7822, 
noisyNet noise sample is [array([-0.5706552], dtype=float32), -1.112183]. 
=============================================
[2019-03-25 23:57:50,314] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.853714e-19 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-25 23:57:50,321] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7841
[2019-03-25 23:57:50,326] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.93333333333333, 93.33333333333334, 1.0, 1.0, 0.1826535967613145, 1.0, 1.0, 0.1826535967613145, 1.0, 2.0, 0.321671747974187, 6.9112, 6.9112, 170.5573041426782, 832436.3421118376, 832436.3421118376, 273462.099682498], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 984000.0000, 
sim time next is 984600.0000, 
raw observation next is [21.95, 93.5, 1.0, 2.0, 0.1933765677727774, 1.0, 2.0, 0.1933765677727774, 1.0, 2.0, 0.3403173593890433, 6.9112, 6.9112, 170.5573041426782, 880368.4518614245, 880368.4518614245, 276304.5890342247], 
processed observation next is [1.0, 0.391304347826087, 0.2393364928909953, 0.935, 1.0, 1.0, 0.02816453948527396, 1.0, 1.0, 0.02816453948527396, 1.0, 1.0, 0.19550897486468694, 0.0, 0.0, 0.8375144448122397, 0.24454679218372902, 0.24454679218372902, 0.4123949090063055], 
reward next is 0.5876, 
noisyNet noise sample is [array([0.60785204], dtype=float32), 1.0072467]. 
=============================================
[2019-03-25 23:57:52,449] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:57:52,459] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2399
[2019-03-25 23:57:52,469] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.13333333333333, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5395362577470485, 6.911200000000001, 6.9112, 168.912956510431, 474551.981476205, 474551.9814762044, 157256.3974010759], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 850800.0000, 
sim time next is 851400.0000, 
raw observation next is [22.1, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5409569024321832, 6.9112, 6.9112, 168.912956510431, 475665.2280685279, 475665.2280685279, 157453.6221528031], 
processed observation next is [0.0, 0.8695652173913043, 0.24644549763033188, 0.825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44019134442949165, 0.0, 0.0, 0.8294399451523027, 0.1321292300190355, 0.1321292300190355, 0.23500540619821356], 
reward next is 0.7650, 
noisyNet noise sample is [array([0.26110837], dtype=float32), -0.7856876]. 
=============================================
[2019-03-25 23:57:54,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4437305e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9975568e-01], sum to 1.0000
[2019-03-25 23:57:54,888] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5350
[2019-03-25 23:57:54,894] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.73333333333333, 78.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1839743728778024, 6.911200000000001, 6.9112, 170.5573041426782, 482664.1668739647, 482664.1668739641, 227666.0445938756], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 844800.0000, 
sim time next is 845400.0000, 
raw observation next is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1831385702155327, 6.911199999999999, 6.9112, 170.5573041426782, 480668.6070544163, 480668.6070544169, 227368.2974215789], 
processed observation next is [0.0, 0.782608695652174, 0.27330173775671435, 0.7883333333333334, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0038275246530886705, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.13351905751511564, 0.1335190575151158, 0.3393556677934013], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2138158], dtype=float32), 0.35217062]. 
=============================================
[2019-03-25 23:57:56,280] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 6.6994152e-36 1.3777760e-34 0.0000000e+00 3.8275728e-13], sum to 1.0000
[2019-03-25 23:57:56,287] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2140
[2019-03-25 23:57:56,294] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.98333333333333, 97.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.641723041052103, 6.9112, 6.9112, 168.912956510431, 553924.9319534475, 553924.9319534475, 172725.2973206681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1032600.0000, 
sim time next is 1033200.0000, 
raw observation next is [22.0, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6504730765475086, 6.9112, 6.9112, 168.912956510431, 561246.9547247476, 561246.9547247476, 174168.2666989505], 
processed observation next is [1.0, 1.0, 0.2417061611374408, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.57374765432623, 0.0, 0.0, 0.8294399451523027, 0.15590193186798543, 0.15590193186798543, 0.2599526368641052], 
reward next is 0.7400, 
noisyNet noise sample is [array([1.4266793], dtype=float32), 0.54006886]. 
=============================================
[2019-03-25 23:57:57,169] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:57:57,182] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7001
[2019-03-25 23:57:57,186] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.81666666666667, 87.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5085023730455244, 6.9112, 6.9112, 168.912956510431, 449618.1873029737, 449618.1873029737, 153093.6915725852], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 881400.0000, 
sim time next is 882000.0000, 
raw observation next is [20.8, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5066104375715913, 6.911199999999999, 6.9112, 168.912956510431, 448091.5260082611, 448091.5260082617, 152847.8840850369], 
processed observation next is [0.0, 0.21739130434782608, 0.1848341232227489, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3983054116726723, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12446986833562808, 0.12446986833562826, 0.22813117027617447], 
reward next is 0.7719, 
noisyNet noise sample is [array([-0.82146555], dtype=float32), -0.2367335]. 
=============================================
[2019-03-25 23:57:57,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.748314]
 [80.77676 ]
 [80.89009 ]
 [80.930504]
 [80.96933 ]], R is [[80.7695694 ]
 [80.73337555]
 [80.69726562]
 [80.66127014]
 [80.62510681]].
[2019-03-25 23:58:01,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-25 23:58:01,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7340
[2019-03-25 23:58:01,106] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5941734604393879, 6.911199999999999, 6.9112, 168.912956510431, 516468.1102545601, 516468.1102545608, 165213.3266816037], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 979200.0000, 
sim time next is 979800.0000, 
raw observation next is [21.9, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6181089625548146, 6.911199999999999, 6.9112, 168.912956510431, 537278.6143519953, 537278.6143519959, 168894.7038893374], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5342792226278227, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14924405954222092, 0.14924405954222109, 0.25208164759602597], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.62169015], dtype=float32), -1.1380974]. 
=============================================
[2019-03-25 23:58:01,156] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-25 23:58:01,160] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-25 23:58:01,162] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-25 23:58:01,162] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:58:01,164] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-25 23:58:01,166] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:58:01,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-25 23:58:01,170] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:58:01,172] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:58:01,167] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-25 23:58:01,175] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-25 23:58:01,191] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-25 23:58:01,212] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-25 23:58:01,213] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-25 23:58:01,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-25 23:58:01,280] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-25 23:58:20,880] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:58:20,881] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.66433679, 58.3148235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7670464100808074, 6.911199999999999, 6.9112, 168.912956510431, 658643.4855300128, 658643.4855300135, 195360.9375422048]
[2019-03-25 23:58:20,881] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:58:20,882] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.48579452642835896
[2019-03-25 23:58:23,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:58:23,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [18.73333333333333, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4303922032017198, 6.911199999999999, 6.9112, 168.912956510431, 387411.1531995882, 387411.1531995889, 143575.1247755402]
[2019-03-25 23:58:23,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-25 23:58:23,082] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6919253242296763
[2019-03-25 23:58:40,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:58:40,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.9, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8928904383362068, 6.911199999999999, 6.9112, 168.912956510431, 734809.4335724226, 734809.4335724232, 221697.9584634929]
[2019-03-25 23:58:40,385] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-25 23:58:40,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.11430642177950812
[2019-03-25 23:58:49,955] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:58:49,955] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.28333333333333, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8617672359934689, 6.911199999999999, 6.9112, 168.912956510431, 714844.2608981506, 714844.2608981512, 214862.2415808139]
[2019-03-25 23:58:49,956] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-25 23:58:49,961] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.656555206471816
[2019-03-25 23:58:53,464] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:58:53,464] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.24162553333333, 95.09711964333333, 1.0, 1.0, 0.6417648497388247, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9127616454624, 896850.2994185232, 896850.2994185239, 208783.2839632801]
[2019-03-25 23:58:53,466] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:58:53,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1706501e-21], sampled 0.6456755424889815
[2019-03-25 23:58:53,470] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 896850.2994185232 W.
[2019-03-25 23:59:17,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:59:17,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.819255765, 71.88188083833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9801975569355974, 6.9112, 6.9112, 168.912956510431, 806150.2005124582, 806150.2005124582, 242802.9933309837]
[2019-03-25 23:59:17,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:59:17,763] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5886764323976381
[2019-03-25 23:59:59,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-25 23:59:59,570] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.31279509833333, 83.62743689666665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6531250833084594, 6.9112, 6.9112, 168.912956510431, 565419.4668342371, 565419.4668342371, 174591.4484784735]
[2019-03-25 23:59:59,571] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-25 23:59:59,575] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9920718552331014
[2019-03-26 00:00:03,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.95883], dtype=float32), 0.17749782]
[2019-03-26 00:00:03,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.2, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6989739859182388, 6.911199999999999, 6.9112, 168.912956510431, 599959.0578519736, 599959.0578519743, 182546.349428622]
[2019-03-26 00:00:03,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:00:03,330] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.790701799861696
[2019-03-26 00:00:09,885] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7961.7012 3015807726.8901 1363.0000
[2019-03-26 00:00:10,245] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7402.6963 3129384940.4150 1867.0000
[2019-03-26 00:00:10,304] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8086.9378 2968238053.4737 1208.0000
[2019-03-26 00:00:10,603] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7095.5500 3208904492.9908 2230.0000
[2019-03-26 00:00:10,642] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7337.7670 3343239957.1008 1935.0000
[2019-03-26 00:00:11,659] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 875000, evaluation results [875000.0, 7337.76703154107, 3343239957.100797, 1935.0, 7402.696305947413, 3129384940.4150114, 1867.0, 8086.937819865536, 2968238053.4736733, 1208.0, 7095.549966042604, 3208904492.99082, 2230.0, 7961.701168246136, 3015807726.8900995, 1363.0]
[2019-03-26 00:00:17,294] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:00:17,306] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5053
[2019-03-26 00:00:17,310] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.73333333333333, 96.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6332488484868561, 6.9112, 6.9112, 168.912956510431, 548783.4748986296, 548783.4748986296, 171327.3063828459], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1016400.0000, 
sim time next is 1017000.0000, 
raw observation next is [21.75, 96.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6334313819159126, 6.911199999999999, 6.9112, 168.912956510431, 548965.748357719, 548965.7483577196, 171356.431082285], 
processed observation next is [1.0, 0.782608695652174, 0.2298578199052133, 0.965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5529650998974545, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15249048565492196, 0.15249048565492213, 0.25575586728699257], 
reward next is 0.7442, 
noisyNet noise sample is [array([-0.515892], dtype=float32), 1.2804697]. 
=============================================
[2019-03-26 00:00:17,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.288826]
 [69.122406]
 [68.83339 ]
 [68.337524]
 [68.177605]], R is [[69.69335938]
 [69.74071503]
 [69.78633118]
 [69.83258057]
 [69.87828064]].
[2019-03-26 00:00:22,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.631781e-27], sum to 1.0000
[2019-03-26 00:00:22,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3733
[2019-03-26 00:00:22,088] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.26666666666667, 73.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7409646829854372, 6.9112, 6.9112, 168.912956510431, 644281.3714847021, 644281.3714847021, 190224.0215321245], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1082400.0000, 
sim time next is 1083000.0000, 
raw observation next is [24.43333333333334, 72.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7179133080389848, 6.911199999999999, 6.9112, 168.912956510431, 625421.1522860407, 625421.1522860414, 185887.7351868344], 
processed observation next is [1.0, 0.5217391304347826, 0.3570300157977887, 0.7266666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6559918390719327, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17372809785723353, 0.17372809785723373, 0.27744438087587225], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.891036], dtype=float32), 1.233828]. 
=============================================
[2019-03-26 00:00:22,116] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[59.75309 ]
 [60.88534 ]
 [59.979168]
 [59.1914  ]
 [58.960037]], R is [[59.41391754]
 [59.53586197]
 [58.94050217]
 [58.94492722]
 [58.93149567]].
[2019-03-26 00:00:42,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1013784e-13], sum to 1.0000
[2019-03-26 00:00:42,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6329
[2019-03-26 00:00:42,207] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.1, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.579554429353421, 6.9112, 6.9112, 168.912956510431, 507499.2158023393, 507499.2158023393, 162962.1140926449], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1364400.0000, 
sim time next is 1365000.0000, 
raw observation next is [21.11666666666667, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5793072350852353, 6.911200000000001, 6.9112, 168.912956510431, 507093.7928781672, 507093.7928781665, 162930.8635228664], 
processed observation next is [1.0, 0.8260869565217391, 0.19984202211690388, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48696004278687227, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14085938691060199, 0.1408593869106018, 0.24318039331771102], 
reward next is 0.7568, 
noisyNet noise sample is [array([-0.5435758], dtype=float32), 0.56387424]. 
=============================================
[2019-03-26 00:00:42,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[64.82961 ]
 [64.98215 ]
 [65.260796]
 [65.446655]
 [64.96874 ]], R is [[65.12230682]
 [65.2278595 ]
 [65.3326416 ]
 [65.43698883]
 [65.5398407 ]].
[2019-03-26 00:00:48,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8147856e-26], sum to 1.0000
[2019-03-26 00:00:48,158] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5070
[2019-03-26 00:00:48,169] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.7, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5855765059991198, 6.9112, 6.9112, 168.912956510431, 511347.4340524618, 511347.4340524618, 163883.7600232314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1561800.0000, 
sim time next is 1562400.0000, 
raw observation next is [21.7, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.585019010347259, 6.911199999999999, 6.9112, 168.912956510431, 510860.6854647915, 510860.6854647922, 163801.2103076859], 
processed observation next is [1.0, 0.08695652173913043, 0.2274881516587678, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49392562237470605, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14190574596244207, 0.14190574596244226, 0.24447941836968043], 
reward next is 0.7555, 
noisyNet noise sample is [array([0.3702299], dtype=float32), -0.14514302]. 
=============================================
[2019-03-26 00:00:57,130] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.4646085e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 00:00:57,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6085
[2019-03-26 00:00:57,144] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.63333333333334, 85.0, 1.0, 2.0, 0.2563383536618779, 1.0, 1.0, 0.2563383536618779, 1.0, 1.0, 0.4449706725531438, 6.9112, 6.9112, 170.5573041426782, 1142987.17129081, 1142987.17129081, 294368.1859698978], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1592400.0000, 
sim time next is 1593000.0000, 
raw observation next is [23.65, 85.0, 1.0, 2.0, 0.2556064912013811, 1.0, 2.0, 0.2556064912013811, 1.0, 2.0, 0.4429393465219646, 6.9112, 6.9112, 170.5573041426782, 1136791.415471146, 1136791.415471146, 293766.4118217128], 
processed observation next is [1.0, 0.43478260869565216, 0.31990521327014215, 0.85, 1.0, 1.0, 0.10314035084503749, 1.0, 1.0, 0.10314035084503749, 1.0, 1.0, 0.3206577396609324, 0.0, 0.0, 0.8375144448122397, 0.3157753931864295, 0.3157753931864295, 0.43845733107718327], 
reward next is 0.5615, 
noisyNet noise sample is [array([0.8709656], dtype=float32), -0.43495944]. 
=============================================
[2019-03-26 00:00:57,161] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[57.457138]
 [58.08726 ]
 [57.96329 ]
 [58.999905]
 [59.08494 ]], R is [[57.73947906]
 [57.72273254]
 [57.76535797]
 [57.18770599]
 [56.61582947]].
[2019-03-26 00:00:59,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6713406e-36], sum to 1.0000
[2019-03-26 00:00:59,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0703
[2019-03-26 00:00:59,032] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7217736967489842, 6.9112, 6.9112, 168.912956510431, 614413.0298833767, 614413.0298833767, 186682.8557376438], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1641600.0000, 
sim time next is 1642200.0000, 
raw observation next is [23.11666666666667, 98.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7227695064910701, 6.9112, 6.9112, 168.912956510431, 615138.4424327402, 615138.4424327402, 186866.4878507637], 
processed observation next is [1.0, 0.0, 0.29462875197472377, 0.9816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.661914032306183, 0.0, 0.0, 0.8294399451523027, 0.17087178956465007, 0.17087178956465007, 0.2789052057474085], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.70871246], dtype=float32), -0.4742615]. 
=============================================
[2019-03-26 00:01:04,498] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:01:04,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4129
[2019-03-26 00:01:04,516] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.45, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8591385695310747, 6.911199999999999, 6.9112, 168.912956510431, 711043.1117648128, 711043.1117648135, 214225.2265996367], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1716600.0000, 
sim time next is 1717200.0000, 
raw observation next is [26.4, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8613103741092889, 6.911199999999999, 6.9112, 168.912956510431, 712627.5384505275, 712627.538450528, 214699.5006499139], 
processed observation next is [1.0, 0.9130434782608695, 0.45023696682464454, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8308663098893767, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1979520940140354, 0.19795209401403557, 0.32044701589539387], 
reward next is 0.6796, 
noisyNet noise sample is [array([1.3893355], dtype=float32), 0.089570045]. 
=============================================
[2019-03-26 00:01:07,391] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:01:07,402] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3348
[2019-03-26 00:01:07,412] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.76666666666667, 89.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7680943529784626, 6.9112, 6.9112, 168.912956510431, 648744.9343583201, 648744.9343583201, 195491.2160661881], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1893000.0000, 
sim time next is 1893600.0000, 
raw observation next is [24.7, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7652404910790512, 6.9112, 6.9112, 168.912956510431, 646514.4292675435, 646514.4292675435, 194931.1941267917], 
processed observation next is [1.0, 0.9565217391304348, 0.3696682464454976, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7137079159500623, 0.0, 0.0, 0.8294399451523027, 0.1795873414632065, 0.1795873414632065, 0.2909420807862563], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.6395726], dtype=float32), -0.19606833]. 
=============================================
[2019-03-26 00:01:11,369] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.4006294e-30 4.1889840e-35 1.6592217e-28 2.9861053e-10], sum to 1.0000
[2019-03-26 00:01:11,381] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8190
[2019-03-26 00:01:11,395] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1160808.270206744 W.
[2019-03-26 00:01:11,400] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.3820683901828987, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6727808763855224, 6.911200000000001, 6.9112, 168.912956510431, 1160808.270206744, 1160808.270206743, 259766.6664861452], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1771800.0000, 
sim time next is 1772400.0000, 
raw observation next is [23.16666666666667, 85.66666666666667, 1.0, 2.0, 0.6946370548760876, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1064085.33671249, 1064085.336712491, 231458.7830972764], 
processed observation next is [1.0, 0.5217391304347826, 0.2969984202211693, 0.8566666666666667, 1.0, 1.0, 0.6320928372001056, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2955792601979139, 0.29557926019791414, 0.3454608702944424], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.3503995], dtype=float32), 0.20823261]. 
=============================================
[2019-03-26 00:01:19,272] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 00:01:19,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:01:19,277] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:01:19,278] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:01:19,278] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:01:19,279] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:01:19,280] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:01:19,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:01:19,281] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:01:19,282] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:01:19,287] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:01:19,314] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-26 00:01:19,336] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-26 00:01:19,337] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-26 00:01:19,373] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-26 00:01:19,404] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-26 00:01:25,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:01:25,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.66476166333334, 83.62968563999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.577445382872187, 6.9112, 6.9112, 168.912956510431, 505290.2200993362, 505290.2200993362, 162663.4429550859]
[2019-03-26 00:01:25,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:01:25,425] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4614079574811567
[2019-03-26 00:01:48,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:01:48,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.72292419, 91.2789257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7304129886508576, 6.9112, 6.9112, 168.912956510431, 620812.4066970196, 620812.4066970196, 188284.502464698]
[2019-03-26 00:01:48,599] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:01:48,602] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.17177925886503143
[2019-03-26 00:02:27,265] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:02:27,266] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.48333333333333, 52.66666666666667, 1.0, 2.0, 0.3483401425826545, 1.0, 1.0, 0.3483401425826545, 1.0, 1.0, 0.6049516298205925, 6.9112, 6.9112, 178.6582176852504, 1460722.695159889, 1460722.695159889, 328160.3151176222]
[2019-03-26 00:02:27,267] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:02:27,268] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.260749e-10 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.7882018415221312
[2019-03-26 00:02:27,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:02:27,994] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666667, 68.33333333333334, 1.0, 2.0, 0.6158849449608891, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564927939, 860669.126569043, 860669.126569043, 203745.3225831508]
[2019-03-26 00:02:27,997] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:02:28,001] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24990666392265715
[2019-03-26 00:02:43,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:02:43,244] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.17033841, 67.49979331, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.02597287265926, 6.9112, 168.9121609267414, 910256.1295637871, 828832.6870298769, 254812.2985747685]
[2019-03-26 00:02:43,250] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:02:43,252] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6308974e-27], sampled 0.765513675174573
[2019-03-26 00:02:43,253] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 910256.1295637871 W.
[2019-03-26 00:03:28,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7397.2590 3131357551.8961 1887.0000
[2019-03-26 00:03:28,190] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1172462], dtype=float32), 0.13165241]
[2019-03-26 00:03:28,192] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.1, 80.0, 1.0, 2.0, 0.4596330741669794, 1.0, 1.0, 0.4596330741669794, 1.0, 1.0, 0.7893372847609768, 6.911199999999999, 6.9112, 170.5573041426782, 1927895.648903749, 1927895.648903749, 386887.425166135]
[2019-03-26 00:03:28,193] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:03:28,197] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.035097e-23 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.2996338499916885
[2019-03-26 00:03:28,585] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7092.0575 3208450465.4339 2256.0000
[2019-03-26 00:03:28,800] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7977.1311 3011127288.3516 1363.0000
[2019-03-26 00:03:28,839] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8108.4653 2964379694.9533 1210.0000
[2019-03-26 00:03:29,063] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7339.8977 3340646604.4125 1929.0000
[2019-03-26 00:03:30,078] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 900000, evaluation results [900000.0, 7339.89767483705, 3340646604.412502, 1929.0, 7397.259020269691, 3131357551.896086, 1887.0, 8108.465291927008, 2964379694.953286, 1210.0, 7092.05747395899, 3208450465.4339337, 2256.0, 7977.131076082704, 3011127288.3515973, 1363.0]
[2019-03-26 00:03:35,113] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:03:35,122] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3432
[2019-03-26 00:03:35,129] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7813014066543494, 6.9112, 6.9112, 168.912956510431, 657284.2766999168, 657284.2766999168, 198078.9342248318], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2073600.0000, 
sim time next is 2074200.0000, 
raw observation next is [24.48333333333333, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7807104273353572, 6.911199999999999, 6.9112, 168.912956510431, 656852.309046034, 656852.3090460347, 197961.4176605196], 
processed observation next is [0.0, 0.0, 0.3593996840442337, 0.9416666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7325736918723867, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18245897473500944, 0.18245897473500963, 0.29546480247838747], 
reward next is 0.7045, 
noisyNet noise sample is [array([-0.8391147], dtype=float32), -0.9508871]. 
=============================================
[2019-03-26 00:03:39,420] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:03:39,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0695
[2019-03-26 00:03:39,445] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1688846.293892239 W.
[2019-03-26 00:03:39,452] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.26666666666667, 68.0, 1.0, 2.0, 0.4026857653871062, 1.0, 1.0, 0.4026857653871062, 1.0, 2.0, 0.6993320042598297, 6.9112, 6.9112, 170.5573041426782, 1688846.293892239, 1688846.293892239, 354380.2291994921], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2205600.0000, 
sim time next is 2206200.0000, 
raw observation next is [31.38333333333333, 67.5, 1.0, 2.0, 0.6003214614988621, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.922952381068697, 6.9112, 168.9128345045134, 1678485.789491736, 1670148.251898075, 364723.8380537732], 
processed observation next is [1.0, 0.5217391304347826, 0.6864139020537123, 0.675, 1.0, 1.0, 0.5184595921673038, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0011752381068697026, 0.0, 0.8294393460473927, 0.46624605263659336, 0.4639300699716875, 0.5443639373936914], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.42608172], dtype=float32), 0.336143]. 
=============================================
[2019-03-26 00:03:45,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.4834093e-33 0.0000000e+00 5.2143262e-36 5.5006138e-17], sum to 1.0000
[2019-03-26 00:03:45,825] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3055
[2019-03-26 00:03:45,836] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2294763.226177633 W.
[2019-03-26 00:03:45,841] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.06666666666667, 62.33333333333334, 1.0, 2.0, 0.8205120624731458, 1.0, 2.0, 0.8205120624731458, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2294763.226177633, 2294763.226177633, 429980.1897399617], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2287200.0000, 
sim time next is 2287800.0000, 
raw observation next is [32.05, 62.5, 1.0, 2.0, 0.815902321416819, 1.0, 2.0, 0.815902321416819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2281859.184801035, 2281859.184801035, 427686.9379494168], 
processed observation next is [1.0, 0.4782608695652174, 0.7180094786729857, 0.625, 1.0, 1.0, 0.778195567972071, 1.0, 1.0, 0.778195567972071, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.633849773555843, 0.633849773555843, 0.6383387133573385], 
reward next is 0.3617, 
noisyNet noise sample is [array([-0.8385573], dtype=float32), 0.44342163]. 
=============================================
[2019-03-26 00:03:47,624] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:03:47,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4278
[2019-03-26 00:03:47,645] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 76.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9872952323046522, 6.911200000000001, 6.9112, 168.9129408107538, 795848.8170738516, 795848.8170738509, 243802.4485661858], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2123400.0000, 
sim time next is 2124000.0000, 
raw observation next is [30.0, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9923447817971603, 6.911200000000001, 6.9112, 168.9129085736027, 799564.4256696717, 799564.4256696712, 245068.8312203905], 
processed observation next is [0.0, 0.6086956521739131, 0.6208530805687204, 0.77, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9906643680453173, 8.881784197001253e-17, 0.0, 0.8294397097605302, 0.22210122935268659, 0.22210122935268642, 0.3657743749558067], 
reward next is 0.6342, 
noisyNet noise sample is [array([0.20410322], dtype=float32), 0.5415119]. 
=============================================
[2019-03-26 00:03:47,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[65.999916]
 [66.00252 ]
 [65.99793 ]
 [65.99058 ]
 [65.97433 ]], R is [[66.04505157]
 [66.02072144]
 [65.99807739]
 [65.97716522]
 [65.95779419]].
[2019-03-26 00:03:47,978] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:03:47,987] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9729
[2019-03-26 00:03:47,992] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.7, 73.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.000446176690999, 6.9112, 6.9112, 168.9128653994849, 805183.7448228493, 805183.7448228493, 247094.3349145394], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2133600.0000, 
sim time next is 2134200.0000, 
raw observation next is [30.75, 73.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9925079302936582, 6.9112, 6.9112, 168.912956510431, 798621.1651405103, 798621.1651405103, 245048.3564453954], 
processed observation next is [0.0, 0.6956521739130435, 0.6563981042654029, 0.7316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9908633296264123, 0.0, 0.0, 0.8294399451523027, 0.22183921253903063, 0.22183921253903063, 0.36574381559014235], 
reward next is 0.6343, 
noisyNet noise sample is [array([3.0781043], dtype=float32), -0.2764787]. 
=============================================
[2019-03-26 00:03:49,468] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.600298e-34], sum to 1.0000
[2019-03-26 00:03:49,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6572
[2019-03-26 00:03:49,484] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.43333333333334, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.901238336697591, 6.9112, 6.9112, 168.912956510431, 740708.6681487999, 740708.6681487999, 223590.5225457454], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2154000.0000, 
sim time next is 2154600.0000, 
raw observation next is [26.35, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.898732691776683, 6.9112, 6.9112, 168.912956510431, 739065.9208751115, 739065.9208751115, 223025.7655875788], 
processed observation next is [0.0, 0.9565217391304348, 0.4478672985781992, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8765032826544914, 0.0, 0.0, 0.8294399451523027, 0.20529608913197542, 0.20529608913197542, 0.3328742769963863], 
reward next is 0.6671, 
noisyNet noise sample is [array([-0.31996667], dtype=float32), 1.7991422]. 
=============================================
[2019-03-26 00:03:52,498] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:03:52,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1559
[2019-03-26 00:03:52,519] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.16666666666667, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9808743983256825, 6.9112, 6.9112, 168.912956510431, 789256.7697840806, 789256.7697840806, 242095.3210009401], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2398800.0000, 
sim time next is 2399400.0000, 
raw observation next is [32.05, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9827089060049008, 6.911199999999999, 6.9112, 168.912956510431, 790733.4487027564, 790733.448702757, 242558.6233680475], 
processed observation next is [1.0, 0.782608695652174, 0.7180094786729857, 0.67, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9789133000059765, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21964818019521012, 0.2196481801952103, 0.3620277960717127], 
reward next is 0.6380, 
noisyNet noise sample is [array([-0.58453184], dtype=float32), -0.03457863]. 
=============================================
[2019-03-26 00:04:00,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:04:00,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6900
[2019-03-26 00:04:00,120] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.3, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9996763699730691, 6.911200000000001, 6.9112, 168.9128617168459, 804423.3234212768, 804423.3234212762, 246887.9009486062], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2313000.0000, 
sim time next is 2313600.0000, 
raw observation next is [31.16666666666666, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9874425588070754, 6.9112, 6.9112, 168.912956510431, 794682.3904074217, 794682.3904074217, 243766.0677389991], 
processed observation next is [1.0, 0.782608695652174, 0.6761453396524484, 0.7066666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9846860473257018, 0.0, 0.0, 0.8294399451523027, 0.22074510844650605, 0.22074510844650605, 0.3638299518492524], 
reward next is 0.6362, 
noisyNet noise sample is [array([-0.16382238], dtype=float32), -0.10842049]. 
=============================================
[2019-03-26 00:04:07,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:04:07,325] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8512
[2019-03-26 00:04:07,335] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.98333333333333, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9032877997541884, 6.9112, 6.9112, 168.912956510431, 742891.0211590385, 742891.0211590385, 224087.4115959725], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2581800.0000, 
sim time next is 2582400.0000, 
raw observation next is [26.86666666666667, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8996939143843141, 6.911199999999999, 6.9112, 168.912956510431, 740625.8949625496, 740625.8949625501, 223279.1004767983], 
processed observation next is [1.0, 0.9130434782608695, 0.4723538704581361, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8776755053467246, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20572941526737487, 0.20572941526737504, 0.3332523887713408], 
reward next is 0.6667, 
noisyNet noise sample is [array([0.0620361], dtype=float32), -0.30465487]. 
=============================================
[2019-03-26 00:04:10,225] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.9484308e-29], sum to 1.0000
[2019-03-26 00:04:10,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4846
[2019-03-26 00:04:10,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1851727.415349119 W.
[2019-03-26 00:04:10,252] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.75, 87.5, 1.0, 2.0, 0.6832768278278536, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980639338699459, 6.9112, 168.9125427695614, 1851727.415349119, 1802464.879223677, 381969.4968245532], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2471400.0000, 
sim time next is 2472000.0000, 
raw observation next is [26.86666666666667, 87.0, 1.0, 2.0, 0.468517155377199, 1.0, 1.0, 0.468517155377199, 1.0, 2.0, 0.802301440357695, 6.9112, 6.9112, 170.5573041426782, 1965193.421617712, 1965193.421617712, 392138.304934691], 
processed observation next is [1.0, 0.6086956521739131, 0.4723538704581361, 0.87, 1.0, 1.0, 0.35965922334602296, 1.0, 0.5, 0.35965922334602296, 1.0, 1.0, 0.7589041955581646, 0.0, 0.0, 0.8375144448122397, 0.5458870615604755, 0.5458870615604755, 0.5852810521413299], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1734097], dtype=float32), 0.31750524]. 
=============================================
[2019-03-26 00:04:10,268] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[44.913578]
 [45.270958]
 [44.855923]
 [43.939495]
 [43.880165]], R is [[44.58938599]
 [44.22619247]
 [44.22270203]
 [43.78047562]
 [43.34267044]].
[2019-03-26 00:04:12,677] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9608660e-01 1.3490618e-28 1.9360449e-33 1.8753976e-34 3.9133509e-03], sum to 1.0000
[2019-03-26 00:04:12,693] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0037
[2019-03-26 00:04:12,702] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1903727.045357007 W.
[2019-03-26 00:04:12,709] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.51666666666667, 88.5, 1.0, 2.0, 0.4538761085268153, 1.0, 2.0, 0.4538761085268153, 1.0, 2.0, 0.7765505565164292, 6.911200000000001, 6.9112, 170.5573041426782, 1903727.045357007, 1903727.045357006, 382810.5674892178], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2470200.0000, 
sim time next is 2470800.0000, 
raw observation next is [26.63333333333333, 88.0, 1.0, 2.0, 0.4427119481705192, 1.0, 2.0, 0.4427119481705192, 1.0, 2.0, 0.7576985621640264, 6.9112, 6.9112, 170.5573041426782, 1856859.782657557, 1856859.782657557, 376025.4648987902], 
processed observation next is [1.0, 0.6086956521739131, 0.46129541864139006, 0.88, 1.0, 1.0, 0.32856861225363754, 1.0, 1.0, 0.32856861225363754, 1.0, 1.0, 0.7045104416634469, 0.0, 0.0, 0.8375144448122397, 0.5157943840715435, 0.5157943840715435, 0.5612320371623734], 
reward next is 0.4388, 
noisyNet noise sample is [array([0.26093936], dtype=float32), -0.22949846]. 
=============================================
[2019-03-26 00:04:14,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:04:14,600] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7600
[2019-03-26 00:04:14,609] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.768394686923353, 6.911199999999999, 6.9112, 168.912956510431, 648801.6628119738, 648801.6628119744, 195547.6369018385], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2617200.0000, 
sim time next is 2617800.0000, 
raw observation next is [25.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7736039088787285, 6.911200000000001, 6.9112, 168.912956510431, 652397.9849566373, 652397.9849566367, 196567.8398945526], 
processed observation next is [0.0, 0.30434782608695654, 0.39178515007898923, 0.8816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.723907205949669, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1812216624879548, 0.18122166248795463, 0.29338483566351137], 
reward next is 0.7066, 
noisyNet noise sample is [array([1.1452483], dtype=float32), 0.14117585]. 
=============================================
[2019-03-26 00:04:22,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:04:23,001] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6135
[2019-03-26 00:04:23,004] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.7, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7283161198809123, 6.9112, 6.9112, 168.912956510431, 620108.3355902537, 620108.3355902537, 187901.0536411631], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2613600.0000, 
sim time next is 2614200.0000, 
raw observation next is [23.91666666666666, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7328669072856678, 6.9112, 6.9112, 168.912956510431, 623604.2796600653, 623604.2796600653, 188751.8056164346], 
processed observation next is [0.0, 0.2608695652173913, 0.3325434439178513, 0.9233333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.674227935714229, 0.0, 0.0, 0.8294399451523027, 0.17322341101668481, 0.17322341101668481, 0.2817191128603502], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.33703384], dtype=float32), -0.40187898]. 
=============================================
[2019-03-26 00:04:30,386] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.6504333e-26], sum to 1.0000
[2019-03-26 00:04:30,397] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7394
[2019-03-26 00:04:30,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6854239289085083, 6.911200000000001, 6.9112, 168.912956510431, 587708.875068235, 587708.8750682343, 180145.6035270717], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2745600.0000, 
sim time next is 2746200.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6845291822457872, 6.9112, 6.9112, 168.912956510431, 586941.4729008859, 586941.4729008859, 179988.790809052], 
processed observation next is [0.0, 0.782608695652174, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6152794905436428, 0.0, 0.0, 0.8294399451523027, 0.16303929802802386, 0.16303929802802386, 0.2686399862821672], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.2826902], dtype=float32), 0.99718446]. 
=============================================
[2019-03-26 00:04:30,723] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5720764e-04 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9984276e-01], sum to 1.0000
[2019-03-26 00:04:30,738] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9742
[2019-03-26 00:04:30,746] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.2379909411442128, 1.0, 2.0, 0.2379909411442128, 1.0, 2.0, 0.4171678647835524, 6.9112, 6.9112, 170.5573041426782, 1076949.822929454, 1076949.822929454, 289662.6729083147], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.206855030158567, 1.0, 2.0, 0.206855030158567, 1.0, 2.0, 0.3628761572200368, 6.9112, 6.9112, 170.5573041426782, 937125.1764481722, 937125.1764481722, 279728.6085566351], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.04440365079345419, 1.0, 1.0, 0.04440365079345419, 1.0, 1.0, 0.22301970392687415, 0.0, 0.0, 0.8375144448122397, 0.2603125490133812, 0.2603125490133812, 0.4175053859054255], 
reward next is 0.5825, 
noisyNet noise sample is [array([0.31085002], dtype=float32), 0.45035088]. 
=============================================
[2019-03-26 00:04:30,968] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0416437e-19], sum to 1.0000
[2019-03-26 00:04:30,981] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6062
[2019-03-26 00:04:30,988] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6729223686568182, 6.9112, 6.9112, 168.912956510431, 578844.3630699599, 578844.3630699599, 177970.5488242769], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2724600.0000, 
sim time next is 2725200.0000, 
raw observation next is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.672712892041941, 6.911199999999999, 6.9112, 168.912956510431, 578664.1240266127, 578664.1240266134, 177934.5266765819], 
processed observation next is [0.0, 0.5652173913043478, 0.2417061611374408, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6008693805389523, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16074003445183688, 0.16074003445183707, 0.2655739204128088], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.14485279], dtype=float32), 0.773165]. 
=============================================
[2019-03-26 00:04:37,131] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:04:37,140] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5820
[2019-03-26 00:04:37,146] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8824806501458732, 6.911200000000001, 6.9112, 168.912956510431, 756014.3720377667, 756014.372037766, 219957.3551068832], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2810400.0000, 
sim time next is 2811000.0000, 
raw observation next is [24.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8636982000147424, 6.911200000000001, 6.9112, 168.912956510431, 739374.2680924275, 739374.2680924268, 215709.0768529175], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8337782927009054, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20538174113678542, 0.20538174113678523, 0.3219538460491306], 
reward next is 0.6780, 
noisyNet noise sample is [array([-0.40508345], dtype=float32), 1.8553348]. 
=============================================
[2019-03-26 00:04:37,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.146416]
 [76.35774 ]
 [77.51238 ]
 [77.4088  ]
 [77.544044]], R is [[77.14548492]
 [77.04573822]
 [76.90856934]
 [76.73150635]
 [76.54455566]].
[2019-03-26 00:04:37,686] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 00:04:37,689] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:04:37,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:04:37,693] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:04:37,694] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:04:37,695] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:04:37,696] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:04:37,698] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:04:37,699] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:04:37,698] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:04:37,705] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:04:37,722] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-26 00:04:37,744] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-26 00:04:37,745] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-26 00:04:37,766] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-26 00:04:37,788] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-26 00:04:42,021] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:04:42,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.33333333333333, 50.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5464632283114971, 6.9112, 6.9112, 168.912956510431, 481277.3856618857, 481277.3856618857, 158179.7163353995]
[2019-03-26 00:04:42,024] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:04:42,026] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.40924487200288995
[2019-03-26 00:04:43,484] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:04:43,485] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.9, 57.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5040575331437199, 6.9112, 6.9112, 168.912956510431, 447243.7849133009, 447243.7849133009, 152461.4697710449]
[2019-03-26 00:04:43,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:04:43,489] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3062310801735493
[2019-03-26 00:05:05,026] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:05:05,028] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.3, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7255023634238419, 6.911199999999999, 6.9112, 168.912956510431, 618909.1375643703, 618909.1375643709, 187383.628411906]
[2019-03-26 00:05:05,029] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:05:05,032] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.3964976e-16], sampled 0.2517803769725856
[2019-03-26 00:05:17,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:05:17,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6427077778738816, 6.911199999999999, 6.9112, 168.912956510431, 554261.7831214335, 554261.783121434, 172890.1066025334]
[2019-03-26 00:05:17,739] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:05:17,741] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6777171254809374
[2019-03-26 00:05:32,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:05:32,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.3, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9717852468075204, 6.9112, 6.9112, 168.912956510431, 789263.3664624955, 789263.3664624955, 240211.0711135616]
[2019-03-26 00:05:32,347] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:05:32,354] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4307457e-31], sampled 0.8348556718405536
[2019-03-26 00:06:24,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:06:24,070] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.05, 87.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9050352354208333, 6.911200000000001, 6.9112, 168.912956510431, 743619.6459154993, 743619.6459154987, 224466.3670995483]
[2019-03-26 00:06:24,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:06:24,077] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.247572e-12], sampled 0.06378539955310125
[2019-03-26 00:06:29,700] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2073213], dtype=float32), 0.15213244]
[2019-03-26 00:06:29,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.571877785, 27.41504625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4362946069023203, 6.9112, 6.9112, 168.912956510431, 395188.5866473711, 395188.5866473711, 143945.9045281537]
[2019-03-26 00:06:29,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:06:29,706] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9927354187835618
[2019-03-26 00:06:46,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7792.6009 3217104969.1315 1028.0000
[2019-03-26 00:06:46,822] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8003.5558 3101691077.2244 752.0000
[2019-03-26 00:06:47,033] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7605.9858 3281054046.4028 1062.0000
[2019-03-26 00:06:47,214] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7566.2568 3397414264.8714 1186.0000
[2019-03-26 00:06:47,342] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8141.9054 3062966072.2077 667.0000
[2019-03-26 00:06:48,359] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 925000, evaluation results [925000.0, 7566.256836200985, 3397414264.871432, 1186.0, 7792.600944309174, 3217104969.131521, 1028.0, 8141.905402599338, 3062966072.207707, 667.0, 7605.985795936536, 3281054046.402796, 1062.0, 8003.555809522051, 3101691077.224354, 752.0]
[2019-03-26 00:06:49,542] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:06:49,554] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2817
[2019-03-26 00:06:49,561] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7060698724079599, 6.9112, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955557, 183818.885899404], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2834400.0000, 
sim time next is 2835000.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7063607201167755, 6.9112, 6.9112, 168.912956510431, 603119.3580655647, 603119.3580655647, 183871.5271128181], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6419033172155798, 0.0, 0.0, 0.8294399451523027, 0.1675331550182124, 0.1675331550182124, 0.27443511509375834], 
reward next is 0.7256, 
noisyNet noise sample is [array([-0.53035426], dtype=float32), -0.12203031]. 
=============================================
[2019-03-26 00:06:49,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.751915]
 [72.7095  ]
 [72.53871 ]
 [72.6199  ]
 [72.90021 ]], R is [[72.60780334]
 [72.6073761 ]
 [72.60682678]
 [72.60424805]
 [72.60136414]].
[2019-03-26 00:06:51,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:06:51,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5082
[2019-03-26 00:06:51,578] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.600990096893383, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 166249.7708888511], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [21.66666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6065102661851364, 6.9112, 6.9112, 168.912956510431, 526683.8322111608, 526683.8322111608, 167099.4344992244], 
processed observation next is [1.0, 0.30434782608695654, 0.22590837282780438, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5201344709574833, 0.0, 0.0, 0.8294399451523027, 0.1463010645031002, 0.1463010645031002, 0.2494021410436185], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.86063695], dtype=float32), -0.42301762]. 
=============================================
[2019-03-26 00:07:06,539] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:06,551] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2009
[2019-03-26 00:07:06,560] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7838270871496524, 6.911199999999999, 6.9112, 168.912956510431, 658335.4629072178, 658335.4629072185, 198566.3620750686], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3222600.0000, 
sim time next is 3223200.0000, 
raw observation next is [26.33333333333334, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7865832341281436, 6.911199999999999, 6.9112, 168.912956510431, 660264.1175018304, 660264.1175018311, 199115.2262756518], 
processed observation next is [0.0, 0.30434782608695654, 0.44707740916271754, 0.8233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7397356513757849, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.183406699306064, 0.1834066993060642, 0.29718690488903254], 
reward next is 0.7028, 
noisyNet noise sample is [array([-0.31096298], dtype=float32), -0.2321189]. 
=============================================
[2019-03-26 00:07:08,102] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.299993e-34], sum to 1.0000
[2019-03-26 00:07:08,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1192
[2019-03-26 00:07:08,126] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7339829759278997, 6.911199999999999, 6.9112, 168.912956510431, 623687.5206354492, 623687.5206354499, 188954.4613113691], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3091200.0000, 
sim time next is 3091800.0000, 
raw observation next is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7333257402029856, 6.9112, 6.9112, 168.912956510431, 623128.9560877865, 623128.9560877865, 188830.659181588], 
processed observation next is [1.0, 0.782608695652174, 0.28909952606635075, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6747874880524215, 0.0, 0.0, 0.8294399451523027, 0.1730913766910518, 0.1730913766910518, 0.2818368047486388], 
reward next is 0.7182, 
noisyNet noise sample is [array([-0.05159016], dtype=float32), -0.51301277]. 
=============================================
[2019-03-26 00:07:09,934] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.2572388e-34 2.0636864e-32 2.7093440e-14], sum to 1.0000
[2019-03-26 00:07:09,940] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6663
[2019-03-26 00:07:09,944] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 913720.6200729 W.
[2019-03-26 00:07:09,955] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 73.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.030854464635233, 6.9112, 168.9121333147582, 913720.6200729, 828834.0383677755, 254812.6866453038], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3335400.0000, 
sim time next is 3336000.0000, 
raw observation next is [31.33333333333333, 73.66666666666667, 1.0, 1.0, 0.2043963917829512, 1.0, 1.0, 0.2043963917829512, 1.0, 2.0, 0.3549689376073126, 6.9112, 6.9112, 170.5573041426782, 856897.0679558425, 856897.0679558425, 271073.7900603295], 
processed observation next is [0.0, 0.6086956521739131, 0.6840442338072668, 0.7366666666666667, 1.0, 0.5, 0.0414414358830737, 1.0, 0.5, 0.0414414358830737, 1.0, 1.0, 0.21337675317964946, 0.0, 0.0, 0.8375144448122397, 0.23802696332106735, 0.23802696332106735, 0.4045877463587007], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.37348], dtype=float32), -0.4599146]. 
=============================================
[2019-03-26 00:07:09,992] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.508503]
 [63.163742]
 [63.70713 ]
 [64.30144 ]
 [65.12934 ]], R is [[62.18153763]
 [61.5597229 ]
 [60.94412613]
 [60.33468628]
 [59.7313385 ]].
[2019-03-26 00:07:16,728] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:16,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2413
[2019-03-26 00:07:16,752] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9505966989269147, 6.9112, 6.9112, 168.912956510431, 771322.2190636318, 771322.2190636318, 234924.7777911012], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3243600.0000, 
sim time next is 3244200.0000, 
raw observation next is [32.16666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9618969323447665, 6.9112, 6.9112, 168.912956510431, 780561.5530396695, 780561.5530396695, 237712.2261014185], 
processed observation next is [0.0, 0.5652173913043478, 0.7235387045813588, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9535328443228858, 0.0, 0.0, 0.8294399451523027, 0.21682265362213043, 0.21682265362213043, 0.35479436731555], 
reward next is 0.6452, 
noisyNet noise sample is [array([-1.0349319], dtype=float32), 0.5290586]. 
=============================================
[2019-03-26 00:07:17,620] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:17,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4038
[2019-03-26 00:07:17,637] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.804113175827328, 6.9112, 6.9112, 168.912956510431, 672559.9086517622, 672559.9086517622, 202649.6422009784], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3227400.0000, 
sim time next is 3228000.0000, 
raw observation next is [27.66666666666666, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8054996317899533, 6.9112, 6.9112, 168.912956510431, 673425.7185285833, 673425.7185285833, 202929.7153865673], 
processed observation next is [0.0, 0.34782608695652173, 0.5102685624012636, 0.7566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7628044290121381, 0.0, 0.0, 0.8294399451523027, 0.18706269959127314, 0.18706269959127314, 0.3028801722187571], 
reward next is 0.6971, 
noisyNet noise sample is [array([1.0502539], dtype=float32), 0.19965458]. 
=============================================
[2019-03-26 00:07:17,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.85344 ]
 [64.878494]
 [64.91617 ]
 [64.95016 ]
 [64.9015  ]], R is [[64.88593292]
 [64.93461609]
 [64.98348999]
 [65.03290558]
 [65.08274841]].
[2019-03-26 00:07:18,014] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:18,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1778
[2019-03-26 00:07:18,027] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.914934975625134, 6.9112, 6.9112, 168.912956510431, 747361.497230726, 747361.497230726, 226600.9206582649], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3232800.0000, 
sim time next is 3233400.0000, 
raw observation next is [29.16666666666667, 77.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9235821091146548, 6.9112, 6.9112, 168.912956510431, 752852.1300982146, 752852.1300982146, 228576.2134131197], 
processed observation next is [0.0, 0.43478260869565216, 0.581358609794629, 0.7750000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9068074501398229, 0.0, 0.0, 0.8294399451523027, 0.2091255916939485, 0.2091255916939485, 0.3411585274822682], 
reward next is 0.6588, 
noisyNet noise sample is [array([-0.8820463], dtype=float32), -0.16075495]. 
=============================================
[2019-03-26 00:07:22,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:22,631] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9150
[2019-03-26 00:07:22,637] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7338571928123899, 6.9112, 6.9112, 168.912956510431, 624174.6248391629, 624174.6248391629, 188935.8480736673], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3297600.0000, 
sim time next is 3298200.0000, 
raw observation next is [25.0, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7292354694352138, 6.9112, 6.9112, 168.912956510431, 621057.2932498611, 621057.2932498611, 188074.1912464714], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6697993529697728, 0.0, 0.0, 0.8294399451523027, 0.1725159147916281, 0.1725159147916281, 0.2807077481290618], 
reward next is 0.7193, 
noisyNet noise sample is [array([-0.29197052], dtype=float32), -1.0253651]. 
=============================================
[2019-03-26 00:07:37,252] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9999499e-01 6.0253101e-24 1.0649256e-19 4.9784057e-06 1.1346483e-37], sum to 1.0000
[2019-03-26 00:07:37,263] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1425
[2019-03-26 00:07:37,275] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2680672.449417875 W.
[2019-03-26 00:07:37,283] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.00000000000001, 1.0, 2.0, 0.6388995501816407, 1.0, 2.0, 0.6388995501816407, 1.0, 1.0, 1.03, 7.000640215241808, 6.9112, 170.5573041426782, 2680672.449417875, 2616602.785044824, 502513.9104934893], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3504000.0000, 
sim time next is 3504600.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.7064564204272981, 1.0, 2.0, 0.6738182497279117, 1.0, 2.0, 1.03, 7.00509824124424, 6.9112, 170.5573041426782, 2827348.903655387, 2760085.77391589, 523376.0434870935], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.63, 1.0, 1.0, 0.6463330366593952, 1.0, 1.0, 0.607009939431219, 1.0, 1.0, 1.0365853658536586, 0.009389824124423995, 0.0, 0.8375144448122397, 0.7853746954598297, 0.7666904927544139, 0.7811582738613336], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.82162195], dtype=float32), 0.6938501]. 
=============================================
[2019-03-26 00:07:37,593] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.1720062e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 00:07:37,606] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5193
[2019-03-26 00:07:37,609] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8833599085996746, 6.9112, 6.9112, 168.912956510431, 729304.8407424163, 729304.8407424163, 219604.1821012417], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3621600.0000, 
sim time next is 3622200.0000, 
raw observation next is [28.0, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8801908828328628, 6.9112, 6.9112, 168.912956510431, 726882.0397544084, 726882.0397544084, 218890.5588649314], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.7900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8538913205278815, 0.0, 0.0, 0.8294399451523027, 0.2019116777095579, 0.2019116777095579, 0.3267023266640767], 
reward next is 0.6733, 
noisyNet noise sample is [array([0.73184973], dtype=float32), -0.18303216]. 
=============================================
[2019-03-26 00:07:43,981] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:07:43,995] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2071
[2019-03-26 00:07:44,002] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9274870920716178, 6.9112, 6.9112, 168.912956510431, 756412.8167941896, 756412.8167941896, 229524.7678908043], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3610800.0000, 
sim time next is 3611400.0000, 
raw observation next is [30.66666666666667, 67.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9212023864306117, 6.911199999999999, 6.9112, 168.912956510431, 752875.382412427, 752875.3824124276, 228100.9848114177], 
processed observation next is [1.0, 0.8260869565217391, 0.6524486571879939, 0.6733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9039053493056239, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091320506701186, 0.20913205067011878, 0.34044923106181746], 
reward next is 0.6596, 
noisyNet noise sample is [array([0.47050443], dtype=float32), -0.5073307]. 
=============================================
[2019-03-26 00:07:53,966] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.00000000e+00 0.00000000e+00 1.13515855e-33 1.23130106e-25
 0.00000000e+00], sum to 1.0000
[2019-03-26 00:07:53,977] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9266
[2019-03-26 00:07:53,983] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 79.00000000000001, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8722127611635225, 6.9112, 6.9112, 168.912956510431, 738919.9567902519, 738919.9567902519, 217561.1956228421], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [26.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8253153576869268, 6.9112, 6.9112, 168.912956510431, 699630.4478140464, 699630.4478140464, 207275.6212909755], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7869699483986912, 0.0, 0.0, 0.8294399451523027, 0.19434179105945734, 0.19434179105945734, 0.3093665989417545], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.24510625], dtype=float32), 0.41678256]. 
=============================================
[2019-03-26 00:07:55,940] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 00:07:55,942] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:07:55,943] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:07:55,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:07:55,944] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:07:55,946] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:07:55,946] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:07:55,948] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:07:55,950] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:07:55,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:07:55,953] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:07:55,982] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-26 00:07:55,983] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-26 00:07:56,021] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-26 00:07:56,044] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-26 00:07:56,044] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-26 00:07:58,660] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:07:58,663] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.61666666666667, 91.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7219102234000182, 6.9112, 6.9112, 168.912956510431, 624520.2171965144, 624520.2171965144, 186694.6079686998]
[2019-03-26 00:07:58,664] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:07:58,667] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6922678694479072
[2019-03-26 00:08:07,651] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:08:07,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.8, 55.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5481524979874334, 6.9112, 6.9112, 168.912956510431, 484544.3822641329, 484544.3822641329, 158342.80185509]
[2019-03-26 00:08:07,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:08:07,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.13776546948999813
[2019-03-26 00:08:08,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:08:08,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.01666666666667, 53.5, 1.0, 2.0, 0.2747582734295335, 1.0, 2.0, 0.2747582734295335, 1.0, 2.0, 0.4584365187360571, 6.9112, 6.9112, 169.0403247858759, 1152041.285786148, 1152041.285786148, 292403.3709671004]
[2019-03-26 00:08:08,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:08:08,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999559e-01 1.6136526e-29 1.4780425e-25 9.2743461e-23 4.4312133e-06], sampled 0.07878250049541813
[2019-03-26 00:08:08,709] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1152041.285786148 W.
[2019-03-26 00:08:37,658] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:08:37,660] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.45, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9356411128051166, 6.911199999999999, 6.9112, 168.912956510431, 763636.5397815894, 763636.53978159, 231509.3288126902]
[2019-03-26 00:08:37,660] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:08:37,664] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5285486114390882
[2019-03-26 00:08:43,371] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:08:43,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 71.66666666666667, 1.0, 2.0, 0.7056376626271851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 986152.396603493, 986152.3966034923, 222066.5432905096]
[2019-03-26 00:08:43,372] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:08:43,375] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9998915e-01 1.2990621e-31 2.3697753e-27 3.2221337e-24 1.0821037e-05], sampled 0.013149134782751815
[2019-03-26 00:08:43,376] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 986152.396603493 W.
[2019-03-26 00:09:21,197] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2273417], dtype=float32), 0.13036032]
[2019-03-26 00:09:21,200] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.53013858, 61.83941945, 1.0, 2.0, 0.9027775261757202, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1261825.693721286, 1261825.693721286, 270705.132565902]
[2019-03-26 00:09:21,200] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:09:21,205] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.4690024e-34 1.0058689e-30 6.6730761e-26 2.3139290e-12], sampled 0.5035900422302839
[2019-03-26 00:09:21,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1261825.693721286 W.
[2019-03-26 00:10:05,256] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8160.7405 3006323295.5360 914.0000
[2019-03-26 00:10:05,466] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8041.3996 3050095408.2637 1051.0000
[2019-03-26 00:10:05,622] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7338.3883 3245307322.0834 1643.0000
[2019-03-26 00:10:05,653] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7448.6259 3369072954.2292 1513.0000
[2019-03-26 00:10:05,740] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7568.6203 3174934122.5700 1465.0000
[2019-03-26 00:10:06,753] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 950000, evaluation results [950000.0, 7448.625856429412, 3369072954.229208, 1513.0, 7568.620264250098, 3174934122.570038, 1465.0, 8160.740547308315, 3006323295.535957, 914.0, 7338.3882931940025, 3245307322.083418, 1643.0, 8041.399572560618, 3050095408.2636657, 1051.0]
[2019-03-26 00:10:10,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:10:10,831] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7145
[2019-03-26 00:10:10,840] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9135074792782445, 6.9112, 6.9112, 168.912956510431, 748941.9802847204, 748941.9802847204, 226385.0013341731], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3822000.0000, 
sim time next is 3822600.0000, 
raw observation next is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9132888369505465, 6.911200000000001, 6.9112, 168.912956510431, 748762.6625055044, 748762.6625055039, 226333.5023349304], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8942546792079836, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20798962847375124, 0.20798962847375108, 0.3378111975148215], 
reward next is 0.6622, 
noisyNet noise sample is [array([0.98851043], dtype=float32), 1.7214369]. 
=============================================
[2019-03-26 00:10:10,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.3032218e-01 3.1234206e-36 1.3869513e-28 8.2458671e-28 2.6967782e-01], sum to 1.0000
[2019-03-26 00:10:10,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9148
[2019-03-26 00:10:10,894] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.66666666666667, 1.0, 2.0, 0.1958791390402272, 1.0, 2.0, 0.1958791390402272, 1.0, 2.0, 0.3401772862917247, 6.9112, 6.9112, 170.5573041426782, 821176.2659802352, 821176.2659802352, 268714.8980375532], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3923400.0000, 
sim time next is 3924000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.1944439928846249, 1.0, 2.0, 0.1944439928846249, 1.0, 2.0, 0.3376849120295298, 6.911199999999999, 6.9112, 170.5573041426782, 815157.4738159709, 815157.4738159715, 268327.302985359], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.67, 1.0, 1.0, 0.02945059383689747, 1.0, 1.0, 0.02945059383689747, 1.0, 1.0, 0.19229867320674368, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.22643263161554747, 0.22643263161554764, 0.40048851191844625], 
reward next is 0.5995, 
noisyNet noise sample is [array([-0.40409675], dtype=float32), 0.26494512]. 
=============================================
[2019-03-26 00:10:10,917] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[43.7741 ]
 [43.33516]
 [42.74105]
 [41.98316]
 [42.06811]], R is [[44.56581879]
 [44.71909332]
 [44.87028885]
 [45.01944351]
 [45.16601181]].
[2019-03-26 00:10:11,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6013101e-36 6.8575916e-33], sum to 1.0000
[2019-03-26 00:10:11,501] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7457
[2019-03-26 00:10:11,509] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 895893.4784358781 W.
[2019-03-26 00:10:11,516] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [35.0, 55.66666666666667, 1.0, 1.0, 0.2136943553198432, 1.0, 1.0, 0.2136943553198432, 1.0, 2.0, 0.3711164253873656, 6.9112, 6.9112, 170.5573041426782, 895893.4784358781, 895893.4784358781, 273763.0878027746], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.2973435244303943, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5163873689294494, 6.911199999999999, 6.9112, 168.9129564355799, 831034.513259891, 831034.5132598915, 221112.4583690604], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.15342593304866783, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.41022849869445044, -8.881784197001253e-17, 0.0, 0.8294399447847495, 0.2308429203499697, 0.23084292034996987, 0.33001859458068716], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41898152], dtype=float32), 0.039709833]. 
=============================================
[2019-03-26 00:10:13,048] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1934424e-06 3.3871104e-18 1.7715552e-15 3.3937421e-01 6.6062462e-01], sum to 1.0000
[2019-03-26 00:10:13,056] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1255
[2019-03-26 00:10:13,060] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666667, 65.0, 1.0, 2.0, 0.907588893290841, 1.0, 2.0, 0.907588893290841, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2538542.305985423, 2538542.305985423, 475646.7201706274], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4015200.0000, 
sim time next is 4015800.0000, 
raw observation next is [32.0, 64.5, 1.0, 2.0, 0.9145335259651848, 1.0, 2.0, 0.9145335259651848, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2557986.450577142, 2557986.450577143, 479481.9550893045], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.645, 1.0, 1.0, 0.8970283445363672, 1.0, 1.0, 0.8970283445363672, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7105517918269839, 0.7105517918269842, 0.7156447090885142], 
reward next is 0.2844, 
noisyNet noise sample is [array([0.55092627], dtype=float32), 0.5454071]. 
=============================================
[2019-03-26 00:10:20,464] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.5838734e-38], sum to 1.0000
[2019-03-26 00:10:20,476] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3302
[2019-03-26 00:10:20,481] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.76666666666667, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9255775044966227, 6.911199999999999, 6.9112, 168.912956510431, 756369.7125669847, 756369.7125669853, 229138.7369418912], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4065000.0000, 
sim time next is 4065600.0000, 
raw observation next is [27.73333333333333, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9242285808037457, 6.911200000000001, 6.9112, 168.912956510431, 755363.7698949806, 755363.76989498, 228821.4918569837], 
processed observation next is [1.0, 0.043478260869565216, 0.513428120063191, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9075958302484702, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20982326941527238, 0.2098232694152722, 0.341524614711916], 
reward next is 0.6585, 
noisyNet noise sample is [array([-0.7259923], dtype=float32), 0.8927367]. 
=============================================
[2019-03-26 00:10:20,489] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.5436683e-01 2.4373101e-11 1.1486239e-14 4.4563323e-01 1.8223013e-23], sum to 1.0000
[2019-03-26 00:10:20,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9838
[2019-03-26 00:10:20,506] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3468370.417353291 W.
[2019-03-26 00:10:20,513] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.66666666666667, 65.0, 1.0, 2.0, 1.011571252557167, 1.0, 2.0, 0.8263756657928465, 1.0, 2.0, 1.03, 7.005122312130185, 6.9112, 170.5573041426782, 3468370.417353291, 3401090.04465905, 637828.8712270579], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116000.0000, 
sim time next is 4116600.0000, 
raw observation next is [35.5, 65.5, 1.0, 2.0, 1.004732078063095, 1.0, 2.0, 0.8229560785458099, 1.0, 2.0, 1.03, 7.005121772308616, 6.9112, 170.5573041426782, 3453998.260697901, 3386718.274699809, 634882.3624112287], 
processed observation next is [1.0, 0.6521739130434783, 0.8815165876777251, 0.655, 1.0, 1.0, 1.0057012988711989, 1.0, 1.0, 0.7866940705371204, 1.0, 1.0, 1.0365853658536586, 0.00939217723086161, 0.0, 0.8375144448122397, 0.9594439613049724, 0.9407550763055025, 0.9475856155391473], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8169466], dtype=float32), -0.25592914]. 
=============================================
[2019-03-26 00:10:23,271] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9904019e-01 1.5745861e-14 2.0728650e-24 9.5978798e-04 2.9742905e-14], sum to 1.0000
[2019-03-26 00:10:23,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7483
[2019-03-26 00:10:23,291] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3586070.266514647 W.
[2019-03-26 00:10:23,300] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.5, 65.5, 1.0, 2.0, 1.04, 1.0, 2.0, 0.8543786736515468, 1.0, 2.0, 1.03, 7.058988273842038, 6.9112, 170.5573041426782, 3586070.266514647, 3480203.514318976, 653520.079043628], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116600.0000, 
sim time next is 4117200.0000, 
raw observation next is [35.33333333333334, 66.0, 1.0, 2.0, 0.9948995849580661, 1.0, 2.0, 0.8180398319932954, 1.0, 2.0, 1.03, 7.005120996243751, 6.9112, 170.5573041426782, 3433336.096172187, 3366056.666100926, 630676.9582738188], 
processed observation next is [1.0, 0.6521739130434783, 0.8736176935229073, 0.66, 1.0, 1.0, 0.9938549216362241, 1.0, 1.0, 0.7807708819196331, 1.0, 1.0, 1.0365853658536586, 0.009392099624375127, 0.0, 0.8375144448122397, 0.9537044711589409, 0.9350157405835906, 0.9413088929459983], 
reward next is 0.0000, 
noisyNet noise sample is [array([3.1933362], dtype=float32), -1.5734552]. 
=============================================
[2019-03-26 00:10:25,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1190211e-02 1.0281456e-16 9.9156725e-19 8.3446245e-12 9.7880977e-01], sum to 1.0000
[2019-03-26 00:10:25,102] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7667
[2019-03-26 00:10:25,108] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.5, 54.5, 1.0, 2.0, 0.9293765098143774, 1.0, 2.0, 0.9293765098143774, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2599546.052551259, 2599546.052551259, 487778.9380458018], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4188600.0000, 
sim time next is 4189200.0000, 
raw observation next is [35.66666666666667, 54.0, 1.0, 2.0, 0.6523943607375597, 1.0, 2.0, 0.6467872198830423, 1.0, 1.0, 1.03, 7.005093978795887, 6.9112, 170.5573041426782, 2713803.20355163, 2646543.127177273, 506713.9316322186], 
processed observation next is [1.0, 0.4782608695652174, 0.8894154818325437, 0.54, 1.0, 1.0, 0.5811980249850117, 1.0, 1.0, 0.5744424335940269, 1.0, 0.5, 1.0365853658536586, 0.009389397879588656, 0.0, 0.8375144448122397, 0.7538342232087861, 0.7351508686603536, 0.7562894501973412], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.40326956], dtype=float32), -0.9479528]. 
=============================================
[2019-03-26 00:10:35,772] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000000e+00 1.11036835e-35 8.79364942e-31 2.74529820e-26
 1.00000000e+00], sum to 1.0000
[2019-03-26 00:10:35,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5438
[2019-03-26 00:10:35,791] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.83333333333334, 57.5, 1.0, 2.0, 0.7315179005278883, 1.0, 2.0, 0.6863489897782068, 1.0, 2.0, 1.03, 7.005100217444723, 6.9112, 170.5573041426782, 2879988.640418373, 2812724.095046083, 531471.1961153903], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4276200.0000, 
sim time next is 4276800.0000, 
raw observation next is [37.0, 57.0, 1.0, 2.0, 0.8123808997958626, 1.0, 2.0, 0.7267804894121938, 1.0, 2.0, 1.03, 7.005106594959098, 6.9112, 170.5573041426782, 3049850.360735386, 2982581.24689013, 559176.1032664925], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.57, 1.0, 1.0, 0.7739528913203163, 1.0, 1.0, 0.6708198667616792, 1.0, 1.0, 1.0365853658536586, 0.009390659495909759, 0.0, 0.8375144448122397, 0.8471806557598295, 0.8284947908028139, 0.8345911989052127], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3757598], dtype=float32), 0.36011687]. 
=============================================
[2019-03-26 00:10:53,610] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4491412e-01 7.9203039e-37 1.2848693e-32 5.8928013e-33 8.5508591e-01], sum to 1.0000
[2019-03-26 00:10:53,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7080
[2019-03-26 00:10:53,631] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8345565226929986, 6.9112, 6.9112, 168.912956510431, 696322.1120910884, 696322.1120910884, 209033.3003086341], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4510200.0000, 
sim time next is 4510800.0000, 
raw observation next is [26.0, 89.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.2835162185873397, 6.911199999999999, 6.9112, 170.5573041426782, 703278.9826833922, 703278.9826833928, 260453.4024282988], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 0.5, 0.0, 1.0, 0.5, 0.0, 1.0, 1.0, 0.1262392909601704, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19535527296760896, 0.19535527296760913, 0.38873642153477433], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11838844], dtype=float32), 1.4993621]. 
=============================================
[2019-03-26 00:10:58,787] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7284066e-36 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 00:10:58,796] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8427
[2019-03-26 00:10:58,804] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2846276889460861, 6.911200000000001, 6.9112, 170.5573041426782, 706527.7133132378, 706527.7133132372, 260928.7087837228], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4514400.0000, 
sim time next is 4515000.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2852421951861832, 6.9112, 6.9112, 170.5573041426782, 707657.6597202108, 707657.6597202108, 261104.1925255317], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.12834414047095513, 0.0, 0.0, 0.8375144448122397, 0.196571572144503, 0.196571572144503, 0.389707750038107], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14736359], dtype=float32), 0.5601246]. 
=============================================
[2019-03-26 00:10:58,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[53.666336]
 [53.528973]
 [53.50419 ]
 [53.465088]
 [53.428635]], R is [[53.15023041]
 [52.61872864]
 [52.09254074]
 [51.57161713]
 [51.05590057]].
[2019-03-26 00:11:10,477] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 00:11:10,491] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0785
[2019-03-26 00:11:10,496] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.25, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2596598448949367, 6.9112, 6.9112, 170.5573041426782, 654390.1852913715, 654390.1852913715, 253057.5761277706], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4656600.0000, 
sim time next is 4657200.0000, 
raw observation next is [24.33333333333333, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.261472375610415, 6.9112, 6.9112, 170.5573041426782, 658227.9123813506, 658227.9123813506, 253625.9196588204], 
processed observation next is [1.0, 0.9130434782608695, 0.35229067930489716, 0.94, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0993565556224573, 0.0, 0.0, 0.8375144448122397, 0.1828410867725974, 0.1828410867725974, 0.3785461487445081], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.01192795], dtype=float32), 1.6744736]. 
=============================================
[2019-03-26 00:11:11,554] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 00:11:11,566] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8886
[2019-03-26 00:11:11,571] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666666, 76.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2762206343707336, 6.9112, 6.9112, 170.5573041426782, 688352.4916521535, 688352.4916521535, 258180.2851436146], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4833600.0000, 
sim time next is 4834200.0000, 
raw observation next is [27.58333333333334, 76.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2762934341332059, 6.911199999999999, 6.9112, 170.5573041426782, 688558.101622272, 688558.1016222727, 258210.3099608865], 
processed observation next is [1.0, 0.9565217391304348, 0.506319115323855, 0.765, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.11743101723561694, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19126613933952002, 0.19126613933952022, 0.38538852232968135], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46282002], dtype=float32), 1.8650179]. 
=============================================
[2019-03-26 00:11:14,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0516852e-24 0.0000000e+00 0.0000000e+00 3.7098347e-34 1.0000000e+00], sum to 1.0000
[2019-03-26 00:11:14,194] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4654
[2019-03-26 00:11:14,205] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.4304303830017685, 1.0, 2.0, 0.4304303830017685, 1.0, 2.0, 0.7384710859315055, 6.9112, 6.9112, 170.5573041426782, 1805304.010774747, 1805304.010774747, 368988.7955378062], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4870800.0000, 
sim time next is 4871400.0000, 
raw observation next is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.508875014154325, 1.0, 2.0, 0.508875014154325, 1.0, 2.0, 0.8746221902062183, 6.9112, 6.9112, 170.5573041426782, 2134642.964109144, 2134642.964109144, 419650.1276681945], 
processed observation next is [1.0, 0.391304347826087, 0.581358609794629, 0.7333333333333334, 1.0, 1.0, 0.4082831495835241, 1.0, 1.0, 0.4082831495835241, 1.0, 1.0, 0.8471002319588029, 0.0, 0.0, 0.8375144448122397, 0.5929563789192067, 0.5929563789192067, 0.6263434741316335], 
reward next is 0.3737, 
noisyNet noise sample is [array([-0.18196286], dtype=float32), -0.05365467]. 
=============================================
[2019-03-26 00:11:14,407] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 00:11:14,409] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:11:14,410] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:11:14,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:11:14,414] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:11:14,412] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:11:14,414] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:11:14,417] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:11:14,420] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:11:14,423] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:11:14,420] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:11:14,445] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-26 00:11:14,445] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-26 00:11:14,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-26 00:11:14,466] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-26 00:11:14,549] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-26 00:11:19,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:11:19,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4401217716328512, 6.911199999999999, 6.9112, 168.912956510431, 395154.720043473, 395154.7200434736, 144693.2523414076]
[2019-03-26 00:11:19,435] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:11:19,440] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.070992e-16], sampled 0.2038579714060249
[2019-03-26 00:11:54,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:11:54,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.51666666666667, 78.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6233683606021514, 6.9112, 6.9112, 168.912956510431, 543431.4005276918, 543431.4005276918, 169695.2695211221]
[2019-03-26 00:11:54,082] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:11:54,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0217365e-29], sampled 0.1739742795461786
[2019-03-26 00:11:55,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:11:55,994] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.37326028, 95.6987905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6524739545390451, 6.911200000000001, 6.9112, 168.912956510431, 564069.6121931851, 564069.6121931844, 174490.7741188615]
[2019-03-26 00:11:55,996] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:11:56,000] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.04184615e-27], sampled 0.16032776225365386
[2019-03-26 00:12:19,956] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:12:19,958] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.66666666666667, 73.66666666666667, 1.0, 2.0, 0.204723805400754, 1.0, 2.0, 0.204723805400754, 1.0, 2.0, 0.3555375467840981, 6.9112, 6.9112, 170.5573041426782, 858270.2430417028, 858270.2430417028, 271166.6712166865]
[2019-03-26 00:12:19,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:12:19,964] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0209536e-05 0.0000000e+00 2.6701416e-36 9.6863820e-36 9.9997973e-01], sampled 0.8716735124945533
[2019-03-26 00:12:22,412] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:12:22,415] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.16666666666667, 67.0, 1.0, 2.0, 0.782117257991597, 1.0, 2.0, 0.7116486685100611, 1.0, 2.0, 1.03, 7.005104207918953, 6.9112, 170.5573041426782, 2986275.511489806, 2919008.107578503, 548521.6213015821]
[2019-03-26 00:12:22,416] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:12:22,421] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9737790e-18 0.0000000e+00 1.8692748e-35 1.1075690e-33 1.0000000e+00], sampled 0.022247351251320602
[2019-03-26 00:12:36,047] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:12:36,048] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.49914775, 78.83106753, 1.0, 2.0, 0.2284115082201451, 1.0, 2.0, 0.2284115082201451, 1.0, 2.0, 0.3859448745725604, 6.9112, 6.9112, 184.5923449428631, 957588.7979360543, 957588.7979360543, 281594.9560600473]
[2019-03-26 00:12:36,049] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:12:36,052] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6928010e-07 0.0000000e+00 4.8954167e-36 1.3909737e-33 9.9999988e-01], sampled 0.009286578342526886
[2019-03-26 00:12:39,906] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:12:39,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 72.0, 1.0, 2.0, 0.6656188553775625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564808582, 930200.2726392888, 930200.2726392888, 213604.9580819835]
[2019-03-26 00:12:39,907] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:12:39,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6906543e-02 0.0000000e+00 1.9329839e-32 1.7245125e-32 9.8309350e-01], sampled 0.45384352519546556
[2019-03-26 00:12:58,058] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:12:58,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.25, 63.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.012010145713628, 6.9112, 6.9112, 168.9128005979578, 817986.5503033794, 817986.5503033794, 250291.6626077569]
[2019-03-26 00:12:58,061] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:12:58,063] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999738e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6285759e-06], sampled 0.1638358319805333
[2019-03-26 00:13:20,218] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2103877], dtype=float32), 0.11005001]
[2019-03-26 00:13:20,219] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.76666666666667, 63.33333333333334, 1.0, 2.0, 0.4115843975146581, 1.0, 2.0, 0.4115843975146581, 1.0, 2.0, 0.6952868247276794, 6.9112, 6.9112, 170.5573041426782, 1726196.82235979, 1726196.82235979, 356514.9107455159]
[2019-03-26 00:13:20,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:13:20,222] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2608013e-21 0.0000000e+00 0.0000000e+00 2.3003931e-38 1.0000000e+00], sampled 0.7941690035061629
[2019-03-26 00:13:23,532] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7812.4862 3330057461.3796 426.0000
[2019-03-26 00:13:23,958] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7776.8377 3169231229.3978 435.0000
[2019-03-26 00:13:24,184] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7952.1481 3274264924.4041 529.0000
[2019-03-26 00:13:24,261] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7867.1454 3145076941.3726 409.0000
[2019-03-26 00:13:24,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7569.1334 3446559084.4635 829.0000
[2019-03-26 00:13:25,338] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 975000, evaluation results [975000.0, 7569.13336443292, 3446559084.4635196, 829.0, 7952.148089497994, 3274264924.4041443, 529.0, 7867.145373483739, 3145076941.3726387, 409.0, 7812.4861547749715, 3330057461.3796206, 426.0, 7776.837654835181, 3169231229.3978257, 435.0]
[2019-03-26 00:13:25,876] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9898547e-01 0.0000000e+00 2.0267108e-35 6.9675153e-27 1.0145415e-03], sum to 1.0000
[2019-03-26 00:13:25,889] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9235
[2019-03-26 00:13:25,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2068089.344142333 W.
[2019-03-26 00:13:25,904] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.66666666666666, 1.0, 2.0, 0.7395370135337462, 1.0, 2.0, 0.7395370135337462, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2068089.344142333, 2068089.344142333, 391509.2304039471], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4708200.0000, 
sim time next is 4708800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4926468063510611, 1.0, 2.0, 0.4926468063510611, 1.0, 1.0, 0.8527498680793176, 6.9112, 6.9112, 170.5573041426782, 2066502.767770354, 2066502.767770354, 409599.0646774529], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3887310919892303, 1.0, 1.0, 0.3887310919892303, 1.0, 0.5, 0.8204266683894116, 0.0, 0.0, 0.8375144448122397, 0.574028546602876, 0.574028546602876, 0.6113418875782879], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7786656], dtype=float32), 0.93560547]. 
=============================================
[2019-03-26 00:13:30,797] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 8.4034623e-36 2.4562020e-31 9.7538036e-21], sum to 1.0000
[2019-03-26 00:13:30,803] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0064
[2019-03-26 00:13:30,814] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2130163.212273538 W.
[2019-03-26 00:13:30,822] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 66.0, 1.0, 2.0, 0.7617122282891788, 1.0, 2.0, 0.7617122282891788, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2130163.212273538, 2130163.212273538, 401657.3416704098], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4897800.0000, 
sim time next is 4898400.0000, 
raw observation next is [30.33333333333334, 66.0, 1.0, 2.0, 0.7584628396695597, 1.0, 2.0, 0.7584628396695597, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2121067.15116671, 2121067.15116671, 400150.6083146549], 
processed observation next is [1.0, 0.6956521739130435, 0.6366508688783573, 0.66, 1.0, 1.0, 0.7089913730958551, 1.0, 1.0, 0.7089913730958551, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5891853197685306, 0.5891853197685306, 0.59723971390247], 
reward next is 0.4028, 
noisyNet noise sample is [array([-2.3075323], dtype=float32), -1.1177529]. 
=============================================
[2019-03-26 00:13:31,223] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:13:31,231] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8795
[2019-03-26 00:13:31,238] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666666, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.010297850038129, 6.911200000000001, 6.9112, 168.9128543937095, 834926.2567412441, 834926.2567412435, 250727.6492677016], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4776000.0000, 
sim time next is 4776600.0000, 
raw observation next is [27.83333333333334, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.007444782281672, 6.9112, 168.9122999750583, 916018.4027994954, 847739.3071491524, 255796.70704765], 
processed observation next is [1.0, 0.2608695652173913, 0.5181674565560824, 0.8316666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009624478228167189, 0.0, 0.8294367212630595, 0.25444955633319316, 0.23548314087476455, 0.38178612992186567], 
reward next is 0.1370, 
noisyNet noise sample is [array([0.75726616], dtype=float32), -0.49218535]. 
=============================================
[2019-03-26 00:13:34,746] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:13:34,756] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3193
[2019-03-26 00:13:34,766] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8088457053256681, 6.911199999999999, 6.9112, 168.912956510431, 677210.6389611713, 677210.6389611718, 203649.0893878806], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4845000.0000, 
sim time next is 4845600.0000, 
raw observation next is [27.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8091339048039571, 6.9112, 6.9112, 168.912956510431, 677452.0127284338, 677452.0127284338, 203709.1472435989], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7672364692731183, 0.0, 0.0, 0.8294399451523027, 0.18818111464678716, 0.18818111464678716, 0.3040435033486551], 
reward next is 0.6960, 
noisyNet noise sample is [array([0.5656827], dtype=float32), -0.27494624]. 
=============================================
[2019-03-26 00:13:37,842] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 8.5582466e-32 1.7964941e-25 1.6191311e-19 1.0029952e-15], sum to 1.0000
[2019-03-26 00:13:37,854] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0277
[2019-03-26 00:13:37,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2191222.361323108 W.
[2019-03-26 00:13:37,880] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.9258517130757591, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98349333874637, 6.9112, 168.9125260120039, 2191222.361323108, 2139935.109384847, 442136.6001373258], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4982400.0000, 
sim time next is 4983000.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.063463305650886, 6.9112, 168.9121075819239, 2409595.711952699, 2301575.425320195, 476588.8960686346], 
processed observation next is [1.0, 0.6956521739130435, 0.6682464454976303, 0.63, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.015226330565088642, 0.0, 0.8294357765246916, 0.669332142209083, 0.6393265070333876, 0.7113267105502009], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5388911], dtype=float32), 0.5552881]. 
=============================================
[2019-03-26 00:13:37,892] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[39.636448]
 [39.440712]
 [39.004673]
 [39.55859 ]
 [39.388363]], R is [[39.04181671]
 [38.65139771]
 [38.63664246]
 [38.25027466]
 [37.86777115]].
[2019-03-26 00:13:40,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:13:40,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4659
[2019-03-26 00:13:40,861] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666666, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8666421691131913, 6.911199999999999, 6.9112, 168.912956510431, 716029.1678607106, 716029.1678607112, 215851.7565001187], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5049600.0000, 
sim time next is 5050200.0000, 
raw observation next is [30.83333333333334, 63.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8707271189811538, 6.9112, 6.9112, 168.912956510431, 718741.8941740311, 718741.8941740311, 216742.6666820238], 
processed observation next is [0.0, 0.43478260869565216, 0.6603475513428123, 0.635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8423501450989681, 0.0, 0.0, 0.8294399451523027, 0.19965052615945308, 0.19965052615945308, 0.3234965174358564], 
reward next is 0.6765, 
noisyNet noise sample is [array([1.1164781], dtype=float32), -0.2393396]. 
=============================================
[2019-03-26 00:13:52,584] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8779305e-38 4.5411465e-08], sum to 1.0000
[2019-03-26 00:13:52,595] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1034
[2019-03-26 00:13:52,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1555482.824035353 W.
[2019-03-26 00:13:52,614] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.370909856587659, 1.0, 2.0, 0.370909856587659, 1.0, 1.0, 0.6297537578476082, 6.9112, 6.9112, 170.5573041426782, 1555482.824035353, 1555482.824035353, 335501.5434446464], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5194200.0000, 
sim time next is 5194800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4816482826118551, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8122855333464624, 6.911199999999999, 6.9112, 168.912956510431, 1346467.702785003, 1346467.702785004, 294109.2848051875], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3754798585685001, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.7710799187151979, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3740188063291675, 0.3740188063291678, 0.4389690817987873], 
reward next is 0.5610, 
noisyNet noise sample is [array([-1.958114], dtype=float32), 0.041625064]. 
=============================================
[2019-03-26 00:13:58,001] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:13:58,011] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5542
[2019-03-26 00:13:58,018] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.947455769418981, 6.9112, 6.9112, 168.912956510431, 769982.4822518156, 769982.4822518156, 234218.2080132016], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5151000.0000, 
sim time next is 5151600.0000, 
raw observation next is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.947176330864754, 6.9112, 6.9112, 168.912956510431, 769755.305596947, 769755.305596947, 234150.0055582089], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9355808912984803, 0.0, 0.0, 0.8294399451523027, 0.21382091822137417, 0.21382091822137417, 0.3494776202361327], 
reward next is 0.6505, 
noisyNet noise sample is [array([0.03242355], dtype=float32), 1.1224014]. 
=============================================
[2019-03-26 00:13:59,099] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 5.410741e-28 2.924571e-28 8.871539e-18 4.177161e-12], sum to 1.0000
[2019-03-26 00:13:59,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8928
[2019-03-26 00:13:59,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1030117.791301852 W.
[2019-03-26 00:13:59,120] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.3685425654457969, 1.0, 1.0, 0.3685425654457969, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1030117.791301852, 1030117.791301852, 264487.0988528708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5202000.0000, 
sim time next is 5202600.0000, 
raw observation next is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8342135760457341, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1165940.213533462, 1165940.213533462, 252479.9230917807], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.8816666666666667, 1.0, 1.0, 0.800257320537029, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32387228153707276, 0.32387228153707276, 0.3768357061071354], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7761705], dtype=float32), 1.1703571]. 
=============================================
[2019-03-26 00:14:01,790] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.4294218e-21 6.0468863e-26 2.2517548e-16 2.6680543e-16], sum to 1.0000
[2019-03-26 00:14:01,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4493
[2019-03-26 00:14:01,808] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2459175.650988701 W.
[2019-03-26 00:14:01,818] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8792413330817133, 1.0, 2.0, 0.8792413330817133, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2459175.650988701, 2459175.650988701, 460296.2613120817], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5235000.0000, 
sim time next is 5235600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.804482279736665, 1.0, 2.0, 0.804482279736665, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2249891.652777005, 2249891.652777005, 422064.6734120482], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.7644364816104398, 1.0, 1.0, 0.7644364816104398, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.624969903549168, 0.624969903549168, 0.6299472737493257], 
reward next is 0.3701, 
noisyNet noise sample is [array([1.0273619], dtype=float32), -1.6407831]. 
=============================================
[2019-03-26 00:14:08,642] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:14:08,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0115
[2019-03-26 00:14:08,654] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 73.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9424216082697815, 6.9112, 6.9112, 168.912956510428, 758304.8373919434, 758304.8373919434, 232591.4981199537], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5419800.0000, 
sim time next is 5420400.0000, 
raw observation next is [30.9, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9596347228907655, 6.9112, 6.9112, 168.912956510431, 772160.1355788165, 772160.1355788165, 236797.076868985], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7366666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9507740523058114, 0.0, 0.0, 0.8294399451523027, 0.21448892654967125, 0.21448892654967125, 0.3534284729387836], 
reward next is 0.6466, 
noisyNet noise sample is [array([-0.6652567], dtype=float32), 0.3483399]. 
=============================================
[2019-03-26 00:14:11,984] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.4824281e-07 1.4335753e-22 5.0596611e-21 8.8000526e-23 9.9999905e-01], sum to 1.0000
[2019-03-26 00:14:11,996] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0593
[2019-03-26 00:14:12,004] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.9, 92.0, 1.0, 2.0, 0.3367250848369594, 1.0, 2.0, 0.3367250848369594, 1.0, 2.0, 0.584780115674613, 6.9112, 6.9112, 170.5573041426782, 1412027.885059462, 1412027.885059462, 320628.7426291073], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5454000.0000, 
sim time next is 5454600.0000, 
raw observation next is [27.86666666666667, 92.0, 1.0, 2.0, 0.343942217862629, 1.0, 2.0, 0.343942217862629, 1.0, 2.0, 0.5973138889978378, 6.9112, 6.9112, 170.5573041426782, 1442312.677655382, 1442312.677655382, 324028.6377647512], 
processed observation next is [1.0, 0.13043478260869565, 0.519747235387046, 0.92, 1.0, 1.0, 0.20956893718389039, 1.0, 1.0, 0.20956893718389039, 1.0, 1.0, 0.5089193768266314, 0.0, 0.0, 0.8375144448122397, 0.40064241045982835, 0.40064241045982835, 0.4836248324847033], 
reward next is 0.5164, 
noisyNet noise sample is [array([0.5364892], dtype=float32), 0.9675413]. 
=============================================
[2019-03-26 00:14:30,062] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:14:30,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2623
[2019-03-26 00:14:30,083] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.55, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9319938943908277, 6.911199999999999, 6.9112, 168.912956510431, 758671.5109867644, 758671.5109867651, 230537.3160835402], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5596200.0000, 
sim time next is 5596800.0000, 
raw observation next is [30.26666666666667, 71.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9404722690066846, 6.911199999999999, 6.9112, 168.912956510431, 765308.3097198021, 765308.3097198027, 232569.3986952193], 
processed observation next is [1.0, 0.782608695652174, 0.6334913112164299, 0.7166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9274052061057128, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21258564158883392, 0.21258564158883408, 0.3471185055152527], 
reward next is 0.6529, 
noisyNet noise sample is [array([0.21779305], dtype=float32), -0.9936472]. 
=============================================
[2019-03-26 00:14:32,961] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 00:14:32,964] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:14:32,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:14:32,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:14:32,967] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:14:32,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:14:32,969] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:14:32,968] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:14:32,975] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:14:32,975] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:14:32,973] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:14:33,639] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-26 00:14:33,785] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-26 00:14:33,865] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-26 00:14:33,865] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-26 00:14:33,912] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-26 00:14:44,026] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:14:44,030] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.53333333333333, 65.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4564368363331131, 6.911200000000001, 6.9112, 168.912956510431, 407927.7554038041, 407927.7554038034, 146621.3693795771]
[2019-03-26 00:14:44,031] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:14:44,034] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7959369954808092
[2019-03-26 00:15:12,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:15:12,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.93333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6777862900670424, 6.9112, 6.9112, 168.912956510431, 583679.4679384802, 583679.4679384802, 178807.4779091147]
[2019-03-26 00:15:12,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:15:12,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4812013425012486
[2019-03-26 00:15:21,765] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:15:21,765] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.75, 74.5, 1.0, 2.0, 0.7961723009167424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1112743.892992118, 1112743.892992119, 242974.7808951232]
[2019-03-26 00:15:21,768] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:15:21,770] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0937131e-05 2.0606682e-38 3.0924446e-34 5.5636134e-38 9.9998903e-01], sampled 0.3391895758318376
[2019-03-26 00:15:29,488] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:15:29,488] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.74788479, 92.12078424, 1.0, 2.0, 0.7404739295238604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104243, 1034860.974078096, 1034860.974078095, 229832.8311102102]
[2019-03-26 00:15:29,489] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:15:29,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0070570e-06 0.0000000e+00 2.8687937e-38 0.0000000e+00 9.9999404e-01], sampled 0.4959942100255169
[2019-03-26 00:15:32,894] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:15:32,895] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.42190751, 61.02453535333333, 1.0, 2.0, 0.5840450779945792, 1.0, 2.0, 0.5840450779945792, 1.0, 2.0, 1.014293153817837, 6.9112, 6.9112, 171.5212843490159, 2450276.787176732, 2450276.787176732, 478336.3116513547]
[2019-03-26 00:15:32,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:15:32,899] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8441283e-02 7.1854875e-38 1.5385165e-31 1.8515770e-38 9.1155869e-01], sampled 0.3975151931557981
[2019-03-26 00:16:24,255] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:16:24,257] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.76666666666667, 56.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9195356397497542, 6.911200000000001, 6.9112, 168.912956510431, 753056.097497899, 753056.0974978984, 227773.9145783299]
[2019-03-26 00:16:24,261] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:16:24,264] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9928750045304526
[2019-03-26 00:16:27,005] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:16:27,007] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.828569055, 75.87276541333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7677106714606564, 6.911200000000001, 6.9112, 168.912956510431, 649518.3380131257, 649518.3380131251, 195431.5633651323]
[2019-03-26 00:16:27,009] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:16:27,011] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4048034209836432
[2019-03-26 00:16:30,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:16:30,445] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.66800243, 96.35801693, 1.0, 2.0, 0.8677436970270543, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1212830.417005593, 1212830.417005593, 261223.4691323656]
[2019-03-26 00:16:30,445] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:16:30,448] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9690944e-01 5.5388548e-35 1.6861483e-32 4.9371534e-37 3.0905823e-03], sampled 0.2606763272981878
[2019-03-26 00:16:30,449] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1212830.417005593 W.
[2019-03-26 00:16:40,210] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2315103], dtype=float32), 0.11646499]
[2019-03-26 00:16:40,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.90172714, 74.52478628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9573881021752434, 6.9112, 6.9112, 168.912956510431, 778388.7300393892, 778388.7300393892, 236672.712197464]
[2019-03-26 00:16:40,216] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:16:40,219] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7250080389529718
[2019-03-26 00:16:41,985] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7523.4944 3376846373.1354 1314.0000
[2019-03-26 00:16:42,456] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7483.2599 3256785658.1456 1261.0000
[2019-03-26 00:16:42,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7662.1439 3196510912.2641 1231.0000
[2019-03-26 00:16:42,671] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8094.6821 3058603257.7623 852.0000
[2019-03-26 00:16:42,841] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8193.9506 3021636210.7957 789.0000
[2019-03-26 00:16:43,857] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1000000, evaluation results [1000000.0, 7523.494415447447, 3376846373.135429, 1314.0, 7662.143909220506, 3196510912.2640686, 1231.0, 8193.950604716147, 3021636210.795663, 789.0, 7483.2599303083725, 3256785658.145628, 1261.0, 8094.682088228312, 3058603257.7622604, 852.0]
[2019-03-26 00:16:51,382] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 1.7625427e-37 4.6975893e-32 0.0000000e+00 1.8424687e-16], sum to 1.0000
[2019-03-26 00:16:51,395] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1206
[2019-03-26 00:16:51,405] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1186817.015127982 W.
[2019-03-26 00:16:51,411] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 0.2830489530034818, 1.0, 2.0, 0.2830489530034818, 1.0, 2.0, 0.4915624256479104, 6.9112, 6.9112, 170.5573041426782, 1186817.015127982, 1186817.015127982, 297603.3657603684], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5850600.0000, 
sim time next is 5851200.0000, 
raw observation next is [31.6, 67.0, 1.0, 2.0, 0.2628817092861307, 1.0, 2.0, 0.2628817092861307, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 734682.690642735, 734682.6906427356, 243116.8063897048], 
processed observation next is [1.0, 0.7391304347826086, 0.6966824644549764, 0.67, 1.0, 1.0, 0.11190567383871167, 1.0, 1.0, 0.11190567383871167, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2040785251785375, 0.20407852517853767, 0.36286090505926094], 
reward next is 0.6371, 
noisyNet noise sample is [array([1.2612207], dtype=float32), 0.3794961]. 
=============================================
[2019-03-26 00:16:58,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:16:58,704] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3891
[2019-03-26 00:16:58,708] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.93333333333334, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9657205900902444, 6.9112, 6.9112, 168.912956510431, 783788.9707603672, 783788.9707603672, 238668.254924361], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5952000.0000, 
sim time next is 5952600.0000, 
raw observation next is [27.85, 87.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9630867941214487, 6.911199999999999, 6.9112, 168.912956510431, 781758.525956424, 781758.5259564245, 238019.1559379256], 
processed observation next is [1.0, 0.9130434782608695, 0.5189573459715641, 0.875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9549838952700594, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21715514609900666, 0.21715514609900682, 0.3552524715491427], 
reward next is 0.6447, 
noisyNet noise sample is [array([-2.4323032], dtype=float32), -1.2235451]. 
=============================================
[2019-03-26 00:17:03,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 1.4444794e-36 6.4561061e-34 1.8652674e-32 1.3872730e-21], sum to 1.0000
[2019-03-26 00:17:03,454] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2465
[2019-03-26 00:17:03,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1639398.221689027 W.
[2019-03-26 00:17:03,482] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 72.83333333333334, 1.0, 2.0, 0.3909044588701929, 1.0, 2.0, 0.3909044588701929, 1.0, 1.0, 0.6609348726227948, 6.9112, 6.9112, 170.5573041426782, 1639398.221689027, 1639398.221689027, 345419.9875901915], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6007800.0000, 
sim time next is 6008400.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.549414087163706, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9144246611138005, 6.9112, 6.9112, 168.912956510431, 1536586.15013362, 1536586.15013362, 327983.9573930857], 
processed observation next is [1.0, 0.5652173913043478, 0.4786729857819906, 0.74, 1.0, 1.0, 0.45712540622133246, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.895639830626586, 0.0, 0.0, 0.8294399451523027, 0.4268294861482278, 0.4268294861482278, 0.48952829461654584], 
reward next is 0.5105, 
noisyNet noise sample is [array([0.31367537], dtype=float32), -0.54247236]. 
=============================================
[2019-03-26 00:17:24,356] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8673976e-36], sum to 1.0000
[2019-03-26 00:17:24,367] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6167
[2019-03-26 00:17:24,374] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666666, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8850593359303488, 6.911200000000001, 6.9112, 168.912956510431, 728926.1772644205, 728926.1772644198, 219924.985666754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6221400.0000, 
sim time next is 6222000.0000, 
raw observation next is [26.63333333333333, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8844199424577299, 6.911199999999999, 6.9112, 168.912956510431, 728476.0523341013, 728476.052334102, 219782.0802236311], 
processed observation next is [0.0, 0.0, 0.46129541864139006, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8590487103143047, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2023544589816948, 0.202354458981695, 0.3280329555576584], 
reward next is 0.6720, 
noisyNet noise sample is [array([0.94276494], dtype=float32), -0.90428144]. 
=============================================
[2019-03-26 00:17:24,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[54.24747 ]
 [53.115677]
 [52.641098]
 [52.666473]
 [52.69318 ]], R is [[55.55657959]
 [55.67276764]
 [55.78768539]
 [55.90154266]
 [56.01428604]].
[2019-03-26 00:17:30,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4992275e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 00:17:30,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0494
[2019-03-26 00:17:30,933] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1035981.199075062 W.
[2019-03-26 00:17:30,942] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 85.66666666666667, 1.0, 2.0, 0.7412750931680266, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1035981.199075062, 1035981.199075063, 230005.1791978356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6414000.0000, 
sim time next is 6414600.0000, 
raw observation next is [26.93333333333333, 85.33333333333334, 1.0, 2.0, 0.7363159338064797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1029047.086283136, 1029047.086283136, 228877.8231737778], 
processed observation next is [1.0, 0.21739130434782608, 0.4755134281200631, 0.8533333333333334, 1.0, 1.0, 0.6823083539837105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2858464128564267, 0.2858464128564267, 0.341608691304146], 
reward next is 0.6584, 
noisyNet noise sample is [array([-0.4774168], dtype=float32), -0.24884643]. 
=============================================
[2019-03-26 00:17:33,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:17:33,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4579
[2019-03-26 00:17:33,539] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.1, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8568390827273908, 6.9112, 6.9112, 168.912956510431, 709863.3065473617, 709863.3065473617, 213740.9979476792], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6459600.0000, 
sim time next is 6460200.0000, 
raw observation next is [29.0, 71.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8541980288195893, 6.911200000000001, 6.9112, 168.912956510431, 707797.9601494962, 707797.9601494956, 213162.6679509676], 
processed observation next is [1.0, 0.782608695652174, 0.5734597156398105, 0.715, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8221927180726697, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19661054448597118, 0.196610544485971, 0.3181532357477128], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.00093504], dtype=float32), 0.4332412]. 
=============================================
[2019-03-26 00:17:38,190] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 8.9864108e-29 9.1870729e-28 1.1048621e-29 9.6357505e-11], sum to 1.0000
[2019-03-26 00:17:38,198] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6594
[2019-03-26 00:17:38,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1030719.565679432 W.
[2019-03-26 00:17:38,217] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.76666666666667, 81.0, 1.0, 2.0, 0.2458385048506482, 1.0, 2.0, 0.2458385048506482, 1.0, 1.0, 0.4193684944326157, 6.9112, 6.9112, 170.5573041426782, 1030719.565679432, 1030719.565679432, 283416.6711141788], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6421200.0000, 
sim time next is 6421800.0000, 
raw observation next is [27.83333333333333, 80.5, 1.0, 2.0, 0.3636625467668358, 1.0, 2.0, 0.3636625467668358, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1016471.12688288, 1016471.12688288, 263358.9695569684], 
processed observation next is [1.0, 0.30434782608695654, 0.518167456556082, 0.805, 1.0, 1.0, 0.23332836959859735, 1.0, 1.0, 0.23332836959859735, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2823530908008, 0.2823530908008, 0.3930730888909976], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7570668], dtype=float32), -0.39716122]. 
=============================================
[2019-03-26 00:17:51,592] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 00:17:51,595] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:17:51,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:17:51,598] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:17:51,599] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:17:51,599] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:17:51,601] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:17:51,603] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:17:51,603] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:17:51,604] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:17:51,606] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:17:51,628] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-26 00:17:51,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-26 00:17:51,676] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-26 00:17:51,678] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-26 00:17:51,721] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-26 00:18:23,215] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.3572931], dtype=float32), 0.12588143]
[2019-03-26 00:18:23,216] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.91666666666666, 92.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7120092238897219, 6.911200000000001, 6.9112, 168.912956510431, 612158.36694274, 612158.3669427395, 184900.9022042583]
[2019-03-26 00:18:23,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:18:23,221] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6073952614977414
[2019-03-26 00:18:48,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.3572931], dtype=float32), 0.12588143]
[2019-03-26 00:18:48,499] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.91397852333333, 85.99552603333333, 1.0, 2.0, 0.4643830921024848, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8010253740431189, 6.911200000000001, 6.9112, 168.912956510431, 1298172.638935106, 1298172.638935106, 288677.580721093]
[2019-03-26 00:18:48,500] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:18:48,502] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 6.8329499e-38 0.0000000e+00 5.4526646e-18], sampled 0.10809855316510408
[2019-03-26 00:18:48,503] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1298172.638935106 W.
[2019-03-26 00:19:25,171] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.3572931], dtype=float32), 0.12588143]
[2019-03-26 00:19:25,172] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8625735648361792, 6.911199999999999, 6.9112, 168.912956510431, 716415.5864571698, 716415.5864571705, 215070.276260585]
[2019-03-26 00:19:25,173] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:19:25,177] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5440974413466575
[2019-03-26 00:19:32,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.3572931], dtype=float32), 0.12588143]
[2019-03-26 00:19:32,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.16666666666667, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8988254074793511, 6.9112, 6.9112, 168.912956510431, 740554.3939961429, 740554.3939961429, 223102.7254158378]
[2019-03-26 00:19:32,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:19:32,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.11103785364675922
[2019-03-26 00:19:55,133] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.3572931], dtype=float32), 0.12588143]
[2019-03-26 00:19:55,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.13333333333333, 58.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7377750848796335, 6.9112, 6.9112, 168.912956510431, 628264.1856546797, 628264.1856546797, 189682.1486467206]
[2019-03-26 00:19:55,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:19:55,137] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5386925797831881
[2019-03-26 00:20:00,037] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7338.2116 3248667042.4360 1518.0000
[2019-03-26 00:20:00,386] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8142.2080 3010020069.4356 875.0000
[2019-03-26 00:20:01,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7441.1073 3371663595.5843 1473.0000
[2019-03-26 00:20:01,054] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8017.0655 3051820468.9683 993.0000
[2019-03-26 00:20:01,162] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7546.4963 3183026889.8422 1413.0000
[2019-03-26 00:20:02,176] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1025000, evaluation results [1025000.0, 7441.107281315151, 3371663595.5842915, 1473.0, 7546.496300997685, 3183026889.842205, 1413.0, 8142.20799421202, 3010020069.43561, 875.0, 7338.2116149802005, 3248667042.436029, 1518.0, 8017.065499426672, 3051820468.968317, 993.0]
[2019-03-26 00:20:05,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:20:05,309] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5226
[2019-03-26 00:20:05,313] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.96666666666667, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6105522492114864, 6.9112, 6.9112, 168.912956510431, 531305.4473124315, 531305.4473124315, 167705.5099687044], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6739800.0000, 
sim time next is 6740400.0000, 
raw observation next is [23.83333333333334, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6084843875137576, 6.9112, 6.9112, 168.912956510431, 529808.52405428, 529808.52405428, 167380.1167524434], 
processed observation next is [1.0, 0.0, 0.32859399684044266, 0.77, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5225419359923872, 0.0, 0.0, 0.8294399451523027, 0.14716903445952223, 0.14716903445952223, 0.24982106977976626], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.89043254], dtype=float32), -0.51784587]. 
=============================================
[2019-03-26 00:20:06,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:20:06,104] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2928
[2019-03-26 00:20:06,111] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6247114385294196, 6.911200000000001, 6.9112, 168.912956510431, 548351.653543969, 548351.6535439685, 169811.6298351666], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6751200.0000, 
sim time next is 6751800.0000, 
raw observation next is [21.95, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.628039935756559, 6.911199999999999, 6.9112, 168.912956510431, 551418.800703751, 551418.8007037516, 170336.2327768765], 
processed observation next is [1.0, 0.13043478260869565, 0.2393364928909953, 0.845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5463901655567793, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1531718890843753, 0.15317188908437546, 0.2542331832490694], 
reward next is 0.7458, 
noisyNet noise sample is [array([-0.71965283], dtype=float32), 1.8369592]. 
=============================================
[2019-03-26 00:20:24,660] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:20:24,667] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8923
[2019-03-26 00:20:24,674] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.811394413865721, 6.9112, 6.9112, 168.912956510431, 678469.6492092324, 678469.6492092324, 204159.7964741724], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7068600.0000, 
sim time next is 7069200.0000, 
raw observation next is [26.36666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8135595103909928, 6.9112, 6.9112, 168.912956510431, 680145.4825121628, 680145.4825121628, 204609.561478491], 
processed observation next is [1.0, 0.8260869565217391, 0.4486571879936811, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7726335492573082, 0.0, 0.0, 0.8294399451523027, 0.188929300697823, 0.188929300697823, 0.3053874051917776], 
reward next is 0.6946, 
noisyNet noise sample is [array([0.03640392], dtype=float32), 0.5755909]. 
=============================================
[2019-03-26 00:21:09,686] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 00:21:09,687] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:21:09,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:21:09,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:21:09,688] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:21:09,689] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:21:09,690] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:21:09,690] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:21:09,691] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:21:09,691] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:21:09,695] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:21:09,733] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-26 00:21:09,734] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-26 00:21:09,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-26 00:21:09,794] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-26 00:21:09,828] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-26 00:21:11,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:21:11,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.36666666666667, 88.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6176253293497992, 6.9112, 6.9112, 168.912956510431, 537359.5098161101, 537359.5098161101, 168810.2365738529]
[2019-03-26 00:21:11,691] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:21:11,697] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7301246328119113
[2019-03-26 00:21:49,143] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:21:49,144] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.13333333333334, 60.66666666666667, 1.0, 2.0, 0.8128096896562904, 1.0, 2.0, 0.8128096896562904, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2273202.059929941, 2273202.059929941, 426159.7411448776]
[2019-03-26 00:21:49,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:21:49,151] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1687366e-27], sampled 0.9051628073531437
[2019-03-26 00:21:49,152] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2273202.059929941 W.
[2019-03-26 00:21:57,639] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:21:57,639] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6150449117895812, 6.911199999999999, 6.9112, 168.912956510431, 534749.1700828475, 534749.170082848, 168412.5308654691]
[2019-03-26 00:21:57,640] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:21:57,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7304824906069155
[2019-03-26 00:22:09,442] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:22:09,443] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.15372342833334, 63.39096568333333, 1.0, 2.0, 0.9768085760414628, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599211874912, 6.9112, 168.9123159340768, 2262541.842386633, 2195293.305883743, 456300.2847968857]
[2019-03-26 00:22:09,445] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:22:09,447] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8813482e-14], sampled 0.23411097556658456
[2019-03-26 00:22:09,448] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2262541.842386633 W.
[2019-03-26 00:22:26,684] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:22:26,685] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.16277550166667, 70.08758636333334, 1.0, 2.0, 0.5870649578966414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820379.0485302509, 820379.0485302509, 198355.0501535515]
[2019-03-26 00:22:26,685] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:22:26,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.9287876e-36], sampled 0.5931148159827956
[2019-03-26 00:23:01,807] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:23:01,808] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.82085, 57.23569445666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5608571331966956, 6.911199999999999, 6.9112, 168.912956510431, 493760.0768818818, 493760.0768818824, 160190.0849322377]
[2019-03-26 00:23:01,809] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:23:01,813] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6755786768593441
[2019-03-26 00:23:06,204] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2269446], dtype=float32), 0.09222584]
[2019-03-26 00:23:06,206] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.825579685, 42.47135551166667, 1.0, 2.0, 0.9214206266932213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1355646.37874878, 1355646.378748781, 285842.2341773104]
[2019-03-26 00:23:06,207] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:23:06,210] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.860575e-08], sampled 0.8553079518143798
[2019-03-26 00:23:06,212] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1355646.37874878 W.
[2019-03-26 00:23:18,963] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7372.0767 3130132464.8827 1908.0000
[2019-03-26 00:23:19,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7310.8928 3339991814.3611 1985.0000
[2019-03-26 00:23:19,285] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8092.4851 2962682500.6991 1241.0000
[2019-03-26 00:23:19,572] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7037.8828 3207906987.6683 2325.0000
[2019-03-26 00:23:19,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7952.5807 3010007527.3863 1391.0000
[2019-03-26 00:23:20,669] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1050000, evaluation results [1050000.0, 7310.892801324724, 3339991814.361061, 1985.0, 7372.0767376373715, 3130132464.8826556, 1908.0, 8092.4851494508885, 2962682500.6990824, 1241.0, 7037.88280536015, 3207906987.668274, 2325.0, 7952.580697030657, 3010007527.3863463, 1391.0]
[2019-03-26 00:23:24,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:23:24,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0339
[2019-03-26 00:23:24,609] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.9, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7927060223202969, 6.911200000000001, 6.9112, 168.912956510431, 665715.192146436, 665715.1921464354, 200366.1159631511], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7693200.0000, 
sim time next is 7693800.0000, 
raw observation next is [24.85, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7918350282609602, 6.9112, 6.9112, 168.912956510431, 664950.3579151256, 664950.3579151256, 200187.7846103869], 
processed observation next is [1.0, 0.043478260869565216, 0.37677725118483424, 0.925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7461402783670245, 0.0, 0.0, 0.8294399451523027, 0.18470843275420154, 0.18470843275420154, 0.2987877382244581], 
reward next is 0.7012, 
noisyNet noise sample is [array([1.3734521], dtype=float32), 0.97996324]. 
=============================================
[2019-03-26 00:23:26,377] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:23:26,392] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0878
[2019-03-26 00:23:26,402] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.93333333333333, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9683229553232153, 6.911200000000001, 6.9112, 168.912956510429, 806829.9892318106, 806829.98923181, 240194.3157741841], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7796400.0000, 
sim time next is 7797000.0000, 
raw observation next is [26.06666666666666, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9706187138045858, 6.9112, 6.9112, 168.912956510431, 808077.8958828462, 808077.8958828462, 240750.5682215927], 
processed observation next is [1.0, 0.21739130434782608, 0.4344391785150076, 0.885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.964169163176324, 0.0, 0.0, 0.8294399451523027, 0.2244660821896795, 0.2244660821896795, 0.3593292063008846], 
reward next is 0.6407, 
noisyNet noise sample is [array([0.22202303], dtype=float32), 2.3976896]. 
=============================================
[2019-03-26 00:23:26,430] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[52.507687]
 [51.472828]
 [51.639496]
 [54.201946]
 [54.319897]], R is [[53.772789  ]
 [53.87656403]
 [53.33779907]
 [53.5087204 ]
 [53.40012741]].
[2019-03-26 00:23:28,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:23:28,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7615
[2019-03-26 00:23:29,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1912341.36210756 W.
[2019-03-26 00:23:29,007] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.75, 59.0, 1.0, 2.0, 0.4559280551953364, 1.0, 2.0, 0.4559280551953364, 1.0, 1.0, 0.7716103379095252, 6.9112, 6.9112, 170.5573041426782, 1912341.36210756, 1912341.36210756, 382662.0582522348], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7651800.0000, 
sim time next is 7652400.0000, 
raw observation next is [30.83333333333333, 58.66666666666667, 1.0, 2.0, 0.7021470514348225, 1.0, 2.0, 0.7021470514348225, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1963433.80889507, 1963433.80889507, 375034.5615542017], 
processed observation next is [1.0, 0.5652173913043478, 0.6603475513428118, 0.5866666666666667, 1.0, 1.0, 0.6411410258250873, 1.0, 1.0, 0.6411410258250873, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5453982802486306, 0.5453982802486306, 0.5597530769465697], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8027365], dtype=float32), -0.027953003]. 
=============================================
[2019-03-26 00:23:38,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:38,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:38,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-26 00:23:39,298] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7884665e-26], sum to 1.0000
[2019-03-26 00:23:39,305] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3105
[2019-03-26 00:23:39,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1627634.034380064 W.
[2019-03-26 00:23:39,317] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 76.33333333333334, 1.0, 2.0, 0.5821479332727331, 0.0, 1.0, 0.0, 1.0, 2.0, 0.993857085382265, 6.911199999999999, 6.9112, 168.9129565104306, 1627634.034380064, 1627634.034380065, 352603.2564333332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7810800.0000, 
sim time next is 7811400.0000, 
raw observation next is [28.88333333333333, 75.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.710124926941855, 6.9112, 168.9085593110346, 2020913.251417784, 1454143.16610661, 311350.2025413477], 
processed observation next is [1.0, 0.391304347826087, 0.5679304897314374, 0.7566666666666666, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.07989249269418552, 0.0, 0.8294183528902197, 0.5613647920604956, 0.40392865725183613, 0.4647017948378324], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3785675], dtype=float32), -0.123370044]. 
=============================================
[2019-03-26 00:23:40,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:40,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:40,243] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-26 00:23:40,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:40,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:40,605] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-26 00:23:42,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:42,298] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:42,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-26 00:23:43,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:43,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:43,299] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-26 00:23:43,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.4190900e-03 8.4362851e-34 2.6756657e-30 1.1166168e-32 9.9158090e-01], sum to 1.0000
[2019-03-26 00:23:43,385] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3722
[2019-03-26 00:23:43,390] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.2, 67.0, 1.0, 2.0, 0.2716603559109066, 1.0, 2.0, 0.2716603559109066, 1.0, 2.0, 0.472153339491849, 6.911200000000001, 6.9112, 170.5573041426782, 1213608.90681012, 1213608.90681012, 300404.0038943664], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 39600.0000, 
sim time next is 40200.0000, 
raw observation next is [26.3, 66.66666666666667, 1.0, 2.0, 0.296249136182375, 1.0, 2.0, 0.296249136182375, 1.0, 2.0, 0.5142640974339394, 6.911200000000001, 6.9112, 170.5573041426782, 1321106.741243104, 1321106.741243104, 310135.9244228894], 
processed observation next is [1.0, 0.4782608695652174, 0.4454976303317536, 0.6666666666666667, 1.0, 1.0, 0.15210739299081322, 1.0, 1.0, 0.15210739299081322, 1.0, 1.0, 0.4076391432121212, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3669740947897511, 0.3669740947897511, 0.4628894394371484], 
reward next is 0.5371, 
noisyNet noise sample is [array([1.5903043], dtype=float32), -0.3240612]. 
=============================================
[2019-03-26 00:23:44,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:44,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:44,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-26 00:23:44,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:44,375] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:44,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-26 00:23:44,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.9476735e-32], sum to 1.0000
[2019-03-26 00:23:44,519] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8820
[2019-03-26 00:23:44,529] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.3, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6161956072154033, 6.911200000000001, 6.9112, 168.912956510431, 536190.0994677328, 536190.0994677322, 168584.9125472638], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 88800.0000, 
sim time next is 89400.0000, 
raw observation next is [22.3, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6156589282971588, 6.9112, 6.9112, 168.912956510431, 535723.0073958393, 535723.0073958393, 168500.9612810219], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5312913759721449, 0.0, 0.0, 0.8294399451523027, 0.14881194649884424, 0.14881194649884424, 0.25149397206122676], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.29925677], dtype=float32), -1.2090005]. 
=============================================
[2019-03-26 00:23:48,351] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:48,353] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:48,437] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-26 00:23:48,605] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1060746: loss 0.8692
[2019-03-26 00:23:48,608] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1060747: learning rate 0.0010
[2019-03-26 00:23:48,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:48,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:49,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-26 00:23:49,740] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1061283: loss 0.8072
[2019-03-26 00:23:49,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1061283: learning rate 0.0010
[2019-03-26 00:23:49,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 3.2740427e-34 4.4033097e-31 4.0801492e-34 5.1844708e-08], sum to 1.0000
[2019-03-26 00:23:49,853] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6260
[2019-03-26 00:23:49,854] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1165126.877324925 W.
[2019-03-26 00:23:49,859] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.9, 93.0, 1.0, 2.0, 0.264550891966799, 1.0, 2.0, 0.264550891966799, 1.0, 2.0, 0.4554336972242909, 6.911199999999999, 6.9112, 170.5573041426782, 1165126.877324925, 1165126.877324925, 295761.0353691], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 120600.0000, 
sim time next is 121200.0000, 
raw observation next is [22.9, 93.33333333333334, 1.0, 2.0, 0.3532187967968622, 1.0, 2.0, 0.3532187967968622, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1043136.182775187, 1043136.182775187, 267874.0491657825], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9333333333333335, 1.0, 1.0, 0.22074553830947252, 1.0, 1.0, 0.22074553830947252, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2897600507708853, 0.2897600507708853, 0.3998120136802724], 
reward next is 0.6002, 
noisyNet noise sample is [array([1.0930787], dtype=float32), 0.025496392]. 
=============================================
[2019-03-26 00:23:49,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:49,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:49,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-26 00:23:49,990] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1061400: loss 0.0259
[2019-03-26 00:23:49,994] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1061400: learning rate 0.0010
[2019-03-26 00:23:50,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-26 00:23:50,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-26 00:23:50,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,363] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-26 00:23:50,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-26 00:23:50,713] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,714] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-26 00:23:50,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:23:50,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:23:50,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-26 00:23:50,974] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1061873: loss 0.0419
[2019-03-26 00:23:50,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1061873: learning rate 0.0010
[2019-03-26 00:23:51,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:23:51,365] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6692
[2019-03-26 00:23:51,369] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6673373225239775, 6.911200000000001, 6.9112, 168.912956510431, 572194.3413916377, 572194.3413916371, 177017.7651174091], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 64800.0000, 
sim time next is 65400.0000, 
raw observation next is [25.43333333333334, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.675338481482099, 6.911199999999999, 6.9112, 168.912956510431, 579468.8303373059, 579468.8303373066, 178390.2817736222], 
processed observation next is [1.0, 0.782608695652174, 0.40442338072669864, 0.7566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6040713188806085, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16096356398258496, 0.16096356398258516, 0.26625415190092866], 
reward next is 0.7337, 
noisyNet noise sample is [array([-1.3038588], dtype=float32), 1.4263653]. 
=============================================
[2019-03-26 00:23:51,377] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1062151: loss 0.0537
[2019-03-26 00:23:51,380] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1062153: learning rate 0.0010
[2019-03-26 00:23:52,281] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1062603: loss 0.7445
[2019-03-26 00:23:52,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1062603: learning rate 0.0010
[2019-03-26 00:23:52,910] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1062834: loss 0.9843
[2019-03-26 00:23:52,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1062835: learning rate 0.0010
[2019-03-26 00:23:57,479] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1064523: loss 0.6134
[2019-03-26 00:23:57,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1064523: learning rate 0.0010
[2019-03-26 00:23:58,931] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1065054: loss 0.0267
[2019-03-26 00:23:58,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1065054: learning rate 0.0010
[2019-03-26 00:24:01,064] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1065845: loss 0.8229
[2019-03-26 00:24:01,066] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1065845: learning rate 0.0010
[2019-03-26 00:24:01,647] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1066062: loss 0.0660
[2019-03-26 00:24:01,650] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1066063: learning rate 0.0010
[2019-03-26 00:24:02,171] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1066250: loss 0.2379
[2019-03-26 00:24:02,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1066250: learning rate 0.0010
[2019-03-26 00:24:02,446] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1066356: loss 0.0626
[2019-03-26 00:24:02,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1066357: learning rate 0.0010
[2019-03-26 00:24:03,150] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1066611: loss 0.0423
[2019-03-26 00:24:03,155] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1066611: learning rate 0.0010
[2019-03-26 00:24:03,185] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1066623: loss 0.1840
[2019-03-26 00:24:03,188] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1066623: learning rate 0.0010
[2019-03-26 00:24:03,529] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1066753: loss 1.0379
[2019-03-26 00:24:03,532] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1066755: learning rate 0.0010
[2019-03-26 00:24:06,085] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1067709: loss 0.2635
[2019-03-26 00:24:06,086] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1067709: learning rate 0.0010
[2019-03-26 00:24:08,232] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1068509: loss 0.1748
[2019-03-26 00:24:08,235] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1068509: learning rate 0.0010
[2019-03-26 00:24:08,871] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1068744: loss 0.0726
[2019-03-26 00:24:08,874] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1068744: learning rate 0.0010
[2019-03-26 00:24:10,578] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 2.7936880e-38 1.2176776e-37 1.3769491e-37 7.9347584e-11], sum to 1.0000
[2019-03-26 00:24:10,588] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9475
[2019-03-26 00:24:10,598] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.63333333333333, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5242433220881926, 6.9112, 6.9112, 168.912956510431, 462374.4486972084, 462374.4486972084, 155171.1158504856], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [20.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5221845831601394, 6.911199999999999, 6.9112, 168.912956510431, 460693.7622599908, 460693.7622599914, 154896.351811156], 
processed observation next is [0.0, 0.9565217391304348, 0.17535545023696694, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41729827214651144, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12797048951666412, 0.12797048951666426, 0.23118858479277016], 
reward next is 0.7688, 
noisyNet noise sample is [array([0.95328945], dtype=float32), 0.8499357]. 
=============================================
[2019-03-26 00:24:11,286] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1069591: loss 0.2957
[2019-03-26 00:24:11,289] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1069591: learning rate 0.0010
[2019-03-26 00:24:12,511] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1070049: loss 0.3830
[2019-03-26 00:24:12,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1070050: learning rate 0.0010
[2019-03-26 00:24:13,806] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1070527: loss 0.0975
[2019-03-26 00:24:13,807] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1070528: learning rate 0.0010
[2019-03-26 00:24:14,410] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1070748: loss 0.0140
[2019-03-26 00:24:14,417] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1070748: learning rate 0.0010
[2019-03-26 00:24:16,867] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.0233514e-21], sum to 1.0000
[2019-03-26 00:24:16,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2046
[2019-03-26 00:24:16,875] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.43333333333334, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4042504477358408, 6.911199999999999, 6.9112, 168.912956510431, 364661.1482953058, 364661.1482953064, 140853.0855622918], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 602400.0000, 
sim time next is 603000.0000, 
raw observation next is [18.35, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4019262382679117, 6.9112, 6.9112, 168.912956510431, 362624.6104155854, 362624.6104155854, 140620.8924208071], 
processed observation next is [1.0, 1.0, 0.06872037914691956, 0.855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.27064175398525814, 0.0, 0.0, 0.8294399451523027, 0.10072905844877372, 0.10072905844877372, 0.20988192898627925], 
reward next is 0.7901, 
noisyNet noise sample is [array([0.7869407], dtype=float32), -1.247606]. 
=============================================
[2019-03-26 00:24:16,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[80.89807 ]
 [80.90075 ]
 [80.916794]
 [80.81494 ]
 [80.82303 ]], R is [[80.87617493]
 [80.85718536]
 [80.83789062]
 [80.81840515]
 [80.7988739 ]].
[2019-03-26 00:24:19,061] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1072462: loss 0.1633
[2019-03-26 00:24:19,067] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1072462: learning rate 0.0010
[2019-03-26 00:24:20,517] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1072997: loss 0.2017
[2019-03-26 00:24:20,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1072998: learning rate 0.0010
[2019-03-26 00:24:22,773] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1073833: loss 0.1681
[2019-03-26 00:24:22,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1073833: learning rate 0.0010
[2019-03-26 00:24:23,297] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1074020: loss 0.0379
[2019-03-26 00:24:23,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1074021: learning rate 0.0010
[2019-03-26 00:24:23,751] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1074194: loss 0.1785
[2019-03-26 00:24:23,753] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1074194: learning rate 0.0010
[2019-03-26 00:24:24,006] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1074286: loss 0.0410
[2019-03-26 00:24:24,009] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1074286: learning rate 0.0010
[2019-03-26 00:24:24,774] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1074567: loss 0.2596
[2019-03-26 00:24:24,782] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1074570: learning rate 0.0010
[2019-03-26 00:24:24,875] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1074609: loss 0.4895
[2019-03-26 00:24:24,879] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1074610: learning rate 0.0010
[2019-03-26 00:24:25,119] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1074697: loss 0.1169
[2019-03-26 00:24:25,123] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1074699: learning rate 0.0010
[2019-03-26 00:24:25,942] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 00:24:25,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:24:25,944] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:24:25,944] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:24:25,944] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:24:25,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:24:25,946] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:24:25,949] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:24:25,947] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:24:25,953] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:24:25,954] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:24:25,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-26 00:24:26,002] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-26 00:24:26,025] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-26 00:24:26,056] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-26 00:24:26,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-26 00:24:32,143] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:24:32,143] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.65218989333334, 83.20358259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6260659416202485, 6.9112, 6.9112, 168.912956510431, 542884.2888818984, 542884.2888818984, 170172.5309965608]
[2019-03-26 00:24:32,143] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:24:32,145] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7924909e-31], sampled 0.870065241157552
[2019-03-26 00:25:11,087] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:25:11,091] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.757282472436405, 6.9112, 6.9112, 168.912956510431, 640248.9777405265, 640248.9777405265, 193380.2827673716]
[2019-03-26 00:25:11,093] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:25:11,096] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.26953170685448313
[2019-03-26 00:25:55,669] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:25:55,669] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.36666666666667, 60.66666666666666, 1.0, 1.0, 0.6069381573090659, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.91286983045, 848161.4359388481, 848161.4359388481, 202044.1850428677]
[2019-03-26 00:25:55,669] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:25:55,671] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.6843136e-04 0.0000000e+00 2.9991757e-36 0.0000000e+00 9.9963152e-01], sampled 0.6967826636564838
[2019-03-26 00:26:21,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:26:21,244] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.26666666666667, 52.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.762057511710174, 6.911200000000001, 6.9112, 168.912956510431, 643087.5587748671, 643087.5587748665, 194294.571841729]
[2019-03-26 00:26:21,246] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:26:21,251] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6177797e-33], sampled 0.26820673312679055
[2019-03-26 00:26:24,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:26:24,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.84999999999999, 57.5, 1.0, 2.0, 0.3996359120312301, 1.0, 2.0, 0.3996359120312301, 1.0, 2.0, 0.69403541758259, 6.911200000000001, 6.9112, 169.0403247858759, 1676057.103381127, 1676057.103381126, 352392.5633421393]
[2019-03-26 00:26:24,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:26:24,741] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.021602e-12 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.7345546382935518
[2019-03-26 00:26:27,074] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:26:27,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.787204555, 92.30342181166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5115928767072638, 6.911199999999999, 6.9112, 168.912956510431, 454022.0711292263, 454022.0711292269, 153410.502806198]
[2019-03-26 00:26:27,076] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:26:27,082] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1270392e-21], sampled 0.6809681886363401
[2019-03-26 00:26:27,432] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2031102], dtype=float32), -0.016673516]
[2019-03-26 00:26:27,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.66395214666667, 92.58765017333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6969160611892483, 6.911199999999999, 6.9112, 168.912956510431, 597268.7904827115, 597268.7904827121, 182178.3904761096]
[2019-03-26 00:26:27,433] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:26:27,436] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3431937875907848
[2019-03-26 00:26:35,194] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7826.4109 3229540309.3920 802.0000
[2019-03-26 00:26:35,324] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8113.9876 3078798921.8591 575.0000
[2019-03-26 00:26:35,554] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8036.4314 3110633489.7966 596.0000
[2019-03-26 00:26:35,790] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7713.4501 3289080721.8650 737.0000
[2019-03-26 00:26:35,927] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7622.9344 3400585797.5459 1024.0000
[2019-03-26 00:26:36,943] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1075000, evaluation results [1075000.0, 7622.934417547482, 3400585797.54595, 1024.0, 7826.410912427188, 3229540309.391963, 802.0, 8113.987648934918, 3078798921.859056, 575.0, 7713.450059726104, 3289080721.865008, 737.0, 8036.431413901591, 3110633489.796616, 596.0]
[2019-03-26 00:26:39,128] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1075816: loss 6.3568
[2019-03-26 00:26:39,131] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1075817: learning rate 0.0010
[2019-03-26 00:26:41,152] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1076570: loss 11.4103
[2019-03-26 00:26:41,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1076570: learning rate 0.0010
[2019-03-26 00:26:41,379] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7249171e-12], sum to 1.0000
[2019-03-26 00:26:41,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6890
[2019-03-26 00:26:41,394] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.26666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4451668416805808, 6.911200000000001, 6.9112, 168.912956510431, 399198.2861125637, 399198.2861125631, 145277.4931798205], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 685200.0000, 
sim time next is 685800.0000, 
raw observation next is [19.15, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4425237429167874, 6.911200000000001, 6.9112, 168.912956510431, 396925.4472254777, 396925.4472254771, 144982.7161697208], 
processed observation next is [1.0, 0.9565217391304348, 0.10663507109004738, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3201509059960822, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11025706867374381, 0.11025706867374363, 0.21639211368615047], 
reward next is 0.7836, 
noisyNet noise sample is [array([-0.8466591], dtype=float32), -0.63869834]. 
=============================================
[2019-03-26 00:26:41,736] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1076788: loss 8.4334
[2019-03-26 00:26:41,739] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1076788: learning rate 0.0010
[2019-03-26 00:26:43,996] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1077621: loss 0.6315
[2019-03-26 00:26:43,998] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1077621: learning rate 0.0010
[2019-03-26 00:26:44,043] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.439602e-29 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 00:26:44,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8114
[2019-03-26 00:26:44,057] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.8, 61.0, 1.0, 2.0, 0.2527928152962354, 1.0, 2.0, 0.2527928152962354, 1.0, 2.0, 0.460731898984981, 6.911200000000001, 6.9112, 170.5573041426782, 1220119.236006174, 1220119.236006174, 301398.1184792669], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 572400.0000, 
sim time next is 573000.0000, 
raw observation next is [23.76666666666667, 61.16666666666667, 1.0, 2.0, 0.2099293303269962, 1.0, 2.0, 0.2099293303269962, 1.0, 2.0, 0.3829064767539455, 6.9112, 6.9112, 170.5573041426782, 1014618.935374694, 1014618.935374694, 286212.2410172903], 
processed observation next is [1.0, 0.6521739130434783, 0.32543443917851517, 0.6116666666666667, 1.0, 1.0, 0.04810762689999542, 1.0, 1.0, 0.04810762689999542, 1.0, 1.0, 0.24744692287066525, 0.0, 0.0, 0.8375144448122397, 0.2818385931596372, 0.2818385931596372, 0.42718244927953775], 
reward next is 0.5728, 
noisyNet noise sample is [array([2.2268424], dtype=float32), -1.7217029]. 
=============================================
[2019-03-26 00:26:44,072] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.0182  ]
 [72.00178 ]
 [71.94926 ]
 [71.896454]
 [71.97065 ]], R is [[72.36910248]
 [72.19556427]
 [72.02102661]
 [71.84860992]
 [71.68047333]].
[2019-03-26 00:26:45,316] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1078108: loss 3.3884
[2019-03-26 00:26:45,318] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1078108: learning rate 0.0010
[2019-03-26 00:26:46,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9550506e-36], sum to 1.0000
[2019-03-26 00:26:46,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5585
[2019-03-26 00:26:46,186] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.28333333333333, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3795060728844891, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 138409.1736609268], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 611400.0000, 
sim time next is 612000.0000, 
raw observation next is [17.2, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3781733240147248, 6.911200000000001, 6.9112, 168.912956510431, 342264.3359998239, 342264.3359998233, 138277.4870677902], 
processed observation next is [1.0, 0.08695652173913043, 0.014218009478673018, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.24167478538381074, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09507342666661775, 0.09507342666661758, 0.2063843090564033], 
reward next is 0.7936, 
noisyNet noise sample is [array([-0.7615723], dtype=float32), 1.6889346]. 
=============================================
[2019-03-26 00:26:46,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.417725]
 [78.53561 ]
 [78.46233 ]
 [78.225296]
 [77.7573  ]], R is [[78.56824493]
 [78.57598114]
 [78.58339691]
 [78.59042358]
 [78.59709167]].
[2019-03-26 00:26:46,431] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1078513: loss 7.2507
[2019-03-26 00:26:46,436] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1078513: learning rate 0.0010
[2019-03-26 00:26:47,232] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1078810: loss 0.2879
[2019-03-26 00:26:47,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1078810: learning rate 0.0010
[2019-03-26 00:26:51,846] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1080513: loss 0.5869
[2019-03-26 00:26:51,850] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1080513: learning rate 0.0010
[2019-03-26 00:26:52,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:26:52,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2046
[2019-03-26 00:26:52,664] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.68333333333333, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4883159400032541, 6.911199999999999, 6.9112, 168.912956510431, 440045.684673672, 440045.6846736726, 150048.5262833584], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 699000.0000, 
sim time next is 699600.0000, 
raw observation next is [17.66666666666667, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4313308222395038, 6.9112, 6.9112, 168.912956510431, 388733.2498148087, 388733.2498148087, 143630.0344889979], 
processed observation next is [1.0, 0.08695652173913043, 0.03633491311216459, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3065010027311022, 0.0, 0.0, 0.8294399451523027, 0.1079814582818913, 0.1079814582818913, 0.21437318580447445], 
reward next is 0.7856, 
noisyNet noise sample is [array([-0.06856654], dtype=float32), 0.5098333]. 
=============================================
[2019-03-26 00:26:53,327] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1081059: loss 0.6062
[2019-03-26 00:26:53,330] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1081059: learning rate 0.0010
[2019-03-26 00:26:55,524] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1081869: loss 2.3652
[2019-03-26 00:26:55,527] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1081869: learning rate 0.0010
[2019-03-26 00:26:56,123] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1082090: loss 1.9285
[2019-03-26 00:26:56,131] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1082091: learning rate 0.0010
[2019-03-26 00:26:56,333] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1082165: loss 0.3813
[2019-03-26 00:26:56,337] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1082165: learning rate 0.0010
[2019-03-26 00:26:56,818] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1082338: loss 1.8741
[2019-03-26 00:26:56,822] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1082338: learning rate 0.0010
[2019-03-26 00:26:57,606] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1082632: loss 6.8053
[2019-03-26 00:26:57,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1082632: learning rate 0.0010
[2019-03-26 00:26:57,660] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1082651: loss 17.9515
[2019-03-26 00:26:57,663] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1082651: learning rate 0.0010
[2019-03-26 00:26:58,021] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1082782: loss 3.3654
[2019-03-26 00:26:58,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1082783: learning rate 0.0010
[2019-03-26 00:27:00,694] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1083780: loss 0.0055
[2019-03-26 00:27:00,700] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1083780: learning rate 0.0010
[2019-03-26 00:27:02,797] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1084560: loss 0.0035
[2019-03-26 00:27:02,800] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1084561: learning rate 0.0010
[2019-03-26 00:27:03,424] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1084793: loss 0.0115
[2019-03-26 00:27:03,427] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1084793: learning rate 0.0010
[2019-03-26 00:27:05,541] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1085565: loss 0.1969
[2019-03-26 00:27:05,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1085565: learning rate 0.0010
[2019-03-26 00:27:07,018] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1086110: loss 0.5416
[2019-03-26 00:27:07,022] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1086110: learning rate 0.0010
[2019-03-26 00:27:07,799] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1086395: loss 0.1803
[2019-03-26 00:27:07,805] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1086397: learning rate 0.0010
[2019-03-26 00:27:08,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:27:08,298] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6569
[2019-03-26 00:27:08,307] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.05, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5691759704565186, 6.9112, 6.9112, 168.912956510431, 497365.1924567967, 497365.1924567967, 161482.7508256612], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [24.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5704203678157002, 6.9112, 6.9112, 168.912956510431, 498329.8766802831, 498329.8766802831, 161664.6028694742], 
processed observation next is [0.0, 0.7391304347826086, 0.3364928909952607, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47612239977524407, 0.0, 0.0, 0.8294399451523027, 0.1384249657445231, 0.1384249657445231, 0.24129045204399135], 
reward next is 0.7587, 
noisyNet noise sample is [array([1.3939884], dtype=float32), -0.052228794]. 
=============================================
[2019-03-26 00:27:08,795] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1086760: loss 0.1168
[2019-03-26 00:27:08,798] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1086760: learning rate 0.0010
[2019-03-26 00:27:13,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1088459: loss 0.1329
[2019-03-26 00:27:13,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1088459: learning rate 0.0010
[2019-03-26 00:27:14,936] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1089014: loss 0.1982
[2019-03-26 00:27:14,938] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1089014: learning rate 0.0010
[2019-03-26 00:27:16,959] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1089764: loss 0.1170
[2019-03-26 00:27:16,961] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1089764: learning rate 0.0010
[2019-03-26 00:27:17,543] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1089986: loss 0.2309
[2019-03-26 00:27:17,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1089986: learning rate 0.0010
[2019-03-26 00:27:17,651] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1090018: loss 0.1747
[2019-03-26 00:27:17,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1090019: learning rate 0.0010
[2019-03-26 00:27:18,290] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1090260: loss 0.1225
[2019-03-26 00:27:18,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1090261: learning rate 0.0010
[2019-03-26 00:27:18,883] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1090478: loss 0.3414
[2019-03-26 00:27:18,884] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1090478: learning rate 0.0010
[2019-03-26 00:27:19,037] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1090534: loss 0.3605
[2019-03-26 00:27:19,042] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1090536: learning rate 0.0010
[2019-03-26 00:27:19,480] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1090700: loss 0.3110
[2019-03-26 00:27:19,483] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1090701: learning rate 0.0010
[2019-03-26 00:27:22,847] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1091944: loss 106.0089
[2019-03-26 00:27:22,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1091944: learning rate 0.0010
[2019-03-26 00:27:24,475] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.9248194e-01 2.7828862e-28 7.8188471e-32 5.1036659e-31 3.0751801e-01], sum to 1.0000
[2019-03-26 00:27:24,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1744
[2019-03-26 00:27:24,493] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.33333333333334, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2589811103194636, 6.9112, 6.9112, 170.5573041426782, 651784.140764042, 651784.140764042, 252686.9683694566], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [24.31666666666667, 94.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7728283856945495, 6.9112, 6.9112, 168.912956510431, 649898.7238106679, 649898.7238106679, 196380.0146238603], 
processed observation next is [1.0, 1.0, 0.3515007898894157, 0.94, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.7229614459689628, 0.0, 0.0, 0.8294399451523027, 0.18052742328074106, 0.18052742328074106, 0.29310449943859745], 
reward next is 0.7069, 
noisyNet noise sample is [array([-1.0762967], dtype=float32), 0.57447815]. 
=============================================
[2019-03-26 00:27:25,124] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1092782: loss 4.5977
[2019-03-26 00:27:25,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1092782: learning rate 0.0010
[2019-03-26 00:27:25,659] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1092979: loss 3.5517
[2019-03-26 00:27:25,662] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1092979: learning rate 0.0010
[2019-03-26 00:27:27,615] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.4102956e-38 9.4659337e-38 7.4686078e-36 4.3295056e-09], sum to 1.0000
[2019-03-26 00:27:27,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4928
[2019-03-26 00:27:27,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1092495.214914314 W.
[2019-03-26 00:27:27,636] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.245010493862213, 1.0, 2.0, 0.245010493862213, 1.0, 2.0, 0.4253182432293377, 6.9112, 6.9112, 170.5573041426782, 1092495.214914314, 1092495.214914314, 290313.6215396835], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [22.4, 93.0, 1.0, 2.0, 0.2626180149108047, 1.0, 2.0, 0.2626180149108047, 1.0, 2.0, 0.4569568014509027, 6.911200000000001, 6.9112, 170.5573041426782, 1175204.604357714, 1175204.604357714, 297169.1929676406], 
processed observation next is [1.0, 0.5217391304347826, 0.2606635071090047, 0.93, 1.0, 1.0, 0.11158796977205387, 1.0, 1.0, 0.11158796977205387, 1.0, 1.0, 0.3377521968913448, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.32644572343269834, 0.32644572343269834, 0.44353610890692624], 
reward next is 0.5565, 
noisyNet noise sample is [array([-0.22497796], dtype=float32), 1.1711588]. 
=============================================
[2019-03-26 00:27:27,642] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1093707: loss -174.0725
[2019-03-26 00:27:27,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1093707: learning rate 0.0010
[2019-03-26 00:27:28,941] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1094193: loss -10.4214
[2019-03-26 00:27:28,948] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1094194: learning rate 0.0010
[2019-03-26 00:27:29,594] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1094436: loss -70.8264
[2019-03-26 00:27:29,600] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1094437: learning rate 0.0010
[2019-03-26 00:27:30,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2104781e-38], sum to 1.0000
[2019-03-26 00:27:30,875] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5983
[2019-03-26 00:27:30,882] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.36666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7757017571984075, 6.9112, 6.9112, 168.912956510431, 653740.3159248809, 653740.3159248809, 196978.5940701483], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1293600.0000, 
sim time next is 1294200.0000, 
raw observation next is [24.35, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7740971538541656, 6.9112, 6.9112, 168.912956510431, 652529.954117756, 652529.954117756, 196661.1469166613], 
processed observation next is [1.0, 1.0, 0.35308056872037924, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7245087242123969, 0.0, 0.0, 0.8294399451523027, 0.18125832058826555, 0.18125832058826555, 0.2935240998756139], 
reward next is 0.7065, 
noisyNet noise sample is [array([0.45415613], dtype=float32), -0.18023191]. 
=============================================
[2019-03-26 00:27:30,897] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1094911: loss -34.2024
[2019-03-26 00:27:30,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1094911: learning rate 0.0010
[2019-03-26 00:27:30,905] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.274111e-38], sum to 1.0000
[2019-03-26 00:27:30,917] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1860
[2019-03-26 00:27:30,924] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.35, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7740971538541656, 6.9112, 6.9112, 168.912956510431, 652529.954117756, 652529.954117756, 196661.1469166613], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1294200.0000, 
sim time next is 1294800.0000, 
raw observation next is [24.33333333333334, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7726754605595864, 6.9112, 6.9112, 168.912956510431, 651474.5764389839, 651474.5764389839, 196380.7275369522], 
processed observation next is [1.0, 1.0, 0.35229067930489766, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7227749519019344, 0.0, 0.0, 0.8294399451523027, 0.18096516012193997, 0.18096516012193997, 0.2931055634879884], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.45415613], dtype=float32), -0.18023191]. 
=============================================
[2019-03-26 00:27:35,539] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1096617: loss 162.5999
[2019-03-26 00:27:35,541] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1096617: learning rate 0.0010
[2019-03-26 00:27:37,027] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1097162: loss -21.8206
[2019-03-26 00:27:37,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1097163: learning rate 0.0010
[2019-03-26 00:27:39,140] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1097943: loss 126.0119
[2019-03-26 00:27:39,146] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1097944: learning rate 0.0010
[2019-03-26 00:27:39,572] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1098096: loss -115.8683
[2019-03-26 00:27:39,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1098096: learning rate 0.0010
[2019-03-26 00:27:39,686] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:27:39,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6099
[2019-03-26 00:27:39,712] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1063899.981961135 W.
[2019-03-26 00:27:39,719] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.5, 88.0, 1.0, 2.0, 0.6739707472248321, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1063899.981961135, 1063899.981961136, 229871.9230778194], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1346400.0000, 
sim time next is 1347000.0000, 
raw observation next is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.2886279779682039, 1.0, 1.0, 0.2886279779682039, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 908076.6604151395, 908076.6604151395, 260018.6555359265], 
processed observation next is [1.0, 0.6086956521739131, 0.21642969984202226, 0.8816666666666667, 1.0, 1.0, 0.1429252746604866, 1.0, 0.5, 0.1429252746604866, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2522435167819832, 0.2522435167819832, 0.38808754557600966], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.550336], dtype=float32), 0.969827]. 
=============================================
[2019-03-26 00:27:39,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[57.157154]
 [57.10176 ]
 [56.908943]
 [56.95134 ]
 [57.887405]], R is [[56.76513672]
 [56.19748688]
 [55.63551331]
 [55.07915878]
 [54.52836609]].
[2019-03-26 00:27:39,778] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1098172: loss 22.1052
[2019-03-26 00:27:39,780] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1098172: learning rate 0.0010
[2019-03-26 00:27:40,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:27:40,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2391
[2019-03-26 00:27:40,440] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.13333333333333, 91.66666666666667, 1.0, 2.0, 0.2800166759450037, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4975678806778459, 6.911199999999999, 6.9112, 168.912956510431, 862834.146718252, 862834.1467182526, 222752.7840669286], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1341600.0000, 
sim time next is 1342200.0000, 
raw observation next is [22.06666666666667, 91.33333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9717556974937847, 6.9112, 6.9112, 168.912956510431, 843653.780688946, 843653.780688946, 241355.6726110755], 
processed observation next is [1.0, 0.5217391304347826, 0.2448657187993683, 0.9133333333333334, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9655557286509568, 0.0, 0.0, 0.8294399451523027, 0.23434827241359613, 0.23434827241359613, 0.36023234718070973], 
reward next is 0.6398, 
noisyNet noise sample is [array([1.3461679], dtype=float32), 0.014426621]. 
=============================================
[2019-03-26 00:27:40,485] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1098427: loss -99.0861
[2019-03-26 00:27:40,489] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1098427: learning rate 0.0010
[2019-03-26 00:27:40,981] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1098613: loss -33.7160
[2019-03-26 00:27:40,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1098614: learning rate 0.0010
[2019-03-26 00:27:41,392] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1098763: loss 5.8451
[2019-03-26 00:27:41,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1098763: learning rate 0.0010
[2019-03-26 00:27:41,511] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:27:41,525] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8110
[2019-03-26 00:27:41,540] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.03333333333333, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5755125934215576, 6.911199999999999, 6.9112, 168.912956510431, 504488.8921073263, 504488.8921073269, 162357.9985991739], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1363200.0000, 
sim time next is 1363800.0000, 
raw observation next is [21.06666666666667, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5783026877218271, 6.9112, 6.9112, 168.912956510431, 506670.1759969085, 506670.1759969085, 162771.829242332], 
processed observation next is [1.0, 0.782608695652174, 0.19747235387045833, 0.9383333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48573498502661844, 0.0, 0.0, 0.8294399451523027, 0.1407417155546968, 0.1407417155546968, 0.2429430287198985], 
reward next is 0.7571, 
noisyNet noise sample is [array([0.23473874], dtype=float32), 0.32671157]. 
=============================================
[2019-03-26 00:27:41,598] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1098837: loss -87.8569
[2019-03-26 00:27:41,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1098842: learning rate 0.0010
[2019-03-26 00:27:44,097] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1099766: loss 0.6851
[2019-03-26 00:27:44,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1099766: learning rate 0.0010
[2019-03-26 00:27:44,744] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 00:27:44,745] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:27:44,746] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:27:44,747] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:27:44,748] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:27:44,748] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:27:44,748] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:27:44,750] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:27:44,751] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:27:44,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:27:44,754] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:27:44,782] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-26 00:27:44,804] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-26 00:27:44,829] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-26 00:27:44,849] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-26 00:27:44,871] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-26 00:28:00,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2969788], dtype=float32), -0.039532553]
[2019-03-26 00:28:00,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.8, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6041245652711931, 6.9112, 6.9112, 168.912956510431, 524994.6937871921, 524994.6937871921, 166727.050845814]
[2019-03-26 00:28:00,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:28:00,926] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3255501302868644
[2019-03-26 00:28:24,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2969788], dtype=float32), -0.039532553]
[2019-03-26 00:28:24,935] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.66581443, 79.749955695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8677813528374313, 6.9112, 6.9112, 168.912956510431, 721766.0323965616, 721766.0323965616, 216264.8174135317]
[2019-03-26 00:28:24,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:28:24,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7912379806604227
[2019-03-26 00:28:46,270] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2969788], dtype=float32), -0.039532553]
[2019-03-26 00:28:46,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.23417633166667, 74.54330522500001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5745638503265388, 6.9112, 6.9112, 168.912956510431, 506480.2060686256, 506480.2060686256, 162127.8966424105]
[2019-03-26 00:28:46,274] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:28:46,277] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9877209586763405
[2019-03-26 00:28:56,801] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2969788], dtype=float32), -0.039532553]
[2019-03-26 00:28:56,802] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.70066809, 78.76598925, 1.0, 2.0, 0.7702139721154511, 1.0, 2.0, 0.7056970255719882, 1.0, 2.0, 1.03, 6.997955738396936, 6.9112, 184.5923449428631, 2961004.309301301, 2893743.639511954, 548641.5556194353]
[2019-03-26 00:28:56,803] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:28:56,810] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.39665848315995345
[2019-03-26 00:29:12,219] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2969788], dtype=float32), -0.039532553]
[2019-03-26 00:29:12,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 80.0, 1.0, 2.0, 0.7110729730856661, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996102223625991, 6.9112, 168.9123823813732, 1890623.917533222, 1830391.562712225, 387446.1140069411]
[2019-03-26 00:29:12,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:29:12,224] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4209805e-11], sampled 0.748111774578998
[2019-03-26 00:29:12,225] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1890623.917533222 W.
[2019-03-26 00:29:54,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7469.3162 3145830388.2815 1710.0000
[2019-03-26 00:29:54,473] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7185.3736 3224357985.8970 2022.0000
[2019-03-26 00:29:54,584] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8000.9435 3030553694.9409 1242.0000
[2019-03-26 00:29:54,627] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7367.0411 3353261267.5742 1798.0000
[2019-03-26 00:29:54,796] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8122.7751 2983831169.2417 1068.0000
[2019-03-26 00:29:55,812] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1100000, evaluation results [1100000.0, 7367.0411029580455, 3353261267.57418, 1798.0, 7469.316179462675, 3145830388.2815433, 1710.0, 8122.775068680331, 2983831169.241653, 1068.0, 7185.37359316624, 3224357985.896973, 2022.0, 8000.943541813301, 3030553694.94091, 1242.0]
[2019-03-26 00:29:57,513] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1100630: loss 0.4772
[2019-03-26 00:29:57,518] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1100630: learning rate 0.0010
[2019-03-26 00:29:58,067] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1100836: loss 0.4491
[2019-03-26 00:29:58,069] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1100836: learning rate 0.0010
[2019-03-26 00:29:59,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.3478154e-37 0.0000000e+00 8.9978294e-15], sum to 1.0000
[2019-03-26 00:29:59,414] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0899
[2019-03-26 00:29:59,427] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1280277.610993213 W.
[2019-03-26 00:29:59,431] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.9148675636845297, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1280277.610993213, 1280277.610993212, 274287.8467081173], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1674000.0000, 
sim time next is 1674600.0000, 
raw observation next is [24.53333333333333, 93.50000000000001, 1.0, 2.0, 0.4123993040080229, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6875574600483397, 6.911199999999999, 6.9112, 168.912956510431, 1155978.621428414, 1155978.621428415, 260864.2926641621], 
processed observation next is [1.0, 0.391304347826087, 0.36176935229067925, 0.9350000000000002, 1.0, 1.0, 0.2920473542265336, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.6189725122540727, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3211051726190039, 0.3211051726190042, 0.38934969054352553], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.56143844], dtype=float32), -0.2537335]. 
=============================================
[2019-03-26 00:29:59,923] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1101514: loss 1.5625
[2019-03-26 00:29:59,926] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1101514: learning rate 0.0010
[2019-03-26 00:30:01,177] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1101989: loss 0.9034
[2019-03-26 00:30:01,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1101989: learning rate 0.0010
[2019-03-26 00:30:02,090] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1102323: loss 1.9620
[2019-03-26 00:30:02,093] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1102323: learning rate 0.0010
[2019-03-26 00:30:03,272] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1102763: loss 0.4468
[2019-03-26 00:30:03,278] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1102766: learning rate 0.0010
[2019-03-26 00:30:07,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:30:07,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5572
[2019-03-26 00:30:07,402] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1214894.055889733 W.
[2019-03-26 00:30:07,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [23.73333333333333, 85.00000000000001, 1.0, 2.0, 0.806514416988817, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1214894.055889733, 1214894.055889734, 257382.9892561467], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1595400.0000, 
sim time next is 1596000.0000, 
raw observation next is [23.76666666666667, 85.0, 1.0, 2.0, 0.3983212858900521, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6972939353215362, 6.911199999999999, 6.9112, 168.912956510431, 1199319.146286436, 1199319.146286436, 265683.1570896564], 
processed observation next is [1.0, 0.4782608695652174, 0.32543443917851517, 0.85, 1.0, 1.0, 0.2750858866145206, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.6308462625872393, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33314420730178773, 0.33314420730178773, 0.39654202550694984], 
reward next is 0.6035, 
noisyNet noise sample is [array([0.8417757], dtype=float32), 0.34363565]. 
=============================================
[2019-03-26 00:30:07,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[45.46799 ]
 [45.1083  ]
 [44.972286]
 [45.313366]
 [45.968628]], R is [[45.86728668]
 [46.02445984]
 [46.13523483]
 [45.67388153]
 [45.21714401]].
[2019-03-26 00:30:07,855] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1104449: loss 0.1920
[2019-03-26 00:30:07,857] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1104449: learning rate 0.0010
[2019-03-26 00:30:08,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:30:08,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1052
[2019-03-26 00:30:08,195] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.88333333333333, 96.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6418071237322004, 6.9112, 6.9112, 168.912956510431, 555663.2742153206, 555663.2742153206, 172723.1675326844], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1829400.0000, 
sim time next is 1830000.0000, 
raw observation next is [21.86666666666667, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6205052091592389, 6.9112, 6.9112, 168.912956510431, 537080.5314178695, 537080.5314178695, 169302.8495659686], 
processed observation next is [1.0, 0.17391304347826086, 0.23538704581358633, 0.9666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5372014745844377, 0.0, 0.0, 0.8294399451523027, 0.14918903650496376, 0.14918903650496376, 0.25269082024771433], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.23171376], dtype=float32), -0.0904471]. 
=============================================
[2019-03-26 00:30:08,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.9826 ]
 [68.21555]
 [68.36569]
 [68.46693]
 [68.54769]], R is [[67.89302826]
 [67.95629883]
 [68.02441406]
 [68.09210968]
 [68.15947723]].
[2019-03-26 00:30:09,582] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1105084: loss 0.0421
[2019-03-26 00:30:09,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1105084: learning rate 0.0010
[2019-03-26 00:30:11,355] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1105747: loss 0.2784
[2019-03-26 00:30:11,361] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1105747: learning rate 0.0010
[2019-03-26 00:30:11,736] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1105889: loss 0.0252
[2019-03-26 00:30:11,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1105889: learning rate 0.0010
[2019-03-26 00:30:12,030] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1105994: loss 0.2197
[2019-03-26 00:30:12,034] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1105994: learning rate 0.0010
[2019-03-26 00:30:12,667] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1106236: loss 0.0660
[2019-03-26 00:30:12,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1106236: learning rate 0.0010
[2019-03-26 00:30:13,157] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1106413: loss 1.1827
[2019-03-26 00:30:13,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1106413: learning rate 0.0010
[2019-03-26 00:30:13,640] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1106594: loss 1.2556
[2019-03-26 00:30:13,642] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1106594: learning rate 0.0010
[2019-03-26 00:30:13,685] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1106613: loss 0.3662
[2019-03-26 00:30:13,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1106614: learning rate 0.0010
[2019-03-26 00:30:17,419] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1107985: loss -103.9399
[2019-03-26 00:30:17,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1107985: learning rate 0.0010
[2019-03-26 00:30:19,815] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1108871: loss -76.0360
[2019-03-26 00:30:19,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1108871: learning rate 0.0010
[2019-03-26 00:30:19,927] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1108908: loss -67.8385
[2019-03-26 00:30:19,931] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1108910: learning rate 0.0010
[2019-03-26 00:30:21,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9985135e-01 1.8185243e-32 1.5276899e-28 4.2248927e-31 1.4861923e-04], sum to 1.0000
[2019-03-26 00:30:21,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8746
[2019-03-26 00:30:21,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 898518.9275780926 W.
[2019-03-26 00:30:21,471] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 92.66666666666667, 1.0, 2.0, 0.1917746041496196, 1.0, 2.0, 0.1917746041496196, 1.0, 2.0, 0.3436370381068549, 6.9112, 6.9112, 170.5573041426782, 898518.9275780926, 898518.9275780926, 278524.6850726874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1783200.0000, 
sim time next is 1783800.0000, 
raw observation next is [21.0, 93.0, 1.0, 2.0, 0.5729703164070502, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 904384.5954191234, 904384.5954191227, 207810.2639021624], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.93, 1.0, 1.0, 0.485506405309699, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2512179431719787, 0.25121794317197854, 0.3101645729883021], 
reward next is 0.6898, 
noisyNet noise sample is [array([0.6273273], dtype=float32), -0.6771537]. 
=============================================
[2019-03-26 00:30:21,858] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1109620: loss -96.9679
[2019-03-26 00:30:21,861] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1109621: learning rate 0.0010
[2019-03-26 00:30:23,114] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1110087: loss -8.3408
[2019-03-26 00:30:23,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1110089: learning rate 0.0010
[2019-03-26 00:30:23,853] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1110364: loss -16.3153
[2019-03-26 00:30:23,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1110364: learning rate 0.0010
[2019-03-26 00:30:25,191] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1110857: loss 68.9744
[2019-03-26 00:30:25,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1110857: learning rate 0.0010
[2019-03-26 00:30:26,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.6450408e-09 1.3705094e-36 2.1918965e-30 3.2685735e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 00:30:26,124] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4821
[2019-03-26 00:30:26,135] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.7, 86.66666666666666, 1.0, 2.0, 0.3845156629563086, 1.0, 2.0, 0.3845156629563086, 1.0, 2.0, 0.6472560866104969, 6.9112, 6.9112, 170.5573041426782, 1612584.353388657, 1612584.353388657, 341673.2647347876], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1856400.0000, 
sim time next is 1857000.0000, 
raw observation next is [25.8, 86.33333333333334, 1.0, 2.0, 0.3832089161146959, 1.0, 2.0, 0.3832089161146959, 1.0, 2.0, 0.645447590324868, 6.9112, 6.9112, 170.5573041426782, 1607100.000750551, 1607100.000750551, 341055.1949208584], 
processed observation next is [1.0, 0.4782608695652174, 0.42180094786729866, 0.8633333333333334, 1.0, 1.0, 0.2568782121863806, 1.0, 1.0, 0.2568782121863806, 1.0, 1.0, 0.5676190125913024, 0.0, 0.0, 0.8375144448122397, 0.4464166668751531, 0.4464166668751531, 0.5090376043594902], 
reward next is 0.4910, 
noisyNet noise sample is [array([-0.38203815], dtype=float32), -0.9423554]. 
=============================================
[2019-03-26 00:30:26,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[51.456985]
 [51.41673 ]
 [52.064144]
 [53.117073]
 [53.32138 ]], R is [[51.36972046]
 [51.34606171]
 [50.83260345]
 [50.82275009]
 [50.83912659]].
[2019-03-26 00:30:29,146] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:30:29,157] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1972
[2019-03-26 00:30:29,165] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7625662225229773, 6.9112, 6.9112, 168.912956510431, 644460.7817445411, 644460.7817445411, 194408.9175393443], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1900800.0000, 
sim time next is 1901400.0000, 
raw observation next is [24.3, 93.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7621828167803159, 6.911200000000001, 6.9112, 168.912956510431, 643957.0982953869, 643957.0982953862, 194331.1111918032], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.9333333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7099790448540436, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17887697174871858, 0.1788769717487184, 0.29004643461463164], 
reward next is 0.7100, 
noisyNet noise sample is [array([-2.0437782], dtype=float32), 0.4324938]. 
=============================================
[2019-03-26 00:30:29,823] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1112565: loss -15.1867
[2019-03-26 00:30:29,826] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1112566: learning rate 0.0010
[2019-03-26 00:30:31,981] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1113362: loss 58.4251
[2019-03-26 00:30:31,986] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1113363: learning rate 0.0010
[2019-03-26 00:30:33,404] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1113887: loss -234.9176
[2019-03-26 00:30:33,406] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1113889: learning rate 0.0010
[2019-03-26 00:30:33,491] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1113919: loss -41.6096
[2019-03-26 00:30:33,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1113920: learning rate 0.0010
[2019-03-26 00:30:34,216] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1114184: loss 48.2464
[2019-03-26 00:30:34,221] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1114186: learning rate 0.0010
[2019-03-26 00:30:34,747] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1114385: loss 20.1282
[2019-03-26 00:30:34,749] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1114385: learning rate 0.0010
[2019-03-26 00:30:35,213] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1114556: loss -145.3334
[2019-03-26 00:30:35,216] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1114556: learning rate 0.0010
[2019-03-26 00:30:35,708] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1114735: loss 16.5365
[2019-03-26 00:30:35,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1114736: learning rate 0.0010
[2019-03-26 00:30:35,840] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1114784: loss 51.1355
[2019-03-26 00:30:35,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1114784: learning rate 0.0010
[2019-03-26 00:30:36,319] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:30:36,333] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1124
[2019-03-26 00:30:36,342] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.43333333333333, 74.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004744259309999, 6.911200000000001, 6.9112, 168.9128325462855, 808990.0455562797, 808990.0455562791, 248223.9240908657], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2136000.0000, 
sim time next is 2136600.0000, 
raw observation next is [30.25, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9873668205235482, 6.911200000000001, 6.9112, 168.912956510431, 795774.716345593, 795774.7163455924, 243813.056566872], 
processed observation next is [0.0, 0.7391304347826086, 0.6327014218009479, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9845936835653026, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22104853231822028, 0.2210485323182201, 0.3639000844281672], 
reward next is 0.6361, 
noisyNet noise sample is [array([-0.15938121], dtype=float32), -0.46662962]. 
=============================================
[2019-03-26 00:30:38,995] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1115948: loss -62.9402
[2019-03-26 00:30:38,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1115948: learning rate 0.0010
[2019-03-26 00:30:41,358] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1116820: loss -81.5098
[2019-03-26 00:30:41,359] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1116821: learning rate 0.0010
[2019-03-26 00:30:41,550] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1116892: loss -155.1633
[2019-03-26 00:30:41,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1116894: learning rate 0.0010
[2019-03-26 00:30:42,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:30:42,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7759
[2019-03-26 00:30:42,580] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.25, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7936510098750925, 6.911199999999999, 6.9112, 168.912956510431, 665801.3348068182, 665801.3348068189, 200544.0683564257], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2061000.0000, 
sim time next is 2061600.0000, 
raw observation next is [25.2, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7934565015557233, 6.9112, 6.9112, 168.912956510431, 665684.0783574794, 665684.0783574794, 200505.3292474944], 
processed observation next is [0.0, 0.8695652173913043, 0.3933649289099526, 0.9033333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7481176848240528, 0.0, 0.0, 0.8294399451523027, 0.18491224398818873, 0.18491224398818873, 0.2992616854440215], 
reward next is 0.7007, 
noisyNet noise sample is [array([0.2493991], dtype=float32), 0.10423068]. 
=============================================
[2019-03-26 00:30:43,285] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1117535: loss -60.0328
[2019-03-26 00:30:43,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1117536: learning rate 0.0010
[2019-03-26 00:30:44,705] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1118058: loss -62.9997
[2019-03-26 00:30:44,708] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1118058: learning rate 0.0010
[2019-03-26 00:30:45,176] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1118232: loss -119.9640
[2019-03-26 00:30:45,180] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1118233: learning rate 0.0010
[2019-03-26 00:30:46,666] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1118779: loss -141.5047
[2019-03-26 00:30:46,668] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1118779: learning rate 0.0010
[2019-03-26 00:30:49,862] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 1.4539627e-33 1.4282892e-27 1.3117015e-28 9.5505431e-14], sum to 1.0000
[2019-03-26 00:30:49,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7077
[2019-03-26 00:30:49,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2512687.582678727 W.
[2019-03-26 00:30:49,885] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 0.5989030023082831, 1.0, 2.0, 0.5989030023082831, 1.0, 1.0, 1.03, 6.922549922946173, 6.9112, 170.5573041426782, 2512687.582678727, 2504557.171187037, 487580.5650847551], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [31.9, 65.66666666666666, 1.0, 2.0, 0.8743944040492946, 1.0, 2.0, 0.8743944040492946, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2445605.86805163, 2445605.86805163, 457715.6750109681], 
processed observation next is [1.0, 0.5652173913043478, 0.7109004739336492, 0.6566666666666666, 1.0, 1.0, 0.8486679566858971, 1.0, 1.0, 0.8486679566858971, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.679334963347675, 0.679334963347675, 0.6831577238969674], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06218506], dtype=float32), -0.9434173]. 
=============================================
[2019-03-26 00:30:51,031] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1120388: loss -64.5050
[2019-03-26 00:30:51,033] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1120389: learning rate 0.0010
[2019-03-26 00:30:53,057] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1121138: loss -22.3645
[2019-03-26 00:30:53,065] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1121139: learning rate 0.0010
[2019-03-26 00:30:54,518] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1121681: loss -17.6122
[2019-03-26 00:30:54,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1121682: learning rate 0.0010
[2019-03-26 00:30:54,585] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1121703: loss -43.2767
[2019-03-26 00:30:54,590] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1121704: learning rate 0.0010
[2019-03-26 00:30:55,100] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1121890: loss -41.7646
[2019-03-26 00:30:55,103] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1121890: learning rate 0.0010
[2019-03-26 00:30:55,616] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1122081: loss -41.2892
[2019-03-26 00:30:55,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1122081: learning rate 0.0010
[2019-03-26 00:30:56,143] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1122275: loss -69.1434
[2019-03-26 00:30:56,144] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1122275: learning rate 0.0010
[2019-03-26 00:30:56,311] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1122335: loss -93.5919
[2019-03-26 00:30:56,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1122336: learning rate 0.0010
[2019-03-26 00:30:56,611] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1122442: loss -91.4644
[2019-03-26 00:30:56,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1122443: learning rate 0.0010
[2019-03-26 00:30:59,896] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 8.2065944e-31 2.0341994e-22 9.1290619e-32 8.3987942e-11], sum to 1.0000
[2019-03-26 00:30:59,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8287
[2019-03-26 00:30:59,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2295224.739934429 W.
[2019-03-26 00:30:59,927] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 64.0, 1.0, 2.0, 0.8206769292365365, 1.0, 1.0, 0.8206769292365365, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2295224.739934429, 2295224.739934429, 430066.3801139126], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2306400.0000, 
sim time next is 2307000.0000, 
raw observation next is [32.36666666666667, 64.0, 1.0, 2.0, 0.8239645086774893, 1.0, 2.0, 0.8239645086774893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2304427.741037451, 2304427.741037451, 431710.0052327363], 
processed observation next is [1.0, 0.6956521739130435, 0.7330173775671407, 0.64, 1.0, 1.0, 0.7879090465993847, 1.0, 1.0, 0.7879090465993847, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6401188169548475, 0.6401188169548475, 0.6443432913921437], 
reward next is 0.3557, 
noisyNet noise sample is [array([-2.4690278], dtype=float32), -0.75244004]. 
=============================================
[2019-03-26 00:30:59,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[52.9295  ]
 [52.83628 ]
 [52.636505]
 [52.040546]
 [51.516815]], R is [[52.94733047]
 [52.41785812]
 [51.89368057]
 [51.71168137]
 [51.51047897]].
[2019-03-26 00:31:01,637] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1124296: loss 1.1650
[2019-03-26 00:31:01,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1124297: learning rate 0.0010
[2019-03-26 00:31:03,112] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.858994e-21], sum to 1.0000
[2019-03-26 00:31:03,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2048
[2019-03-26 00:31:03,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2018938.161852813 W.
[2019-03-26 00:31:03,134] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.7, 84.0, 1.0, 2.0, 0.4813182508540366, 1.0, 2.0, 0.4813182508540366, 1.0, 2.0, 0.8297084650534472, 6.9112, 6.9112, 170.5573041426782, 2018938.161852813, 2018938.161852813, 401396.3797292302], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2476800.0000, 
sim time next is 2477400.0000, 
raw observation next is [27.81666666666666, 83.66666666666667, 1.0, 2.0, 0.5122715504582567, 1.0, 2.0, 0.5122715504582567, 1.0, 2.0, 0.884607398040516, 6.9112, 6.9112, 170.5573041426782, 2148905.139912625, 2148905.139912625, 422800.5632008021], 
processed observation next is [1.0, 0.6956521739130435, 0.5173775671406, 0.8366666666666667, 1.0, 1.0, 0.4123753619978996, 1.0, 1.0, 0.4123753619978996, 1.0, 1.0, 0.8592773146835562, 0.0, 0.0, 0.8375144448122397, 0.5969180944201735, 0.5969180944201735, 0.631045616717615], 
reward next is 0.3690, 
noisyNet noise sample is [array([1.2987683], dtype=float32), 1.3604573]. 
=============================================
[2019-03-26 00:31:03,539] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 00:31:03,541] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:31:03,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:31:03,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:31:03,543] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:31:03,544] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:31:03,544] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:31:03,546] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:31:03,547] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:31:03,544] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:31:03,550] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:31:03,580] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-26 00:31:03,581] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-26 00:31:03,601] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-26 00:31:03,624] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-26 00:31:03,673] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-26 00:31:10,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2844586], dtype=float32), -0.031217227]
[2019-03-26 00:31:10,657] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.18842945666667, 72.87171755666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4626242863408201, 6.911200000000001, 6.9112, 168.912956510431, 415773.7128832028, 415773.7128832022, 147148.5251221147]
[2019-03-26 00:31:10,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:31:10,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07621876629485214
[2019-03-26 00:31:32,304] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.2844586], dtype=float32), -0.031217227]
[2019-03-26 00:31:32,305] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.65, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8572498837733227, 6.911199999999999, 6.9112, 168.912956510431, 710266.2688861516, 710266.2688861522, 213833.8479584584]
[2019-03-26 00:31:32,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:31:32,312] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8852979636200475
[2019-03-26 00:31:51,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2844586], dtype=float32), -0.031217227]
[2019-03-26 00:31:51,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.925309183574173, 6.9112, 168.9127069556215, 1463771.308409513, 1453761.782769634, 311349.8930592102]
[2019-03-26 00:31:51,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:31:51,432] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8637383e-09 0.0000000e+00 2.4765545e-37 0.0000000e+00 1.0000000e+00], sampled 0.2677325030784051
[2019-03-26 00:32:33,783] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2844586], dtype=float32), -0.031217227]
[2019-03-26 00:32:33,785] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.29022741, 55.82524437, 1.0, 2.0, 0.6367590284663659, 1.0, 1.0, 0.6367590284663659, 1.0, 2.0, 1.03, 6.99598104970617, 6.9112, 171.5212843490159, 2671665.665410365, 2610590.296367312, 501901.7672577099]
[2019-03-26 00:32:33,786] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:32:33,789] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.94873e-15 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00], sampled 0.7047387698456002
[2019-03-26 00:33:11,927] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7593.6880 3269571081.3851 1072.0000
[2019-03-26 00:33:12,901] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8056.5516 3086079756.3255 780.0000
[2019-03-26 00:33:13,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8179.3240 3047412766.5108 695.0000
[2019-03-26 00:33:13,347] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7758.7897 3204605440.8698 1090.0000
[2019-03-26 00:33:13,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7588.1758 3386954874.4138 1203.0000
[2019-03-26 00:33:14,410] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1125000, evaluation results [1125000.0, 7588.175792081992, 3386954874.4138465, 1203.0, 7758.789721979338, 3204605440.8698196, 1090.0, 8179.323971362713, 3047412766.5108156, 695.0, 7593.68799598687, 3269571081.385125, 1072.0, 8056.551562509116, 3086079756.3255343, 780.0]
[2019-03-26 00:33:14,440] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1125015: loss -14.9752
[2019-03-26 00:33:14,443] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1125015: learning rate 0.0010
[2019-03-26 00:33:14,822] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.4905725e-34 0.0000000e+00 2.8530232e-35 9.1169787e-11], sum to 1.0000
[2019-03-26 00:33:14,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6651
[2019-03-26 00:33:14,842] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1150617.558664957 W.
[2019-03-26 00:33:14,849] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.36666666666667, 72.5, 1.0, 2.0, 0.2744202233892558, 1.0, 2.0, 0.2744202233892558, 1.0, 2.0, 0.4697504748004176, 6.9112, 6.9112, 170.5573041426782, 1150617.558664957, 1150617.558664957, 293679.1154863975], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2361000.0000, 
sim time next is 2361600.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.2851394961259076, 1.0, 2.0, 0.2851394961259076, 1.0, 2.0, 0.4884968825600965, 6.9112, 6.9112, 170.5573041426782, 1195587.496443719, 1195587.496443719, 297810.9293401486], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.72, 1.0, 1.0, 0.1387222844890453, 1.0, 1.0, 0.1387222844890453, 1.0, 1.0, 0.37621571043914204, 0.0, 0.0, 0.8375144448122397, 0.33210763790103304, 0.33210763790103304, 0.44449392438828145], 
reward next is 0.5555, 
noisyNet noise sample is [array([-0.44740015], dtype=float32), 0.44575733]. 
=============================================
[2019-03-26 00:33:14,959] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1125210: loss 11.4500
[2019-03-26 00:33:14,961] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1125210: learning rate 0.0010
[2019-03-26 00:33:16,489] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1125778: loss -27.5342
[2019-03-26 00:33:16,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1125778: learning rate 0.0010
[2019-03-26 00:33:17,510] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1126150: loss -68.8083
[2019-03-26 00:33:17,513] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1126151: learning rate 0.0010
[2019-03-26 00:33:18,193] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1126404: loss -24.2411
[2019-03-26 00:33:18,196] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1126404: learning rate 0.0010
[2019-03-26 00:33:19,782] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1126991: loss 38.6619
[2019-03-26 00:33:19,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1126991: learning rate 0.0010
[2019-03-26 00:33:23,696] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1128419: loss -5.6163
[2019-03-26 00:33:23,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1128421: learning rate 0.0010
[2019-03-26 00:33:26,356] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1129408: loss 1.0202
[2019-03-26 00:33:26,363] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1129408: learning rate 0.0010
[2019-03-26 00:33:26,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3946488e-13], sum to 1.0000
[2019-03-26 00:33:26,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1517
[2019-03-26 00:33:26,851] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.16666666666667, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7929453890390153, 6.9112, 6.9112, 168.912956510431, 666012.16823512, 666012.16823512, 200416.8342215048], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2656200.0000, 
sim time next is 2656800.0000, 
raw observation next is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7815811430695767, 6.9112, 6.9112, 168.912956510431, 657914.0540058315, 657914.0540058315, 198142.4844366252], 
processed observation next is [0.0, 0.782608695652174, 0.38388625592417064, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7336355403287521, 0.0, 0.0, 0.8294399451523027, 0.18275390389050875, 0.18275390389050875, 0.2957350513979481], 
reward next is 0.7043, 
noisyNet noise sample is [array([-2.0403125], dtype=float32), -2.0352948]. 
=============================================
[2019-03-26 00:33:27,634] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1129885: loss -115.8767
[2019-03-26 00:33:27,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1129885: learning rate 0.0010
[2019-03-26 00:33:27,717] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1129917: loss -55.5059
[2019-03-26 00:33:27,721] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1129917: learning rate 0.0010
[2019-03-26 00:33:28,291] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1130128: loss -97.4026
[2019-03-26 00:33:28,294] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1130128: learning rate 0.0010
[2019-03-26 00:33:28,900] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1130351: loss -77.9345
[2019-03-26 00:33:28,904] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1130351: learning rate 0.0010
[2019-03-26 00:33:29,309] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1130500: loss -127.1671
[2019-03-26 00:33:29,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1130500: learning rate 0.0010
[2019-03-26 00:33:29,493] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1130562: loss -51.7763
[2019-03-26 00:33:29,495] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1130563: learning rate 0.0010
[2019-03-26 00:33:29,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4973454e-04 7.6163145e-23 1.5396667e-20 5.5349555e-21 9.9955028e-01], sum to 1.0000
[2019-03-26 00:33:29,654] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5048
[2019-03-26 00:33:29,659] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.83333333333333, 83.0, 1.0, 2.0, 0.3521686211652141, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6186502617713199, 6.9112, 6.9112, 168.912956510431, 1065971.931657764, 1065971.931657764, 246882.5232388547], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2807400.0000, 
sim time next is 2808000.0000, 
raw observation next is [24.0, 83.0, 1.0, 2.0, 0.2047688503367286, 1.0, 1.0, 0.2047688503367286, 1.0, 2.0, 0.3558862889424898, 6.9112, 6.9112, 170.5573041426782, 914622.0301366207, 914622.0301366207, 277573.0859100446], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.83, 1.0, 1.0, 0.04189018112858865, 1.0, 0.5, 0.04189018112858865, 1.0, 1.0, 0.2144954743201095, 0.0, 0.0, 0.8375144448122397, 0.2540616750379502, 0.2540616750379502, 0.4142881879254397], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.91157573], dtype=float32), 1.7218792]. 
=============================================
[2019-03-26 00:33:29,675] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[35.931477]
 [36.221954]
 [37.718994]
 [38.030792]
 [40.064514]], R is [[36.10347366]
 [35.74243927]
 [35.38501358]
 [35.03116226]
 [34.68085098]].
[2019-03-26 00:33:29,682] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:33:29,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1209
[2019-03-26 00:33:29,700] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.2, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9204389575133246, 6.9112, 6.9112, 168.912956510431, 752258.7711236184, 752258.7711236184, 227920.2330383236], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2573400.0000, 
sim time next is 2574000.0000, 
raw observation next is [28.1, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9194851480014117, 6.911199999999999, 6.9112, 168.912956510431, 751440.7486453493, 751440.74864535, 227692.3546728402], 
processed observation next is [1.0, 0.8260869565217391, 0.5308056872037916, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9018111560992825, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2087335412903748, 0.208733541290375, 0.33983933533259736], 
reward next is 0.6602, 
noisyNet noise sample is [array([0.73229986], dtype=float32), -2.104547]. 
=============================================
[2019-03-26 00:33:29,707] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1130642: loss -51.1451
[2019-03-26 00:33:29,717] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1130643: learning rate 0.0010
[2019-03-26 00:33:29,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[58.291985]
 [57.871117]
 [57.637947]
 [57.085735]
 [56.269295]], R is [[58.43000793]
 [58.5055275 ]
 [58.57939911]
 [58.65280533]
 [58.72629166]].
[2019-03-26 00:33:30,746] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:33:30,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8266
[2019-03-26 00:33:30,764] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.43333333333333, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7730697653591753, 6.9112, 6.9112, 168.912956510431, 652311.1562969303, 652311.1562969303, 196467.4503987052], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2598000.0000, 
sim time next is 2598600.0000, 
raw observation next is [24.36666666666667, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7672693677256743, 6.9112, 6.9112, 168.912956510431, 648163.2218856651, 648163.2218856651, 195330.0119228926], 
processed observation next is [0.0, 0.043478260869565216, 0.3538704581358612, 0.9216666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7161821557630175, 0.0, 0.0, 0.8294399451523027, 0.18004533941268475, 0.18004533941268475, 0.2915373312281979], 
reward next is 0.7085, 
noisyNet noise sample is [array([0.2001601], dtype=float32), 0.4840435]. 
=============================================
[2019-03-26 00:33:33,264] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1131951: loss 0.1624
[2019-03-26 00:33:33,272] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1131954: learning rate 0.0010
[2019-03-26 00:33:35,509] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1132773: loss 0.2666
[2019-03-26 00:33:35,513] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1132773: learning rate 0.0010
[2019-03-26 00:33:36,180] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1133022: loss 0.3852
[2019-03-26 00:33:36,185] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1133023: learning rate 0.0010
[2019-03-26 00:33:37,755] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1133612: loss 0.0487
[2019-03-26 00:33:37,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1133613: learning rate 0.0010
[2019-03-26 00:33:38,724] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1133968: loss 0.3847
[2019-03-26 00:33:38,728] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1133969: learning rate 0.0010
[2019-03-26 00:33:39,395] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1134213: loss 0.1061
[2019-03-26 00:33:39,397] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1134214: learning rate 0.0010
[2019-03-26 00:33:41,204] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1134871: loss 0.0562
[2019-03-26 00:33:41,208] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1134873: learning rate 0.0010
[2019-03-26 00:33:45,122] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1136328: loss 0.0901
[2019-03-26 00:33:45,124] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1136329: learning rate 0.0010
[2019-03-26 00:33:47,778] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1137311: loss 0.6090
[2019-03-26 00:33:47,779] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1137311: learning rate 0.0010
[2019-03-26 00:33:49,083] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1137791: loss 0.7922
[2019-03-26 00:33:49,084] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1137791: learning rate 0.0010
[2019-03-26 00:33:49,355] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1137889: loss 0.0685
[2019-03-26 00:33:49,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1137889: learning rate 0.0010
[2019-03-26 00:33:49,761] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1138038: loss 11.1496
[2019-03-26 00:33:49,766] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1138041: learning rate 0.0010
[2019-03-26 00:33:50,467] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1138303: loss 0.5318
[2019-03-26 00:33:50,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1138304: learning rate 0.0010
[2019-03-26 00:33:50,728] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1138395: loss 3.7415
[2019-03-26 00:33:50,732] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1138396: learning rate 0.0010
[2019-03-26 00:33:51,007] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1138497: loss 0.9115
[2019-03-26 00:33:51,008] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1138497: learning rate 0.0010
[2019-03-26 00:33:51,297] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1138605: loss 1.1241
[2019-03-26 00:33:51,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1138605: learning rate 0.0010
[2019-03-26 00:33:54,942] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1139959: loss 1.1567
[2019-03-26 00:33:54,943] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1139959: learning rate 0.0010
[2019-03-26 00:33:56,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 00:33:56,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7062
[2019-03-26 00:33:56,495] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1922495982002814, 6.911199999999999, 6.9112, 170.5573041426782, 503161.0419662639, 503161.0419662645, 230616.4663284014], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2932800.0000, 
sim time next is 2933400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1916536952025229, 6.9112, 6.9112, 170.5573041426782, 501622.5798818709, 501622.5798818709, 230402.8310041963], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.94, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.01421182341771086, 0.0, 0.0, 0.8375144448122397, 0.1393396055227419, 0.1393396055227419, 0.34388482239432283], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17821693], dtype=float32), -0.7728314]. 
=============================================
[2019-03-26 00:33:56,867] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.4471386e-31], sum to 1.0000
[2019-03-26 00:33:56,883] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1355
[2019-03-26 00:33:56,888] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.563856735355342, 6.9112, 6.9112, 168.912956510431, 493290.6173517926, 493290.6173517926, 160708.5195984574], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2960400.0000, 
sim time next is 2961000.0000, 
raw observation next is [21.0, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5661534382323817, 6.9112, 6.9112, 168.912956510431, 494738.7411713288, 494738.7411713288, 161049.3877021247], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47091882711266064, 0.0, 0.0, 0.8294399451523027, 0.1374274281031469, 0.1374274281031469, 0.24037222045093237], 
reward next is 0.7596, 
noisyNet noise sample is [array([-0.98964834], dtype=float32), -1.3661761]. 
=============================================
[2019-03-26 00:33:56,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.27001 ]
 [61.196888]
 [61.304756]
 [61.201767]
 [61.169777]], R is [[61.44009399]
 [61.58583069]
 [61.72503281]
 [61.86994553]
 [62.01330566]].
[2019-03-26 00:33:57,079] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4260962e-18 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 00:33:57,093] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9032
[2019-03-26 00:33:57,100] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1880491730258058, 6.9112, 6.9112, 170.5573041426782, 492866.5747112428, 492866.5747112428, 229119.8922313005], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2935800.0000, 
sim time next is 2936400.0000, 
raw observation next is [20.33333333333333, 98.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1868774501657997, 6.911199999999999, 6.9112, 170.5573041426782, 490051.5666387474, 490051.566638748, 228702.6455115043], 
processed observation next is [1.0, 1.0, 0.16271721958925733, 0.98, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.008387134348536237, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.13612543517742984, 0.13612543517743, 0.3413472321067228], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46133652], dtype=float32), -1.0020275]. 
=============================================
[2019-03-26 00:33:57,453] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1140886: loss 5.5389
[2019-03-26 00:33:57,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1140887: learning rate 0.0010
[2019-03-26 00:33:57,967] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1141074: loss -10.4015
[2019-03-26 00:33:57,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1141074: learning rate 0.0010
[2019-03-26 00:33:59,438] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1141617: loss -10.9276
[2019-03-26 00:33:59,441] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1141617: learning rate 0.0010
[2019-03-26 00:34:00,484] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1142001: loss -74.2691
[2019-03-26 00:34:00,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1142003: learning rate 0.0010
[2019-03-26 00:34:01,128] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3910343e-30], sum to 1.0000
[2019-03-26 00:34:01,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9231
[2019-03-26 00:34:01,149] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5642705629580144, 6.911199999999999, 6.9112, 168.912956510431, 494736.1809543727, 494736.1809543734, 160738.2997188341], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3003600.0000, 
sim time next is 3004200.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5629044365396012, 6.9112, 6.9112, 168.912956510431, 494349.0540474131, 494349.0540474131, 160520.3795503456], 
processed observation next is [1.0, 0.782608695652174, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4669566299263429, 0.0, 0.0, 0.8294399451523027, 0.13731918167983698, 0.13731918167983698, 0.23958265604529191], 
reward next is 0.7604, 
noisyNet noise sample is [array([0.9616807], dtype=float32), 0.7512045]. 
=============================================
[2019-03-26 00:34:01,192] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1142261: loss 10.9624
[2019-03-26 00:34:01,194] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1142261: learning rate 0.0010
[2019-03-26 00:34:02,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.921203e-36], sum to 1.0000
[2019-03-26 00:34:02,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9733
[2019-03-26 00:34:02,059] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5521063030133988, 6.9112, 6.9112, 168.912956510431, 485182.2398720158, 485182.2398720158, 158994.6079524672], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3012600.0000, 
sim time next is 3013200.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.552176253364404, 6.9112, 6.9112, 168.912956510431, 485243.725406528, 485243.725406528, 159004.3249533306], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4538734797126878, 0.0, 0.0, 0.8294399451523027, 0.13478992372403556, 0.13478992372403556, 0.23731988799004566], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.9165762], dtype=float32), -0.63045913]. 
=============================================
[2019-03-26 00:34:02,460] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:34:02,471] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1655
[2019-03-26 00:34:02,477] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.546095196824115, 6.911200000000001, 6.9112, 168.912956510431, 479898.5322266188, 479898.5322266183, 158164.5186022875], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3031200.0000, 
sim time next is 3031800.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6413166892571049, 6.9112, 6.9112, 168.912956510431, 563599.6850036548, 563599.6850036548, 172459.4848754251], 
processed observation next is [1.0, 0.08695652173913043, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5625813283623231, 0.0, 0.0, 0.8294399451523027, 0.15655546805657078, 0.15655546805657078, 0.2574022162319778], 
reward next is 0.7426, 
noisyNet noise sample is [array([1.1434819], dtype=float32), -0.043414623]. 
=============================================
[2019-03-26 00:34:02,753] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1142832: loss -25.8238
[2019-03-26 00:34:02,760] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1142833: learning rate 0.0010
[2019-03-26 00:34:06,832] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1144357: loss 0.5567
[2019-03-26 00:34:06,835] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1144358: learning rate 0.0010
[2019-03-26 00:34:07,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.5773254e-17], sum to 1.0000
[2019-03-26 00:34:07,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9390
[2019-03-26 00:34:07,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [33.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.934405217967551, 6.9112, 168.9126206664132, 845269.8874633685, 828807.3399072237, 254812.191233431], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3252600.0000, 
sim time next is 3253200.0000, 
raw observation next is [33.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.949854983093822, 6.9112, 168.9125200082087, 856234.7210814708, 828811.616539412, 254812.1919085951], 
processed observation next is [0.0, 0.6521739130434783, 0.7630331753554502, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.003865498309382165, 0.0, 0.8294378017265596, 0.23784297807818636, 0.23022544903872555, 0.3803167043411867], 
reward next is 0.4264, 
noisyNet noise sample is [array([1.668174], dtype=float32), 0.27987766]. 
=============================================
[2019-03-26 00:34:09,518] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1145351: loss -16.6825
[2019-03-26 00:34:09,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1145351: learning rate 0.0010
[2019-03-26 00:34:10,859] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1145842: loss 6.2490
[2019-03-26 00:34:10,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1145842: learning rate 0.0010
[2019-03-26 00:34:11,164] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1145953: loss 42.0527
[2019-03-26 00:34:11,167] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1145953: learning rate 0.0010
[2019-03-26 00:34:11,578] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1146110: loss 139.8241
[2019-03-26 00:34:11,582] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1146110: learning rate 0.0010
[2019-03-26 00:34:12,146] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1146320: loss -24.6836
[2019-03-26 00:34:12,150] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1146321: learning rate 0.0010
[2019-03-26 00:34:12,387] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1146407: loss 43.3391
[2019-03-26 00:34:12,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1146407: learning rate 0.0010
[2019-03-26 00:34:12,829] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1146575: loss 40.9392
[2019-03-26 00:34:12,834] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1146576: learning rate 0.0010
[2019-03-26 00:34:13,039] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1146651: loss 137.8793
[2019-03-26 00:34:13,043] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1146652: learning rate 0.0010
[2019-03-26 00:34:16,932] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1148091: loss 8.5491
[2019-03-26 00:34:16,936] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1148092: learning rate 0.0010
[2019-03-26 00:34:19,337] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1148981: loss 5.6244
[2019-03-26 00:34:19,340] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1148982: learning rate 0.0010
[2019-03-26 00:34:19,748] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1149134: loss 5.4015
[2019-03-26 00:34:19,753] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1149135: learning rate 0.0010
[2019-03-26 00:34:21,075] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1149623: loss 6.9847
[2019-03-26 00:34:21,080] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1149624: learning rate 0.0010
[2019-03-26 00:34:22,079] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 00:34:22,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:34:22,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:34:22,083] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:34:22,084] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:34:22,084] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:34:22,087] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:34:22,087] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:34:22,087] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:34:22,088] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:34:22,092] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:34:22,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-26 00:34:22,135] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-26 00:34:22,173] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-26 00:34:22,208] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-26 00:34:22,231] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-26 00:34:34,536] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:34:34,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [15.09681328, 85.86739436666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3024156689353249, 6.911200000000001, 6.9112, 168.912956510431, 274170.4904750024, 274170.4904750018, 91588.09936567013]
[2019-03-26 00:34:34,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:34:34,542] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9101850358617214
[2019-03-26 00:34:38,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:34:38,710] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.2, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5010429786352756, 6.911199999999999, 6.9112, 168.912956510431, 445280.5397908872, 445280.5397908878, 152048.0169288224]
[2019-03-26 00:34:38,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:34:38,713] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.023754622464540742
[2019-03-26 00:34:45,858] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:34:45,859] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.64064256, 98.100416005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6561575885303782, 6.9112, 6.9112, 168.912956510431, 565693.1850158037, 565693.1850158037, 175119.3951089783]
[2019-03-26 00:34:45,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:34:45,867] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9437059276710197
[2019-03-26 00:35:31,560] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:35:31,564] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 53.0, 1.0, 2.0, 0.9287857316148741, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005989503954179, 6.9112, 168.912393085716, 2195328.728439494, 2128082.016239799, 442168.1020474049]
[2019-03-26 00:35:31,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:35:31,569] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.06706323081829413
[2019-03-26 00:35:31,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2195328.728439494 W.
[2019-03-26 00:35:34,584] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:35:34,586] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.05116978, 83.2857358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9177772542769757, 6.911200000000001, 6.9112, 168.9129565085227, 755459.029267475, 755459.0292674744, 227517.254868678]
[2019-03-26 00:35:34,589] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:35:34,592] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9670262499987213
[2019-03-26 00:35:38,684] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:35:38,684] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.77374347666667, 67.93700656666667, 1.0, 2.0, 0.7208881249690311, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976587736551, 6.9112, 168.9123160495995, 1904359.079198551, 1837121.560842744, 389251.4247809312]
[2019-03-26 00:35:38,685] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:35:38,690] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32190453323683654
[2019-03-26 00:35:38,692] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1904359.079198551 W.
[2019-03-26 00:36:09,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1904039], dtype=float32), -0.08839242]
[2019-03-26 00:36:09,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.83333333333334, 74.33333333333334, 1.0, 2.0, 0.7305954289323617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104096, 1021048.483245444, 1021048.483245445, 227589.379171185]
[2019-03-26 00:36:09,053] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:36:09,055] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.835844e-26], sampled 0.08818888060480679
[2019-03-26 00:36:09,056] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1021048.483245444 W.
[2019-03-26 00:36:30,616] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7064.9791 3186085200.5953 2392.0000
[2019-03-26 00:36:31,370] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8082.9224 2941438710.0555 1326.0000
[2019-03-26 00:36:31,705] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7361.8901 3108421958.3470 1985.0000
[2019-03-26 00:36:31,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7313.7384 3322101038.9168 2054.0000
[2019-03-26 00:36:31,880] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7965.2159 2990909372.9565 1442.0000
[2019-03-26 00:36:32,896] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1150000, evaluation results [1150000.0, 7313.738396894626, 3322101038.9167867, 2054.0, 7361.890107914128, 3108421958.3470325, 1985.0, 8082.922418530414, 2941438710.0554924, 1326.0, 7064.979060465459, 3186085200.595343, 2392.0, 7965.2158856153155, 2990909372.956494, 1442.0]
[2019-03-26 00:36:32,994] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1150039: loss 5.8449
[2019-03-26 00:36:32,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1150039: learning rate 0.0010
[2019-03-26 00:36:33,734] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1150311: loss 5.4258
[2019-03-26 00:36:33,738] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1150313: learning rate 0.0010
[2019-03-26 00:36:35,366] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1150920: loss 8.6428
[2019-03-26 00:36:35,368] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1150920: learning rate 0.0010
[2019-03-26 00:36:36,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:36,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1425
[2019-03-26 00:36:36,504] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9471814912409141, 6.9112, 6.9112, 168.912956510431, 770885.1992047692, 770885.1992047692, 234206.8818529312], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3531600.0000, 
sim time next is 3532200.0000, 
raw observation next is [28.83333333333334, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9408097141296972, 6.9112, 6.9112, 168.912956510431, 766528.7252994437, 766528.7252994437, 232696.790167631], 
processed observation next is [1.0, 0.9130434782608695, 0.5655608214849924, 0.7900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9278167245484111, 0.0, 0.0, 0.8294399451523027, 0.21292464591651214, 0.21292464591651214, 0.3473086420412403], 
reward next is 0.6527, 
noisyNet noise sample is [array([0.8009127], dtype=float32), -1.4794359]. 
=============================================
[2019-03-26 00:36:39,166] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1152323: loss 5.9007
[2019-03-26 00:36:39,173] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1152324: learning rate 0.0010
[2019-03-26 00:36:41,432] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1153163: loss 6.9794
[2019-03-26 00:36:41,435] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1153163: learning rate 0.0010
[2019-03-26 00:36:42,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:42,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0991
[2019-03-26 00:36:42,474] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8801908828328628, 6.9112, 6.9112, 168.912956510431, 726882.0397544084, 726882.0397544084, 218890.5588649314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3622200.0000, 
sim time next is 3622800.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782389066794034, 6.9112, 6.9112, 168.912956510431, 725325.3483750876, 725325.3483750876, 218449.9776626426], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8515108618041505, 0.0, 0.0, 0.8294399451523027, 0.20147926343752434, 0.20147926343752434, 0.32604474278006357], 
reward next is 0.6740, 
noisyNet noise sample is [array([0.01579242], dtype=float32), -0.5484061]. 
=============================================
[2019-03-26 00:36:42,684] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1153614: loss 5.8800
[2019-03-26 00:36:42,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1153614: learning rate 0.0010
[2019-03-26 00:36:42,787] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1153654: loss 5.2185
[2019-03-26 00:36:42,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1153654: learning rate 0.0010
[2019-03-26 00:36:42,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:42,964] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1813
[2019-03-26 00:36:42,972] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 943303.5168832868 W.
[2019-03-26 00:36:42,978] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.25, 82.5, 1.0, 2.0, 0.3374969031670874, 1.0, 1.0, 0.3374969031670874, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 943303.5168832868, 943303.5168832868, 257519.467066786], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3558600.0000, 
sim time next is 3559200.0000, 
raw observation next is [26.16666666666666, 83.0, 1.0, 2.0, 0.3291621960506146, 1.0, 2.0, 0.3291621960506146, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 919998.0259702144, 919998.0259702144, 255746.2746441364], 
processed observation next is [1.0, 0.17391304347826086, 0.4391785150078987, 0.83, 1.0, 1.0, 0.19176168198869226, 1.0, 1.0, 0.19176168198869226, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.25555500721394847, 0.25555500721394847, 0.38171085767781554], 
reward next is 0.6183, 
noisyNet noise sample is [array([-0.06238402], dtype=float32), 0.62511706]. 
=============================================
[2019-03-26 00:36:43,353] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1153864: loss 7.7914
[2019-03-26 00:36:43,355] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1153864: learning rate 0.0010
[2019-03-26 00:36:44,097] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1154133: loss 5.4587
[2019-03-26 00:36:44,100] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1154134: learning rate 0.0010
[2019-03-26 00:36:44,232] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1154186: loss 5.8556
[2019-03-26 00:36:44,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1154187: learning rate 0.0010
[2019-03-26 00:36:44,539] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1154296: loss 9.7138
[2019-03-26 00:36:44,542] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1154296: learning rate 0.0010
[2019-03-26 00:36:44,781] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1154390: loss 11.5697
[2019-03-26 00:36:44,784] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1154390: learning rate 0.0010
[2019-03-26 00:36:49,876] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1156265: loss -37.7983
[2019-03-26 00:36:49,879] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1156265: learning rate 0.0010
[2019-03-26 00:36:50,540] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:50,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6491
[2019-03-26 00:36:50,554] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.5, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9582417742605287, 6.9112, 6.9112, 168.912956510431, 774710.45454752, 774710.45454752, 236656.2306779468], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3609000.0000, 
sim time next is 3609600.0000, 
raw observation next is [31.33333333333333, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9494993880739293, 6.9112, 6.9112, 168.912956510431, 769895.835154746, 769895.835154746, 234628.7608275844], 
processed observation next is [1.0, 0.782608695652174, 0.6840442338072668, 0.6633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9384138878950358, 0.0, 0.0, 0.8294399451523027, 0.21385995420965165, 0.21385995420965165, 0.3501921803396782], 
reward next is 0.6498, 
noisyNet noise sample is [array([-0.6434711], dtype=float32), 1.644629]. 
=============================================
[2019-03-26 00:36:52,236] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1157130: loss 66.9439
[2019-03-26 00:36:52,240] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1157130: learning rate 0.0010
[2019-03-26 00:36:52,644] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1157282: loss -82.1074
[2019-03-26 00:36:52,647] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1157283: learning rate 0.0010
[2019-03-26 00:36:53,808] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1157710: loss -21.4862
[2019-03-26 00:36:53,810] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1157710: learning rate 0.0010
[2019-03-26 00:36:55,012] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1158148: loss -183.6260
[2019-03-26 00:36:55,013] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1158148: learning rate 0.0010
[2019-03-26 00:36:55,677] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1158394: loss 15.3028
[2019-03-26 00:36:55,681] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1158395: learning rate 0.0010
[2019-03-26 00:36:57,285] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1158990: loss -23.8273
[2019-03-26 00:36:57,289] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1158990: learning rate 0.0010
[2019-03-26 00:36:58,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:58,810] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9618
[2019-03-26 00:36:58,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1254510.640811875 W.
[2019-03-26 00:36:58,823] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 70.66666666666667, 1.0, 2.0, 0.8975470366111844, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1254510.640811875, 1254510.640811875, 269261.6617944508], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3658200.0000, 
sim time next is 3658800.0000, 
raw observation next is [29.0, 71.33333333333334, 1.0, 2.0, 0.6918821797052219, 1.0, 1.0, 0.6918821797052219, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1934703.938337331, 1934703.938337331, 370661.8990449772], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7133333333333334, 1.0, 1.0, 0.6287737104882192, 1.0, 0.5, 0.6287737104882192, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5374177606492586, 0.5374177606492586, 0.5532267149925033], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4495475], dtype=float32), 0.1584804]. 
=============================================
[2019-03-26 00:36:59,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:36:59,760] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4461
[2019-03-26 00:36:59,771] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1626800.03225922 W.
[2019-03-26 00:36:59,776] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.33333333333334, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.974032179474445, 6.9112, 168.9073494169352, 1626800.03225922, 872815.3337569978, 256551.1074952559], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3737400.0000, 
sim time next is 3738000.0000, 
raw observation next is [26.66666666666667, 77.33333333333334, 1.0, 1.0, 0.4265159962602886, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7155892593682824, 6.911199999999999, 6.9112, 168.9120911036062, 1206348.569997086, 1206348.569997087, 268438.8481200172], 
processed observation next is [1.0, 0.2608695652173913, 0.4628751974723541, 0.7733333333333334, 1.0, 0.5, 0.30905541718107055, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6531576333759542, -8.881784197001253e-17, 0.0, 0.829435695608608, 0.3350968249991905, 0.3350968249991908, 0.40065499719405556], 
reward next is 0.5993, 
noisyNet noise sample is [array([0.7779569], dtype=float32), -0.15247694]. 
=============================================
[2019-03-26 00:36:59,793] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[39.062035]
 [41.194412]
 [40.546463]
 [40.35527 ]
 [40.282845]], R is [[39.38596344]
 [38.99210358]
 [39.29198837]
 [39.59124756]
 [39.88835526]].
[2019-03-26 00:37:01,262] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1160447: loss -4.5216
[2019-03-26 00:37:01,265] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1160448: learning rate 0.0010
[2019-03-26 00:37:03,732] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1161364: loss 28.0765
[2019-03-26 00:37:03,737] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1161365: learning rate 0.0010
[2019-03-26 00:37:04,848] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1161777: loss -39.9833
[2019-03-26 00:37:04,853] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1161777: learning rate 0.0010
[2019-03-26 00:37:04,997] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1161827: loss -138.2592
[2019-03-26 00:37:04,999] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1161828: learning rate 0.0010
[2019-03-26 00:37:05,613] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1162058: loss -18.4716
[2019-03-26 00:37:05,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1162058: learning rate 0.0010
[2019-03-26 00:37:06,171] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1162266: loss 17.0290
[2019-03-26 00:37:06,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1162267: learning rate 0.0010
[2019-03-26 00:37:06,468] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1162377: loss -111.2179
[2019-03-26 00:37:06,470] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1162377: learning rate 0.0010
[2019-03-26 00:37:06,680] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:06,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0011
[2019-03-26 00:37:06,696] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.66666666666667, 80.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.957216188950087, 6.9112, 168.9125265421634, 861459.0391948505, 828813.6540977675, 254812.0939631596], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3915600.0000, 
sim time next is 3916200.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.011330918993852, 6.9112, 168.9121689929181, 899864.6229511357, 828828.6339670119, 254812.129991936], 
processed observation next is [0.0, 0.30434782608695654, 0.6129541864139019, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.010013091899385174, 0.0, 0.8294360780807873, 0.24996239526420438, 0.23023017610194776, 0.3803166119282627], 
reward next is 0.1190, 
noisyNet noise sample is [array([-0.06610584], dtype=float32), -0.19361372]. 
=============================================
[2019-03-26 00:37:06,820] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1162507: loss -109.8498
[2019-03-26 00:37:06,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1162507: learning rate 0.0010
[2019-03-26 00:37:06,987] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1162568: loss 64.2490
[2019-03-26 00:37:06,991] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1162569: learning rate 0.0010
[2019-03-26 00:37:11,260] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1164140: loss 0.2632
[2019-03-26 00:37:11,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1164140: learning rate 0.0010
[2019-03-26 00:37:13,581] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1164998: loss 0.2219
[2019-03-26 00:37:13,583] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1164999: learning rate 0.0010
[2019-03-26 00:37:14,280] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1165254: loss 0.1807
[2019-03-26 00:37:14,283] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1165255: learning rate 0.0010
[2019-03-26 00:37:15,334] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1165648: loss 0.0238
[2019-03-26 00:37:15,338] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1165649: learning rate 0.0010
[2019-03-26 00:37:16,774] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1166184: loss 0.1618
[2019-03-26 00:37:16,776] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1166184: learning rate 0.0010
[2019-03-26 00:37:17,206] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1166338: loss 0.0541
[2019-03-26 00:37:17,209] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1166340: learning rate 0.0010
[2019-03-26 00:37:18,252] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:18,263] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4700
[2019-03-26 00:37:18,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 929903.4435259496 W.
[2019-03-26 00:37:18,277] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 59.5, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.053656645821125, 6.9112, 168.9120296632189, 929903.4435259496, 828840.3505503756, 254813.0011697507], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [34.33333333333333, 60.66666666666667, 1.0, 1.0, 0.2079256702809468, 1.0, 1.0, 0.2079256702809468, 1.0, 2.0, 0.3610981272080966, 6.9112, 6.9112, 170.5573041426782, 871698.9802242535, 871698.9802242535, 272080.9141387827], 
processed observation next is [0.0, 0.6956521739130435, 0.8262243285939966, 0.6066666666666667, 1.0, 0.5, 0.04569357865174312, 1.0, 0.5, 0.04569357865174312, 1.0, 1.0, 0.22085137464402022, 0.0, 0.0, 0.8375144448122397, 0.2421386056178482, 0.2421386056178482, 0.4060909166250488], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7831616], dtype=float32), -0.30883986]. 
=============================================
[2019-03-26 00:37:18,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[49.887695]
 [47.61092 ]
 [47.752285]
 [49.538822]
 [47.371773]], R is [[47.88137436]
 [47.40256119]
 [46.92853546]
 [46.4592514 ]
 [45.99465942]].
[2019-03-26 00:37:18,993] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1166999: loss 0.0647
[2019-03-26 00:37:18,996] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1166999: learning rate 0.0010
[2019-03-26 00:37:22,761] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1168385: loss 1.0372
[2019-03-26 00:37:22,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1168385: learning rate 0.0010
[2019-03-26 00:37:24,945] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1169189: loss 1.2988
[2019-03-26 00:37:24,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1169189: learning rate 0.0010
[2019-03-26 00:37:25,888] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:25,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4796
[2019-03-26 00:37:25,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3241232.126601666 W.
[2019-03-26 00:37:25,923] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.373991299774111, 6.9112, 170.5573041426782, 3241232.126601666, 2909715.893427, 551163.0343216046], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.7349419197410173, 1.0, 2.0, 0.6880609993847713, 1.0, 1.0, 1.03, 7.005100487455604, 6.9112, 170.5573041426782, 2887180.70513056, 2819915.966338493, 532592.7784377415], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 0.6806529153506232, 1.0, 1.0, 0.6241698787768328, 1.0, 0.5, 1.0365853658536586, 0.009390048745560441, 0.0, 0.8375144448122397, 0.8019946403140444, 0.7833099906495814, 0.7949145946831961], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01453383], dtype=float32), 0.017901024]. 
=============================================
[2019-03-26 00:37:25,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[31.156044]
 [30.81662 ]
 [30.849913]
 [29.9834  ]
 [29.070793]], R is [[31.28025818]
 [30.96745682]
 [30.6577816 ]
 [30.35120392]
 [30.04769135]].
[2019-03-26 00:37:26,102] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1169620: loss 0.5579
[2019-03-26 00:37:26,103] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1169621: learning rate 0.0010
[2019-03-26 00:37:26,111] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1169621: loss 0.4621
[2019-03-26 00:37:26,113] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1169622: learning rate 0.0010
[2019-03-26 00:37:26,631] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1169813: loss 0.5451
[2019-03-26 00:37:26,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1169813: learning rate 0.0010
[2019-03-26 00:37:27,301] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:27,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7967
[2019-03-26 00:37:27,315] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.16666666666667, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9637822968930139, 6.9112, 6.9112, 168.912956510431, 780779.6846670455, 780779.6846670455, 238111.4319242994], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4042200.0000, 
sim time next is 4042800.0000, 
raw observation next is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9535002526064997, 6.9112, 6.9112, 168.912956510431, 774440.6372907641, 774440.6372907641, 235675.5094197113], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9432929909835363, 0.0, 0.0, 0.8294399451523027, 0.21512239924743448, 0.21512239924743448, 0.3517544916712109], 
reward next is 0.6482, 
noisyNet noise sample is [array([-0.2987523], dtype=float32), 0.5146143]. 
=============================================
[2019-03-26 00:37:27,400] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1170100: loss 0.5004
[2019-03-26 00:37:27,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1170101: learning rate 0.0010
[2019-03-26 00:37:27,718] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1170218: loss 0.4863
[2019-03-26 00:37:27,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1170218: learning rate 0.0010
[2019-03-26 00:37:28,074] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1170353: loss 0.8805
[2019-03-26 00:37:28,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1170353: learning rate 0.0010
[2019-03-26 00:37:28,161] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1170381: loss 0.7378
[2019-03-26 00:37:28,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1170383: learning rate 0.0010
[2019-03-26 00:37:28,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8534361e-28], sum to 1.0000
[2019-03-26 00:37:28,327] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2920
[2019-03-26 00:37:28,335] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2498530.290146931 W.
[2019-03-26 00:37:28,344] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.66666666666667, 71.0, 1.0, 2.0, 0.8932979399158956, 1.0, 2.0, 0.8932979399158956, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2498530.290146931, 2498530.290146932, 467860.7046389528], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [32.83333333333333, 71.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.995417848665088, 6.9112, 168.9069319423394, 3053541.526238118, 2284387.502662514, 473570.8698453059], 
processed observation next is [1.0, 0.4782608695652174, 0.7551342812006318, 0.71, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.10842178486650882, 0.0, 0.8294103617649138, 0.8482059795105883, 0.6345520840729205, 0.706822193798964], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22220227], dtype=float32), 0.044992454]. 
=============================================
[2019-03-26 00:37:33,430] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1172315: loss -35.8563
[2019-03-26 00:37:33,431] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1172315: learning rate 0.0010
[2019-03-26 00:37:35,757] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1173176: loss -103.9517
[2019-03-26 00:37:35,763] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1173178: learning rate 0.0010
[2019-03-26 00:37:36,260] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1173361: loss -16.2254
[2019-03-26 00:37:36,263] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1173361: learning rate 0.0010
[2019-03-26 00:37:37,265] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1173743: loss -100.7753
[2019-03-26 00:37:37,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1173743: learning rate 0.0010
[2019-03-26 00:37:37,740] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:37,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8448
[2019-03-26 00:37:37,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2897961.137988266 W.
[2019-03-26 00:37:37,772] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 53.0, 1.0, 2.0, 0.7400742628386188, 1.0, 2.0, 0.6906271709335718, 1.0, 1.0, 1.03, 7.0051008921871, 6.9112, 170.5573041426782, 2897961.137988266, 2830696.109270565, 534285.7422356346], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4191000.0000, 
sim time next is 4191600.0000, 
raw observation next is [36.0, 53.0, 1.0, 2.0, 0.5949507236445221, 1.0, 2.0, 0.5949507236445221, 1.0, 2.0, 1.03, 6.914833961475231, 6.9112, 170.5573041426782, 2496089.301047038, 2493486.146622228, 486155.6034283385], 
processed observation next is [1.0, 0.5217391304347826, 0.9052132701421801, 0.53, 1.0, 1.0, 0.5119888236680988, 1.0, 1.0, 0.5119888236680988, 1.0, 1.0, 1.0365853658536586, 0.00036339614752307626, 0.0, 0.8375144448122397, 0.6933581391797328, 0.6926350407283967, 0.7256053782512515], 
reward next is 0.2562, 
noisyNet noise sample is [array([-0.2214446], dtype=float32), 0.4904545]. 
=============================================
[2019-03-26 00:37:38,631] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1174244: loss -72.2262
[2019-03-26 00:37:38,632] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1174244: learning rate 0.0010
[2019-03-26 00:37:39,127] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1174428: loss 17.7814
[2019-03-26 00:37:39,129] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1174428: learning rate 0.0010
[2019-03-26 00:37:39,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:37:39,795] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6000
[2019-03-26 00:37:39,803] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 982928.6492032999 W.
[2019-03-26 00:37:39,812] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 84.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.128371332233035, 6.9112, 168.9116381990945, 982928.6492032999, 828861.0341143804, 254813.1118650048], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4412400.0000, 
sim time next is 4413000.0000, 
raw observation next is [30.0, 84.0, 1.0, 1.0, 0.3200852419275693, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5558822114211415, 6.9112, 6.9112, 168.9127796802578, 894621.2745991386, 894621.2745991386, 228642.7085904311], 
processed observation next is [0.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 0.5, 0.18082559268381845, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4583929407574896, 0.0, 0.0, 0.8294390768351991, 0.24850590961087185, 0.24850590961087185, 0.34125777401556884], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7904191], dtype=float32), 0.4273161]. 
=============================================
[2019-03-26 00:37:39,821] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[21.600304]
 [21.441074]
 [22.30502 ]
 [22.25558 ]
 [21.843857]], R is [[20.58887672]
 [20.38298798]
 [20.77361679]
 [20.56588173]
 [20.36022377]].
[2019-03-26 00:37:40,698] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 00:37:40,700] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:37:40,702] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:37:40,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:37:40,703] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:37:40,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:37:40,709] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:37:40,710] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:37:40,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:37:40,717] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:37:40,715] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:37:40,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-26 00:37:40,742] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-26 00:37:40,787] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-26 00:37:40,810] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-26 00:37:40,842] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-26 00:37:47,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:37:47,815] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.50847978333333, 69.29458091333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4682220952184806, 6.911199999999999, 6.9112, 168.912956510431, 418761.58785664, 418761.5878566407, 147943.2951326086]
[2019-03-26 00:37:47,816] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:37:47,821] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.11811304751383911
[2019-03-26 00:38:22,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:38:22,155] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.719762005, 97.67157659, 1.0, 2.0, 0.7998790611722738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1117927.251016982, 1117927.251016983, 243889.1908926396]
[2019-03-26 00:38:22,158] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:38:22,162] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 3.7310181e-37 1.0006925e-34 1.2897855e-20], sampled 0.8407739013262768
[2019-03-26 00:38:22,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1117927.251016982 W.
[2019-03-26 00:39:10,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:39:10,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.98333333333333, 59.0, 1.0, 2.0, 0.9224916566025866, 1.0, 2.0, 0.7818358678155557, 1.0, 1.0, 1.03, 7.005115281978089, 6.9112, 170.5573041426782, 3281187.400581491, 3213912.063871166, 600822.3000095589]
[2019-03-26 00:39:10,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:39:10,459] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9367611304532559
[2019-03-26 00:39:10,462] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3281187.400581491 W.
[2019-03-26 00:39:18,377] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:39:18,383] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.08918154666667, 74.32929843666668, 1.0, 2.0, 0.5863463503448118, 0.0, 2.0, 0.0, 1.0, 2.0, 1.018289702847796, 6.911199999999999, 6.9112, 168.9129087880522, 1639381.502650071, 1639381.502650072, 359032.0998309538]
[2019-03-26 00:39:18,385] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:39:18,389] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.7289039e-34], sampled 0.544009843424046
[2019-03-26 00:39:18,390] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1639381.502650071 W.
[2019-03-26 00:39:25,166] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:39:25,166] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.105037295, 81.57427173166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9340572724234758, 6.9112, 6.9112, 168.912956510431, 764908.3420059381, 764908.3420059381, 231243.7716681212]
[2019-03-26 00:39:25,167] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:39:25,168] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.19299853762139885
[2019-03-26 00:39:39,784] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:39:39,785] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.5, 79.33333333333334, 1.0, 2.0, 0.6347414905600961, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.992247168491906, 6.9112, 168.9124152298833, 1774803.921429958, 1717306.454924411, 371191.8826225084]
[2019-03-26 00:39:39,788] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:39:39,790] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7043494576644707
[2019-03-26 00:39:39,791] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1774803.921429958 W.
[2019-03-26 00:39:44,306] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1951027], dtype=float32), -0.15505777]
[2019-03-26 00:39:44,307] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.21666666666667, 94.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5794232971577735, 6.9112, 6.9112, 168.912956510431, 506079.5579301456, 506079.5579301456, 162974.9417084234]
[2019-03-26 00:39:44,307] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:39:44,310] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.45701279050961197
[2019-03-26 00:39:49,544] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7934.6899 2989463838.4816 1517.0000
[2019-03-26 00:39:49,976] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7037.7977 3185144194.9369 2449.0000
[2019-03-26 00:39:50,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8070.1883 2938859025.7893 1361.0000
[2019-03-26 00:39:50,136] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7351.3793 3106511453.7940 2018.0000
[2019-03-26 00:39:50,458] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7308.7262 3319494219.1157 2084.0000
[2019-03-26 00:39:51,476] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1175000, evaluation results [1175000.0, 7308.726242659556, 3319494219.115658, 2084.0, 7351.379273159181, 3106511453.7940254, 2018.0, 8070.188343940515, 2938859025.789272, 1361.0, 7037.797749349175, 3185144194.936923, 2449.0, 7934.689917385474, 2989463838.4816008, 1517.0]
[2019-03-26 00:39:51,777] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1175117: loss 41.1913
[2019-03-26 00:39:51,780] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1175118: learning rate 0.0010
[2019-03-26 00:39:53,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:39:53,165] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4381
[2019-03-26 00:39:53,176] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2985840.574685579 W.
[2019-03-26 00:39:53,183] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.83333333333334, 54.5, 1.0, 2.0, 0.7819102089919429, 1.0, 2.0, 0.711545144010234, 1.0, 2.0, 1.03, 7.005104191588832, 6.9112, 170.5573041426782, 2985840.574685579, 2918573.182472206, 548450.7438464626], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4279800.0000, 
sim time next is 4280400.0000, 
raw observation next is [38.0, 54.0, 1.0, 2.0, 0.7710721695987917, 1.0, 2.0, 0.7061261243136585, 1.0, 2.0, 1.03, 7.005103336799775, 6.9112, 170.5573041426782, 2963073.894120217, 2895807.114227017, 544719.2192319662], 
processed observation next is [1.0, 0.5652173913043478, 1.0, 0.54, 1.0, 1.0, 0.724183336866014, 1.0, 1.0, 0.6459350895345283, 1.0, 1.0, 1.0365853658536586, 0.009390333679977481, 0.0, 0.8375144448122397, 0.8230760817000602, 0.8043908650630602, 0.8130137600477108], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7384927], dtype=float32), -0.6542059]. 
=============================================
[2019-03-26 00:39:55,414] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1176456: loss -14.2018
[2019-03-26 00:39:55,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1176456: learning rate 0.0010
[2019-03-26 00:39:56,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:39:56,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4935
[2019-03-26 00:39:56,826] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.0, 50.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9208821853136087, 6.9112, 6.9112, 168.912956510431, 756955.7034645611, 756955.7034645611, 228211.9650252482], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4543800.0000, 
sim time next is 4544400.0000, 
raw observation next is [34.0, 51.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.893771552728329, 6.9112, 6.9112, 168.912956510431, 733981.123906031, 733981.123906031, 221838.9126299524], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.51, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8704531130833281, 0.0, 0.0, 0.8294399451523027, 0.20388364552945307, 0.20388364552945307, 0.33110285467157075], 
reward next is 0.6689, 
noisyNet noise sample is [array([0.55742353], dtype=float32), -1.0101882]. 
=============================================
[2019-03-26 00:39:57,758] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1177320: loss 40.6814
[2019-03-26 00:39:57,762] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1177320: learning rate 0.0010
[2019-03-26 00:39:58,821] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1177714: loss 5.4421
[2019-03-26 00:39:58,827] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1177714: learning rate 0.0010
[2019-03-26 00:39:59,002] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1177782: loss -83.0751
[2019-03-26 00:39:59,008] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1177782: learning rate 0.0010
[2019-03-26 00:39:59,581] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1177991: loss -51.0691
[2019-03-26 00:39:59,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1177993: learning rate 0.0010
[2019-03-26 00:40:00,212] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1178226: loss -8.8240
[2019-03-26 00:40:00,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1178226: learning rate 0.0010
[2019-03-26 00:40:00,642] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1178387: loss -81.2097
[2019-03-26 00:40:00,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1178389: learning rate 0.0010
[2019-03-26 00:40:00,800] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1178444: loss -59.2228
[2019-03-26 00:40:00,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1178445: learning rate 0.0010
[2019-03-26 00:40:01,011] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1178521: loss -60.8590
[2019-03-26 00:40:01,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1178521: learning rate 0.0010
[2019-03-26 00:40:04,758] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1179904: loss 1.7876
[2019-03-26 00:40:04,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1179905: learning rate 0.0010
[2019-03-26 00:40:07,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1920371e-36 3.5801824e-37], sum to 1.0000
[2019-03-26 00:40:07,274] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8748
[2019-03-26 00:40:07,284] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 879924.9708687605 W.
[2019-03-26 00:40:07,297] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 75.66666666666666, 1.0, 2.0, 0.3148305070720588, 1.0, 2.0, 0.3148305070720588, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 879924.9708687605, 879924.9708687605, 252823.4323335433], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4441800.0000, 
sim time next is 4442400.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.2105997736547734, 1.0, 2.0, 0.2105997736547734, 1.0, 1.0, 0.3657421604289342, 6.911200000000001, 6.9112, 170.5573041426782, 882914.3954107194, 882914.3954107187, 272855.5842905345], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.0489153899455101, 1.0, 1.0, 0.0489153899455101, 1.0, 0.5, 0.22651482979138315, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.24525399872519985, 0.24525399872519965, 0.407247140732141], 
reward next is 0.5928, 
noisyNet noise sample is [array([0.5710913], dtype=float32), 2.3578262]. 
=============================================
[2019-03-26 00:40:07,500] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1180919: loss 0.0660
[2019-03-26 00:40:07,503] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1180919: learning rate 0.0010
[2019-03-26 00:40:07,835] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1181040: loss 0.4581
[2019-03-26 00:40:07,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1181040: learning rate 0.0010
[2019-03-26 00:40:08,886] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1181428: loss 0.5295
[2019-03-26 00:40:08,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1181430: learning rate 0.0010
[2019-03-26 00:40:09,559] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:09,568] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5859
[2019-03-26 00:40:09,578] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9429302647424311, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 233291.6385115953], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9424706008194192, 6.9112, 6.9112, 168.912956510431, 769578.7510898073, 769578.7510898073, 233180.1597464439], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.929842196121243, 0.0, 0.0, 0.8294399451523027, 0.21377187530272423, 0.21377187530272423, 0.3480300891737969], 
reward next is 0.6520, 
noisyNet noise sample is [array([0.7902758], dtype=float32), 0.15005465]. 
=============================================
[2019-03-26 00:40:10,124] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:10,132] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1369
[2019-03-26 00:40:10,141] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8152891572257049, 6.9112, 6.9112, 168.912956510431, 683393.7833797967, 683393.7833797967, 205015.5752762454], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4508400.0000, 
sim time next is 4509000.0000, 
raw observation next is [26.0, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8207608721418982, 6.911200000000001, 6.9112, 168.912956510431, 686984.0802107382, 686984.0802107377, 206145.9407921921], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7814156977340222, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1908289111696495, 0.19082891116964934, 0.3076805086450628], 
reward next is 0.6923, 
noisyNet noise sample is [array([-0.00091165], dtype=float32), 0.8910001]. 
=============================================
[2019-03-26 00:40:10,164] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.9213  ]
 [75.13748 ]
 [75.245804]
 [75.19292 ]
 [75.26726 ]], R is [[74.67401886]
 [74.62128448]
 [74.57039642]
 [74.52013397]
 [74.46775055]].
[2019-03-26 00:40:10,636] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1182075: loss 0.1981
[2019-03-26 00:40:10,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1182075: learning rate 0.0010
[2019-03-26 00:40:11,075] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1182238: loss 0.0243
[2019-03-26 00:40:11,079] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1182239: learning rate 0.0010
[2019-03-26 00:40:13,282] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1183045: loss 0.0227
[2019-03-26 00:40:13,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1183046: learning rate 0.0010
[2019-03-26 00:40:17,087] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1184448: loss 0.3942
[2019-03-26 00:40:17,091] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1184448: learning rate 0.0010
[2019-03-26 00:40:19,242] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1185241: loss 1.4127
[2019-03-26 00:40:19,245] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1185241: learning rate 0.0010
[2019-03-26 00:40:20,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:20,212] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6555
[2019-03-26 00:40:20,218] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8258374180693658, 6.911200000000001, 6.9112, 168.912956510431, 689658.780671726, 689658.7806717253, 207183.6508083293], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4827000.0000, 
sim time next is 4827600.0000, 
raw observation next is [28.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.827018332632469, 6.9112, 6.9112, 168.912956510431, 690645.8476630813, 690645.8476630813, 207435.2091208883], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.789046747112767, 0.0, 0.0, 0.8294399451523027, 0.19184606879530033, 0.19184606879530033, 0.3096047897326691], 
reward next is 0.6904, 
noisyNet noise sample is [array([-0.0397464], dtype=float32), -1.9127668]. 
=============================================
[2019-03-26 00:40:20,478] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1185699: loss 1.0166
[2019-03-26 00:40:20,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1185700: learning rate 0.0010
[2019-03-26 00:40:20,544] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1185720: loss 0.5840
[2019-03-26 00:40:20,545] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1185720: learning rate 0.0010
[2019-03-26 00:40:21,106] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1185928: loss 3.7473
[2019-03-26 00:40:21,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1185929: learning rate 0.0010
[2019-03-26 00:40:22,013] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1186264: loss 4.7801
[2019-03-26 00:40:22,018] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1186264: learning rate 0.0010
[2019-03-26 00:40:22,381] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1186397: loss 1.8161
[2019-03-26 00:40:22,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1186398: learning rate 0.0010
[2019-03-26 00:40:22,521] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1186449: loss 1.9676
[2019-03-26 00:40:22,527] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1186450: learning rate 0.0010
[2019-03-26 00:40:22,538] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1186455: loss 3.8460
[2019-03-26 00:40:22,541] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1186455: learning rate 0.0010
[2019-03-26 00:40:26,551] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1187936: loss -37.6826
[2019-03-26 00:40:26,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1187936: learning rate 0.0010
[2019-03-26 00:40:26,642] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:26,652] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5314
[2019-03-26 00:40:26,662] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8138702582362097, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 204678.0128298888], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.810818665646079, 6.911199999999999, 6.9112, 168.912956510431, 678290.0615507917, 678290.0615507923, 204046.8913719353], 
processed observation next is [1.0, 0.8695652173913043, 0.4865718799368086, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.76929105566595, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18841390598633104, 0.1884139059863312, 0.30454759906259], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.7717864], dtype=float32), 1.24659]. 
=============================================
[2019-03-26 00:40:29,457] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1189021: loss -156.1734
[2019-03-26 00:40:29,461] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1189021: learning rate 0.0010
[2019-03-26 00:40:29,659] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1189092: loss -167.4905
[2019-03-26 00:40:29,662] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1189094: learning rate 0.0010
[2019-03-26 00:40:31,128] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1189631: loss -123.4826
[2019-03-26 00:40:31,131] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1189632: learning rate 0.0010
[2019-03-26 00:40:32,394] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1190107: loss -125.3295
[2019-03-26 00:40:32,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1190107: learning rate 0.0010
[2019-03-26 00:40:33,028] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1190344: loss -154.6256
[2019-03-26 00:40:33,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1190344: learning rate 0.0010
[2019-03-26 00:40:34,855] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1191019: loss -144.1439
[2019-03-26 00:40:34,858] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1191019: learning rate 0.0010
[2019-03-26 00:40:36,736] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0242940e-12 1.8140949e-34 4.4504913e-24 3.8303222e-30 1.0000000e+00], sum to 1.0000
[2019-03-26 00:40:36,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5378
[2019-03-26 00:40:36,751] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 0.5086653005704549, 1.0, 2.0, 0.5086653005704549, 1.0, 2.0, 0.8753883106207347, 6.9112, 6.9112, 170.5573041426782, 2133762.375593984, 2133762.375593984, 419713.8060509839], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4872000.0000, 
sim time next is 4872600.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.5067702474966893, 1.0, 2.0, 0.5067702474966893, 1.0, 2.0, 0.8727465641803456, 6.9112, 6.9112, 170.5573041426782, 2125805.069932338, 2125805.069932338, 418509.4914931781], 
processed observation next is [1.0, 0.391304347826087, 0.5971563981042655, 0.72, 1.0, 1.0, 0.4057472861405895, 1.0, 1.0, 0.4057472861405895, 1.0, 1.0, 0.844812883146763, 0.0, 0.0, 0.8375144448122397, 0.5905014083145383, 0.5905014083145383, 0.6246410320793704], 
reward next is 0.3754, 
noisyNet noise sample is [array([0.5990214], dtype=float32), -1.4428371]. 
=============================================
[2019-03-26 00:40:38,348] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:38,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4083
[2019-03-26 00:40:38,361] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333334, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8931949187299993, 6.911199999999999, 6.9112, 168.912956510431, 733975.2760108687, 733975.2760108693, 221725.1271176583], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5055600.0000, 
sim time next is 5056200.0000, 
raw observation next is [31.5, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9030987149704129, 6.911200000000001, 6.9112, 168.912956510431, 740496.5441567816, 740496.544156781, 223951.5616734846], 
processed observation next is [0.0, 0.5217391304347826, 0.6919431279620853, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8818277011834302, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2056934844879949, 0.20569348448799474, 0.33425606219923076], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.45899862], dtype=float32), -0.76452583]. 
=============================================
[2019-03-26 00:40:38,934] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1192524: loss -169.8008
[2019-03-26 00:40:38,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1192525: learning rate 0.0010
[2019-03-26 00:40:41,140] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1193342: loss -48.7843
[2019-03-26 00:40:41,142] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1193343: learning rate 0.0010
[2019-03-26 00:40:41,314] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:40:41,324] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7275
[2019-03-26 00:40:41,329] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8138702582362677, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 204678.0128298998], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4912800.0000, 
sim time next is 4913400.0000, 
raw observation next is [27.16666666666666, 78.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8108186656460955, 6.911199999999999, 6.9112, 168.912956510431, 678290.0615507917, 678290.0615507923, 204046.8913719384], 
processed observation next is [1.0, 0.8695652173913043, 0.4865718799368086, 0.7816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.76929105566597, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18841390598633104, 0.1884139059863312, 0.30454759906259465], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.22810061], dtype=float32), -0.17679818]. 
=============================================
[2019-03-26 00:40:42,328] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1193782: loss -142.3113
[2019-03-26 00:40:42,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1193782: learning rate 0.0010
[2019-03-26 00:40:42,403] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1193807: loss -69.6093
[2019-03-26 00:40:42,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1193809: learning rate 0.0010
[2019-03-26 00:40:42,992] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1194030: loss -157.8977
[2019-03-26 00:40:42,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1194031: learning rate 0.0010
[2019-03-26 00:40:43,972] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1194386: loss -97.2173
[2019-03-26 00:40:43,974] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1194387: learning rate 0.0010
[2019-03-26 00:40:44,156] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1194455: loss -114.8989
[2019-03-26 00:40:44,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1194455: learning rate 0.0010
[2019-03-26 00:40:44,414] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1194547: loss -195.6429
[2019-03-26 00:40:44,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1194547: learning rate 0.0010
[2019-03-26 00:40:44,467] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1194564: loss -40.7038
[2019-03-26 00:40:44,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1194566: learning rate 0.0010
[2019-03-26 00:40:47,708] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1195763: loss 1.2022
[2019-03-26 00:40:47,710] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1195763: learning rate 0.0010
[2019-03-26 00:40:50,844] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1196921: loss 2.4293
[2019-03-26 00:40:50,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1196921: learning rate 0.0010
[2019-03-26 00:40:51,112] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1197020: loss 0.1756
[2019-03-26 00:40:51,116] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1197020: learning rate 0.0010
[2019-03-26 00:40:52,878] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1197670: loss 0.2647
[2019-03-26 00:40:52,882] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1197673: learning rate 0.0010
[2019-03-26 00:40:54,106] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1198128: loss 0.0143
[2019-03-26 00:40:54,109] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1198128: learning rate 0.0010
[2019-03-26 00:40:54,753] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1198366: loss 0.0212
[2019-03-26 00:40:54,757] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1198367: learning rate 0.0010
[2019-03-26 00:40:56,178] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1198886: loss 1.3169
[2019-03-26 00:40:56,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1198887: learning rate 0.0010
[2019-03-26 00:40:59,223] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 00:40:59,224] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:40:59,225] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:40:59,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:40:59,228] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:40:59,228] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:40:59,229] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:40:59,230] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:40:59,231] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:40:59,231] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:40:59,234] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:40:59,257] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-26 00:40:59,257] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-26 00:40:59,258] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-26 00:40:59,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-26 00:40:59,340] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-26 00:41:10,159] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:41:10,161] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.18333333333333, 91.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4537176761615633, 6.9112, 6.9112, 168.912956510431, 410712.894295169, 410712.894295169, 145858.9816886446]
[2019-03-26 00:41:10,163] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:41:10,165] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.790329201739518
[2019-03-26 00:41:16,302] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:41:16,305] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.7672784, 97.72755044499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6045901870298823, 6.911199999999999, 6.9112, 168.912956510431, 528137.4309839263, 528137.430983927, 166744.9851232675]
[2019-03-26 00:41:16,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:41:16,309] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.575842089846023
[2019-03-26 00:41:23,534] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:41:23,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.98333333333333, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5059547645701268, 6.911199999999999, 6.9112, 168.912956510431, 449756.3151949081, 449756.3151949086, 152657.6122176031]
[2019-03-26 00:41:23,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:41:23,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08409049573621952
[2019-03-26 00:41:55,245] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:41:55,245] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.83333333333334, 57.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8791646607762501, 6.911200000000001, 6.9112, 168.912956510431, 728257.734122285, 728257.7341222845, 218735.6920376619]
[2019-03-26 00:41:55,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:41:55,251] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8899856358741923
[2019-03-26 00:42:19,442] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:42:19,442] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.76716969, 65.91910450333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7863321366680645, 6.911200000000001, 6.9112, 168.912956510431, 662910.6234550051, 662910.6234550044, 199119.2717213167]
[2019-03-26 00:42:19,444] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:42:19,447] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9732419676678832
[2019-03-26 00:42:36,296] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1351414], dtype=float32), 0.016555337]
[2019-03-26 00:42:36,300] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.24761802333333, 82.77409081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9683171082954073, 6.911200000000001, 6.9112, 168.9129565025115, 787313.2461618622, 787313.2461618616, 239387.4690857139]
[2019-03-26 00:42:36,301] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:42:36,304] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3761668415793724
[2019-03-26 00:43:08,281] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7484.2581 3150900136.8943 1651.0000
[2019-03-26 00:43:08,336] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8164.5180 2975863767.8473 993.0000
[2019-03-26 00:43:08,776] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8051.7380 3022110916.5191 1129.0000
[2019-03-26 00:43:08,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7443.3891 3351057401.0073 1645.0000
[2019-03-26 00:43:09,016] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7245.9333 3221987471.0696 1825.0000
[2019-03-26 00:43:10,035] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1200000, evaluation results [1200000.0, 7443.389111817439, 3351057401.007271, 1645.0, 7484.258106484996, 3150900136.8943176, 1651.0, 8164.518010774617, 2975863767.847283, 993.0, 7245.933285933008, 3221987471.069607, 1825.0, 8051.737967943457, 3022110916.5191464, 1129.0]
[2019-03-26 00:43:11,277] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1200462: loss 1.4490
[2019-03-26 00:43:11,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1200462: learning rate 0.0010
[2019-03-26 00:43:11,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8448611e-11 8.3117985e-37 1.4851121e-27 3.3678218e-33 1.0000000e+00], sum to 1.0000
[2019-03-26 00:43:11,965] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5986
[2019-03-26 00:43:11,970] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.53333333333333, 69.66666666666666, 1.0, 2.0, 0.7232638944891701, 1.0, 2.0, 0.6822219867588476, 1.0, 2.0, 1.03, 7.005099566564112, 6.9112, 170.5573041426782, 2862651.482237654, 2795387.403117622, 528778.4196931073], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5388000.0000, 
sim time next is 5388600.0000, 
raw observation next is [32.71666666666667, 68.83333333333334, 1.0, 2.0, 0.7563845568987276, 1.0, 2.0, 0.6987823179636264, 1.0, 2.0, 1.03, 7.005102178447092, 6.9112, 170.5573041426782, 2932221.357302511, 2864955.407184456, 539730.0427484119], 
processed observation next is [1.0, 0.34782608695652173, 0.7496050552922592, 0.6883333333333335, 1.0, 1.0, 0.7064874179502743, 1.0, 1.0, 0.6370871300766583, 1.0, 1.0, 1.0365853658536586, 0.00939021784470917, 0.0, 0.8375144448122397, 0.8145059325840308, 0.7958209464401267, 0.8055672279827043], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0690655], dtype=float32), 0.64847517]. 
=============================================
[2019-03-26 00:43:13,355] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1201221: loss 16.0340
[2019-03-26 00:43:13,359] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1201222: learning rate 0.0010
[2019-03-26 00:43:14,072] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.0688318e-36 1.2101652e-13], sum to 1.0000
[2019-03-26 00:43:14,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4092
[2019-03-26 00:43:14,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2543970.125323315 W.
[2019-03-26 00:43:14,102] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.26645703294449, 6.9112, 168.9112266676352, 2543970.125323315, 2291941.14021209, 475727.299805805], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5220600.0000, 
sim time next is 5221200.0000, 
raw observation next is [31.0, 67.33333333333334, 1.0, 2.0, 0.8803613511341929, 1.0, 1.0, 0.8803613511341929, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2462311.34853056, 2462311.34853056, 460888.0496484883], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6733333333333335, 1.0, 1.0, 0.8558570495592686, 1.0, 0.5, 0.8558570495592686, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6839753745918221, 0.6839753745918221, 0.6878926114156542], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.37575355], dtype=float32), 0.28445703]. 
=============================================
[2019-03-26 00:43:14,428] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1201615: loss 4.1575
[2019-03-26 00:43:14,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1201615: learning rate 0.0010
[2019-03-26 00:43:14,659] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1201697: loss 3.4847
[2019-03-26 00:43:14,663] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1201698: learning rate 0.0010
[2019-03-26 00:43:14,852] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1201765: loss 6.7014
[2019-03-26 00:43:14,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1201766: learning rate 0.0010
[2019-03-26 00:43:15,875] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1202146: loss 3.8062
[2019-03-26 00:43:15,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1202146: learning rate 0.0010
[2019-03-26 00:43:16,221] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1202274: loss 5.1271
[2019-03-26 00:43:16,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1202274: learning rate 0.0010
[2019-03-26 00:43:16,316] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0996622e-28 0.0000000e+00 1.7753322e-27 7.8547152e-25 1.0000000e+00], sum to 1.0000
[2019-03-26 00:43:16,328] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9548
[2019-03-26 00:43:16,336] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.95, 92.5, 1.0, 2.0, 0.4044130346702435, 1.0, 2.0, 0.4044130346702435, 1.0, 2.0, 0.7023316997879586, 6.9112, 6.9112, 170.5573041426782, 1696096.117633271, 1696096.117633271, 355345.6071719867], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5452200.0000, 
sim time next is 5452800.0000, 
raw observation next is [27.93333333333333, 92.33333333333333, 1.0, 2.0, 0.3687315677805871, 1.0, 2.0, 0.3687315677805871, 1.0, 2.0, 0.640364791842041, 6.9112, 6.9112, 170.5573041426782, 1546341.155033326, 1546341.155033326, 336255.4602921165], 
processed observation next is [1.0, 0.08695652173913043, 0.522906793048973, 0.9233333333333333, 1.0, 1.0, 0.23943562383203262, 1.0, 1.0, 0.23943562383203262, 1.0, 1.0, 0.5614204778561475, 0.0, 0.0, 0.8375144448122397, 0.42953920973147947, 0.42953920973147947, 0.5018738213315171], 
reward next is 0.4981, 
noisyNet noise sample is [array([-0.39768043], dtype=float32), 0.6624749]. 
=============================================
[2019-03-26 00:43:16,626] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1202418: loss 2.1105
[2019-03-26 00:43:16,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1202418: loss 3.4958
[2019-03-26 00:43:16,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1202418: learning rate 0.0010
[2019-03-26 00:43:16,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1202419: learning rate 0.0010
[2019-03-26 00:43:18,507] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 3.1191841e-26 1.8711617e-24 2.1483892e-20 3.8829307e-15], sum to 1.0000
[2019-03-26 00:43:18,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8436
[2019-03-26 00:43:18,532] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3141369.627810909 W.
[2019-03-26 00:43:18,538] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.06666666666667, 53.0, 1.0, 2.0, 0.8559442646475623, 1.0, 2.0, 0.7485621718380439, 1.0, 2.0, 1.03, 7.005110031439324, 6.9112, 170.5573041426782, 3141369.627810909, 3074098.052275079, 575109.6361882304], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5325600.0000, 
sim time next is 5326200.0000, 
raw observation next is [36.05, 53.0, 1.0, 2.0, 0.8420943626941825, 1.0, 2.0, 0.7416372208613539, 1.0, 2.0, 1.03, 7.005108938840261, 6.9112, 170.5573041426782, 3112272.597377299, 3045001.804514622, 569966.6183711599], 
processed observation next is [1.0, 0.6521739130434783, 0.9075829383886255, 0.53, 1.0, 1.0, 0.8097522442098584, 1.0, 1.0, 0.6887195432064505, 1.0, 1.0, 1.0365853658536586, 0.009390893884026141, 0.0, 0.8375144448122397, 0.8645201659381385, 0.8458338345873949, 0.8506964453300894], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6001616], dtype=float32), 1.770247]. 
=============================================
[2019-03-26 00:43:21,258] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1204125: loss 63.8777
[2019-03-26 00:43:21,261] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1204125: learning rate 0.0010
[2019-03-26 00:43:23,577] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1204980: loss 5.2325
[2019-03-26 00:43:23,580] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1204981: learning rate 0.0010
[2019-03-26 00:43:23,973] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1205127: loss 1.0134
[2019-03-26 00:43:23,977] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1205127: learning rate 0.0010
[2019-03-26 00:43:25,664] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1205746: loss -11.2987
[2019-03-26 00:43:25,669] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1205747: learning rate 0.0010
[2019-03-26 00:43:26,994] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1206230: loss -69.0302
[2019-03-26 00:43:26,997] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1206231: learning rate 0.0010
[2019-03-26 00:43:27,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:43:27,546] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2504
[2019-03-26 00:43:27,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3266975.101225579 W.
[2019-03-26 00:43:27,565] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.91666666666666, 55.0, 1.0, 2.0, 0.9157275387585334, 1.0, 2.0, 0.7784538088935293, 1.0, 2.0, 1.03, 7.005114748240672, 6.9112, 170.5573041426782, 3266975.101225579, 3199700.146853076, 598132.416156423], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5406600.0000, 
sim time next is 5407200.0000, 
raw observation next is [36.9, 54.0, 1.0, 2.0, 0.9569284231912351, 1.0, 2.0, 0.7990542511098799, 1.0, 2.0, 1.03, 7.005117999474249, 6.9112, 170.5573041426782, 3353546.075071356, 3286268.79170799, 614775.1978531461], 
processed observation next is [1.0, 0.6086956521739131, 0.947867298578199, 0.54, 1.0, 1.0, 0.9481065339653434, 1.0, 1.0, 0.7578966880841926, 1.0, 1.0, 1.0365853658536586, 0.00939179994742494, 0.0, 0.8375144448122397, 0.9315405764087099, 0.9128524421411084, 0.9175749221688748], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0505991], dtype=float32), 0.1860478]. 
=============================================
[2019-03-26 00:43:27,623] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1206459: loss 80.1603
[2019-03-26 00:43:27,628] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1206461: learning rate 0.0010
[2019-03-26 00:43:29,034] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1206981: loss -42.6359
[2019-03-26 00:43:29,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1206981: learning rate 0.0010
[2019-03-26 00:43:30,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.0112082e-34 6.4633225e-35], sum to 1.0000
[2019-03-26 00:43:30,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9864
[2019-03-26 00:43:30,596] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1306413.560723968 W.
[2019-03-26 00:43:30,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.26666666666667, 79.33333333333334, 1.0, 2.0, 0.934658453821501, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104196, 1306413.560723968, 1306413.560723968, 279654.0169400222], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5469600.0000, 
sim time next is 5470200.0000, 
raw observation next is [30.45, 78.5, 1.0, 2.0, 0.4700485980819485, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8163189674215827, 6.911199999999999, 6.9112, 168.912956510431, 1314020.236540171, 1314020.236540171, 292469.8669945252], 
processed observation next is [1.0, 0.30434782608695654, 0.6421800947867299, 0.785, 1.0, 1.0, 0.36150433503849216, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.7759987407580278, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36500562126115865, 0.36500562126115865, 0.43652218954406746], 
reward next is 0.5635, 
noisyNet noise sample is [array([1.5877074], dtype=float32), -0.76437837]. 
=============================================
[2019-03-26 00:43:33,436] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1208606: loss -55.8933
[2019-03-26 00:43:33,441] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1208608: learning rate 0.0010
[2019-03-26 00:43:35,561] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1209387: loss 52.2404
[2019-03-26 00:43:35,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1209387: learning rate 0.0010
[2019-03-26 00:43:36,799] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1209842: loss -14.7718
[2019-03-26 00:43:36,801] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1209842: learning rate 0.0010
[2019-03-26 00:43:36,901] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1209884: loss -26.5016
[2019-03-26 00:43:36,904] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1209884: learning rate 0.0010
[2019-03-26 00:43:37,179] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1209985: loss -182.2041
[2019-03-26 00:43:37,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1209986: learning rate 0.0010
[2019-03-26 00:43:37,901] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:43:37,910] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5049
[2019-03-26 00:43:37,916] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.6, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.910418577540867, 6.9112, 6.9112, 168.912956510431, 746633.2392995686, 746633.2392995686, 225668.1872943758], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5680800.0000, 
sim time next is 5681400.0000, 
raw observation next is [30.36666666666667, 68.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9325620203407565, 6.9112, 6.9112, 168.912956510431, 764627.7322469015, 764627.7322469015, 230924.9779568324], 
processed observation next is [0.0, 0.782608695652174, 0.6382306477093209, 0.6816666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9177585613911665, 0.0, 0.0, 0.8294399451523027, 0.21239659229080596, 0.21239659229080596, 0.3446641462042274], 
reward next is 0.6553, 
noisyNet noise sample is [array([1.8157034], dtype=float32), 0.3650539]. 
=============================================
[2019-03-26 00:43:38,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 1.2173413e-25 9.6908288e-21 3.5811642e-08], sum to 1.0000
[2019-03-26 00:43:38,130] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0389
[2019-03-26 00:43:38,140] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 952125.8842700975 W.
[2019-03-26 00:43:38,141] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1210336: loss 49.8246
[2019-03-26 00:43:38,143] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1210336: learning rate 0.0010
[2019-03-26 00:43:38,149] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.7, 95.0, 1.0, 2.0, 0.227101323784568, 1.0, 1.0, 0.227101323784568, 1.0, 1.0, 0.3855266376697876, 6.9112, 6.9112, 170.5573041426782, 952125.8842700975, 952125.8842700975, 277257.8341753698], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5545800.0000, 
sim time next is 5546400.0000, 
raw observation next is [25.66666666666667, 95.0, 1.0, 2.0, 0.3359776456369112, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5699984288023788, 6.911200000000001, 6.9112, 168.912956510431, 939059.3369295751, 939059.3369295745, 232633.675261941], 
processed observation next is [1.0, 0.17391304347826086, 0.4154818325434442, 0.95, 1.0, 1.0, 0.1999730670324231, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.47560784000290096, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26084981581377087, 0.2608498158137707, 0.3472144406894642], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.08835477], dtype=float32), -0.817866]. 
=============================================
[2019-03-26 00:43:38,606] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1210510: loss -18.5441
[2019-03-26 00:43:38,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1210510: learning rate 0.0010
[2019-03-26 00:43:38,916] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1210621: loss -93.6753
[2019-03-26 00:43:38,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1210621: learning rate 0.0010
[2019-03-26 00:43:38,922] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1210624: loss -209.7856
[2019-03-26 00:43:38,923] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1210624: learning rate 0.0010
[2019-03-26 00:43:42,194] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1211838: loss 0.5000
[2019-03-26 00:43:42,196] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1211838: learning rate 0.0010
[2019-03-26 00:43:42,423] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:43:42,436] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2420
[2019-03-26 00:43:42,444] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.36666666666667, 85.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9715934899434913, 6.9112, 6.9112, 168.912956510431, 786030.7430535096, 786030.7430535096, 240001.2893588672], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5601000.0000, 
sim time next is 5601600.0000, 
raw observation next is [28.1, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9772486785689829, 6.9112, 6.9112, 168.912956510431, 789834.2658355976, 789834.2658355976, 241378.6247342005], 
processed observation next is [1.0, 0.8695652173913043, 0.5308056872037916, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9722544860597352, 0.0, 0.0, 0.8294399451523027, 0.2193984071765549, 0.2193984071765549, 0.3602666040808963], 
reward next is 0.6397, 
noisyNet noise sample is [array([0.02938841], dtype=float32), 0.90917146]. 
=============================================
[2019-03-26 00:43:44,402] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1212648: loss 0.2904
[2019-03-26 00:43:44,405] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1212649: learning rate 0.0010
[2019-03-26 00:43:44,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:43:44,616] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6906
[2019-03-26 00:43:44,626] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8380873615312615, 6.9112, 6.9112, 168.912956510431, 698395.1503723983, 698395.1503723983, 209770.4726636097], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5631600.0000, 
sim time next is 5632200.0000, 
raw observation next is [25.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8372441877440848, 6.911199999999999, 6.9112, 168.912956510431, 697710.5388176047, 697710.5388176052, 209588.8968349893], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8015173021269327, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19380848300489018, 0.19380848300489034, 0.31281924900744673], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.4483182], dtype=float32), 0.5964642]. 
=============================================
[2019-03-26 00:43:44,687] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1212752: loss 1.3348
[2019-03-26 00:43:44,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1212752: learning rate 0.0010
[2019-03-26 00:43:45,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.7801112e-16], sum to 1.0000
[2019-03-26 00:43:45,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8508
[2019-03-26 00:43:45,995] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2498643.929758819 W.
[2019-03-26 00:43:46,005] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.34999999999999, 62.83333333333333, 1.0, 2.0, 0.8933385288312427, 1.0, 2.0, 0.8933385288312427, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2498643.929758819, 2498643.929758819, 467870.3511436419], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5845800.0000, 
sim time next is 5846400.0000, 
raw observation next is [32.3, 63.0, 1.0, 2.0, 0.5929977203719822, 1.0, 2.0, 0.5929977203719822, 1.0, 1.0, 1.02984093294331, 6.911199999999999, 6.9112, 170.5573041426782, 2487887.405531408, 2487887.405531408, 485410.4944793481], 
processed observation next is [1.0, 0.6956521739130435, 0.7298578199052131, 0.63, 1.0, 1.0, 0.5096358076770869, 1.0, 1.0, 0.5096358076770869, 1.0, 0.5, 1.036391381638183, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6910798348698355, 0.6910798348698355, 0.7244932753423107], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48959765], dtype=float32), -0.91892576]. 
=============================================
[2019-03-26 00:43:46,674] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1213491: loss 0.2180
[2019-03-26 00:43:46,678] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1213493: learning rate 0.0010
[2019-03-26 00:43:48,248] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1214071: loss 0.4962
[2019-03-26 00:43:48,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1214071: learning rate 0.0010
[2019-03-26 00:43:48,942] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1214329: loss 0.1933
[2019-03-26 00:43:48,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1214330: learning rate 0.0010
[2019-03-26 00:43:50,798] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1215022: loss 6.0183
[2019-03-26 00:43:50,802] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1215022: learning rate 0.0010
[2019-03-26 00:43:52,052] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:43:52,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9991
[2019-03-26 00:43:52,074] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.11666666666667, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9172479019881622, 6.9112, 6.9112, 168.912956510431, 752610.9806883372, 752610.9806883372, 227293.5385808297], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5788200.0000, 
sim time next is 5788800.0000, 
raw observation next is [27.1, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9172358522977964, 6.9112, 6.9112, 168.912956510431, 752518.5513056528, 752518.5513056528, 227287.2046627148], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8990681125582882, 0.0, 0.0, 0.8294399451523027, 0.2090329309182369, 0.2090329309182369, 0.33923463382494745], 
reward next is 0.6608, 
noisyNet noise sample is [array([-0.52659893], dtype=float32), -0.5960542]. 
=============================================
[2019-03-26 00:43:54,975] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1216561: loss 3.5712
[2019-03-26 00:43:54,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1216562: learning rate 0.0010
[2019-03-26 00:43:56,023] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2535058e-23 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 00:43:56,030] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5349
[2019-03-26 00:43:56,038] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.7, 89.66666666666667, 1.0, 1.0, 0.6320285530969021, 1.0, 1.0, 0.6320285530969021, 1.0, 2.0, 1.03, 6.981413528575913, 6.9112, 170.5573041426782, 2651812.763843458, 2601515.957220797, 500125.2917158388], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5797200.0000, 
sim time next is 5797800.0000, 
raw observation next is [26.65, 90.0, 1.0, 2.0, 0.4085531735517317, 1.0, 2.0, 0.4085531735517317, 1.0, 2.0, 0.7030192392227496, 6.9112, 6.9112, 170.5573041426782, 1713473.617683152, 1713473.617683152, 356739.9630577149], 
processed observation next is [1.0, 0.08695652173913043, 0.462085308056872, 0.9, 1.0, 1.0, 0.2874134621105201, 1.0, 1.0, 0.2874134621105201, 1.0, 1.0, 0.6378283405155482, 0.0, 0.0, 0.8375144448122397, 0.47596489380087553, 0.47596489380087553, 0.5324477060562909], 
reward next is 0.4676, 
noisyNet noise sample is [array([-0.0113874], dtype=float32), 0.22227554]. 
=============================================
[2019-03-26 00:43:57,065] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1217330: loss 0.7789
[2019-03-26 00:43:57,068] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1217330: learning rate 0.0010
[2019-03-26 00:43:58,398] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1217817: loss 1.2927
[2019-03-26 00:43:58,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1217817: learning rate 0.0010
[2019-03-26 00:43:58,435] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1217830: loss 1.7300
[2019-03-26 00:43:58,442] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1217832: learning rate 0.0010
[2019-03-26 00:43:58,442] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1217832: loss 0.8713
[2019-03-26 00:43:58,449] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1217833: learning rate 0.0010
[2019-03-26 00:43:58,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8026853e-16 6.5624010e-35 1.1928636e-31 7.6196809e-31 1.0000000e+00], sum to 1.0000
[2019-03-26 00:43:58,949] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4077
[2019-03-26 00:43:58,953] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.26666666666667, 76.83333333333333, 1.0, 2.0, 0.4991394467726274, 1.0, 2.0, 0.4991394467726274, 1.0, 2.0, 0.8668401511066203, 6.9112, 6.9112, 170.5573041426782, 2093764.025275702, 2093764.025275702, 414550.8327740689], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6079800.0000, 
sim time next is 6080400.0000, 
raw observation next is [29.4, 76.0, 1.0, 2.0, 0.4617780933791807, 1.0, 2.0, 0.4617780933791807, 1.0, 2.0, 0.8015488580545997, 6.911200000000001, 6.9112, 170.5573041426782, 1936900.900070688, 1936900.900070687, 389653.9712442753], 
processed observation next is [1.0, 0.391304347826087, 0.5924170616113744, 0.76, 1.0, 1.0, 0.3515398715411816, 1.0, 1.0, 0.3515398715411816, 1.0, 1.0, 0.757986412261707, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5380280277974133, 0.5380280277974131, 0.5815730914093661], 
reward next is 0.4184, 
noisyNet noise sample is [array([-0.32532567], dtype=float32), 1.0059175]. 
=============================================
[2019-03-26 00:43:59,569] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1218248: loss 2.2941
[2019-03-26 00:43:59,571] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1218248: learning rate 0.0010
[2019-03-26 00:44:00,185] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1218481: loss 0.0960
[2019-03-26 00:44:00,190] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1218481: learning rate 0.0010
[2019-03-26 00:44:00,249] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1218502: loss 1.4181
[2019-03-26 00:44:00,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1218503: learning rate 0.0010
[2019-03-26 00:44:00,390] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1218554: loss 4.2847
[2019-03-26 00:44:00,395] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1218554: learning rate 0.0010
[2019-03-26 00:44:04,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 3.4874992e-35 5.9434190e-36 1.2462428e-33 1.2009597e-22], sum to 1.0000
[2019-03-26 00:44:04,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9530
[2019-03-26 00:44:04,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1045327.030601331 W.
[2019-03-26 00:44:04,082] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.5, 86.33333333333334, 1.0, 2.0, 0.2493208563042789, 1.0, 2.0, 0.2493208563042789, 1.0, 2.0, 0.4286242125097137, 6.9112, 6.9112, 170.5573041426782, 1045327.030601331, 1045327.030601331, 284847.4455981667], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6072000.0000, 
sim time next is 6072600.0000, 
raw observation next is [27.65, 85.66666666666667, 1.0, 2.0, 0.258096517567548, 1.0, 2.0, 0.258096517567548, 1.0, 2.0, 0.4441935949920742, 6.9112, 6.9112, 170.5573041426782, 1082139.293651735, 1082139.293651735, 287938.4537107564], 
processed observation next is [1.0, 0.2608695652173913, 0.509478672985782, 0.8566666666666667, 1.0, 1.0, 0.10614038261150358, 1.0, 1.0, 0.10614038261150358, 1.0, 1.0, 0.32218731096594416, 0.0, 0.0, 0.8375144448122397, 0.30059424823659303, 0.30059424823659303, 0.42975888613545726], 
reward next is 0.5702, 
noisyNet noise sample is [array([-0.5434322], dtype=float32), -0.6156386]. 
=============================================
[2019-03-26 00:44:04,363] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1220014: loss -13.8725
[2019-03-26 00:44:04,371] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1220014: learning rate 0.0010
[2019-03-26 00:44:05,435] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:44:05,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7081
[2019-03-26 00:44:05,458] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.85, 87.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9630867941214487, 6.911199999999999, 6.9112, 168.912956510431, 781758.525956424, 781758.5259564245, 238019.1559379256], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5952600.0000, 
sim time next is 5953200.0000, 
raw observation next is [27.76666666666667, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9604454574212189, 6.9112, 6.9112, 168.912956510431, 779733.7933461386, 779733.7933461386, 237370.6225569642], 
processed observation next is [1.0, 0.9130434782608695, 0.515007898894155, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.951762752952706, 0.0, 0.0, 0.8294399451523027, 0.21659272037392738, 0.21659272037392738, 0.35428451127905103], 
reward next is 0.6457, 
noisyNet noise sample is [array([0.22013687], dtype=float32), 1.8225167]. 
=============================================
[2019-03-26 00:44:06,398] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1220771: loss -36.9583
[2019-03-26 00:44:06,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1220775: learning rate 0.0010
[2019-03-26 00:44:06,711] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1220890: loss -53.8594
[2019-03-26 00:44:06,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1220890: learning rate 0.0010
[2019-03-26 00:44:08,440] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1221523: loss -9.5797
[2019-03-26 00:44:08,442] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1221524: learning rate 0.0010
[2019-03-26 00:44:10,087] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1222133: loss 21.5074
[2019-03-26 00:44:10,090] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1222133: learning rate 0.0010
[2019-03-26 00:44:10,599] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1222325: loss 99.1048
[2019-03-26 00:44:10,601] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1222325: learning rate 0.0010
[2019-03-26 00:44:11,499] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 7.4706840e-37 4.7018716e-33 8.4717713e-34 3.5474111e-19], sum to 1.0000
[2019-03-26 00:44:11,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4853
[2019-03-26 00:44:11,522] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2018930.097688846 W.
[2019-03-26 00:44:11,529] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.3, 77.0, 1.0, 2.0, 0.4813163301536394, 1.0, 2.0, 0.4813163301536394, 1.0, 1.0, 0.8358872917341725, 6.911200000000001, 6.9112, 170.5573041426782, 2018930.097688846, 2018930.097688845, 402465.762731429], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6174000.0000, 
sim time next is 6174600.0000, 
raw observation next is [29.38333333333333, 76.5, 1.0, 2.0, 0.7253492437268688, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.997396949091298, 6.9112, 168.9124430081732, 1910601.95238924, 1849451.055935618, 390567.4012235233], 
processed observation next is [1.0, 0.4782608695652174, 0.5916271721958924, 0.765, 1.0, 1.0, 0.6690954743697215, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008619694909129815, 0.0, 0.8294374236211345, 0.5307227645525667, 0.5137364044265605, 0.5829364197366019], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7625693], dtype=float32), 0.4842493]. 
=============================================
[2019-03-26 00:44:12,443] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1223000: loss 27.5858
[2019-03-26 00:44:12,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1223000: learning rate 0.0010
[2019-03-26 00:44:16,962] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1224674: loss 0.4975
[2019-03-26 00:44:16,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1224674: learning rate 0.0010
[2019-03-26 00:44:17,859] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 00:44:17,860] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:44:17,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:44:17,862] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:44:17,862] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:44:17,863] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:44:17,865] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:44:17,868] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:44:17,868] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:44:17,864] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:44:17,872] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:44:17,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-26 00:44:17,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-26 00:44:17,897] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-26 00:44:17,964] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-26 00:44:17,990] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-26 00:44:24,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:44:24,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.22293434333334, 81.23121935666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7338868824835222, 6.911199999999999, 6.9112, 168.912956510431, 629010.7448602564, 629010.744860257, 188962.2107297771]
[2019-03-26 00:44:24,984] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:44:24,986] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7767698811547262
[2019-03-26 00:44:47,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:44:47,544] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7699052754769105, 6.9112, 6.9112, 168.912956510431, 654078.0946217645, 654078.0946217645, 195897.3522214015]
[2019-03-26 00:44:47,546] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:44:47,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.025120247096374837
[2019-03-26 00:44:47,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:44:47,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.45392175666666, 93.36124999333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.671924716470588, 6.911200000000001, 6.9112, 168.912956510431, 580087.8379163556, 580087.837916355, 177786.6954327231]
[2019-03-26 00:44:47,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:44:47,765] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08890497504166783
[2019-03-26 00:45:14,052] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:45:14,054] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.48768548, 66.70585274, 1.0, 2.0, 0.5338956687573251, 1.0, 2.0, 0.5338956687573251, 1.0, 2.0, 0.9272002146357902, 6.911200000000001, 6.9112, 171.5212843490159, 2239694.706551416, 2239694.706551415, 439583.0573567344]
[2019-03-26 00:45:14,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:45:14,061] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.99894381e-01 0.00000000e+00 1.40007266e-34 1.16475760e-37
 1.05663676e-04], sampled 0.937917924644978
[2019-03-26 00:45:14,061] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2239694.706551416 W.
[2019-03-26 00:45:21,958] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:45:21,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.3, 66.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.969771379099489, 6.9112, 6.9112, 168.912956510431, 788241.9931039185, 788241.9931039185, 239738.3019309873]
[2019-03-26 00:45:21,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:45:21,960] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.583005998873091
[2019-03-26 00:45:47,936] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.0528904], dtype=float32), -0.036166053]
[2019-03-26 00:45:47,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.63333333333334, 75.66666666666667, 1.0, 2.0, 0.7815129594361547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1092245.211882388, 1092245.211882388, 239425.0956764668]
[2019-03-26 00:45:47,942] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:45:47,944] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9585342e-01 7.6574100e-38 7.4156126e-32 1.0580986e-34 4.1465587e-03], sampled 0.2573752903363976
[2019-03-26 00:45:47,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1092245.211882388 W.
[2019-03-26 00:46:26,691] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8035.2721 3035595787.3257 1138.0000
[2019-03-26 00:46:26,960] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8163.7399 2988925437.1879 970.0000
[2019-03-26 00:46:27,039] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7497.5381 3164355735.8031 1616.0000
[2019-03-26 00:46:27,297] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7423.9834 3361534470.0549 1647.0000
[2019-03-26 00:46:27,335] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7241.2135 3235343379.2525 1842.0000
[2019-03-26 00:46:28,352] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1225000, evaluation results [1225000.0, 7423.9833939962045, 3361534470.054894, 1647.0, 7497.538065491314, 3164355735.8030753, 1616.0, 8163.739921375841, 2988925437.187864, 970.0, 7241.213494862038, 3235343379.2525096, 1842.0, 8035.27212160889, 3035595787.3257337, 1138.0]
[2019-03-26 00:46:29,025] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:46:29,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9548
[2019-03-26 00:46:29,036] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.43333333333334, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.907088154198961, 6.9112, 6.9112, 168.912956510431, 745046.9136892973, 745046.9136892973, 224935.4726306789], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6301200.0000, 
sim time next is 6301800.0000, 
raw observation next is [27.4, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9062034819567315, 6.9112, 6.9112, 168.912956510431, 744666.9390442203, 744666.9390442203, 224742.7837578266], 
processed observation next is [0.0, 0.9565217391304348, 0.4976303317535545, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.885614002386258, 0.0, 0.0, 0.8294399451523027, 0.2068519275122834, 0.2068519275122834, 0.3354369906833233], 
reward next is 0.6646, 
noisyNet noise sample is [array([-0.6725309], dtype=float32), 3.2636864]. 
=============================================
[2019-03-26 00:46:29,274] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1225343: loss 2.2639
[2019-03-26 00:46:29,278] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1225344: learning rate 0.0010
[2019-03-26 00:46:30,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1225870: loss 10.5921
[2019-03-26 00:46:30,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1225870: learning rate 0.0010
[2019-03-26 00:46:30,721] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1225873: loss -57.1972
[2019-03-26 00:46:30,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1225873: learning rate 0.0010
[2019-03-26 00:46:30,828] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1225914: loss 4.2239
[2019-03-26 00:46:30,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1225914: learning rate 0.0010
[2019-03-26 00:46:31,823] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:46:31,832] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4831
[2019-03-26 00:46:31,836] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9025068423612168, 6.911199999999999, 6.9112, 168.912956510431, 741300.0274749418, 741300.0274749424, 223867.2144493561], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6210000.0000, 
sim time next is 6210600.0000, 
raw observation next is [27.25, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9008149174736756, 6.911200000000001, 6.9112, 168.912956510431, 740086.3573434264, 740086.3573434259, 223481.0364890909], 
processed observation next is [1.0, 0.9130434782608695, 0.490521327014218, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8790425822849701, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20557954370650736, 0.2055795437065072, 0.3335537858046133], 
reward next is 0.6664, 
noisyNet noise sample is [array([0.49339813], dtype=float32), -0.28558716]. 
=============================================
[2019-03-26 00:46:32,085] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1226377: loss -15.2122
[2019-03-26 00:46:32,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1226377: learning rate 0.0010
[2019-03-26 00:46:32,610] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1226567: loss 35.1868
[2019-03-26 00:46:32,616] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1226567: learning rate 0.0010
[2019-03-26 00:46:32,722] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1226608: loss -166.3141
[2019-03-26 00:46:32,723] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1226608: learning rate 0.0010
[2019-03-26 00:46:32,766] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1226626: loss -26.6212
[2019-03-26 00:46:32,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1226626: learning rate 0.0010
[2019-03-26 00:46:35,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:46:35,740] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8474
[2019-03-26 00:46:35,746] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.6, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8836849865356541, 6.9112, 6.9112, 168.912956510431, 727968.7037008121, 727968.7037008121, 219618.3277927256], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6222600.0000, 
sim time next is 6223200.0000, 
raw observation next is [26.56666666666666, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8829333318577741, 6.9112, 6.9112, 168.912956510431, 727454.0499943179, 727454.0499943179, 219451.1518409165], 
processed observation next is [0.0, 0.0, 0.4581358609794626, 0.8966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8572357705582612, 0.0, 0.0, 0.8294399451523027, 0.20207056944286608, 0.20207056944286608, 0.3275390325983828], 
reward next is 0.6725, 
noisyNet noise sample is [array([0.09153011], dtype=float32), 0.8015432]. 
=============================================
[2019-03-26 00:46:35,953] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1227811: loss 162.2950
[2019-03-26 00:46:35,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1227812: learning rate 0.0010
[2019-03-26 00:46:38,237] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1228651: loss 43.5795
[2019-03-26 00:46:38,242] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1228652: learning rate 0.0010
[2019-03-26 00:46:38,463] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1228733: loss 166.8269
[2019-03-26 00:46:38,468] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1228734: learning rate 0.0010
[2019-03-26 00:46:40,051] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1229319: loss 59.1118
[2019-03-26 00:46:40,054] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1229319: learning rate 0.0010
[2019-03-26 00:46:41,713] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:46:41,722] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3636
[2019-03-26 00:46:41,726] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.46666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8496487901623365, 6.911200000000001, 6.9112, 168.912956510431, 704838.0802898043, 704838.0802898037, 212190.1092865178], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6280800.0000, 
sim time next is 6281400.0000, 
raw observation next is [30.43333333333333, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8475774564851105, 6.911199999999999, 6.9112, 168.912956510431, 703422.489804685, 703422.4898046857, 211746.6970733462], 
processed observation next is [0.0, 0.6956521739130435, 0.6413902053712479, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8141188493720859, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19539513605685693, 0.19539513605685713, 0.31603984637812865], 
reward next is 0.6840, 
noisyNet noise sample is [array([-0.615593], dtype=float32), 0.49376655]. 
=============================================
[2019-03-26 00:46:42,331] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1230164: loss 404.8295
[2019-03-26 00:46:42,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1230164: learning rate 0.0010
[2019-03-26 00:46:42,841] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1230350: loss 222.4407
[2019-03-26 00:46:42,844] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1230350: learning rate 0.0010
[2019-03-26 00:46:44,704] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1231036: loss 176.2304
[2019-03-26 00:46:44,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1231036: learning rate 0.0010
[2019-03-26 00:46:49,268] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1232710: loss 43.6395
[2019-03-26 00:46:49,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1232711: learning rate 0.0010
[2019-03-26 00:46:50,963] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1233339: loss 440.0529
[2019-03-26 00:46:50,967] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1233341: learning rate 0.0010
[2019-03-26 00:46:52,342] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1233852: loss 328.9355
[2019-03-26 00:46:52,345] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1233852: learning rate 0.0010
[2019-03-26 00:46:52,351] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1233853: loss 203.7101
[2019-03-26 00:46:52,354] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1233854: learning rate 0.0010
[2019-03-26 00:46:52,448] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1233891: loss 297.1506
[2019-03-26 00:46:52,457] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1233893: learning rate 0.0010
[2019-03-26 00:46:53,664] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1234334: loss 531.7174
[2019-03-26 00:46:53,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1234335: learning rate 0.0010
[2019-03-26 00:46:53,979] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1234448: loss 306.4868
[2019-03-26 00:46:53,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1234448: learning rate 0.0010
[2019-03-26 00:46:54,025] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1234464: loss 614.0571
[2019-03-26 00:46:54,028] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1234465: learning rate 0.0010
[2019-03-26 00:46:54,133] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1234501: loss 438.7292
[2019-03-26 00:46:54,136] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1234502: learning rate 0.0010
[2019-03-26 00:46:54,954] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 3.5763605e-37 2.5143156e-38 1.9039919e-35 2.2841753e-28], sum to 1.0000
[2019-03-26 00:46:54,971] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7698
[2019-03-26 00:46:54,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2112112.955407888 W.
[2019-03-26 00:46:54,989] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.13333333333333, 59.0, 1.0, 2.0, 0.8693336404699623, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.973165875522238, 6.9112, 168.912587167859, 2112112.955407888, 2068152.326735441, 426808.1309861924], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6535200.0000, 
sim time next is 6535800.0000, 
raw observation next is [31.0, 59.5, 1.0, 2.0, 0.4995689631372301, 1.0, 1.0, 0.4995689631372301, 1.0, 2.0, 0.8497077165333258, 6.9112, 6.9112, 170.5573041426782, 2095567.500334606, 2095567.500334606, 411579.5372130381], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.595, 1.0, 1.0, 0.3970710399243736, 1.0, 0.5, 0.3970710399243736, 1.0, 1.0, 0.8167167274796656, 0.0, 0.0, 0.8375144448122397, 0.5821020834262794, 0.5821020834262794, 0.6142978167358777], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7423265], dtype=float32), 0.65256035]. 
=============================================
[2019-03-26 00:46:55,664] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:46:55,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1505
[2019-03-26 00:46:55,682] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.5, 66.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7823035014249438, 6.9112, 6.9112, 168.912956510431, 651185.3154776565, 651185.3154776565, 198117.3797588429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6543600.0000, 
sim time next is 6544200.0000, 
raw observation next is [29.4, 66.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7928683979953391, 6.9112, 6.9112, 168.912956510431, 660720.9029628974, 660720.9029628974, 200276.1848358663], 
processed observation next is [1.0, 0.7391304347826086, 0.5924170616113744, 0.6666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7474004853601696, 0.0, 0.0, 0.8294399451523027, 0.1835335841563604, 0.1835335841563604, 0.29891967885950194], 
reward next is 0.7011, 
noisyNet noise sample is [array([0.15502454], dtype=float32), -0.009028374]. 
=============================================
[2019-03-26 00:46:56,698] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0372609e-14 9.8737842e-29 1.8046600e-26 5.6493411e-26 1.0000000e+00], sum to 1.0000
[2019-03-26 00:46:56,706] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7394
[2019-03-26 00:46:56,712] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.15, 60.66666666666666, 1.0, 2.0, 0.4545411903097504, 1.0, 2.0, 0.4545411903097504, 1.0, 1.0, 0.7671729057120901, 6.9112, 6.9112, 170.5573041426782, 1906519.130387432, 1906519.130387432, 381454.5299695802], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6702600.0000, 
sim time next is 6703200.0000, 
raw observation next is [30.2, 60.0, 1.0, 2.0, 0.4484194238130109, 1.0, 2.0, 0.4484194238130109, 1.0, 2.0, 0.7564632698074851, 6.9112, 6.9112, 170.5573041426782, 1880819.569354195, 1880819.569354195, 377672.240042631], 
processed observation next is [1.0, 0.6086956521739131, 0.6303317535545023, 0.6, 1.0, 1.0, 0.33544508893133845, 1.0, 1.0, 0.33544508893133845, 1.0, 1.0, 0.7030039875701038, 0.0, 0.0, 0.8375144448122397, 0.5224498803761652, 0.5224498803761652, 0.5636899105113895], 
reward next is 0.4363, 
noisyNet noise sample is [array([-1.391179], dtype=float32), -0.87549424]. 
=============================================
[2019-03-26 00:46:58,060] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1235953: loss 0.5435
[2019-03-26 00:46:58,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1235953: learning rate 0.0010
[2019-03-26 00:46:59,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9831938e-07 3.5178610e-31 6.7475386e-32 7.5688289e-32 9.9999917e-01], sum to 1.0000
[2019-03-26 00:46:59,023] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6914
[2019-03-26 00:46:59,031] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 55.33333333333333, 1.0, 2.0, 0.4879758093928143, 1.0, 2.0, 0.4879758093928143, 1.0, 2.0, 0.8317009852839953, 6.911199999999999, 6.9112, 170.5573041426782, 2046890.644386294, 2046890.644386295, 404130.9594662613], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6529200.0000, 
sim time next is 6529800.0000, 
raw observation next is [32.05, 55.16666666666667, 1.0, 2.0, 0.4855244365526843, 1.0, 2.0, 0.4855244365526843, 1.0, 2.0, 0.827590783666352, 6.9112, 6.9112, 170.5573041426782, 2036598.204463568, 2036598.204463568, 402522.53997805], 
processed observation next is [1.0, 0.5652173913043478, 0.7180094786729857, 0.5516666666666667, 1.0, 1.0, 0.3801499235574509, 1.0, 1.0, 0.3801499235574509, 1.0, 1.0, 0.7897448581296974, 0.0, 0.0, 0.8375144448122397, 0.5657217234621023, 0.5657217234621023, 0.600779910415], 
reward next is 0.3992, 
noisyNet noise sample is [array([-0.44783282], dtype=float32), 2.7258642]. 
=============================================
[2019-03-26 00:46:59,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.4496414e-35 1.6362125e-26], sum to 1.0000
[2019-03-26 00:46:59,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7133
[2019-03-26 00:46:59,169] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.75, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8341102912693718, 6.911199999999999, 6.9112, 168.912956510431, 694212.5258494577, 694212.5258494583, 208888.5101017445], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6654600.0000, 
sim time next is 6655200.0000, 
raw observation next is [25.66666666666667, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.832909139008916, 6.9112, 6.9112, 168.912956510431, 693357.5362922495, 693357.5362922495, 208634.7119285845], 
processed observation next is [1.0, 0.0, 0.4154818325434442, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7962306573279464, 0.0, 0.0, 0.8294399451523027, 0.19259931563673596, 0.19259931563673596, 0.3113950924307231], 
reward next is 0.6886, 
noisyNet noise sample is [array([-0.6570199], dtype=float32), 1.4454554]. 
=============================================
[2019-03-26 00:46:59,925] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1236637: loss 0.2348
[2019-03-26 00:46:59,927] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1236637: learning rate 0.0010
[2019-03-26 00:47:00,354] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1236795: loss 0.3328
[2019-03-26 00:47:00,357] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1236795: learning rate 0.0010
[2019-03-26 00:47:01,865] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1237354: loss 0.1645
[2019-03-26 00:47:01,868] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1237354: learning rate 0.0010
[2019-03-26 00:47:04,237] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1238226: loss 0.0291
[2019-03-26 00:47:04,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1238226: learning rate 0.0010
[2019-03-26 00:47:04,517] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1238329: loss 0.0330
[2019-03-26 00:47:04,520] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1238329: learning rate 0.0010
[2019-03-26 00:47:04,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999559e-01 1.2464626e-30 7.2863535e-30 3.4386164e-29 4.4082708e-06], sum to 1.0000
[2019-03-26 00:47:04,898] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5305
[2019-03-26 00:47:04,911] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2212203.188839014 W.
[2019-03-26 00:47:04,916] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.4, 68.0, 1.0, 2.0, 0.5273454312313843, 1.0, 2.0, 0.5273454312313843, 1.0, 2.0, 0.9107260722461737, 6.9112, 6.9112, 170.5573041426782, 2212203.188839014, 2212203.188839014, 433593.7563896792], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6607200.0000, 
sim time next is 6607800.0000, 
raw observation next is [30.55, 67.5, 1.0, 2.0, 0.9974635795546101, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990252507295546, 6.9112, 168.9124860271019, 2291451.852007591, 2235369.438764932, 463111.4597630926], 
processed observation next is [1.0, 0.4782608695652174, 0.6469194312796209, 0.675, 1.0, 1.0, 0.9969440717525423, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007905250729554591, 0.0, 0.8294376348637685, 0.636514403335442, 0.6209359552124811, 0.6912111339747651], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24240169], dtype=float32), 0.83245844]. 
=============================================
[2019-03-26 00:47:06,317] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1238999: loss 0.0559
[2019-03-26 00:47:06,322] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1238999: learning rate 0.0010
[2019-03-26 00:47:07,967] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:07,974] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0628
[2019-03-26 00:47:07,982] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.76666666666667, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8588321642999803, 6.911199999999999, 6.9112, 168.912956510431, 719620.7323303864, 719620.7323303871, 214414.3592683168], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6672000.0000, 
sim time next is 6672600.0000, 
raw observation next is [24.85, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8528676244283921, 6.911199999999999, 6.9112, 168.912956510431, 714060.8127822543, 714060.812782255, 213082.253869839], 
processed observation next is [1.0, 0.21739130434782608, 0.37677725118483424, 0.945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8205702736931612, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19835022577284842, 0.1983502257728486, 0.318033214731103], 
reward next is 0.6820, 
noisyNet noise sample is [array([0.11539764], dtype=float32), 0.5896624]. 
=============================================
[2019-03-26 00:47:11,010] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1240723: loss 0.0603
[2019-03-26 00:47:11,016] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1240723: learning rate 0.0010
[2019-03-26 00:47:11,782] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:11,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5626
[2019-03-26 00:47:11,798] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333333, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5979699405094321, 6.911199999999999, 6.9112, 168.912956510431, 521575.4064605988, 521575.4064605994, 165753.4187776241], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6742200.0000, 
sim time next is 6742800.0000, 
raw observation next is [23.3, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5943068605694317, 6.911199999999999, 6.9112, 168.912956510431, 518690.2853947809, 518690.2853947815, 165193.5832464868], 
processed observation next is [1.0, 0.043478260869565216, 0.3033175355450238, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5052522689871118, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14408063483188358, 0.14408063483188374, 0.24655758693505495], 
reward next is 0.7534, 
noisyNet noise sample is [array([-0.90854025], dtype=float32), -0.1460143]. 
=============================================
[2019-03-26 00:47:12,701] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1241351: loss 0.0209
[2019-03-26 00:47:12,705] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1241351: learning rate 0.0010
[2019-03-26 00:47:14,034] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1241845: loss 0.0138
[2019-03-26 00:47:14,037] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1241845: learning rate 0.0010
[2019-03-26 00:47:14,427] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1241994: loss 0.0369
[2019-03-26 00:47:14,428] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1241994: learning rate 0.0010
[2019-03-26 00:47:14,494] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1242016: loss 0.0199
[2019-03-26 00:47:14,496] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1242017: learning rate 0.0010
[2019-03-26 00:47:15,884] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1242530: loss 0.0680
[2019-03-26 00:47:15,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1242531: learning rate 0.0010
[2019-03-26 00:47:15,973] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1242564: loss 0.0164
[2019-03-26 00:47:15,979] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1242564: learning rate 0.0010
[2019-03-26 00:47:16,031] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1242588: loss 0.0162
[2019-03-26 00:47:16,036] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1242588: learning rate 0.0010
[2019-03-26 00:47:16,129] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1242617: loss 0.0423
[2019-03-26 00:47:16,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1242617: learning rate 0.0010
[2019-03-26 00:47:19,410] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1243828: loss -47.5174
[2019-03-26 00:47:19,413] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1243830: learning rate 0.0010
[2019-03-26 00:47:21,335] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1244543: loss 2.3657
[2019-03-26 00:47:21,336] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1244543: learning rate 0.0010
[2019-03-26 00:47:21,748] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1244698: loss -145.8272
[2019-03-26 00:47:21,752] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1244699: learning rate 0.0010
[2019-03-26 00:47:21,894] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:21,904] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9402
[2019-03-26 00:47:21,909] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.655307534228726, 6.9112, 6.9112, 168.912956510431, 565620.250718045, 565620.250718045, 174971.8625850313], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6858000.0000, 
sim time next is 6858600.0000, 
raw observation next is [26.28333333333334, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6516144487707066, 6.9112, 6.9112, 168.912956510431, 562859.2528873355, 562859.2528873355, 174352.7379882984], 
processed observation next is [0.0, 0.391304347826087, 0.444707740916272, 0.665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5751395716715934, 0.0, 0.0, 0.8294399451523027, 0.1563497924687043, 0.1563497924687043, 0.26022796714671403], 
reward next is 0.7398, 
noisyNet noise sample is [array([-0.5302679], dtype=float32), 0.7381839]. 
=============================================
[2019-03-26 00:47:23,396] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1245302: loss -145.2790
[2019-03-26 00:47:23,398] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1245302: learning rate 0.0010
[2019-03-26 00:47:26,049] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1246283: loss -19.9056
[2019-03-26 00:47:26,051] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1246283: loss -18.7516
[2019-03-26 00:47:26,053] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1246283: learning rate 0.0010
[2019-03-26 00:47:26,055] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1246283: learning rate 0.0010
[2019-03-26 00:47:28,040] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1247017: loss -57.2706
[2019-03-26 00:47:28,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1247017: learning rate 0.0010
[2019-03-26 00:47:28,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:28,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2542
[2019-03-26 00:47:28,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1897677.97415266 W.
[2019-03-26 00:47:28,199] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.6, 67.0, 1.0, 2.0, 0.4524351983764969, 1.0, 2.0, 0.4524351983764969, 1.0, 1.0, 0.7602436911991735, 6.911200000000001, 6.9112, 170.5573041426782, 1897677.97415266, 1897677.97415266, 379593.2341247184], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7124400.0000, 
sim time next is 7125000.0000, 
raw observation next is [28.48333333333333, 67.66666666666667, 1.0, 2.0, 0.5809452665723003, 1.0, 2.0, 0.5809452665723003, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1624256.929356884, 1624256.929356884, 327307.9325295876], 
processed observation next is [1.0, 0.4782608695652174, 0.5489731437598735, 0.6766666666666667, 1.0, 1.0, 0.4951147790027714, 1.0, 1.0, 0.4951147790027714, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4511824803769122, 0.4511824803769122, 0.4885193022829665], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11386423], dtype=float32), -2.043122]. 
=============================================
[2019-03-26 00:47:28,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[47.73911 ]
 [48.163837]
 [48.05707 ]
 [48.22564 ]
 [48.24858 ]], R is [[48.51477432]
 [48.46306992]
 [48.44694901]
 [48.43857956]
 [48.45705032]].
[2019-03-26 00:47:30,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:30,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6127
[2019-03-26 00:47:30,096] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 60.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7890325736140756, 6.911199999999999, 6.9112, 168.912956510431, 662880.5402011555, 662880.5402011562, 199623.3331462009], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6972000.0000, 
sim time next is 6972600.0000, 
raw observation next is [30.0, 59.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.784421306214647, 6.911199999999999, 6.9112, 168.912956510431, 659796.1706858793, 659796.1706858799, 198705.278083413], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7370991539203012, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18327671407941093, 0.18327671407941107, 0.2965750419155418], 
reward next is 0.7034, 
noisyNet noise sample is [array([0.0019002], dtype=float32), -0.33016846]. 
=============================================
[2019-03-26 00:47:31,823] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:47:31,834] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2249
[2019-03-26 00:47:31,846] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1306598.267146076 W.
[2019-03-26 00:47:31,851] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.2, 76.0, 1.0, 2.0, 0.9347905186254374, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1306598.267146076, 1306598.267146076, 279682.8000575444], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7131600.0000, 
sim time next is 7132200.0000, 
raw observation next is [27.08333333333334, 76.83333333333334, 1.0, 2.0, 0.4413703785738169, 1.0, 1.0, 0.4413703785738169, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1233796.817186343, 1233796.817186343, 283039.5356510594], 
processed observation next is [1.0, 0.5652173913043478, 0.4826224328594, 0.7683333333333334, 1.0, 1.0, 0.32695226334194805, 1.0, 0.5, 0.32695226334194805, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.34272133810731753, 0.34272133810731753, 0.4224470681359096], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.04406564], dtype=float32), 0.3336477]. 
=============================================
[2019-03-26 00:47:32,882] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1248801: loss -169.6594
[2019-03-26 00:47:32,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1248801: learning rate 0.0010
[2019-03-26 00:47:34,537] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1249412: loss -13.1937
[2019-03-26 00:47:34,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1249412: learning rate 0.0010
[2019-03-26 00:47:35,678] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1249833: loss 49.0434
[2019-03-26 00:47:35,681] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1249833: learning rate 0.0010
[2019-03-26 00:47:36,135] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 00:47:36,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:47:36,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:47:36,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:47:36,145] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:47:36,146] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:47:36,147] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:47:36,147] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:47:36,148] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:47:36,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:47:36,146] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:47:36,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-26 00:47:36,199] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-26 00:47:36,224] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-26 00:47:36,250] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-26 00:47:36,252] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-26 00:47:53,484] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:47:53,485] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.10459846333334, 93.03613507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7056850730006052, 6.911200000000001, 6.9112, 168.912956510431, 620558.0062415607, 620558.0062415601, 183494.895493469]
[2019-03-26 00:47:53,486] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:47:53,490] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6682532219940106
[2019-03-26 00:48:04,835] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:48:04,837] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.76666666666667, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7761156492092574, 6.911200000000001, 6.9112, 168.912956510431, 664297.7673715941, 664297.7673715935, 197162.6268035705]
[2019-03-26 00:48:04,839] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:48:04,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.20494588444070216
[2019-03-26 00:48:16,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:48:16,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 89.0, 1.0, 2.0, 0.6758741973751667, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.964697482362595, 6.9112, 168.9125931651242, 1841368.793764253, 1803415.920174833, 380837.5960483645]
[2019-03-26 00:48:16,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:48:16,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07615753881378828
[2019-03-26 00:48:16,760] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1841368.793764253 W.
[2019-03-26 00:48:41,329] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:48:41,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [36.47540037, 62.77739141, 1.0, 2.0, 0.7097595977471257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 991915.6299943401, 991915.6299943395, 222978.5462990624]
[2019-03-26 00:48:41,332] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:48:41,334] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3250682e-37], sampled 0.1984502445061288
[2019-03-26 00:48:41,334] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 991915.6299943401 W.
[2019-03-26 00:48:47,112] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:48:47,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.994412325, 69.51098277833333, 1.0, 2.0, 0.8240364315387123, 1.0, 1.0, 0.7326082552836186, 1.0, 2.0, 1.03, 7.004579091458796, 6.9112, 171.5212843490159, 3074314.740510994, 3007045.433377895, 563581.6324049084]
[2019-03-26 00:48:47,116] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:48:47,119] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.9999928e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.7112645e-07], sampled 0.2677031567601703
[2019-03-26 00:48:47,122] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3074314.740510994 W.
[2019-03-26 00:49:14,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0981392], dtype=float32), -0.088219546]
[2019-03-26 00:49:14,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.7, 79.0, 1.0, 2.0, 0.8410555178252075, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984092526516, 6.9112, 168.9123160067589, 2072533.754750822, 2005290.912275961, 418221.749006609]
[2019-03-26 00:49:14,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:49:14,215] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3111564e-09], sampled 0.0684652164477898
[2019-03-26 00:49:14,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2072533.754750822 W.
[2019-03-26 00:49:44,473] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7036.7602 3185427682.4755 2454.0000
[2019-03-26 00:49:44,826] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7348.7691 3106222512.7417 2016.0000
[2019-03-26 00:49:45,371] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7300.8328 3319493447.6708 2110.0000
[2019-03-26 00:49:45,469] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8060.9800 2938183763.2995 1378.0000
[2019-03-26 00:49:45,582] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7929.8545 2989531893.0394 1549.0000
[2019-03-26 00:49:46,600] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1250000, evaluation results [1250000.0, 7300.832786309296, 3319493447.6707664, 2110.0, 7348.769059440644, 3106222512.7416863, 2016.0, 8060.979966859248, 2938183763.2994857, 1378.0, 7036.760161052395, 3185427682.475518, 2454.0, 7929.854532825419, 2989531893.039411, 1549.0]
[2019-03-26 00:49:46,646] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1250021: loss -19.0045
[2019-03-26 00:49:46,651] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1250022: learning rate 0.0010
[2019-03-26 00:49:46,700] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1250046: loss -89.6899
[2019-03-26 00:49:46,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1250048: learning rate 0.0010
[2019-03-26 00:49:47,505] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1250347: loss -165.8817
[2019-03-26 00:49:47,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1250347: learning rate 0.0010
[2019-03-26 00:49:47,830] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1250461: loss -146.2617
[2019-03-26 00:49:47,832] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1250461: learning rate 0.0010
[2019-03-26 00:49:47,993] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1250521: loss -124.4411
[2019-03-26 00:49:48,000] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1250523: loss -14.7931
[2019-03-26 00:49:48,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1250523: learning rate 0.0010
[2019-03-26 00:49:48,004] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1250524: learning rate 0.0010
[2019-03-26 00:49:50,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:49:50,301] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4123
[2019-03-26 00:49:50,309] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.43333333333334, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8655795522983324, 6.911199999999999, 6.9112, 168.912956510431, 729712.0174479375, 729712.0174479381, 216011.9901522545], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7096800.0000, 
sim time next is 7097400.0000, 
raw observation next is [24.4, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8494314705440494, 6.9112, 6.9112, 168.912956510431, 716214.6686024172, 716214.6686024172, 212434.1924314316], 
processed observation next is [1.0, 0.13043478260869565, 0.3554502369668246, 0.935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8163798421268894, 0.0, 0.0, 0.8294399451523027, 0.198948519056227, 0.198948519056227, 0.317065958852883], 
reward next is 0.6829, 
noisyNet noise sample is [array([-1.4283518], dtype=float32), 0.5083774]. 
=============================================
[2019-03-26 00:49:51,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:49:51,328] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8507
[2019-03-26 00:49:51,335] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.16666666666666, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9361501246719399, 6.9112, 6.9112, 168.912956510431, 791467.3507146196, 791467.3507146196, 232524.6439380824], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7103400.0000, 
sim time next is 7104000.0000, 
raw observation next is [24.33333333333333, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8600971674971353, 6.911199999999999, 6.9112, 168.912956510431, 727061.4753997456, 727061.4753997462, 214822.3285776552], 
processed observation next is [1.0, 0.21739130434782608, 0.35229067930489716, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8293867896306526, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20196152094437378, 0.20196152094437395, 0.3206303411606794], 
reward next is 0.6794, 
noisyNet noise sample is [array([-0.360609], dtype=float32), -0.48732626]. 
=============================================
[2019-03-26 00:49:51,361] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[57.596813]
 [57.779785]
 [57.540577]
 [57.5265  ]
 [57.750908]], R is [[58.1662178 ]
 [58.23750305]
 [58.3316803 ]
 [58.40642548]
 [58.47672272]].
[2019-03-26 00:49:51,595] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1251855: loss 13.3683
[2019-03-26 00:49:51,597] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1251855: learning rate 0.0010
[2019-03-26 00:49:53,170] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:49:53,178] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8057
[2019-03-26 00:49:53,192] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1811300.977670049 W.
[2019-03-26 00:49:53,199] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.15, 85.0, 1.0, 2.0, 0.6477885073752685, 1.0, 2.0, 0.6477885073752685, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1811300.977670049, 1811300.977670049, 352563.0775045116], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7144200.0000, 
sim time next is 7144800.0000, 
raw observation next is [26.13333333333333, 85.33333333333333, 1.0, 2.0, 0.6128606123931742, 1.0, 2.0, 0.6128606123931742, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1713559.949109079, 1713559.949109079, 339040.5508671639], 
processed observation next is [1.0, 0.6956521739130435, 0.43759873617693507, 0.8533333333333333, 1.0, 1.0, 0.5335670028833425, 1.0, 1.0, 0.5335670028833425, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.47598887475252194, 0.47598887475252194, 0.5060306729360655], 
reward next is 0.4940, 
noisyNet noise sample is [array([0.47300375], dtype=float32), 0.58679706]. 
=============================================
[2019-03-26 00:49:53,523] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1252565: loss 8.6003
[2019-03-26 00:49:53,527] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1252565: learning rate 0.0010
[2019-03-26 00:49:53,906] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1252707: loss 7.7768
[2019-03-26 00:49:53,911] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1252708: learning rate 0.0010
[2019-03-26 00:49:54,897] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:49:54,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9485
[2019-03-26 00:49:54,912] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.06666666666667, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7918572519719332, 6.9112, 6.9112, 168.912956510431, 665447.246026144, 665447.246026144, 200202.1054787394], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7158000.0000, 
sim time next is 7158600.0000, 
raw observation next is [26.05, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7924186980832952, 6.911200000000001, 6.9112, 168.912956510431, 665866.1073400227, 665866.1073400221, 200315.5412373013], 
processed observation next is [1.0, 0.8695652173913043, 0.43364928909952616, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7468520708332869, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18496280759445075, 0.18496280759445058, 0.29897841975716616], 
reward next is 0.7010, 
noisyNet noise sample is [array([1.1897671], dtype=float32), -1.1440603]. 
=============================================
[2019-03-26 00:49:55,355] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1253247: loss 12.6427
[2019-03-26 00:49:55,357] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1253247: learning rate 0.0010
[2019-03-26 00:49:58,134] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1254268: loss 12.1166
[2019-03-26 00:49:58,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1254269: learning rate 0.0010
[2019-03-26 00:49:58,245] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1254313: loss 6.7898
[2019-03-26 00:49:58,246] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1254313: learning rate 0.0010
[2019-03-26 00:50:00,118] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1255000: loss 6.0368
[2019-03-26 00:50:00,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1255002: learning rate 0.0010
[2019-03-26 00:50:03,666] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:03,676] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7081
[2019-03-26 00:50:03,684] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5725541877302375, 6.911199999999999, 6.9112, 168.912956510431, 501227.6205230787, 501227.6205230793, 161947.2958295382], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7432200.0000, 
sim time next is 7432800.0000, 
raw observation next is [21.23333333333333, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5716613826581763, 6.9112, 6.9112, 168.912956510431, 500480.1425461788, 500480.1425461788, 161817.4545578179], 
processed observation next is [0.0, 0.0, 0.2053712480252764, 0.9266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4776358325099711, 0.0, 0.0, 0.8294399451523027, 0.13902226181838298, 0.13902226181838298, 0.24151858889226552], 
reward next is 0.7585, 
noisyNet noise sample is [array([-0.81140107], dtype=float32), -0.2422031]. 
=============================================
[2019-03-26 00:50:05,136] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1256852: loss 7.2138
[2019-03-26 00:50:05,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1256854: learning rate 0.0010
[2019-03-26 00:50:06,698] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1257430: loss 6.6456
[2019-03-26 00:50:06,700] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1257431: learning rate 0.0010
[2019-03-26 00:50:08,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.8532848e-28 4.9655210e-26 9.5578456e-30 3.4060349e-13], sum to 1.0000
[2019-03-26 00:50:08,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4958
[2019-03-26 00:50:08,119] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.808745557534498, 6.9112, 6.9112, 168.912956510431, 676389.5077499441, 676389.5077499441, 203610.4512449078], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7594200.0000, 
sim time next is 7594800.0000, 
raw observation next is [25.53333333333333, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.806621006119125, 6.9112, 6.9112, 168.912956510431, 674801.5683662118, 674801.5683662118, 203173.1620656586], 
processed observation next is [0.0, 0.9130434782608695, 0.4091627172195892, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7641719586818598, 0.0, 0.0, 0.8294399451523027, 0.1874448801017255, 0.1874448801017255, 0.30324352547113226], 
reward next is 0.6968, 
noisyNet noise sample is [array([-0.31086218], dtype=float32), -1.5879296]. 
=============================================
[2019-03-26 00:50:08,164] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1257976: loss 6.7620
[2019-03-26 00:50:08,168] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1257976: learning rate 0.0010
[2019-03-26 00:50:08,512] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1258097: loss 10.8357
[2019-03-26 00:50:08,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1258098: learning rate 0.0010
[2019-03-26 00:50:08,601] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1258130: loss 6.5095
[2019-03-26 00:50:08,604] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1258131: learning rate 0.0010
[2019-03-26 00:50:09,464] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1258451: loss 8.6084
[2019-03-26 00:50:09,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1258451: learning rate 0.0010
[2019-03-26 00:50:09,683] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:09,695] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2229
[2019-03-26 00:50:09,700] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7800718281239377, 6.9112, 6.9112, 168.912956510431, 654271.7951592732, 654271.7951592732, 197791.6578681049], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7560000.0000, 
sim time next is 7560600.0000, 
raw observation next is [29.0, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7747835614125437, 6.9112, 6.9112, 168.912956510431, 650501.3002128416, 650501.3002128416, 196748.462475401], 
processed observation next is [0.0, 0.5217391304347826, 0.5734597156398105, 0.655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.725345806600663, 0.0, 0.0, 0.8294399451523027, 0.1806948056146782, 0.1806948056146782, 0.2936544216050761], 
reward next is 0.7063, 
noisyNet noise sample is [array([-0.4642087], dtype=float32), 0.044987664]. 
=============================================
[2019-03-26 00:50:09,773] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1258563: loss 8.4066
[2019-03-26 00:50:09,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1258563: learning rate 0.0010
[2019-03-26 00:50:09,924] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1258621: loss 7.9275
[2019-03-26 00:50:09,926] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1258621: learning rate 0.0010
[2019-03-26 00:50:09,956] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1258631: loss 8.8400
[2019-03-26 00:50:09,961] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1258633: learning rate 0.0010
[2019-03-26 00:50:12,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1259587: loss -44.1004
[2019-03-26 00:50:12,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1259588: learning rate 0.0010
[2019-03-26 00:50:14,613] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1260351: loss 3.5662
[2019-03-26 00:50:14,618] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1260351: learning rate 0.0010
[2019-03-26 00:50:15,156] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1260552: loss 2.6925
[2019-03-26 00:50:15,161] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1260552: learning rate 0.0010
[2019-03-26 00:50:16,558] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1261073: loss 3.5754
[2019-03-26 00:50:16,561] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1261074: learning rate 0.0010
[2019-03-26 00:50:16,786] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:16,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6675
[2019-03-26 00:50:16,804] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.3, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5848550636422665, 6.9112, 6.9112, 168.912956510431, 510341.9986282933, 510341.9986282933, 163785.1816145999], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7452000.0000, 
sim time next is 7452600.0000, 
raw observation next is [21.33333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5859955109308823, 6.9112, 6.9112, 168.912956510431, 511212.3303705144, 511212.3303705144, 163956.7623115306], 
processed observation next is [0.0, 0.2608695652173913, 0.21011058451816728, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4951164767449784, 0.0, 0.0, 0.8294399451523027, 0.14200342510292066, 0.14200342510292066, 0.2447115855395979], 
reward next is 0.7553, 
noisyNet noise sample is [array([-1.5392566], dtype=float32), 0.87448704]. 
=============================================
[2019-03-26 00:50:19,613] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1262199: loss -12.5624
[2019-03-26 00:50:19,615] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1262200: learning rate 0.0010
[2019-03-26 00:50:19,680] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1262225: loss -29.4903
[2019-03-26 00:50:19,682] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1262225: learning rate 0.0010
[2019-03-26 00:50:19,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:19,892] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2037
[2019-03-26 00:50:19,901] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.38333333333333, 91.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9481360759468149, 6.9112, 6.9112, 168.912956510431, 802519.4503022372, 802519.4503022372, 235471.745508869], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7629000.0000, 
sim time next is 7629600.0000, 
raw observation next is [24.46666666666667, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9167016482767931, 6.9112, 6.9112, 168.912956510431, 775297.7952836199, 775297.7952836199, 227853.8598991528], 
processed observation next is [1.0, 0.30434782608695654, 0.3586097946287521, 0.9166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8984166442399916, 0.0, 0.0, 0.8294399451523027, 0.2153604986898944, 0.2153604986898944, 0.34008038790918327], 
reward next is 0.6599, 
noisyNet noise sample is [array([-1.0904124], dtype=float32), -0.7880195]. 
=============================================
[2019-03-26 00:50:20,223] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:20,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2431
[2019-03-26 00:50:20,241] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.86666666666667, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8615808191177062, 6.9112, 6.9112, 168.912956510431, 712160.5119462757, 712160.5119462757, 214735.6158323112], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7759200.0000, 
sim time next is 7759800.0000, 
raw observation next is [27.7, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8646184926779446, 6.911199999999999, 6.9112, 168.912956510431, 714250.6617795123, 714250.6617795129, 215396.4636408279], 
processed observation next is [1.0, 0.8260869565217391, 0.5118483412322274, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8349006008267617, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1984029616054201, 0.19840296160542026, 0.32148725916541476], 
reward next is 0.6785, 
noisyNet noise sample is [array([-0.6524159], dtype=float32), -2.4562316]. 
=============================================
[2019-03-26 00:50:20,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:20,745] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8871
[2019-03-26 00:50:20,752] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.9, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6986703061085082, 6.9112, 6.9112, 168.912956510431, 596411.0850560429, 596411.0850560429, 182486.7165659365], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7509600.0000, 
sim time next is 7510200.0000, 
raw observation next is [23.85, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6986792666977948, 6.911199999999999, 6.9112, 168.912956510431, 596370.3290199438, 596370.3290199444, 182488.0876014909], 
processed observation next is [0.0, 0.9565217391304348, 0.3293838862559243, 0.9033333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6325356910948717, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16565842472776218, 0.16565842472776235, 0.27237028000222524], 
reward next is 0.7276, 
noisyNet noise sample is [array([-0.55865693], dtype=float32), 1.697398]. 
=============================================
[2019-03-26 00:50:22,041] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1263104: loss -113.2480
[2019-03-26 00:50:22,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1263105: learning rate 0.0010
[2019-03-26 00:50:26,882] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1264893: loss -24.7306
[2019-03-26 00:50:26,884] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1264893: learning rate 0.0010
[2019-03-26 00:50:28,247] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1265400: loss 61.2348
[2019-03-26 00:50:28,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1265402: learning rate 0.0010
[2019-03-26 00:50:29,520] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1265873: loss -81.2464
[2019-03-26 00:50:29,523] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1265874: learning rate 0.0010
[2019-03-26 00:50:29,795] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1265973: loss 11.3488
[2019-03-26 00:50:29,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1265974: learning rate 0.0010
[2019-03-26 00:50:30,010] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1266057: loss 28.2025
[2019-03-26 00:50:30,014] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1266058: learning rate 0.0010
[2019-03-26 00:50:30,583] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1266267: loss -198.2804
[2019-03-26 00:50:30,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1266267: learning rate 0.0010
[2019-03-26 00:50:30,891] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1266380: loss 32.1391
[2019-03-26 00:50:30,894] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1266381: learning rate 0.0010
[2019-03-26 00:50:31,000] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1266420: loss -33.3718
[2019-03-26 00:50:31,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1266420: learning rate 0.0010
[2019-03-26 00:50:31,191] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1266490: loss -73.8137
[2019-03-26 00:50:31,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1266490: learning rate 0.0010
[2019-03-26 00:50:35,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:35,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:35,462] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-26 00:50:37,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:37,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:37,354] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-26 00:50:37,615] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:37,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:37,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:37,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-26 00:50:37,702] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5622
[2019-03-26 00:50:37,737] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1738180.262223432 W.
[2019-03-26 00:50:37,742] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.86666666666667, 71.66666666666667, 1.0, 2.0, 0.4144393442240963, 1.0, 2.0, 0.4144393442240963, 1.0, 1.0, 0.7158765143274025, 6.9112, 6.9112, 170.5573041426782, 1738180.262223432, 1738180.262223432, 360462.711678899], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7906800.0000, 
sim time next is 7907400.0000, 
raw observation next is [29.9, 71.5, 1.0, 2.0, 0.5678263146384429, 1.0, 2.0, 0.5678263146384429, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1587550.664121664, 1587550.664121664, 322672.8402128091], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.715, 1.0, 1.0, 0.47930881281740106, 1.0, 1.0, 0.47930881281740106, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4409862955893511, 0.4409862955893511, 0.4816012540489688], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.36005625], dtype=float32), 0.9333941]. 
=============================================
[2019-03-26 00:50:38,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:38,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:38,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-26 00:50:40,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:40,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5647
[2019-03-26 00:50:40,667] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6376780613504052, 6.911200000000001, 6.9112, 168.912956510431, 553123.3707078852, 553123.3707078845, 172037.2673360319], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 75600.0000, 
sim time next is 76200.0000, 
raw observation next is [23.03333333333334, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6364033404540197, 6.9112, 6.9112, 168.912956510431, 552198.1601016796, 552198.1601016796, 171827.9983087161], 
processed observation next is [1.0, 0.9130434782608695, 0.2906793048973147, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5565894395780727, 0.0, 0.0, 0.8294399451523027, 0.15338837780602213, 0.15338837780602213, 0.256459698968233], 
reward next is 0.7435, 
noisyNet noise sample is [array([0.72799224], dtype=float32), -2.9445205]. 
=============================================
[2019-03-26 00:50:40,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:40,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:40,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:40,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:40,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-26 00:50:40,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-26 00:50:42,703] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:42,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:42,800] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-26 00:50:42,961] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:42,969] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2867
[2019-03-26 00:50:42,977] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5173546123773224, 6.911200000000001, 6.9112, 168.912956510431, 456876.1405156444, 456876.1405156438, 154251.0521290146], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 15600.0000, 
sim time next is 16200.0000, 
raw observation next is [21.3, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.517849202102429, 6.9112, 6.9112, 168.912956510431, 457095.9576430458, 457095.9576430458, 154323.6088784192], 
processed observation next is [1.0, 0.17391304347826086, 0.2085308056872039, 0.855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41201122207613294, 0.0, 0.0, 0.8294399451523027, 0.1269710993452905, 0.1269710993452905, 0.23033374459465553], 
reward next is 0.7697, 
noisyNet noise sample is [array([-1.3902999], dtype=float32), -1.6868497]. 
=============================================
[2019-03-26 00:50:46,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:46,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:46,801] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-26 00:50:47,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:47,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:47,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-26 00:50:48,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:48,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:49,067] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-26 00:50:49,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:49,256] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:49,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:49,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:49,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-26 00:50:49,367] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-26 00:50:49,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:49,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:49,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-26 00:50:49,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:49,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:49,909] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-26 00:50:50,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:50,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:50,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 00:50:50,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:50,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-26 00:50:50,132] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-26 00:50:50,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:50:50,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9631
[2019-03-26 00:50:50,558] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.43333333333334, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6432511412770751, 6.911200000000001, 6.9112, 168.912956510431, 557055.4001379946, 557055.400137994, 172957.8812330622], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 73200.0000, 
sim time next is 73800.0000, 
raw observation next is [23.35, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6404487821758668, 6.911199999999999, 6.9112, 168.912956510431, 554858.1105003783, 554858.110500379, 172496.8823229723], 
processed observation next is [1.0, 0.8695652173913043, 0.3056872037914693, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5615229050925205, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15412725291677176, 0.15412725291677196, 0.2574580333178691], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.0857742], dtype=float32), 0.758103]. 
=============================================
[2019-03-26 00:50:51,650] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 00:50:51,652] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:50:51,653] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:50:51,653] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:51,657] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:50:51,658] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:50:51,659] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:50:51,658] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:51,660] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:51,661] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:51,661] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:50:51,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-26 00:50:51,712] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-26 00:50:51,739] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-26 00:50:51,740] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-26 00:50:51,740] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-26 00:50:53,728] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:50:53,730] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.3, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7892102568440234, 6.9112, 6.9112, 168.912956510431, 666086.7010739654, 666086.7010739654, 199715.5275871693]
[2019-03-26 00:50:53,731] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:50:53,735] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6426795337105645
[2019-03-26 00:51:03,946] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:51:03,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.725961065, 64.17235532333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4087588334016815, 6.911200000000001, 6.9112, 168.912956510431, 368976.8677111315, 368976.8677111309, 141271.2728757448]
[2019-03-26 00:51:03,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:51:03,956] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9359263858082408
[2019-03-26 00:51:10,895] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:51:10,896] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.26938642, 87.52541111666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5826182620504394, 6.9112, 6.9112, 168.912956510431, 513555.1999343172, 513555.1999343172, 163304.9632415111]
[2019-03-26 00:51:10,898] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:51:10,901] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7069945768542583
[2019-03-26 00:51:14,012] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:51:14,012] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.15896900333334, 92.85903874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.755533924465106, 6.9112, 6.9112, 168.912956510431, 648635.4359817256, 648635.4359817256, 193105.7574566219]
[2019-03-26 00:51:14,012] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:51:14,016] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.30022646468516057
[2019-03-26 00:51:30,634] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:51:30,635] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.70668960333333, 87.55195362666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.044150347751907, 6.9112, 168.9120630308904, 923156.7725712669, 828837.7189895818, 254812.2366759656]
[2019-03-26 00:51:30,636] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:51:30,638] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.22924950346442707
[2019-03-26 00:51:30,639] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 923156.7725712669 W.
[2019-03-26 00:51:43,527] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:51:43,529] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.46854128333333, 99.02662297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6729259809401373, 6.911199999999999, 6.9112, 168.912956510431, 576479.1909495788, 576479.1909495795, 177974.7085834409]
[2019-03-26 00:51:43,531] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:51:43,534] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7168564304486741
[2019-03-26 00:52:02,111] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:52:02,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.11666666666667, 60.66666666666667, 1.0, 2.0, 0.9756387617031635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1363730.34379566, 1363730.34379566, 291594.3126108509]
[2019-03-26 00:52:02,112] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:52:02,114] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.741321e-28], sampled 0.5386055050691368
[2019-03-26 00:52:02,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1363730.34379566 W.
[2019-03-26 00:52:33,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:52:33,850] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.88333333333333, 62.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8702814790296923, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 216669.4402958438]
[2019-03-26 00:52:33,850] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:52:33,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.13646736905189216
[2019-03-26 00:52:42,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:52:42,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.7, 71.66666666666667, 1.0, 1.0, 0.6313000448370806, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128000642082, 882219.9227769213, 882219.9227769219, 206720.4534249576]
[2019-03-26 00:52:42,377] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:52:42,379] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3768145888052036
[2019-03-26 00:52:42,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 882219.9227769213 W.
[2019-03-26 00:52:57,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1446483], dtype=float32), -0.005465387]
[2019-03-26 00:52:57,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.74517391333333, 57.83089513666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8212978468210463, 6.911199999999999, 6.9112, 168.912956510431, 707877.9918669756, 707877.9918669761, 206460.4505305445]
[2019-03-26 00:52:57,900] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:52:57,902] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9462139500870683
[2019-03-26 00:52:59,838] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7035.9874 3185212445.6800 2454.0000
[2019-03-26 00:53:00,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7296.8238 3319606692.8608 2112.0000
[2019-03-26 00:53:01,069] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7350.1456 3106283961.5930 2019.0000
[2019-03-26 00:53:01,184] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7931.6421 2989345976.8926 1549.0000
[2019-03-26 00:53:01,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8063.4523 2938190728.8955 1376.0000
[2019-03-26 00:53:02,284] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1275000, evaluation results [1275000.0, 7296.823837369567, 3319606692.8608027, 2112.0, 7350.145605699338, 3106283961.592964, 2019.0, 8063.452318183717, 2938190728.895457, 1376.0, 7035.987370796898, 3185212445.679971, 2454.0, 7931.64206446019, 2989345976.8925967, 1549.0]
[2019-03-26 00:53:02,650] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:53:02,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0133
[2019-03-26 00:53:02,669] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.56666666666667, 76.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5700220617672406, 6.9112, 6.9112, 168.912956510431, 498449.7570176365, 498449.7570176365, 161596.2226706574], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 305400.0000, 
sim time next is 306000.0000, 
raw observation next is [23.6, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5687227490229843, 6.9112, 6.9112, 168.912956510431, 497271.3438763592, 497271.3438763592, 161410.4925349423], 
processed observation next is [0.0, 0.5652173913043478, 0.3175355450236968, 0.76, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4740521329548588, 0.0, 0.0, 0.8294399451523027, 0.13813092885454423, 0.13813092885454423, 0.2409111828879736], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.900046], dtype=float32), -0.2747924]. 
=============================================
[2019-03-26 00:53:02,686] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.89718]
 [76.85742]
 [76.81268]
 [76.75799]
 [76.69978]], R is [[77.06171417]
 [77.0499115 ]
 [77.03832245]
 [77.02748108]
 [77.01714325]].
[2019-03-26 00:53:13,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2961247e-33], sum to 1.0000
[2019-03-26 00:53:13,506] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6408
[2019-03-26 00:53:13,513] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.46666666666667, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5388486788404139, 6.911199999999999, 6.9112, 168.912956510431, 473710.9967790074, 473710.9967790081, 157170.9727901973], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 321600.0000, 
sim time next is 322200.0000, 
raw observation next is [22.4, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5379474581248979, 6.9112, 6.9112, 168.912956510431, 473010.2810107688, 473010.2810107688, 157046.1703571021], 
processed observation next is [0.0, 0.7391304347826086, 0.2606635071090047, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4365212903962169, 0.0, 0.0, 0.8294399451523027, 0.13139174472521356, 0.13139174472521356, 0.2343972691897046], 
reward next is 0.7656, 
noisyNet noise sample is [array([-0.10958245], dtype=float32), 0.3273183]. 
=============================================
[2019-03-26 00:53:31,081] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2748310e-14 0.0000000e+00 5.3605315e-35 4.0373065e-35 1.0000000e+00], sum to 1.0000
[2019-03-26 00:53:31,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3429
[2019-03-26 00:53:31,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.0, 59.0, 1.0, 2.0, 0.1809767854226183, 1.0, 2.0, 0.1809767854226183, 1.0, 2.0, 0.3308826589274169, 6.9112, 6.9112, 170.5573041426782, 878653.4972840261, 878653.4972840261, 277666.9464121276], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 644400.0000, 
sim time next is 645000.0000, 
raw observation next is [24.03333333333333, 58.66666666666667, 1.0, 2.0, 0.1871092551325964, 1.0, 2.0, 0.1871092551325964, 1.0, 2.0, 0.3421746935360935, 6.911200000000001, 6.9112, 170.5573041426782, 908859.028712233, 908859.0287122324, 279421.8924279296], 
processed observation next is [1.0, 0.4782608695652174, 0.3380726698262243, 0.5866666666666667, 1.0, 1.0, 0.02061356040071855, 1.0, 1.0, 0.02061356040071855, 1.0, 1.0, 0.19777401650743107, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2524608413089536, 0.2524608413089534, 0.41704760063870094], 
reward next is 0.5830, 
noisyNet noise sample is [array([0.35543874], dtype=float32), -0.70702636]. 
=============================================
[2019-03-26 00:53:31,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.594795]
 [59.48682 ]
 [57.760307]
 [56.174843]
 [57.124443]], R is [[63.13458633]
 [63.08881378]
 [63.03520203]
 [62.40485001]
 [62.38916779]].
[2019-03-26 00:53:31,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3081327e-37], sum to 1.0000
[2019-03-26 00:53:31,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5615
[2019-03-26 00:53:31,215] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 85.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.440536644187894, 6.9112, 6.9112, 168.912956510431, 394263.9203123889, 394263.9203123889, 144832.2314511939], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 436200.0000, 
sim time next is 436800.0000, 
raw observation next is [19.6, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4420646047249143, 6.9112, 6.9112, 168.912956510431, 395649.4862059359, 395649.4862059359, 144996.2660746832], 
processed observation next is [1.0, 0.043478260869565216, 0.127962085308057, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3195909813718467, 0.0, 0.0, 0.8294399451523027, 0.1099026350572044, 0.1099026350572044, 0.2164123374249003], 
reward next is 0.7836, 
noisyNet noise sample is [array([-0.8417093], dtype=float32), -0.5719252]. 
=============================================
[2019-03-26 00:53:41,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1846532e-35 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 00:53:41,410] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4349
[2019-03-26 00:53:41,416] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.88333333333333, 59.33333333333334, 1.0, 2.0, 0.2368279698514997, 1.0, 2.0, 0.2368279698514997, 1.0, 2.0, 0.4327697127055073, 6.9112, 6.9112, 170.5573041426782, 1148779.726309074, 1148779.726309074, 295628.0934766059], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 569400.0000, 
sim time next is 570000.0000, 
raw observation next is [23.86666666666667, 59.66666666666667, 1.0, 2.0, 0.2484537461242366, 1.0, 2.0, 0.2484537461242366, 1.0, 2.0, 0.4536028993047371, 6.9112, 6.9112, 170.5573041426782, 1203089.070373707, 1203089.070373707, 299898.0517027522], 
processed observation next is [1.0, 0.6086956521739131, 0.33017377567140627, 0.5966666666666667, 1.0, 1.0, 0.0945225856918513, 1.0, 1.0, 0.0945225856918513, 1.0, 1.0, 0.33366207232285006, 0.0, 0.0, 0.8375144448122397, 0.33419140843714085, 0.33419140843714085, 0.4476090323921674], 
reward next is 0.5524, 
noisyNet noise sample is [array([-0.24073128], dtype=float32), 0.39880666]. 
=============================================
[2019-03-26 00:53:41,437] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.12708 ]
 [70.296646]
 [69.67128 ]
 [69.501915]
 [69.31307 ]], R is [[70.00337219]
 [69.86209869]
 [69.7386322 ]
 [69.61029053]
 [69.48410797]].
[2019-03-26 00:53:42,806] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 00:53:42,816] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8701
[2019-03-26 00:53:42,823] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 63.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 460554.7023758483, 460554.702375849, 223905.2159509109], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 826200.0000, 
sim time next is 826800.0000, 
raw observation next is [24.66666666666666, 63.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 459434.0937916546, 459434.0937916553, 223661.0205982119], 
processed observation next is [0.0, 0.5652173913043478, 0.36808846761453373, 0.63, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.12762058160879294, 0.12762058160879314, 0.3338224188033014], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.37072274], dtype=float32), 0.67130816]. 
=============================================
[2019-03-26 00:53:43,194] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.4030766e-38], sum to 1.0000
[2019-03-26 00:53:43,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9096
[2019-03-26 00:53:43,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1087730.442613916 W.
[2019-03-26 00:53:43,226] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004470089913138, 7.181299965459, 6.9112, 168.9117561775233, 1087730.442613916, 896113.5926754103, 248569.3684795258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 566400.0000, 
sim time next is 567000.0000, 
raw observation next is [24.15, 57.5, 1.0, 1.0, 0.6565623380652018, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9127365834479, 1076995.246831231, 1076995.246831232, 228299.2683610389], 
processed observation next is [1.0, 0.5652173913043478, 0.34360189573459715, 0.575, 1.0, 0.5, 0.5862196844159058, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294388652101327, 0.29916534634200864, 0.29916534634200886, 0.34074517665826703], 
reward next is 0.6593, 
noisyNet noise sample is [array([0.41292953], dtype=float32), -0.58162]. 
=============================================
[2019-03-26 00:53:43,245] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[60.066265]
 [61.79986 ]
 [62.66424 ]
 [63.00216 ]
 [64.99742 ]], R is [[59.48812866]
 [58.89324951]
 [59.00231171]
 [59.13498306]
 [59.28596878]].
[2019-03-26 00:53:45,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.2382547e-38 3.8969994e-37 5.6556025e-38 2.0965803e-21], sum to 1.0000
[2019-03-26 00:53:45,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5241
[2019-03-26 00:53:45,921] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4077022001775687, 6.9112, 6.9112, 168.912956510431, 366683.8036039399, 366683.8036039399, 141291.7058657538], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 630000.0000, 
sim time next is 630600.0000, 
raw observation next is [19.83333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4091638691587317, 6.9112, 6.9112, 168.912956510431, 367760.7904005787, 367760.7904005787, 141456.5614480046], 
processed observation next is [1.0, 0.30434782608695654, 0.13902053712480286, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2794681331204045, 0.0, 0.0, 0.8294399451523027, 0.10215577511127186, 0.10215577511127186, 0.21112919619105164], 
reward next is 0.7889, 
noisyNet noise sample is [array([-0.05178912], dtype=float32), -0.84672076]. 
=============================================
[2019-03-26 00:53:51,355] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.891508e-28], sum to 1.0000
[2019-03-26 00:53:51,368] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1089
[2019-03-26 00:53:51,372] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.4, 89.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6025785401387265, 6.9112, 6.9112, 168.912956510431, 521939.2159411, 521939.2159411, 166513.6333627493], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 939600.0000, 
sim time next is 940200.0000, 
raw observation next is [22.35, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6018764871311331, 6.9112, 6.9112, 168.912956510431, 522615.7137316845, 522615.7137316845, 166389.6125934226], 
processed observation next is [0.0, 0.9130434782608695, 0.25829383886255936, 0.895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5144835208916256, 0.0, 0.0, 0.8294399451523027, 0.14517103159213457, 0.14517103159213457, 0.2483427053633173], 
reward next is 0.7517, 
noisyNet noise sample is [array([0.48315343], dtype=float32), -0.68482965]. 
=============================================
[2019-03-26 00:54:00,244] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:54:00,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6540
[2019-03-26 00:54:00,261] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.16666666666667, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.537822769838392, 6.9112, 6.9112, 168.912956510431, 472871.0608527992, 472871.0608527992, 157030.3088688684], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 906600.0000, 
sim time next is 907200.0000, 
raw observation next is [23.3, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5376233712024568, 6.9112, 6.9112, 168.912956510431, 472648.2819308657, 472648.2819308657, 157004.9193751083], 
processed observation next is [0.0, 0.5217391304347826, 0.3033175355450238, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4361260624420205, 0.0, 0.0, 0.8294399451523027, 0.13129118942524048, 0.13129118942524048, 0.23433570055986314], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.34491238], dtype=float32), 0.69001895]. 
=============================================
[2019-03-26 00:54:02,062] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.99721575 0.         0.         0.         0.00278424], sum to 1.0000
[2019-03-26 00:54:02,072] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7686
[2019-03-26 00:54:02,084] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 963381.156404456 W.
[2019-03-26 00:54:02,091] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.2128552650056063, 1.0, 2.0, 0.2128552650056063, 1.0, 2.0, 0.373165193173521, 6.9112, 6.9112, 170.5573041426782, 963381.156404456, 963381.156404456, 281454.776658368], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [25.65, 68.5, 1.0, 2.0, 0.327338441226613, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5783322613194707, 6.9112, 6.9112, 168.912956510431, 999630.074430057, 999630.074430057, 238315.165593942], 
processed observation next is [1.0, 0.6086956521739131, 0.41469194312796204, 0.685, 1.0, 1.0, 0.18956438702001563, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.4857710503895984, 0.0, 0.0, 0.8294399451523027, 0.2776750206750158, 0.2776750206750158, 0.3556942770058836], 
reward next is 0.6443, 
noisyNet noise sample is [array([-0.8360377], dtype=float32), 2.3470404]. 
=============================================
[2019-03-26 00:54:02,104] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.31285 ]
 [67.57297 ]
 [66.27657 ]
 [64.88913 ]
 [63.826153]], R is [[68.07559967]
 [67.97476196]
 [67.87536621]
 [67.76390839]
 [67.08627319]].
[2019-03-26 00:54:02,776] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:54:02,786] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7392
[2019-03-26 00:54:02,793] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5977330747593802, 6.911199999999999, 6.9112, 168.912956510431, 519988.7483065462, 519988.7483065468, 165743.8585190575], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [24.86666666666667, 70.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6014968400775372, 6.911199999999999, 6.9112, 168.912956510431, 523551.3131005259, 523551.3131005265, 166310.9657299046], 
processed observation next is [1.0, 0.782608695652174, 0.3775671406003162, 0.705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5140205366799234, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14543092030570162, 0.14543092030570182, 0.24822532198493222], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.8804484], dtype=float32), -1.1586283]. 
=============================================
[2019-03-26 00:54:09,965] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 00:54:09,966] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:54:09,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:54:09,967] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:54:09,969] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:54:09,969] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:54:09,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:54:09,974] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:54:09,974] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:54:09,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:54:09,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:54:10,021] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-26 00:54:10,022] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-26 00:54:10,042] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-26 00:54:10,065] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-26 00:54:10,091] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-26 00:54:37,139] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:54:37,141] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.51572495, 91.95890703333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5815135217443105, 6.911200000000001, 6.9112, 168.912956510431, 517779.3323941131, 517779.3323941125, 162891.3327284393]
[2019-03-26 00:54:37,141] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:54:37,149] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6308843173432425
[2019-03-26 00:54:46,909] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:54:46,911] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.72590473, 73.512704165, 1.0, 2.0, 0.4046808551464188, 1.0, 2.0, 0.4046808551464188, 1.0, 2.0, 0.7027968153854901, 6.911199999999999, 6.9112, 171.5212843490159, 1697212.684566885, 1697212.684566885, 355697.7247493604]
[2019-03-26 00:54:46,911] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 00:54:46,916] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.00582515 0.         0.         0.         0.9941749 ], sampled 0.03566275188312473
[2019-03-26 00:55:07,307] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:55:07,309] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.16666666666667, 66.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.444908228318626, 6.9112, 168.9103933346391, 2662652.219550769, 2284026.979111593, 474919.8053567103]
[2019-03-26 00:55:07,309] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:55:07,312] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9999225e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 7.7977083e-06], sampled 0.436586698975168
[2019-03-26 00:55:07,313] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2662652.219550769 W.
[2019-03-26 00:55:17,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:55:17,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.6, 75.0, 1.0, 2.0, 0.1974648666825315, 1.0, 1.0, 0.1974648666825315, 1.0, 1.0, 0.3429311707982627, 6.9112, 6.9112, 169.0403247858759, 827829.4812012475, 827829.4812012475, 268856.5040917418]
[2019-03-26 00:55:17,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:55:17,726] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9475895e-21 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.6883796926574205
[2019-03-26 00:55:58,429] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:55:58,430] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.25, 61.5, 1.0, 2.0, 0.5505350159732824, 1.0, 2.0, 0.5505350159732824, 1.0, 2.0, 0.9462281752791202, 6.9112, 6.9112, 170.5573041426782, 2309572.971465984, 2309572.971465984, 449859.4474368236]
[2019-03-26 00:55:58,431] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 00:55:58,434] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4515343e-12], sampled 0.543632047546258
[2019-03-26 00:55:58,435] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2309572.971465984 W.
[2019-03-26 00:56:04,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:56:04,168] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.36666666666667, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9385482131473187, 6.9112, 6.9112, 168.912956510431, 764814.5599642654, 764814.5599642654, 232155.454606283]
[2019-03-26 00:56:04,171] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 00:56:04,174] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4551092325326077
[2019-03-26 00:56:05,447] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1505995], dtype=float32), 0.035198964]
[2019-03-26 00:56:05,448] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9085466373157295, 6.911199999999999, 6.9112, 168.912956510431, 748173.9717036417, 748173.9717036424, 225354.5080415556]
[2019-03-26 00:56:05,448] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 00:56:05,450] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4325027e-33], sampled 0.8621647867217486
[2019-03-26 00:56:19,009] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7585.3603 3177551604.5214 1508.0000
[2019-03-26 00:56:19,378] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8013.0840 3052849092.6525 1071.0000
[2019-03-26 00:56:19,511] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7429.2548 3368943804.1154 1561.0000
[2019-03-26 00:56:19,615] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8144.8452 3011586255.4350 940.0000
[2019-03-26 00:56:19,764] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7335.6888 3246038040.7514 1674.0000
[2019-03-26 00:56:20,780] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1300000, evaluation results [1300000.0, 7429.254764406107, 3368943804.115364, 1561.0, 7585.360297078532, 3177551604.521414, 1508.0, 8144.845224800613, 3011586255.434957, 940.0, 7335.688776592159, 3246038040.7513556, 1674.0, 8013.084039073777, 3052849092.652452, 1071.0]
[2019-03-26 00:56:26,545] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:56:26,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2421
[2019-03-26 00:56:26,562] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.8, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6299960214666558, 6.9112, 6.9112, 168.912956510431, 546063.4439535406, 546063.4439535406, 170803.1726148241], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1021800.0000, 
sim time next is 1022400.0000, 
raw observation next is [21.8, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6285967582914076, 6.911199999999999, 6.9112, 168.912956510431, 544850.3118745922, 544850.3118745928, 170579.1305455085], 
processed observation next is [1.0, 0.8695652173913043, 0.23222748815165886, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5470692174285459, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1513473088540534, 0.15134730885405356, 0.2545957172321022], 
reward next is 0.7454, 
noisyNet noise sample is [array([-0.0246042], dtype=float32), 0.84161216]. 
=============================================
[2019-03-26 00:56:33,244] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9761814e-01 8.1892693e-30 6.9751256e-33 1.9815026e-32 2.3818826e-03], sum to 1.0000
[2019-03-26 00:56:33,258] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0996
[2019-03-26 00:56:33,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1140390.037012961 W.
[2019-03-26 00:56:33,277] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.05, 90.66666666666667, 1.0, 2.0, 0.2430683835192503, 1.0, 2.0, 0.2430683835192503, 1.0, 2.0, 0.4358773326994909, 6.9112, 6.9112, 170.5573041426782, 1140390.037012961, 1140390.037012961, 295422.8156728077], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1353000.0000, 
sim time next is 1353600.0000, 
raw observation next is [21.0, 91.0, 1.0, 2.0, 0.7121739397386327, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1128799.738084808, 1128799.738084807, 239519.3851269294], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.91, 1.0, 1.0, 0.653221614142931, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31355548280133555, 0.3135554828013353, 0.357491619592432], 
reward next is 0.6425, 
noisyNet noise sample is [array([-1.3530425], dtype=float32), -1.2880466]. 
=============================================
[2019-03-26 00:56:35,453] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999928e-01 6.6246379e-38 1.7173584e-35 1.7859859e-30 7.1681796e-07], sum to 1.0000
[2019-03-26 00:56:35,466] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6865
[2019-03-26 00:56:35,479] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1127430.810010932 W.
[2019-03-26 00:56:35,487] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.7, 72.33333333333334, 1.0, 2.0, 0.366008826512662, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6501820651197453, 6.9112, 6.9112, 168.912956510431, 1127430.810010932, 1127430.810010932, 254628.8481771869], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1159800.0000, 
sim time next is 1160400.0000, 
raw observation next is [24.9, 71.66666666666667, 1.0, 2.0, 0.3700076963061453, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6576066020477564, 6.9112, 6.9112, 168.912956510431, 1140642.707241191, 1140642.707241191, 256420.1823490759], 
processed observation next is [1.0, 0.43478260869565216, 0.3791469194312796, 0.7166666666666667, 1.0, 1.0, 0.2409731280796931, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5824470756679956, 0.0, 0.0, 0.8294399451523027, 0.3168451964558864, 0.3168451964558864, 0.38271669007324766], 
reward next is 0.6173, 
noisyNet noise sample is [array([-1.2558609], dtype=float32), -1.9562565]. 
=============================================
[2019-03-26 00:56:41,367] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:56:41,380] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2029
[2019-03-26 00:56:41,383] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.46666666666667, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6710547595874319, 6.9112, 6.9112, 168.912956510431, 577820.2629453858, 577820.2629453858, 177647.3782498798], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1423200.0000, 
sim time next is 1423800.0000, 
raw observation next is [23.6, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6722037371572617, 6.9112, 6.9112, 168.912956510431, 577888.3025299233, 577888.3025299233, 177848.2973879826], 
processed observation next is [0.0, 0.4782608695652174, 0.3175355450236968, 0.885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.60024845994788, 0.0, 0.0, 0.8294399451523027, 0.16052452848053425, 0.16052452848053425, 0.2654452199820636], 
reward next is 0.7346, 
noisyNet noise sample is [array([0.33843073], dtype=float32), 0.08918738]. 
=============================================
[2019-03-26 00:56:47,613] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0470658e-02 3.6123438e-32 1.9145096e-32 3.7091937e-34 9.8952931e-01], sum to 1.0000
[2019-03-26 00:56:47,630] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9453
[2019-03-26 00:56:47,639] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.98333333333333, 95.0, 1.0, 2.0, 0.2765037826353024, 1.0, 1.0, 0.2765037826353024, 1.0, 2.0, 0.4725107677485057, 6.9112, 6.9112, 170.5573041426782, 1204642.27822147, 1204642.27822147, 298772.9603035225], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1332600.0000, 
sim time next is 1333200.0000, 
raw observation next is [22.96666666666667, 95.0, 1.0, 2.0, 0.2765572161947981, 1.0, 2.0, 0.2765572161947981, 1.0, 2.0, 0.4719965194997006, 6.911200000000001, 6.9112, 170.5573041426782, 1202618.574444228, 1202618.574444228, 298524.660674181], 
processed observation next is [1.0, 0.43478260869565216, 0.2875197472353872, 0.95, 1.0, 1.0, 0.12838218818650374, 1.0, 1.0, 0.12838218818650374, 1.0, 1.0, 0.35609331646304954, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3340607151233967, 0.3340607151233967, 0.44555919503609104], 
reward next is 0.5544, 
noisyNet noise sample is [array([-2.0521476], dtype=float32), -0.044614293]. 
=============================================
[2019-03-26 00:56:50,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:56:50,945] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8747
[2019-03-26 00:56:50,952] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6357968004253977, 6.9112, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311062, 171754.5118414955], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [27.85, 57.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6323376007712833, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 171192.753931685], 
processed observation next is [0.0, 0.6956521739130435, 0.5189573459715641, 0.5733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5516312204527845, 0.0, 0.0, 0.8294399451523027, 0.15191858677415285, 0.15191858677415285, 0.25551157303236566], 
reward next is 0.7445, 
noisyNet noise sample is [array([-0.16896875], dtype=float32), -1.2967619]. 
=============================================
[2019-03-26 00:56:50,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[82.37562]
 [82.21843]
 [82.18798]
 [82.15226]
 [82.11005]], R is [[82.32518005]
 [82.24558258]
 [82.166008  ]
 [82.08667755]
 [82.00791168]].
[2019-03-26 00:57:02,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:57:02,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3288
[2019-03-26 00:57:02,112] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.06666666666667, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.857191466135507, 6.9112, 6.9112, 168.912956510431, 707648.3718230554, 707648.3718230554, 213732.2976120299], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1707000.0000, 
sim time next is 1707600.0000, 
raw observation next is [27.93333333333333, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8564922682248295, 6.911199999999999, 6.9112, 168.912956510431, 707318.5229853182, 707318.5229853189, 213586.8084970737], 
processed observation next is [1.0, 0.782608695652174, 0.522906793048973, 0.7933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8249905710058897, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19647736749592173, 0.19647736749592193, 0.318786281338916], 
reward next is 0.6812, 
noisyNet noise sample is [array([-0.25918496], dtype=float32), 0.5823604]. 
=============================================
[2019-03-26 00:57:14,269] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 4.1565753e-35 0.0000000e+00 1.1017995e-34 4.6007187e-34], sum to 1.0000
[2019-03-26 00:57:14,279] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1767
[2019-03-26 00:57:14,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1752979.298855159 W.
[2019-03-26 00:57:14,296] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.13333333333333, 79.5, 1.0, 2.0, 0.4179650374661328, 1.0, 2.0, 0.4179650374661328, 1.0, 1.0, 0.7168619179646702, 6.911199999999999, 6.9112, 170.5573041426782, 1752979.298855159, 1752979.29885516, 361723.919913577], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1695000.0000, 
sim time next is 1695600.0000, 
raw observation next is [28.2, 79.0, 1.0, 2.0, 0.5743539988658263, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9786144004394431, 6.9112, 6.9112, 168.912956510431, 1605826.399396993, 1605826.399396993, 347397.7224061386], 
processed observation next is [1.0, 0.6521739130434783, 0.5355450236966824, 0.79, 1.0, 1.0, 0.4871734926094292, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.9739200005359061, 0.0, 0.0, 0.8294399451523027, 0.44606288872138694, 0.44606288872138694, 0.5185040632927442], 
reward next is 0.4815, 
noisyNet noise sample is [array([-1.0509307], dtype=float32), -0.3149858]. 
=============================================
[2019-03-26 00:57:14,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:57:14,672] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0349
[2019-03-26 00:57:14,681] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.85, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8605529963143494, 6.9112, 6.9112, 168.912956510431, 712673.9413427857, 712673.9413427857, 214554.2644955112], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1722600.0000, 
sim time next is 1723200.0000, 
raw observation next is [25.8, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8615398419274389, 6.911200000000001, 6.9112, 168.912956510431, 713313.674629549, 713313.6746295483, 214767.3474225762], 
processed observation next is [1.0, 0.9565217391304348, 0.42180094786729866, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8311461486919988, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19814268739709695, 0.19814268739709676, 0.32054827973518835], 
reward next is 0.6795, 
noisyNet noise sample is [array([-0.88311434], dtype=float32), -0.24932894]. 
=============================================
[2019-03-26 00:57:20,074] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.5935065e-01 1.4397069e-28 1.8055728e-32 5.0156769e-32 4.0649422e-02], sum to 1.0000
[2019-03-26 00:57:20,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4985
[2019-03-26 00:57:20,096] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1165419.684211225 W.
[2019-03-26 00:57:20,100] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.63333333333334, 83.33333333333334, 1.0, 2.0, 0.3867008895660871, 1.0, 1.0, 0.3867008895660871, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1165419.684211225, 1165419.684211225, 278749.2021857814], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1768800.0000, 
sim time next is 1769400.0000, 
raw observation next is [23.55, 84.0, 1.0, 2.0, 0.7861308182036679, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1195508.13693822, 1195508.13693822, 253373.5437660851], 
processed observation next is [1.0, 0.4782608695652174, 0.3151658767772513, 0.84, 1.0, 1.0, 0.742326286992371, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33208559359394996, 0.33208559359394996, 0.3781694683075897], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09273764], dtype=float32), 0.28904656]. 
=============================================
[2019-03-26 00:57:22,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:57:22,653] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7476
[2019-03-26 00:57:22,663] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.3, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5716855502803754, 6.911200000000001, 6.9112, 168.912956510431, 501096.7797237209, 501096.7797237202, 161804.8410243785], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1800000.0000, 
sim time next is 1800600.0000, 
raw observation next is [21.31666666666667, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.572886861671454, 6.9112, 6.9112, 168.912956510431, 502100.6510673204, 502100.6510673204, 161979.7802885658], 
processed observation next is [1.0, 0.8695652173913043, 0.20932069510268583, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4791303191115292, 0.0, 0.0, 0.8294399451523027, 0.13947240307425565, 0.13947240307425565, 0.241760866102337], 
reward next is 0.7582, 
noisyNet noise sample is [array([-0.2880158], dtype=float32), 0.056016337]. 
=============================================
[2019-03-26 00:57:25,165] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:57:25,175] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6433
[2019-03-26 00:57:25,182] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.31666666666667, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8420541744934633, 6.911200000000001, 6.9112, 168.912956510431, 722343.5373102932, 722343.5373102926, 210933.636299046], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1840200.0000, 
sim time next is 1840800.0000, 
raw observation next is [23.43333333333333, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7441724040053087, 6.9112, 6.9112, 168.912956510431, 637823.911124251, 637823.911124251, 190915.7967668498], 
processed observation next is [1.0, 0.30434782608695654, 0.30963665086887826, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6880151268357422, 0.0, 0.0, 0.8294399451523027, 0.17717330864562528, 0.17717330864562528, 0.2849489503982833], 
reward next is 0.7151, 
noisyNet noise sample is [array([-1.3279173], dtype=float32), 0.2787694]. 
=============================================
[2019-03-26 00:57:28,458] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 00:57:28,461] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 00:57:28,464] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 00:57:28,464] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:57:28,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:57:28,466] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 00:57:28,467] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 00:57:28,469] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:57:28,473] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:57:28,468] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 00:57:28,475] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 00:57:28,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-26 00:57:28,524] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-26 00:57:28,526] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-26 00:57:28,526] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-26 00:57:28,601] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-26 00:58:04,860] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.207217], dtype=float32), 0.054975923]
[2019-03-26 00:58:04,864] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.87542276666667, 83.06097992833332, 1.0, 2.0, 0.261105604262941, 1.0, 1.0, 0.261105604262941, 1.0, 1.0, 0.4534540856619096, 6.911199999999999, 6.9112, 184.5923449428631, 1094719.651865347, 1094719.651865348, 293407.4837826147]
[2019-03-26 00:58:04,865] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 00:58:04,868] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2390813e-28 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.3505107842691749
[2019-03-26 00:59:37,427] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7397.8984 3359254344.3274 1748.0000
[2019-03-26 00:59:37,543] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8117.6967 2994879000.9804 1050.0000
[2019-03-26 00:59:38,278] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7234.0471 3230954984.4916 1954.0000
[2019-03-26 00:59:38,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7501.3715 3157067175.5817 1671.0000
[2019-03-26 00:59:38,463] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8009.4471 3039150167.5984 1215.0000
[2019-03-26 00:59:39,479] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1325000, evaluation results [1325000.0, 7397.898427031843, 3359254344.327415, 1748.0, 7501.371498789055, 3157067175.581694, 1671.0, 8117.696698284, 2994879000.980358, 1050.0, 7234.047075344256, 3230954984.491588, 1954.0, 8009.447137038751, 3039150167.5983806, 1215.0]
[2019-03-26 00:59:45,911] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:59:45,920] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0191
[2019-03-26 00:59:45,926] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.43333333333333, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7793979119293686, 6.9112, 6.9112, 168.912956510431, 655661.7408489607, 655661.7408489607, 197696.3425977274], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2076000.0000, 
sim time next is 2076600.0000, 
raw observation next is [24.41666666666666, 94.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.779389744867293, 6.9112, 6.9112, 168.912956510431, 655613.2064257066, 655613.2064257066, 197693.9096637248], 
processed observation next is [0.0, 0.0, 0.35624012638230623, 0.9483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7309631034966986, 0.0, 0.0, 0.8294399451523027, 0.18211477956269628, 0.18211477956269628, 0.2950655368115295], 
reward next is 0.7049, 
noisyNet noise sample is [array([-0.5202153], dtype=float32), 0.17904377]. 
=============================================
[2019-03-26 00:59:49,326] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:59:49,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8862
[2019-03-26 00:59:49,341] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.1, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9901313810673317, 6.9112, 6.9112, 168.912956510431, 797168.9587472043, 797168.9587472043, 244468.840059775], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2125200.0000, 
sim time next is 2125800.0000, 
raw observation next is [30.15, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.003844203611888, 6.911200000000001, 6.9112, 168.912848057333, 807930.0579188232, 807930.0579188226, 247971.6355150129], 
processed observation next is [0.0, 0.6086956521739131, 0.6279620853080569, 0.765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0046880531852291, 8.881784197001253e-17, 0.0, 0.8294394125979415, 0.224425016088562, 0.22442501608856183, 0.3701069186791237], 
reward next is 0.6299, 
noisyNet noise sample is [array([0.89331394], dtype=float32), 1.123161]. 
=============================================
[2019-03-26 00:59:55,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:59:55,263] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4214
[2019-03-26 00:59:55,271] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1666056.314241549 W.
[2019-03-26 00:59:55,278] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.5, 63.5, 1.0, 2.0, 0.5958794647840826, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.916698154877714, 6.9112, 168.9128721633026, 1666056.314241549, 1662155.735887177, 363807.2900598263], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2377800.0000, 
sim time next is 2378400.0000, 
raw observation next is [32.53333333333333, 63.33333333333334, 1.0, 2.0, 0.5753059200314181, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9991161265195281, 6.911200000000001, 6.9112, 168.9129520335983, 1608489.876051974, 1608489.876051973, 352029.5238475909], 
processed observation next is [1.0, 0.5217391304347826, 0.7409162717219588, 0.6333333333333334, 1.0, 1.0, 0.4883203855800218, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9989221055116196, 8.881784197001253e-17, 0.0, 0.8294399231690046, 0.44680274334777054, 0.4468027433477703, 0.5254171997725238], 
reward next is 0.4746, 
noisyNet noise sample is [array([-2.1297288], dtype=float32), -0.28644827]. 
=============================================
[2019-03-26 00:59:55,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:59:55,944] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7018
[2019-03-26 00:59:55,956] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.05, 76.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9981192560938135, 6.911200000000001, 6.9112, 168.9128703671369, 803894.3670526021, 803894.3670526015, 246529.7430192794], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2124600.0000, 
sim time next is 2125200.0000, 
raw observation next is [30.1, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9901313810673317, 6.9112, 6.9112, 168.912956510431, 797168.9587472043, 797168.9587472043, 244468.840059775], 
processed observation next is [0.0, 0.6086956521739131, 0.6255924170616115, 0.7666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9879650988625994, 0.0, 0.0, 0.8294399451523027, 0.22143582187422342, 0.22143582187422342, 0.3648788657608582], 
reward next is 0.6351, 
noisyNet noise sample is [array([-0.09369159], dtype=float32), 1.904865]. 
=============================================
[2019-03-26 00:59:57,564] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 00:59:57,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9542
[2019-03-26 00:59:57,580] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.33333333333334, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.951588428636511, 6.911200000000001, 6.9112, 168.912956510431, 773142.9587436481, 773142.9587436474, 235219.266174265], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2143200.0000, 
sim time next is 2143800.0000, 
raw observation next is [28.15, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.948589828622511, 6.9112, 6.9112, 168.912956510431, 771358.7945473588, 771358.7945473588, 234517.7685864094], 
processed observation next is [0.0, 0.8260869565217391, 0.533175355450237, 0.845, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9373046690518426, 0.0, 0.0, 0.8294399451523027, 0.21426633181871077, 0.21426633181871077, 0.35002652027822295], 
reward next is 0.6500, 
noisyNet noise sample is [array([-0.4545979], dtype=float32), -0.5827213]. 
=============================================
[2019-03-26 01:00:02,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.2786629e-31 1.6018739e-34 1.4012781e-27 2.4841210e-13], sum to 1.0000
[2019-03-26 01:00:02,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0321
[2019-03-26 01:00:02,130] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2355723.686434849 W.
[2019-03-26 01:00:02,137] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.5, 80.33333333333334, 1.0, 1.0, 0.561525645022209, 1.0, 1.0, 0.561525645022209, 1.0, 2.0, 0.9750979794544481, 6.911199999999999, 6.9112, 170.5573041426782, 2355723.686434849, 2355723.686434849, 460281.0371982959], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2427600.0000, 
sim time next is 2428200.0000, 
raw observation next is [28.45, 80.5, 1.0, 2.0, 0.5386615800196535, 1.0, 2.0, 0.5386615800196535, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1505953.490313682, 1505953.490313682, 312702.1427247814], 
processed observation next is [1.0, 0.08695652173913043, 0.54739336492891, 0.805, 1.0, 1.0, 0.44417057833693185, 1.0, 1.0, 0.44417057833693185, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.41832041397602276, 0.41832041397602276, 0.46671961600713646], 
reward next is 0.5333, 
noisyNet noise sample is [array([0.15723388], dtype=float32), -0.59968174]. 
=============================================
[2019-03-26 01:00:03,638] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.2266588e-07 1.7455152e-27 1.0160572e-30 2.7547785e-27 9.9999940e-01], sum to 1.0000
[2019-03-26 01:00:03,647] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5113
[2019-03-26 01:00:03,654] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.81666666666667, 88.83333333333334, 1.0, 2.0, 0.4238843795507833, 1.0, 2.0, 0.4238843795507833, 1.0, 2.0, 0.7184323136841869, 6.9112, 6.9112, 170.5573041426782, 1777826.094898343, 1777826.094898343, 363807.3613979436], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [25.7, 89.0, 1.0, 2.0, 0.4299781650222115, 1.0, 2.0, 0.4299781650222115, 1.0, 2.0, 0.7278652462782746, 6.9112, 6.9112, 170.5573041426782, 1803405.729477914, 1803405.729477914, 367180.6707137632], 
processed observation next is [1.0, 0.4782608695652174, 0.4170616113744076, 0.89, 1.0, 1.0, 0.313226704846038, 1.0, 1.0, 0.313226704846038, 1.0, 1.0, 0.6681283491198472, 0.0, 0.0, 0.8375144448122397, 0.5009460359660872, 0.5009460359660872, 0.5480308518115868], 
reward next is 0.4520, 
noisyNet noise sample is [array([0.6284503], dtype=float32), -0.31776595]. 
=============================================
[2019-03-26 01:00:05,756] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 2.9676514e-33 0.0000000e+00 3.3015236e-33 1.5893660e-26], sum to 1.0000
[2019-03-26 01:00:05,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6035
[2019-03-26 01:00:05,778] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1795463.110081782 W.
[2019-03-26 01:00:05,784] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6421290392586982, 1.0, 2.0, 0.6421290392586982, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1795463.110081782, 1795463.110081782, 350331.9714618889], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.6336775476274876, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.971305298453592, 6.9112, 168.9125816285747, 1771826.540176471, 1729185.867424792, 371685.4848088892], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.5586476477439609, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0060105298453591695, 0.0, 0.8294381043107667, 0.4921740389379086, 0.48032940761799775, 0.5547544549386406], 
reward next is 0.1447, 
noisyNet noise sample is [array([0.11839528], dtype=float32), 1.0758994]. 
=============================================
[2019-03-26 01:00:08,637] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 1.4723913e-20 1.0404401e-27 1.9030289e-17 5.5629257e-10], sum to 1.0000
[2019-03-26 01:00:08,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7258
[2019-03-26 01:00:08,658] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2371574.366123388 W.
[2019-03-26 01:00:08,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.93333333333333, 70.5, 1.0, 2.0, 0.5653003345188883, 1.0, 2.0, 0.5653003345188883, 1.0, 1.0, 0.9760112532107532, 6.9112, 6.9112, 170.5573041426782, 2371574.366123388, 2371574.366123388, 462053.1646718852], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2560200.0000, 
sim time next is 2560800.0000, 
raw observation next is [29.86666666666667, 71.0, 1.0, 2.0, 0.8510415759098775, 1.0, 2.0, 0.8510415759098775, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2380227.811900753, 2380227.811900753, 445480.33664911], 
processed observation next is [1.0, 0.6521739130434783, 0.6145339652448659, 0.71, 1.0, 1.0, 0.8205320191685271, 1.0, 1.0, 0.8205320191685271, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6611743921946537, 0.6611743921946537, 0.6648960248494179], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6989733], dtype=float32), 0.2599245]. 
=============================================
[2019-03-26 01:00:09,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 2.9986855e-32 0.0000000e+00 1.6158492e-33 6.7933082e-25], sum to 1.0000
[2019-03-26 01:00:09,923] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3564
[2019-03-26 01:00:09,934] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2087468.340596233 W.
[2019-03-26 01:00:09,941] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.3, 80.66666666666667, 1.0, 2.0, 0.8517259449910499, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990296376968868, 6.9112, 168.9124856738017, 2087468.340596233, 2031354.804901498, 421604.342022276], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [28.4, 80.0, 1.0, 2.0, 0.4778760833745824, 1.0, 1.0, 0.4778760833745824, 1.0, 2.0, 0.8244902848707965, 6.9112, 6.9112, 170.5573041426782, 2004486.133899232, 2004486.133899232, 399252.7999403544], 
processed observation next is [1.0, 0.4782608695652174, 0.5450236966824644, 0.8, 1.0, 1.0, 0.37093504021034024, 1.0, 0.5, 0.37093504021034024, 1.0, 1.0, 0.7859637620375567, 0.0, 0.0, 0.8375144448122397, 0.5568017038608978, 0.5568017038608978, 0.5958997014035141], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.86670166], dtype=float32), 0.60680723]. 
=============================================
[2019-03-26 01:00:18,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:00:18,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4023
[2019-03-26 01:00:18,827] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7345178396523152, 6.911200000000001, 6.9112, 168.912956510431, 624140.126048276, 624140.1260482754, 189055.2802516758], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2713800.0000, 
sim time next is 2714400.0000, 
raw observation next is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.732394700174857, 6.911199999999999, 6.9112, 168.912956510431, 622337.012059926, 622337.0120599266, 188655.4746401174], 
processed observation next is [0.0, 0.43478260869565216, 0.28909952606635075, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6736520733839719, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17287139223886835, 0.17287139223886852, 0.28157533528375733], 
reward next is 0.7184, 
noisyNet noise sample is [array([-0.7239035], dtype=float32), -0.58286]. 
=============================================
[2019-03-26 01:00:29,233] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:00:29,245] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0343
[2019-03-26 01:00:29,255] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6251911802121561, 6.9112, 6.9112, 168.912956510431, 541574.4567812396, 541574.4567812396, 170039.9496198307], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [22.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6217241036219074, 6.911199999999999, 6.9112, 168.912956510431, 538978.7727720743, 538978.772772075, 169485.2668332584], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5386879312462286, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971632577002064, 0.14971632577002084, 0.25296308482575885], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.33949503], dtype=float32), 0.26596752]. 
=============================================
[2019-03-26 01:00:35,479] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.869874e-22], sum to 1.0000
[2019-03-26 01:00:35,490] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1264
[2019-03-26 01:00:35,496] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6886049693292974, 6.9112, 6.9112, 168.912956510431, 590437.1791572652, 590437.1791572652, 180704.8642253236], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2670600.0000, 
sim time next is 2671200.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6883789484237833, 6.911200000000001, 6.9112, 168.912956510431, 590243.3263223405, 590243.3263223398, 180665.037012302], 
processed observation next is [0.0, 0.9565217391304348, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6199743273460772, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16395647953398348, 0.1639564795339833, 0.2696493089735851], 
reward next is 0.7304, 
noisyNet noise sample is [array([0.5780354], dtype=float32), 2.8448155]. 
=============================================
[2019-03-26 01:00:39,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:00:39,598] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3951
[2019-03-26 01:00:39,605] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7986908004284822, 6.9112, 6.9112, 168.912956510431, 669691.6689820278, 669691.6689820278, 201569.7295550462], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2705400.0000, 
sim time next is 2706000.0000, 
raw observation next is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7984858358149794, 6.9112, 6.9112, 168.912956510431, 669519.7517283816, 669519.7517283816, 201527.5829944622], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7542510192865602, 0.0, 0.0, 0.8294399451523027, 0.18597770881343934, 0.18597770881343934, 0.30078743730516744], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.6204879], dtype=float32), -0.28996325]. 
=============================================
[2019-03-26 01:00:39,619] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[78.603905]
 [78.628334]
 [78.65287 ]
 [78.66149 ]
 [78.5104  ]], R is [[78.50405121]
 [78.41815948]
 [78.33341217]
 [78.25034332]
 [78.16872406]].
[2019-03-26 01:00:41,947] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.266072e-23], sum to 1.0000
[2019-03-26 01:00:41,954] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8091
[2019-03-26 01:00:41,960] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 94.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9098755786108665, 6.9112, 6.9112, 168.9129565104199, 796359.0454033173, 796359.0454033173, 226075.2833892515], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2997000.0000, 
sim time next is 2997600.0000, 
raw observation next is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.020124458429642, 6.9112, 6.9112, 168.9128740288885, 893652.3707698144, 893652.3707698144, 253666.3649259258], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0245420224751733, 0.0, 0.0, 0.8294395401301687, 0.24823676965828176, 0.24823676965828176, 0.3786065148148146], 
reward next is 0.6214, 
noisyNet noise sample is [array([0.3458279], dtype=float32), -0.34684393]. 
=============================================
[2019-03-26 01:00:47,242] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 01:00:47,244] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:00:47,245] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:00:47,246] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:00:47,248] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:00:47,249] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:00:47,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:00:47,250] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:00:47,251] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:00:47,255] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:00:47,251] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:00:47,283] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-26 01:00:47,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-26 01:00:47,308] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-26 01:00:47,328] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-26 01:00:47,377] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-26 01:01:07,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:01:07,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.43333333333333, 88.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6161592864884222, 6.911199999999999, 6.9112, 168.912956510431, 535479.6854406107, 535479.6854406114, 168590.7158082771]
[2019-03-26 01:01:07,696] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:01:07,699] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.057112e-19], sampled 0.08298669059718333
[2019-03-26 01:01:55,626] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:01:55,627] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.3161323731744438, 1.0, 2.0, 0.3161323731744438, 1.0, 2.0, 0.5490173856306348, 6.9112, 6.9112, 170.5573041426782, 1325620.712787759, 1325620.712787759, 311323.68812817]
[2019-03-26 01:01:55,629] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:01:55,632] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2012802e-03 0.0000000e+00 3.8135242e-38 0.0000000e+00 9.9879873e-01], sampled 0.8660722199054272
[2019-03-26 01:02:07,014] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:07,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.76525858666667, 68.67165561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.95219262886177, 6.911199999999999, 6.9112, 168.912956510431, 797744.4754270889, 797744.4754270896, 236301.7310607289]
[2019-03-26 01:02:07,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:02:07,020] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.1325757e-34], sampled 0.15920349845170145
[2019-03-26 01:02:29,529] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:29,531] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.66861615333333, 87.56032165333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7532322748610618, 6.911200000000001, 6.9112, 168.912956510431, 641500.3887195456, 641500.388719545, 192644.5413254292]
[2019-03-26 01:02:29,532] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:02:29,535] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.850892e-15], sampled 0.37182703036565057
[2019-03-26 01:02:45,289] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:45,289] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.8, 90.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8293605360639046, 6.9112, 6.9112, 168.912956510431, 689627.7115367845, 689627.7115367845, 207852.1627324521]
[2019-03-26 01:02:45,290] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:02:45,293] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24529332899378886
[2019-03-26 01:02:46,132] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:46,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.26666666666667, 70.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.091900755858119, 6.9112, 168.9117900707539, 957134.9893535026, 828940.5391691433, 254817.2728304579]
[2019-03-26 01:02:46,136] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:02:46,141] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.943521809531192
[2019-03-26 01:02:46,142] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 957134.9893535026 W.
[2019-03-26 01:02:51,218] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:51,219] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.48277441, 64.64942621, 1.0, 2.0, 0.3238201849532916, 1.0, 2.0, 0.3238201849532916, 1.0, 2.0, 0.5452364671092866, 6.9112, 6.9112, 171.5212843490159, 1357873.180314223, 1357873.180314223, 313024.5247140413]
[2019-03-26 01:02:51,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:02:51,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.4417159e-06 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9999356e-01], sampled 0.23539727714341552
[2019-03-26 01:02:53,003] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.056557], dtype=float32), 0.056653395]
[2019-03-26 01:02:53,005] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.40000000000001, 56.66666666666667, 1.0, 2.0, 0.5201633010430199, 1.0, 2.0, 0.5201633010430199, 1.0, 2.0, 0.9033516332794413, 6.9112, 6.9112, 169.0403247858759, 2182063.558096626, 2182063.558096626, 429069.966810533]
[2019-03-26 01:02:53,006] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:02:53,012] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00245924 0.         0.         0.         0.99754083], sampled 0.1964237293348352
[2019-03-26 01:02:55,948] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7839.6992 3121593262.3614 719.0000
[2019-03-26 01:02:56,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7939.1716 3087960590.7401 642.0000
[2019-03-26 01:02:56,635] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7626.6339 3239105212.1406 994.0000
[2019-03-26 01:02:56,796] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7347.9833 3428334505.4213 1134.0000
[2019-03-26 01:02:56,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7451.8423 3302411381.6709 1024.0000
[2019-03-26 01:02:57,887] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1350000, evaluation results [1350000.0, 7347.983325772811, 3428334505.4212937, 1134.0, 7626.633866151878, 3239105212.140624, 994.0, 7939.171587605866, 3087960590.7401237, 642.0, 7451.842280106236, 3302411381.6708536, 1024.0, 7839.699245091362, 3121593262.36138, 719.0]
[2019-03-26 01:03:05,060] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:05,070] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3830
[2019-03-26 01:03:05,077] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6852700588442938, 6.9112, 6.9112, 168.912956510431, 588240.3927870096, 588240.3927870096, 180118.039852286], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3119400.0000, 
sim time next is 3120000.0000, 
raw observation next is [22.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6740086723551515, 6.9112, 6.9112, 168.912956510431, 578874.8120788713, 578874.8120788713, 178160.1204256495], 
processed observation next is [1.0, 0.08695652173913043, 0.2575039494470777, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6024496004331116, 0.0, 0.0, 0.8294399451523027, 0.16079855891079758, 0.16079855891079758, 0.26591062750096944], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.480211], dtype=float32), -2.0480683]. 
=============================================
[2019-03-26 01:03:05,100] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[78.328064]
 [78.07904 ]
 [77.30952 ]
 [76.97925 ]
 [76.65752 ]], R is [[78.33986664]
 [78.2876358 ]
 [78.22705078]
 [78.12628174]
 [78.07715607]].
[2019-03-26 01:03:06,727] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:06,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4322
[2019-03-26 01:03:06,748] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5739234225898006, 6.9112, 6.9112, 168.912956510431, 502849.9317344112, 502849.9317344112, 162134.1866752233], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2932800.0000, 
sim time next is 2933400.0000, 
raw observation next is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5721712068626197, 6.9112, 6.9112, 168.912956510431, 501313.8800556624, 501313.8800556624, 161880.7278711151], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47825756934465813, 0.0, 0.0, 0.8294399451523027, 0.13925385557101733, 0.13925385557101733, 0.24161302667330609], 
reward next is 0.7584, 
noisyNet noise sample is [array([-1.08633], dtype=float32), -0.20258498]. 
=============================================
[2019-03-26 01:03:09,045] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:09,057] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7268
[2019-03-26 01:03:09,062] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7664062201824519, 6.911200000000001, 6.9112, 168.912956510431, 646267.9345350269, 646267.9345350263, 195141.2256797599], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3216000.0000, 
sim time next is 3216600.0000, 
raw observation next is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7668027587411408, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 195219.3994030775], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7156131204160254, 0.0, 0.0, 0.8294399451523027, 0.17961178347060539, 0.17961178347060539, 0.291372237915041], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.571469], dtype=float32), 1.6890895]. 
=============================================
[2019-03-26 01:03:13,913] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 3.064409e-36], sum to 1.0000
[2019-03-26 01:03:13,922] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2582
[2019-03-26 01:03:13,936] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 936673.3203838279 W.
[2019-03-26 01:03:13,945] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.975959395616831, 6.9112, 168.9124855472238, 936673.3203838279, 890730.904383893, 256588.8443312149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3054600.0000, 
sim time next is 3055200.0000, 
raw observation next is [22.0, 98.0, 1.0, 1.0, 0.5764074237160138, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129037806036, 876375.7118462814, 876375.7118462814, 205114.7098920419], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.98, 1.0, 0.5, 0.4896474984530287, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294396862247104, 0.24343769773507817, 0.24343769773507817, 0.3061413580478237], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.7667957], dtype=float32), 1.4931314]. 
=============================================
[2019-03-26 01:03:14,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9999940e-01 4.3576319e-34 5.8668199e-31 7.0944522e-35 5.5586355e-07], sum to 1.0000
[2019-03-26 01:03:14,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0724
[2019-03-26 01:03:14,641] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 878313.318026995 W.
[2019-03-26 01:03:14,649] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.0, 99.0, 1.0, 2.0, 0.294266891136639, 1.0, 2.0, 0.294266891136639, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 878313.318026995, 878313.318026995, 256113.1566230523], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3055800.0000, 
sim time next is 3056400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.326408759952629, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5695913267593589, 6.911199999999999, 6.9112, 168.9129565104282, 977945.2859002494, 977945.2859002501, 235927.1187815908], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.18844428909955305, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.47511137409677906, -8.881784197001253e-17, 0.0, 0.8294399451522889, 0.2716514683056248, 0.27165146830562503, 0.3521300280322251], 
reward next is 0.6479, 
noisyNet noise sample is [array([2.9058084], dtype=float32), 0.95935464]. 
=============================================
[2019-03-26 01:03:32,030] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:32,043] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4796
[2019-03-26 01:03:32,050] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.16666666666667, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8009480907368747, 6.911199999999999, 6.9112, 168.912956510431, 672093.0460930732, 672093.0460930738, 202045.7483042865], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3286200.0000, 
sim time next is 3286800.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7976639944974733, 6.9112, 6.9112, 168.912956510431, 669721.4291463997, 669721.4291463997, 201377.9325685576], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7532487737774064, 0.0, 0.0, 0.8294399451523027, 0.18603373031844436, 0.18603373031844436, 0.30056407846053373], 
reward next is 0.6994, 
noisyNet noise sample is [array([-0.48545486], dtype=float32), 0.73941624]. 
=============================================
[2019-03-26 01:03:33,213] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:33,225] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7384
[2019-03-26 01:03:33,235] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1600549.47170588 W.
[2019-03-26 01:03:33,239] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5724680285119166, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9941876474552206, 6.911199999999999, 6.9112, 168.9129565104302, 1600549.47170588, 1600549.47170588, 350267.2234228236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3587400.0000, 
sim time next is 3588000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 9.97935861944461, 6.9112, 168.8951082142504, 3631533.166817985, 1455105.842209738, 306372.0144735329], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.3068158619444611, 0.0, 0.8293523018470705, 1.0087592130049958, 0.4041960672804828, 0.4572716633933327], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6563873], dtype=float32), 1.1345532]. 
=============================================
[2019-03-26 01:03:33,255] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[38.451183]
 [37.921337]
 [34.99973 ]
 [34.6591  ]
 [34.88983 ]], R is [[34.54288101]
 [34.19745255]
 [34.33624649]
 [34.28144455]
 [34.11674118]].
[2019-03-26 01:03:36,626] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:03:36,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5762
[2019-03-26 01:03:36,642] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.3, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8980914307231469, 6.9112, 6.9112, 168.912956510431, 739041.8707001244, 739041.8707001244, 222897.1575422503], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3376800.0000, 
sim time next is 3377400.0000, 
raw observation next is [26.25, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 11.02169229771372, 6.9112, 168.8820148055104, 3764642.657731354, 849052.1291935723, 255809.8467695521], 
processed observation next is [1.0, 0.08695652173913043, 0.4431279620853081, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.41104922977137204, 0.0, 0.8292880072159687, 1.0457340715920427, 0.2358478136648812, 0.3818057414470927], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7408336], dtype=float32), 3.2806842]. 
=============================================
[2019-03-26 01:03:48,059] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.00000000e+00 2.81597019e-34 1.13646064e-29 2.77337525e-23
 5.24078990e-13], sum to 1.0000
[2019-03-26 01:03:48,071] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1318
[2019-03-26 01:03:48,084] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1893346.645071503 W.
[2019-03-26 01:03:48,089] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333333, 65.33333333333334, 1.0, 2.0, 0.4514034554758391, 1.0, 2.0, 0.4514034554758391, 1.0, 2.0, 0.7723425718409548, 6.9112, 6.9112, 170.5573041426782, 1893346.645071503, 1893346.645071503, 381288.0406393638], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3750000.0000, 
sim time next is 3750600.0000, 
raw observation next is [30.66666666666667, 64.16666666666666, 1.0, 2.0, 0.4598542617691183, 1.0, 2.0, 0.4598542617691183, 1.0, 2.0, 0.7880180849759245, 6.911200000000001, 6.9112, 170.5573041426782, 1928824.238444183, 1928824.238444182, 386742.7073694456], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879939, 0.6416666666666666, 1.0, 1.0, 0.34922200213146787, 1.0, 1.0, 0.34922200213146787, 1.0, 1.0, 0.7414854694828347, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5357845106789397, 0.5357845106789395, 0.5772279214469337], 
reward next is 0.4228, 
noisyNet noise sample is [array([1.8808329], dtype=float32), -1.1719545]. 
=============================================
[2019-03-26 01:04:01,479] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:04:01,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7726
[2019-03-26 01:04:01,495] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.41666666666667, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9956914855070335, 6.911200000000001, 6.9112, 168.912956510431, 804688.1479370558, 804688.1479370552, 246062.5457596916], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3895800.0000, 
sim time next is 3896400.0000, 
raw observation next is [27.33333333333334, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9931340569789783, 6.9112, 6.9112, 168.912956510431, 803052.3269038689, 803052.3269038689, 245431.034217188], 
processed observation next is [0.0, 0.08695652173913043, 0.4944707740916275, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9916268987548514, 0.0, 0.0, 0.8294399451523027, 0.22307009080663026, 0.22307009080663026, 0.36631497644356414], 
reward next is 0.6337, 
noisyNet noise sample is [array([-0.15562683], dtype=float32), -0.31681302]. 
=============================================
[2019-03-26 01:04:03,988] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 1.9687887e-36 1.1646597e-35 2.7259958e-31 2.5025993e-26], sum to 1.0000
[2019-03-26 01:04:03,998] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8100
[2019-03-26 01:04:04,013] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.66666666666667, 84.0, 1.0, 2.0, 0.3066083504166516, 1.0, 2.0, 0.3066083504166516, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 856935.5486256512, 856935.5486256512, 251178.0567581918], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3975600.0000, 
sim time next is 3976200.0000, 
raw observation next is [29.5, 84.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.056316599799934, 6.9112, 168.9120112889055, 931791.2212299688, 828841.0869067899, 254812.5287162991], 
processed observation next is [1.0, 0.0, 0.5971563981042655, 0.84, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.014511659979993397, 0.0, 0.8294353036818881, 0.25883089478610244, 0.2302336352518861, 0.38031720703925237], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.759534], dtype=float32), -0.98375]. 
=============================================
[2019-03-26 01:04:05,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:04:05,203] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9379
[2019-03-26 01:04:05,213] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [35.0, 55.33333333333333, 1.0, 1.0, 0.1999189647399677, 1.0, 1.0, 0.1999189647399677, 1.0, 2.0, 0.3471931275414, 6.9112, 6.9112, 170.5573041426782, 838118.8829093355, 838118.8829093355, 269821.4246075939], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [35.0, 55.16666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.95722550844788, 6.9112, 168.9125329844898, 861465.654563151, 828813.6566650633, 254812.4651298561], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.5516666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004602550844787956, 0.0, 0.8294378654460403, 0.2392960151564308, 0.2302260157402954, 0.38031711213411357], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8607091], dtype=float32), -2.4253035]. 
=============================================
[2019-03-26 01:04:05,524] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 01:04:05,527] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:04:05,528] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:04:05,529] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:04:05,532] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:04:05,533] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:04:05,537] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:04:05,537] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:04:05,538] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:04:05,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:04:05,540] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:04:05,566] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-26 01:04:05,566] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-26 01:04:05,612] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-26 01:04:05,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-26 01:04:05,660] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-26 01:04:12,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:12,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.5, 65.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4180135386467268, 6.911200000000001, 6.9112, 168.912956510431, 376183.074520756, 376183.0745207553, 142305.4997093067]
[2019-03-26 01:04:12,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:04:12,094] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5026721839122412
[2019-03-26 01:04:27,714] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:27,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.68333333333333, 86.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4179467222550814, 6.9112, 6.9112, 168.912956510431, 376204.8523432268, 376204.8523432268, 142291.504421281]
[2019-03-26 01:04:27,718] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:04:27,722] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9460944159185454
[2019-03-26 01:04:34,355] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:34,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.89389896, 90.99596102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8643195729180341, 6.9112, 6.9112, 168.912956510431, 718523.8849188082, 718523.8849188082, 215479.7984709509]
[2019-03-26 01:04:34,358] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:04:34,360] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9105355254709165
[2019-03-26 01:04:36,588] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:36,588] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.55, 84.0, 1.0, 2.0, 0.6015794744767613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 919441.7119100587, 919441.711910058, 210683.3023794186]
[2019-03-26 01:04:36,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:04:36,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 1.5225630e-37 7.0851905e-32 4.0741050e-31 2.8913318e-09], sampled 0.39301050092968726
[2019-03-26 01:04:36,594] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 919441.7119100587 W.
[2019-03-26 01:04:37,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:37,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.93333333333333, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6795777240716856, 6.911200000000001, 6.9112, 168.912956510431, 582598.7978465337, 582598.7978465331, 179124.9312910692]
[2019-03-26 01:04:37,431] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:04:37,437] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7195541523495634
[2019-03-26 01:04:39,239] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:39,240] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.5, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8466552624851676, 6.911200000000001, 6.9112, 168.912956510431, 702542.5567312111, 702542.5567312104, 211541.7561370014]
[2019-03-26 01:04:39,242] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:04:39,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7926397609489448
[2019-03-26 01:04:53,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:53,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.65, 77.5, 1.0, 2.0, 0.709491320175747, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129561813382, 991540.52700727, 991540.52700727, 222904.7690709326]
[2019-03-26 01:04:53,045] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:04:53,048] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.318629445552314
[2019-03-26 01:04:53,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 991540.52700727 W.
[2019-03-26 01:04:55,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1752448], dtype=float32), 0.10931989]
[2019-03-26 01:04:55,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5651447595459453, 6.9112, 6.9112, 168.912956510431, 496642.9052064221, 496642.9052064221, 160828.6207439675]
[2019-03-26 01:04:55,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:04:55,578] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3670925855225844
[2019-03-26 01:06:14,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8079.1434 2941730502.1088 1325.0000
[2019-03-26 01:06:15,134] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7302.7160 3323216556.5311 2076.0000
[2019-03-26 01:06:15,507] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7041.7301 3186789356.8484 2440.0000
[2019-03-26 01:06:15,611] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7354.1101 3107565843.7238 2002.0000
[2019-03-26 01:06:15,622] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7945.8673 2992254453.7163 1479.0000
[2019-03-26 01:06:16,637] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1375000, evaluation results [1375000.0, 7302.716027005829, 3323216556.5310564, 2076.0, 7354.110148346735, 3107565843.72376, 2002.0, 8079.143426788007, 2941730502.1088185, 1325.0, 7041.730060308685, 3186789356.848404, 2440.0, 7945.867267602287, 2992254453.7162905, 1479.0]
[2019-03-26 01:06:21,465] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:06:21,477] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0741
[2019-03-26 01:06:21,491] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1997026.241099049 W.
[2019-03-26 01:06:21,497] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.7141489147103276, 1.0, 2.0, 0.7141489147103276, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1997026.241099049, 1997026.241099049, 380234.6176393908], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4008000.0000, 
sim time next is 4008600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.7442486315925435, 1.0, 2.0, 0.7442486315925435, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2081278.018435281, 2081278.018435281, 393636.4258067719], 
processed observation next is [1.0, 0.391304347826087, 0.5260663507109005, 0.79, 1.0, 1.0, 0.6918658211958355, 1.0, 1.0, 0.6918658211958355, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5781327828986892, 0.5781327828986892, 0.5875170534429431], 
reward next is 0.4125, 
noisyNet noise sample is [array([-0.43212828], dtype=float32), 1.200213]. 
=============================================
[2019-03-26 01:06:33,629] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:06:33,638] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3214
[2019-03-26 01:06:33,641] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.016339945624219, 6.911200000000001, 6.9112, 168.9128091844515, 819181.1964119937, 819181.1964119931, 251291.9259612268], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4150200.0000, 
sim time next is 4150800.0000, 
raw observation next is [28.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9931261750591639, 6.9112, 6.9112, 168.9129565076569, 801038.5643969907, 801038.5643969907, 245316.6935808947], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9916172866575168, 0.0, 0.0, 0.8294399451386806, 0.22251071233249742, 0.22251071233249742, 0.3661443187774548], 
reward next is 0.6339, 
noisyNet noise sample is [array([0.09744275], dtype=float32), 0.012749168]. 
=============================================
[2019-03-26 01:06:33,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 4.2325234e-38 0.0000000e+00 7.5747605e-31 6.6972622e-38], sum to 1.0000
[2019-03-26 01:06:33,804] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9301
[2019-03-26 01:06:33,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 924110.3160291838 W.
[2019-03-26 01:06:33,819] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.045493923670755, 6.9112, 168.9120587225272, 924110.3160291838, 828838.0909203403, 254812.5346738525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [31.0, 75.0, 1.0, 1.0, 0.6164565780305276, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128471626197, 861468.2800505797, 861468.2800505797, 203850.8660335873], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 0.5, 0.5378994916030452, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294394082044896, 0.23929674445849436, 0.23929674445849436, 0.30425502393072734], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4267313], dtype=float32), -0.03413639]. 
=============================================
[2019-03-26 01:06:34,014] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 7.3872861e-36 0.0000000e+00 2.3823763e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 01:06:34,027] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5630
[2019-03-26 01:06:34,032] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 75.0, 1.0, 1.0, 0.3059765075367174, 1.0, 1.0, 0.3059765075367174, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 855168.9154033519, 855168.9154033519, 251053.074812883], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4237200.0000, 
sim time next is 4237800.0000, 
raw observation next is [30.83333333333334, 75.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.022388030057219, 6.9112, 168.9121886718727, 907711.9513973265, 828831.6946507954, 254812.475010987], 
processed observation next is [1.0, 0.043478260869565216, 0.6603475513428123, 0.7566666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.011118803005721923, 0.0, 0.8294361747134634, 0.2521422087214796, 0.23023102629188763, 0.38031712688207014], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.956626], dtype=float32), 0.9707434]. 
=============================================
[2019-03-26 01:06:34,875] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 4.8974995e-25 0.0000000e+00 5.5258835e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 01:06:34,886] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7805
[2019-03-26 01:06:34,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3174958.052833151 W.
[2019-03-26 01:06:34,902] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 57.0, 1.0, 2.0, 0.871931625268296, 1.0, 2.0, 0.7565558521484108, 1.0, 2.0, 1.03, 7.005111292722729, 6.9112, 170.5573041426782, 3174958.052833151, 3107685.573788712, 581136.2052201856], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4197600.0000, 
sim time next is 4198200.0000, 
raw observation next is [36.16666666666666, 55.83333333333333, 1.0, 2.0, 0.8295822107582633, 1.0, 2.0, 0.7353811448933943, 1.0, 2.0, 1.03, 7.005107951818137, 6.9112, 170.5573041426782, 3085986.606713873, 3018716.520895293, 565382.6568803716], 
processed observation next is [1.0, 0.6086956521739131, 0.9131121642969979, 0.5583333333333332, 1.0, 1.0, 0.7946773623593534, 1.0, 1.0, 0.681182102281198, 1.0, 1.0, 1.0365853658536586, 0.009390795181813693, 0.0, 0.8375144448122397, 0.8572185018649647, 0.8385323669153592, 0.8438547117617486], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5025169], dtype=float32), -1.3463315]. 
=============================================
[2019-03-26 01:06:41,602] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:06:41,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3843
[2019-03-26 01:06:41,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3488452.686799362 W.
[2019-03-26 01:06:41,629] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 53.33333333333334, 1.0, 2.0, 1.02112752537051, 1.0, 2.0, 0.8311538021995176, 1.0, 1.0, 1.03, 7.005123066435322, 6.9112, 170.5573041426782, 3488452.686799362, 3421171.773765669, 641974.7518117873], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4281600.0000, 
sim time next is 4282200.0000, 
raw observation next is [38.0, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 0.8505538313631091, 1.0, 2.0, 1.03, 7.04404671589436, 6.9112, 170.5573041426782, 3569993.362020559, 3474829.855566527, 652611.609940718], 
processed observation next is [1.0, 0.5652173913043478, 1.0, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 0.819944375136276, 1.0, 1.0, 1.0365853658536586, 0.01328467158943596, 0.0, 0.8375144448122397, 0.9916648227834887, 0.9652305154351465, 0.9740471790159971], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8603301], dtype=float32), 1.1114241]. 
=============================================
[2019-03-26 01:06:50,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 8.1423489e-33 2.7645130e-38 2.8211488e-29 3.7800897e-27], sum to 1.0000
[2019-03-26 01:06:50,770] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4472
[2019-03-26 01:06:50,776] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 75.0, 1.0, 1.0, 0.205527215300544, 1.0, 1.0, 0.205527215300544, 1.0, 2.0, 0.3569328040883196, 6.9112, 6.9112, 170.5573041426782, 861639.7574342468, 861639.7574342468, 271394.3537340757], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4234800.0000, 
sim time next is 4235400.0000, 
raw observation next is [31.0, 75.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.030923071797945, 6.9112, 168.9121386016128, 913769.313757847, 828834.0573489966, 254812.5313245117], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.011972307179794495, 0.0, 0.829435928845565, 0.2538248093771797, 0.23023168259694352, 0.380317210932107], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0254757], dtype=float32), -0.96768034]. 
=============================================
[2019-03-26 01:06:57,776] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:06:57,786] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9716
[2019-03-26 01:06:57,792] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8429514042008706, 6.9112, 6.9112, 168.912956510431, 702188.4758776496, 702188.4758776496, 210817.3237868299], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4510800.0000, 
sim time next is 4511400.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8465931623651404, 6.9112, 6.9112, 168.912956510431, 704564.6133413261, 704564.6133413261, 211591.5585089658], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8129184906891956, 0.0, 0.0, 0.8294399451523027, 0.19571239259481282, 0.19571239259481282, 0.3158082962820385], 
reward next is 0.6842, 
noisyNet noise sample is [array([-0.52980274], dtype=float32), -0.8139813]. 
=============================================
[2019-03-26 01:07:00,953] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:07:00,965] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0707
[2019-03-26 01:07:00,975] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 936807.3285488776 W.
[2019-03-26 01:07:00,980] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 79.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.063384486486759, 6.9112, 168.9119748592911, 936807.3285488776, 828843.04349609, 254812.9293190726], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4437600.0000, 
sim time next is 4438200.0000, 
raw observation next is [30.83333333333334, 79.0, 1.0, 1.0, 0.3139798375055295, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5452791430281542, 6.911200000000001, 6.9112, 168.9128325953568, 877549.943208729, 877549.9432087283, 226568.0059710049], 
processed observation next is [0.0, 0.34782608695652173, 0.6603475513428123, 0.79, 1.0, 0.5, 0.1734696837416018, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44546236954652946, 8.881784197001253e-17, 0.0, 0.8294393366725598, 0.24376387311353584, 0.24376387311353565, 0.33816120294179836], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14053592], dtype=float32), 0.84519506]. 
=============================================
[2019-03-26 01:07:04,836] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:07:04,846] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9498
[2019-03-26 01:07:04,851] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8812946506813131, 6.911199999999999, 6.9112, 168.912956510431, 727452.7462166852, 727452.7462166859, 219129.0031319059], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4529400.0000, 
sim time next is 4530000.0000, 
raw observation next is [28.33333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.891033251196414, 6.911200000000001, 6.9112, 168.912956510431, 733800.9636913787, 733800.9636913781, 221291.2454771031], 
processed observation next is [0.0, 0.43478260869565216, 0.5418641390205374, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8671137209712364, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20383360102538298, 0.2038336010253828, 0.33028544101060164], 
reward next is 0.6697, 
noisyNet noise sample is [array([0.81096], dtype=float32), 0.37274224]. 
=============================================
[2019-03-26 01:07:04,870] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.99643 ]
 [72.010376]
 [71.86681 ]
 [71.85098 ]
 [71.81983 ]], R is [[71.91717529]
 [71.87094879]
 [71.82888794]
 [71.79006195]
 [71.75424957]].
[2019-03-26 01:07:06,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:07:06,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8916
[2019-03-26 01:07:06,590] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666667, 70.0, 1.0, 2.0, 0.2584232469089048, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4456688160350644, 6.911199999999999, 6.9112, 168.9129565104066, 722220.6761557962, 722220.6761557967, 209290.7049136631], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4728000.0000, 
sim time next is 4728600.0000, 
raw observation next is [30.5, 70.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8716160956702388, 6.9112, 6.9112, 168.912956510431, 708401.8239281392, 708401.8239281392, 216479.6354279607], 
processed observation next is [1.0, 0.7391304347826086, 0.6445497630331753, 0.7, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8434342630124864, 0.0, 0.0, 0.8294399451523027, 0.1967782844244831, 0.1967782844244831, 0.3231039334745682], 
reward next is 0.6769, 
noisyNet noise sample is [array([0.4642019], dtype=float32), -0.16109812]. 
=============================================
[2019-03-26 01:07:10,629] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:07:10,638] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4014
[2019-03-26 01:07:10,644] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8737319547053178, 6.911199999999999, 6.9112, 168.912956510431, 721624.3149491311, 721624.3149491318, 217432.7478213958], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4742400.0000, 
sim time next is 4743000.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8792916457752651, 6.911199999999999, 6.9112, 168.912956510431, 726217.7004945856, 726217.7004945861, 218689.3805937183], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8527946899698354, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20172713902627376, 0.20172713902627393, 0.32640206058763926], 
reward next is 0.6736, 
noisyNet noise sample is [array([0.03336742], dtype=float32), -1.9321159]. 
=============================================
[2019-03-26 01:07:10,664] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[49.00358 ]
 [49.0054  ]
 [48.834843]
 [48.80754 ]
 [49.04311 ]], R is [[49.11497116]
 [49.29929352]
 [49.48241806]
 [49.66350937]
 [49.84236145]].
[2019-03-26 01:07:17,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1993512e-08 2.2851280e-18 3.1674677e-35 2.4349471e-26 9.9999988e-01], sum to 1.0000
[2019-03-26 01:07:17,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6563
[2019-03-26 01:07:17,452] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.8726875672030958, 1.0, 2.0, 0.7569338231158105, 1.0, 1.0, 1.03, 7.005111352362583, 6.9112, 170.5573041426782, 3176546.260993259, 3109273.739226366, 581423.1911462751], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 0.8561489192453328, 1.0, 2.0, 0.748664499136929, 1.0, 2.0, 1.03, 7.005110047584616, 6.9112, 170.5573041426782, 3141799.588360416, 3074528.001259057, 575186.7100420159], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 0.8266854448738948, 1.0, 1.0, 0.6971861435384686, 1.0, 1.0, 1.0365853658536586, 0.00939100475846164, 0.0, 0.8375144448122397, 0.8727221078778933, 0.8540355559052936, 0.8584876269283819], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.366329], dtype=float32), -1.2002773]. 
=============================================
[2019-03-26 01:07:23,816] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:07:23,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7316
[2019-03-26 01:07:23,830] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 84.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.2884561923065117, 6.9112, 6.9112, 170.5573041426782, 712263.7993351643, 712263.7993351643, 261850.8898217296], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4926600.0000, 
sim time next is 4927200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2877572085691146, 6.9112, 6.9112, 170.5573041426782, 711141.4289680858, 711141.4289680858, 261672.4594404111], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.13141122996233484, 0.0, 0.0, 0.8375144448122397, 0.19753928582446828, 0.19753928582446828, 0.3905559096125539], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5885143], dtype=float32), 0.8296002]. 
=============================================
[2019-03-26 01:07:24,375] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 01:07:24,378] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:07:24,379] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:07:24,380] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:07:24,383] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:07:24,384] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:07:24,384] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:07:24,387] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:07:24,388] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:07:24,380] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:07:24,393] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:07:24,420] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-26 01:07:24,421] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-26 01:07:24,467] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-26 01:07:24,492] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-26 01:07:24,513] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-26 01:07:26,790] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:07:26,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.07094561, 84.20484855333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2209282097763519, 6.911199999999999, 6.9112, 184.5923449428631, 572710.346627504, 572710.3466275047, 244849.3491031936]
[2019-03-26 01:07:26,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:07:26,799] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.3992871e-01 1.6247327e-30 4.0678031e-36 1.5153884e-30 6.0071245e-02], sampled 0.39711519184624366
[2019-03-26 01:07:44,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:07:44,208] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.66433679, 58.3148235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7670464100808074, 6.911199999999999, 6.9112, 168.912956510431, 658643.4855300128, 658643.4855300135, 195360.9375422048]
[2019-03-26 01:07:44,209] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:07:44,212] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8254596e-15], sampled 0.6092813205017082
[2019-03-26 01:08:00,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:08:00,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.0, 93.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2298719810012795, 6.9112, 6.9112, 169.0403247858759, 589312.6859840875, 589312.6859840875, 243221.7951960742]
[2019-03-26 01:08:00,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:08:00,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.1945366e-11 1.1666480e-34 0.0000000e+00 1.7800779e-36 1.0000000e+00], sampled 0.4701938619565871
[2019-03-26 01:08:06,129] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:08:06,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333333, 95.66666666666667, 1.0, 2.0, 0.2919296115658213, 1.0, 2.0, 0.2919296115658213, 1.0, 2.0, 0.5038047702773358, 6.9112, 6.9112, 170.5573041426782, 1224074.646380874, 1224074.646380874, 300838.9111634776]
[2019-03-26 01:08:06,134] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:08:06,137] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.7546773e-02 7.7518287e-34 5.1939562e-35 7.9343076e-31 9.1245323e-01], sampled 0.16882756990315484
[2019-03-26 01:08:37,188] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:08:37,190] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.55464414333333, 75.78631479166667, 1.0, 2.0, 0.2465813198443199, 1.0, 2.0, 0.2465813198443199, 1.0, 2.0, 0.42823020688102, 6.9112, 6.9112, 184.5923449428631, 1033797.554016252, 1033797.554016252, 288307.0024578595]
[2019-03-26 01:08:37,193] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:08:37,195] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.5037365743719137
[2019-03-26 01:09:07,512] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:09:07,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.96579574, 71.299357725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6379820200933302, 6.9112, 6.9112, 168.912956510431, 557057.348529394, 557057.348529394, 172019.9522813952]
[2019-03-26 01:09:07,515] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:09:07,518] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.49770772053154777
[2019-03-26 01:09:33,416] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7738.6462 3124496909.8749 592.0000
[2019-03-26 01:09:34,188] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1454794], dtype=float32), -0.007942136]
[2019-03-26 01:09:34,189] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.41666666666666, 77.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8796642890621025, 6.9112, 6.9112, 168.912956510431, 724589.924356995, 724589.924356995, 218702.0394313794]
[2019-03-26 01:09:34,190] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:09:34,194] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9491411684006114
[2019-03-26 01:09:34,212] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7671.1758 3151829587.4797 640.0000
[2019-03-26 01:09:34,334] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7168.3610 3458316439.3355 1060.0000
[2019-03-26 01:09:34,477] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7569.7445 3264911208.3014 887.0000
[2019-03-26 01:09:34,507] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7428.2880 3322796184.0243 864.0000
[2019-03-26 01:09:35,524] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1400000, evaluation results [1400000.0, 7168.360966811879, 3458316439.3355007, 1060.0, 7569.744543274817, 3264911208.301434, 887.0, 7738.646182990921, 3124496909.87488, 592.0, 7428.287964642137, 3322796184.024286, 864.0, 7671.1757778346355, 3151829587.479662, 640.0]
[2019-03-26 01:09:36,911] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 2.7699044e-25 5.6526621e-33 2.5054707e-29 3.2472944e-15], sum to 1.0000
[2019-03-26 01:09:36,919] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3500
[2019-03-26 01:09:36,929] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2002035.871858922 W.
[2019-03-26 01:09:36,935] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4772924781620123, 1.0, 2.0, 0.4772924781620123, 1.0, 2.0, 0.8261652108697066, 6.9112, 6.9112, 170.5573041426782, 2002035.871858922, 2002035.871858922, 399330.5572392152], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4878000.0000, 
sim time next is 4878600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4951307417624461, 1.0, 2.0, 0.4951307417624461, 1.0, 2.0, 0.8571912357778914, 6.9112, 6.9112, 170.5573041426782, 2076932.218116486, 2076932.218116486, 411315.7879392066], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.39172378525595913, 1.0, 1.0, 0.39172378525595913, 1.0, 1.0, 0.825842970460843, 0.0, 0.0, 0.8375144448122397, 0.5769256161434684, 0.5769256161434684, 0.6139041611032934], 
reward next is 0.3861, 
noisyNet noise sample is [array([-1.4119415], dtype=float32), -0.93912774]. 
=============================================
[2019-03-26 01:09:38,570] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 6.2412887e-33 0.0000000e+00 5.0937528e-35 1.9023153e-27], sum to 1.0000
[2019-03-26 01:09:38,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2506
[2019-03-26 01:09:38,591] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8479566595792942, 6.9112, 6.9112, 168.912956510431, 703949.863263855, 703949.863263855, 211836.1486153702], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4750200.0000, 
sim time next is 4750800.0000, 
raw observation next is [27.66666666666666, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8425540160671485, 6.9112, 6.9112, 168.912956510431, 700763.0954832536, 700763.0954832536, 210698.8820638934], 
processed observation next is [1.0, 1.0, 0.5102685624012636, 0.7733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8079927025209128, 0.0, 0.0, 0.8294399451523027, 0.1946564154120149, 0.1946564154120149, 0.31447594337894536], 
reward next is 0.6855, 
noisyNet noise sample is [array([-0.6900864], dtype=float32), -0.038673356]. 
=============================================
[2019-03-26 01:09:46,215] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.1027317e-36 0.0000000e+00 0.0000000e+00 7.4903132e-23], sum to 1.0000
[2019-03-26 01:09:46,223] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0672
[2019-03-26 01:09:46,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 997208.5415371908 W.
[2019-03-26 01:09:46,243] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.7135451332144479, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564914772, 997208.5415371908, 997208.5415371902, 223794.3769369111], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4853400.0000, 
sim time next is 4854000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.3157997674915294, 1.0, 1.0, 0.3157997674915294, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 882635.0875655959, 882635.0875655959, 252995.158204809], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.84, 1.0, 1.0, 0.17566237047172215, 1.0, 0.5, 0.17566237047172215, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2451764132126655, 0.2451764132126655, 0.3776047137385209], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3143531], dtype=float32), -0.11667593]. 
=============================================
[2019-03-26 01:09:46,256] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[46.456177]
 [48.070873]
 [49.74766 ]
 [49.01286 ]
 [49.573917]], R is [[44.59415436]
 [44.14821243]
 [43.70672989]
 [43.26966476]
 [42.83696747]].
[2019-03-26 01:09:46,728] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.434007e-22], sum to 1.0000
[2019-03-26 01:09:46,737] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0318
[2019-03-26 01:09:46,741] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8718522399577014, 6.9112, 6.9112, 168.912956510431, 722447.478955866, 722447.478955866, 217092.1946143786], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [27.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8708854689663331, 6.911200000000001, 6.9112, 168.912956510431, 721646.1178474359, 721646.1178474353, 216874.8636170184], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8425432548369914, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20045725495762107, 0.20045725495762093, 0.3236938262940573], 
reward next is 0.6763, 
noisyNet noise sample is [array([-1.7885623], dtype=float32), -0.9017047]. 
=============================================
[2019-03-26 01:09:48,859] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:09:48,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2777
[2019-03-26 01:09:48,881] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.16666666666667, 72.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8874589976520353, 6.911200000000001, 6.9112, 168.912956510431, 731521.0212447365, 731521.0212447359, 220496.8690905058], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5127000.0000, 
sim time next is 5127600.0000, 
raw observation next is [29.33333333333334, 71.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8845403656616174, 6.911199999999999, 6.9112, 168.912956510431, 729564.4327851983, 729564.4327851989, 219846.8954175102], 
processed observation next is [0.0, 0.34782608695652173, 0.5892575039494474, 0.7133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8591955678800212, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2026567868847773, 0.20265678688477748, 0.3281296946530003], 
reward next is 0.6719, 
noisyNet noise sample is [array([-0.4437691], dtype=float32), -0.108498424]. 
=============================================
[2019-03-26 01:09:52,773] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1946495e-35], sum to 1.0000
[2019-03-26 01:09:52,782] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0157
[2019-03-26 01:09:52,789] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7994072433436223, 6.9112, 6.9112, 168.912956510431, 671499.0081530006, 671499.0081530006, 201743.0173660473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5119200.0000, 
sim time next is 5119800.0000, 
raw observation next is [26.16666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8047487769092135, 6.9112, 6.9112, 168.912956510431, 675352.8243392322, 675352.8243392322, 202833.3642284786], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7618887523283091, 0.0, 0.0, 0.8294399451523027, 0.18759800676089783, 0.18759800676089783, 0.3027363645201173], 
reward next is 0.6973, 
noisyNet noise sample is [array([0.3969436], dtype=float32), 0.212505]. 
=============================================
[2019-03-26 01:09:54,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1035809e-04 3.2781306e-22 7.3082221e-38 2.3199088e-30 9.9918967e-01], sum to 1.0000
[2019-03-26 01:09:54,202] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7876
[2019-03-26 01:09:54,210] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.6020941493311913, 1.0, 2.0, 0.6020941493311913, 1.0, 2.0, 1.03, 6.928780015728591, 6.9112, 170.5573041426782, 2526089.52049534, 2513496.240119632, 488738.6902222295], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5244600.0000, 
sim time next is 5245200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.6695818648676755, 1.0, 2.0, 0.6553809719481003, 1.0, 2.0, 1.03, 7.005095333836923, 6.9112, 170.5573041426782, 2749900.73124356, 2682639.68419819, 511892.507007957], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.6019058612863559, 1.0, 1.0, 0.5847963517446991, 1.0, 1.0, 1.0365853658536586, 0.009389533383692328, 0.0, 0.8375144448122397, 0.7638613142343222, 0.7451776900550527, 0.7640186671760553], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31798032], dtype=float32), -0.70413923]. 
=============================================
[2019-03-26 01:09:54,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.5623472e-27], sum to 1.0000
[2019-03-26 01:09:54,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1439
[2019-03-26 01:09:54,784] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8875510760664918, 6.911199999999999, 6.9112, 168.912956510431, 730100.8392509497, 730100.8392509504, 220460.0760950433], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5061600.0000, 
sim time next is 5062200.0000, 
raw observation next is [31.16666666666667, 62.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9072507808684513, 6.911199999999999, 6.9112, 168.912956510431, 747134.1094698196, 747134.1094698202, 225051.9069551557], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.6233333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8868911961810383, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20753725263050543, 0.2075372526305056, 0.3358983685897846], 
reward next is 0.6641, 
noisyNet noise sample is [array([1.7265729], dtype=float32), 0.39725643]. 
=============================================
[2019-03-26 01:10:05,080] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:05,088] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4508
[2019-03-26 01:10:05,094] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8568016392164056, 6.911200000000001, 6.9112, 168.912956510431, 712394.8416375022, 712394.8416375016, 213814.1485409866], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5101200.0000, 
sim time next is 5101800.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.854232274770525, 6.911200000000001, 6.9112, 168.912956510431, 710540.4599146695, 710540.4599146689, 213255.6872539773], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8222344814274694, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19737234997629707, 0.19737234997629693, 0.3182920705283243], 
reward next is 0.6817, 
noisyNet noise sample is [array([-1.1740413], dtype=float32), 0.6165275]. 
=============================================
[2019-03-26 01:10:07,950] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:07,965] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8039
[2019-03-26 01:10:07,970] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333333, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9349780235106199, 6.9112, 6.9112, 168.912956510431, 762168.5117103037, 762168.5117103037, 231305.8876491188], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5157600.0000, 
sim time next is 5158200.0000, 
raw observation next is [31.16666666666667, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.931266580816203, 6.9112, 6.9112, 168.912956510431, 759881.1769350927, 759881.1769350927, 230448.0481031945], 
processed observation next is [0.0, 0.6956521739130435, 0.6761453396524489, 0.655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9161787570929303, 0.0, 0.0, 0.8294399451523027, 0.21107810470419242, 0.21107810470419242, 0.34395231060178283], 
reward next is 0.6560, 
noisyNet noise sample is [array([-0.53765154], dtype=float32), 0.41918495]. 
=============================================
[2019-03-26 01:10:09,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:09,144] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1867
[2019-03-26 01:10:09,151] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8485817923747239, 6.9112, 6.9112, 168.912956510431, 706588.3305639104, 706588.3305639104, 212037.0613730939], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5182800.0000, 
sim time next is 5183400.0000, 
raw observation next is [27.0, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8558079716225674, 6.9112, 6.9112, 168.912956510431, 711383.3171582368, 711383.3171582368, 213589.1541899505], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.8316666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8241560629543504, 0.0, 0.0, 0.8294399451523027, 0.1976064769883991, 0.1976064769883991, 0.31878978237306044], 
reward next is 0.6812, 
noisyNet noise sample is [array([0.55742854], dtype=float32), -1.4598315]. 
=============================================
[2019-03-26 01:10:11,534] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.3508568e-38 0.0000000e+00 1.1732788e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 01:10:11,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0815
[2019-03-26 01:10:11,554] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1562723.643751109 W.
[2019-03-26 01:10:11,562] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 77.0, 1.0, 2.0, 0.372635195362345, 1.0, 2.0, 0.372635195362345, 1.0, 2.0, 0.6471441020021863, 6.9112, 6.9112, 170.5573041426782, 1562723.643751109, 1562723.643751109, 338258.9913607239], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5383200.0000, 
sim time next is 5383800.0000, 
raw observation next is [31.2, 76.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.574606129388861, 6.9112, 168.9093327841076, 1924710.339794181, 1454077.29573835, 311356.2906540902], 
processed observation next is [1.0, 0.30434782608695654, 0.6777251184834123, 0.76, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.06634061293888607, 0.0, 0.8294221509971057, 0.5346417610539392, 0.40391035992731944, 0.4647108815732689], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8589481], dtype=float32), 2.0789104]. 
=============================================
[2019-03-26 01:10:12,820] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.1200605e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 01:10:12,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2181
[2019-03-26 01:10:12,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2715022.848676683 W.
[2019-03-26 01:10:12,847] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.9706163779684097, 1.0, 1.0, 0.9706163779684097, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2715022.848676683, 2715022.848676683, 511483.1178392071], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5232600.0000, 
sim time next is 5233200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.8695171753001846, 1.0, 2.0, 0.8695171753001846, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2431951.404769565, 2431951.404769565, 455137.6761985502], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.8427917774701019, 1.0, 1.0, 0.8427917774701019, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6755420568804347, 0.6755420568804347, 0.6793099644754481], 
reward next is 0.3207, 
noisyNet noise sample is [array([0.19869083], dtype=float32), 0.86725575]. 
=============================================
[2019-03-26 01:10:13,124] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:13,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6070
[2019-03-26 01:10:13,144] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2535695.50917851 W.
[2019-03-26 01:10:13,148] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.3, 52.0, 1.0, 2.0, 0.6043814185589047, 1.0, 2.0, 0.6043814185589047, 1.0, 2.0, 1.03, 6.933245504304485, 6.9112, 170.5573041426782, 2535695.50917851, 2519903.417691427, 489573.6346556428], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5490000.0000, 
sim time next is 5490600.0000, 
raw observation next is [36.41666666666666, 51.0, 1.0, 2.0, 0.8123774375003934, 1.0, 2.0, 0.8123774375003934, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2271992.072318485, 2271992.072318485, 425952.5354447008], 
processed observation next is [1.0, 0.5652173913043478, 0.9249605055292255, 0.51, 1.0, 1.0, 0.773948719879992, 1.0, 1.0, 0.773948719879992, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6311089089773569, 0.6311089089773569, 0.6357500529025385], 
reward next is 0.3642, 
noisyNet noise sample is [array([-2.3019454], dtype=float32), -0.17839773]. 
=============================================
[2019-03-26 01:10:13,523] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:13,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4466
[2019-03-26 01:10:13,544] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2319425.153788156 W.
[2019-03-26 01:10:13,548] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5528813123174013, 1.0, 2.0, 0.5528813123174013, 1.0, 1.0, 0.9601719988513739, 6.9112, 6.9112, 170.5573041426782, 2319425.153788156, 2319425.153788156, 453641.1448204559], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5229000.0000, 
sim time next is 5229600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9521032331263757, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005990752458736, 6.9112, 168.9123931440838, 2227963.612749976, 2160716.014797895, 448905.4386922828], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9422930519594888, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479075245873592, 0.0, 0.8294371787656277, 0.6188787813194379, 0.6001988929994153, 0.6700081174511684], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2039574], dtype=float32), -0.73490536]. 
=============================================
[2019-03-26 01:10:16,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:16,279] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9086
[2019-03-26 01:10:16,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2795686.033003749 W.
[2019-03-26 01:10:16,306] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.65, 49.0, 1.0, 2.0, 0.9994211082458239, 1.0, 2.0, 0.9994211082458239, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2795686.033003749, 2795686.033003749, 528639.0948820362], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5491800.0000, 
sim time next is 5492400.0000, 
raw observation next is [36.76666666666667, 48.0, 1.0, 2.0, 1.007495861523675, 1.0, 2.0, 1.007495861523675, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2818299.063217343, 2818299.063217344, 533533.9740001368], 
processed observation next is [1.0, 0.5652173913043478, 0.9415481832543446, 0.48, 1.0, 1.0, 1.009031158462259, 1.0, 1.0, 1.009031158462259, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7828608508937064, 0.7828608508937067, 0.7963193641793087], 
reward next is 0.2037, 
noisyNet noise sample is [array([-0.6528418], dtype=float32), -0.85154057]. 
=============================================
[2019-03-26 01:10:21,993] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:22,002] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5702
[2019-03-26 01:10:22,006] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.83333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.945291411364861, 6.9112, 168.9125555715127, 852995.916436522, 828810.3532864732, 254811.9736350152], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5516400.0000, 
sim time next is 5517000.0000, 
raw observation next is [29.7, 79.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.957651604091544, 6.9112, 168.9124612868667, 861768.0448121452, 828813.7747452768, 254811.9148177416], 
processed observation next is [1.0, 0.8695652173913043, 0.6066350710900474, 0.795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004645160409154414, 0.0, 0.8294375133778876, 0.2393800124478181, 0.23022604854035467, 0.38031629077274864], 
reward next is 0.3874, 
noisyNet noise sample is [array([0.9311819], dtype=float32), -0.7185513]. 
=============================================
[2019-03-26 01:10:22,025] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[51.860996]
 [51.860558]
 [51.97913 ]
 [51.858326]
 [51.843628]], R is [[51.54459763]
 [51.4783783 ]
 [51.50880432]
 [51.61825943]
 [51.73302078]].
[2019-03-26 01:10:27,137] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:27,149] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7960
[2019-03-26 01:10:27,157] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.927484491037817, 6.911199999999999, 6.9112, 168.912956510431, 757301.8589368007, 757301.8589368014, 229565.6755480697], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5650200.0000, 
sim time next is 5650800.0000, 
raw observation next is [30.16666666666666, 71.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9298575944844938, 6.911199999999999, 6.9112, 168.912956510431, 758688.2026011939, 758688.2026011946, 230108.2073899057], 
processed observation next is [0.0, 0.391304347826087, 0.6287519747235385, 0.7133333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.914460481078651, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2107467229447761, 0.21074672294477628, 0.34344508565657567], 
reward next is 0.6566, 
noisyNet noise sample is [array([-0.6078966], dtype=float32), 0.6975203]. 
=============================================
[2019-03-26 01:10:30,061] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:30,067] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4936
[2019-03-26 01:10:30,074] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.61666666666667, 91.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8424986372668292, 6.9112, 6.9112, 168.912956510431, 701398.7692300805, 701398.7692300805, 210706.9476797289], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5629800.0000, 
sim time next is 5630400.0000, 
raw observation next is [25.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.838584477310783, 6.9112, 6.9112, 168.912956510431, 698525.4422616044, 698525.4422616044, 209869.8495089748], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8031518015985157, 0.0, 0.0, 0.8294399451523027, 0.19403484507266788, 0.19403484507266788, 0.3132385813566788], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.32728928], dtype=float32), -1.5023868]. 
=============================================
[2019-03-26 01:10:30,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0950187e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 01:10:30,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5178
[2019-03-26 01:10:30,144] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 93.0, 1.0, 1.0, 0.3005988659618645, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5220408206117179, 6.911199999999999, 6.9112, 168.9128959162356, 840136.3463672827, 840136.3463672833, 222150.7121427561], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5450400.0000, 
sim time next is 5451000.0000, 
raw observation next is [27.98333333333333, 92.83333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 13.48605217194962, 6.9112, 170.166695422931, 5529485.467482461, 830437.8261036855, 256401.3195811635], 
processed observation next is [1.0, 0.08695652173913043, 0.5252764612954185, 0.9283333333333332, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.657485217194962, 0.0, 0.8355963771768354, 1.5359681854117946, 0.23067717391769044, 0.3826885366883037], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.62333333], dtype=float32), 0.5159651]. 
=============================================
[2019-03-26 01:10:30,167] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[31.477179]
 [31.520046]
 [32.111652]
 [32.68947 ]
 [31.158056]], R is [[25.15692711]
 [24.90535736]
 [24.65630341]
 [25.0772686 ]
 [24.87581062]].
[2019-03-26 01:10:34,899] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:10:34,911] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7610
[2019-03-26 01:10:34,918] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.86666666666667, 66.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9690495411944974, 6.9112, 6.9112, 168.912956510431, 781156.358872527, 781156.358872527, 239210.8462056388], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5511000.0000, 
sim time next is 5511600.0000, 
raw observation next is [31.6, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9775963059176052, 6.9112, 6.9112, 168.912956510431, 787713.9552443086, 787713.9552443086, 241332.2116558686], 
processed observation next is [1.0, 0.8260869565217391, 0.6966824644549764, 0.68, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.972678421850738, 0.0, 0.0, 0.8294399451523027, 0.21880943201230793, 0.21880943201230793, 0.3601973308296546], 
reward next is 0.6398, 
noisyNet noise sample is [array([-0.11660499], dtype=float32), 0.37490788]. 
=============================================
[2019-03-26 01:10:37,294] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 3.8084674e-30 0.0000000e+00 5.6990877e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 01:10:37,303] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0023
[2019-03-26 01:10:37,314] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2404924.019688589 W.
[2019-03-26 01:10:37,319] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.21666666666667, 73.83333333333333, 1.0, 2.0, 0.8598631242705336, 1.0, 2.0, 0.8598631242705336, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2404924.019688589, 2404924.019688589, 450067.7226228701], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5562600.0000, 
sim time next is 5563200.0000, 
raw observation next is [30.43333333333334, 72.66666666666667, 1.0, 2.0, 0.5565362681681553, 1.0, 2.0, 0.5565362681681553, 1.0, 1.0, 0.9665194484517635, 6.9112, 6.9112, 170.5573041426782, 2334772.598952089, 2334772.598952089, 456443.4793418221], 
processed observation next is [1.0, 0.391304347826087, 0.6413902053712484, 0.7266666666666667, 1.0, 1.0, 0.4657063471905485, 1.0, 1.0, 0.4657063471905485, 1.0, 0.5, 0.9591700590875164, 0.0, 0.0, 0.8375144448122397, 0.648547944153358, 0.648547944153358, 0.6812589243907793], 
reward next is 0.3187, 
noisyNet noise sample is [array([0.8932615], dtype=float32), 0.11901655]. 
=============================================
[2019-03-26 01:10:43,188] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 01:10:43,189] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:10:43,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:10:43,191] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:10:43,192] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:10:43,192] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:10:43,192] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:10:43,193] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:10:43,194] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:10:43,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:10:43,197] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:10:43,232] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-26 01:10:43,254] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-26 01:10:43,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-26 01:10:43,298] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-26 01:10:43,299] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-26 01:10:49,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1405164], dtype=float32), -0.021528615]
[2019-03-26 01:10:49,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.06666666666667, 77.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5507241018882522, 6.9112, 6.9112, 168.912956510431, 482884.8895118933, 482884.8895118933, 158835.3088887343]
[2019-03-26 01:10:49,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:10:49,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28803521372972263
[2019-03-26 01:11:10,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1405164], dtype=float32), -0.021528615]
[2019-03-26 01:11:10,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.1, 85.0, 1.0, 2.0, 0.7733287925358211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1156952.56078432, 1156952.560784321, 247672.1549154911]
[2019-03-26 01:11:10,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:11:10,222] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.375512e-22], sampled 0.8036302396549924
[2019-03-26 01:11:10,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1156952.56078432 W.
[2019-03-26 01:12:17,965] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1405164], dtype=float32), -0.021528615]
[2019-03-26 01:12:17,966] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.7, 52.83333333333334, 1.0, 2.0, 0.80605970148488, 1.0, 1.0, 0.80605970148488, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 2254325.445119439, 2254325.445119439, 422559.8814074487]
[2019-03-26 01:12:17,968] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:12:17,971] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7976179699558514
[2019-03-26 01:12:17,973] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2254325.445119439 W.
[2019-03-26 01:12:19,148] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1405164], dtype=float32), -0.021528615]
[2019-03-26 01:12:19,148] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.51666666666667, 80.83333333333333, 1.0, 2.0, 0.6645241175621125, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973068472892, 6.9112, 168.9123160568819, 1825486.641569801, 1758251.619888391, 377140.1157508083]
[2019-03-26 01:12:19,150] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:12:19,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.7147446e-35], sampled 0.3589364078691054
[2019-03-26 01:12:19,154] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1825486.641569801 W.
[2019-03-26 01:12:27,732] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1405164], dtype=float32), -0.021528615]
[2019-03-26 01:12:27,734] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.530828375, 86.19207791166667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.041260353550192, 6.9112, 168.912024886703, 1546086.899596045, 1453818.117358343, 311348.2322860604]
[2019-03-26 01:12:27,735] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:12:27,738] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.595761534722791
[2019-03-26 01:12:27,739] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1546086.899596045 W.
[2019-03-26 01:12:51,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7043.1138 3185177524.4430 2436.0000
[2019-03-26 01:12:52,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8071.7916 2939488597.7078 1356.0000
[2019-03-26 01:12:52,830] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7303.5610 3320558803.5521 2082.0000
[2019-03-26 01:12:52,918] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7351.3811 3106529573.4220 2019.0000
[2019-03-26 01:12:53,039] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7941.5188 2990490179.7632 1505.0000
[2019-03-26 01:12:54,057] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1425000, evaluation results [1425000.0, 7303.560969145158, 3320558803.552095, 2082.0, 7351.381074325899, 3106529573.422031, 2019.0, 8071.791599012506, 2939488597.7078214, 1356.0, 7043.11379642357, 3185177524.443003, 2436.0, 7941.5188350598455, 2990490179.763239, 1505.0]
[2019-03-26 01:12:59,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:12:59,127] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5953
[2019-03-26 01:12:59,134] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.76666666666667, 91.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8598641410989889, 6.9112, 6.9112, 168.912956510431, 714527.4904560291, 714527.4904560291, 214479.641921763], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5719200.0000, 
sim time next is 5719800.0000, 
raw observation next is [25.73333333333333, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.859055861330876, 6.911200000000001, 6.9112, 168.912956510431, 713734.9349794587, 713734.934979458, 214296.6957301406], 
processed observation next is [0.0, 0.17391304347826086, 0.41864139020537117, 0.9166666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8281169040620437, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19825970416096075, 0.19825970416096056, 0.3198458145225979], 
reward next is 0.6802, 
noisyNet noise sample is [array([-0.3806821], dtype=float32), 0.8824779]. 
=============================================
[2019-03-26 01:13:01,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.13702917e-01 1.07007508e-31 3.69851592e-33 1.06518383e-16
 2.86297053e-01], sum to 1.0000
[2019-03-26 01:13:01,765] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6751
[2019-03-26 01:13:01,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 922792.3022613301 W.
[2019-03-26 01:13:01,780] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 94.0, 1.0, 2.0, 0.330160136106124, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5600031935214422, 6.911200000000001, 6.9112, 168.912956510431, 922792.3022613301, 922792.3022613295, 230601.2241135997], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5979600.0000, 
sim time next is 5980200.0000, 
raw observation next is [26.05, 93.33333333333334, 1.0, 2.0, 0.2208126367089679, 1.0, 1.0, 0.2208126367089679, 1.0, 2.0, 0.3765685070069722, 6.9112, 6.9112, 170.5573041426782, 925749.0842245921, 925749.0842245921, 275464.7283193881], 
processed observation next is [1.0, 0.21739130434782608, 0.43364928909952616, 0.9333333333333335, 1.0, 1.0, 0.061220044227672146, 1.0, 0.5, 0.061220044227672146, 1.0, 1.0, 0.2397176914719173, 0.0, 0.0, 0.8375144448122397, 0.25715252339572003, 0.25715252339572003, 0.41114138555132557], 
reward next is 0.5889, 
noisyNet noise sample is [array([0.3777384], dtype=float32), 1.3662533]. 
=============================================
[2019-03-26 01:13:06,351] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:06,361] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0890
[2019-03-26 01:13:06,369] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9223208417361658, 6.9112, 6.9112, 168.912956510431, 755573.8258657443, 755573.8258657443, 228445.8420018067], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6043200.0000, 
sim time next is 6043800.0000, 
raw observation next is [26.95, 89.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9221300056307055, 6.911200000000001, 6.9112, 168.912956510431, 755449.2659855877, 755449.2659855871, 228401.8316023033], 
processed observation next is [1.0, 0.9565217391304348, 0.476303317535545, 0.8966666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9050365922325675, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20984701832932992, 0.20984701832932975, 0.34089825612284075], 
reward next is 0.6591, 
noisyNet noise sample is [array([-0.9665854], dtype=float32), 0.8455031]. 
=============================================
[2019-03-26 01:13:11,286] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:11,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2339
[2019-03-26 01:13:11,304] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.2, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8927000847459456, 6.9112, 6.9112, 168.912956510431, 734411.5238320105, 734411.5238320105, 221644.6962856336], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6121800.0000, 
sim time next is 6122400.0000, 
raw observation next is [27.2, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8923788124811307, 6.911200000000001, 6.9112, 168.912956510431, 734149.8838328961, 734149.8838328955, 221570.8705537354], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8687546493672326, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2039305232869156, 0.20393052328691544, 0.3307027918712469], 
reward next is 0.6693, 
noisyNet noise sample is [array([1.1417837], dtype=float32), 0.5777851]. 
=============================================
[2019-03-26 01:13:13,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8313137e-27 7.3450074e-33 2.4653508e-34 1.8952936e-15 1.0000000e+00], sum to 1.0000
[2019-03-26 01:13:13,063] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0709
[2019-03-26 01:13:13,068] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 77.66666666666667, 1.0, 2.0, 0.5802286517619868, 1.0, 2.0, 0.5802286517619868, 1.0, 2.0, 1.007665283563266, 6.9112, 6.9112, 170.5573041426782, 2434263.339922105, 2434263.339922105, 475058.4018522289], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5995200.0000, 
sim time next is 5995800.0000, 
raw observation next is [30.15, 77.0, 1.0, 2.0, 0.5736642453973844, 1.0, 2.0, 0.5736642453973844, 1.0, 2.0, 0.996265080590309, 6.911200000000001, 6.9112, 170.5573041426782, 2406696.824879337, 2406696.824879337, 469823.6317432459], 
processed observation next is [1.0, 0.391304347826087, 0.6279620853080569, 0.77, 1.0, 1.0, 0.48634246433419803, 1.0, 1.0, 0.48634246433419803, 1.0, 1.0, 0.9954452202320843, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6685268957998158, 0.6685268957998158, 0.7012293011093222], 
reward next is 0.2988, 
noisyNet noise sample is [array([-0.36631814], dtype=float32), -0.8460746]. 
=============================================
[2019-03-26 01:13:19,824] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:19,834] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9792
[2019-03-26 01:13:19,842] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.86666666666667, 62.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8745130685315566, 6.911200000000001, 6.9112, 168.912956510431, 722141.0243194934, 722141.0243194929, 217604.1159265221], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6270000.0000, 
sim time next is 6270600.0000, 
raw observation next is [30.88333333333333, 62.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8702814790296923, 6.911199999999999, 6.9112, 168.912956510431, 719122.070858105, 719122.0708581056, 216669.4402958438], 
processed observation next is [0.0, 0.5652173913043478, 0.6627172195892573, 0.6233333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8418066817435271, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19975613079391805, 0.1997561307939182, 0.3233872243221549], 
reward next is 0.6766, 
noisyNet noise sample is [array([0.14332704], dtype=float32), -1.7982141]. 
=============================================
[2019-03-26 01:13:23,231] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:23,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4408
[2019-03-26 01:13:23,250] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.56666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8613811511554436, 6.9112, 6.9112, 168.912956510431, 713745.3142059045, 713745.3142059045, 214750.927369121], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6279000.0000, 
sim time next is 6279600.0000, 
raw observation next is [30.53333333333333, 63.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.854141290336258, 6.911200000000001, 6.9112, 168.912956510431, 707977.6423860716, 707977.6423860709, 213157.6132540257], 
processed observation next is [0.0, 0.6956521739130435, 0.646129541864139, 0.6300000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8221235248003146, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19666045621835324, 0.19666045621835304, 0.31814569142391896], 
reward next is 0.6819, 
noisyNet noise sample is [array([0.01671745], dtype=float32), 0.45751724]. 
=============================================
[2019-03-26 01:13:31,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.4439617e-31 0.0000000e+00 0.0000000e+00 1.3151376e-37 1.0000000e+00], sum to 1.0000
[2019-03-26 01:13:31,791] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5850
[2019-03-26 01:13:31,800] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.9, 85.0, 1.0, 2.0, 0.3195334457782482, 1.0, 2.0, 0.3195334457782482, 1.0, 2.0, 0.5440915292408568, 6.9112, 6.9112, 170.5573041426782, 1339891.158724605, 1339891.158724605, 311654.8820728725], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6404400.0000, 
sim time next is 6405000.0000, 
raw observation next is [26.86666666666667, 85.16666666666667, 1.0, 2.0, 0.3121695191554316, 1.0, 2.0, 0.3121695191554316, 1.0, 2.0, 0.5314749242325107, 6.9112, 6.9112, 170.5573041426782, 1308993.346766304, 1308993.346766304, 308487.3266208119], 
processed observation next is [1.0, 0.13043478260869565, 0.4723538704581361, 0.8516666666666667, 1.0, 1.0, 0.17128857729570074, 1.0, 1.0, 0.17128857729570074, 1.0, 1.0, 0.4286279563811106, 0.0, 0.0, 0.8375144448122397, 0.36360926299064, 0.36360926299064, 0.4604288457027043], 
reward next is 0.5396, 
noisyNet noise sample is [array([0.00601702], dtype=float32), -0.8368178]. 
=============================================
[2019-03-26 01:13:31,815] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[40.68619 ]
 [40.23645 ]
 [40.067875]
 [40.23254 ]
 [41.05755 ]], R is [[41.13951492]
 [41.26296234]
 [41.3842926 ]
 [41.49359512]
 [41.58218002]].
[2019-03-26 01:13:33,469] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:33,480] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3542
[2019-03-26 01:13:33,486] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.3, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8831654077440599, 6.9112, 6.9112, 168.912956510431, 728424.1880645432, 728424.1880645432, 219533.2766038986], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6366000.0000, 
sim time next is 6366600.0000, 
raw observation next is [29.95, 67.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8788970122800305, 6.9112, 6.9112, 168.912956510431, 725548.1012805, 725548.1012805, 218587.379815971], 
processed observation next is [0.0, 0.6956521739130435, 0.6184834123222749, 0.675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8523134296097932, 0.0, 0.0, 0.8294399451523027, 0.2015411392445833, 0.2015411392445833, 0.3262498206208523], 
reward next is 0.6738, 
noisyNet noise sample is [array([0.6848326], dtype=float32), 0.8003168]. 
=============================================
[2019-03-26 01:13:38,656] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:38,664] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3443
[2019-03-26 01:13:38,669] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.65, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9244235840331463, 6.9112, 6.9112, 168.912956510431, 754842.4443038275, 754842.4443038275, 228836.9534756049], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6262200.0000, 
sim time next is 6262800.0000, 
raw observation next is [30.66666666666666, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9209489992327021, 6.9112, 6.9112, 168.912956510431, 752494.217547921, 752494.217547921, 228033.0289417249], 
processed observation next is [0.0, 0.4782608695652174, 0.6524486571879934, 0.6766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9035963405276853, 0.0, 0.0, 0.8294399451523027, 0.20902617154108916, 0.20902617154108916, 0.34034780439063417], 
reward next is 0.6597, 
noisyNet noise sample is [array([-0.8155578], dtype=float32), -0.15736094]. 
=============================================
[2019-03-26 01:13:39,151] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:39,163] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2198
[2019-03-26 01:13:39,172] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.56666666666667, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9195231856430954, 6.9112, 6.9112, 168.912956510431, 751419.2030989968, 751419.2030989968, 227699.0140419197], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6246600.0000, 
sim time next is 6247200.0000, 
raw observation next is [27.63333333333333, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9209915281205902, 6.9112, 6.9112, 168.912956510431, 752381.075847506, 752381.075847506, 228036.479454877], 
processed observation next is [0.0, 0.30434782608695654, 0.5086887835703, 0.8633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9036482050251098, 0.0, 0.0, 0.8294399451523027, 0.2089947432909739, 0.2089947432909739, 0.3403529544102642], 
reward next is 0.6596, 
noisyNet noise sample is [array([-0.7825166], dtype=float32), -1.9024869]. 
=============================================
[2019-03-26 01:13:39,691] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:39,702] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3847
[2019-03-26 01:13:39,711] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.43333333333333, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8748149158594757, 6.9112, 6.9112, 168.912956510431, 723873.1352450701, 723873.1352450701, 217724.7152373997], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6372600.0000, 
sim time next is 6373200.0000, 
raw observation next is [28.36666666666667, 76.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782913909482543, 6.911199999999999, 6.9112, 168.912956510431, 726355.9391702501, 726355.9391702507, 218497.0700301032], 
processed observation next is [0.0, 0.782608695652174, 0.543443917851501, 0.7633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.851574867010066, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017655386584028, 0.20176553865840297, 0.3261150298956764], 
reward next is 0.6739, 
noisyNet noise sample is [array([-3.1690907], dtype=float32), 0.57959235]. 
=============================================
[2019-03-26 01:13:43,724] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 3.0559760e-24 1.1921836e-27 3.1537360e-12 2.9889218e-24], sum to 1.0000
[2019-03-26 01:13:43,733] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5068
[2019-03-26 01:13:43,747] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1040706.911510356 W.
[2019-03-26 01:13:43,757] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.744654823655234, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1040706.911510356, 1040706.911510356, 230778.741063467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6503400.0000, 
sim time next is 6504000.0000, 
raw observation next is [26.7, 89.66666666666666, 1.0, 2.0, 0.7288014880123371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1018540.146896946, 1018540.146896947, 227184.4123480727], 
processed observation next is [1.0, 0.2608695652173913, 0.46445497630331756, 0.8966666666666666, 1.0, 1.0, 0.6732548048341411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.282927818582485, 0.2829278185824853, 0.33908121245981004], 
reward next is 0.6609, 
noisyNet noise sample is [array([0.31393012], dtype=float32), -1.1963158]. 
=============================================
[2019-03-26 01:13:43,769] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[37.29616 ]
 [37.170395]
 [36.840187]
 [36.83771 ]
 [36.143288]], R is [[37.89570618]
 [38.17230606]
 [38.42029953]
 [38.60186386]
 [38.21584702]].
[2019-03-26 01:13:45,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4723300e-26 0.0000000e+00 0.0000000e+00 1.0012413e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 01:13:45,222] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3066
[2019-03-26 01:13:45,227] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 92.0, 1.0, 2.0, 0.2189669109508654, 1.0, 2.0, 0.2189669109508654, 1.0, 2.0, 0.371921572056201, 6.9112, 6.9112, 170.5573041426782, 918007.6329125246, 918007.6329125246, 274814.1883542929], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6580800.0000, 
sim time next is 6581400.0000, 
raw observation next is [25.86666666666667, 92.16666666666667, 1.0, 2.0, 0.2010649931579533, 1.0, 2.0, 0.2010649931579533, 1.0, 2.0, 0.3413509346653482, 6.911200000000001, 6.9112, 170.5573041426782, 842925.25793573, 842925.2579357293, 269711.1327798967], 
processed observation next is [1.0, 0.17391304347826086, 0.42496050552922615, 0.9216666666666667, 1.0, 1.0, 0.037427702599943706, 1.0, 1.0, 0.037427702599943706, 1.0, 1.0, 0.19676943251871734, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.23414590498214724, 0.23414590498214705, 0.4025539295222339], 
reward next is 0.5974, 
noisyNet noise sample is [array([0.05533855], dtype=float32), -0.63624495]. 
=============================================
[2019-03-26 01:13:49,616] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:49,631] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5059
[2019-03-26 01:13:49,636] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.73333333333334, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8638473111858586, 6.9112, 6.9112, 168.912956510431, 714970.2389922425, 714970.2389922425, 215271.9152553863], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6643200.0000, 
sim time next is 6643800.0000, 
raw observation next is [26.7, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8649798793418048, 6.9112, 6.9112, 168.912956510431, 716001.5143104362, 716001.5143104362, 215527.5235396067], 
processed observation next is [1.0, 0.9130434782608695, 0.46445497630331756, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8353413162704935, 0.0, 0.0, 0.8294399451523027, 0.1988893095306767, 0.1988893095306767, 0.3216828709546368], 
reward next is 0.6783, 
noisyNet noise sample is [array([1.8198282], dtype=float32), 1.0586369]. 
=============================================
[2019-03-26 01:13:53,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2856307e-10 1.5840589e-32 0.0000000e+00 1.0000000e+00 9.0109412e-32], sum to 1.0000
[2019-03-26 01:13:53,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8743
[2019-03-26 01:13:53,200] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.7, 75.0, 1.0, 2.0, 0.6227399241172046, 1.0, 2.0, 0.6227399241172046, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1741204.973462928, 1741204.973462928, 342798.9798554039], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6690600.0000, 
sim time next is 6691200.0000, 
raw observation next is [28.86666666666667, 74.0, 1.0, 2.0, 0.6312753681936195, 1.0, 2.0, 0.6312753681936195, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1765090.061769981, 1765090.061769981, 346086.9686484142], 
processed observation next is [1.0, 0.43478260869565216, 0.567140600315956, 0.74, 1.0, 1.0, 0.5557534556549633, 1.0, 1.0, 0.5557534556549633, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4903027949361058, 0.4903027949361058, 0.5165477144006182], 
reward next is 0.4835, 
noisyNet noise sample is [array([0.8130083], dtype=float32), 2.0584705]. 
=============================================
[2019-03-26 01:13:59,487] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:13:59,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5823
[2019-03-26 01:13:59,508] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.46666666666667, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6454556727977668, 6.9112, 6.9112, 168.912956510431, 558275.86428137, 558275.86428137, 173327.3971759119], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6733200.0000, 
sim time next is 6733800.0000, 
raw observation next is [25.35, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6429688991822017, 6.911199999999999, 6.9112, 168.912956510431, 556491.2114429169, 556491.2114429175, 172915.2456829403], 
processed observation next is [1.0, 0.9565217391304348, 0.4004739336492892, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5645962185148801, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1545808920674769, 0.15458089206747708, 0.2580824562431945], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.06315756], dtype=float32), 0.22175688]. 
=============================================
[2019-03-26 01:14:01,831] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 01:14:01,832] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:14:01,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:14:01,833] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:14:01,835] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:14:01,835] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:14:01,836] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:14:01,836] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:14:01,838] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:14:01,838] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:14:01,838] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:14:01,869] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-26 01:14:01,894] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-26 01:14:01,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-26 01:14:01,896] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-26 01:14:01,945] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-26 01:14:25,077] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:14:25,078] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.23126661666667, 95.83035260833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4327023312308603, 6.9112, 6.9112, 168.912956510431, 389807.0137124317, 389807.0137124317, 143789.2053704641]
[2019-03-26 01:14:25,078] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:14:25,081] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.988042533377134
[2019-03-26 01:14:37,456] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:14:37,457] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7256566360549073, 6.911200000000001, 6.9112, 168.912956510431, 618870.1804892571, 618870.1804892565, 187411.4638328748]
[2019-03-26 01:14:37,458] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:14:37,460] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7546639172274742
[2019-03-26 01:14:49,841] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:14:49,844] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.722441015, 85.75442015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6127373396705116, 6.911200000000001, 6.9112, 168.912956510431, 534463.7729038822, 534463.7729038816, 168020.5778664685]
[2019-03-26 01:14:49,845] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:14:49,849] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2312999381992631
[2019-03-26 01:15:16,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:15:16,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.98096715833333, 64.43820436666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.003429593673704, 6.911200000000001, 6.9112, 168.9129118851413, 808835.6340530155, 808835.6340530149, 247936.2359482666]
[2019-03-26 01:15:16,354] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:15:16,356] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.0047878e-20], sampled 0.643321438058046
[2019-03-26 01:15:51,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:15:51,276] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.10853824166667, 85.01154207166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.763679943777499, 6.9112, 6.9112, 168.912956510431, 648686.6599891876, 648686.6599891876, 194668.1056351424]
[2019-03-26 01:15:51,278] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:15:51,280] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8313078037669309
[2019-03-26 01:15:57,957] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0354593], dtype=float32), 0.025206305]
[2019-03-26 01:15:57,957] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.10703147, 81.28393645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7104146602221757, 6.911199999999999, 6.9112, 168.912956510431, 609435.9680131993, 609435.9680132, 184613.2427314332]
[2019-03-26 01:15:57,959] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:15:57,960] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8847001984978243
[2019-03-26 01:16:10,776] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8055.2157 3054438692.0357 1007.0000
[2019-03-26 01:16:11,418] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7383.6410 3248921243.4250 1564.0000
[2019-03-26 01:16:11,423] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7611.4906 3177404646.2750 1420.0000
[2019-03-26 01:16:11,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7475.7950 3371241351.9581 1491.0000
[2019-03-26 01:16:11,654] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8163.1492 3011246458.7509 868.0000
[2019-03-26 01:16:12,671] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1450000, evaluation results [1450000.0, 7475.79501974199, 3371241351.9580593, 1491.0, 7611.490557213732, 3177404646.274992, 1420.0, 8163.1492061349045, 3011246458.750944, 868.0, 7383.64103071775, 3248921243.424982, 1564.0, 8055.215730377959, 3054438692.035722, 1007.0]
[2019-03-26 01:16:13,608] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:13,622] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3368
[2019-03-26 01:16:13,628] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.73333333333333, 76.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5995758359824853, 6.911200000000001, 6.9112, 168.912956510431, 523223.6843403783, 523223.6843403777, 165991.8550477106], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6828000.0000, 
sim time next is 6828600.0000, 
raw observation next is [23.66666666666667, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5968978524298731, 6.9112, 6.9112, 168.912956510431, 520974.8087419514, 520974.8087419514, 165584.0060024429], 
processed observation next is [0.0, 0.0, 0.3206951026856243, 0.7666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5084120151583819, 0.0, 0.0, 0.8294399451523027, 0.14471522465054207, 0.14471522465054207, 0.2471403074663327], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.21838453], dtype=float32), -1.4576602]. 
=============================================
[2019-03-26 01:16:18,339] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:18,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2784
[2019-03-26 01:16:18,354] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.43333333333333, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8300976793835552, 6.9112, 6.9112, 168.912956510431, 691494.9054366194, 691494.9054366194, 208045.9770103851], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6657000.0000, 
sim time next is 6657600.0000, 
raw observation next is [25.36666666666667, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8298046780017991, 6.9112, 6.9112, 168.912956510431, 691455.1192242103, 691455.1192242103, 207989.0563729081], 
processed observation next is [1.0, 0.043478260869565216, 0.40126382306477115, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7924447292704868, 0.0, 0.0, 0.8294399451523027, 0.19207086645116953, 0.19207086645116953, 0.3104314274222509], 
reward next is 0.6896, 
noisyNet noise sample is [array([-0.00752778], dtype=float32), 2.03671]. 
=============================================
[2019-03-26 01:16:21,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.685403e-27 0.000000e+00], sum to 1.0000
[2019-03-26 01:16:21,893] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5197
[2019-03-26 01:16:21,904] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1834478.78894483 W.
[2019-03-26 01:16:21,909] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.73333333333334, 65.66666666666667, 1.0, 2.0, 0.4373804424903086, 1.0, 1.0, 0.4373804424903086, 1.0, 2.0, 0.7424360964192812, 6.9112, 6.9112, 170.5573041426782, 1834478.78894483, 1834478.78894483, 371843.659328169], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6711600.0000, 
sim time next is 6712200.0000, 
raw observation next is [29.7, 66.0, 1.0, 2.0, 0.4188345612235362, 1.0, 2.0, 0.4188345612235362, 1.0, 2.0, 0.7117650485445036, 6.911199999999999, 6.9112, 170.5573041426782, 1756629.141263082, 1756629.141263083, 361222.517965207], 
processed observation next is [1.0, 0.6956521739130435, 0.6066350710900474, 0.66, 1.0, 1.0, 0.29980067617293515, 1.0, 1.0, 0.29980067617293515, 1.0, 1.0, 0.6484939616396385, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.48795253923974496, 0.4879525392397453, 0.5391380865152343], 
reward next is 0.4609, 
noisyNet noise sample is [array([0.5319853], dtype=float32), 3.1982958]. 
=============================================
[2019-03-26 01:16:25,004] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:25,016] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7384
[2019-03-26 01:16:25,020] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666666, 53.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5878184679435654, 6.911199999999999, 6.9112, 168.912956510431, 513037.9967472589, 513037.9967472595, 164222.4291985029], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6806400.0000, 
sim time next is 6807000.0000, 
raw observation next is [27.53333333333333, 53.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5851575727705032, 6.9112, 6.9112, 168.912956510431, 511002.1262415127, 511002.1262415127, 163821.216225199], 
processed observation next is [1.0, 0.782608695652174, 0.5039494470774091, 0.5366666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.494094600939638, 0.0, 0.0, 0.8294399451523027, 0.14194503506708686, 0.14194503506708686, 0.2445092779480582], 
reward next is 0.7555, 
noisyNet noise sample is [array([0.50607663], dtype=float32), 0.28321186]. 
=============================================
[2019-03-26 01:16:25,034] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.86292]
 [72.64331]
 [71.5537 ]
 [71.00894]
 [69.78185]], R is [[73.35798645]
 [73.37929535]
 [73.40000153]
 [73.42037201]
 [73.44143677]].
[2019-03-26 01:16:29,521] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.9562515e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 01:16:29,528] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0409
[2019-03-26 01:16:29,538] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2136498.780138339 W.
[2019-03-26 01:16:29,547] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 53.5, 1.0, 2.0, 0.5093169791887078, 1.0, 2.0, 0.5093169791887078, 1.0, 2.0, 0.8511673092772214, 6.9112, 6.9112, 170.5573041426782, 2136498.780138339, 2136498.780138339, 415307.4036046755], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7053000.0000, 
sim time next is 7053600.0000, 
raw observation next is [30.5, 55.0, 1.0, 2.0, 0.7045490312764366, 1.0, 2.0, 0.7045490312764366, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1970156.709439431, 1970156.709439432, 376061.7428685706], 
processed observation next is [1.0, 0.6521739130434783, 0.6445497630331753, 0.55, 1.0, 1.0, 0.64403497744149, 1.0, 1.0, 0.64403497744149, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5472657526220642, 0.5472657526220644, 0.5612861833859263], 
reward next is 0.4387, 
noisyNet noise sample is [array([-0.31481695], dtype=float32), -1.088964]. 
=============================================
[2019-03-26 01:16:42,614] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:42,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8110
[2019-03-26 01:16:42,629] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.45, 84.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.015035792287108, 6.9112, 6.9112, 168.9128045641516, 857640.3581329661, 857640.3581329661, 252526.9468161336], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7018200.0000, 
sim time next is 7018800.0000, 
raw observation next is [25.6, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9682839058569708, 6.9112, 6.9112, 168.912956510431, 818408.2482249424, 818408.2482249424, 240469.409999573], 
processed observation next is [1.0, 0.21739130434782608, 0.4123222748815167, 0.8333333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9613218364109399, 0.0, 0.0, 0.8294399451523027, 0.22733562450692843, 0.22733562450692843, 0.3589095671635418], 
reward next is 0.6411, 
noisyNet noise sample is [array([0.9990832], dtype=float32), 0.90160114]. 
=============================================
[2019-03-26 01:16:54,380] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 3.6113053e-22 1.6426089e-35 4.9638782e-09 1.4669581e-36], sum to 1.0000
[2019-03-26 01:16:54,388] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1746
[2019-03-26 01:16:54,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1652833.791043068 W.
[2019-03-26 01:16:54,400] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.36666666666667, 83.16666666666667, 1.0, 2.0, 0.5911584292030552, 1.0, 1.0, 0.5911584292030552, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1652833.791043068, 1652833.791043068, 330998.8905080297], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7139400.0000, 
sim time next is 7140000.0000, 
raw observation next is [26.33333333333334, 83.33333333333334, 1.0, 2.0, 0.3947743826643749, 1.0, 2.0, 0.3947743826643749, 1.0, 1.0, 0.6657912655937258, 6.911200000000001, 6.9112, 170.5573041426782, 1655640.673769457, 1655640.673769456, 347224.8599699246], 
processed observation next is [1.0, 0.6521739130434783, 0.44707740916271754, 0.8333333333333335, 1.0, 1.0, 0.27081250923418665, 1.0, 1.0, 0.27081250923418665, 1.0, 0.5, 0.5924283726752754, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4599001871581825, 0.45990018715818226, 0.5182460596566039], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0895122], dtype=float32), 1.1135459]. 
=============================================
[2019-03-26 01:16:54,415] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[59.84176 ]
 [59.103127]
 [60.245842]
 [60.384556]
 [61.074554]], R is [[59.07922745]
 [58.48843384]
 [57.90354919]
 [57.32451248]
 [56.91615677]].
[2019-03-26 01:16:57,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:57,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9921
[2019-03-26 01:16:57,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1648674.549999012 W.
[2019-03-26 01:16:57,815] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.8, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 8.019001594238093, 6.9112, 168.9071114758301, 1648674.549999012, 862789.1670404936, 256329.7889211143], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7179000.0000, 
sim time next is 7179600.0000, 
raw observation next is [25.8, 87.66666666666667, 1.0, 1.0, 0.4347844969660495, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7275433468767604, 6.911199999999999, 6.9112, 168.9120544872487, 1215383.066925576, 1215383.066925577, 270802.2818727567], 
processed observation next is [1.0, 0.08695652173913043, 0.42180094786729866, 0.8766666666666667, 1.0, 0.5, 0.31901746622415594, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6677357888740979, -8.881784197001253e-17, 0.0, 0.8294355158055295, 0.33760640747932663, 0.3376064074793269, 0.40418251025784585], 
reward next is 0.5958, 
noisyNet noise sample is [array([-0.78655237], dtype=float32), -1.7159802]. 
=============================================
[2019-03-26 01:16:57,961] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:57,968] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9718
[2019-03-26 01:16:57,976] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5794456574662594, 6.9112, 6.9112, 168.912956510431, 506162.5699249672, 506162.5699249672, 162976.7758604276], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7446000.0000, 
sim time next is 7446600.0000, 
raw observation next is [21.25, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5798231551391797, 6.9112, 6.9112, 168.912956510431, 506466.2105589872, 506466.2105589872, 163032.6845265364], 
processed observation next is [0.0, 0.17391304347826086, 0.20616113744075834, 0.945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48758921358436547, 0.0, 0.0, 0.8294399451523027, 0.14068505848860757, 0.14068505848860757, 0.2433323649649797], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.13923456], dtype=float32), 0.6823621]. 
=============================================
[2019-03-26 01:16:58,653] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:16:58,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4906
[2019-03-26 01:16:58,670] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.45, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6266532057038385, 6.9112, 6.9112, 168.912956510431, 548833.0656638375, 548833.0656638375, 170156.0479439969], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7276200.0000, 
sim time next is 7276800.0000, 
raw observation next is [21.5, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.583366483388225, 6.911199999999999, 6.9112, 168.912956510431, 510886.6264841023, 510886.6264841029, 163520.9515730287], 
processed observation next is [1.0, 0.21739130434782608, 0.21800947867298584, 0.9033333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49191034559539637, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14191295180113953, 0.1419129518011397, 0.2440611217507891], 
reward next is 0.7559, 
noisyNet noise sample is [array([1.1132671], dtype=float32), 0.10877313]. 
=============================================
[2019-03-26 01:17:05,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:17:05,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8210
[2019-03-26 01:17:05,504] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.66666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5153455351765596, 6.9112, 6.9112, 168.912956510431, 456471.5194258331, 456471.5194258331, 153933.1662004066], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7407600.0000, 
sim time next is 7408200.0000, 
raw observation next is [20.73333333333333, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5147694783788467, 6.911200000000001, 6.9112, 168.912956510431, 455956.5888777226, 455956.588877722, 153859.3100472261], 
processed observation next is [1.0, 0.7391304347826086, 0.18167456556082143, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4082554614376179, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12665460802158962, 0.12665460802158943, 0.2296407612645166], 
reward next is 0.7704, 
noisyNet noise sample is [array([-0.35499093], dtype=float32), -1.9173036]. 
=============================================
[2019-03-26 01:17:08,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.8560112e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 01:17:08,238] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8960
[2019-03-26 01:17:08,244] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.6, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6993234325511963, 6.911199999999999, 6.9112, 168.912956510431, 596810.241652311, 596810.2416523115, 182602.9195275543], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7515000.0000, 
sim time next is 7515600.0000, 
raw observation next is [23.6, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7004253671267773, 6.9112, 6.9112, 168.912956510431, 597597.7098401077, 597597.7098401077, 182799.7163286796], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6346650818619235, 0.0, 0.0, 0.8294399451523027, 0.16599936384447436, 0.16599936384447436, 0.27283539750549196], 
reward next is 0.7272, 
noisyNet noise sample is [array([-0.18648463], dtype=float32), 0.1619133]. 
=============================================
[2019-03-26 01:17:11,846] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.4412278e-22 9.4026888e-22 6.5660426e-28 9.9870956e-01 1.2904253e-03], sum to 1.0000
[2019-03-26 01:17:11,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2230
[2019-03-26 01:17:11,860] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.06666666666667, 66.66666666666667, 1.0, 2.0, 0.5019030583300116, 1.0, 2.0, 0.5019030583300116, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1403119.246326981, 1403119.246326981, 300834.9997949194], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7641600.0000, 
sim time next is 7642200.0000, 
raw observation next is [29.33333333333334, 65.33333333333333, 1.0, 2.0, 0.5076052973695879, 1.0, 2.0, 0.5076052973695879, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1419070.967156617, 1419070.967156617, 302622.2608731348], 
processed observation next is [1.0, 0.43478260869565216, 0.5892575039494474, 0.6533333333333333, 1.0, 1.0, 0.4067533703248047, 1.0, 1.0, 0.4067533703248047, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.39418637976572696, 0.39418637976572696, 0.4516750162285594], 
reward next is 0.5483, 
noisyNet noise sample is [array([0.5938649], dtype=float32), -0.49772763]. 
=============================================
[2019-03-26 01:17:20,419] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 01:17:20,421] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:17:20,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:17:20,422] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:17:20,422] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:17:20,425] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:17:20,426] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:17:20,427] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:17:20,428] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:17:20,429] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:17:20,429] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:17:20,463] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-26 01:17:20,486] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-26 01:17:20,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-26 01:17:20,509] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-26 01:17:20,532] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-26 01:17:54,755] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:17:54,756] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.45939800666667, 94.92928153333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7299462273230706, 6.9112, 6.9112, 168.912956510431, 622286.2472919745, 622286.2472919745, 188211.5100901575]
[2019-03-26 01:17:54,758] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:17:54,761] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28036805056932823
[2019-03-26 01:17:59,981] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:17:59,981] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.0, 65.0, 1.0, 2.0, 0.2307100477835821, 1.0, 2.0, 0.2307100477835821, 0.0, 1.0, 0.0, 6.9112, 6.9112, 172.86236624, 706008.7935435973, 706008.7935435973, 247292.8338666322]
[2019-03-26 01:17:59,983] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:17:59,992] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.3414661e-08 5.8757103e-26 2.0162046e-25 9.9989367e-01 1.0636698e-04], sampled 0.6655239222594816
[2019-03-26 01:18:32,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:18:32,151] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.51516327, 63.41844492000001, 1.0, 2.0, 0.5942890357454591, 1.0, 2.0, 0.5942890357454591, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1661576.336706446, 1661576.336706446, 332664.1424667966]
[2019-03-26 01:18:32,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:18:32,154] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1184655e-07 0.0000000e+00 0.0000000e+00 9.9999988e-01 0.0000000e+00], sampled 0.533801943821564
[2019-03-26 01:18:59,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:18:59,954] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.9, 81.66666666666666, 1.0, 2.0, 0.5707443409147718, 1.0, 2.0, 0.5707443409147718, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1595725.728784118, 1595725.728784118, 323415.1898229728]
[2019-03-26 01:18:59,955] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:18:59,961] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0742007e-08 3.7617357e-34 0.0000000e+00 1.0000000e+00 0.0000000e+00], sampled 0.2617870873300965
[2019-03-26 01:19:13,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:19:13,059] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.0, 56.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9277264846680835, 6.911200000000001, 6.9112, 168.912956510431, 757791.3706966676, 757791.370696667, 229636.7387853313]
[2019-03-26 01:19:13,060] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:19:13,062] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9478888894320036
[2019-03-26 01:19:13,614] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.96474355], dtype=float32), 0.058500264]
[2019-03-26 01:19:13,615] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.31869497333334, 81.42167599000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6667049713345112, 6.911200000000001, 6.9112, 168.912956510431, 576945.8793573247, 576945.8793573241, 176880.7576224076]
[2019-03-26 01:19:13,616] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:19:13,619] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.09110509547015921
[2019-03-26 01:19:29,520] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8107.5198 3153242052.4361 685.0000
[2019-03-26 01:19:29,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7918.5062 3334687659.8633 663.0000
[2019-03-26 01:19:29,964] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8420.3321 3007887428.4610 504.0000
[2019-03-26 01:19:30,377] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8011.6217 3205033882.1338 563.0000
[2019-03-26 01:19:30,471] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8337.2528 3044830810.1397 482.0000
[2019-03-26 01:19:31,487] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1475000, evaluation results [1475000.0, 7918.506194026688, 3334687659.8632913, 663.0, 8107.519788955703, 3153242052.436148, 685.0, 8420.332130119326, 3007887428.4609637, 504.0, 8011.621651968827, 3205033882.133804, 563.0, 8337.252752750273, 3044830810.139743, 482.0]
[2019-03-26 01:19:36,571] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0196081e-12 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 01:19:36,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3990
[2019-03-26 01:19:36,588] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.4, 85.66666666666667, 1.0, 2.0, 0.2486594512920066, 1.0, 2.0, 0.2486594512920066, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 694922.4820338206, 694922.4820338206, 240724.6459814709], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7782000.0000, 
sim time next is 7782600.0000, 
raw observation next is [26.4, 85.33333333333334, 1.0, 2.0, 0.2477769821713893, 1.0, 2.0, 0.2477769821713893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 692455.4711216894, 692455.4711216894, 240580.7296099519], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8533333333333334, 1.0, 1.0, 0.09370720743540878, 1.0, 1.0, 0.09370720743540878, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19234874197824706, 0.19234874197824706, 0.3590757158357491], 
reward next is 0.6409, 
noisyNet noise sample is [array([0.6378541], dtype=float32), -0.41073456]. 
=============================================
[2019-03-26 01:19:43,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:43,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:43,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-26 01:19:45,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:45,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:45,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:45,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:45,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-26 01:19:46,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-26 01:19:46,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:46,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:46,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-26 01:19:48,387] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:48,387] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:48,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-26 01:19:49,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:49,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:49,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-26 01:19:50,327] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:50,328] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:50,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-26 01:19:53,979] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:53,980] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:54,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-26 01:19:56,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:56,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:56,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-26 01:19:57,491] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.644828e-31 0.000000e+00], sum to 1.0000
[2019-03-26 01:19:57,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1555
[2019-03-26 01:19:57,504] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.41666666666667, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.946227126784769, 6.9112, 6.9112, 168.912956510431, 822520.922189616, 822520.922189616, 234973.2250580736], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 94200.0000, 
sim time next is 94800.0000, 
raw observation next is [22.43333333333333, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7834974953484926, 6.911200000000001, 6.9112, 168.912956510431, 680774.4790538431, 680774.4790538425, 198569.3748347278], 
processed observation next is [1.0, 0.08695652173913043, 0.2622432859399683, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7359725553030397, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18910402195940085, 0.18910402195940068, 0.2963722012458624], 
reward next is 0.7036, 
noisyNet noise sample is [array([1.4030689], dtype=float32), 1.0085092]. 
=============================================
[2019-03-26 01:19:59,392] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:59,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:59,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-26 01:19:59,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:19:59,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:19:59,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-26 01:20:00,321] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 6.865096e-20 0.000000e+00], sum to 1.0000
[2019-03-26 01:20:00,327] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2552
[2019-03-26 01:20:00,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1153640.810216267 W.
[2019-03-26 01:20:00,336] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.56666666666667, 96.0, 1.0, 2.0, 0.3911867542959215, 1.0, 2.0, 0.3911867542959215, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1153640.810216267, 1153640.810216267, 277146.5612148031], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 145200.0000, 
sim time next is 145800.0000, 
raw observation next is [22.55, 96.0, 1.0, 2.0, 0.380072976173447, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6610368517020877, 6.911200000000001, 6.9112, 168.912956510431, 1133101.777106109, 1133101.777106108, 256368.7132405914], 
processed observation next is [1.0, 0.6956521739130435, 0.26777251184834133, 0.96, 1.0, 1.0, 0.25309997129330963, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.5866303069537655, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3147504936405858, 0.31475049364058555, 0.38263987050834536], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.01989883], dtype=float32), 0.37287357]. 
=============================================
[2019-03-26 01:20:00,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:20:00,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:00,394] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-26 01:20:00,472] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:20:00,473] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:00,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-26 01:20:01,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:20:01,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:01,726] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-26 01:20:01,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:20:01,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:01,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-26 01:20:01,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:20:01,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:01,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-26 01:20:02,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5482274e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 01:20:02,655] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2984
[2019-03-26 01:20:02,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1878416.295210552 W.
[2019-03-26 01:20:02,661] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.83333333333334, 63.0, 1.0, 2.0, 0.6488514901073698, 1.0, 2.0, 0.6488514901073698, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1878416.295210552, 1878416.295210552, 360774.9527575837], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 51600.0000, 
sim time next is 52200.0000, 
raw observation next is [27.75, 63.5, 1.0, 2.0, 0.6458502789318628, 1.0, 2.0, 0.6458502789318628, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1868891.257933783, 1868891.257933783, 359440.9906187113], 
processed observation next is [1.0, 0.6086956521739131, 0.514218009478673, 0.635, 1.0, 1.0, 0.5733135890745334, 1.0, 1.0, 0.5733135890745334, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.519136460537162, 0.519136460537162, 0.5364790904756884], 
reward next is 0.4635, 
noisyNet noise sample is [array([-0.06135472], dtype=float32), 0.6362231]. 
=============================================
[2019-03-26 01:20:03,679] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:20:03,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4872
[2019-03-26 01:20:03,694] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.03333333333333, 85.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.629743208648792, 6.9112, 6.9112, 168.912956510431, 557446.6730675341, 557446.6730675341, 170429.5429129713], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 11400.0000, 
sim time next is 12000.0000, 
raw observation next is [21.06666666666667, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5829752358509589, 6.9112, 6.9112, 168.912956510431, 515866.1548263548, 515866.1548263548, 163273.558051802], 
processed observation next is [1.0, 0.13043478260869565, 0.19747235387045833, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49143321445238886, 0.0, 0.0, 0.8294399451523027, 0.14329615411843188, 0.14329615411843188, 0.24369187768925674], 
reward next is 0.7563, 
noisyNet noise sample is [array([-0.17114872], dtype=float32), 0.36535743]. 
=============================================
[2019-03-26 01:20:03,710] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.264244]
 [69.4666  ]
 [70.23729 ]
 [71.09497 ]
 [71.0146  ]], R is [[67.58181763]
 [67.65162659]
 [67.74141693]
 [67.83181   ]
 [67.92022705]].
[2019-03-26 01:20:13,901] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:20:13,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9391
[2019-03-26 01:20:13,919] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4399785946774332, 6.9112, 6.9112, 168.912956510431, 393702.6486954252, 393702.6486954252, 144776.2989883266], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 435600.0000, 
sim time next is 436200.0000, 
raw observation next is [19.6, 85.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.440536644187894, 6.9112, 6.9112, 168.912956510431, 394263.9203123889, 394263.9203123889, 144832.2314511939], 
processed observation next is [1.0, 0.043478260869565216, 0.127962085308057, 0.8500000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3177276148632854, 0.0, 0.0, 0.8294399451523027, 0.10951775564233025, 0.10951775564233025, 0.21616750962864759], 
reward next is 0.7838, 
noisyNet noise sample is [array([-1.4361459], dtype=float32), -0.7472064]. 
=============================================
[2019-03-26 01:20:16,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:20:16,251] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1599
[2019-03-26 01:20:16,258] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.21666666666667, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5532731379642186, 6.9112, 6.9112, 168.912956510431, 485981.2259200464, 485981.2259200464, 159164.0309180335], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 215400.0000, 
sim time next is 216000.0000, 
raw observation next is [21.3, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5573130282934191, 6.911200000000001, 6.9112, 168.912956510431, 489334.1027988617, 489334.102798861, 159734.7795327555], 
processed observation next is [0.0, 0.5217391304347826, 0.2085308056872039, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4601378393822184, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13592613966635048, 0.13592613966635028, 0.23841011870560522], 
reward next is 0.7616, 
noisyNet noise sample is [array([0.15506698], dtype=float32), 1.0238271]. 
=============================================
[2019-03-26 01:20:16,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.98588 ]
 [78.00048 ]
 [77.98542 ]
 [77.97781 ]
 [77.976265]], R is [[78.06764984]
 [78.04941559]
 [78.03181458]
 [78.01428223]
 [77.99713898]].
[2019-03-26 01:20:30,243] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.6983763e-26], sum to 1.0000
[2019-03-26 01:20:30,253] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5278
[2019-03-26 01:20:30,258] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.28333333333333, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4855557204083724, 6.911200000000001, 6.9112, 168.912956510431, 433725.1113663855, 433725.1113663848, 150020.7242485712], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 719400.0000, 
sim time next is 720000.0000, 
raw observation next is [21.5, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4680583697993959, 6.911200000000001, 6.9112, 168.912956510431, 417782.1902334738, 417782.1902334731, 147979.1956923273], 
processed observation next is [1.0, 0.34782608695652173, 0.21800947867298584, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.35129069487731207, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11605060839818716, 0.11605060839818697, 0.22086447118257807], 
reward next is 0.7791, 
noisyNet noise sample is [array([-0.13474719], dtype=float32), -1.1644408]. 
=============================================
[2019-03-26 01:20:30,273] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[73.288216]
 [73.45738 ]
 [73.59228 ]
 [73.6226  ]
 [73.59004 ]], R is [[73.50775146]
 [73.54875946]
 [73.59243011]
 [73.63928986]
 [73.6867981 ]].
[2019-03-26 01:20:33,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:20:33,279] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4772
[2019-03-26 01:20:33,289] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.35, 57.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5648124305558458, 6.9112, 6.9112, 168.912956510431, 502920.417909641, 502920.417909641, 160487.5366137702], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 478200.0000, 
sim time next is 478800.0000, 
raw observation next is [24.5, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6677186696043199, 6.9112, 6.9112, 168.912956510431, 594544.1725442652, 594544.1725442652, 176485.1827746564], 
processed observation next is [1.0, 0.5652173913043478, 0.3601895734597157, 0.57, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5947788653711218, 0.0, 0.0, 0.8294399451523027, 0.16515115904007366, 0.16515115904007366, 0.2634107205591886], 
reward next is 0.7366, 
noisyNet noise sample is [array([-0.4593457], dtype=float32), 1.4547303]. 
=============================================
[2019-03-26 01:20:34,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.5646052e-15 0.0000000e+00 0.0000000e+00 4.3765195e-22 1.0000000e+00], sum to 1.0000
[2019-03-26 01:20:34,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2538
[2019-03-26 01:20:34,426] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.13333333333333, 56.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 403581.1447705969, 403581.1447705963, 212774.3135931684], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 494400.0000, 
sim time next is 495000.0000, 
raw observation next is [23.9, 57.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 399818.4032478103, 399818.4032478103, 212063.1481672926], 
processed observation next is [1.0, 0.7391304347826086, 0.33175355450236965, 0.575, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.1110606675688362, 0.1110606675688362, 0.3165121614437203], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10063003], dtype=float32), 1.4543977]. 
=============================================
[2019-03-26 01:20:34,447] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[72.38577 ]
 [71.211   ]
 [69.709724]
 [69.32347 ]
 [68.90086 ]], R is [[72.60864258]
 [71.8825531 ]
 [71.16372681]
 [71.03375244]
 [70.9069519 ]].
[2019-03-26 01:20:36,308] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 01:20:36,314] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:20:36,315] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:36,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:20:36,319] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:36,320] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:20:36,321] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:36,321] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:20:36,325] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:36,326] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:20:36,348] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:20:37,655] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-26 01:20:37,671] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-26 01:20:37,743] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-26 01:20:37,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-26 01:20:37,875] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-26 01:20:59,193] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.96628696], dtype=float32), 0.15670688]
[2019-03-26 01:20:59,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.597968425, 81.143358255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7871317289867351, 6.9112, 6.9112, 168.912956510431, 669310.1773314426, 669310.1773314426, 199357.0322572243]
[2019-03-26 01:20:59,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:20:59,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.999999e-01 0.000000e+00 0.000000e+00 0.000000e+00 8.489174e-08], sampled 0.21249255504851428
[2019-03-26 01:21:35,330] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.96628696], dtype=float32), 0.15670688]
[2019-03-26 01:21:35,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.60241915333334, 80.36172526333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.793154707488066, 6.911200000000001, 6.9112, 168.912956510431, 667057.5507136832, 667057.5507136827, 200476.3611486714]
[2019-03-26 01:21:35,331] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:21:35,333] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.009078749506998718
[2019-03-26 01:21:48,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.96628696], dtype=float32), 0.15670688]
[2019-03-26 01:21:48,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.86485404666666, 58.70353918166666, 1.0, 2.0, 0.3477404775145854, 1.0, 2.0, 0.3477404775145854, 1.0, 2.0, 0.603910209909625, 6.9112, 6.9112, 184.5923449428631, 1458176.000564746, 1458176.000564746, 329935.7261795024]
[2019-03-26 01:21:48,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:21:48,663] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.58070507e-08 1.03463764e-35 0.00000000e+00 3.60243064e-22
 1.00000000e+00], sampled 0.4674102880282526
[2019-03-26 01:21:56,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.96628696], dtype=float32), 0.15670688]
[2019-03-26 01:21:56,497] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.41414277, 84.76571192, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 1.0, 2.0, 1.03, 8.726812165760887, 6.9112, 184.5923449428631, 5149280.719291172, 3741658.669115884, 699084.7495531235]
[2019-03-26 01:21:56,498] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:21:56,500] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.3030636782005188
[2019-03-26 01:22:48,113] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7332.8488 3194409734.7363 593.0000
[2019-03-26 01:22:48,183] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7395.8436 3171296100.4152 507.0000
[2019-03-26 01:22:48,194] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7238.4165 3314614263.4890 806.0000
[2019-03-26 01:22:48,236] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7156.2357 3365706592.6711 753.0000
[2019-03-26 01:22:48,353] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6832.6430 3498996054.3316 1027.0000
[2019-03-26 01:22:49,370] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1500000, evaluation results [1500000.0, 6832.64303151451, 3498996054.3316045, 1027.0, 7238.416497590584, 3314614263.489025, 806.0, 7395.843622154317, 3171296100.415203, 507.0, 7156.23567817501, 3365706592.6711254, 753.0, 7332.848810590692, 3194409734.736276, 593.0]
[2019-03-26 01:22:53,120] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:22:53,133] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7759
[2019-03-26 01:22:53,138] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.3, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4912077425363247, 6.911199999999999, 6.9112, 168.912956510431, 436119.7882263062, 436119.7882263068, 150856.821658147], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 799200.0000, 
sim time next is 799800.0000, 
raw observation next is [20.45, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4934729678595544, 6.9112, 6.9112, 168.912956510431, 437923.1044617129, 437923.1044617129, 151144.3445096104], 
processed observation next is [0.0, 0.2608695652173913, 0.16824644549763035, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.382284107145798, 0.0, 0.0, 0.8294399451523027, 0.12164530679492025, 0.12164530679492025, 0.2255885738949409], 
reward next is 0.7744, 
noisyNet noise sample is [array([1.4529147], dtype=float32), -0.058424737]. 
=============================================
[2019-03-26 01:22:54,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:22:54,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2991
[2019-03-26 01:22:54,430] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4455276956524958, 6.911199999999999, 6.9112, 168.912956510431, 399087.149993665, 399087.1499936656, 145349.0861806342], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 590400.0000, 
sim time next is 591000.0000, 
raw observation next is [20.86666666666667, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4437305511021722, 6.9112, 6.9112, 168.912956510431, 397648.4020922547, 397648.4020922547, 145140.6925992473], 
processed observation next is [1.0, 0.8695652173913043, 0.18799368088467638, 0.7366666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3216226232953319, 0.0, 0.0, 0.8294399451523027, 0.11045788947007075, 0.11045788947007075, 0.21662789940186164], 
reward next is 0.7834, 
noisyNet noise sample is [array([-1.0016179], dtype=float32), -0.72371787]. 
=============================================
[2019-03-26 01:22:54,457] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.19769]
 [75.21697]
 [75.58501]
 [75.79403]
 [76.21564]], R is [[75.01451874]
 [75.04743195]
 [75.07962799]
 [75.11084747]
 [75.14086151]].
[2019-03-26 01:23:00,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.0906876e-28 1.3760669e-37 9.7412971e-16 2.9275624e-13], sum to 1.0000
[2019-03-26 01:23:00,569] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9355
[2019-03-26 01:23:00,587] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 996084.1435917852 W.
[2019-03-26 01:23:00,591] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 58.0, 1.0, 2.0, 0.609784874782733, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 996084.1435917852, 996084.1435917858, 217637.3874622985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 646200.0000, 
sim time next is 646800.0000, 
raw observation next is [24.13333333333333, 57.66666666666667, 1.0, 2.0, 0.6098791469107221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 999803.6475582586, 999803.6475582586, 217732.9126713256], 
processed observation next is [1.0, 0.4782608695652174, 0.3428120063191152, 0.5766666666666667, 1.0, 1.0, 0.5299748757960507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2777232354328496, 0.2777232354328496, 0.3249744965243666], 
reward next is 0.6750, 
noisyNet noise sample is [array([0.64538854], dtype=float32), 2.5139558]. 
=============================================
[2019-03-26 01:23:06,930] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:23:06,941] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5402
[2019-03-26 01:23:06,948] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.6, 92.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4078632080399794, 6.9112, 6.9112, 168.912956510431, 367787.6481783985, 367787.6481783985, 141220.5455279799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 702600.0000, 
sim time next is 703200.0000, 
raw observation next is [17.6, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4006937592120055, 6.9112, 6.9112, 168.912956510431, 361364.6682528363, 361364.6682528363, 140515.8967141489], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.26913873074634814, 0.0, 0.0, 0.8294399451523027, 0.10037907451467674, 0.10037907451467674, 0.20972521897634164], 
reward next is 0.7903, 
noisyNet noise sample is [array([-0.8018655], dtype=float32), 1.0795407]. 
=============================================
[2019-03-26 01:23:08,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3276016e-24], sum to 1.0000
[2019-03-26 01:23:08,107] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4724
[2019-03-26 01:23:08,115] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.7, 97.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6344032597021272, 6.911199999999999, 6.9112, 168.912956510431, 547516.2493092454, 547516.249309246, 171535.9127160095], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1012800.0000, 
sim time next is 1013400.0000, 
raw observation next is [21.7, 97.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.630394559796972, 6.911200000000001, 6.9112, 168.912956510431, 545365.695659187, 545365.6956591863, 170879.0311722669], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5492616582889902, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15149047101644084, 0.15149047101644064, 0.25504333010786107], 
reward next is 0.7450, 
noisyNet noise sample is [array([1.0751841], dtype=float32), -1.1778948]. 
=============================================
[2019-03-26 01:23:15,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:23:15,522] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0208
[2019-03-26 01:23:15,531] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.03333333333333, 61.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5261116136425107, 6.9112, 6.9112, 168.912956510431, 463378.7550511046, 463378.7550511046, 155440.2534756812], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 820200.0000, 
sim time next is 820800.0000, 
raw observation next is [25.0, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5262090354559018, 6.911200000000001, 6.9112, 168.912956510431, 463480.6529531262, 463480.6529531256, 155452.5299114135], 
processed observation next is [0.0, 0.5217391304347826, 0.38388625592417064, 0.62, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4222061407998802, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12874462582031282, 0.12874462582031265, 0.23201870136031866], 
reward next is 0.7680, 
noisyNet noise sample is [array([-0.56185746], dtype=float32), -0.09776142]. 
=============================================
[2019-03-26 01:23:22,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 4.3690341e-35 0.0000000e+00 3.3146913e-37 3.2832995e-18], sum to 1.0000
[2019-03-26 01:23:22,817] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3021
[2019-03-26 01:23:22,825] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 913632.4249110278 W.
[2019-03-26 01:23:22,831] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.1, 77.5, 1.0, 2.0, 0.2900004985648549, 1.0, 1.0, 0.2900004985648549, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 913632.4249110278, 913632.4249110278, 260413.6216349829], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1078200.0000, 
sim time next is 1078800.0000, 
raw observation next is [23.26666666666667, 77.0, 1.0, 2.0, 0.202747500371236, 1.0, 2.0, 0.202747500371236, 1.0, 1.0, 0.3632019219033242, 6.9112, 6.9112, 170.5573041426782, 949533.9282772163, 949533.9282772163, 281742.1840890883], 
processed observation next is [1.0, 0.4782608695652174, 0.3017377567140602, 0.77, 1.0, 1.0, 0.039454819724380724, 1.0, 1.0, 0.039454819724380724, 1.0, 0.5, 0.22341697793088314, 0.0, 0.0, 0.8375144448122397, 0.26375942452144896, 0.26375942452144896, 0.4205107225210273], 
reward next is 0.5795, 
noisyNet noise sample is [array([-1.631844], dtype=float32), 0.45316833]. 
=============================================
[2019-03-26 01:23:32,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:23:32,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0027
[2019-03-26 01:23:32,901] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5977017367634966, 6.911200000000001, 6.9112, 168.912956510431, 519988.7483065581, 519988.7483065575, 165738.6443610718], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1101600.0000, 
sim time next is 1102200.0000, 
raw observation next is [24.86666666666667, 70.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6014877990604356, 6.911199999999999, 6.9112, 168.912956510431, 523551.3131005259, 523551.3131005265, 166309.4462021269], 
processed observation next is [1.0, 0.782608695652174, 0.3775671406003162, 0.705, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5140095110493117, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14543092030570162, 0.14543092030570182, 0.24822305403302525], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.43429673], dtype=float32), 0.1068452]. 
=============================================
[2019-03-26 01:23:39,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8984151e-27], sum to 1.0000
[2019-03-26 01:23:39,437] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.916661e-31 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 01:23:39,439] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1579
[2019-03-26 01:23:39,445] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.1, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5803269170316111, 6.9112, 6.9112, 168.912956510431, 507450.7647921137, 507450.7647921137, 163093.7459684171], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1404000.0000, 
sim time next is 1404600.0000, 
raw observation next is [21.18333333333334, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5827091560689328, 6.9112, 6.9112, 168.912956510431, 509367.5124877075, 509367.5124877075, 163447.9902834612], 
processed observation next is [0.0, 0.2608695652173913, 0.20300157977883138, 0.9466666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4911087269133327, 0.0, 0.0, 0.8294399451523027, 0.14149097569102986, 0.14149097569102986, 0.24395222430367344], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.2525513], dtype=float32), 0.5419828]. 
=============================================
[2019-03-26 01:23:39,446] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4287
[2019-03-26 01:23:39,453] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.16666666666667, 94.16666666666667, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.1955495529425686, 6.911200000000001, 6.9112, 170.5573041426782, 510490.505526386, 510490.5055263854, 231770.5181326085], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1368600.0000, 
sim time next is 1369200.0000, 
raw observation next is [21.13333333333333, 94.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1951581754497147, 6.9112, 6.9112, 170.5573041426782, 509695.7490587576, 509695.7490587576, 231636.5145784123], 
processed observation next is [1.0, 0.8695652173913043, 0.20063191153238533, 0.9433333333333335, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.01848557981672525, 0.0, 0.0, 0.8375144448122397, 0.14158215251632156, 0.14158215251632156, 0.3457261411618094], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84859836], dtype=float32), 0.05834845]. 
=============================================
[2019-03-26 01:23:44,609] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.2745143e-11 8.5183622e-36 6.4880898e-38 2.4659389e-30 1.0000000e+00], sum to 1.0000
[2019-03-26 01:23:44,622] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3465
[2019-03-26 01:23:44,633] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.4, 93.0, 1.0, 2.0, 0.2626179998266603, 1.0, 2.0, 0.2626179998266603, 1.0, 2.0, 0.4569567925997707, 6.911200000000001, 6.9112, 170.5573041426782, 1175204.604357714, 1175204.604357714, 297169.1948232923], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1339200.0000, 
sim time next is 1339800.0000, 
raw observation next is [22.33333333333334, 92.66666666666667, 1.0, 2.0, 0.2922973065686376, 1.0, 2.0, 0.2922973065686376, 1.0, 2.0, 0.509159540804297, 6.9112, 6.9112, 170.5573041426782, 1310279.115460746, 1310279.115460746, 309226.9714883598], 
processed observation next is [1.0, 0.5217391304347826, 0.2575039494470777, 0.9266666666666667, 1.0, 1.0, 0.14734615249233446, 1.0, 1.0, 0.14734615249233446, 1.0, 1.0, 0.40141407415158176, 0.0, 0.0, 0.8375144448122397, 0.3639664209613183, 0.3639664209613183, 0.46153279326620866], 
reward next is 0.5385, 
noisyNet noise sample is [array([-0.08388348], dtype=float32), 0.8285623]. 
=============================================
[2019-03-26 01:23:45,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:23:45,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5339
[2019-03-26 01:23:45,429] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 57.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6358075358687575, 6.9112, 6.9112, 168.912956510431, 549558.4744311062, 549558.4744311062, 171756.3348697701], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1526400.0000, 
sim time next is 1527000.0000, 
raw observation next is [27.85, 57.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6323406606178705, 6.9112, 6.9112, 168.912956510431, 546906.9123869502, 546906.9123869502, 171193.2726254889], 
processed observation next is [0.0, 0.6956521739130435, 0.5189573459715641, 0.5733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5516349519730128, 0.0, 0.0, 0.8294399451523027, 0.15191858677415285, 0.15191858677415285, 0.2555123472022222], 
reward next is 0.7445, 
noisyNet noise sample is [array([2.2213438], dtype=float32), 0.2099092]. 
=============================================
[2019-03-26 01:23:45,454] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.27168 ]
 [73.124176]
 [72.93069 ]
 [72.100105]
 [70.82232 ]], R is [[73.09536743]
 [73.10806274]
 [73.11985016]
 [73.13095093]
 [73.14163208]].
[2019-03-26 01:23:56,441] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:23:56,451] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8030
[2019-03-26 01:23:56,458] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7221020069017042, 6.9112, 6.9112, 168.912956510431, 614692.7642501778, 614692.7642501778, 186743.6669238317], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1641000.0000, 
sim time next is 1641600.0000, 
raw observation next is [23.1, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7217736967489842, 6.9112, 6.9112, 168.912956510431, 614413.0298833767, 614413.0298833767, 186682.8557376438], 
processed observation next is [1.0, 0.0, 0.2938388625592418, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6606996301816879, 0.0, 0.0, 0.8294399451523027, 0.17067028607871573, 0.17067028607871573, 0.27863112796663253], 
reward next is 0.7214, 
noisyNet noise sample is [array([-1.3797551], dtype=float32), 1.6015604]. 
=============================================
[2019-03-26 01:23:56,895] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 01:23:56,897] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:23:56,899] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:23:56,899] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:23:56,900] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:23:56,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:23:56,901] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:23:56,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:23:56,904] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:23:56,905] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:23:56,907] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:23:56,943] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-26 01:23:56,968] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-26 01:23:56,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-26 01:23:57,030] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-26 01:23:57,031] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-26 01:24:22,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:24:22,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.76672513, 77.31877033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5678492797892738, 6.911200000000001, 6.9112, 168.912956510431, 499078.081988142, 499078.0819881413, 161212.8685054419]
[2019-03-26 01:24:22,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:24:22,857] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.35608381236706177
[2019-03-26 01:25:13,680] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:25:13,682] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 63.0, 1.0, 2.0, 0.6413511404702912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896271.9066966529, 896271.9066966529, 208703.1581387743]
[2019-03-26 01:25:13,684] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:25:13,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.747194e-34], sampled 0.6774631996419541
[2019-03-26 01:25:13,690] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 896271.9066966529 W.
[2019-03-26 01:25:22,749] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:25:22,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.07099105, 91.04062854, 1.0, 2.0, 0.7127512136298569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104271, 996098.4860521873, 996098.4860521879, 223636.6865999837]
[2019-03-26 01:25:22,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:25:22,756] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.38615855e-11], sampled 0.5889470729290176
[2019-03-26 01:25:22,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 996098.4860521873 W.
[2019-03-26 01:25:36,534] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:25:36,535] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.76666666666667, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9813944672006996, 6.9112, 6.9112, 168.912956510431, 795977.6404814876, 795977.6404814876, 242574.433107353]
[2019-03-26 01:25:36,536] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:25:36,541] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.46077570051428385
[2019-03-26 01:25:40,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:25:40,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.25463190333333, 70.63751384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8565124557208138, 6.9112, 6.9112, 168.912956510431, 712794.4983345371, 712794.4983345371, 213768.9530978831]
[2019-03-26 01:25:40,924] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:25:40,926] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5029424209096359
[2019-03-26 01:25:55,645] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.85179037], dtype=float32), 0.17941809]
[2019-03-26 01:25:55,647] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.76334329833334, 50.71433737833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7116864070567324, 6.911200000000001, 6.9112, 168.912956510431, 624717.7573285779, 624717.7573285773, 184620.7401522977]
[2019-03-26 01:25:55,648] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:25:55,652] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8744999248403258
[2019-03-26 01:26:05,731] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7101.1991 3193780816.9458 2262.0000
[2019-03-26 01:26:05,858] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7396.0308 3116378589.0789 1908.0000
[2019-03-26 01:26:05,890] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8103.2394 2951100614.6367 1242.0000
[2019-03-26 01:26:06,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7992.4449 2999293133.7549 1368.0000
[2019-03-26 01:26:06,211] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7340.1849 3330398201.3748 1969.0000
[2019-03-26 01:26:07,227] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1525000, evaluation results [1525000.0, 7340.184888697655, 3330398201.3747754, 1969.0, 7396.030821776052, 3116378589.0789013, 1908.0, 8103.239369681589, 2951100614.6366863, 1242.0, 7101.199117237328, 3193780816.9457893, 2262.0, 7992.4449420038, 2999293133.754917, 1368.0]
[2019-03-26 01:26:09,466] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:26:09,474] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7748
[2019-03-26 01:26:09,481] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.66666666666666, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8627613960834634, 6.9112, 6.9112, 168.912956510431, 713059.1795424036, 713059.1795424036, 214995.1656021141], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1708800.0000, 
sim time next is 1709400.0000, 
raw observation next is [27.53333333333333, 81.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8602788032848966, 6.9112, 6.9112, 168.912956510431, 711304.1724018042, 711304.1724018042, 214454.3631105994], 
processed observation next is [1.0, 0.782608695652174, 0.5039494470774091, 0.8133333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8296082966888981, 0.0, 0.0, 0.8294399451523027, 0.19758449233383452, 0.19758449233383452, 0.3200811389710439], 
reward next is 0.6799, 
noisyNet noise sample is [array([0.17511962], dtype=float32), 0.40104476]. 
=============================================
[2019-03-26 01:26:15,891] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:26:15,903] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8600
[2019-03-26 01:26:15,915] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.06666666666667, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5703730067660929, 6.911199999999999, 6.9112, 168.912956510431, 499050.6682210477, 499050.6682210483, 161639.467658429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1792200.0000, 
sim time next is 1792800.0000, 
raw observation next is [22.0, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5769500279395099, 6.9112, 6.9112, 168.912956510431, 504834.306482217, 504834.306482217, 162591.5607856256], 
processed observation next is [1.0, 0.782608695652174, 0.2417061611374408, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48408539992623156, 0.0, 0.0, 0.8294399451523027, 0.14023175180061584, 0.14023175180061584, 0.24267397132182925], 
reward next is 0.7573, 
noisyNet noise sample is [array([2.090279], dtype=float32), -0.0070974785]. 
=============================================
[2019-03-26 01:26:31,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:26:31,389] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3852
[2019-03-26 01:26:31,395] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.95, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5838390050858215, 6.9112, 6.9112, 168.912956510431, 510889.4066064937, 510889.4066064937, 163601.395356926], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1793400.0000, 
sim time next is 1794000.0000, 
raw observation next is [21.9, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5803606916950969, 6.9112, 6.9112, 168.912956510431, 507877.6350391586, 507877.6350391586, 163088.7575943145], 
processed observation next is [1.0, 0.782608695652174, 0.23696682464454974, 0.8766666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4882447459696303, 0.0, 0.0, 0.8294399451523027, 0.1410771208442107, 0.1410771208442107, 0.24341605611091716], 
reward next is 0.7566, 
noisyNet noise sample is [array([0.39504188], dtype=float32), 0.98303586]. 
=============================================
[2019-03-26 01:26:31,413] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[65.200386]
 [64.54425 ]
 [63.694653]
 [62.516033]
 [61.23916 ]], R is [[65.60901642]
 [65.70874786]
 [65.80898285]
 [65.90963745]
 [66.01045227]].
[2019-03-26 01:26:38,883] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:26:38,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8821
[2019-03-26 01:26:38,902] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.56666666666666, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8323014425261559, 6.9112, 6.9112, 168.912956510431, 687959.8033660662, 687959.8033660662, 208349.8573946518], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1878000.0000, 
sim time next is 1878600.0000, 
raw observation next is [26.48333333333333, 87.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8351456016312238, 6.9112, 6.9112, 168.912956510431, 691619.658404476, 691619.658404476, 209004.0297781997], 
processed observation next is [1.0, 0.7391304347826086, 0.4541864139020536, 0.8716666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7989580507697851, 0.0, 0.0, 0.8294399451523027, 0.1921165717790211, 0.1921165717790211, 0.3119463131017906], 
reward next is 0.6881, 
noisyNet noise sample is [array([-0.6413811], dtype=float32), -0.38474935]. 
=============================================
[2019-03-26 01:26:44,029] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:26:44,040] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9592
[2019-03-26 01:26:44,047] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8523378750955264, 6.9112, 6.9112, 168.912956510431, 706231.4460809233, 706231.4460809233, 212752.7747411453], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2019600.0000, 
sim time next is 2020200.0000, 
raw observation next is [25.58333333333334, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.852502541252131, 6.9112, 6.9112, 168.912956510431, 706441.4263932309, 706441.4263932309, 212791.3608108014], 
processed observation next is [0.0, 0.391304347826087, 0.4115323854660351, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8201250503074768, 0.0, 0.0, 0.8294399451523027, 0.19623372955367524, 0.19623372955367524, 0.3175990459862707], 
reward next is 0.6824, 
noisyNet noise sample is [array([-1.0757467], dtype=float32), -1.4624741]. 
=============================================
[2019-03-26 01:26:45,717] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.0612107e-34], sum to 1.0000
[2019-03-26 01:26:45,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5138
[2019-03-26 01:26:45,738] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 924410.3666976637 W.
[2019-03-26 01:26:45,746] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.93333333333334, 76.66666666666667, 1.0, 2.0, 0.2204934590836899, 1.0, 2.0, 0.2204934590836899, 1.0, 1.0, 0.3730684664746757, 6.911199999999999, 6.9112, 170.5573041426782, 924410.3666976637, 924410.3666976643, 275170.7975195228], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2272800.0000, 
sim time next is 2273400.0000, 
raw observation next is [28.1, 76.0, 1.0, 2.0, 0.225436712246422, 1.0, 2.0, 0.225436712246422, 1.0, 2.0, 0.3818874104621849, 6.9112, 6.9112, 170.5573041426782, 945143.898647437, 945143.898647437, 276690.5883589901], 
processed observation next is [1.0, 0.30434782608695654, 0.5308056872037916, 0.76, 1.0, 1.0, 0.0667912195740024, 1.0, 1.0, 0.0667912195740024, 1.0, 1.0, 0.24620415910022544, 0.0, 0.0, 0.8375144448122397, 0.2625399718465103, 0.2625399718465103, 0.41297102740147773], 
reward next is 0.5870, 
noisyNet noise sample is [array([-0.11324397], dtype=float32), 0.15543611]. 
=============================================
[2019-03-26 01:26:58,230] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.750349e-10], sum to 1.0000
[2019-03-26 01:26:58,239] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7014
[2019-03-26 01:26:58,245] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.35, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.898732691776683, 6.9112, 6.9112, 168.912956510431, 739065.9208751115, 739065.9208751115, 223025.7655875788], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2154600.0000, 
sim time next is 2155200.0000, 
raw observation next is [26.26666666666667, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8962060893148841, 6.911200000000001, 6.9112, 168.912956510431, 737405.3500806413, 737405.3500806406, 222457.5918547321], 
processed observation next is [0.0, 0.9565217391304348, 0.44391785150079005, 0.9233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8734220601401025, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20483481946684481, 0.20483481946684462, 0.33202625649960016], 
reward next is 0.6680, 
noisyNet noise sample is [array([-1.4916549], dtype=float32), 0.06459528]. 
=============================================
[2019-03-26 01:26:59,030] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 2.0571375e-26 4.0553607e-30 3.0503093e-13 1.4826509e-18], sum to 1.0000
[2019-03-26 01:26:59,047] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7247
[2019-03-26 01:26:59,063] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1828914.377173497 W.
[2019-03-26 01:26:59,069] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.73333333333333, 81.33333333333334, 1.0, 1.0, 0.4360548945624108, 1.0, 1.0, 0.4360548945624108, 1.0, 2.0, 0.7478173914616602, 6.9112, 6.9112, 170.5573041426782, 1828914.377173497, 1828914.377173497, 372273.5080482272], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2341200.0000, 
sim time next is 2341800.0000, 
raw observation next is [27.7, 81.5, 1.0, 2.0, 0.4579974707006589, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7792747248124954, 6.911200000000001, 6.9112, 168.9129558797664, 1280311.012411233, 1280311.012411232, 283715.9372505123], 
processed observation next is [1.0, 0.08695652173913043, 0.5118483412322274, 0.815, 1.0, 1.0, 0.3469849044586251, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.7308228351371895, 8.881784197001253e-17, 0.0, 0.8294399420554508, 0.3556419478920092, 0.3556419478920089, 0.42345662276195867], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6097418], dtype=float32), 0.21592712]. 
=============================================
[2019-03-26 01:27:01,691] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 7.4456873e-34 0.0000000e+00 2.8142426e-33 3.6594734e-19], sum to 1.0000
[2019-03-26 01:27:01,702] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3230
[2019-03-26 01:27:01,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 989352.082985271 W.
[2019-03-26 01:27:01,717] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.96666666666667, 95.16666666666667, 1.0, 2.0, 0.7079261202852376, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989352.082985271, 989352.082985271, 222562.5511200573], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2171400.0000, 
sim time next is 2172000.0000, 
raw observation next is [24.93333333333333, 95.33333333333334, 1.0, 2.0, 0.3220445934108734, 1.0, 1.0, 0.3220445934108734, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 900096.2010256223, 900096.2010256223, 254267.556722396], 
processed observation next is [1.0, 0.13043478260869565, 0.38072669826224315, 0.9533333333333335, 1.0, 1.0, 0.1831862571215342, 1.0, 0.5, 0.1831862571215342, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2500267225071173, 0.2500267225071173, 0.37950381600357613], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57291234], dtype=float32), -0.6215636]. 
=============================================
[2019-03-26 01:27:01,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[38.289932]
 [38.97763 ]
 [38.8198  ]
 [39.01692 ]
 [38.85964 ]], R is [[37.67541122]
 [37.96647263]
 [38.15594482]
 [38.34114075]
 [38.5173111 ]].
[2019-03-26 01:27:02,879] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5662977e-23 3.2548420e-18 2.7372171e-32 1.0000000e+00 1.9792341e-20], sum to 1.0000
[2019-03-26 01:27:02,888] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8989
[2019-03-26 01:27:02,896] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.23333333333333, 64.0, 1.0, 2.0, 0.829806816477688, 1.0, 2.0, 0.829806816477688, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2320782.422616116, 2320782.422616117, 434645.4919792792], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2373000.0000, 
sim time next is 2373600.0000, 
raw observation next is [32.26666666666667, 64.0, 1.0, 2.0, 0.858140359753712, 1.0, 2.0, 0.858140359753712, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2400101.046776043, 2400101.046776043, 449170.5596807845], 
processed observation next is [1.0, 0.4782608695652174, 0.7282780410742499, 0.64, 1.0, 1.0, 0.8290847707876048, 1.0, 1.0, 0.8290847707876048, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6666947352155674, 0.6666947352155674, 0.6704038204190814], 
reward next is 0.3296, 
noisyNet noise sample is [array([0.8478962], dtype=float32), -0.21656106]. 
=============================================
[2019-03-26 01:27:05,013] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 7.5697515e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 01:27:05,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2391
[2019-03-26 01:27:05,033] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.8, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8666478493914554, 6.911200000000001, 6.9112, 168.912956510431, 717112.9254612543, 717112.9254612536, 215890.6704754438], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2250000.0000, 
sim time next is 2250600.0000, 
raw observation next is [26.75, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8629250400591179, 6.911199999999999, 6.9112, 168.912956510431, 714304.2341105256, 714304.2341105263, 215069.9214664098], 
processed observation next is [1.0, 0.043478260869565216, 0.4668246445497631, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8328354147062411, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19841784280847932, 0.19841784280847952, 0.3209998827856863], 
reward next is 0.6790, 
noisyNet noise sample is [array([-0.00122123], dtype=float32), 1.3485584]. 
=============================================
[2019-03-26 01:27:05,640] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.9029652e-22 1.5483676e-25 1.7541291e-32 5.4150826e-11 1.0000000e+00], sum to 1.0000
[2019-03-26 01:27:05,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0402
[2019-03-26 01:27:05,666] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.46666666666667, 86.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.89937844077533, 6.9112, 168.9077422075359, 2155261.318438186, 1454235.163254184, 311350.9178389768], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2449200.0000, 
sim time next is 2449800.0000, 
raw observation next is [27.35, 86.5, 1.0, 2.0, 0.4747532221320804, 1.0, 1.0, 0.4747532221320804, 1.0, 1.0, 0.8181895474052221, 6.9112, 6.9112, 170.5573041426782, 1991374.88999312, 1991374.88999312, 397051.6004583096], 
processed observation next is [1.0, 0.34782608695652173, 0.4952606635071091, 0.865, 1.0, 1.0, 0.367172556785639, 1.0, 0.5, 0.367172556785639, 1.0, 0.5, 0.778279935860027, 0.0, 0.0, 0.8375144448122397, 0.5531596916647555, 0.5531596916647555, 0.5926143290422531], 
reward next is 0.4074, 
noisyNet noise sample is [array([1.1969243], dtype=float32), -1.0890845]. 
=============================================
[2019-03-26 01:27:09,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.8292014e-24 3.1373424e-24 5.5198187e-30 8.2595581e-01 1.7404422e-01], sum to 1.0000
[2019-03-26 01:27:09,487] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3849
[2019-03-26 01:27:09,495] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.9, 90.33333333333333, 1.0, 2.0, 0.4451067161542556, 1.0, 2.0, 0.4451067161542556, 1.0, 2.0, 0.7684553501829223, 6.9112, 6.9112, 170.5573041426782, 1866912.871152126, 1866912.871152126, 378543.0461827556], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2536800.0000, 
sim time next is 2537400.0000, 
raw observation next is [27.0, 89.66666666666667, 1.0, 2.0, 0.6698580374778582, 1.0, 2.0, 0.6698580374778582, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1873064.193887303, 1873064.193887303, 361487.0933949744], 
processed observation next is [1.0, 0.34782608695652173, 0.4786729857819906, 0.8966666666666667, 1.0, 1.0, 0.6022385993709135, 1.0, 1.0, 0.6022385993709135, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5202956094131397, 0.5202956094131397, 0.5395329752163798], 
reward next is 0.4605, 
noisyNet noise sample is [array([-0.13620178], dtype=float32), -0.5091179]. 
=============================================
[2019-03-26 01:27:11,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:27:11,564] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4659
[2019-03-26 01:27:11,574] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.98333333333333, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.954600263087322, 6.9112, 6.9112, 168.912956510431, 775957.4144992592, 775957.4144992592, 235977.4146590983], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2491800.0000, 
sim time next is 2492400.0000, 
raw observation next is [26.96666666666667, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9562057418094825, 6.911200000000001, 6.9112, 168.912956510431, 777417.2082954989, 777417.2082954982, 236380.4849620111], 
processed observation next is [1.0, 0.8695652173913043, 0.47709320695102697, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9465923680603445, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21594922452652746, 0.21594922452652726, 0.3528066939731509], 
reward next is 0.6472, 
noisyNet noise sample is [array([0.9696733], dtype=float32), -0.672676]. 
=============================================
[2019-03-26 01:27:14,791] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 01:27:14,795] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:27:14,796] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:27:14,797] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:27:14,797] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:27:14,798] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:27:14,797] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:27:14,799] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:27:14,805] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:27:14,804] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:27:14,810] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:27:14,836] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-26 01:27:14,868] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-26 01:27:14,895] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-26 01:27:14,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-26 01:27:14,943] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-26 01:27:18,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:27:18,074] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.46666666666667, 40.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9223822234861354, 6.9112, 6.9112, 168.912956510431, 828866.7321686547, 828866.7321686547, 227583.2863995403]
[2019-03-26 01:27:18,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:27:18,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9100584051815575
[2019-03-26 01:27:35,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:27:35,135] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.28342043666667, 78.59808471333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.524790084433193, 6.9112, 6.9112, 168.912956510431, 465268.5429510876, 465268.5429510876, 155140.5220119125]
[2019-03-26 01:27:35,136] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:27:35,138] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.1077333829989664
[2019-03-26 01:27:54,914] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:27:54,916] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.15, 66.5, 1.0, 2.0, 0.7775991537227663, 1.0, 1.0, 0.7775991537227663, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2174636.82714422, 2174636.82714422, 409112.9710358283]
[2019-03-26 01:27:54,918] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:27:54,921] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1959609e-07 1.1301854e-23 5.4484071e-29 9.9999881e-01 7.5309629e-07], sampled 0.3820467934473708
[2019-03-26 01:28:12,564] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:28:12,567] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.05, 72.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000544873577458, 6.9112, 168.9122349849679, 892209.7057524942, 828825.6481747393, 254812.0235683455]
[2019-03-26 01:28:12,569] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:28:12,571] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 3.757478e-33 0.000000e+00], sampled 0.4527042251020875
[2019-03-26 01:28:12,571] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 892209.7057524942 W.
[2019-03-26 01:28:20,985] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:28:20,985] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.53633347, 75.79985604, 1.0, 2.0, 1.024743826061528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104114, 1432414.829126031, 1432414.829126031, 306564.8475303093]
[2019-03-26 01:28:20,986] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:28:20,988] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9999869e-01 3.1301550e-28 1.3171553e-28 2.9230362e-15 1.3359553e-06], sampled 0.4355327861205426
[2019-03-26 01:28:20,990] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1432414.829126031 W.
[2019-03-26 01:28:37,694] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:28:37,694] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.25, 76.0, 1.0, 2.0, 0.20964826836289, 1.0, 1.0, 0.20964826836289, 1.0, 1.0, 0.3640897104045412, 6.9112, 6.9112, 169.0403247858759, 878926.9206531312, 878926.9206531312, 272288.338479791]
[2019-03-26 01:28:37,695] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:28:37,699] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7705204e-08 2.5525739e-32 9.8698700e-36 3.7133546e-22 1.0000000e+00], sampled 0.3672022325753165
[2019-03-26 01:28:38,355] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:28:38,356] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8801401478173233, 6.911200000000001, 6.9112, 168.912956510431, 725072.5661193123, 725072.5661193117, 218813.3594606472]
[2019-03-26 01:28:38,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:28:38,360] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5500419187465824
[2019-03-26 01:28:41,489] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:28:41,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.35, 61.0, 1.0, 2.0, 0.584345869447784, 0.0, 2.0, 0.0, 1.0, 2.0, 1.0148155290988, 6.911199999999999, 6.9112, 168.9128793217192, 1633783.999610046, 1633783.999610046, 357719.7164327538]
[2019-03-26 01:28:41,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:28:41,494] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9999166e-01 6.9423469e-31 1.4052419e-37 8.3837322e-06 1.3191962e-30], sampled 0.8792098342316753
[2019-03-26 01:28:41,496] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1633783.999610046 W.
[2019-03-26 01:29:11,610] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:29:11,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.9, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.940086923486063, 6.9112, 168.9125705249489, 850637.3194968732, 830143.9885944021, 254891.1996763749]
[2019-03-26 01:29:11,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:29:11,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0603021e-26 8.9312253e-35], sampled 0.5216265290537037
[2019-03-26 01:29:14,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:29:14,689] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.33333333333333, 75.0, 1.0, 2.0, 0.6751888042765766, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991963624821368, 6.9112, 168.9124755811392, 1840409.71828781, 1783113.386304779, 379890.6780362824]
[2019-03-26 01:29:14,690] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:29:14,692] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9998665e-01 4.6643080e-27 5.6033557e-34 1.3308648e-05 4.5260172e-18], sampled 0.37617103603080726
[2019-03-26 01:29:14,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1840409.71828781 W.
[2019-03-26 01:29:16,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9099757], dtype=float32), 0.19512515]
[2019-03-26 01:29:16,854] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.2, 66.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7113298291646085, 6.9112, 6.9112, 168.912956510431, 609023.2104848872, 609023.2104848872, 184779.2485206607]
[2019-03-26 01:29:16,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:29:16,864] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8160060209741892
[2019-03-26 01:29:23,968] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7584.2395 3140866129.0031 1509.0000
[2019-03-26 01:29:24,031] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7389.5920 3203951936.9460 1539.0000
[2019-03-26 01:29:24,124] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8084.3305 3018083634.3882 1013.0000
[2019-03-26 01:29:24,319] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8179.6460 2976181992.0293 964.0000
[2019-03-26 01:29:24,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7516.3515 3337725677.4053 1417.0000
[2019-03-26 01:29:25,651] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1550000, evaluation results [1550000.0, 7516.351520979799, 3337725677.4052777, 1417.0, 7584.2395467518345, 3140866129.0030565, 1509.0, 8179.645960549881, 2976181992.029317, 964.0, 7389.591979380884, 3203951936.9459825, 1539.0, 8084.330510312875, 3018083634.3881593, 1013.0]
[2019-03-26 01:29:28,996] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:29:29,007] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8194
[2019-03-26 01:29:29,013] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7911383745795443, 6.9112, 6.9112, 168.912956510431, 664551.6690290824, 664551.6690290824, 200049.683046578], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2623800.0000, 
sim time next is 2624400.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7917447917355087, 6.9112, 6.9112, 168.912956510431, 665060.7513070958, 665060.7513070958, 200173.2316265412], 
processed observation next is [0.0, 0.391304347826087, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7460302338237911, 0.0, 0.0, 0.8294399451523027, 0.18473909758530438, 0.18473909758530438, 0.2987660173530466], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.6937623], dtype=float32), -0.009137571]. 
=============================================
[2019-03-26 01:29:29,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 7.0734773e-30 1.7109122e-35 3.3886555e-13 2.0612647e-23], sum to 1.0000
[2019-03-26 01:29:29,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3647
[2019-03-26 01:29:29,301] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 998187.6505370787 W.
[2019-03-26 01:29:29,306] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 83.0, 1.0, 2.0, 0.3571226991260188, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6083342680876319, 6.911200000000001, 6.9112, 168.912956510431, 998187.6505370787, 998187.650537078, 240599.7737309468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2435400.0000, 
sim time next is 2436000.0000, 
raw observation next is [27.7, 83.33333333333334, 1.0, 2.0, 0.72440626249048, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012394.649632712, 1012394.649632711, 226201.2749275727], 
processed observation next is [1.0, 0.17391304347826086, 0.5118483412322274, 0.8333333333333335, 1.0, 1.0, 0.6679593523981687, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28122073600908665, 0.28122073600908637, 0.33761384317548165], 
reward next is 0.6624, 
noisyNet noise sample is [array([-1.0355533], dtype=float32), 0.8344826]. 
=============================================
[2019-03-26 01:29:29,322] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[55.410027]
 [55.07978 ]
 [54.514297]
 [53.70206 ]
 [55.18045 ]], R is [[55.4548645 ]
 [55.54121399]
 [54.9858017 ]
 [54.4359436 ]
 [53.8915863 ]].
[2019-03-26 01:29:41,219] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 1.850187e-32 0.000000e+00], sum to 1.0000
[2019-03-26 01:29:41,229] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4331
[2019-03-26 01:29:41,237] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7151951457534187, 6.911199999999999, 6.9112, 168.912956510431, 610664.6346321506, 610664.6346321513, 185481.3933606687], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2832600.0000, 
sim time next is 2833200.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7140109089499339, 6.9112, 6.9112, 168.912956510431, 609653.2496629966, 609653.2496629966, 185264.3689875097], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6512328157926022, 0.0, 0.0, 0.8294399451523027, 0.16934812490638795, 0.16934812490638795, 0.2765139835634473], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.0826868], dtype=float32), 0.30745375]. 
=============================================
[2019-03-26 01:29:50,029] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.7058982e-38 4.8353396e-21], sum to 1.0000
[2019-03-26 01:29:50,043] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9780
[2019-03-26 01:29:50,052] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1213093.678451794 W.
[2019-03-26 01:29:50,059] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.5, 94.0, 1.0, 1.0, 0.4026815358253437, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7051695394606251, 6.911200000000001, 6.9112, 168.9123519499542, 1213093.678451794, 1213093.678451793, 267721.7849520816], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2899800.0000, 
sim time next is 2900400.0000, 
raw observation next is [22.33333333333334, 94.0, 1.0, 2.0, 0.2440030196921875, 1.0, 1.0, 0.2440030196921875, 1.0, 2.0, 0.4265093128650242, 6.9112, 6.9112, 170.5573041426782, 1099447.862285859, 1099447.862285859, 291250.7156041535], 
processed observation next is [1.0, 0.5652173913043478, 0.2575039494470777, 0.94, 1.0, 1.0, 0.08916026468938251, 1.0, 0.5, 0.08916026468938251, 1.0, 1.0, 0.3006211132500295, 0.0, 0.0, 0.8375144448122397, 0.30540218396829416, 0.30540218396829416, 0.4347025606032142], 
reward next is 0.5653, 
noisyNet noise sample is [array([-0.6932622], dtype=float32), -2.3701158]. 
=============================================
[2019-03-26 01:29:51,260] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:29:51,270] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3719
[2019-03-26 01:29:51,277] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6822046340842732, 6.9112, 6.9112, 168.912956510431, 586100.983660108, 586100.983660108, 179580.9564602829], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2730600.0000, 
sim time next is 2731200.0000, 
raw observation next is [22.66666666666666, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6814317492730755, 6.911200000000001, 6.9112, 168.912956510431, 585125.0563520129, 585125.0563520123, 179446.8119918662], 
processed observation next is [0.0, 0.6086956521739131, 0.27330173775671385, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6115021332598481, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16253473787555914, 0.16253473787555897, 0.2678310626744272], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.89955205], dtype=float32), -2.2011826]. 
=============================================
[2019-03-26 01:29:52,070] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:29:52,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6635
[2019-03-26 01:29:52,085] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.560088250516019, 6.911199999999999, 6.9112, 168.912956510431, 490725.9357552922, 490725.9357552929, 160155.3658792593], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2950800.0000, 
sim time next is 2951400.0000, 
raw observation next is [21.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5593989279329894, 6.911200000000001, 6.9112, 168.912956510431, 490120.9203563386, 490120.9203563379, 160058.1500816606], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.46268161943047487, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13614470009898294, 0.13614470009898275, 0.23889276131591133], 
reward next is 0.7611, 
noisyNet noise sample is [array([-0.3183757], dtype=float32), -0.6355663]. 
=============================================
[2019-03-26 01:29:57,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.9502757e-34], sum to 1.0000
[2019-03-26 01:29:57,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7064
[2019-03-26 01:29:57,813] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6815330874777292, 6.911200000000001, 6.9112, 168.912956510431, 584371.8003753352, 584371.8003753345, 179465.2739098473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3117600.0000, 
sim time next is 3118200.0000, 
raw observation next is [22.83333333333334, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8532637302775073, 6.9112, 6.9112, 168.912956510431, 731826.4920069216, 731826.4920069216, 213391.5172606782], 
processed observation next is [1.0, 0.08695652173913043, 0.2812006319115327, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8210533296067162, 0.0, 0.0, 0.8294399451523027, 0.20328513666858933, 0.20328513666858933, 0.3184948018816093], 
reward next is 0.6815, 
noisyNet noise sample is [array([-0.81546664], dtype=float32), 1.3628213]. 
=============================================
[2019-03-26 01:29:58,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1380693e-33], sum to 1.0000
[2019-03-26 01:29:58,709] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2127
[2019-03-26 01:29:58,716] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5554504683553412, 6.9112, 6.9112, 168.912956510431, 488121.714566459, 488121.714566459, 159460.6323573001], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3020400.0000, 
sim time next is 3021000.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5551060697247132, 6.911199999999999, 6.9112, 168.912956510431, 487818.9925796859, 487818.9925796866, 159412.4995469643], 
processed observation next is [1.0, 1.0, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45744642649355266, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13550527571657942, 0.1355052757165796, 0.23792910380143925], 
reward next is 0.7621, 
noisyNet noise sample is [array([-0.96550506], dtype=float32), -1.7255036]. 
=============================================
[2019-03-26 01:29:58,737] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[88.82879 ]
 [88.641396]
 [88.58751 ]
 [88.3746  ]
 [88.26122 ]], R is [[88.76066589]
 [88.63506317]
 [88.51174927]
 [88.3891983 ]
 [88.26766205]].
[2019-03-26 01:30:00,659] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:00,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2320
[2019-03-26 01:30:00,675] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7111677321559977, 6.9112, 6.9112, 168.912956510431, 607224.7099600784, 607224.7099600784, 184744.8727462733], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2831400.0000, 
sim time next is 2832000.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.714517053268795, 6.911199999999999, 6.9112, 168.912956510431, 610085.4948599548, 610085.4948599555, 185357.078965324], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6518500649619452, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1694681930166541, 0.1694681930166543, 0.2766523566646627], 
reward next is 0.7233, 
noisyNet noise sample is [array([0.29726428], dtype=float32), -1.0699092]. 
=============================================
[2019-03-26 01:30:00,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.56437 ]
 [76.4213  ]
 [76.4459  ]
 [76.257935]
 [76.17136 ]], R is [[76.36999512]
 [76.33055115]
 [76.29239655]
 [76.25550079]
 [76.22141266]].
[2019-03-26 01:30:01,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:01,612] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2110
[2019-03-26 01:30:01,617] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6246889867510592, 6.9112, 6.9112, 168.912956510431, 541715.1016214245, 541715.1016214245, 169952.9574582687], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2876400.0000, 
sim time next is 2877000.0000, 
raw observation next is [22.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6356114737868201, 6.911200000000001, 6.9112, 168.912956510431, 551189.273692111, 551189.2736921103, 171704.2405172741], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5556237485205123, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15310813158114195, 0.15310813158114175, 0.25627498584667774], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.0673834], dtype=float32), 0.4409533]. 
=============================================
[2019-03-26 01:30:01,647] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.639885]
 [75.474976]
 [75.38268 ]
 [75.30173 ]
 [75.29316 ]], R is [[75.64311218]
 [75.63301849]
 [75.62477112]
 [75.61629486]
 [75.60732269]].
[2019-03-26 01:30:07,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:07,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5815
[2019-03-26 01:30:07,368] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 884351.8953833702 W.
[2019-03-26 01:30:07,376] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 1.0, 0.5730519346153865, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884351.8953833702, 884351.8953833702, 205871.1637705519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2969400.0000, 
sim time next is 2970000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.543320078629653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837069.7218474587, 837069.7218474587, 199993.023892666], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4497832272646422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23251936717984964, 0.23251936717984964, 0.2984970505860687], 
reward next is 0.7015, 
noisyNet noise sample is [array([1.2981194], dtype=float32), -0.06873784]. 
=============================================
[2019-03-26 01:30:07,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[58.69586 ]
 [60.93014 ]
 [61.938927]
 [63.35689 ]
 [64.736626]], R is [[59.9146843 ]
 [60.00826645]
 [60.03326416]
 [60.06937027]
 [60.14994812]].
[2019-03-26 01:30:08,370] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:08,385] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0767
[2019-03-26 01:30:08,391] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.33333333333333, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5581016480962161, 6.9112, 6.9112, 168.912956510431, 489744.1291214715, 489744.1291214715, 159853.9442968429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2936400.0000, 
sim time next is 2937000.0000, 
raw observation next is [20.16666666666667, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5550235053991579, 6.911199999999999, 6.9112, 168.912956510431, 487289.0878156127, 487289.0878156133, 159415.1424539598], 
processed observation next is [1.0, 1.0, 0.15481832543443946, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4573457382916559, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1353580799487813, 0.13535807994878146, 0.23793304843874596], 
reward next is 0.7621, 
noisyNet noise sample is [array([0.08482642], dtype=float32), 0.28169972]. 
=============================================
[2019-03-26 01:30:08,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[80.21453 ]
 [80.04615 ]
 [79.900085]
 [79.779564]
 [79.6724  ]], R is [[80.34254456]
 [80.30053711]
 [80.2582016 ]
 [80.2156601 ]
 [80.17303467]].
[2019-03-26 01:30:10,377] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:10,389] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2426
[2019-03-26 01:30:10,397] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6050518029746351, 6.9112, 6.9112, 168.912956510431, 526324.1206999633, 526324.1206999633, 166860.607992694], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6047955851145521, 6.911199999999999, 6.9112, 168.912956510431, 526098.6374142431, 526098.6374142438, 166821.3323748149], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.518043396481161, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1461385103928453, 0.1461385103928455, 0.24898706324599237], 
reward next is 0.7510, 
noisyNet noise sample is [array([-1.0516982], dtype=float32), -0.5421789]. 
=============================================
[2019-03-26 01:30:10,564] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:10,574] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5562
[2019-03-26 01:30:10,582] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5457150417199633, 6.9112, 6.9112, 168.912956510431, 479564.4341337842, 479564.4341337842, 158112.3479465352], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3030600.0000, 
sim time next is 3031200.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5460951390746781, 6.911200000000001, 6.9112, 168.912956510431, 479898.5322266188, 479898.5322266183, 158164.5090756135], 
processed observation next is [1.0, 0.08695652173913043, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4464574866764367, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13330514784072744, 0.1333051478407273, 0.23606643145613954], 
reward next is 0.7639, 
noisyNet noise sample is [array([0.92637396], dtype=float32), 2.3019693]. 
=============================================
[2019-03-26 01:30:11,791] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 3.3679031e-38 2.9851070e-30], sum to 1.0000
[2019-03-26 01:30:11,805] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9120
[2019-03-26 01:30:11,813] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.33333333333334, 98.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8409363486508417, 6.9112, 6.9112, 168.912956510431, 735439.0417609026, 735439.0417609026, 210489.376356331], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2983800.0000, 
sim time next is 2984400.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9354461251332752, 6.9112, 6.9112, 168.912956510431, 820787.9087465834, 820787.9087465834, 232115.1588027711], 
processed observation next is [1.0, 0.5652173913043478, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9212757623576525, 0.0, 0.0, 0.8294399451523027, 0.2279966413184954, 0.2279966413184954, 0.34644053552652404], 
reward next is 0.6536, 
noisyNet noise sample is [array([-0.28102726], dtype=float32), -0.87177634]. 
=============================================
[2019-03-26 01:30:14,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:14,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3383
[2019-03-26 01:30:14,498] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.83333333333334, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6086819914897945, 6.9112, 6.9112, 168.912956510431, 528301.0408940149, 528301.0408940149, 167438.2877268808], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3131400.0000, 
sim time next is 3132000.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6108486040429242, 6.9112, 6.9112, 168.912956510431, 529919.3263179462, 529919.3263179462, 167777.3816982773], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5254251268816148, 0.0, 0.0, 0.8294399451523027, 0.14719981286609615, 0.14719981286609615, 0.2504140025347423], 
reward next is 0.7496, 
noisyNet noise sample is [array([1.3176243], dtype=float32), 0.5510941]. 
=============================================
[2019-03-26 01:30:14,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.82975]
 [72.78654]
 [72.71962]
 [72.62577]
 [72.46481]], R is [[73.00508118]
 [73.0251236 ]
 [73.04541016]
 [73.0662384 ]
 [73.08797455]].
[2019-03-26 01:30:16,173] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0034804e-10 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 01:30:16,188] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9308
[2019-03-26 01:30:16,199] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333333, 73.66666666666667, 1.0, 1.0, 0.3065973132619953, 1.0, 1.0, 0.3065973132619953, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 856904.6887070008, 856904.6887070008, 251175.962738607], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3336000.0000, 
sim time next is 3336600.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.1995366097667658, 1.0, 2.0, 0.1995366097667658, 1.0, 1.0, 0.3465291033996701, 6.9112, 6.9112, 170.5573041426782, 836515.313683912, 836515.313683912, 269715.6884068024], 
processed observation next is [0.0, 0.6086956521739131, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.03558627682742865, 1.0, 1.0, 0.03558627682742865, 1.0, 0.5, 0.20308427243862204, 0.0, 0.0, 0.8375144448122397, 0.23236536491219778, 0.23236536491219778, 0.40256072896537676], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1629214], dtype=float32), -0.7848131]. 
=============================================
[2019-03-26 01:30:18,280] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.2540836e-34], sum to 1.0000
[2019-03-26 01:30:18,290] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0382
[2019-03-26 01:30:18,295] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9158887041140378, 6.9112, 6.9112, 168.912956510431, 750896.2687323521, 750896.2687323521, 226946.7643770709], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3365400.0000, 
sim time next is 3366000.0000, 
raw observation next is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9155903468379007, 6.9112, 6.9112, 168.912956510431, 750650.6161097612, 750650.6161097612, 226876.2606199396], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8970613985828058, 0.0, 0.0, 0.8294399451523027, 0.20851406003048922, 0.20851406003048922, 0.33862128450737256], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.1470343], dtype=float32), -0.1961066]. 
=============================================
[2019-03-26 01:30:18,317] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[63.86312]
 [64.13574]
 [64.09899]
 [64.03136]
 [64.17422]], R is [[64.04545593]
 [64.06627655]
 [64.08664703]
 [64.10652161]
 [64.12631989]].
[2019-03-26 01:30:27,768] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:30:27,780] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7958
[2019-03-26 01:30:27,790] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8134942559918039, 6.9112, 6.9112, 168.912956510431, 679835.1618634638, 679835.1618634638, 204589.4516941792], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3196800.0000, 
sim time next is 3197400.0000, 
raw observation next is [25.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8135958057854448, 6.9112, 6.9112, 168.912956510431, 679920.5693278117, 679920.5693278117, 204610.7451715409], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7726778119334691, 0.0, 0.0, 0.8294399451523027, 0.18886682481328104, 0.18886682481328104, 0.3053891718978223], 
reward next is 0.6946, 
noisyNet noise sample is [array([2.587226], dtype=float32), 0.045073524]. 
=============================================
[2019-03-26 01:30:33,227] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 01:30:33,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:30:33,231] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:30:33,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:30:33,232] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:30:33,233] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:30:33,234] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:30:33,234] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:30:33,235] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:30:33,236] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:30:33,236] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:30:33,260] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-26 01:30:33,285] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-26 01:30:33,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-26 01:30:33,340] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-26 01:30:33,363] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-26 01:30:35,788] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:30:35,790] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.68048067166667, 84.74116276666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.019434671088543, 7.528134375510553, 6.9112, 168.9098886093524, 1333883.952155038, 896217.4222386128, 253360.1230479271]
[2019-03-26 01:30:35,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:30:35,793] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.057965207135968755
[2019-03-26 01:30:35,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1333883.952155038 W.
[2019-03-26 01:30:38,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:30:38,161] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.49239164666667, 82.57160783666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5482175196595902, 6.9112, 6.9112, 168.912956510431, 483438.766815418, 483438.766815418, 158398.1861888545]
[2019-03-26 01:30:38,161] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:30:38,166] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9868807048434644
[2019-03-26 01:30:42,353] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:30:42,354] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.69004551166667, 55.37207300333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6277853516249386, 6.911199999999999, 6.9112, 168.912956510431, 566307.7312989456, 566307.7312989463, 169336.5431727855]
[2019-03-26 01:30:42,354] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:30:42,358] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07505018307737743
[2019-03-26 01:31:09,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:31:09,487] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.06666666666667, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6862902526713661, 6.9112, 6.9112, 168.912956510431, 589423.0103120767, 589423.0103120767, 180296.559235202]
[2019-03-26 01:31:09,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:31:09,493] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6745695830187448
[2019-03-26 01:31:39,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:31:39,421] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.60329850666666, 84.25136790666667, 1.0, 2.0, 0.6354363530500667, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994651891624347, 6.9112, 168.9123765971081, 1776748.456832406, 1717545.015980077, 371337.2685927473]
[2019-03-26 01:31:39,422] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:31:39,424] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0868345e-24 9.4361640e-36], sampled 0.9432028254733408
[2019-03-26 01:31:39,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1776748.456832406 W.
[2019-03-26 01:32:24,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:32:24,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.33333333333334, 44.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9917525042124274, 6.911200000000001, 6.9112, 168.9129223994333, 806763.503548577, 806763.5035485764, 245333.8704186863]
[2019-03-26 01:32:24,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:32:24,075] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7381926968765647
[2019-03-26 01:32:36,006] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.88670945], dtype=float32), 0.2011467]
[2019-03-26 01:32:36,007] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.51666666666667, 64.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7607611677055012, 6.9112, 6.9112, 168.912956510431, 644495.5615245127, 644495.5615245127, 194076.6399575501]
[2019-03-26 01:32:36,008] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:32:36,014] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2473059805283082
[2019-03-26 01:32:41,485] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7360.5730 3106361995.2857 1981.0000
[2019-03-26 01:32:42,153] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7349.9648 3317119289.6452 1968.0000
[2019-03-26 01:32:42,748] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7114.4526 3179033481.4995 2256.0000
[2019-03-26 01:32:42,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7976.3449 2987753151.5296 1408.0000
[2019-03-26 01:32:42,804] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8077.1858 2939565536.9863 1321.0000
[2019-03-26 01:32:43,817] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1575000, evaluation results [1575000.0, 7349.9647583988735, 3317119289.6452274, 1968.0, 7360.572981170721, 3106361995.285714, 1981.0, 8077.185842029526, 2939565536.9863462, 1321.0, 7114.452553740906, 3179033481.4995136, 2256.0, 7976.344883075315, 2987753151.52962, 1408.0]
[2019-03-26 01:33:11,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 01:33:11,396] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1438
[2019-03-26 01:33:11,403] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.012954679236881, 1.0, 2.0, 1.012954679236881, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2833586.500272607, 2833586.500272607, 536863.994713661], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3773400.0000, 
sim time next is 3774000.0000, 
raw observation next is [33.0, 63.00000000000001, 1.0, 2.0, 1.001182281662596, 1.0, 2.0, 1.001182281662596, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2800618.095257174, 2800618.095257174, 529701.3739151458], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.6300000000000001, 1.0, 1.0, 1.0014244357380673, 1.0, 1.0, 1.0014244357380673, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7779494709047706, 0.7779494709047706, 0.7905990655449937], 
reward next is 0.2094, 
noisyNet noise sample is [array([0.36933476], dtype=float32), -2.2032404]. 
=============================================
[2019-03-26 01:33:11,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[57.958965]
 [57.732506]
 [57.352943]
 [57.139698]
 [56.793358]], R is [[57.68725204]
 [57.30908966]
 [56.92927933]
 [56.56647491]
 [56.20478439]].
[2019-03-26 01:33:14,338] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2455216e-31 5.3036377e-36 0.0000000e+00 1.0000000e+00 4.4237153e-31], sum to 1.0000
[2019-03-26 01:33:14,351] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3429
[2019-03-26 01:33:14,356] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.4011238865172908, 1.0, 2.0, 0.4011238865172908, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1121233.851560505, 1121233.851560505, 272403.796485014], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3744000.0000, 
sim time next is 3744600.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.4723502985032603, 1.0, 2.0, 0.4723502985032603, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1320450.721576276, 1320450.721576276, 291881.6874867354], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.3642774680762173, 1.0, 1.0, 0.3642774680762173, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3667918671045211, 0.3667918671045211, 0.43564430968169465], 
reward next is 0.5644, 
noisyNet noise sample is [array([-0.8896559], dtype=float32), 1.2680202]. 
=============================================
[2019-03-26 01:33:24,365] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.232994e-24 0.000000e+00 0.000000e+00 2.723448e-32 1.000000e+00], sum to 1.0000
[2019-03-26 01:33:24,372] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3383
[2019-03-26 01:33:24,378] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.1840178210810243, 1.0, 2.0, 0.1840178210810243, 1.0, 2.0, 0.3176303333558667, 6.9112, 6.9112, 170.5573041426782, 771432.6548023792, 771432.6548023792, 265512.6532927194], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3880800.0000, 
sim time next is 3881400.0000, 
raw observation next is [29.0, 79.83333333333334, 1.0, 2.0, 0.1859187324171868, 1.0, 2.0, 0.1859187324171868, 1.0, 2.0, 0.3215241832961409, 6.9112, 6.9112, 170.5573041426782, 779404.4799700173, 779404.4799700173, 266023.9087018972], 
processed observation next is [0.0, 0.9565217391304348, 0.5734597156398105, 0.7983333333333335, 1.0, 1.0, 0.01917919568335756, 1.0, 1.0, 0.01917919568335756, 1.0, 1.0, 0.1725904674343182, 0.0, 0.0, 0.8375144448122397, 0.21650124443611593, 0.21650124443611593, 0.39705061000283165], 
reward next is 0.6029, 
noisyNet noise sample is [array([1.637716], dtype=float32), -1.3170986]. 
=============================================
[2019-03-26 01:33:33,626] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 01:33:33,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9448
[2019-03-26 01:33:33,644] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.271441383583465, 1.0, 2.0, 0.271441383583465, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 758613.1059536219, 758613.1059536219, 244608.7090974728], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4036800.0000, 
sim time next is 4037400.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.2705960356921671, 1.0, 2.0, 0.2705960356921671, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 756249.7298710267, 756249.7298710273, 244459.7150963367], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.12120004300261097, 1.0, 1.0, 0.12120004300261097, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.21006936940861853, 0.2100693694086187, 0.36486524641244283], 
reward next is 0.6351, 
noisyNet noise sample is [array([0.55663484], dtype=float32), 1.2763637]. 
=============================================
[2019-03-26 01:33:42,910] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9030439e-26 8.5975984e-35 1.4003975e-33 1.0000000e+00 2.7368694e-08], sum to 1.0000
[2019-03-26 01:33:42,923] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2985
[2019-03-26 01:33:42,935] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.5349077624427413, 1.0, 2.0, 0.5349077624427413, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1495451.500519766, 1495451.500519766, 311460.6193812962], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4165800.0000, 
sim time next is 4166400.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.4865055069199624, 1.0, 2.0, 0.4865055069199624, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1360046.573019084, 1360046.573019084, 296121.7140059994], 
processed observation next is [1.0, 0.21739130434782608, 0.5260663507109005, 0.89, 1.0, 1.0, 0.38133193604814747, 1.0, 1.0, 0.38133193604814747, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.37779071472752335, 0.37779071472752335, 0.44197270747164086], 
reward next is 0.5580, 
noisyNet noise sample is [array([-0.0782735], dtype=float32), -0.7284057]. 
=============================================
[2019-03-26 01:33:47,000] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 01:33:47,012] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6906
[2019-03-26 01:33:47,025] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3361680.002498693 W.
[2019-03-26 01:33:47,032] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 61.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.541938939293342, 6.9112, 170.5573041426782, 3361680.002498693, 2909856.043463217, 550194.570933194], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [34.5, 64.5, 1.0, 2.0, 0.9283255628633755, 1.0, 2.0, 0.7847528209459502, 1.0, 1.0, 1.03, 7.00511574232489, 6.9112, 170.5573041426782, 3293445.333232985, 3226169.666757521, 603153.7438595287], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.645, 1.0, 1.0, 0.9136452564618982, 1.0, 1.0, 0.7406660493324702, 1.0, 0.5, 1.0365853658536586, 0.009391574232489042, 0.0, 0.8375144448122397, 0.9148459258980514, 0.8961582407659782, 0.9002294684470578], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2982152], dtype=float32), -0.5446134]. 
=============================================
[2019-03-26 01:33:51,535] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 01:33:51,537] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:33:51,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:33:51,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:33:51,538] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:33:51,539] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:33:51,540] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:33:51,540] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:33:51,539] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:33:51,541] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:33:51,541] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:33:51,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-26 01:33:51,578] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-26 01:33:51,604] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-26 01:33:51,626] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-26 01:33:51,664] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-26 01:33:55,039] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:33:55,041] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.649296085, 99.91323871, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2137750390820075, 6.911200000000001, 6.9112, 171.5212843490159, 552655.3883125698, 552655.3883125691, 238275.4172058318]
[2019-03-26 01:33:55,042] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:33:55,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.4588734634086069
[2019-03-26 01:34:37,962] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:34:37,963] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 65.0, 1.0, 2.0, 0.2410503925350232, 1.0, 2.0, 0.2410503925350232, 1.0, 2.0, 0.4105955697027844, 6.911200000000001, 6.9112, 178.6582176852504, 1010613.539585757, 1010613.539585757, 283795.9951872596]
[2019-03-26 01:34:37,964] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:34:37,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6157806e-34 1.0000000e+00], sampled 0.07053570924351027
[2019-03-26 01:34:45,666] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:34:45,672] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.4, 61.0, 1.0, 2.0, 0.1777795579394221, 1.0, 2.0, 0.1777795579394221, 1.0, 2.0, 0.303269058567218, 6.911199999999999, 6.9112, 178.6582176852504, 745260.0144882946, 745260.0144882952, 265811.2778483553]
[2019-03-26 01:34:45,672] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:34:45,674] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.625079050072512
[2019-03-26 01:35:03,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:35:03,305] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.66666666666666, 56.83333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.881555985152606, 6.9112, 170.5573041426782, 3605244.954328778, 2910139.490606145, 548077.613277731]
[2019-03-26 01:35:03,306] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:35:03,311] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9930036e-01 6.9966179e-04], sampled 0.18563497748468227
[2019-03-26 01:35:03,313] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3605244.954328778 W.
[2019-03-26 01:35:21,037] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:35:21,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 74.66666666666667, 1.0, 2.0, 0.1857173866927041, 1.0, 2.0, 0.1857173866927041, 1.0, 2.0, 0.3201022315574686, 6.9112, 6.9112, 169.0403247858759, 778562.6323418191, 778562.6323418191, 265634.0617710845]
[2019-03-26 01:35:21,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:35:21,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.3073300166242948
[2019-03-26 01:35:25,427] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:35:25,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.63333333333333, 75.83333333333333, 1.0, 2.0, 0.1746330487514225, 1.0, 2.0, 0.1746330487514225, 1.0, 2.0, 0.2976020937695332, 6.9112, 6.9112, 170.5573041426782, 732076.7352408511, 732076.7352408511, 263038.8889285728]
[2019-03-26 01:35:25,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:35:25,439] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1672224e-35 0.0000000e+00 0.0000000e+00 3.0399320e-38 1.0000000e+00], sampled 0.308683887175802
[2019-03-26 01:35:36,464] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7606642], dtype=float32), 0.22642405]
[2019-03-26 01:35:36,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.4, 66.66666666666666, 1.0, 2.0, 0.1899482468194345, 1.0, 2.0, 0.1899482468194345, 1.0, 2.0, 0.3291675147859891, 6.9112, 6.9112, 169.0403247858759, 796305.8467024361, 796305.8467024361, 266809.0482389012]
[2019-03-26 01:35:36,466] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:35:36,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.36931623219778453
[2019-03-26 01:36:01,415] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 4091.3389 3604098116.2414 47.0000
[2019-03-26 01:36:01,612] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 4120.7677 3842129313.4865 429.0000
[2019-03-26 01:36:01,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 4051.9280 3586457598.5545 87.0000
[2019-03-26 01:36:01,674] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 4978.3767 3686702030.6321 16.0000
[2019-03-26 01:36:01,817] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 4863.7189 3663980095.0355 78.0000
[2019-03-26 01:36:02,834] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1600000, evaluation results [1600000.0, 4120.767662221863, 3842129313.48647, 429.0, 4863.718887068733, 3663980095.0354505, 78.0, 4091.3389207997748, 3604098116.241374, 47.0, 4978.376688927958, 3686702030.63214, 16.0, 4051.928007877201, 3586457598.554537, 87.0]
[2019-03-26 01:36:08,584] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 1.082236e-36 6.709941e-01 3.290059e-01], sum to 1.0000
[2019-03-26 01:36:08,593] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9580
[2019-03-26 01:36:08,600] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.3258202426676786, 1.0, 2.0, 0.3258202426676786, 1.0, 2.0, 0.5658420111129848, 6.9112, 6.9112, 170.5573041426782, 1366270.218101364, 1366270.218101364, 315629.1553994599], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.3287103503373133, 1.0, 2.0, 0.3287103503373133, 1.0, 2.0, 0.570861172361929, 6.9112, 6.9112, 170.5573041426782, 1378397.171217401, 1378397.171217401, 316938.4573717318], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.8483333333333333, 1.0, 1.0, 0.19121728956302803, 1.0, 1.0, 0.19121728956302803, 1.0, 1.0, 0.4766599662950354, 0.0, 0.0, 0.8375144448122397, 0.38288810311594473, 0.38288810311594473, 0.473042473689152], 
reward next is 0.5270, 
noisyNet noise sample is [array([-1.1219994], dtype=float32), -0.08967984]. 
=============================================
[2019-03-26 01:36:11,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 01:36:11,758] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6222
[2019-03-26 01:36:11,766] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.5, 73.0, 1.0, 2.0, 0.9929293016109746, 1.0, 2.0, 0.9929293016109746, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2777506.279678359, 2777506.279678358, 524744.5562795339], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [33.66666666666666, 72.33333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.086815944165276, 6.9112, 170.5573041426782, 3035277.125190073, 2909476.28078461, 552747.0936568228], 
processed observation next is [1.0, 0.391304347826087, 0.7946287519747232, 0.7233333333333333, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.017561594416527625, 0.0, 0.8375144448122397, 0.8431325347750204, 0.8081878557735028, 0.8249956621743624], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.09238099], dtype=float32), 1.7509494]. 
=============================================
[2019-03-26 01:36:15,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9730357e-22 1.0000000e+00], sum to 1.0000
[2019-03-26 01:36:15,308] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0025
[2019-03-26 01:36:15,321] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.2057414930510894, 1.0, 2.0, 0.2057414930510894, 1.0, 2.0, 0.3573049336782819, 6.9112, 6.9112, 170.5573041426782, 862538.4436483484, 862538.4436483484, 271455.7316673507], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4402800.0000, 
sim time next is 4403400.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.2060978884201204, 1.0, 2.0, 0.2060978884201204, 1.0, 2.0, 0.3579238745725392, 6.9112, 6.9112, 170.5573041426782, 864033.1763562466, 864033.1763562466, 271557.2679978667], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.043491431831470334, 1.0, 1.0, 0.043491431831470334, 1.0, 1.0, 0.21698033484456, 0.0, 0.0, 0.8375144448122397, 0.24000921565451294, 0.24000921565451294, 0.4053093552206965], 
reward next is 0.5947, 
noisyNet noise sample is [array([-0.64617914], dtype=float32), 1.2722821]. 
=============================================
[2019-03-26 01:36:21,865] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:36:21,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5264
[2019-03-26 01:36:21,887] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.1708603750866964, 1.0, 2.0, 0.1708603750866964, 1.0, 2.0, 0.2896963416398568, 6.9112, 6.9112, 170.5573041426782, 716256.0826918965, 716256.0826918965, 262090.4114928085], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4518600.0000, 
sim time next is 4519200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.1709519156278983, 1.0, 2.0, 0.1709519156278983, 1.0, 2.0, 0.2898555998304558, 6.911199999999999, 6.9112, 170.5573041426782, 716639.9538675573, 716639.9538675579, 262111.7379920187], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.0011468862986726345, 1.0, 1.0, 0.0011468862986726345, 1.0, 1.0, 0.13397024369567778, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19906665385209926, 0.19906665385209943, 0.39121154924181895], 
reward next is 0.6088, 
noisyNet noise sample is [array([0.1706184], dtype=float32), 0.3115826]. 
=============================================
[2019-03-26 01:36:28,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:36:28,238] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1546
[2019-03-26 01:36:28,244] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 88.16666666666667, 1.0, 2.0, 0.1902959682971571, 1.0, 2.0, 0.1902959682971571, 1.0, 2.0, 0.3304811650937912, 6.9112, 6.9112, 170.5573041426782, 797761.457905602, 797761.457905602, 267222.9934122812], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.1917347125131463, 1.0, 2.0, 0.1917347125131463, 1.0, 2.0, 0.3329797879969815, 6.9112, 6.9112, 170.5573041426782, 803795.2419801167, 803795.2419801167, 267603.2842249718], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.026186400618248548, 1.0, 1.0, 0.026186400618248548, 1.0, 1.0, 0.18656071706948965, 0.0, 0.0, 0.8375144448122397, 0.22327645610558797, 0.22327645610558797, 0.39940788690294293], 
reward next is 0.6006, 
noisyNet noise sample is [array([-0.63231033], dtype=float32), -0.6139676]. 
=============================================
[2019-03-26 01:36:32,977] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:36:32,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4232
[2019-03-26 01:36:32,997] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.33333333333334, 82.33333333333334, 1.0, 2.0, 0.17136894109604, 1.0, 2.0, 0.17136894109604, 1.0, 2.0, 0.2909429075253319, 6.911199999999999, 6.9112, 170.5573041426782, 718388.7333021365, 718388.7333021371, 262224.8457528187], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4747200.0000, 
sim time next is 4747800.0000, 
raw observation next is [27.16666666666666, 83.16666666666666, 1.0, 2.0, 0.1709161874671958, 1.0, 2.0, 0.1709161874671958, 1.0, 2.0, 0.2900055537590502, 6.9112, 6.9112, 170.5573041426782, 716490.1294032346, 716490.1294032346, 262112.6511544445], 
processed observation next is [1.0, 0.9565217391304348, 0.4865718799368086, 0.8316666666666666, 1.0, 1.0, 0.0011038403219226444, 1.0, 1.0, 0.0011038403219226444, 1.0, 1.0, 0.1341531143403051, 0.0, 0.0, 0.8375144448122397, 0.19902503594534293, 0.19902503594534293, 0.39121291217081267], 
reward next is 0.6088, 
noisyNet noise sample is [array([0.9564179], dtype=float32), -0.18164401]. 
=============================================
[2019-03-26 01:36:36,916] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:36:36,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5286
[2019-03-26 01:36:36,938] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.5662035827107947, 1.0, 2.0, 0.9833083767825556, 6.9112, 6.9112, 170.5573041426782, 2375367.316592513, 2375367.316592513, 463945.6516273014], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726800.0000, 
sim time next is 4727400.0000, 
raw observation next is [30.83333333333334, 70.0, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.2561871253093548, 1.0, 2.0, 0.4449123142853757, 6.9112, 6.9112, 170.5573041426782, 1074129.641498495, 1074129.641498495, 287578.1815063162], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.10383991001127081, 1.0, 1.0, 0.32306379790899475, 0.0, 0.0, 0.8375144448122397, 0.29836934486069305, 0.29836934486069305, 0.4292211664273376], 
reward next is 0.5708, 
noisyNet noise sample is [array([0.11951395], dtype=float32), 1.090767]. 
=============================================
[2019-03-26 01:36:48,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.405243e-11], sum to 1.0000
[2019-03-26 01:36:48,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5483
[2019-03-26 01:36:48,498] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7994072433436225, 6.9112, 6.9112, 168.912956510431, 671499.0081530006, 671499.0081530006, 201743.0173660473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5119200.0000, 
sim time next is 5119800.0000, 
raw observation next is [26.16666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8047487769092135, 6.9112, 6.9112, 168.912956510431, 675352.8243392322, 675352.8243392322, 202833.3642284786], 
processed observation next is [0.0, 0.2608695652173913, 0.4391785150078992, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7618887523283091, 0.0, 0.0, 0.8294399451523027, 0.18759800676089783, 0.18759800676089783, 0.3027363645201173], 
reward next is 0.6973, 
noisyNet noise sample is [array([1.3710005], dtype=float32), 1.6820465]. 
=============================================
[2019-03-26 01:36:58,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 1.3216898e-31 0.0000000e+00 8.6937886e-23 6.2247589e-27], sum to 1.0000
[2019-03-26 01:36:58,707] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7252
[2019-03-26 01:36:58,715] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2452272.543864462 W.
[2019-03-26 01:36:58,721] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8767756502307338, 1.0, 2.0, 0.8767756502307338, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2452272.543864462, 2452272.543864462, 458980.138293697], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5223600.0000, 
sim time next is 5224200.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.8995498551386768, 1.0, 2.0, 0.8995498551386768, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2516034.334230165, 2516034.334230166, 471246.5991165988], 
processed observation next is [1.0, 0.4782608695652174, 0.6761453396524489, 0.695, 1.0, 1.0, 0.8789757290827431, 1.0, 1.0, 0.8789757290827431, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6988984261750458, 0.698898426175046, 0.7033531330098489], 
reward next is 0.2966, 
noisyNet noise sample is [array([2.1019166], dtype=float32), -0.12167532]. 
=============================================
[2019-03-26 01:37:05,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.9802901e-02 1.0710159e-12 3.9836406e-17 3.3931261e-10 9.4019711e-01], sum to 1.0000
[2019-03-26 01:37:05,858] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0903
[2019-03-26 01:37:05,864] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.550284396385779, 6.9112, 168.9034234212145, 3447508.251548234, 2284750.995079651, 471933.8409391992], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5302800.0000, 
sim time next is 5303400.0000, 
raw observation next is [32.81666666666667, 68.33333333333334, 1.0, 2.0, 0.7761218432501518, 1.0, 1.0, 0.7086509611393385, 1.0, 2.0, 1.03, 7.005103735060347, 6.9112, 170.5573041426782, 2973681.324415619, 2906414.259232171, 546450.4403164027], 
processed observation next is [1.0, 0.391304347826087, 0.7543443917851502, 0.6833333333333335, 1.0, 1.0, 0.7302672810242792, 1.0, 0.5, 0.6489770616136608, 1.0, 1.0, 1.0365853658536586, 0.009390373506034689, 0.0, 0.8375144448122397, 0.8260225901154498, 0.8073372942311586, 0.8155976721140339], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45376143], dtype=float32), -0.9582052]. 
=============================================
[2019-03-26 01:37:09,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 8.271357e-34 0.000000e+00 4.793536e-30 1.948273e-36], sum to 1.0000
[2019-03-26 01:37:09,129] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5767
[2019-03-26 01:37:09,139] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2711983.102037278 W.
[2019-03-26 01:37:09,148] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.9, 64.66666666666667, 1.0, 2.0, 0.9695308515773379, 1.0, 2.0, 0.9695308515773379, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2711983.102037278, 2711983.102037278, 510855.4189336544], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5392200.0000, 
sim time next is 5392800.0000, 
raw observation next is [34.1, 64.0, 1.0, 2.0, 0.6784019862827794, 1.0, 2.0, 0.6597910326556523, 1.0, 1.0, 1.03, 7.005096029234503, 6.9112, 170.5573041426782, 2768425.325337674, 2701163.780150731, 514595.1571752484], 
processed observation next is [1.0, 0.43478260869565216, 0.8151658767772513, 0.64, 1.0, 1.0, 0.6125325135937101, 1.0, 1.0, 0.5901096778983762, 1.0, 0.5, 1.0365853658536586, 0.00938960292345028, 0.0, 0.8375144448122397, 0.7690070348160205, 0.750323272264092, 0.7680524733958931], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.61261785], dtype=float32), 0.052357357]. 
=============================================
[2019-03-26 01:37:10,546] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 01:37:10,548] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:37:10,549] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:37:10,550] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:37:10,550] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:37:10,552] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:37:10,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:37:10,551] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:37:10,555] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:37:10,556] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:37:10,557] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:37:10,587] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-26 01:37:10,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-26 01:37:10,642] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-26 01:37:10,666] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-26 01:37:10,688] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-26 01:37:21,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:37:21,126] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.9, 56.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.553413332730558, 6.9112, 6.9112, 168.912956510431, 487913.5590945772, 487913.5590945772, 159121.6389514449]
[2019-03-26 01:37:21,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:37:21,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24215395081608027
[2019-03-26 01:37:26,059] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:37:26,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.93333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5699661657385768, 6.911199999999999, 6.9112, 168.912956510431, 497794.5797429986, 497794.5797429992, 161602.3491272733]
[2019-03-26 01:37:26,061] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:37:26,067] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.31231447679837254
[2019-03-26 01:37:32,479] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:37:32,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.05, 44.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4299653444249379, 6.911199999999999, 6.9112, 168.912956510431, 387511.750855148, 387511.7508551487, 143486.0816945118]
[2019-03-26 01:37:32,481] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:37:32,483] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2957595962125996
[2019-03-26 01:37:41,807] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:37:41,808] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.88333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6857863068287045, 6.9112, 6.9112, 168.912956510431, 587900.0624692708, 587900.0624692708, 180209.1780864084]
[2019-03-26 01:37:41,809] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:37:41,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.34411696797498703
[2019-03-26 01:38:04,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:04,399] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.44503000666667, 74.31387482666666, 1.0, 1.0, 0.6142302670788147, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912859956253, 858355.8608824984, 858355.8608824977, 203426.9731342029]
[2019-03-26 01:38:04,400] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:38:04,401] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.20411136406434394
[2019-03-26 01:38:10,708] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:10,711] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.29517988833333, 76.44467062166667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.273470795607432, 6.9112, 168.9109583926361, 1710935.278190675, 1453930.947599076, 311348.9176901592]
[2019-03-26 01:38:10,711] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:38:10,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.9674031  0.         0.         0.         0.03259684], sampled 0.27018680274273577
[2019-03-26 01:38:10,715] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1710935.278190675 W.
[2019-03-26 01:38:14,791] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:14,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9600106098544013, 6.911199999999999, 6.9112, 168.912956510431, 781678.0245270559, 781678.0245270566, 237378.0292224327]
[2019-03-26 01:38:14,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:38:14,795] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8000113049564069
[2019-03-26 01:38:27,223] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:27,223] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.68863845166667, 94.78396854833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 12.75787218831078, 6.9112, 169.2162648505174, 5008298.223739822, 853019.6997921446, 257473.6898061095]
[2019-03-26 01:38:27,224] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:38:27,230] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.28744415046520544
[2019-03-26 01:38:27,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5008298.223739822 W.
[2019-03-26 01:38:36,673] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:36,677] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.35, 78.16666666666666, 1.0, 2.0, 1.006630350644434, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993715758283, 6.9112, 168.9123159100904, 2304282.446435351, 2237032.776973043, 465348.382653164]
[2019-03-26 01:38:36,677] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:38:36,679] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.0506151550310634
[2019-03-26 01:38:36,679] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2304282.446435351 W.
[2019-03-26 01:38:43,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:38:43,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.2, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9045256241859578, 6.911200000000001, 6.9112, 168.912956510431, 742728.2732834636, 742728.2732834629, 224328.0875742401]
[2019-03-26 01:38:43,525] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:38:43,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6710743651044035
[2019-03-26 01:39:12,906] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8690514], dtype=float32), 0.20300913]
[2019-03-26 01:39:12,908] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.69303182, 91.00268346499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5736020954075646, 6.911199999999999, 6.9112, 168.912956510431, 503208.0526951316, 503208.0526951322, 162069.9653429087]
[2019-03-26 01:39:12,909] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:39:12,912] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5044254891747515
[2019-03-26 01:39:19,026] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7133.6488 3196516063.9932 2205.0000
[2019-03-26 01:39:19,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7340.5074 3329144471.2886 1941.0000
[2019-03-26 01:39:20,030] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8102.1725 2952221790.3779 1238.0000
[2019-03-26 01:39:20,214] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7986.4653 2998430570.2992 1344.0000
[2019-03-26 01:39:20,294] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7395.8882 3124741897.8085 1887.0000
[2019-03-26 01:39:21,311] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1625000, evaluation results [1625000.0, 7340.507366483596, 3329144471.2885776, 1941.0, 7395.888157108751, 3124741897.8085403, 1887.0, 8102.172505259201, 2952221790.3778687, 1238.0, 7133.648785046582, 3196516063.9932055, 2205.0, 7986.46529339055, 2998430570.2991805, 1344.0]
[2019-03-26 01:39:22,627] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:39:22,637] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0164
[2019-03-26 01:39:22,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 902256.1227569138 W.
[2019-03-26 01:39:22,654] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.014700629306528, 6.9112, 168.9121452195253, 902256.1227569138, 828829.5667804204, 254812.2302914522], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5446800.0000, 
sim time next is 5447400.0000, 
raw observation next is [28.41666666666666, 89.66666666666667, 1.0, 1.0, 0.2007245160508354, 1.0, 1.0, 0.2007245160508354, 1.0, 2.0, 0.3485921037684878, 6.911199999999999, 6.9112, 170.5573041426782, 841497.3149488346, 841497.3149488353, 270044.4340702679], 
processed observation next is [1.0, 0.043478260869565216, 0.5458135860979461, 0.8966666666666667, 1.0, 0.5, 0.037017489217873976, 1.0, 0.5, 0.037017489217873976, 1.0, 1.0, 0.20560012654693635, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.23374925415245407, 0.23374925415245426, 0.4030513941347282], 
reward next is 0.5969, 
noisyNet noise sample is [array([-2.8150482], dtype=float32), 0.7979044]. 
=============================================
[2019-03-26 01:39:51,271] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0162789e-33 3.8758661e-33 0.0000000e+00 3.7584977e-32 1.0000000e+00], sum to 1.0000
[2019-03-26 01:39:51,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8046
[2019-03-26 01:39:51,291] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.7, 62.33333333333334, 1.0, 2.0, 0.7049122102652852, 1.0, 2.0, 0.673046144646905, 1.0, 2.0, 1.03, 7.00509811948238, 6.9112, 170.5573041426782, 2824105.479966694, 2756842.437450171, 522884.611231937], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5836800.0000, 
sim time next is 5837400.0000, 
raw observation next is [32.75, 62.0, 1.0, 2.0, 0.7726737925456733, 1.0, 2.0, 0.7069269357870992, 1.0, 2.0, 1.03, 7.005103463116764, 6.9112, 170.5573041426782, 2966438.280748544, 2899171.410369347, 545264.6090767307], 
processed observation next is [1.0, 0.5652173913043478, 0.7511848341232228, 0.62, 1.0, 1.0, 0.7261130030670762, 1.0, 1.0, 0.6468999226350592, 1.0, 1.0, 1.0365853658536586, 0.009390346311676368, 0.0, 0.8375144448122397, 0.8240106335412621, 0.805325391769263, 0.8138277747413891], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.115143], dtype=float32), 0.98795587]. 
=============================================
[2019-03-26 01:39:53,641] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:39:53,650] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5040
[2019-03-26 01:39:53,662] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.9, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8572892607107457, 6.911200000000001, 6.9112, 168.912956510431, 712568.7404214252, 712568.7404214245, 213914.9073891063], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5716800.0000, 
sim time next is 5717400.0000, 
raw observation next is [25.86666666666667, 90.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8574781383646807, 6.911199999999999, 6.9112, 168.912956510431, 712827.7173717367, 712827.7173717372, 213959.8215317876], 
processed observation next is [0.0, 0.17391304347826086, 0.42496050552922615, 0.9033333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8261928516642446, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19800769926992684, 0.198007699269927, 0.3193430172116233], 
reward next is 0.6807, 
noisyNet noise sample is [array([0.91806257], dtype=float32), 1.8802395]. 
=============================================
[2019-03-26 01:39:54,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.336641e-12], sum to 1.0000
[2019-03-26 01:39:54,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6543
[2019-03-26 01:39:54,698] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 69.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9626306414585646, 6.911200000000001, 6.9112, 168.912956510431, 780680.4913811622, 780680.4913811616, 237869.4130058261], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5766600.0000, 
sim time next is 5767200.0000, 
raw observation next is [30.7, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9609302768663373, 6.9112, 6.9112, 168.912956510431, 779819.2208083083, 779819.2208083083, 237474.8180284696], 
processed observation next is [0.0, 0.782608695652174, 0.6540284360189573, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9523539961784601, 0.0, 0.0, 0.8294399451523027, 0.2166164502245301, 0.2166164502245301, 0.35444002690816356], 
reward next is 0.6456, 
noisyNet noise sample is [array([2.360789], dtype=float32), -1.1754955]. 
=============================================
[2019-03-26 01:39:58,957] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 2.150973e-12], sum to 1.0000
[2019-03-26 01:39:58,969] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3434
[2019-03-26 01:39:58,977] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.46666666666667, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9593886417648153, 6.9112, 6.9112, 168.912956510431, 779158.9681857496, 779158.9681857496, 237123.6492453018], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5955600.0000, 
sim time next is 5956200.0000, 
raw observation next is [27.4, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.961619421313245, 6.911200000000001, 6.9112, 168.912956510431, 780923.6634958013, 780923.6634958007, 237673.4673839755], 
processed observation next is [1.0, 0.9565217391304348, 0.4976303317535545, 0.905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9531944162356646, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2169232398599448, 0.21692323985994463, 0.35473651848354554], 
reward next is 0.6453, 
noisyNet noise sample is [array([0.47901273], dtype=float32), 0.5872202]. 
=============================================
[2019-03-26 01:39:59,465] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 8.6926214e-32], sum to 1.0000
[2019-03-26 01:39:59,475] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5138
[2019-03-26 01:39:59,481] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.6, 82.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9631300350449474, 6.9112, 6.9112, 168.912956510431, 780578.3316076435, 780578.3316076435, 237966.8243985176], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [28.5, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.956700976527254, 6.9112, 6.9112, 168.912956510431, 776268.6273891949, 776268.6273891949, 236423.9346668081], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9471963128381147, 0.0, 0.0, 0.8294399451523027, 0.21563017427477638, 0.21563017427477638, 0.35287154427881806], 
reward next is 0.6471, 
noisyNet noise sample is [array([0.05558316], dtype=float32), 0.37388223]. 
=============================================
[2019-03-26 01:40:09,264] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0973875e-04 2.2905433e-32 1.5777628e-27 0.0000000e+00 9.9949026e-01], sum to 1.0000
[2019-03-26 01:40:09,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6243
[2019-03-26 01:40:09,283] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.96666666666667, 94.33333333333334, 1.0, 1.0, 0.6898278782843167, 1.0, 1.0, 0.6898278782843167, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1928954.342456348, 1928954.342456348, 369797.1339844131], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5883600.0000, 
sim time next is 5884200.0000, 
raw observation next is [25.95, 94.5, 1.0, 2.0, 0.3198144821540113, 1.0, 2.0, 0.3198144821540113, 1.0, 1.0, 0.5479318807081778, 6.9112, 6.9112, 170.5573041426782, 1341070.358281401, 1341070.358281401, 312145.1179832992], 
processed observation next is [1.0, 0.08695652173913043, 0.42890995260663506, 0.945, 1.0, 1.0, 0.18049937608917024, 1.0, 1.0, 0.18049937608917024, 1.0, 0.5, 0.44869741549777775, 0.0, 0.0, 0.8375144448122397, 0.37251954396705583, 0.37251954396705583, 0.46588823579596894], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48162222], dtype=float32), 0.91419274]. 
=============================================
[2019-03-26 01:40:12,218] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:40:12,228] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8462
[2019-03-26 01:40:12,236] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.25, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8941928493726952, 6.9112, 6.9112, 168.912956510431, 736170.9527711275, 736170.9527711275, 222009.407441859], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5879400.0000, 
sim time next is 5880000.0000, 
raw observation next is [26.2, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8930385725290799, 6.911199999999999, 6.9112, 168.912956510431, 735301.0247756047, 735301.0247756054, 221746.5327088014], 
processed observation next is [1.0, 0.043478260869565216, 0.44075829383886256, 0.9266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8695592347915609, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2042502846598902, 0.2042502846598904, 0.33096497419224086], 
reward next is 0.6690, 
noisyNet noise sample is [array([2.2010348], dtype=float32), -1.222961]. 
=============================================
[2019-03-26 01:40:12,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[46.589817]
 [46.936424]
 [46.988037]
 [47.162457]
 [47.266155]], R is [[46.58644485]
 [46.78922653]
 [46.98934174]
 [47.18645096]
 [47.38059616]].
[2019-03-26 01:40:14,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0756272e-01 2.8302409e-08 1.2622440e-20 3.6473253e-01 2.7704749e-02], sum to 1.0000
[2019-03-26 01:40:14,321] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6743
[2019-03-26 01:40:14,325] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.096887874485695, 6.9112, 168.911743308784, 2425800.471115784, 2294068.048234921, 476191.2150134456], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6189600.0000, 
sim time next is 6190200.0000, 
raw observation next is [29.9, 72.0, 1.0, 2.0, 0.8416059986625621, 1.0, 1.0, 0.8416059986625621, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2353813.150728903, 2353813.150728903, 440631.90524802], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.72, 1.0, 1.0, 0.8091638538103159, 1.0, 0.5, 0.8091638538103159, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6538369863135841, 0.6538369863135841, 0.6576595600716717], 
reward next is 0.3423, 
noisyNet noise sample is [array([1.1666095], dtype=float32), -0.16568613]. 
=============================================
[2019-03-26 01:40:28,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:40:28,108] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2737
[2019-03-26 01:40:28,114] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8660824409978584, 6.911200000000001, 6.9112, 168.912956510431, 716125.1428498683, 716125.1428498676, 215746.332222438], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6271200.0000, 
sim time next is 6271800.0000, 
raw observation next is [30.86666666666667, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8636884149729191, 6.9112, 6.9112, 168.912956510431, 714550.4691691964, 714550.4691691964, 215226.6328187855], 
processed observation next is [0.0, 0.6086956521739131, 0.6619273301737759, 0.62, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8337663597230721, 0.0, 0.0, 0.8294399451523027, 0.1984862414358879, 0.1984862414358879, 0.32123378032654554], 
reward next is 0.6788, 
noisyNet noise sample is [array([-0.6226451], dtype=float32), -0.459946]. 
=============================================
[2019-03-26 01:40:29,074] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 01:40:29,075] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:40:29,076] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:40:29,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:40:29,079] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:40:29,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:40:29,080] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:40:29,083] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:40:29,077] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:40:29,084] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:40:29,088] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:40:29,115] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-26 01:40:29,143] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-26 01:40:29,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-26 01:40:29,145] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-26 01:40:29,226] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-26 01:40:43,428] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:40:43,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.9, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7787511570018704, 6.9112, 6.9112, 168.912956510431, 657957.0042874248, 657957.0042874248, 197615.3304263936]
[2019-03-26 01:40:43,429] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:40:43,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3224558318378886
[2019-03-26 01:40:47,594] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:40:47,595] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 67.16666666666667, 1.0, 2.0, 0.8018096043338514, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1230452.972713725, 1230452.972713725, 258877.1138704478]
[2019-03-26 01:40:47,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:40:47,599] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.0572909e-37 0.0000000e+00 0.0000000e+00 1.2369706e-08], sampled 0.8527169170736056
[2019-03-26 01:40:47,601] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1230452.972713725 W.
[2019-03-26 01:40:49,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:40:49,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.5, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6177612288269362, 6.9112, 6.9112, 168.912956510431, 536737.8478110825, 536737.8478110825, 168843.9317958304]
[2019-03-26 01:40:49,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:40:49,448] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.10546756958427073
[2019-03-26 01:41:14,847] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:41:14,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.38085961833334, 93.59808148333335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5960490968813352, 6.911200000000001, 6.9112, 168.912956510431, 524892.1788734456, 524892.178873445, 165321.3338521762]
[2019-03-26 01:41:14,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:41:14,857] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.337506630505816
[2019-03-26 01:41:56,421] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:41:56,421] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.9, 58.66666666666667, 1.0, 1.0, 0.6074256033535613, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9127929676586, 848842.8863192606, 848842.8863192606, 202138.6790833207]
[2019-03-26 01:41:56,422] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:41:56,424] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5562335140192833
[2019-03-26 01:42:27,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:42:27,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.642691565, 97.456386625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6712278243450462, 6.911200000000001, 6.9112, 168.912956510431, 580068.4870080574, 580068.4870080567, 177661.8388599817]
[2019-03-26 01:42:27,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:42:27,531] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.291204e-28], sampled 0.7141447906802967
[2019-03-26 01:42:34,407] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.88293785], dtype=float32), 0.22248007]
[2019-03-26 01:42:34,407] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.45644223333333, 58.26787931666667, 1.0, 2.0, 0.3136737154253775, 1.0, 2.0, 0.3136737154253775, 1.0, 2.0, 0.5334943351507911, 6.9112, 6.9112, 171.5212843490159, 1315300.096908867, 1315300.096908867, 309266.6422247401]
[2019-03-26 01:42:34,408] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:42:34,410] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8287123e-15 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.3490482973143969
[2019-03-26 01:42:38,735] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7513.0765 3382681770.1832 1322.0000
[2019-03-26 01:42:38,870] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8180.7489 3033783746.8448 762.0000
[2019-03-26 01:42:38,893] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7507.4642 3265731683.5856 1242.0000
[2019-03-26 01:42:38,901] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7669.1154 3202989034.9540 1178.0000
[2019-03-26 01:42:38,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8061.4886 3072735968.2929 876.0000
[2019-03-26 01:42:39,967] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1650000, evaluation results [1650000.0, 7513.076483723984, 3382681770.1832356, 1322.0, 7669.115354761058, 3202989034.954014, 1178.0, 8180.748852737629, 3033783746.844806, 762.0, 7507.464184098907, 3265731683.585567, 1242.0, 8061.488559486711, 3072735968.2928715, 876.0]
[2019-03-26 01:42:50,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.2142022e-32], sum to 1.0000
[2019-03-26 01:42:50,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7057
[2019-03-26 01:42:50,068] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.35, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9296643558082544, 6.9112, 6.9112, 168.912956510431, 758545.0581727669, 758545.0581727669, 230062.5343070616], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6251400.0000, 
sim time next is 6252000.0000, 
raw observation next is [28.5, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9287935333688111, 6.9112, 6.9112, 168.912956510431, 757887.8363718415, 757887.8363718415, 229856.4682734633], 
processed observation next is [0.0, 0.34782608695652173, 0.5497630331753555, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9131628455717209, 0.0, 0.0, 0.8294399451523027, 0.2105243989921782, 0.2105243989921782, 0.3430693556320348], 
reward next is 0.6569, 
noisyNet noise sample is [array([-3.3320735], dtype=float32), -0.37469497]. 
=============================================
[2019-03-26 01:42:50,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[68.56717 ]
 [68.572525]
 [68.5801  ]
 [68.575485]
 [68.44945 ]], R is [[68.54103088]
 [68.51224518]
 [68.48381042]
 [68.45615387]
 [68.42907715]].
[2019-03-26 01:43:05,938] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5852469e-28], sum to 1.0000
[2019-03-26 01:43:05,947] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3258
[2019-03-26 01:43:05,952] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.83333333333334, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6084843875137576, 6.9112, 6.9112, 168.912956510431, 529808.52405428, 529808.52405428, 167380.1167524434], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6740400.0000, 
sim time next is 6741000.0000, 
raw observation next is [23.7, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6055381978512225, 6.9112, 6.9112, 168.912956510431, 527551.4511512365, 527551.4511512365, 166920.5433632654], 
processed observation next is [1.0, 0.0, 0.3222748815165877, 0.775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5189490217697835, 0.0, 0.0, 0.8294399451523027, 0.14654206976423237, 0.14654206976423237, 0.24913513934815731], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.06362621], dtype=float32), 1.8792913]. 
=============================================
[2019-03-26 01:43:05,967] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[55.115726]
 [55.86465 ]
 [57.543674]
 [60.839367]
 [60.84049 ]], R is [[54.8468132 ]
 [55.04852676]
 [55.24773788]
 [55.44435501]
 [55.63829041]].
[2019-03-26 01:43:18,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:43:18,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7774
[2019-03-26 01:43:18,032] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.8, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5895273933790524, 6.9112, 6.9112, 168.912956510431, 514241.124734033, 514241.124734033, 164483.1980792697], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6805800.0000, 
sim time next is 6806400.0000, 
raw observation next is [27.66666666666666, 53.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5878185514650346, 6.911199999999999, 6.9112, 168.912956510431, 513037.9967472589, 513037.9967472595, 164222.44319148], 
processed observation next is [1.0, 0.782608695652174, 0.5102685624012636, 0.5333333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4973396969085787, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14251055465201634, 0.14251055465201654, 0.24510812416638808], 
reward next is 0.7549, 
noisyNet noise sample is [array([0.04007456], dtype=float32), 0.7424409]. 
=============================================
[2019-03-26 01:43:18,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:43:18,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7679
[2019-03-26 01:43:18,566] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.03333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7419077753762071, 6.9112, 6.9112, 168.912956510431, 647443.1173423212, 647443.1173423212, 190354.4310792508], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6768600.0000, 
sim time next is 6769200.0000, 
raw observation next is [24.26666666666667, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.027015571612641, 7.350329652845409, 6.9112, 168.9110050301053, 1207694.280834182, 896164.1906314968, 255623.8572033666], 
processed observation next is [1.0, 0.34782608695652173, 0.34913112164297017, 0.7333333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0329458190398062, 0.04391296528454092, 0.0, 0.8294303624905118, 0.3354706335650505, 0.248934497397638, 0.38152814507965166], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.9700335], dtype=float32), 1.0648981]. 
=============================================
[2019-03-26 01:43:24,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:43:24,221] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7251
[2019-03-26 01:43:24,233] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.1, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6032661890779797, 6.9112, 6.9112, 168.912956510431, 529244.9349324272, 529244.9349324272, 166482.313532265], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6750000.0000, 
sim time next is 6750600.0000, 
raw observation next is [22.05, 84.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6664579310555901, 6.911199999999999, 6.9112, 168.912956510431, 584851.8768985935, 584851.8768985941, 176669.8897524843], 
processed observation next is [1.0, 0.13043478260869565, 0.24407582938388633, 0.8416666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5932413793360856, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16245885469405374, 0.1624588546940539, 0.26368640261564824], 
reward next is 0.7363, 
noisyNet noise sample is [array([-0.9427299], dtype=float32), -0.27765623]. 
=============================================
[2019-03-26 01:43:25,780] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.49626204e-01 1.48655032e-25 6.31337739e-33 1.14654515e-26
 7.50373781e-01], sum to 1.0000
[2019-03-26 01:43:25,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0775
[2019-03-26 01:43:25,802] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1569731.576734437 W.
[2019-03-26 01:43:25,808] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.3, 49.66666666666667, 1.0, 2.0, 0.3743050309068295, 1.0, 2.0, 0.3743050309068295, 1.0, 2.0, 0.6229187705145894, 6.9112, 6.9112, 170.5573041426782, 1569731.576734437, 1569731.576734437, 335469.0397884456], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7042800.0000, 
sim time next is 7043400.0000, 
raw observation next is [31.4, 49.0, 1.0, 2.0, 0.3374003045387668, 1.0, 2.0, 0.3374003045387668, 1.0, 2.0, 0.5639522032113801, 6.9112, 6.9112, 170.5573041426782, 1423936.950929098, 1423936.950929098, 319160.078669948], 
processed observation next is [1.0, 0.5217391304347826, 0.6872037914691943, 0.49, 1.0, 1.0, 0.20168711390212865, 1.0, 1.0, 0.20168711390212865, 1.0, 1.0, 0.46823439416021956, 0.0, 0.0, 0.8375144448122397, 0.39553804192474945, 0.39553804192474945, 0.4763583263730567], 
reward next is 0.5236, 
noisyNet noise sample is [array([-2.9023836], dtype=float32), 0.17834823]. 
=============================================
[2019-03-26 01:43:27,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.677893e-13], sum to 1.0000
[2019-03-26 01:43:27,809] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0730
[2019-03-26 01:43:27,826] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 993177.4699196932 W.
[2019-03-26 01:43:27,833] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.55, 70.0, 1.0, 2.0, 0.3519415220542707, 1.0, 2.0, 0.3519415220542707, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 993177.4699196932, 993177.4699196932, 261902.7492564903], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7026600.0000, 
sim time next is 7027200.0000, 
raw observation next is [27.7, 69.0, 1.0, 2.0, 0.3616643991853168, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6103841640810216, 6.9112, 6.9112, 168.912956510431, 1031547.193397669, 1031547.193397669, 243056.2695378447], 
processed observation next is [1.0, 0.34782608695652173, 0.5118483412322274, 0.69, 1.0, 1.0, 0.23092096287387565, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.5248587366841726, 0.0, 0.0, 0.8294399451523027, 0.28654088705490804, 0.28654088705490804, 0.36277055154902194], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.23810211], dtype=float32), -0.56788874]. 
=============================================
[2019-03-26 01:43:34,192] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:43:34,206] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1734
[2019-03-26 01:43:34,211] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.25, 61.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7564558672639127, 6.911199999999999, 6.9112, 168.912956510431, 639732.0855265777, 639732.0855265782, 193222.0287692495], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6948600.0000, 
sim time next is 6949200.0000, 
raw observation next is [29.4, 61.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7576832725305815, 6.911199999999999, 6.9112, 168.912956510431, 640416.241973249, 640416.2419732496, 193455.8585222087], 
processed observation next is [0.0, 0.43478260869565216, 0.5924170616113744, 0.61, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7044917957690019, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17789340054812472, 0.17789340054812489, 0.28874008734658013], 
reward next is 0.7113, 
noisyNet noise sample is [array([1.3152092], dtype=float32), 0.35546154]. 
=============================================
[2019-03-26 01:43:34,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.28934915e-14 7.67908362e-33 0.00000000e+00 1.11684580e-29
 1.00000000e+00], sum to 1.0000
[2019-03-26 01:43:34,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0370
[2019-03-26 01:43:34,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.48333333333333, 67.66666666666667, 1.0, 2.0, 0.3872968443815335, 1.0, 2.0, 0.3872968443815335, 1.0, 2.0, 0.6506342623245913, 6.9112, 6.9112, 170.5573041426782, 1624256.929356884, 1624256.929356884, 342926.707667352], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7125000.0000, 
sim time next is 7125600.0000, 
raw observation next is [28.36666666666667, 68.33333333333334, 1.0, 2.0, 0.4120588078124585, 1.0, 2.0, 0.4120588078124585, 1.0, 2.0, 0.6921212303572971, 6.911199999999999, 6.9112, 170.5573041426782, 1728188.117241074, 1728188.117241075, 356168.567370922], 
processed observation next is [1.0, 0.4782608695652174, 0.543443917851501, 0.6833333333333335, 1.0, 1.0, 0.29163711784633556, 1.0, 1.0, 0.29163711784633556, 1.0, 1.0, 0.6245380858015818, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4800522547891872, 0.4800522547891875, 0.531594876673018], 
reward next is 0.4684, 
noisyNet noise sample is [array([-0.4254188], dtype=float32), 0.21279265]. 
=============================================
[2019-03-26 01:43:47,615] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 01:43:47,617] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:43:47,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:43:47,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:43:47,618] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:43:47,619] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:43:47,620] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:43:47,621] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:43:47,624] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:43:47,625] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:43:47,628] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:43:47,655] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-26 01:43:47,687] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-26 01:43:47,688] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-26 01:43:47,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-26 01:43:47,689] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-26 01:44:08,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9107239], dtype=float32), 0.2678842]
[2019-03-26 01:44:08,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [17.36666666666667, 92.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4056152458459287, 6.911199999999999, 6.9112, 168.912956510431, 366331.2811807501, 366331.2811807506, 140942.674177398]
[2019-03-26 01:44:08,129] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:44:08,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6191152695262798
[2019-03-26 01:44:23,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9107239], dtype=float32), 0.2678842]
[2019-03-26 01:44:23,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.33333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7075552795719723, 6.9112, 6.9112, 168.912956510431, 605922.1673904671, 605922.1673904671, 184092.8503752203]
[2019-03-26 01:44:23,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:44:23,225] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9136019984639475
[2019-03-26 01:45:27,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9107239], dtype=float32), 0.2678842]
[2019-03-26 01:45:27,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.5, 91.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9050156975467482, 6.911200000000001, 6.9112, 168.912956510431, 743142.1708332984, 743142.1708332978, 224442.897677973]
[2019-03-26 01:45:27,924] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:45:27,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9551054320217327
[2019-03-26 01:45:40,797] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9107239], dtype=float32), 0.2678842]
[2019-03-26 01:45:40,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.89659684333333, 63.68277557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8584248709929195, 6.9112, 6.9112, 168.912956510431, 712134.4176793018, 712134.4176793018, 214122.4977832011]
[2019-03-26 01:45:40,800] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:45:40,803] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5759581038294621
[2019-03-26 01:45:56,358] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7480.3468 3258516240.2212 1286.0000
[2019-03-26 01:45:56,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8089.1983 3059992097.0372 881.0000
[2019-03-26 01:45:56,912] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8177.9930 3021560252.0784 800.0000
[2019-03-26 01:45:56,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7657.5248 3194974841.6238 1223.0000
[2019-03-26 01:45:57,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7503.7526 3375694397.0236 1353.0000
[2019-03-26 01:45:58,200] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1675000, evaluation results [1675000.0, 7503.752625987663, 3375694397.023629, 1353.0, 7657.524774817706, 3194974841.623782, 1223.0, 8177.993044222366, 3021560252.0784416, 800.0, 7480.346795112751, 3258516240.2211914, 1286.0, 8089.1983122419015, 3059992097.0371823, 881.0]
[2019-03-26 01:45:59,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:45:59,652] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1889
[2019-03-26 01:45:59,660] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.8, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.716082175362271, 6.9112, 6.9112, 168.912956510431, 618301.9594440827, 618301.9594440827, 185631.1575236179], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7364400.0000, 
sim time next is 7365000.0000, 
raw observation next is [22.6, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.693214401351571, 6.9112, 6.9112, 168.912956510431, 599718.7713122347, 599718.7713122347, 181491.3757602066], 
processed observation next is [1.0, 0.21739130434782608, 0.27014218009478685, 0.8983333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6258712211604524, 0.0, 0.0, 0.8294399451523027, 0.16658854758673186, 0.16658854758673186, 0.27088265038836806], 
reward next is 0.7291, 
noisyNet noise sample is [array([-0.9623077], dtype=float32), -0.39248016]. 
=============================================
[2019-03-26 01:45:59,671] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.5422  ]
 [69.419266]
 [69.233345]
 [68.73204 ]
 [69.24942 ]], R is [[69.70650482]
 [69.7323761 ]
 [69.75225067]
 [69.77597046]
 [69.76387024]].
[2019-03-26 01:46:01,091] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:01,104] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8006
[2019-03-26 01:46:01,113] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.71666666666667, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7875602056435306, 6.911200000000001, 6.9112, 168.912956510431, 661640.8689527683, 661640.8689527677, 199324.619088386], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7171800.0000, 
sim time next is 7172400.0000, 
raw observation next is [25.73333333333333, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7880291150717457, 6.911199999999999, 6.9112, 168.912956510431, 661879.9016697747, 661879.9016697753, 199416.5266129913], 
processed observation next is [1.0, 0.0, 0.41864139020537117, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7414989208192019, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1838555282416041, 0.18385552824160423, 0.2976366068850616], 
reward next is 0.7024, 
noisyNet noise sample is [array([-0.44920003], dtype=float32), -1.925513]. 
=============================================
[2019-03-26 01:46:01,524] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:01,534] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7992
[2019-03-26 01:46:01,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.81666666666667, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6479457314250092, 6.9112, 6.9112, 168.912956510431, 559822.073719512, 559822.073719512, 173743.7051953389], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7344600.0000, 
sim time next is 7345200.0000, 
raw observation next is [24.83333333333334, 75.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6455234549525476, 6.9112, 6.9112, 168.912956510431, 557927.7927616842, 557927.7927616842, 173342.5315806836], 
processed observation next is [1.0, 0.0, 0.3759873617693526, 0.7533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.567711530429936, 0.0, 0.0, 0.8294399451523027, 0.15497994243380114, 0.15497994243380114, 0.25872019638908], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.1110818], dtype=float32), -0.12284467]. 
=============================================
[2019-03-26 01:46:05,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 3.6881010e-36 0.0000000e+00 0.0000000e+00 2.1880926e-25], sum to 1.0000
[2019-03-26 01:46:05,880] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3593
[2019-03-26 01:46:05,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1715764.152047599 W.
[2019-03-26 01:46:05,902] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.13333333333333, 85.33333333333333, 1.0, 2.0, 0.6136435438369207, 0.0, 1.0, 0.0, 1.0, 2.0, 1.029908881109447, 6.9112, 6.9112, 168.9129082963629, 1715764.152047599, 1715764.152047599, 368409.3473072968], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7144800.0000, 
sim time next is 7145400.0000, 
raw observation next is [26.11666666666667, 85.66666666666667, 1.0, 2.0, 0.604937540652381, 0.0, 2.0, 0.0, 1.0, 2.0, 1.013592829621706, 6.911199999999999, 6.9112, 168.912956510431, 1691402.530485577, 1691402.530485578, 362493.408116333], 
processed observation next is [1.0, 0.6956521739130435, 0.43680884676145365, 0.8566666666666667, 1.0, 1.0, 0.5240211333161217, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0165766214898855, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4698340362459936, 0.4698340362459939, 0.5410349374870642], 
reward next is 0.4590, 
noisyNet noise sample is [array([-0.24184738], dtype=float32), 1.2299325]. 
=============================================
[2019-03-26 01:46:08,141] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:08,151] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8352
[2019-03-26 01:46:08,157] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.8, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7957035562720708, 6.911200000000001, 6.9112, 168.912956510431, 667154.1380232726, 667154.1380232719, 200955.8994921667], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7176600.0000, 
sim time next is 7177200.0000, 
raw observation next is [25.8, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7971547724312148, 6.9112, 6.9112, 168.912956510431, 668160.7894325441, 668160.7894325441, 201248.7573651385], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.752627771257579, 0.0, 0.0, 0.8294399451523027, 0.18560021928681783, 0.18560021928681783, 0.3003712796494605], 
reward next is 0.6996, 
noisyNet noise sample is [array([-0.23193759], dtype=float32), -0.9886002]. 
=============================================
[2019-03-26 01:46:09,259] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 3.0705039e-37 0.0000000e+00 0.0000000e+00 1.2530561e-16], sum to 1.0000
[2019-03-26 01:46:09,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0246
[2019-03-26 01:46:09,282] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1534545.670104416 W.
[2019-03-26 01:46:09,291] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.5320541816922181, 1.0, 1.0, 0.5320541816922181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1534545.670104416, 1534545.670104416, 316084.6313993064], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7225200.0000, 
sim time next is 7225800.0000, 
raw observation next is [24.03333333333333, 88.5, 1.0, 2.0, 0.4327127786873572, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7421272719817911, 6.911200000000001, 6.9112, 168.9129564680874, 1263512.868031928, 1263512.868031927, 276631.4626831421], 
processed observation next is [1.0, 0.6521739130434783, 0.3380726698262243, 0.885, 1.0, 1.0, 0.31652142010524964, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.6855210633924281, 8.881784197001253e-17, 0.0, 0.8294399449443762, 0.3509757966755356, 0.35097579667553525, 0.4128827801240927], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5826553], dtype=float32), 0.547638]. 
=============================================
[2019-03-26 01:46:15,447] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:15,455] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9702
[2019-03-26 01:46:15,465] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.06666666666667, 89.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6009874039425575, 6.911199999999999, 6.9112, 168.912956510431, 523749.3957909404, 523749.3957909411, 166221.1872510813], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7260000.0000, 
sim time next is 7260600.0000, 
raw observation next is [22.03333333333333, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5973165732247417, 6.911199999999999, 6.9112, 168.912956510431, 520838.4301610519, 520838.4301610525, 165657.9394785351], 
processed observation next is [1.0, 0.0, 0.2432859399684044, 0.8916666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5089226502740752, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1446773417114033, 0.14467734171140348, 0.24725065593811207], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.36874098], dtype=float32), 0.6667825]. 
=============================================
[2019-03-26 01:46:16,485] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:16,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4477
[2019-03-26 01:46:16,500] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.5, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7000926690771971, 6.9112, 6.9112, 168.912956510431, 597606.4208987879, 597606.4208987879, 182741.4221963228], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7522200.0000, 
sim time next is 7522800.0000, 
raw observation next is [23.5, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.697908149534602, 6.911199999999999, 6.9112, 168.912956510431, 595742.8224814701, 595742.8224814708, 182350.3153101753], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6315953043104903, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1654841173559639, 0.1654841173559641, 0.27216464971667953], 
reward next is 0.7278, 
noisyNet noise sample is [array([1.2942464], dtype=float32), 0.07202204]. 
=============================================
[2019-03-26 01:46:18,503] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:18,516] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4203
[2019-03-26 01:46:18,523] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.55, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8245594908866136, 6.9112, 6.9112, 168.912956510431, 700896.0771408944, 700896.0771408944, 207135.7238732412], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7615800.0000, 
sim time next is 7616400.0000, 
raw observation next is [23.5, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8193263491350282, 6.9112, 6.9112, 168.912956510431, 696882.8024645454, 696882.8024645454, 206029.0554807602], 
processed observation next is [1.0, 0.13043478260869565, 0.31279620853080575, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7796662794329613, 0.0, 0.0, 0.8294399451523027, 0.1935785562401515, 0.1935785562401515, 0.3075060529563585], 
reward next is 0.6925, 
noisyNet noise sample is [array([1.3254044], dtype=float32), -0.95786214]. 
=============================================
[2019-03-26 01:46:22,294] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:22,306] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3775
[2019-03-26 01:46:22,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.3, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7710839195812493, 6.911199999999999, 6.9112, 168.912956510431, 650328.9176731273, 650328.917673128, 196068.0030753214], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7606800.0000, 
sim time next is 7607400.0000, 
raw observation next is [24.23333333333333, 94.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7706296744112773, 6.911199999999999, 6.9112, 168.912956510431, 650251.0205941919, 650251.0205941924, 195982.944748908], 
processed observation next is [1.0, 0.043478260869565216, 0.3475513428120062, 0.9416666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7202800907454601, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18062528349838664, 0.1806252834983868, 0.29251185783419104], 
reward next is 0.7075, 
noisyNet noise sample is [array([1.35664], dtype=float32), 0.6758954]. 
=============================================
[2019-03-26 01:46:23,718] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:23,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0269
[2019-03-26 01:46:23,731] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333333, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7570095013768041, 6.911199999999999, 6.9112, 168.912956510431, 639194.3125513236, 639194.3125513243, 193314.9016468707], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7581000.0000, 
sim time next is 7581600.0000, 
raw observation next is [27.7, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7577472460643164, 6.9112, 6.9112, 168.912956510431, 639507.7533714027, 639507.7533714027, 193453.6663108039], 
processed observation next is [0.0, 0.782608695652174, 0.5118483412322274, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7045698122735566, 0.0, 0.0, 0.8294399451523027, 0.17764104260316743, 0.17764104260316743, 0.28873681538925955], 
reward next is 0.7113, 
noisyNet noise sample is [array([-1.6081502], dtype=float32), -0.002753361]. 
=============================================
[2019-03-26 01:46:24,297] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:24,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8059
[2019-03-26 01:46:24,325] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5725541877302375, 6.911199999999999, 6.9112, 168.912956510431, 501227.6205230787, 501227.6205230793, 161947.2958295382], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7432200.0000, 
sim time next is 7432800.0000, 
raw observation next is [21.23333333333333, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5716613826581763, 6.9112, 6.9112, 168.912956510431, 500480.1425461788, 500480.1425461788, 161817.4545578179], 
processed observation next is [0.0, 0.0, 0.2053712480252764, 0.9266666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4776358325099711, 0.0, 0.0, 0.8294399451523027, 0.13902226181838298, 0.13902226181838298, 0.24151858889226552], 
reward next is 0.7585, 
noisyNet noise sample is [array([-1.9438932], dtype=float32), 0.66406083]. 
=============================================
[2019-03-26 01:46:25,097] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:25,106] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6674
[2019-03-26 01:46:25,111] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.96666666666667, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8347928343874019, 6.911199999999999, 6.9112, 168.912956510431, 695249.6811494034, 695249.6811494041, 209048.7490014418], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7677600.0000, 
sim time next is 7678200.0000, 
raw observation next is [25.93333333333333, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8307417333166585, 6.911199999999999, 6.9112, 168.912956510431, 692419.763877984, 692419.7638779847, 208194.6444799759], 
processed observation next is [1.0, 0.8695652173913043, 0.42812006319115314, 0.8816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7935874796544615, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19233882329944, 0.1923388232994402, 0.3107382753432476], 
reward next is 0.6893, 
noisyNet noise sample is [array([-0.18071592], dtype=float32), 0.71677566]. 
=============================================
[2019-03-26 01:46:28,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 8.5592064e-29 0.0000000e+00 5.4504826e-25 3.1656004e-17], sum to 1.0000
[2019-03-26 01:46:28,388] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3719
[2019-03-26 01:46:28,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1673875.131073352 W.
[2019-03-26 01:46:28,403] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.01666666666667, 62.33333333333334, 1.0, 2.0, 0.5986782770854416, 1.0, 2.0, 0.5986782770854416, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1673875.131073352, 1673875.131073352, 333754.4288641568], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7645800.0000, 
sim time next is 7646400.0000, 
raw observation next is [30.1, 62.0, 1.0, 2.0, 0.6334611335264201, 1.0, 2.0, 0.6334611335264201, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1771206.665474075, 1771206.665474075, 346929.8476343782], 
processed observation next is [1.0, 0.5217391304347826, 0.6255924170616115, 0.62, 1.0, 1.0, 0.5583869078631567, 1.0, 1.0, 0.5583869078631567, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49200185152057635, 0.49200185152057635, 0.5178057427378778], 
reward next is 0.4822, 
noisyNet noise sample is [array([0.11510119], dtype=float32), 2.2797916]. 
=============================================
[2019-03-26 01:46:31,764] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.9423905e-25 3.9887271e-38], sum to 1.0000
[2019-03-26 01:46:31,772] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5227
[2019-03-26 01:46:31,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2311863.149662422 W.
[2019-03-26 01:46:31,789] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.43333333333333, 63.33333333333333, 1.0, 2.0, 0.8266206350359411, 1.0, 1.0, 0.8266206350359411, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2311863.149662422, 2311863.149662422, 433036.0410811969], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7732200.0000, 
sim time next is 7732800.0000, 
raw observation next is [31.5, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.049234101472591, 6.9112, 168.9120595730412, 2392424.837015587, 2294499.21465718, 476306.7909901315], 
processed observation next is [1.0, 0.5217391304347826, 0.6919431279620853, 0.63, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.013803410147259143, 0.0, 0.8294355407790991, 0.664562454726552, 0.6373608929603278, 0.7109056581942261], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1549394], dtype=float32), 0.3955128]. 
=============================================
[2019-03-26 01:46:34,463] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:34,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1581
[2019-03-26 01:46:34,477] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.6, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8977341684954778, 6.9112, 6.9112, 168.912956510431, 735200.7958745301, 735200.7958745301, 222668.5830209708], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7842600.0000, 
sim time next is 7843200.0000, 
raw observation next is [28.46666666666667, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8982589908445455, 6.9112, 6.9112, 168.912956510431, 735333.4575026871, 735333.4575026871, 222777.2093749487], 
processed observation next is [1.0, 0.782608695652174, 0.5481832543443919, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8759255985909092, 0.0, 0.0, 0.8294399451523027, 0.20425929375074642, 0.20425929375074642, 0.33250329757455027], 
reward next is 0.6675, 
noisyNet noise sample is [array([-0.3481507], dtype=float32), -0.37593257]. 
=============================================
[2019-03-26 01:46:35,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:35,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9090
[2019-03-26 01:46:35,584] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.35, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.771672732479572, 6.911200000000001, 6.9112, 168.912956510431, 650580.904096326, 650580.9040963253, 196180.7190807442], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7606200.0000, 
sim time next is 7606800.0000, 
raw observation next is [24.3, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7710839195812493, 6.911199999999999, 6.9112, 168.912956510431, 650328.9176731273, 650328.917673128, 196068.0030753214], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7208340482698162, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18064692157586867, 0.18064692157586887, 0.2926388105601812], 
reward next is 0.7074, 
noisyNet noise sample is [array([-1.5050492], dtype=float32), -0.99102974]. 
=============================================
[2019-03-26 01:46:36,413] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:36,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8097
[2019-03-26 01:46:36,430] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.4, 86.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8424533514155615, 6.911199999999999, 6.9112, 168.912956510431, 700220.1590717006, 700220.1590717013, 210663.2798859811], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7780800.0000, 
sim time next is 7781400.0000, 
raw observation next is [26.4, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.838323938965461, 6.9112, 6.9112, 168.912956510431, 697254.5544403295, 697254.5544403295, 209782.8994909751], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8028340719090986, 0.0, 0.0, 0.8294399451523027, 0.19368182067786932, 0.19368182067786932, 0.3131088052104106], 
reward next is 0.6869, 
noisyNet noise sample is [array([0.07743207], dtype=float32), -1.425844]. 
=============================================
[2019-03-26 01:46:43,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:43,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:43,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-26 01:46:44,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:44,618] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:44,720] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-26 01:46:45,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:45,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:45,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-26 01:46:45,750] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:45,751] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:45,817] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-26 01:46:47,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:47,662] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:47,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-26 01:46:48,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:48,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:48,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-26 01:46:49,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:49,826] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:49,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-26 01:46:53,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:53,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:53,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-26 01:46:53,703] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:53,711] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4896
[2019-03-26 01:46:53,718] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.05, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.907379515742985, 6.9112, 6.9112, 168.912956510431, 743986.0789735976, 743986.0789735976, 224949.381088465], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7851000.0000, 
sim time next is 7851600.0000, 
raw observation next is [27.0, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9012715543897007, 6.911199999999999, 6.9112, 168.912956510431, 739800.1570060969, 739800.1570060975, 223559.9540805109], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8795994565728058, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2055000436128047, 0.20550004361280486, 0.3336715732544939], 
reward next is 0.6663, 
noisyNet noise sample is [array([0.7104579], dtype=float32), 1.9511366]. 
=============================================
[2019-03-26 01:46:55,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:55,345] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:55,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-26 01:46:56,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:46:56,143] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3186
[2019-03-26 01:46:56,148] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.4, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6699816620748655, 6.9112, 6.9112, 168.912956510431, 576836.0541452381, 576836.0541452381, 177463.7088358473], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [22.4, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6670600286204875, 6.911200000000001, 6.9112, 168.912956510431, 574320.0518680543, 574320.0518680537, 176964.7064353726], 
processed observation next is [1.0, 0.8695652173913043, 0.2606635071090047, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5939756446591311, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595333477411262, 0.15953334774112604, 0.2641264275154815], 
reward next is 0.7359, 
noisyNet noise sample is [array([-0.07079961], dtype=float32), 0.99547344]. 
=============================================
[2019-03-26 01:46:58,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:58,356] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:58,443] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-26 01:46:58,902] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:58,903] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:59,000] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-26 01:46:59,167] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:59,167] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:59,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-26 01:46:59,669] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:46:59,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:46:59,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-26 01:47:00,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:47:00,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:00,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-26 01:47:00,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:47:00,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:00,533] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-26 01:47:01,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 01:47:01,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:01,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-26 01:47:03,142] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 01:47:03,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:47:03,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:03,149] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:47:03,150] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:03,151] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:47:03,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:47:03,155] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:03,155] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:03,156] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:47:03,158] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:47:03,188] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-26 01:47:03,211] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-26 01:47:03,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-26 01:47:03,262] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-26 01:47:03,284] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-26 01:47:15,362] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:47:15,363] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.9, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4108695388688975, 6.9112, 6.9112, 168.912956510431, 369961.1697761447, 369961.1697761447, 141568.3752275801]
[2019-03-26 01:47:15,363] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:47:15,365] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6137372845701428
[2019-03-26 01:47:48,764] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:47:48,766] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6839530477075992, 6.9112, 6.9112, 168.912956510431, 586467.0610255925, 586467.0610255925, 179887.9308657329]
[2019-03-26 01:47:48,766] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:47:48,771] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9958842590384976
[2019-03-26 01:47:49,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:47:49,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.2, 52.0, 1.0, 2.0, 0.8395768442357225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173440.337738507, 1173440.337738507, 253856.0161899558]
[2019-03-26 01:47:49,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:47:49,561] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999785e-01 0.0000000e+00 0.0000000e+00 1.6725437e-36 2.1148137e-06], sampled 0.46289349260410895
[2019-03-26 01:47:49,561] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1173440.337738507 W.
[2019-03-26 01:48:07,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:48:07,559] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.81925994, 39.04009161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9560058606445427, 6.911199999999999, 6.9112, 168.912956510431, 777078.9079207028, 777078.9079207034, 236322.1817377099]
[2019-03-26 01:48:07,561] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:48:07,564] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.620354800622654
[2019-03-26 01:48:16,250] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:48:16,252] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.5, 61.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9799395074975688, 6.9112, 6.9112, 168.912956510431, 793533.2203769472, 793533.2203769472, 242139.3914826068]
[2019-03-26 01:48:16,252] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:48:16,255] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9399417122431266
[2019-03-26 01:48:35,075] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.95009035], dtype=float32), 0.20913556]
[2019-03-26 01:48:35,076] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.7, 94.0, 1.0, 2.0, 0.635745728673159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888435.2099138169, 888435.2099138169, 207594.1028304695]
[2019-03-26 01:48:35,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:48:35,081] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.6807684e-24], sampled 0.5089873569396047
[2019-03-26 01:48:35,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 888435.2099138169 W.
[2019-03-26 01:49:12,624] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8098.0398 2951769324.3075 1244.0000
[2019-03-26 01:49:12,801] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7419.2525 3127687263.8183 1855.0000
[2019-03-26 01:49:12,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7997.2557 2998396527.7771 1316.0000
[2019-03-26 01:49:13,105] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7368.6372 3327753931.1466 1881.0000
[2019-03-26 01:49:13,142] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7143.5920 3195731462.2112 2145.0000
[2019-03-26 01:49:14,159] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1700000, evaluation results [1700000.0, 7368.637163050753, 3327753931.146644, 1881.0, 7419.252457570268, 3127687263.8182974, 1855.0, 8098.039753994434, 2951769324.307503, 1244.0, 7143.59195047809, 3195731462.211183, 2145.0, 7997.255724967705, 2998396527.7771144, 1316.0]
[2019-03-26 01:49:23,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:49:23,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7340
[2019-03-26 01:49:23,991] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.16666666666667, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4708950013490149, 6.9112, 6.9112, 168.912956510431, 418275.7382502115, 418275.7382502115, 148423.4836753563], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 357600.0000, 
sim time next is 358200.0000, 
raw observation next is [20.15, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4694696053755552, 6.9112, 6.9112, 168.912956510431, 416996.7051916098, 416996.7051916098, 148258.2617904314], 
processed observation next is [1.0, 0.13043478260869565, 0.15402843601895733, 0.885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3530117138726283, 0.0, 0.0, 0.8294399451523027, 0.1158324181087805, 0.1158324181087805, 0.22128098774691254], 
reward next is 0.7787, 
noisyNet noise sample is [array([0.74840105], dtype=float32), -0.8924782]. 
=============================================
[2019-03-26 01:49:26,292] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.9999976e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8043603e-07], sum to 1.0000
[2019-03-26 01:49:26,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9853
[2019-03-26 01:49:26,309] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 85.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4570278152561281, 6.9112, 6.9112, 168.912956510431, 408293.7364843456, 408293.7364843456, 146698.408922395], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 427200.0000, 
sim time next is 427800.0000, 
raw observation next is [19.75, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4550531231332387, 6.911199999999999, 6.9112, 168.912956510431, 406555.8672708419, 406555.8672708425, 146475.0545220589], 
processed observation next is [1.0, 0.9565217391304348, 0.13507109004739343, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3354306379673643, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11293218535301164, 0.1129321853530118, 0.21861948436128192], 
reward next is 0.7814, 
noisyNet noise sample is [array([-0.09590148], dtype=float32), -1.2417679]. 
=============================================
[2019-03-26 01:49:30,392] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:49:30,402] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9009
[2019-03-26 01:49:30,408] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.2, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4796048702755407, 6.9112, 6.9112, 168.912956510431, 426112.745323793, 426112.745323793, 149443.7598605716], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 277200.0000, 
sim time next is 277800.0000, 
raw observation next is [19.35, 95.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4811416522969248, 6.9112, 6.9112, 168.912956510431, 427461.9519288958, 427461.9519288958, 149627.6777722056], 
processed observation next is [0.0, 0.21739130434782608, 0.11611374407582951, 0.9533333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3672459174352741, 0.0, 0.0, 0.8294399451523027, 0.11873943109135994, 0.11873943109135994, 0.22332489219732182], 
reward next is 0.7767, 
noisyNet noise sample is [array([-0.05376825], dtype=float32), 1.7465]. 
=============================================
[2019-03-26 01:49:43,680] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2086647e-26 1.4731427e-37 1.4640063e-36 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 01:49:43,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0441
[2019-03-26 01:49:43,700] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.48333333333333, 55.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1974508039343007, 6.9112, 6.9112, 170.5573041426782, 525622.1530650073, 525622.1530650073, 232164.9272365334], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 666600.0000, 
sim time next is 667200.0000, 
raw observation next is [24.26666666666667, 56.00000000000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 397493.6685079886, 397493.6685079879, 211816.3820281339], 
processed observation next is [1.0, 0.7391304347826086, 0.34913112164297017, 0.56, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.11041490791888572, 0.11041490791888553, 0.3161438537733342], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47969693], dtype=float32), 0.95027536]. 
=============================================
[2019-03-26 01:49:43,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 01:49:43,711] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2247
[2019-03-26 01:49:43,716] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.25, 81.66666666666667, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 409167.5723205105, 409167.5723205105, 213666.2986913249], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 420600.0000, 
sim time next is 421200.0000, 
raw observation next is [20.2, 82.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 410329.9971843064, 410329.9971843064, 213843.5242135174], 
processed observation next is [1.0, 0.9130434782608695, 0.15639810426540288, 0.82, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.11398055477341844, 0.11398055477341844, 0.31916943912465284], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.02138224], dtype=float32), 0.014495128]. 
=============================================
[2019-03-26 01:49:45,380] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.0480797e-21 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 01:49:45,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9845
[2019-03-26 01:49:45,393] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [19.9, 81.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 398399.7202120456, 398399.7202120462, 211420.179202723], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 681600.0000, 
sim time next is 682200.0000, 
raw observation next is [19.8, 82.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 397344.6864724963, 397344.6864724969, 211226.952745932], 
processed observation next is [1.0, 0.9130434782608695, 0.13744075829383895, 0.82, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.11037352402013786, 0.11037352402013803, 0.31526410857601794], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.81615704], dtype=float32), -0.5821036]. 
=============================================
[2019-03-26 01:49:46,146] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 0.000000e+00 0.000000e+00 7.444934e-38 1.000000e+00], sum to 1.0000
[2019-03-26 01:49:46,160] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7000
[2019-03-26 01:49:46,165] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 54.0, 1.0, 2.0, 0.2181718716088756, 1.0, 2.0, 0.2181718716088756, 1.0, 2.0, 0.3991943879861195, 6.9112, 6.9112, 170.5573041426782, 1060948.618844535, 1060948.618844535, 289170.4781771348], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 664200.0000, 
sim time next is 664800.0000, 
raw observation next is [24.7, 54.0, 1.0, 2.0, 0.2197264493526025, 1.0, 2.0, 0.2197264493526025, 1.0, 2.0, 0.4019627761702574, 6.9112, 6.9112, 170.5573041426782, 1068108.532797406, 1068108.532797406, 289684.0578750668], 
processed observation next is [1.0, 0.6956521739130435, 0.3696682464454976, 0.54, 1.0, 1.0, 0.059911384762171664, 1.0, 1.0, 0.059911384762171664, 1.0, 1.0, 0.27068631240275287, 0.0, 0.0, 0.8375144448122397, 0.2966968146659461, 0.2966968146659461, 0.43236426548517437], 
reward next is 0.5676, 
noisyNet noise sample is [array([1.5462685], dtype=float32), 0.91702664]. 
=============================================
[2019-03-26 01:49:48,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:49:48,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9151
[2019-03-26 01:49:48,802] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.8, 90.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3954439206194383, 6.911199999999999, 6.9112, 168.912956510431, 356970.2964023377, 356970.2964023383, 139978.1148387035], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 537000.0000, 
sim time next is 537600.0000, 
raw observation next is [18.0, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3907907126505288, 6.9112, 6.9112, 168.912956510431, 352579.6429575228, 352579.6429575228, 139556.5941869392], 
processed observation next is [1.0, 0.21739130434782608, 0.052132701421801014, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2570618446957668, 0.0, 0.0, 0.8294399451523027, 0.09793878971042301, 0.09793878971042301, 0.20829342415961077], 
reward next is 0.7917, 
noisyNet noise sample is [array([-0.47567865], dtype=float32), -0.46065012]. 
=============================================
[2019-03-26 01:49:49,846] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:49:49,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1026
[2019-03-26 01:49:49,864] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.15, 69.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5466985247612454, 6.911199999999999, 6.9112, 168.912956510431, 480020.9036320557, 480020.9036320564, 158260.0684337316], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 834600.0000, 
sim time next is 835200.0000, 
raw observation next is [24.1, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5481257503585942, 6.911199999999999, 6.9112, 168.912956510431, 481149.0376832679, 481149.0376832685, 158460.3261572785], 
processed observation next is [0.0, 0.6956521739130435, 0.3412322274881518, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44893384190072466, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13365251046757443, 0.1336525104675746, 0.23650794948847537], 
reward next is 0.7635, 
noisyNet noise sample is [array([0.6297425], dtype=float32), 1.4484214]. 
=============================================
[2019-03-26 01:49:52,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.450162e-30], sum to 1.0000
[2019-03-26 01:49:52,348] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3798
[2019-03-26 01:49:52,355] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.26666666666667, 80.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5383898709159044, 6.911199999999999, 6.9112, 168.912956510431, 473573.2531905661, 473573.2531905668, 157100.2467484164], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [22.2, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5370744113290877, 6.9112, 6.9112, 168.912956510431, 472506.0193142873, 472506.0193142873, 156919.6735143215], 
processed observation next is [0.0, 0.8695652173913043, 0.2511848341232228, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4354565991818143, 0.0, 0.0, 0.8294399451523027, 0.13125167203174648, 0.13125167203174648, 0.23420846793182315], 
reward next is 0.7658, 
noisyNet noise sample is [array([-2.342901], dtype=float32), -0.11282209]. 
=============================================
[2019-03-26 01:49:54,501] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5892916e-28 0.0000000e+00 0.0000000e+00 3.4680805e-33 1.0000000e+00], sum to 1.0000
[2019-03-26 01:49:54,513] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3363
[2019-03-26 01:49:54,517] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.86666666666667, 59.66666666666667, 1.0, 2.0, 0.2484537476405858, 1.0, 2.0, 0.2484537476405858, 1.0, 2.0, 0.453602900633704, 6.9112, 6.9112, 170.5573041426782, 1203089.070373707, 1203089.070373707, 299898.0519864451], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 570000.0000, 
sim time next is 570600.0000, 
raw observation next is [23.85, 60.0, 1.0, 2.0, 0.2521485645559394, 1.0, 2.0, 0.2521485645559394, 1.0, 2.0, 0.4600859617215953, 6.9112, 6.9112, 170.5573041426782, 1219656.660551749, 1219656.660551749, 301267.6667428307], 
processed observation next is [1.0, 0.6086956521739131, 0.3293838862559243, 0.6, 1.0, 1.0, 0.09897417416378243, 1.0, 1.0, 0.09897417416378243, 1.0, 1.0, 0.3415682460019455, 0.0, 0.0, 0.8375144448122397, 0.33879351681993025, 0.33879351681993025, 0.44965323394452344], 
reward next is 0.5503, 
noisyNet noise sample is [array([0.9674242], dtype=float32), 1.3849534]. 
=============================================
[2019-03-26 01:50:03,682] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:50:03,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5851
[2019-03-26 01:50:03,705] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.3, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5376233712024568, 6.9112, 6.9112, 168.912956510431, 472648.2819308657, 472648.2819308657, 157004.9193751083], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 907200.0000, 
sim time next is 907800.0000, 
raw observation next is [23.45, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.539169661235421, 6.911199999999999, 6.9112, 168.912956510431, 473890.9216368221, 473890.9216368227, 157217.7312298363], 
processed observation next is [0.0, 0.5217391304347826, 0.3104265402843602, 0.7333333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4380117819944158, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13163636712133947, 0.13163636712133964, 0.2346533301937855], 
reward next is 0.7653, 
noisyNet noise sample is [array([0.7737106], dtype=float32), 0.13241939]. 
=============================================
[2019-03-26 01:50:10,674] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1914306e-27], sum to 1.0000
[2019-03-26 01:50:10,685] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4413
[2019-03-26 01:50:10,692] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.03333333333333, 61.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5261116136425107, 6.9112, 6.9112, 168.912956510431, 463378.7550511046, 463378.7550511046, 155440.2534756812], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 820200.0000, 
sim time next is 820800.0000, 
raw observation next is [25.0, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5262090354559018, 6.911200000000001, 6.9112, 168.912956510431, 463480.6529531262, 463480.6529531256, 155452.5299114135], 
processed observation next is [0.0, 0.5217391304347826, 0.38388625592417064, 0.62, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4222061407998802, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12874462582031282, 0.12874462582031265, 0.23201870136031866], 
reward next is 0.7680, 
noisyNet noise sample is [array([0.3812276], dtype=float32), -0.0076619145]. 
=============================================
[2019-03-26 01:50:17,892] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:50:17,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6242
[2019-03-26 01:50:17,911] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.75, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.499785594328309, 6.9112, 6.9112, 168.912956510431, 442485.7190894212, 442485.7190894212, 151972.8930747661], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1139400.0000, 
sim time next is 1140000.0000, 
raw observation next is [19.7, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5012057227931606, 6.9112, 6.9112, 168.912956510431, 443789.9161265534, 443789.9161265534, 152147.8760245101], 
processed observation next is [1.0, 0.17391304347826086, 0.1327014218009479, 0.9533333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.39171429608922015, 0.0, 0.0, 0.8294399451523027, 0.12327497670182039, 0.12327497670182039, 0.22708638212613447], 
reward next is 0.7729, 
noisyNet noise sample is [array([0.14216958], dtype=float32), -0.32877937]. 
=============================================
[2019-03-26 01:50:17,934] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[68.646996]
 [68.67777 ]
 [68.621666]
 [68.5534  ]
 [68.46886 ]], R is [[68.67810059]
 [68.76449585]
 [68.85079956]
 [68.93515778]
 [69.01696777]].
[2019-03-26 01:50:21,918] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 01:50:21,920] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:50:21,921] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:50:21,922] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:50:21,924] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:50:21,924] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:50:21,927] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:50:21,928] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:50:21,927] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:50:21,931] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:50:21,931] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:50:21,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-26 01:50:21,985] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-26 01:50:22,012] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-26 01:50:22,013] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-26 01:50:22,014] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-26 01:50:36,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:50:36,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.13333333333333, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7118395421332632, 6.911199999999999, 6.9112, 168.912956510431, 609226.1791177787, 609226.1791177792, 184871.864794424]
[2019-03-26 01:50:36,159] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:50:36,161] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.874881974859675
[2019-03-26 01:50:36,558] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:50:36,559] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.3, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6817952521996827, 6.911199999999999, 6.9112, 168.912956510431, 587501.8490547116, 587501.8490547123, 179502.309930339]
[2019-03-26 01:50:36,560] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:50:36,566] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.09341488914338858
[2019-03-26 01:50:42,924] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:50:42,925] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.10092286, 83.866629135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7818454403483384, 6.9112, 6.9112, 168.912956510431, 662651.843291101, 662651.843291101, 198265.2500940436]
[2019-03-26 01:50:42,926] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:50:42,927] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.35787171138964513
[2019-03-26 01:51:02,713] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:51:02,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.28333333333333, 88.16666666666667, 1.0, 2.0, 0.7034141114497046, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.975109929295304, 6.9112, 168.9125755393575, 1879906.354991389, 1834566.553800195, 386455.1569362745]
[2019-03-26 01:51:02,714] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:51:02,717] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9999928e-01 0.0000000e+00 0.0000000e+00 1.7221514e-38 6.6556453e-07], sampled 0.38822854853328437
[2019-03-26 01:51:02,718] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1879906.354991389 W.
[2019-03-26 01:51:03,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:51:03,300] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9529628443114737, 6.9112, 6.9112, 168.912956510431, 774602.8479003058, 774602.8479003058, 235573.7694375404]
[2019-03-26 01:51:03,301] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:51:03,306] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.30016550955249643
[2019-03-26 01:51:18,990] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:51:18,990] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.75, 68.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987746431367228, 6.9112, 168.9123950993829, 904707.0573879168, 850402.5649420111, 255909.3895607021]
[2019-03-26 01:51:18,991] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:51:18,992] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.41158701501600126
[2019-03-26 01:51:18,993] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 904707.0573879168 W.
[2019-03-26 01:51:27,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:51:27,467] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.88747748333333, 64.59959959333334, 1.0, 2.0, 0.2562459579030933, 1.0, 2.0, 0.2562459579030933, 1.0, 2.0, 0.4450144870444634, 6.9112, 6.9112, 184.5923449428631, 1074335.513605555, 1074335.513605555, 291670.6901034155]
[2019-03-26 01:51:27,470] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:51:27,472] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4342638e-08], sampled 0.6738512035403272
[2019-03-26 01:51:27,475] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1074335.513605555 W.
[2019-03-26 01:52:12,405] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:52:12,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.7, 50.0, 1.0, 2.0, 0.3568542276586669, 1.0, 1.0, 0.3568542276586669, 1.0, 1.0, 0.6171463937906206, 6.9112, 6.9112, 178.6582176852504, 1496449.283604462, 1496449.283604462, 332001.5356555699]
[2019-03-26 01:52:12,409] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:52:12,412] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.2205295e-11 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.3457868233045417
[2019-03-26 01:52:26,379] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1086487], dtype=float32), 0.17801544]
[2019-03-26 01:52:26,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.24993951666666, 81.07603591333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9111837347414217, 6.9112, 6.9112, 168.912956510431, 769321.8602446585, 769321.8602446585, 226524.1856976283]
[2019-03-26 01:52:26,380] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:52:26,381] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32842445773285744
[2019-03-26 01:52:31,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7484.2795 3151046802.8056 1679.0000
[2019-03-26 01:52:31,394] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8029.7286 3016533384.1456 1167.0000
[2019-03-26 01:52:31,458] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7250.9537 3217483034.1250 1844.0000
[2019-03-26 01:52:31,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7407.1999 3346373671.7986 1678.0000
[2019-03-26 01:52:31,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8132.2257 2973999179.1472 1078.0000
[2019-03-26 01:52:32,781] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1725000, evaluation results [1725000.0, 7407.199880432568, 3346373671.798578, 1678.0, 7484.2794592111495, 3151046802.805595, 1679.0, 8132.225690855254, 2973999179.14718, 1078.0, 7250.9537470205905, 3217483034.1249547, 1844.0, 8029.728574623792, 3016533384.1455846, 1167.0]
[2019-03-26 01:52:35,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:35,119] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5325
[2019-03-26 01:52:35,126] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.03333333333333, 65.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6133477847939979, 6.911200000000001, 6.9112, 168.912956510431, 532432.5811592638, 532432.5811592632, 168160.9599149521], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1190400.0000, 
sim time next is 1191000.0000, 
raw observation next is [25.86666666666667, 66.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6135682566680225, 6.911199999999999, 6.9112, 168.912956510431, 532736.5550255472, 532736.5550255479, 168193.6562709197], 
processed observation next is [1.0, 0.782608695652174, 0.42496050552922615, 0.6616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5287417764244177, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14798237639598533, 0.14798237639598552, 0.2510353078670443], 
reward next is 0.7490, 
noisyNet noise sample is [array([2.658864], dtype=float32), 0.94962484]. 
=============================================
[2019-03-26 01:52:35,140] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.378174]
 [71.664734]
 [71.847565]
 [71.379654]
 [70.82626 ]], R is [[71.13293457]
 [71.17062378]
 [71.20830536]
 [71.24694061]
 [71.28581238]].
[2019-03-26 01:52:38,502] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9963403e-01 0.0000000e+00 1.5597405e-37 2.7146320e-32 3.6596067e-04], sum to 1.0000
[2019-03-26 01:52:38,516] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9935
[2019-03-26 01:52:38,531] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1001072.106578205 W.
[2019-03-26 01:52:38,541] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [25.65, 68.5, 1.0, 2.0, 0.2211888888831091, 1.0, 2.0, 0.2211888888831091, 1.0, 2.0, 0.3877639571106813, 6.911199999999999, 6.9112, 170.5573041426782, 1001072.106578205, 1001072.106578206, 284078.4855811181], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1089000.0000, 
sim time next is 1089600.0000, 
raw observation next is [25.66666666666667, 68.33333333333333, 1.0, 2.0, 0.3269329975487627, 0.0, 1.0, 0.0, 1.0, 2.0, 0.577675679581863, 6.9112, 6.9112, 168.912956510431, 998553.0985812228, 998553.0985812228, 238181.0733104847], 
processed observation next is [1.0, 0.6086956521739131, 0.4154818325434442, 0.6833333333333332, 1.0, 1.0, 0.18907590066115987, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.4849703409534915, 0.0, 0.0, 0.8294399451523027, 0.2773758607170063, 0.2773758607170063, 0.35549413926938017], 
reward next is 0.6445, 
noisyNet noise sample is [array([-1.4342164], dtype=float32), 0.90950936]. 
=============================================
[2019-03-26 01:52:41,439] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0159825e-33], sum to 1.0000
[2019-03-26 01:52:41,447] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7940
[2019-03-26 01:52:41,452] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.8, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5902727385848233, 6.9112, 6.9112, 168.912956510431, 515696.0292191395, 515696.0292191395, 164576.776836732], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1107000.0000, 
sim time next is 1107600.0000, 
raw observation next is [23.66666666666666, 75.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5863606790336816, 6.911199999999999, 6.9112, 168.912956510431, 512441.549193929, 512441.5491939296, 163990.5623328825], 
processed observation next is [1.0, 0.8260869565217391, 0.3206951026856238, 0.7566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.49556180369961167, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1423448747760914, 0.14234487477609156, 0.24476203333266042], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.2947958], dtype=float32), -1.4900788]. 
=============================================
[2019-03-26 01:52:45,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:45,136] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9391
[2019-03-26 01:52:45,145] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.85, 96.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5731061429291906, 6.911199999999999, 6.9112, 168.912956510431, 501507.0583683559, 501507.0583683565, 162032.3860340648], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1402200.0000, 
sim time next is 1402800.0000, 
raw observation next is [20.93333333333333, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5762928303751996, 6.9112, 6.9112, 168.912956510431, 504170.4427427043, 504170.4427427043, 162498.0868626765], 
processed observation next is [0.0, 0.21739130434782608, 0.19115323854660338, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48328393948195075, 0.0, 0.0, 0.8294399451523027, 0.14004734520630674, 0.14004734520630674, 0.24253445800399476], 
reward next is 0.7575, 
noisyNet noise sample is [array([0.81640923], dtype=float32), -0.26820838]. 
=============================================
[2019-03-26 01:52:48,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:48,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7489
[2019-03-26 01:52:48,121] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.46666666666667, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8139954785004315, 6.911199999999998, 6.9112, 168.912956510431, 687491.0278524919, 687491.0278524932, 204844.4861037745], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1309800.0000, 
sim time next is 1310400.0000, 
raw observation next is [24.5, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8182880225114826, 6.911199999999999, 6.9112, 168.912956510431, 691015.7770398007, 691015.7770398012, 205746.7021740518], 
processed observation next is [1.0, 0.17391304347826086, 0.3601895734597157, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7784000274530276, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19194882695550017, 0.19194882695550033, 0.3070846301105251], 
reward next is 0.6929, 
noisyNet noise sample is [array([0.17494719], dtype=float32), -0.23017247]. 
=============================================
[2019-03-26 01:52:51,978] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:51,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1352
[2019-03-26 01:52:51,997] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.8, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6685020806506624, 6.9112, 6.9112, 168.912956510431, 575046.238524547, 575046.238524547, 177212.9155295578], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1456800.0000, 
sim time next is 1457400.0000, 
raw observation next is [22.75, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6678229504196921, 6.911200000000001, 6.9112, 168.912956510431, 574517.8006273448, 574517.800627344, 177096.7721021461], 
processed observation next is [0.0, 0.8695652173913043, 0.27725118483412325, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5949060370971855, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595882779520402, 0.15958827795204, 0.2643235404509643], 
reward next is 0.7357, 
noisyNet noise sample is [array([0.87505454], dtype=float32), 0.6611815]. 
=============================================
[2019-03-26 01:52:52,816] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:52,824] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0505
[2019-03-26 01:52:52,830] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.98333333333333, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5799487948958961, 6.911200000000001, 6.9112, 168.912956510431, 507551.1779769111, 507551.1779769105, 163027.4877680758], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1372200.0000, 
sim time next is 1372800.0000, 
raw observation next is [20.96666666666667, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5797265261124808, 6.911199999999999, 6.9112, 168.912956510431, 507438.32780335, 507438.3278033506, 162992.812576369], 
processed observation next is [1.0, 0.9130434782608695, 0.1927330173775673, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48747137330790336, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1409550910564861, 0.14095509105648626, 0.24327285459159553], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.08084201], dtype=float32), 0.7111362]. 
=============================================
[2019-03-26 01:52:53,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:52:53,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4314
[2019-03-26 01:52:53,203] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.75, 54.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6398278804073635, 6.9112, 6.9112, 168.912956510431, 551734.8920864526, 551734.8920864526, 172419.7381484324], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1523400.0000, 
sim time next is 1524000.0000, 
raw observation next is [28.6, 55.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6420785678182614, 6.911199999999999, 6.9112, 168.912956510431, 553951.0811976786, 553951.0811976792, 172785.4693406732], 
processed observation next is [0.0, 0.6521739130434783, 0.5545023696682465, 0.55, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5635104485588553, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15387530033268848, 0.15387530033268865, 0.25788876020996004], 
reward next is 0.7421, 
noisyNet noise sample is [array([-0.7184322], dtype=float32), -1.3117609]. 
=============================================
[2019-03-26 01:52:53,214] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[81.29754 ]
 [81.21193 ]
 [81.032646]
 [80.96074 ]
 [80.86792 ]], R is [[81.27882385]
 [81.20869446]
 [81.13757324]
 [81.0664978 ]
 [80.99549103]].
[2019-03-26 01:53:01,375] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:01,387] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6454
[2019-03-26 01:53:01,394] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.28333333333333, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7972126689815492, 6.9112, 6.9112, 168.912956510431, 675854.8918283229, 675854.8918283229, 201393.8710909068], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1651800.0000, 
sim time next is 1652400.0000, 
raw observation next is [23.3, 99.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7918965711869093, 6.911200000000001, 6.9112, 168.912956510431, 671398.792533349, 671398.7925333484, 200305.7722704183], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.746215330715743, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18649966459259695, 0.1864996645925968, 0.29896383920957953], 
reward next is 0.7010, 
noisyNet noise sample is [array([0.899874], dtype=float32), 2.2053332]. 
=============================================
[2019-03-26 01:53:03,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:03,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6405
[2019-03-26 01:53:03,255] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.9, 54.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6467070602991505, 6.9112, 6.9112, 168.912956510431, 557318.4125408985, 557318.4125408985, 173549.7514967731], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1522800.0000, 
sim time next is 1523400.0000, 
raw observation next is [28.75, 54.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6398278804073635, 6.9112, 6.9112, 168.912956510431, 551734.8920864526, 551734.8920864526, 172419.7381484324], 
processed observation next is [0.0, 0.6521739130434783, 0.561611374407583, 0.545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5607657078138579, 0.0, 0.0, 0.8294399451523027, 0.1532596922462368, 0.1532596922462368, 0.2573428927588543], 
reward next is 0.7427, 
noisyNet noise sample is [array([-3.6655061], dtype=float32), 1.0944421]. 
=============================================
[2019-03-26 01:53:03,335] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:03,346] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9456
[2019-03-26 01:53:03,352] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.01666666666667, 96.0, 1.0, 2.0, 0.4769120612620389, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701366.4014867935, 701366.4014867935, 184451.2427898443], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1617000.0000, 
sim time next is 1617600.0000, 
raw observation next is [23.03333333333333, 96.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6842143559443921, 6.911200000000001, 6.9112, 168.912956510431, 584570.2571019263, 584570.2571019257, 179930.0734314655], 
processed observation next is [1.0, 0.7391304347826086, 0.29067930489731436, 0.96, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.6148955560297464, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1623806269727573, 0.16238062697275712, 0.2685523484051724], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.5524364], dtype=float32), -1.1031374]. 
=============================================
[2019-03-26 01:53:04,049] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:04,058] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9255
[2019-03-26 01:53:04,065] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.25, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5522323760237611, 6.911200000000001, 6.9112, 168.912956510431, 485239.5302921806, 485239.53029218, 159013.8333558154], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1391400.0000, 
sim time next is 1392000.0000, 
raw observation next is [20.26666666666667, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5524481516932243, 6.911200000000001, 6.9112, 168.912956510431, 485352.8525933396, 485352.8525933389, 159046.2251882924], 
processed observation next is [0.0, 0.08695652173913043, 0.15955766192733034, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45420506304051733, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13482023683148323, 0.13482023683148303, 0.23738242565416776], 
reward next is 0.7626, 
noisyNet noise sample is [array([1.0028036], dtype=float32), 1.1065342]. 
=============================================
[2019-03-26 01:53:04,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.18797 ]
 [76.312485]
 [76.64717 ]
 [76.49308 ]
 [76.39058 ]], R is [[76.22554016]
 [76.22595215]
 [76.22641754]
 [76.22696686]
 [76.22750092]].
[2019-03-26 01:53:10,349] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:10,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6413
[2019-03-26 01:53:10,370] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.95, 90.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8068490951239626, 6.911200000000002, 6.9112, 168.9129565104286, 678251.3990267029, 678251.3990267016, 203292.957178623], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1753800.0000, 
sim time next is 1754400.0000, 
raw observation next is [25.0, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7848213505972541, 6.9112, 6.9112, 168.912956510431, 660096.2477095983, 660096.2477095983, 198785.4176008842], 
processed observation next is [1.0, 0.30434782608695654, 0.38388625592417064, 0.9033333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7375870129234806, 0.0, 0.0, 0.8294399451523027, 0.18336006880822175, 0.18336006880822175, 0.296694653135648], 
reward next is 0.7033, 
noisyNet noise sample is [array([-1.8868128], dtype=float32), 0.30535343]. 
=============================================
[2019-03-26 01:53:12,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3000538e-37 1.7038533e-26], sum to 1.0000
[2019-03-26 01:53:12,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9526
[2019-03-26 01:53:12,823] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1053305.946533566 W.
[2019-03-26 01:53:12,833] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.06666666666667, 95.33333333333333, 1.0, 2.0, 0.361616045317624, 1.0, 2.0, 0.361616045317624, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1053305.946533566, 1053305.946533566, 268163.444819831], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1615800.0000, 
sim time next is 1616400.0000, 
raw observation next is [23.0, 96.0, 1.0, 2.0, 0.3940645302256797, 1.0, 2.0, 0.3940645302256797, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1146693.790157893, 1146693.790157893, 276116.3197636558], 
processed observation next is [1.0, 0.7391304347826086, 0.28909952606635075, 0.96, 1.0, 1.0, 0.26995726533214426, 1.0, 1.0, 0.26995726533214426, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.31852605282163693, 0.31852605282163693, 0.41211391009500864], 
reward next is 0.5879, 
noisyNet noise sample is [array([-0.2631804], dtype=float32), -0.8838203]. 
=============================================
[2019-03-26 01:53:14,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:14,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4935
[2019-03-26 01:53:14,132] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.8, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6268018563104215, 6.911199999999999, 6.9112, 168.912956510431, 543324.9260934021, 543324.9260934027, 170292.1428466415], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1537200.0000, 
sim time next is 1537800.0000, 
raw observation next is [24.6, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6284248763437603, 6.9112, 6.9112, 168.912956510431, 544581.0258525242, 544581.0258525242, 170553.1823199164], 
processed observation next is [0.0, 0.8260869565217391, 0.36492890995260674, 0.755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5468596052972686, 0.0, 0.0, 0.8294399451523027, 0.15127250718125673, 0.15127250718125673, 0.25455698853718867], 
reward next is 0.7454, 
noisyNet noise sample is [array([1.3578761], dtype=float32), -0.27774358]. 
=============================================
[2019-03-26 01:53:18,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:18,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8897
[2019-03-26 01:53:18,809] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.85, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6246717537790681, 6.9112, 6.9112, 168.912956510431, 540549.4454802179, 540549.4454802179, 169963.992211198], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1830600.0000, 
sim time next is 1831200.0000, 
raw observation next is [21.83333333333334, 97.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6242666989311538, 6.9112, 6.9112, 168.912956510431, 540060.1532347103, 540060.1532347103, 169901.1067873626], 
processed observation next is [1.0, 0.17391304347826086, 0.23380726698262277, 0.9733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5417886572331144, 0.0, 0.0, 0.8294399451523027, 0.15001670923186397, 0.15001670923186397, 0.2535837414736755], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.35709873], dtype=float32), -0.22894008]. 
=============================================
[2019-03-26 01:53:18,870] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:18,872] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4288
[2019-03-26 01:53:18,886] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.2, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7104794858284521, 6.9112, 6.9112, 168.912956510431, 606635.0646343129, 606635.0646343129, 184619.4560487925], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1628400.0000, 
sim time next is 1629000.0000, 
raw observation next is [23.2, 95.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7133950960474341, 6.911199999999999, 6.9112, 168.912956510431, 608971.9607899502, 608971.9607899509, 185150.9901507673], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.955, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6504818244480903, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16915887799720838, 0.16915887799720858, 0.2763447614190557], 
reward next is 0.7237, 
noisyNet noise sample is [array([-2.0672536], dtype=float32), 0.9509014]. 
=============================================
[2019-03-26 01:53:18,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.93811 ]
 [70.094894]
 [70.25516 ]
 [70.240845]
 [70.18716 ]], R is [[69.92689514]
 [69.95207977]
 [69.97734833]
 [70.00243378]
 [70.02689362]].
[2019-03-26 01:53:20,930] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:20,940] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8768
[2019-03-26 01:53:20,949] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.7, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7269040038617297, 6.9112, 6.9112, 168.912956510431, 616791.2632780032, 616791.2632780032, 187618.4068407553], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1915200.0000, 
sim time next is 1915800.0000, 
raw observation next is [23.65, 94.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.777186009729439, 6.911200000000001, 6.9112, 168.912956510431, 659884.8310636559, 659884.8310636552, 197343.4816501675], 
processed observation next is [1.0, 0.17391304347826086, 0.31990521327014215, 0.9483333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.728275621621267, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18330134196212663, 0.18330134196212644, 0.29454250992562314], 
reward next is 0.7055, 
noisyNet noise sample is [array([-0.80405784], dtype=float32), 0.26984245]. 
=============================================
[2019-03-26 01:53:23,267] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.7055667e-30 1.2198546e-35 5.1890836e-22 1.3940612e-09], sum to 1.0000
[2019-03-26 01:53:23,281] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4706
[2019-03-26 01:53:23,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1595688.931059608 W.
[2019-03-26 01:53:23,299] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.61666666666667, 85.66666666666667, 1.0, 2.0, 0.5707308663074476, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9641543678443977, 6.911200000000001, 6.9112, 168.912956510431, 1595688.931059608, 1595688.931059608, 343394.9812938857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1685400.0000, 
sim time next is 1686000.0000, 
raw observation next is [26.73333333333333, 85.33333333333334, 1.0, 2.0, 0.87590301741472, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1224241.133283962, 1224241.133283962, 263393.1924443293], 
processed observation next is [1.0, 0.5217391304347826, 0.4660347551342811, 0.8533333333333334, 1.0, 1.0, 0.850485563150265, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3400669814677672, 0.3400669814677672, 0.3931241678273572], 
reward next is 0.6069, 
noisyNet noise sample is [array([0.7247476], dtype=float32), -1.2041285]. 
=============================================
[2019-03-26 01:53:23,316] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[52.192913]
 [52.271545]
 [53.111744]
 [53.279396]
 [52.418037]], R is [[53.9050827 ]
 [53.36603165]
 [52.83237076]
 [52.84815598]
 [52.84978485]].
[2019-03-26 01:53:23,651] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.4814018e-36 9.0150508e-28], sum to 1.0000
[2019-03-26 01:53:23,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5060
[2019-03-26 01:53:23,671] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1364300.687036065 W.
[2019-03-26 01:53:23,682] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 82.33333333333334, 1.0, 2.0, 0.9617081192015087, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129562418792, 1364300.687036065, 1364300.687036064, 290549.7091256905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1932000.0000, 
sim time next is 1932600.0000, 
raw observation next is [25.75, 82.16666666666667, 1.0, 2.0, 0.9595227831790585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565103639, 1370474.693239353, 1370474.693239353, 291305.6674680298], 
processed observation next is [1.0, 0.34782608695652173, 0.41943127962085314, 0.8216666666666668, 1.0, 1.0, 0.9512322688904319, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451519732, 0.38068741478870916, 0.38068741478870916, 0.4347845783104922], 
reward next is 0.5652, 
noisyNet noise sample is [array([-1.5820773], dtype=float32), 0.53432614]. 
=============================================
[2019-03-26 01:53:26,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:26,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2118
[2019-03-26 01:53:26,524] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.4, 93.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.861114323236742, 6.9112, 6.9112, 168.912956510431, 726592.295516293, 726592.295516293, 215026.6461944017], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1748400.0000, 
sim time next is 1749000.0000, 
raw observation next is [24.45, 93.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8784423864054451, 6.911199999999999, 6.9112, 168.912956510431, 740943.7219133435, 740943.7219133441, 218921.1640465659], 
processed observation next is [1.0, 0.21739130434782608, 0.3578199052132702, 0.9316666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8517590078115185, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20581770053148432, 0.2058177005314845, 0.3267480060396506], 
reward next is 0.6733, 
noisyNet noise sample is [array([1.2077333], dtype=float32), -0.67390454]. 
=============================================
[2019-03-26 01:53:26,546] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.454082]
 [63.53257 ]
 [63.414433]
 [63.19641 ]
 [63.608063]], R is [[63.33255386]
 [63.3782959 ]
 [63.43701553]
 [63.49169922]
 [63.50891876]].
[2019-03-26 01:53:27,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:27,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2946
[2019-03-26 01:53:27,063] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1379394.213994946 W.
[2019-03-26 01:53:27,075] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.15, 95.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.632312093785874, 6.9112, 168.9093699861447, 1379394.213994946, 867823.3383017469, 256458.7068860848], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1908600.0000, 
sim time next is 1909200.0000, 
raw observation next is [24.1, 95.66666666666666, 1.0, 1.0, 0.3464946718769987, 1.0, 1.0, 0.3464946718769987, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 968463.6278082344, 968463.6278082344, 259478.0693397474], 
processed observation next is [1.0, 0.08695652173913043, 0.3412322274881518, 0.9566666666666666, 1.0, 0.5, 0.21264418298433582, 1.0, 0.5, 0.21264418298433582, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.26901767439117624, 0.26901767439117624, 0.3872807005070857], 
reward next is 0.6127, 
noisyNet noise sample is [array([0.35177886], dtype=float32), -0.6874408]. 
=============================================
[2019-03-26 01:53:31,880] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:31,896] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8610
[2019-03-26 01:53:31,901] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.63333333333333, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8475298225554896, 6.911199999999999, 6.9112, 168.912956510431, 703164.1418835776, 703164.1418835782, 211729.3441175795], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2047200.0000, 
sim time next is 2047800.0000, 
raw observation next is [26.56666666666667, 85.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.84502065702476, 6.9112, 6.9112, 168.912956510431, 701501.294962455, 701501.294962455, 211195.3372604203], 
processed observation next is [0.0, 0.6956521739130435, 0.45813586097946307, 0.8583333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8110008012497074, 0.0, 0.0, 0.8294399451523027, 0.19486147082290417, 0.19486147082290417, 0.31521692128420936], 
reward next is 0.6848, 
noisyNet noise sample is [array([-0.18068983], dtype=float32), -0.28260866]. 
=============================================
[2019-03-26 01:53:34,794] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6166273e-24 3.3841137e-25], sum to 1.0000
[2019-03-26 01:53:34,804] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9413
[2019-03-26 01:53:34,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2129801.640371602 W.
[2019-03-26 01:53:34,822] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.96666666666667, 86.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.863514893015822, 6.9112, 168.9076934000957, 2129801.640371602, 1454217.730098185, 311349.667363245], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1873200.0000, 
sim time next is 1873800.0000, 
raw observation next is [26.95, 87.0, 1.0, 2.0, 0.4341072596694766, 1.0, 1.0, 0.4341072596694766, 1.0, 1.0, 0.7439574857846356, 6.911199999999999, 6.9112, 170.5573041426782, 1820738.608640497, 1820738.608640498, 371033.8266635476], 
processed observation next is [1.0, 0.6956521739130435, 0.476303317535545, 0.87, 1.0, 1.0, 0.3182015176740682, 1.0, 0.5, 0.3182015176740682, 1.0, 0.5, 0.6877530314446776, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5057607246223603, 0.5057607246223605, 0.5537818308411159], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.01617062], dtype=float32), 0.79111314]. 
=============================================
[2019-03-26 01:53:36,028] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.9999988e-01 0.0000000e+00 0.0000000e+00 1.6704681e-29 9.9447647e-08], sum to 1.0000
[2019-03-26 01:53:36,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4773
[2019-03-26 01:53:36,049] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1512504.598653792 W.
[2019-03-26 01:53:36,054] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.3606687873590741, 1.0, 1.0, 0.3606687873590741, 1.0, 2.0, 0.613297254346536, 6.9112, 6.9112, 170.5573041426782, 1512504.598653792, 1512504.598653792, 330548.5679925569], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1864800.0000, 
sim time next is 1865400.0000, 
raw observation next is [27.08333333333334, 83.33333333333334, 1.0, 2.0, 0.4006724595381561, 1.0, 2.0, 0.4006724595381561, 1.0, 2.0, 0.6826650275553849, 6.911199999999999, 6.9112, 170.5573041426782, 1680395.957999421, 1680395.957999422, 351394.308373886], 
processed observation next is [1.0, 0.6086956521739131, 0.4826224328594, 0.8333333333333335, 1.0, 1.0, 0.27791862594958566, 1.0, 1.0, 0.27791862594958566, 1.0, 1.0, 0.6130061311651036, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.46677665499983917, 0.46677665499983945, 0.5244691169759492], 
reward next is 0.4755, 
noisyNet noise sample is [array([0.02639058], dtype=float32), -0.6225047]. 
=============================================
[2019-03-26 01:53:39,747] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:53:39,754] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5541
[2019-03-26 01:53:39,762] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 75.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9716395060241141, 6.9112, 6.9112, 168.912956510431, 785313.9094984246, 785313.9094984246, 239971.8998720089], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2119800.0000, 
sim time next is 2120400.0000, 
raw observation next is [30.0, 76.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9725938257150663, 6.9112, 6.9112, 168.912956510431, 785737.3384797098, 785737.3384797098, 240191.7298514048], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.76, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9665778362378855, 0.0, 0.0, 0.8294399451523027, 0.2182603717999194, 0.2182603717999194, 0.3584951191812012], 
reward next is 0.6415, 
noisyNet noise sample is [array([-0.9626065], dtype=float32), -1.5642123]. 
=============================================
[2019-03-26 01:53:40,429] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 01:53:40,430] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:53:40,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:53:40,432] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:53:40,432] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:53:40,433] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:53:40,435] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:53:40,436] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:53:40,436] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:53:40,438] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:53:40,438] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:53:40,482] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-26 01:53:40,506] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-26 01:53:40,533] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-26 01:53:40,534] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-26 01:53:40,588] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-26 01:53:44,465] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0738416], dtype=float32), 0.18790552]
[2019-03-26 01:53:44,468] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.65045439833333, 93.90179871333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.451296793849068, 6.911199999999999, 6.9112, 168.912956510431, 404026.8097271066, 404026.8097271072, 145999.5533249687]
[2019-03-26 01:53:44,470] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:53:44,475] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6382466104796354
[2019-03-26 01:53:56,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0738416], dtype=float32), 0.18790552]
[2019-03-26 01:53:56,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.31666666666667, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6195783560780982, 6.911200000000001, 6.9112, 168.912956510431, 538601.9938697481, 538601.9938697475, 169124.9848622402]
[2019-03-26 01:53:56,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:53:56,128] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.20441068989601385
[2019-03-26 01:54:06,974] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0738416], dtype=float32), 0.18790552]
[2019-03-26 01:54:06,975] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.25, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.616397906876663, 6.9112, 6.9112, 168.912956510431, 534059.7326084058, 534059.7326084058, 168651.0132184689]
[2019-03-26 01:54:06,978] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:54:06,981] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8054128532768469
[2019-03-26 01:54:42,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0738416], dtype=float32), 0.18790552]
[2019-03-26 01:54:42,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.11774885, 83.03866627, 1.0, 2.0, 0.8499792439666141, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00598465002585, 6.9112, 168.9123158383708, 2085023.603275208, 2017780.365359666, 420626.5927198037]
[2019-03-26 01:54:42,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 01:54:42,668] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.19771123e-01 8.32382409e-37 3.91456560e-37 1.43230495e-27
 3.80228877e-01], sampled 0.5605002669985262
[2019-03-26 01:54:42,670] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2085023.603275208 W.
[2019-03-26 01:54:57,368] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0738416], dtype=float32), 0.18790552]
[2019-03-26 01:54:57,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.69704206333333, 95.27813007333333, 1.0, 2.0, 0.9852805402906212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1377216.190758942, 1377216.190758941, 294471.9868541009]
[2019-03-26 01:54:57,371] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 01:54:57,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.999995e-01 0.000000e+00 0.000000e+00 0.000000e+00 4.708245e-07], sampled 0.48002784894100836
[2019-03-26 01:54:57,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1377216.190758942 W.
[2019-03-26 01:55:49,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7180.5996 3209617715.3243 2030.0000
[2019-03-26 01:55:49,514] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8119.6071 2963924035.0461 1140.0000
[2019-03-26 01:55:49,774] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7368.0712 3340313286.9094 1793.0000
[2019-03-26 01:55:50,003] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7999.5007 3009180894.4852 1244.0000
[2019-03-26 01:55:50,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7430.7247 3140154949.2605 1792.0000
[2019-03-26 01:55:51,161] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1750000, evaluation results [1750000.0, 7368.071162081166, 3340313286.9093823, 1793.0, 7430.7246644437555, 3140154949.2604995, 1792.0, 8119.607130219403, 2963924035.04613, 1140.0, 7180.599605571853, 3209617715.3243427, 2030.0, 7999.500723138247, 3009180894.4851985, 1244.0]
[2019-03-26 01:56:02,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1831346e-07 0.0000000e+00 0.0000000e+00 9.9999976e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 01:56:03,005] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0232
[2019-03-26 01:56:03,010] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 64.66666666666666, 1.0, 2.0, 0.8090498106202181, 1.0, 2.0, 0.8090498106202181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2262677.207623485, 2262677.207623485, 424302.1829795183], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2295600.0000, 
sim time next is 2296200.0000, 
raw observation next is [31.81666666666667, 64.83333333333334, 1.0, 2.0, 0.8278535415278669, 1.0, 2.0, 0.8278535415278669, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2315314.492805202, 2315314.492805202, 433659.9778201368], 
processed observation next is [1.0, 0.5652173913043478, 0.7069510268562403, 0.6483333333333334, 1.0, 1.0, 0.7925946283468276, 1.0, 1.0, 0.7925946283468276, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6431429146681117, 0.6431429146681117, 0.6472536982390101], 
reward next is 0.3527, 
noisyNet noise sample is [array([-1.4894623], dtype=float32), 0.119702466]. 
=============================================
[2019-03-26 01:56:10,978] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5283593e-22 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 01:56:10,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6729
[2019-03-26 01:56:10,991] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 74.5, 1.0, 2.0, 0.7759121468843961, 1.0, 2.0, 0.7759121468843961, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2169914.158501467, 2169914.158501467, 408316.0422898773], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2197800.0000, 
sim time next is 2198400.0000, 
raw observation next is [30.0, 74.0, 1.0, 2.0, 0.7725416688824238, 1.0, 2.0, 0.7725416688824238, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2160478.781470233, 2160478.781470232, 406726.3766469124], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.74, 1.0, 1.0, 0.7259538179306311, 1.0, 1.0, 0.7259538179306311, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6001329948528425, 0.6001329948528422, 0.6070542935028543], 
reward next is 0.3929, 
noisyNet noise sample is [array([-1.0674913], dtype=float32), 0.88827217]. 
=============================================
[2019-03-26 01:56:25,741] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:25,753] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2373
[2019-03-26 01:56:25,760] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7483345853117528, 6.9112, 6.9112, 168.912956510431, 633801.0473858983, 633801.0473858983, 191664.0896653905], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [24.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7470465313417556, 6.9112, 6.9112, 168.912956510431, 632708.5283133048, 632708.5283133048, 191416.6183618696], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6915201601728727, 0.0, 0.0, 0.8294399451523027, 0.17575236897591798, 0.17575236897591798, 0.2856964453162233], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.2640525], dtype=float32), -1.2978444]. 
=============================================
[2019-03-26 01:56:34,433] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:34,440] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2269
[2019-03-26 01:56:34,449] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 98.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7775359309615912, 6.911200000000001, 6.9112, 168.912956510431, 654089.4929247305, 654089.49292473, 197323.3395221275], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2697000.0000, 
sim time next is 2697600.0000, 
raw observation next is [24.0, 98.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7805168395777924, 6.911199999999999, 6.9112, 168.912956510431, 656228.650162088, 656228.6501620887, 197913.6887367729], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.9866666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7323376092412103, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18228573615613555, 0.18228573615613575, 0.29539356527876554], 
reward next is 0.7046, 
noisyNet noise sample is [array([0.9049436], dtype=float32), -0.11589429]. 
=============================================
[2019-03-26 01:56:36,958] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:36,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9505
[2019-03-26 01:56:36,977] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6857269895483128, 6.9112, 6.9112, 168.912956510431, 587969.2755099604, 587969.2755099604, 180198.7668066724], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2735400.0000, 
sim time next is 2736000.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6858130707090846, 6.9112, 6.9112, 168.912956510431, 588042.7687557265, 588042.7687557265, 180213.8718330498], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6168452081818105, 0.0, 0.0, 0.8294399451523027, 0.16334521354325737, 0.16334521354325737, 0.26897592810902954], 
reward next is 0.7310, 
noisyNet noise sample is [array([0.44629186], dtype=float32), 0.2481132]. 
=============================================
[2019-03-26 01:56:36,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[75.45446]
 [75.4132 ]
 [75.35027]
 [75.28673]
 [75.2364 ]], R is [[75.64161682]
 [75.61624908]
 [75.59119415]
 [75.5665741 ]
 [75.54241943]].
[2019-03-26 01:56:41,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:41,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4639
[2019-03-26 01:56:41,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1068659.326803073 W.
[2019-03-26 01:56:41,992] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 83.0, 1.0, 2.0, 0.2382238372841409, 1.0, 1.0, 0.2382238372841409, 1.0, 2.0, 0.4151976012313355, 6.9112, 6.9112, 170.5573041426782, 1068659.326803073, 1068659.326803073, 288704.4622189488], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2807400.0000, 
sim time next is 2808000.0000, 
raw observation next is [24.0, 83.0, 1.0, 2.0, 0.6052391269904935, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 912104.7657356473, 912104.765735648, 209976.8966371613], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.83, 1.0, 1.0, 0.5243844903499922, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2533624349265687, 0.2533624349265689, 0.313398353189793], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21112686], dtype=float32), -1.1337614]. 
=============================================
[2019-03-26 01:56:42,019] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[52.873806]
 [53.38168 ]
 [53.03513 ]
 [53.583843]
 [54.591457]], R is [[51.25422668]
 [51.31078339]
 [51.4369278 ]
 [51.60446167]
 [51.70149612]].
[2019-03-26 01:56:42,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3996257e-26], sum to 1.0000
[2019-03-26 01:56:42,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3180
[2019-03-26 01:56:42,892] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1075419.997230314 W.
[2019-03-26 01:56:42,897] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.7012367260735929, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1075419.997230314, 1075419.997230314, 233160.6617666142], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2910600.0000, 
sim time next is 2911200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.2067640318518218, 1.0, 1.0, 0.2067640318518218, 1.0, 1.0, 0.3631268183470592, 6.9112, 6.9112, 170.5573041426782, 938343.9466954537, 938343.9466954537, 279886.0679584393], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.04429401427930336, 1.0, 0.5, 0.04429401427930336, 1.0, 0.5, 0.22332538822812095, 0.0, 0.0, 0.8375144448122397, 0.2606510963042927, 0.2606510963042927, 0.4177403999379691], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1852194], dtype=float32), -1.1043622]. 
=============================================
[2019-03-26 01:56:45,814] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:45,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7876
[2019-03-26 01:56:45,831] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8352331841783956, 6.911199999999999, 6.9112, 168.912956510431, 694847.4702376963, 694847.470237697, 209121.2902118245], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2652000.0000, 
sim time next is 2652600.0000, 
raw observation next is [26.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8355080964515369, 6.911200000000001, 6.9112, 168.912956510431, 695073.5940608928, 695073.5940608921, 209180.3927606604], 
processed observation next is [0.0, 0.6956521739130435, 0.4312796208530806, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7994001176238255, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.193075998350248, 0.1930759983502478, 0.3122095414338215], 
reward next is 0.6878, 
noisyNet noise sample is [array([-0.03120932], dtype=float32), -0.83990824]. 
=============================================
[2019-03-26 01:56:48,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:56:48,614] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4967
[2019-03-26 01:56:48,624] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6816633994057076, 6.9112, 6.9112, 168.912956510431, 585944.8756998703, 585944.8756998703, 179485.7188064767], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2730000.0000, 
sim time next is 2730600.0000, 
raw observation next is [22.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6822046340842732, 6.9112, 6.9112, 168.912956510431, 586100.983660108, 586100.983660108, 179580.9564602829], 
processed observation next is [0.0, 0.6086956521739131, 0.2654028436018958, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6124446757125283, 0.0, 0.0, 0.8294399451523027, 0.16280582879447442, 0.16280582879447442, 0.2680312782989297], 
reward next is 0.7320, 
noisyNet noise sample is [array([1.6931666], dtype=float32), -0.5741199]. 
=============================================
[2019-03-26 01:56:58,765] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 01:56:58,768] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 01:56:58,769] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 01:56:58,770] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:56:58,771] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 01:56:58,771] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:56:58,775] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:56:58,774] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 01:56:58,777] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 01:56:58,778] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:56:58,782] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 01:56:58,802] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-26 01:56:58,829] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-26 01:56:58,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-26 01:56:58,858] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-26 01:56:58,859] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-26 01:57:23,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1086844], dtype=float32), 0.19438668]
[2019-03-26 01:57:23,617] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.4, 83.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5848034006271283, 6.9112, 6.9112, 168.912956510431, 512139.0748846732, 512139.0748846732, 163733.3239483837]
[2019-03-26 01:57:23,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 01:57:23,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9946024902046089
[2019-03-26 01:58:19,311] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1086844], dtype=float32), 0.19438668]
[2019-03-26 01:58:19,311] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.9, 82.0, 1.0, 1.0, 0.6372198458567425, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9127810962738, 890496.1075908613, 890496.1075908607, 207882.9042022582]
[2019-03-26 01:58:19,313] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:58:19,317] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.782117e-31], sampled 0.7320076679558315
[2019-03-26 01:58:19,318] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 890496.1075908613 W.
[2019-03-26 01:58:31,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1086844], dtype=float32), 0.19438668]
[2019-03-26 01:58:31,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.5, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8739200056937145, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 217578.1457620178]
[2019-03-26 01:58:31,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 01:58:31,947] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7934910379933047
[2019-03-26 01:58:34,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1086844], dtype=float32), 0.19438668]
[2019-03-26 01:58:34,424] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.38333333333333, 73.5, 1.0, 1.0, 0.6333472851693538, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128005503716, 885082.0622331436, 885082.0622331442, 207121.3380991568]
[2019-03-26 01:58:34,425] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 01:58:34,427] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3618873e-24], sampled 0.06842581551770766
[2019-03-26 01:58:34,428] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 885082.0622331436 W.
[2019-03-26 01:59:07,265] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8066.1487 2938639401.1795 1354.0000
[2019-03-26 01:59:07,847] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7303.0409 3320246092.5106 2090.0000
[2019-03-26 01:59:08,462] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7352.9086 3106249956.4336 1992.0000
[2019-03-26 01:59:08,537] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7942.2192 2988668220.9707 1485.0000
[2019-03-26 01:59:08,560] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7051.3911 3184824527.8936 2413.0000
[2019-03-26 01:59:09,573] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1775000, evaluation results [1775000.0, 7303.04091395964, 3320246092.5106, 2090.0, 7352.908611733497, 3106249956.433567, 1992.0, 8066.14870853991, 2938639401.1795273, 1354.0, 7051.391079859361, 3184824527.8935766, 2413.0, 7942.21915881169, 2988668220.970668, 1485.0]
[2019-03-26 01:59:14,428] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:14,442] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6830
[2019-03-26 01:59:14,447] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6244317416969462, 6.911199999999999, 6.9112, 168.912956510431, 541706.0223128739, 541706.0223128746, 169909.2347394127], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3052800.0000, 
sim time next is 3053400.0000, 
raw observation next is [22.0, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7282512251041535, 6.9112, 6.9112, 168.912956510431, 631262.74867106, 631262.74867106, 187858.8524811709], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6685990550050653, 0.0, 0.0, 0.8294399451523027, 0.17535076351973888, 0.17535076351973888, 0.28038634698682224], 
reward next is 0.7196, 
noisyNet noise sample is [array([-0.7822492], dtype=float32), -0.15998787]. 
=============================================
[2019-03-26 01:59:17,906] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:17,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4375
[2019-03-26 01:59:17,919] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.551772695856971, 6.9112, 6.9112, 168.912956510431, 484889.0046768906, 484889.0046768906, 158948.283747491], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5521063026057051, 6.9112, 6.9112, 168.912956510431, 485182.2398720158, 485182.2398720158, 158994.6078845106], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4537881739093965, 0.0, 0.0, 0.8294399451523027, 0.13477284440889328, 0.13477284440889328, 0.2373053849022546], 
reward next is 0.7627, 
noisyNet noise sample is [array([0.35452688], dtype=float32), -0.0031072083]. 
=============================================
[2019-03-26 01:59:19,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:19,697] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4713
[2019-03-26 01:59:19,705] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7921946352787352, 6.911199999999999, 6.9112, 168.912956510431, 664518.9565794333, 664518.956579434, 200245.4647070423], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3204600.0000, 
sim time next is 3205200.0000, 
raw observation next is [25.0, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7892950295281866, 6.9112, 6.9112, 168.912956510431, 662484.2189085392, 662484.2189085392, 199663.8657795361], 
processed observation next is [0.0, 0.08695652173913043, 0.38388625592417064, 0.9133333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.743042718936813, 0.0, 0.0, 0.8294399451523027, 0.18402339414126087, 0.18402339414126087, 0.29800576982020316], 
reward next is 0.7020, 
noisyNet noise sample is [array([-0.25827882], dtype=float32), 0.20991892]. 
=============================================
[2019-03-26 01:59:23,986] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:23,995] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0713
[2019-03-26 01:59:24,001] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5981115608380878, 6.9112, 6.9112, 168.912956510431, 520281.9062229052, 520281.9062229052, 165801.8900650148], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3048600.0000, 
sim time next is 3049200.0000, 
raw observation next is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6034247649099435, 6.9112, 6.9112, 168.912956510431, 524904.8674610177, 524904.8674610177, 166611.285686001], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5163716645243213, 0.0, 0.0, 0.8294399451523027, 0.14580690762806048, 0.14580690762806048, 0.24867356072537464], 
reward next is 0.7513, 
noisyNet noise sample is [array([2.2871048], dtype=float32), 0.96875054]. 
=============================================
[2019-03-26 01:59:30,711] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:30,728] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6657
[2019-03-26 01:59:30,734] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.917038839338267, 6.911199999999999, 6.9112, 168.912956510431, 751894.4923981376, 751894.4923981382, 227220.9663428659], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3363600.0000, 
sim time next is 3364200.0000, 
raw observation next is [27.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9173937933889053, 6.9112, 6.9112, 168.912956510431, 752145.5371931671, 752145.5371931671, 227303.2222336854], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8992607236450063, 0.0, 0.0, 0.8294399451523027, 0.20892931588699087, 0.20892931588699087, 0.33925854064729166], 
reward next is 0.6607, 
noisyNet noise sample is [array([0.3072523], dtype=float32), -0.8111516]. 
=============================================
[2019-03-26 01:59:35,000] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:35,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0825
[2019-03-26 01:59:35,021] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666667, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8540300510005505, 6.911200000000001, 6.9112, 168.912956510431, 708231.983350676, 708231.9833506754, 213144.3903580837], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3460800.0000, 
sim time next is 3461400.0000, 
raw observation next is [26.5, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8511439990408703, 6.911200000000001, 6.9112, 168.912956510431, 706268.9355070755, 706268.9355070748, 212523.9151802899], 
processed observation next is [1.0, 0.043478260869565216, 0.4549763033175356, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8184682915132564, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1961858154186321, 0.1961858154186319, 0.31719987340341776], 
reward next is 0.6828, 
noisyNet noise sample is [array([1.0634152], dtype=float32), -0.8339029]. 
=============================================
[2019-03-26 01:59:37,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 01:59:37,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0805
[2019-03-26 01:59:37,260] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7668027587411408, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 195219.3994030775], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3216600.0000, 
sim time next is 3217200.0000, 
raw observation next is [25.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7667725735148422, 6.911200000000001, 6.9112, 168.912956510431, 646576.9607043526, 646576.960704352, 195213.4472166886], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7155763091644416, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17960471130676461, 0.17960471130676445, 0.2913633540547591], 
reward next is 0.7086, 
noisyNet noise sample is [array([-0.6331079], dtype=float32), 2.0562346]. 
=============================================
[2019-03-26 01:59:56,223] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2866997e-04 2.9945302e-33 0.0000000e+00 9.9957138e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 01:59:56,231] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5308
[2019-03-26 01:59:56,244] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.578944265312822, 1.0, 2.0, 0.578944265312822, 1.0, 1.0, 1.005434729054156, 6.911200000000001, 6.9112, 170.5573041426782, 2428869.649993361, 2428869.64999336, 474029.6092987918], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3499800.0000, 
sim time next is 3500400.0000, 
raw observation next is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.7717142940305638, 1.0, 2.0, 0.7717142940305638, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2158162.626062531, 2158162.626062531, 406342.3089183496], 
processed observation next is [1.0, 0.5217391304347826, 0.7314375987361774, 0.6566666666666667, 1.0, 1.0, 0.7249569807597154, 1.0, 1.0, 0.7249569807597154, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.599489618350703, 0.599489618350703, 0.606481058087089], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.42760298], dtype=float32), -0.46614885]. 
=============================================
[2019-03-26 01:59:57,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7513106e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 01:59:57,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9371
[2019-03-26 01:59:57,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2842645.982640526 W.
[2019-03-26 01:59:57,135] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.0161895956398, 1.0, 2.0, 1.0161895956398, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2842645.982640526, 2842645.982640526, 538846.420487672], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3600600.0000, 
sim time next is 3601200.0000, 
raw observation next is [33.0, 63.00000000000001, 1.0, 2.0, 1.022215441872828, 1.0, 2.0, 1.022215441872828, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2859521.724579817, 2859521.724579817, 542555.6678241096], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.6300000000000001, 1.0, 1.0, 1.026765592617865, 1.0, 1.0, 1.026765592617865, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7943115901610603, 0.7943115901610603, 0.8097845788419545], 
reward next is 0.1902, 
noisyNet noise sample is [array([0.31808257], dtype=float32), 0.066248186]. 
=============================================
[2019-03-26 01:59:57,517] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7763866e-01 3.9757777e-38 0.0000000e+00 2.2361282e-02 0.0000000e+00], sum to 1.0000
[2019-03-26 01:59:57,528] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4679
[2019-03-26 01:59:57,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2458931.36334346 W.
[2019-03-26 01:59:57,549] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 0.5861027183695566, 1.0, 2.0, 0.5861027183695566, 1.0, 2.0, 1.017866594677103, 6.9112, 6.9112, 170.5573041426782, 2458931.36334346, 2458931.36334346, 479792.9179584563], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3516600.0000, 
sim time next is 3517200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.582044052427085, 6.9112, 168.9092111998286, 2760026.088286235, 2284116.781405303, 474598.387965405], 
processed observation next is [1.0, 0.7391304347826086, 0.7156398104265403, 0.67, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.06708440524270846, 0.0, 0.8294215539626343, 0.7666739134128431, 0.6344768837236954, 0.7083558029334404], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8967019], dtype=float32), -0.4056101]. 
=============================================
[2019-03-26 01:59:58,633] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6258976e-17 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 01:59:58,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9219
[2019-03-26 01:59:58,659] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 0.8791540771113333, 1.0, 2.0, 0.8791540771113333, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2458931.362103197, 2458931.362103196, 460249.0602132813], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3516600.0000, 
sim time next is 3517200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.8641527805264414, 1.0, 2.0, 0.8641527805264414, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2416933.226439743, 2416933.226439743, 452315.6027989822], 
processed observation next is [1.0, 0.7391304347826086, 0.7156398104265403, 0.67, 1.0, 1.0, 0.8363286512366763, 1.0, 1.0, 0.8363286512366763, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6713703406777063, 0.6713703406777063, 0.6750979146253465], 
reward next is 0.3249, 
noisyNet noise sample is [array([0.07375038], dtype=float32), 0.7168363]. 
=============================================
[2019-03-26 02:00:02,457] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 4.7729031e-36 0.0000000e+00 2.7871294e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 02:00:02,468] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6880
[2019-03-26 02:00:02,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1873058.883554624 W.
[2019-03-26 02:00:02,484] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.66666666666667, 63.0, 1.0, 2.0, 0.4465707600125838, 1.0, 2.0, 0.4465707600125838, 1.0, 1.0, 0.7755457269347902, 6.9112, 6.9112, 170.5573041426782, 1873058.883554624, 1873058.883554624, 380166.1770103064], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [32.83333333333333, 63.0, 1.0, 2.0, 0.6579788465921198, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005972616483412, 6.9112, 168.9123932518746, 1816327.958099, 1749093.226346188, 375798.9680402038], 
processed observation next is [1.0, 0.5217391304347826, 0.7551342812006318, 0.63, 1.0, 1.0, 0.5879263211953251, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009477261648341173, 0.0, 0.8294371792949298, 0.5045355439163889, 0.4858592295406078, 0.5608939821495579], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.40629187], dtype=float32), -0.686808]. 
=============================================
[2019-03-26 02:00:02,514] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[45.237488]
 [44.069263]
 [43.51254 ]
 [41.430515]
 [42.81515 ]], R is [[44.86979294]
 [44.85368347]
 [44.75326157]
 [44.30572891]
 [43.8626709 ]].
[2019-03-26 02:00:03,594] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:00:03,605] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9403
[2019-03-26 02:00:03,611] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9465397375460392, 6.911199999999999, 6.9112, 168.912956510431, 771116.3254456399, 771116.3254456405, 234086.926013281], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3880800.0000, 
sim time next is 3881400.0000, 
raw observation next is [29.0, 79.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9578235698602193, 6.9112, 6.9112, 168.912956510431, 779084.0109606222, 779084.0109606222, 236797.230672655], 
processed observation next is [0.0, 0.9565217391304348, 0.5734597156398105, 0.7983333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9485653290978284, 0.0, 0.0, 0.8294399451523027, 0.2164122252668395, 0.2164122252668395, 0.3534287024965], 
reward next is 0.6466, 
noisyNet noise sample is [array([-1.0534328], dtype=float32), -0.48610863]. 
=============================================
[2019-03-26 02:00:08,799] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.9602261e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 02:00:08,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0373
[2019-03-26 02:00:08,813] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.66666666666666, 72.33333333333333, 1.0, 1.0, 0.3104055376733657, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5390717662587027, 6.911200000000001, 6.9112, 168.9128454099878, 867555.9649253932, 867555.9649253925, 225366.0936782738], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3966000.0000, 
sim time next is 3966600.0000, 
raw observation next is [31.83333333333333, 71.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.083530276994667, 6.9112, 168.9118583408885, 951104.8553004098, 828848.6204910191, 254812.7982477977], 
processed observation next is [0.0, 0.9130434782608695, 0.7077409162717218, 0.7166666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.017233027699466684, 0.0, 0.8294345526371049, 0.2641957931390027, 0.23023572791417196, 0.3803176093250712], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94663644], dtype=float32), -0.82999027]. 
=============================================
[2019-03-26 02:00:09,530] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.8827107e-30 3.1472399e-31 4.7548289e-14 6.9282085e-28], sum to 1.0000
[2019-03-26 02:00:09,544] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0182
[2019-03-26 02:00:09,554] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1034791.420997972 W.
[2019-03-26 02:00:09,558] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.2468092209210946, 1.0, 1.0, 0.2468092209210946, 1.0, 1.0, 0.4106903316036678, 6.9112, 6.9112, 170.5573041426782, 1034791.420997972, 1034791.420997972, 282893.9147729384], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3728400.0000, 
sim time next is 3729000.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.3577476151373725, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6006948604714751, 6.911199999999999, 6.9112, 168.912956510431, 1012914.683651231, 1012914.683651231, 240630.2245346526], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.74, 1.0, 1.0, 0.2262019459486416, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.5130425127700916, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2813651899031197, 0.2813651899031197, 0.35914958885769044], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.46940008], dtype=float32), 0.4757735]. 
=============================================
[2019-03-26 02:00:09,578] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[31.866001]
 [32.10371 ]
 [32.320465]
 [32.46604 ]
 [32.976025]], R is [[31.4106369 ]
 [31.09653091]
 [30.78556633]
 [30.47771072]
 [30.17293358]].
[2019-03-26 02:00:12,687] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.7170704e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 02:00:12,694] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8986
[2019-03-26 02:00:12,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1077748.052077554 W.
[2019-03-26 02:00:12,715] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.33333333333334, 72.66666666666667, 1.0, 2.0, 0.2570497069997873, 1.0, 2.0, 0.2570497069997873, 1.0, 2.0, 0.4340027766254777, 6.9112, 6.9112, 170.5573041426782, 1077748.052077554, 1077748.052077554, 286875.218239985], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3741600.0000, 
sim time next is 3742200.0000, 
raw observation next is [28.5, 72.0, 1.0, 2.0, 0.2626452980917673, 1.0, 2.0, 0.2626452980917673, 1.0, 2.0, 0.4437834890892729, 6.9112, 6.9112, 170.5573041426782, 1101221.077317549, 1101221.077317549, 288863.3673554498], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.72, 1.0, 1.0, 0.11162084107441841, 1.0, 1.0, 0.11162084107441841, 1.0, 1.0, 0.32168718181618644, 0.0, 0.0, 0.8375144448122397, 0.3058947436993191, 0.3058947436993191, 0.4311393542618654], 
reward next is 0.5689, 
noisyNet noise sample is [array([0.36779842], dtype=float32), 1.7489988]. 
=============================================
[2019-03-26 02:00:15,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 9.739182e-38 0.000000e+00], sum to 1.0000
[2019-03-26 02:00:15,263] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7565
[2019-03-26 02:00:15,268] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.917339454392556, 6.9112, 168.9127088425868, 835766.8653860342, 831411.3315225334, 254965.5783574623], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3892800.0000, 
sim time next is 3893400.0000, 
raw observation next is [27.75, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.026897928660139, 6.9112, 6.9112, 168.9127618046545, 829092.6427256791, 829092.6427256791, 254153.511185265], 
processed observation next is [0.0, 0.043478260869565216, 0.514218009478673, 0.905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0328023520245597, 0.0, 0.0, 0.8294389890578034, 0.2303035118682442, 0.2303035118682442, 0.3793335987839776], 
reward next is 0.6207, 
noisyNet noise sample is [array([0.42367685], dtype=float32), -0.01863959]. 
=============================================
[2019-03-26 02:00:17,139] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 02:00:17,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:00:17,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:00:17,145] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:00:17,146] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:00:17,148] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:00:17,147] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:00:17,148] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:00:17,151] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:00:17,156] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:00:17,156] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:00:17,181] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-26 02:00:17,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-26 02:00:17,231] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-26 02:00:17,256] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-26 02:00:17,257] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-26 02:00:44,763] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:00:44,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.18333333333333, 95.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7022400254700761, 6.9112, 6.9112, 168.912956510431, 599732.7955540614, 599732.7955540614, 183128.396732446]
[2019-03-26 02:00:44,767] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:00:44,772] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6936264304821885
[2019-03-26 02:00:50,034] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:00:50,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.46666666666667, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6754145808376667, 6.911200000000001, 6.9112, 168.912956510431, 582090.7609759013, 582090.7609759006, 178394.6465397232]
[2019-03-26 02:00:50,037] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:00:50,041] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.21866204101508002
[2019-03-26 02:01:36,039] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:01:36,040] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.65, 85.0, 1.0, 2.0, 0.6199502468967143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 866352.504453877, 866352.5044538764, 204521.1482666163]
[2019-03-26 02:01:36,042] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:01:36,047] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.1575693e-15], sampled 0.3989692102745024
[2019-03-26 02:01:43,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:01:43,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.0, 67.0, 1.0, 2.0, 0.7508795044168585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049410.634305745, 1049410.634305745, 232216.9342765888]
[2019-03-26 02:01:43,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:01:43,544] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1994174e-23], sampled 0.8360030719390915
[2019-03-26 02:01:43,545] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1049410.634305745 W.
[2019-03-26 02:01:55,196] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:01:55,201] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 74.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.933708323090071, 6.911199999999999, 6.9112, 168.912956510431, 765443.4517879876, 765443.4517879882, 231195.6366964886]
[2019-03-26 02:01:55,203] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:01:55,206] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.007275593227534793
[2019-03-26 02:02:05,259] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1530286], dtype=float32), 0.11872427]
[2019-03-26 02:02:05,261] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8808629675738998, 6.9112, 6.9112, 168.912956510431, 726135.4099857132, 726135.4099857132, 218995.1039388075]
[2019-03-26 02:02:05,261] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:02:05,263] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9608477544269467
[2019-03-26 02:02:26,399] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7446.7739 3320609957.2773 1518.0000
[2019-03-26 02:02:26,415] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8148.5404 2957080847.5405 1084.0000
[2019-03-26 02:02:26,607] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8055.5033 2998082933.7057 1079.0000
[2019-03-26 02:02:26,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7307.2839 3185105119.5569 1641.0000
[2019-03-26 02:02:26,832] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7497.5896 3127649105.2137 1651.0000
[2019-03-26 02:02:27,849] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1800000, evaluation results [1800000.0, 7446.7739365223615, 3320609957.277304, 1518.0, 7497.589613044306, 3127649105.21369, 1651.0, 8148.5404236469685, 2957080847.5405364, 1084.0, 7307.283870709027, 3185105119.5568776, 1641.0, 8055.503323387878, 2998082933.7056675, 1079.0]
[2019-03-26 02:02:31,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8542813e-01 1.0468877e-33 0.0000000e+00 1.1457194e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 02:02:31,211] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2603
[2019-03-26 02:02:31,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2948109.947320574 W.
[2019-03-26 02:02:31,230] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 67.0, 1.0, 2.0, 0.7639485006931455, 1.0, 2.0, 0.7025642898608353, 1.0, 1.0, 1.03, 7.00510277497764, 6.9112, 170.5573041426782, 2948109.947320574, 2880843.569883422, 542289.2956049516], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4122000.0000, 
sim time next is 4122600.0000, 
raw observation next is [33.83333333333334, 67.66666666666667, 1.0, 2.0, 0.4575474373731645, 1.0, 2.0, 0.4575474373731645, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1279044.779054824, 1279044.779054824, 287618.0485569345], 
processed observation next is [1.0, 0.7391304347826086, 0.8025276461295423, 0.6766666666666667, 1.0, 1.0, 0.34644269563031865, 1.0, 1.0, 0.34644269563031865, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3552902164041178, 0.3552902164041178, 0.42928066948796195], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34470275], dtype=float32), -1.0571288]. 
=============================================
[2019-03-26 02:02:35,338] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.84322269e-28 1.28229425e-20 1.02031105e-22 7.15601840e-13
 1.00000000e+00], sum to 1.0000
[2019-03-26 02:02:35,348] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6806
[2019-03-26 02:02:35,356] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.33333333333334, 76.33333333333334, 1.0, 2.0, 0.8309928609472053, 1.0, 1.0, 0.7360864699878652, 1.0, 1.0, 1.03, 7.005108063095315, 6.9112, 170.5573041426782, 3088950.125033265, 3021679.959502316, 565897.0138201539], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4177200.0000, 
sim time next is 4177800.0000, 
raw observation next is [32.5, 75.0, 1.0, 2.0, 0.7657877718806385, 1.0, 2.0, 0.7034839254545819, 1.0, 2.0, 1.03, 7.005102920034087, 6.9112, 170.5573041426782, 2951973.494920485, 2884707.0135735, 542914.6214854912], 
processed observation next is [1.0, 0.34782608695652173, 0.7393364928909952, 0.75, 1.0, 1.0, 0.7178165926272753, 1.0, 1.0, 0.6427517174151589, 1.0, 1.0, 1.0365853658536586, 0.009390292003408707, 0.0, 0.8375144448122397, 0.8199926374779125, 0.8013075037704166, 0.8103203305753599], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.35613412], dtype=float32), -1.0409656]. 
=============================================
[2019-03-26 02:02:37,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:37,173] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7067
[2019-03-26 02:02:37,178] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.66666666666666, 72.33333333333333, 1.0, 1.0, 0.6208114059884275, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912845407571, 867556.427171227, 867556.427171227, 204686.3996106625], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3966000.0000, 
sim time next is 3966600.0000, 
raw observation next is [31.83333333333333, 71.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.083530527880241, 6.9112, 168.9118583392926, 951105.0333544116, 828848.6205604722, 254812.7982478187], 
processed observation next is [0.0, 0.9130434782608695, 0.7077409162717218, 0.7166666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.017233052788024138, 0.0, 0.8294345526292681, 0.2641958425984477, 0.2302357279334645, 0.3803176093251025], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5613119], dtype=float32), 0.32847786]. 
=============================================
[2019-03-26 02:02:48,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:48,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2235
[2019-03-26 02:02:48,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2839159.396964317 W.
[2019-03-26 02:02:48,776] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.0, 52.0, 1.0, 2.0, 0.7120794211958437, 1.0, 2.0, 0.6766297501121844, 1.0, 2.0, 1.03, 7.005098684626426, 6.9112, 170.5573041426782, 2839159.396964317, 2771895.949612124, 525171.9214517173], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4206000.0000, 
sim time next is 4206600.0000, 
raw observation next is [36.0, 51.5, 1.0, 2.0, 0.729965596640141, 1.0, 2.0, 0.6855728378343331, 1.0, 2.0, 1.03, 7.005100095034359, 6.9112, 170.5573041426782, 2876728.079478685, 2809463.621793919, 530960.943579534], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.515, 1.0, 1.0, 0.6746573453495676, 1.0, 1.0, 0.6211720937763049, 1.0, 1.0, 1.0365853658536586, 0.00939000950343587, 0.0, 0.8375144448122397, 0.7990911331885235, 0.7804065616094219, 0.7924790202679611], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5040817], dtype=float32), -0.27276587]. 
=============================================
[2019-03-26 02:02:51,053] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:51,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9357
[2019-03-26 02:02:51,071] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2777506.279673854 W.
[2019-03-26 02:02:51,077] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.5, 73.0, 1.0, 2.0, 0.9929293016093661, 1.0, 2.0, 0.9929293016093661, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2777506.279673854, 2777506.279673854, 524744.5562786593], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4354200.0000, 
sim time next is 4354800.0000, 
raw observation next is [33.66666666666666, 72.33333333333333, 1.0, 2.0, 0.7956086073765876, 1.0, 2.0, 0.7183943432025564, 1.0, 1.0, 1.03, 7.005105272017084, 6.9112, 170.5573041426782, 3014616.396934978, 2947348.230766899, 553229.4577528121], 
processed observation next is [1.0, 0.391304347826087, 0.7946287519747232, 0.7233333333333333, 1.0, 1.0, 0.7537453100922742, 1.0, 1.0, 0.6607160761476583, 1.0, 0.5, 1.0365853658536586, 0.009390527201708387, 0.0, 0.8375144448122397, 0.8373934435930495, 0.8187078418796941, 0.8257156085862867], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25456882], dtype=float32), -1.1391233]. 
=============================================
[2019-03-26 02:02:55,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:55,904] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7830
[2019-03-26 02:02:55,913] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.66666666666666, 58.66666666666667, 1.0, 1.0, 0.3026899035175271, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5256722613292171, 6.9112, 6.9112, 168.9128779097612, 845982.8630760867, 845982.8630760867, 222836.7559590751], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4304400.0000, 
sim time next is 4305000.0000, 
raw observation next is [34.33333333333334, 60.83333333333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.033527893743012, 6.9112, 168.912133879479, 915617.9737802838, 828834.7784068034, 254813.1278750738], 
processed observation next is [1.0, 0.8260869565217391, 0.8262243285939973, 0.6083333333333333, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.012232789374301233, 0.0, 0.8294359056577263, 0.25433832605007883, 0.23023188289077873, 0.3803181013060803], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1223687], dtype=float32), -1.5809243]. 
=============================================
[2019-03-26 02:02:55,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[23.075386]
 [25.786633]
 [25.942406]
 [25.945675]
 [26.068241]], R is [[21.48472214]
 [21.93728256]
 [21.85498238]
 [22.05117798]
 [22.42283249]].
[2019-03-26 02:02:57,627] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:57,636] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3789
[2019-03-26 02:02:57,644] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9459898530095288, 6.9112, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315182, 234037.4501749378], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9414787852830724, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 232939.3033989572], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9286326649793566, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.34767060208799583], 
reward next is 0.6523, 
noisyNet noise sample is [array([0.3816019], dtype=float32), 0.9813856]. 
=============================================
[2019-03-26 02:02:57,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:57,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0187
[2019-03-26 02:02:58,003] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333334, 76.33333333333334, 1.0, 2.0, 0.3102549575983953, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5388102584660709, 6.911199999999999, 6.9112, 168.912956510431, 867134.9349280995, 867134.9349281002, 225322.6628126341], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4224000.0000, 
sim time next is 4224600.0000, 
raw observation next is [31.5, 75.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.141662459303089, 6.9112, 168.9115567060784, 992361.35928631, 828864.7136808868, 254813.1074867715], 
processed observation next is [1.0, 0.9130434782608695, 0.6919431279620853, 0.75, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.023046245930308906, 0.0, 0.8294330714720981, 0.27565593313508613, 0.23024019824469077, 0.38031807087577835], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32078797], dtype=float32), -0.34071854]. 
=============================================
[2019-03-26 02:02:58,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:02:58,065] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0946
[2019-03-26 02:02:58,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 885312.4325484104 W.
[2019-03-26 02:02:58,081] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.16666666666667, 59.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990826384818768, 6.9112, 168.9122887987933, 885312.4325484104, 828822.9579326616, 254812.5909925696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4215000.0000, 
sim time next is 4215600.0000, 
raw observation next is [34.0, 60.0, 1.0, 1.0, 0.5994743694041144, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9128916676205, 837727.1049636556, 837727.104963655, 200648.296358823], 
processed observation next is [1.0, 0.8260869565217391, 0.8104265402843602, 0.6, 1.0, 0.5, 0.5174389992820655, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439626744418, 0.23270197360101544, 0.23270197360101527, 0.2994750691922731], 
reward next is 0.7005, 
noisyNet noise sample is [array([-0.9030432], dtype=float32), -1.3627576]. 
=============================================
[2019-03-26 02:03:05,369] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:05,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3891
[2019-03-26 02:03:05,394] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1542050.828375929 W.
[2019-03-26 02:03:05,400] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5515600251819582, 0.0, 1.0, 0.0, 1.0, 2.0, 0.957877360053442, 6.911200000000001, 6.9112, 168.912956510431, 1542050.828375929, 1542050.828375928, 337545.9708781464], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4332000.0000, 
sim time next is 4332600.0000, 
raw observation next is [29.5, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.194505044777188, 6.9112, 168.9112432299847, 1654876.887681398, 1453892.576909504, 311356.1909983776], 
processed observation next is [1.0, 0.13043478260869565, 0.5971563981042655, 0.865, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.028330504477718767, 0.0, 0.8294315321609682, 0.4596880243559439, 0.40385904914152887, 0.4647107328333994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8688104], dtype=float32), -0.5646435]. 
=============================================
[2019-03-26 02:03:09,585] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:09,594] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7158
[2019-03-26 02:03:09,601] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 64.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9553060444068258, 6.9112, 6.9112, 168.912956510431, 772090.1655176576, 772090.1655176576, 235920.2254252924], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4387200.0000, 
sim time next is 4387800.0000, 
raw observation next is [32.0, 63.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.949884078646609, 6.9112, 6.9112, 168.912956510431, 769224.7252520331, 769224.7252520331, 234671.8411329422], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6366666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.938883022739767, 0.0, 0.0, 0.8294399451523027, 0.21367353479223142, 0.21367353479223142, 0.3502564793028988], 
reward next is 0.6497, 
noisyNet noise sample is [array([-0.50818807], dtype=float32), 0.8468866]. 
=============================================
[2019-03-26 02:03:11,006] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:11,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6703
[2019-03-26 02:03:11,026] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9702149386570234, 6.9112, 6.9112, 168.912956510431, 780676.5250469598, 780676.5250469598, 239421.0102528523], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4385400.0000, 
sim time next is 4386000.0000, 
raw observation next is [32.0, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.967818394302069, 6.9112, 6.9112, 168.912956510431, 779160.2719094914, 779160.2719094914, 238847.3987044761], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9607541393927669, 0.0, 0.0, 0.8294399451523027, 0.2164334088637476, 0.2164334088637476, 0.3564886547828002], 
reward next is 0.6435, 
noisyNet noise sample is [array([-0.17239527], dtype=float32), 0.23308374]. 
=============================================
[2019-03-26 02:03:11,045] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[47.621895]
 [45.76181 ]
 [44.298996]
 [41.49906 ]
 [38.061813]], R is [[48.29182434]
 [48.45156097]
 [48.61024094]
 [48.76940918]
 [48.93400955]].
[2019-03-26 02:03:14,850] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:14,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3121
[2019-03-26 02:03:14,871] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 897560.4208357706 W.
[2019-03-26 02:03:14,877] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 84.0, 1.0, 1.0, 0.2140917989722881, 1.0, 1.0, 0.2140917989722881, 1.0, 2.0, 0.3718066535750378, 6.9112, 6.9112, 170.5573041426782, 897560.4208357706, 897560.4208357706, 273881.1798737252], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4405800.0000, 
sim time next is 4406400.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.3095317403599446, 1.0, 2.0, 0.3095317403599446, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 865109.3872403906, 865109.3872403906, 251758.9480980269], 
processed observation next is [0.0, 0.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.1681105305541501, 1.0, 1.0, 0.1681105305541501, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.24030816312233072, 0.24030816312233072, 0.3757596240269058], 
reward next is 0.6242, 
noisyNet noise sample is [array([0.9841133], dtype=float32), 1.1461452]. 
=============================================
[2019-03-26 02:03:17,811] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:17,820] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7814
[2019-03-26 02:03:17,826] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8361102659790878, 6.911199999999999, 6.9112, 168.912956510431, 698088.7306454465, 698088.730645447, 209380.3463392431], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4505400.0000, 
sim time next is 4506000.0000, 
raw observation next is [26.0, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8277493799833088, 6.9112, 6.9112, 168.912956510431, 692208.163827431, 692208.163827431, 207615.3733136083], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7899382682723276, 0.0, 0.0, 0.8294399451523027, 0.19228004550761973, 0.19228004550761973, 0.3098736915128482], 
reward next is 0.6901, 
noisyNet noise sample is [array([-0.89445657], dtype=float32), 0.6464923]. 
=============================================
[2019-03-26 02:03:17,848] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.5224  ]
 [66.37102 ]
 [66.20794 ]
 [66.023735]
 [65.75683 ]], R is [[66.77073669]
 [66.79051971]
 [66.80780029]
 [66.82315826]
 [66.83718109]].
[2019-03-26 02:03:22,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:22,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6075
[2019-03-26 02:03:22,156] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8236232373313827, 6.9112, 6.9112, 168.912956510431, 687809.8816305388, 687809.8816305388, 206713.0482622803], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4828800.0000, 
sim time next is 4829400.0000, 
raw observation next is [28.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8224630674143456, 6.911200000000001, 6.9112, 168.912956510431, 686840.7208903376, 686840.7208903369, 206466.9834523022], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7834915456272508, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19078908913620488, 0.19078908913620468, 0.3081596767944809], 
reward next is 0.6918, 
noisyNet noise sample is [array([-1.4523202], dtype=float32), 0.67354023]. 
=============================================
[2019-03-26 02:03:22,779] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:03:22,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9492
[2019-03-26 02:03:22,798] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 884333.54217065 W.
[2019-03-26 02:03:22,804] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.3164072104998575, 1.0, 1.0, 0.3164072104998575, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 884333.54217065, 884333.54217065, 253113.4558796122], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4769400.0000, 
sim time next is 4770000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.1990874382478266, 1.0, 2.0, 0.1990874382478266, 1.0, 1.0, 0.3343141877794746, 6.9112, 6.9112, 170.5573041426782, 834631.523739663, 834631.523739663, 268957.9207657617], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.79, 1.0, 1.0, 0.03504510632268264, 1.0, 1.0, 0.03504510632268264, 1.0, 0.5, 0.188188033877408, 0.0, 0.0, 0.8375144448122397, 0.23184208992768415, 0.23184208992768415, 0.4014297324862115], 
reward next is 0.5986, 
noisyNet noise sample is [array([0.10991268], dtype=float32), 1.9754165]. 
=============================================
[2019-03-26 02:03:22,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[39.009342]
 [40.472202]
 [42.24675 ]
 [41.116035]
 [41.509396]], R is [[38.49650955]
 [38.73376465]
 [38.34642792]
 [37.9629631 ]
 [37.58333206]].
[2019-03-26 02:03:35,418] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 02:03:35,420] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:03:35,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:03:35,422] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:03:35,422] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:03:35,423] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:03:35,425] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:03:35,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:03:35,427] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:03:35,425] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:03:35,428] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:03:35,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-26 02:03:35,483] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-26 02:03:35,507] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-26 02:03:35,509] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-26 02:03:35,509] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-26 02:04:20,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:04:20,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.336410535, 98.081025845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6415164450817735, 6.9112, 6.9112, 168.912956510431, 556144.7897460267, 556144.7897460267, 172666.4665324924]
[2019-03-26 02:04:20,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:04:20,270] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6362499429589842
[2019-03-26 02:04:22,049] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:04:22,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.4, 77.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.903048260208624, 6.9112, 6.9112, 168.912956510431, 744567.7241068663, 744567.7241068663, 224104.8143596262]
[2019-03-26 02:04:22,054] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:04:22,057] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4581358336573236
[2019-03-26 02:04:31,269] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:04:31,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.35487534, 93.41004057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7399543426841996, 6.9112, 6.9112, 168.912956510431, 631680.6846793208, 631680.6846793208, 190104.9886085814]
[2019-03-26 02:04:31,272] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:04:31,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9185389807576623
[2019-03-26 02:04:34,083] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:04:34,084] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.26666666666667, 50.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.022262974524669, 6.9112, 168.9124174003668, 1532600.530719342, 1453808.885574667, 311353.125495034]
[2019-03-26 02:04:34,086] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:04:34,089] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.08017649537927674
[2019-03-26 02:04:34,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1532600.530719342 W.
[2019-03-26 02:04:40,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:04:40,001] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.35829538333333, 75.08614613833333, 1.0, 2.0, 0.6635181176897417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 927263.2166689639, 927263.2166689639, 213176.1907062705]
[2019-03-26 02:04:40,002] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:04:40,005] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.45491538798211917
[2019-03-26 02:04:40,008] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 927263.2166689639 W.
[2019-03-26 02:05:01,898] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:05:01,901] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.41667292333333, 78.4253612, 1.0, 1.0, 0.6027917659750468, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129066811316, 842364.7928795556, 842364.7928795556, 201267.0894423792]
[2019-03-26 02:05:01,903] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:05:01,909] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5607213038300874
[2019-03-26 02:05:25,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:05:25,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.162954785, 69.917209445, 1.0, 2.0, 0.8062756411348467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1126871.985061717, 1126871.985061716, 245457.406157436]
[2019-03-26 02:05:25,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:05:25,663] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8550751235529397
[2019-03-26 02:05:25,665] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1126871.985061717 W.
[2019-03-26 02:05:42,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.2273811], dtype=float32), 0.075513445]
[2019-03-26 02:05:42,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.51666666666667, 83.5, 1.0, 2.0, 1.022437328889547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1429188.574204229, 1429188.574204229, 305838.8070891271]
[2019-03-26 02:05:42,733] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:05:42,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7642864239231707
[2019-03-26 02:05:42,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1429188.574204229 W.
[2019-03-26 02:05:44,205] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7347.3663 3105759986.0026 2011.0000
[2019-03-26 02:05:44,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7289.9086 3319539945.3921 2143.0000
[2019-03-26 02:05:45,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7925.3164 2989339394.3799 1566.0000
[2019-03-26 02:05:45,161] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8061.0837 2937849718.7621 1381.0000
[2019-03-26 02:05:45,207] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7030.6175 3185157732.2717 2464.0000
[2019-03-26 02:05:46,225] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1825000, evaluation results [1825000.0, 7289.908576300456, 3319539945.3920636, 2143.0, 7347.36625577973, 3105759986.0025682, 2011.0, 8061.083746410787, 2937849718.7620935, 1381.0, 7030.617469616518, 3185157732.2716994, 2464.0, 7925.316405625689, 2989339394.3799295, 1566.0]
[2019-03-26 02:05:46,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:05:46,792] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3049
[2019-03-26 02:05:46,800] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2127837.272406468 W.
[2019-03-26 02:05:46,806] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.93333333333333, 63.0, 1.0, 2.0, 0.880567887537323, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982325905253897, 6.9112, 168.9125334016632, 2127837.272406468, 2077378.233649394, 429571.1534071422], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4981200.0000, 
sim time next is 4981800.0000, 
raw observation next is [30.96666666666667, 63.0, 1.0, 2.0, 0.513818996979912, 1.0, 1.0, 0.513818996979912, 1.0, 2.0, 0.8815540721810393, 6.9112, 6.9112, 170.5573041426782, 2155402.985432427, 2155402.985432427, 422815.8820421736], 
processed observation next is [1.0, 0.6521739130434783, 0.6666666666666667, 0.63, 1.0, 1.0, 0.41423975539748426, 1.0, 0.5, 0.41423975539748426, 1.0, 1.0, 0.855553746562243, 0.0, 0.0, 0.8375144448122397, 0.5987230515090075, 0.5987230515090075, 0.6310684806599606], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.0955065], dtype=float32), -1.690138]. 
=============================================
[2019-03-26 02:05:50,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:05:50,110] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1222
[2019-03-26 02:05:50,115] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9014907300439298, 6.9112, 6.9112, 168.912956510431, 739553.6332888254, 739553.6332888254, 223593.2358692416], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5074200.0000, 
sim time next is 5074800.0000, 
raw observation next is [30.0, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9029835186019803, 6.911200000000001, 6.9112, 168.912956510431, 740771.9149529624, 740771.9149529617, 223940.1405627995], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8816872178072931, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2057699763758229, 0.2057699763758227, 0.3342390157653724], 
reward next is 0.6658, 
noisyNet noise sample is [array([-0.8001187], dtype=float32), 0.96915895]. 
=============================================
[2019-03-26 02:05:51,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:05:51,125] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1054
[2019-03-26 02:05:51,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2075267.377658935 W.
[2019-03-26 02:05:51,140] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.83333333333334, 63.0, 1.0, 2.0, 0.4947342357997717, 1.0, 2.0, 0.4947342357997717, 1.0, 2.0, 0.8482130507506881, 6.9112, 6.9112, 170.5573041426782, 2075267.377658935, 2075267.377658935, 409552.8504737044], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4979400.0000, 
sim time next is 4980000.0000, 
raw observation next is [30.86666666666667, 63.00000000000001, 1.0, 2.0, 0.7266278213844886, 1.0, 2.0, 0.7266278213844886, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2031955.003649053, 2031955.003649053, 385726.7787284649], 
processed observation next is [1.0, 0.6521739130434783, 0.6619273301737759, 0.6300000000000001, 1.0, 1.0, 0.6706359293789018, 1.0, 1.0, 0.6706359293789018, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5644319454580703, 0.5644319454580703, 0.575711610042485], 
reward next is 0.4243, 
noisyNet noise sample is [array([-0.40083015], dtype=float32), -0.4406658]. 
=============================================
[2019-03-26 02:05:51,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[48.076294]
 [47.768147]
 [46.951283]
 [47.181915]
 [47.375328]], R is [[47.90383148]
 [47.81351852]
 [47.33538437]
 [47.25469208]
 [47.14375305]].
[2019-03-26 02:06:01,041] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 3.577631e-36 0.000000e+00], sum to 1.0000
[2019-03-26 02:06:01,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8617
[2019-03-26 02:06:01,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2440936.491859858 W.
[2019-03-26 02:06:01,068] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.123930732989216, 6.9112, 168.9116774261549, 2440936.491859858, 2290019.131072782, 475935.5017141041], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4892400.0000, 
sim time next is 4893000.0000, 
raw observation next is [31.41666666666666, 65.16666666666667, 1.0, 2.0, 0.9567071852435389, 1.0, 1.0, 0.9567071852435389, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2676074.174041111, 2676074.174041111, 503369.029499013], 
processed observation next is [1.0, 0.6521739130434783, 0.6879936808846759, 0.6516666666666667, 1.0, 1.0, 0.9478399822211312, 1.0, 0.5, 0.9478399822211312, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.743353937233642, 0.743353937233642, 0.7512970589537508], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2282358], dtype=float32), 0.8244866]. 
=============================================
[2019-03-26 02:06:01,085] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[41.806435]
 [40.67306 ]
 [41.396725]
 [40.814133]
 [41.254295]], R is [[40.15950394]
 [39.75790787]
 [39.36032867]
 [38.9667244 ]
 [38.57705688]].
[2019-03-26 02:06:05,593] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:05,602] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8779
[2019-03-26 02:06:05,615] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1210116.371305152 W.
[2019-03-26 02:06:05,621] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8658029848775513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210116.371305152, 1210116.371305152, 260708.04260938], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5286600.0000, 
sim time next is 5287200.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.2845927384003036, 1.0, 1.0, 0.2845927384003036, 1.0, 1.0, 0.4942434703445577, 6.911199999999999, 6.9112, 170.5573041426782, 1193293.670093313, 1193293.670093314, 298210.4163151552], 
processed observation next is [1.0, 0.17391304347826086, 0.5545023696682465, 0.88, 1.0, 1.0, 0.1380635402413296, 1.0, 0.5, 0.1380635402413296, 1.0, 0.5, 0.3832237443226313, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.33147046391480917, 0.33147046391480944, 0.44509017360470926], 
reward next is 0.5549, 
noisyNet noise sample is [array([-0.52721417], dtype=float32), 1.1130275]. 
=============================================
[2019-03-26 02:06:07,237] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:07,250] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8006
[2019-03-26 02:06:07,260] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2846061.341069337 W.
[2019-03-26 02:06:07,266] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666667, 68.0, 1.0, 2.0, 0.7153654267260696, 1.0, 2.0, 0.6782727528772974, 1.0, 2.0, 1.03, 7.005098943736852, 6.9112, 170.5573041426782, 2846061.341069337, 2778797.708105807, 526225.7675049404], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5242800.0000, 
sim time next is 5243400.0000, 
raw observation next is [31.5, 68.5, 1.0, 2.0, 0.72063473558592, 1.0, 2.0, 0.6809074073072225, 1.0, 2.0, 1.03, 7.005099359242087, 6.9112, 170.5573041426782, 2857129.106012347, 2789865.175405515, 527925.2780221851], 
processed observation next is [1.0, 0.6956521739130435, 0.6919431279620853, 0.685, 1.0, 1.0, 0.6634153440794216, 1.0, 1.0, 0.6155510931412319, 1.0, 1.0, 1.0365853658536586, 0.009389935924208715, 0.0, 0.8375144448122397, 0.7936469738923186, 0.7749625487237541, 0.787948176152515], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2488333], dtype=float32), 2.0302885]. 
=============================================
[2019-03-26 02:06:11,035] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:11,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4312
[2019-03-26 02:06:11,052] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8589263740333171, 6.9112, 6.9112, 168.912956510431, 711329.0709204307, 711329.0709204307, 214193.7696572291], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8549327959628396, 6.911200000000001, 6.9112, 168.912956510431, 708355.0793879715, 708355.079387971, 213322.7986113017], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8230887755644384, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1967652998299921, 0.19676529982999194, 0.3183922367332861], 
reward next is 0.6816, 
noisyNet noise sample is [array([-0.36345473], dtype=float32), -0.2944563]. 
=============================================
[2019-03-26 02:06:12,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3910417e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 02:06:12,341] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5667
[2019-03-26 02:06:12,355] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2701535.833748727 W.
[2019-03-26 02:06:12,362] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.76666666666667, 63.33333333333334, 1.0, 2.0, 0.9657999989759063, 1.0, 2.0, 0.9657999989759063, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2701535.833748727, 2701535.833748727, 508669.2570675359], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5305200.0000, 
sim time next is 5305800.0000, 
raw observation next is [34.08333333333334, 61.66666666666666, 1.0, 2.0, 0.6760738157573871, 1.0, 2.0, 0.6586269473929561, 1.0, 1.0, 1.03, 7.005095845674468, 6.9112, 170.5573041426782, 2763535.523112178, 2696274.109416755, 513878.9766712632], 
processed observation next is [1.0, 0.391304347826087, 0.8143759873617699, 0.6166666666666666, 1.0, 1.0, 0.6097274888643218, 1.0, 1.0, 0.5887071655336821, 1.0, 0.5, 1.0365853658536586, 0.009389584567446808, 0.0, 0.8375144448122397, 0.7676487564200494, 0.748965030393543, 0.7669835472705421], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06120073], dtype=float32), -0.15349954]. 
=============================================
[2019-03-26 02:06:20,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:20,982] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4331
[2019-03-26 02:06:20,990] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1486999.339713172 W.
[2019-03-26 02:06:20,997] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.91666666666666, 92.16666666666667, 1.0, 2.0, 0.3545910709071403, 1.0, 2.0, 0.3545910709071403, 1.0, 1.0, 0.6158074251066386, 6.9112, 6.9112, 170.5573041426782, 1486999.339713172, 1486999.339713172, 329176.8894453949], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5453400.0000, 
sim time next is 5454000.0000, 
raw observation next is [27.9, 92.0, 1.0, 2.0, 0.504403879894993, 0.0, 1.0, 0.0, 1.0, 2.0, 0.8759827304655319, 6.9112, 6.9112, 168.912956510431, 1410124.166386742, 1410124.166386742, 310566.0195138537], 
processed observation next is [1.0, 0.13043478260869565, 0.5213270142180094, 0.92, 1.0, 1.0, 0.402896240837341, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.8487594273969901, 0.0, 0.0, 0.8294399451523027, 0.39170115732965055, 0.39170115732965055, 0.4635313724087369], 
reward next is 0.5365, 
noisyNet noise sample is [array([0.92303693], dtype=float32), 0.64740056]. 
=============================================
[2019-03-26 02:06:21,005] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[6.68165  ]
 [6.909464 ]
 [7.4102464]
 [9.363451 ]
 [8.784139 ]], R is [[7.49860907]
 [7.42362309]
 [7.34938669]
 [7.27589273]
 [7.58523369]].
[2019-03-26 02:06:30,722] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:30,731] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8132
[2019-03-26 02:06:30,736] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [34.9, 58.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.912877357347885, 6.9112, 168.9127627041674, 829991.354541874, 828801.380874446, 254813.1908265441], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5334000.0000, 
sim time next is 5334600.0000, 
raw observation next is [34.65, 59.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.952158065571084, 6.9112, 168.9125461299762, 857869.244766855, 828812.2539781401, 254813.2126867169], 
processed observation next is [1.0, 0.7391304347826086, 0.8412322274881516, 0.5983333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004095806557108439, 0.0, 0.8294379299963965, 0.23829701243523752, 0.2302256261050389, 0.38031822789062225], 
reward next is 0.4149, 
noisyNet noise sample is [array([-0.6766191], dtype=float32), -1.5606637]. 
=============================================
[2019-03-26 02:06:38,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:38,707] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8287
[2019-03-26 02:06:38,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.85, 77.0, 1.0, 1.0, 0.2993767440462565, 1.0, 1.0, 0.2993767440462565, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 836716.1450959519, 836716.1450959519, 249769.4775561565], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5423400.0000, 
sim time next is 5424000.0000, 
raw observation next is [30.83333333333333, 77.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.97720062831482, 6.9112, 168.9124342309503, 875642.1585038177, 828819.185988218, 254812.8484683899], 
processed observation next is [1.0, 0.782608695652174, 0.6603475513428118, 0.7766666666666667, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0066000628314819565, 0.0, 0.829437380520952, 0.24323393291772716, 0.2302275516633939, 0.38031768428117896], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.36068153], dtype=float32), -0.23606986]. 
=============================================
[2019-03-26 02:06:38,731] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[47.138233]
 [54.186565]
 [53.398598]
 [51.508514]
 [49.545708]], R is [[45.69820786]
 [45.86843491]
 [45.72648621]
 [45.84283066]
 [46.01387405]].
[2019-03-26 02:06:42,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:42,483] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3064
[2019-03-26 02:06:42,491] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 69.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9626306414585646, 6.911200000000001, 6.9112, 168.912956510431, 780680.4913811622, 780680.4913811616, 237869.4130058261], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5766600.0000, 
sim time next is 5767200.0000, 
raw observation next is [30.7, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9609302768663373, 6.9112, 6.9112, 168.912956510431, 779819.2208083083, 779819.2208083083, 237474.8180284696], 
processed observation next is [0.0, 0.782608695652174, 0.6540284360189573, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9523539961784601, 0.0, 0.0, 0.8294399451523027, 0.2166164502245301, 0.2166164502245301, 0.35444002690816356], 
reward next is 0.6456, 
noisyNet noise sample is [array([0.17912427], dtype=float32), 0.40255797]. 
=============================================
[2019-03-26 02:06:43,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:43,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8413
[2019-03-26 02:06:43,730] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333333, 70.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9323529650449315, 6.9112, 6.9112, 168.912956510431, 760188.0323666892, 760188.0323666892, 230681.9057701059], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5651400.0000, 
sim time next is 5652000.0000, 
raw observation next is [30.5, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9348837557847477, 6.9112, 6.9112, 168.912956510431, 761728.5319104239, 761728.5319104239, 231266.0142106487], 
processed observation next is [0.0, 0.43478260869565216, 0.6445497630331753, 0.7, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9205899460789605, 0.0, 0.0, 0.8294399451523027, 0.21159125886400665, 0.21159125886400665, 0.34517315553828165], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.66843617], dtype=float32), 1.1876065]. 
=============================================
[2019-03-26 02:06:43,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.165  ]
 [66.18624]
 [66.19203]
 [66.19992]
 [66.19526]], R is [[66.34131622]
 [66.33360291]
 [66.32682037]
 [66.32091522]
 [66.31570435]].
[2019-03-26 02:06:48,296] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:06:48,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3488
[2019-03-26 02:06:48,317] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2363583.172171449 W.
[2019-03-26 02:06:48,322] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.90000000000001, 50.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.002111514007911, 6.9112, 168.9123608666858, 2363583.172171449, 2299087.642635076, 476605.2350567354], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5578200.0000, 
sim time next is 5578800.0000, 
raw observation next is [34.0, 49.33333333333334, 1.0, 2.0, 0.5505060804946484, 1.0, 1.0, 0.5505060804946484, 1.0, 2.0, 0.9455482088113606, 6.9112, 6.9112, 170.5573041426782, 2309451.470840448, 2309451.470840448, 449709.45733931], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.4933333333333334, 1.0, 1.0, 0.4584410608369258, 1.0, 0.5, 0.4584410608369258, 1.0, 1.0, 0.9335953765992201, 0.0, 0.0, 0.8375144448122397, 0.64151429745568, 0.64151429745568, 0.6712081452825522], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5844176], dtype=float32), -0.22181177]. 
=============================================
[2019-03-26 02:06:52,907] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 5.6373917e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 02:06:52,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3412
[2019-03-26 02:06:52,930] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2161113.632846708 W.
[2019-03-26 02:06:52,936] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.2, 78.0, 1.0, 2.0, 0.5151789663869464, 1.0, 2.0, 0.5151789663869464, 1.0, 2.0, 0.8946954923265005, 6.9112, 6.9112, 170.5573041426782, 2161113.632846708, 2161113.632846708, 425801.9884618388], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5907600.0000, 
sim time next is 5908200.0000, 
raw observation next is [30.35, 77.33333333333333, 1.0, 2.0, 1.024379343942034, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005994622919536, 6.9112, 168.9123931210846, 2329125.823812698, 2261875.48004083, 470891.0687555172], 
processed observation next is [1.0, 0.391304347826087, 0.637440758293839, 0.7733333333333333, 1.0, 1.0, 1.0293727035446192, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479462291953577, 0.0, 0.8294371786526911, 0.6469793955035272, 0.6282987444557862, 0.7028224906798765], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.08607582], dtype=float32), -1.6413321]. 
=============================================
[2019-03-26 02:06:53,795] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 02:06:53,800] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:06:53,802] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:06:53,803] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:06:53,805] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:06:53,806] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:06:53,805] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:06:53,804] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:06:53,807] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:06:53,808] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:06:53,809] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:06:53,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-26 02:06:53,872] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-26 02:06:53,903] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-26 02:06:53,904] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-26 02:06:53,958] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-26 02:07:30,993] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:07:30,994] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.36897418, 75.11267387, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6468618691010539, 6.9112, 6.9112, 168.912956510431, 554311.5126843146, 554311.5126843146, 173583.3724212459]
[2019-03-26 02:07:30,995] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:07:30,998] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.03415504861254304
[2019-03-26 02:07:35,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:07:35,288] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.97729449333333, 93.83087187333334, 1.0, 2.0, 0.7002407297683741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 978606.5260708167, 978606.5260708161, 220894.0021064481]
[2019-03-26 02:07:35,289] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:07:35,292] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.744303e-29], sampled 0.16500844673223047
[2019-03-26 02:07:35,292] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 978606.5260708167 W.
[2019-03-26 02:07:39,186] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:07:39,187] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.8, 55.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9625404927570704, 6.9112, 6.9112, 168.912956510431, 781685.5843106072, 781685.5843106072, 237902.5721720872]
[2019-03-26 02:07:39,187] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:07:39,189] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8840787003076185
[2019-03-26 02:07:44,125] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:07:44,127] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7335599766008563, 6.9112, 6.9112, 168.912956510431, 623325.327861791, 623325.327861791, 188874.743023008]
[2019-03-26 02:07:44,129] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:07:44,131] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6492649179274268
[2019-03-26 02:08:05,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:08:05,653] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.35, 46.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.656137196645775, 6.9112, 168.9087297529453, 1982587.786537258, 1454116.924886395, 311355.443279307]
[2019-03-26 02:08:05,658] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:08:05,660] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.311379e-26], sampled 0.44523402028852643
[2019-03-26 02:08:05,663] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1982587.786537258 W.
[2019-03-26 02:08:45,989] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:08:45,991] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.549329085, 57.26336711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6076823847036551, 6.911200000000001, 6.9112, 168.912956510431, 530558.2402622065, 530558.240262206, 167226.4252690524]
[2019-03-26 02:08:45,993] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:08:45,996] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8292985625980714
[2019-03-26 02:08:46,610] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:08:46,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.03333333333334, 69.0, 1.0, 2.0, 0.6444247908901453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900569.0760793983, 900569.0760793983, 209314.0114228594]
[2019-03-26 02:08:46,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:08:46,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.3650007e-27], sampled 0.6559397624722231
[2019-03-26 02:08:46,615] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 900569.0760793983 W.
[2019-03-26 02:09:02,579] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8066.4927 2938126982.1590 1367.0000
[2019-03-26 02:09:02,938] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1672183], dtype=float32), 0.07462876]
[2019-03-26 02:09:02,939] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.5, 62.0, 1.0, 2.0, 0.514925261504682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815602.3133601305, 815602.3133601305, 196925.204137357]
[2019-03-26 02:09:02,940] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:09:02,943] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.9111403e-24], sampled 0.3171880468397811
[2019-03-26 02:09:03,436] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7292.7175 3320192789.1535 2137.0000
[2019-03-26 02:09:03,499] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7355.6993 3106522150.4491 2012.0000
[2019-03-26 02:09:03,657] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7928.1133 2989533345.8271 1543.0000
[2019-03-26 02:09:03,730] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7036.2410 3185749135.6529 2455.0000
[2019-03-26 02:09:04,748] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1850000, evaluation results [1850000.0, 7292.7174695392405, 3320192789.1535153, 2137.0, 7355.699260287252, 3106522150.4491467, 2012.0, 8066.492712512164, 2938126982.158986, 1367.0, 7036.241018268984, 3185749135.65287, 2455.0, 7928.11329688303, 2989533345.827106, 1543.0]
[2019-03-26 02:09:05,322] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:05,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5860
[2019-03-26 02:09:05,340] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.83333333333334, 72.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9256073654362227, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 229143.543655411], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5649600.0000, 
sim time next is 5650200.0000, 
raw observation next is [30.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.927484491037817, 6.911199999999999, 6.9112, 168.912956510431, 757301.8589368007, 757301.8589368014, 229565.6755480697], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.72, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9115664524851428, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21036162748244466, 0.21036162748244483, 0.34263533663891], 
reward next is 0.6574, 
noisyNet noise sample is [array([-1.7730572], dtype=float32), -0.69214183]. 
=============================================
[2019-03-26 02:09:13,166] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:13,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4992
[2019-03-26 02:09:13,181] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333334, 70.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8685900457516822, 6.9112, 6.9112, 168.912956510431, 709184.5388892707, 709184.5388892707, 215950.9359382207], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6025200.0000, 
sim time next is 6025800.0000, 
raw observation next is [30.16666666666666, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8873866743541058, 6.911200000000001, 6.9112, 168.912956510431, 724395.7064655559, 724395.7064655554, 220188.4944876709], 
processed observation next is [1.0, 0.7391304347826086, 0.6287519747235385, 0.71, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8626666760415923, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20122102957376553, 0.20122102957376536, 0.32863954401144907], 
reward next is 0.6714, 
noisyNet noise sample is [array([0.2352574], dtype=float32), 0.54443187]. 
=============================================
[2019-03-26 02:09:22,409] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:22,421] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0680
[2019-03-26 02:09:22,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2249772.726801912 W.
[2019-03-26 02:09:22,434] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.6, 73.0, 1.0, 2.0, 0.9676854207263067, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991445554459156, 6.9112, 168.912478909362, 2249772.726801912, 2192843.929596273, 454063.8707408551], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6192000.0000, 
sim time next is 6192600.0000, 
raw observation next is [29.5, 73.5, 1.0, 2.0, 0.8016627904103062, 1.0, 1.0, 0.8016627904103062, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2241999.327991275, 2241999.327991275, 420678.6226706957], 
processed observation next is [1.0, 0.6956521739130435, 0.5971563981042655, 0.735, 1.0, 1.0, 0.7610395065184412, 1.0, 0.5, 0.7610395065184412, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6227775911086876, 0.6227775911086876, 0.6278785412995458], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.34764], dtype=float32), -0.9278804]. 
=============================================
[2019-03-26 02:09:25,968] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:25,979] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2684
[2019-03-26 02:09:25,985] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.55, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9078185260801325, 6.911199999999999, 6.9112, 168.912956510431, 745140.7977649419, 745140.7977649425, 225085.492570618], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6049800.0000, 
sim time next is 6050400.0000, 
raw observation next is [26.5, 91.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9050156975467482, 6.911200000000001, 6.9112, 168.912956510431, 743142.1708332984, 743142.1708332978, 224442.897677973], 
processed observation next is [1.0, 0.0, 0.4549763033175356, 0.9166666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8841654848131074, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20642838078702733, 0.20642838078702716, 0.3349893995193627], 
reward next is 0.6650, 
noisyNet noise sample is [array([0.57213175], dtype=float32), -0.078844205]. 
=============================================
[2019-03-26 02:09:29,197] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:29,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0844
[2019-03-26 02:09:29,220] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1043425.397041172 W.
[2019-03-26 02:09:29,225] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.25, 85.5, 1.0, 2.0, 0.7465990207154516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1043425.397041172, 1043425.397041172, 231227.2041146707], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5988600.0000, 
sim time next is 5989200.0000, 
raw observation next is [28.4, 85.0, 1.0, 2.0, 0.3719526265031032, 1.0, 1.0, 0.3719526265031032, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1039653.915037135, 1039653.915037135, 265299.5097338154], 
processed observation next is [1.0, 0.30434782608695654, 0.5450236966824644, 0.85, 1.0, 1.0, 0.2433164174736183, 1.0, 0.5, 0.2433164174736183, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.28879275417698197, 0.28879275417698197, 0.3959694175131573], 
reward next is 0.6040, 
noisyNet noise sample is [array([-1.3978974], dtype=float32), -0.64806235]. 
=============================================
[2019-03-26 02:09:34,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:34,907] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4530
[2019-03-26 02:09:34,919] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.9, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8660824409978584, 6.911200000000001, 6.9112, 168.912956510431, 716125.1428498683, 716125.1428498676, 215746.332222438], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6271200.0000, 
sim time next is 6271800.0000, 
raw observation next is [30.86666666666667, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8636884149729191, 6.9112, 6.9112, 168.912956510431, 714550.4691691964, 714550.4691691964, 215226.6328187855], 
processed observation next is [0.0, 0.6086956521739131, 0.6619273301737759, 0.62, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8337663597230721, 0.0, 0.0, 0.8294399451523027, 0.1984862414358879, 0.1984862414358879, 0.32123378032654554], 
reward next is 0.6788, 
noisyNet noise sample is [array([-0.03558144], dtype=float32), 1.2193782]. 
=============================================
[2019-03-26 02:09:35,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:35,640] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3521
[2019-03-26 02:09:35,644] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9065132177278262, 6.911200000000001, 6.9112, 168.912956510431, 744705.6636257953, 744705.6636257946, 224806.4135978945], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6312000.0000, 
sim time next is 6312600.0000, 
raw observation next is [27.3, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9068893275492245, 6.9112, 6.9112, 168.912956510431, 745001.8913374706, 745001.8913374706, 224893.8423780881], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8864503994502737, 0.0, 0.0, 0.8294399451523027, 0.20694496981596405, 0.20694496981596405, 0.33566245131057926], 
reward next is 0.6643, 
noisyNet noise sample is [array([0.5916681], dtype=float32), -0.1315245]. 
=============================================
[2019-03-26 02:09:53,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9185153e-01 5.8336853e-14 2.7002135e-16 8.0814850e-01 4.2840685e-13], sum to 1.0000
[2019-03-26 02:09:53,714] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9301
[2019-03-26 02:09:53,722] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.91666666666667, 74.83333333333334, 1.0, 2.0, 0.4737976617794069, 1.0, 2.0, 0.4737976617794069, 1.0, 2.0, 0.8141852383859811, 6.911199999999999, 6.9112, 170.5573041426782, 1987363.029152949, 1987363.02915295, 396024.3023554514], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6601800.0000, 
sim time next is 6602400.0000, 
raw observation next is [29.1, 74.0, 1.0, 2.0, 0.7084225736297752, 1.0, 2.0, 0.7084225736297752, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1980998.454661385, 1980998.454661385, 377747.6707779086], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.74, 1.0, 1.0, 0.6487018959394882, 1.0, 1.0, 0.6487018959394882, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5502773485170513, 0.5502773485170513, 0.563802493698371], 
reward next is 0.4362, 
noisyNet noise sample is [array([0.48584712], dtype=float32), -0.75877607]. 
=============================================
[2019-03-26 02:09:53,955] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:09:53,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4967
[2019-03-26 02:09:53,972] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.4, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9199347958841663, 6.911199999999999, 6.9112, 168.912956510431, 753197.3754166673, 753197.3754166679, 227860.6671449768], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6344400.0000, 
sim time next is 6345000.0000, 
raw observation next is [30.55, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9198626209454766, 6.911199999999999, 6.9112, 168.912956510431, 752838.8486701441, 752838.8486701448, 227830.3963827685], 
processed observation next is [0.0, 0.43478260869565216, 0.6469194312796209, 0.68, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9022714889578982, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20912190240837336, 0.20912190240837356, 0.3400453677354754], 
reward next is 0.6600, 
noisyNet noise sample is [array([-0.12366296], dtype=float32), -1.5630693]. 
=============================================
[2019-03-26 02:09:54,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.70352 ]
 [68.72782 ]
 [68.73414 ]
 [68.562325]
 [68.61588 ]], R is [[68.64849854]
 [68.62192535]
 [68.59658051]
 [68.57306671]
 [68.55106354]].
[2019-03-26 02:09:59,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 1.4727894e-36 0.0000000e+00 6.5720194e-13 0.0000000e+00], sum to 1.0000
[2019-03-26 02:09:59,276] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3276
[2019-03-26 02:09:59,285] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1996867.435940448 W.
[2019-03-26 02:09:59,290] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 68.0, 1.0, 2.0, 0.7140921778819447, 1.0, 2.0, 0.7140921778819447, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1996867.435940448, 1996867.435940448, 380207.982566754], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6696000.0000, 
sim time next is 6696600.0000, 
raw observation next is [29.56666666666667, 67.33333333333334, 1.0, 2.0, 0.5316143023933698, 1.0, 2.0, 0.5316143023933698, 1.0, 1.0, 0.90608904309717, 6.911199999999999, 6.9112, 170.5573041426782, 2230127.056402405, 2230127.056402406, 434343.2236030977], 
processed observation next is [1.0, 0.5217391304347826, 0.6003159557661929, 0.6733333333333335, 1.0, 1.0, 0.43567988240165034, 1.0, 1.0, 0.43567988240165034, 1.0, 0.5, 0.8854744428014268, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6194797378895569, 0.6194797378895572, 0.648273468064325], 
reward next is 0.3517, 
noisyNet noise sample is [array([0.62852365], dtype=float32), -0.6906338]. 
=============================================
[2019-03-26 02:10:08,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:10:08,956] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1675
[2019-03-26 02:10:08,965] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.13333333333333, 78.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6710597579395087, 6.9112, 6.9112, 168.912956510431, 587538.6706923113, 587538.6706923113, 177494.7021451158], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6765000.0000, 
sim time next is 6765600.0000, 
raw observation next is [23.26666666666667, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6278090202634649, 6.911199999999999, 6.9112, 168.912956510431, 549370.9688078815, 549370.9688078822, 170353.0038753136], 
processed observation next is [1.0, 0.30434782608695654, 0.3017377567140602, 0.78, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5461085612969083, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15260304689107818, 0.15260304689107837, 0.254258214739274], 
reward next is 0.7457, 
noisyNet noise sample is [array([-0.22166096], dtype=float32), 0.4251402]. 
=============================================
[2019-03-26 02:10:09,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4367773e-34], sum to 1.0000
[2019-03-26 02:10:09,164] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1140
[2019-03-26 02:10:09,178] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.8, 62.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5829513664478181, 6.911199999999999, 6.9112, 168.912956510431, 510070.4290974011, 510070.4290974017, 163471.5749362541], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6814800.0000, 
sim time next is 6815400.0000, 
raw observation next is [25.68333333333333, 62.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5816296182593611, 6.911199999999999, 6.9112, 168.912956510431, 508767.8689311079, 508767.8689311085, 163280.6826234373], 
processed observation next is [1.0, 0.9130434782608695, 0.41627172195892564, 0.6283333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4897922173894647, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14132440803641888, 0.14132440803641902, 0.2437025113782646], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.7276605], dtype=float32), -0.20317893]. 
=============================================
[2019-03-26 02:10:09,362] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0537132e-09 1.3478458e-15 6.0873314e-22 1.0000000e+00 1.5980580e-13], sum to 1.0000
[2019-03-26 02:10:09,379] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7861
[2019-03-26 02:10:09,388] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.6719201476830932, 1.0, 2.0, 0.6719201476830932, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1878835.344056895, 1878835.344056895, 362330.814894569], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6688800.0000, 
sim time next is 6689400.0000, 
raw observation next is [28.36666666666667, 77.0, 1.0, 2.0, 0.6055197761304827, 1.0, 2.0, 0.6055197761304827, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1693018.737305257, 1693018.737305257, 336295.8660231592], 
processed observation next is [1.0, 0.43478260869565216, 0.543443917851501, 0.77, 1.0, 1.0, 0.524722621843955, 1.0, 1.0, 0.524722621843955, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4702829825847936, 0.4702829825847936, 0.5019341283927748], 
reward next is 0.4981, 
noisyNet noise sample is [array([1.1155802], dtype=float32), 0.46722916]. 
=============================================
[2019-03-26 02:10:11,043] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:10:11,054] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7254
[2019-03-26 02:10:11,062] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.86666666666667, 31.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4661693020018569, 6.911199999999999, 6.9112, 168.912956510431, 416885.9759888996, 416885.9759889003, 147709.884189609], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6876600.0000, 
sim time next is 6877200.0000, 
raw observation next is [29.83333333333333, 32.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4678059346310448, 6.9112, 6.9112, 168.912956510431, 417827.8009668326, 417827.8009668326, 147932.8725087931], 
processed observation next is [0.0, 0.6086956521739131, 0.6129541864139019, 0.3233333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3509828471110302, 0.0, 0.0, 0.8294399451523027, 0.1160632780463424, 0.1160632780463424, 0.2207953321026763], 
reward next is 0.7792, 
noisyNet noise sample is [array([-1.1004773], dtype=float32), 0.9000796]. 
=============================================
[2019-03-26 02:10:12,406] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 02:10:12,407] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:10:12,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:10:12,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:10:12,410] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:10:12,410] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:10:12,410] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:10:12,412] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:10:12,413] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:10:12,414] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:10:12,415] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:10:12,450] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-26 02:10:12,477] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-26 02:10:12,478] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-26 02:10:12,539] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-26 02:10:12,539] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-26 02:10:52,838] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9058742], dtype=float32), 0.04446254]
[2019-03-26 02:10:52,839] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.15, 72.0, 1.0, 2.0, 0.7414522894294971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104287, 1067074.45717689, 1067074.457176891, 234231.4291751807]
[2019-03-26 02:10:52,839] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:10:52,841] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.2934879e-06 1.5282747e-37 1.6545423e-35 0.0000000e+00 9.9999070e-01], sampled 0.4067541387849042
[2019-03-26 02:10:54,957] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9058742], dtype=float32), 0.04446254]
[2019-03-26 02:10:54,959] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.02009309666667, 89.067025935, 1.0, 2.0, 0.6305813477991397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 935985.7479052703, 935985.7479052697, 213539.2527689629]
[2019-03-26 02:10:54,960] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:10:54,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4349788e-23], sampled 0.23606542164441635
[2019-03-26 02:10:54,962] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 935985.7479052703 W.
[2019-03-26 02:11:46,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9058742], dtype=float32), 0.04446254]
[2019-03-26 02:11:46,587] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.41666666666667, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9141222436340499, 6.9112, 6.9112, 168.912956510431, 750568.1525656576, 750568.1525656576, 226577.0506439368]
[2019-03-26 02:11:46,588] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:11:46,589] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5993217e-32], sampled 0.15514729282320583
[2019-03-26 02:12:06,121] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.9058742], dtype=float32), 0.04446254]
[2019-03-26 02:12:06,125] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.6, 39.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5090486092353919, 6.9112, 6.9112, 168.912956510431, 449936.831319606, 449936.831319606, 153170.034363171]
[2019-03-26 02:12:06,127] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:12:06,129] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.639122228653208
[2019-03-26 02:12:11,040] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9058742], dtype=float32), 0.04446254]
[2019-03-26 02:12:11,041] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 77.66666666666667, 1.0, 2.0, 0.631850429326633, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987034248561555, 6.9112, 168.912432367944, 1766713.481177672, 1712914.222176096, 370610.4033861433]
[2019-03-26 02:12:11,045] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:12:11,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.1928647e-05 5.9686308e-23 1.8347343e-21 1.5989396e-08 9.9990809e-01], sampled 0.3241804429051033
[2019-03-26 02:12:21,913] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7492.6006 3132226003.7117 1661.0000
[2019-03-26 02:12:21,977] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7298.8093 3188102616.0060 1674.0000
[2019-03-26 02:12:22,235] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8130.7165 2961059196.6605 1112.0000
[2019-03-26 02:12:22,344] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8057.5322 3000014373.3821 1093.0000
[2019-03-26 02:12:22,499] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7440.0900 3322693623.4272 1556.0000
[2019-03-26 02:12:23,517] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1875000, evaluation results [1875000.0, 7440.090044497509, 3322693623.4271717, 1556.0, 7492.600631695016, 3132226003.7116504, 1661.0, 8130.71649932935, 2961059196.6604996, 1112.0, 7298.809289281628, 3188102616.0060115, 1674.0, 8057.532205527551, 3000014373.382145, 1093.0]
[2019-03-26 02:12:24,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:24,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5105
[2019-03-26 02:12:24,764] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.3, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7326227733146478, 6.9112, 6.9112, 168.912956510431, 642075.9602737251, 642075.9602737251, 188523.3992421797], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6747600.0000, 
sim time next is 6748200.0000, 
raw observation next is [22.25, 83.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6609888914096477, 6.911199999999999, 6.9112, 168.912956510431, 579438.8657669369, 579438.8657669376, 175764.0643377784], 
processed observation next is [1.0, 0.08695652173913043, 0.2535545023696683, 0.835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5865718187922532, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1609552404908158, 0.160955240490816, 0.26233442438474386], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.48043728], dtype=float32), -0.85842377]. 
=============================================
[2019-03-26 02:12:27,824] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 3.9987334e-17 4.3021608e-16 3.6984857e-09 3.9315721e-19], sum to 1.0000
[2019-03-26 02:12:27,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3888
[2019-03-26 02:12:27,836] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.83333333333333, 95.0, 1.0, 1.0, 0.5766282898808166, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9128920271531, 805789.0592517407, 805789.05925174, 196466.2211815791], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6666000.0000, 
sim time next is 6666600.0000, 
raw observation next is [24.81666666666667, 95.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9184231013409263, 6.911200000000002, 6.9112, 168.9129564943294, 767884.6532177986, 767884.6532177973, 228071.2282104509], 
processed observation next is [1.0, 0.13043478260869565, 0.37519747235387063, 0.95, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9005159772450321, 1.7763568394002506e-16, 0.0, 0.8294399450732364, 0.2133012925604996, 0.21330129256049923, 0.3404048182245536], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2905642], dtype=float32), -0.177579]. 
=============================================
[2019-03-26 02:12:39,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.9999702e-01 4.9512556e-29 4.2165919e-29 3.0042665e-06 1.3195902e-31], sum to 1.0000
[2019-03-26 02:12:39,145] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1717
[2019-03-26 02:12:39,153] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.1, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7898987591591798, 6.911200000000001, 6.9112, 168.912956510431, 664085.7216083327, 664085.721608332, 199809.0226444357], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7086600.0000, 
sim time next is 7087200.0000, 
raw observation next is [25.06666666666666, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7889317169247827, 6.9112, 6.9112, 168.912956510431, 663170.6182680933, 663170.6182680933, 199610.4748618624], 
processed observation next is [1.0, 0.0, 0.38704581358609763, 0.9033333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7425996547863204, 0.0, 0.0, 0.8294399451523027, 0.18421406063002593, 0.18421406063002593, 0.2979260818833767], 
reward next is 0.7021, 
noisyNet noise sample is [array([1.8267114], dtype=float32), -0.6284147]. 
=============================================
[2019-03-26 02:12:41,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:41,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9866
[2019-03-26 02:12:41,365] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.03333333333333, 83.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6101816559288056, 6.9112, 6.9112, 168.912956510431, 531150.203740674, 531150.203740674, 167645.1464033863], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6842400.0000, 
sim time next is 6843000.0000, 
raw observation next is [23.01666666666667, 83.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.610749015622026, 6.911200000000001, 6.9112, 168.912956510431, 531489.2801868112, 531489.2801868106, 167735.8493590766], 
processed observation next is [0.0, 0.17391304347826086, 0.2898894154818327, 0.8366666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5253036775878366, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14763591116300312, 0.14763591116300295, 0.25035201396877105], 
reward next is 0.7496, 
noisyNet noise sample is [array([-1.2753861], dtype=float32), -0.96282715]. 
=============================================
[2019-03-26 02:12:41,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.23599 ]
 [75.75314 ]
 [76.23053 ]
 [76.738075]
 [77.4671  ]], R is [[75.22150421]
 [75.21907806]
 [75.21685791]
 [75.2150116 ]
 [75.21372223]].
[2019-03-26 02:12:43,957] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:43,973] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4996
[2019-03-26 02:12:43,980] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.9, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8078379249986839, 6.9112, 6.9112, 168.912956510431, 676631.5062332678, 676631.5062332678, 203445.4407745838], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7074000.0000, 
sim time next is 7074600.0000, 
raw observation next is [25.85, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8094039725998785, 6.911199999999999, 6.9112, 168.912956510431, 678219.0997227237, 678219.0997227244, 203778.0330010883], 
processed observation next is [1.0, 0.9130434782608695, 0.4241706161137442, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7675658202437543, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18839419436742325, 0.18839419436742344, 0.3041463179120721], 
reward next is 0.6959, 
noisyNet noise sample is [array([-0.9004933], dtype=float32), 0.030482192]. 
=============================================
[2019-03-26 02:12:46,984] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:46,993] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6488
[2019-03-26 02:12:47,000] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.06666666666667, 72.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6886532185902465, 6.9112, 6.9112, 168.912956510431, 590778.5889644243, 590778.5889644243, 180713.2620561998], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6903600.0000, 
sim time next is 6904200.0000, 
raw observation next is [25.98333333333333, 73.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6926394521740675, 6.911200000000001, 6.9112, 168.912956510431, 593862.1654945095, 593862.1654945088, 181418.1506395326], 
processed observation next is [0.0, 0.9130434782608695, 0.43048973143759867, 0.7316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6251700636269116, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16496171263736376, 0.16496171263736356, 0.2707733591634815], 
reward next is 0.7292, 
noisyNet noise sample is [array([0.53268933], dtype=float32), 1.1462476]. 
=============================================
[2019-03-26 02:12:48,543] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:48,552] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7929
[2019-03-26 02:12:48,560] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 60.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7890325736140756, 6.911199999999999, 6.9112, 168.912956510431, 662880.5402011555, 662880.5402011562, 199623.3331462009], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6972000.0000, 
sim time next is 6972600.0000, 
raw observation next is [30.0, 59.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.784421306214647, 6.911199999999999, 6.9112, 168.912956510431, 659796.1706858793, 659796.1706858799, 198705.278083413], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7370991539203012, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18327671407941093, 0.18327671407941107, 0.2965750419155418], 
reward next is 0.7034, 
noisyNet noise sample is [array([1.0586495], dtype=float32), 0.28895393]. 
=============================================
[2019-03-26 02:12:51,612] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:12:51,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6764
[2019-03-26 02:12:51,636] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1857060.081474332 W.
[2019-03-26 02:12:51,642] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.93333333333334, 57.33333333333334, 1.0, 2.0, 0.4427596620048605, 1.0, 2.0, 0.4427596620048605, 1.0, 1.0, 0.7393317883346181, 6.911199999999999, 6.9112, 170.5573041426782, 1857060.081474332, 1857060.081474333, 373005.1172482636], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7035600.0000, 
sim time next is 7036200.0000, 
raw observation next is [30.1, 56.5, 1.0, 2.0, 0.725112358337985, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950748492844829, 6.9112, 168.9127202929515, 1921954.061306271, 1893897.038564664, 393299.9954442497], 
processed observation next is [1.0, 0.43478260869565216, 0.6255924170616115, 0.565, 1.0, 1.0, 0.6688100702867289, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.003954849284482887, 0.0, 0.8294387852163374, 0.5338761281406308, 0.5260825107124066, 0.5870149185735071], 
reward next is 0.2152, 
noisyNet noise sample is [array([0.23968267], dtype=float32), 0.74879766]. 
=============================================
[2019-03-26 02:13:07,341] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:07,355] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9686
[2019-03-26 02:13:07,366] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.55, 79.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7107783080872127, 6.9112, 6.9112, 168.912956510431, 605155.6039935349, 605155.6039935349, 184663.8531680446], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7497000.0000, 
sim time next is 7497600.0000, 
raw observation next is [25.43333333333333, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7085091543271188, 6.9112, 6.9112, 168.912956510431, 603519.7908392476, 603519.7908392476, 184253.1585522026], 
processed observation next is [0.0, 0.782608695652174, 0.40442338072669815, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6445233589355106, 0.0, 0.0, 0.8294399451523027, 0.16764438634423545, 0.16764438634423545, 0.2750047142570188], 
reward next is 0.7250, 
noisyNet noise sample is [array([-1.84365], dtype=float32), -0.5048916]. 
=============================================
[2019-03-26 02:13:07,572] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:07,582] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6470
[2019-03-26 02:13:07,590] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.96666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6992883358720463, 6.911200000000001, 6.9112, 168.912956510431, 596655.12751652, 596655.1275165194, 182595.988960064], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7500000.0000, 
sim time next is 7500600.0000, 
raw observation next is [24.85, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6980008086935361, 6.911199999999999, 6.9112, 168.912956510431, 595707.356162778, 595707.3561627786, 182366.332360592], 
processed observation next is [0.0, 0.8260869565217391, 0.37677725118483424, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6317083032848001, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16547426560077166, 0.16547426560077183, 0.2721885557620776], 
reward next is 0.7278, 
noisyNet noise sample is [array([-1.9948419], dtype=float32), 1.2457316]. 
=============================================
[2019-03-26 02:13:09,284] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:09,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7533
[2019-03-26 02:13:09,301] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.23333333333333, 90.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6148460594105513, 6.911200000000001, 6.9112, 168.912956510431, 534276.6528967448, 534276.6528967442, 168386.3706964997], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7256400.0000, 
sim time next is 7257000.0000, 
raw observation next is [22.21666666666667, 90.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6119646567452813, 6.911200000000001, 6.9112, 168.912956510431, 531982.5763130432, 531982.5763130425, 167934.1280302046], 
processed observation next is [1.0, 1.0, 0.2519747235387047, 0.9016666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5267861667625382, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14777293786473422, 0.14777293786473403, 0.2506479522838875], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.15066175], dtype=float32), 0.7165235]. 
=============================================
[2019-03-26 02:13:09,312] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[80.02154 ]
 [79.86873 ]
 [79.766525]
 [79.67343 ]
 [79.54541 ]], R is [[80.10260773]
 [80.05026245]
 [79.99665833]
 [79.94291687]
 [79.88936615]].
[2019-03-26 02:13:09,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:09,435] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0220
[2019-03-26 02:13:09,443] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.1, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6519079527887964, 6.9112, 6.9112, 168.912956510431, 561825.6157539217, 561825.6157539217, 174410.9251481395], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7536000.0000, 
sim time next is 7536600.0000, 
raw observation next is [23.15, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6541317616971027, 6.9112, 6.9112, 168.912956510431, 563456.4493623809, 563456.4493623809, 174783.009479138], 
processed observation next is [0.0, 0.21739130434782608, 0.2962085308056872, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5782094654842717, 0.0, 0.0, 0.8294399451523027, 0.15651568037843913, 0.15651568037843913, 0.2608701634016985], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.62418664], dtype=float32), 0.70691264]. 
=============================================
[2019-03-26 02:13:11,170] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:11,180] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6545
[2019-03-26 02:13:11,187] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 94.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5794456574662594, 6.9112, 6.9112, 168.912956510431, 506162.5699249672, 506162.5699249672, 162976.7758604276], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7446000.0000, 
sim time next is 7446600.0000, 
raw observation next is [21.25, 94.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5798231551391797, 6.9112, 6.9112, 168.912956510431, 506466.2105589872, 506466.2105589872, 163032.6845265364], 
processed observation next is [0.0, 0.17391304347826086, 0.20616113744075834, 0.945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.48758921358436547, 0.0, 0.0, 0.8294399451523027, 0.14068505848860757, 0.14068505848860757, 0.2433323649649797], 
reward next is 0.7567, 
noisyNet noise sample is [array([-0.40832898], dtype=float32), 0.5218738]. 
=============================================
[2019-03-26 02:13:20,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.835432e-36 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-26 02:13:20,862] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0168
[2019-03-26 02:13:20,872] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.96666666666667, 92.0, 1.0, 1.0, 0.3368619107804231, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6085791411556177, 6.9112, 6.9112, 168.9126783639938, 1066900.556268533, 1066900.556268533, 245549.816690062], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7392000.0000, 
sim time next is 7392600.0000, 
raw observation next is [20.95, 92.0, 1.0, 2.0, 0.3251569289452035, 1.0, 1.0, 0.3251569289452035, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1027613.393973958, 1027613.393973958, 268432.6335189415], 
processed observation next is [1.0, 0.5652173913043478, 0.19194312796208532, 0.92, 1.0, 1.0, 0.18693605897012466, 1.0, 0.5, 0.18693605897012466, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2854481649927661, 0.2854481649927661, 0.40064572167006196], 
reward next is 0.5994, 
noisyNet noise sample is [array([-0.07677166], dtype=float32), -0.9009547]. 
=============================================
[2019-03-26 02:13:23,279] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:23,291] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0845
[2019-03-26 02:13:23,294] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.21666666666667, 84.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5214475512071297, 6.9112, 6.9112, 168.912956510431, 461171.9876179353, 461171.9876179353, 154754.5288628314], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7411800.0000, 
sim time next is 7412400.0000, 
raw observation next is [21.3, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5210329125152806, 6.9112, 6.9112, 168.912956510431, 460644.8240181939, 460644.8240181939, 154707.301051349], 
processed observation next is [1.0, 0.8260869565217391, 0.2085308056872039, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4158937957503422, 0.0, 0.0, 0.8294399451523027, 0.1279568955606094, 0.1279568955606094, 0.23090641947962537], 
reward next is 0.7691, 
noisyNet noise sample is [array([1.2201499], dtype=float32), -0.5303338]. 
=============================================
[2019-03-26 02:13:24,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3263444e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 02:13:24,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5906
[2019-03-26 02:13:24,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.6, 87.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5141323695064592, 6.9112, 6.9112, 168.912956510431, 455297.3335802221, 455297.3335802221, 153781.7858974973], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7407000.0000, 
sim time next is 7407600.0000, 
raw observation next is [20.66666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.515370029926436, 6.911199999999999, 6.9112, 168.912956510431, 456471.519425816, 456471.5194258167, 153937.3054483234], 
processed observation next is [1.0, 0.7391304347826086, 0.17851500789889443, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4089878413737024, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12679764428494888, 0.12679764428494908, 0.22975717231093046], 
reward next is 0.7702, 
noisyNet noise sample is [array([-0.26740256], dtype=float32), 0.868743]. 
=============================================
[2019-03-26 02:13:26,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:26,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5662
[2019-03-26 02:13:26,821] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6137614273130167, 6.911199999999999, 6.9112, 168.912956510431, 532743.9537262849, 532743.9537262855, 168226.1736521116], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7461600.0000, 
sim time next is 7462200.0000, 
raw observation next is [22.1, 93.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6161121600052204, 6.9112, 6.9112, 168.912956510431, 534445.9834828508, 534445.9834828508, 168598.1379135058], 
processed observation next is [0.0, 0.34782608695652173, 0.24644549763033188, 0.9333333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.531844097567342, 0.0, 0.0, 0.8294399451523027, 0.14845721763412523, 0.14845721763412523, 0.2516390118112027], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.9045621], dtype=float32), -1.5714313]. 
=============================================
[2019-03-26 02:13:28,924] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:13:28,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8486
[2019-03-26 02:13:28,940] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.6, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7057138414198754, 6.9112, 6.9112, 168.912956510431, 601354.902592889, 601354.902592889, 183748.0553740449], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7482600.0000, 
sim time next is 7483200.0000, 
raw observation next is [25.66666666666666, 78.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7074714291496843, 6.9112, 6.9112, 168.912956510431, 602733.0706916737, 602733.0706916737, 184065.5471991888], 
processed observation next is [0.0, 0.6086956521739131, 0.4154818325434437, 0.7866666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6432578404264442, 0.0, 0.0, 0.8294399451523027, 0.16742585296990936, 0.16742585296990936, 0.2747246973122221], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.44842425], dtype=float32), 1.0029161]. 
=============================================
[2019-03-26 02:13:31,247] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 02:13:31,248] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:13:31,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:13:31,250] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:13:31,251] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:13:31,253] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:13:31,253] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:13:31,254] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:13:31,256] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:13:31,254] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:13:31,260] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:13:31,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-26 02:13:31,318] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-26 02:13:31,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-26 02:13:31,371] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-26 02:13:31,372] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-26 02:13:32,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:13:32,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.8, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5368596143145742, 6.911199999999999, 6.9112, 168.912956510431, 476224.7608408195, 476224.7608408201, 156730.2074735438]
[2019-03-26 02:13:32,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:13:32,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4708996014586375
[2019-03-26 02:13:44,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:13:44,423] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.05, 74.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7245949530282749, 6.9112, 6.9112, 168.912956510431, 618889.5111502557, 618889.5111502557, 187218.2942649565]
[2019-03-26 02:13:44,425] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:13:44,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.3564585e-32 0.0000000e+00], sampled 0.8967960915273108
[2019-03-26 02:14:01,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:14:01,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.45392175666666, 93.36124999333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6629034952433921, 6.9112, 6.9112, 168.912956510431, 572297.5156456206, 572297.5156456206, 176248.811074709]
[2019-03-26 02:14:01,300] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:14:01,304] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.09768530321112756
[2019-03-26 02:14:21,304] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:14:21,305] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.620119205, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6223842402443039, 6.9112, 6.9112, 168.912956510431, 539458.5884422424, 539458.5884422424, 169591.1737338688]
[2019-03-26 02:14:21,306] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:14:21,308] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3187717635792422
[2019-03-26 02:14:34,088] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:14:34,088] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.0, 48.0, 1.0, 1.0, 0.6310590871438053, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9128159772675, 881883.0529221329, 881883.0529221335, 206673.3806410385]
[2019-03-26 02:14:34,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:14:34,093] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9210995e-01 1.6408728e-25 6.5112776e-25 7.8900661e-03 5.0004548e-16], sampled 0.5289885382817812
[2019-03-26 02:14:34,094] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 881883.0529221329 W.
[2019-03-26 02:15:31,831] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:15:31,833] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.76666666666667, 92.66666666666667, 1.0, 2.0, 0.5826960961247032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 929857.5751164259, 929857.5751164252, 210624.8376704369]
[2019-03-26 02:15:31,834] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:15:31,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 4.8934908e-30 1.3994758e-30 2.3731627e-15 8.2685302e-16], sampled 0.12935191384391764
[2019-03-26 02:15:31,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 929857.5751164259 W.
[2019-03-26 02:15:36,231] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.64151245], dtype=float32), -0.003973261]
[2019-03-26 02:15:36,233] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.56666666666667, 70.5, 1.0, 2.0, 0.1970176850803264, 1.0, 2.0, 0.1970176850803264, 1.0, 2.0, 0.342154564240448, 6.9112, 6.9112, 178.6582176852504, 825936.7643066024, 825936.7643066024, 271031.4019802866]
[2019-03-26 02:15:36,234] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:15:36,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6944302e-17 2.6542974e-32 3.6034902e-29 2.4214454e-16 1.0000000e+00], sampled 0.10635326339278461
[2019-03-26 02:15:40,668] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7844.3762 3121360538.2608 949.0000
[2019-03-26 02:15:40,678] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8306.3858 2992284230.3961 582.0000
[2019-03-26 02:15:40,869] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7833.2910 3303041608.8592 729.0000
[2019-03-26 02:15:41,053] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7797.2863 3170416935.3527 790.0000
[2019-03-26 02:15:41,261] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8348.3906 2958505417.6798 655.0000
[2019-03-26 02:15:42,280] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1900000, evaluation results [1900000.0, 7833.290975599014, 3303041608.859233, 729.0, 7844.376206677739, 3121360538.260758, 949.0, 8348.390648194938, 2958505417.6797614, 655.0, 7797.286315957282, 3170416935.352692, 790.0, 8306.385765595047, 2992284230.3961287, 582.0]
[2019-03-26 02:15:45,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 5.362394e-27 0.000000e+00], sum to 1.0000
[2019-03-26 02:15:45,781] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2142
[2019-03-26 02:15:45,790] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.16666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8290618087980323, 6.9112, 6.9112, 168.912956510431, 694538.838280199, 694538.838280199, 207925.8124262356], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7710000.0000, 
sim time next is 7710600.0000, 
raw observation next is [25.33333333333333, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8345885780840742, 6.9112, 6.9112, 168.912956510431, 698213.6747139937, 698213.6747139937, 209088.5349771114], 
processed observation next is [1.0, 0.21739130434782608, 0.3996840442338071, 0.915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7982787537610659, 0.0, 0.0, 0.8294399451523027, 0.19394824297610935, 0.19394824297610935, 0.3120724402643454], 
reward next is 0.6879, 
noisyNet noise sample is [array([1.8619766], dtype=float32), 0.41937917]. 
=============================================
[2019-03-26 02:15:48,189] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 6.931871e-24 0.000000e+00], sum to 1.0000
[2019-03-26 02:15:48,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4692
[2019-03-26 02:15:48,212] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1489211.846183492 W.
[2019-03-26 02:15:48,216] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.9, 81.0, 1.0, 2.0, 0.5326738511472787, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9069261354953501, 6.9112, 6.9112, 168.912956510431, 1489211.846183492, 1489211.846183492, 322848.210116277], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7806000.0000, 
sim time next is 7806600.0000, 
raw observation next is [28.0, 80.5, 1.0, 2.0, 0.6078421251708341, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.915962623715656, 6.9112, 168.9129232849864, 1699530.238150635, 1696151.4696345, 366906.7340016907], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.805, 1.0, 1.0, 0.5275206327359447, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0004762623715656389, 0.0, 0.829439782000159, 0.47209173281962086, 0.47115318600958334, 0.5476219910472996], 
reward next is 0.4286, 
noisyNet noise sample is [array([0.14887258], dtype=float32), 0.8920561]. 
=============================================
[2019-03-26 02:15:51,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:51,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:51,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-26 02:15:51,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:51,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:51,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-26 02:15:53,267] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:53,267] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:53,345] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-26 02:15:53,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:53,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:53,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:15:53,847] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5631
[2019-03-26 02:15:53,851] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8277076542769455, 6.9112, 6.9112, 168.912956510431, 694343.2761949665, 694343.2761949665, 207658.0369316988], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7709400.0000, 
sim time next is 7710000.0000, 
raw observation next is [25.16666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8290618087995054, 6.9112, 6.9112, 168.912956510431, 694538.838280199, 694538.838280199, 207925.8124265214], 
processed observation next is [1.0, 0.21739130434782608, 0.39178515007898923, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.791538791218909, 0.0, 0.0, 0.8294399451523027, 0.19292745507783304, 0.19292745507783304, 0.31033703347242003], 
reward next is 0.6897, 
noisyNet noise sample is [array([-0.7455359], dtype=float32), -1.2762253]. 
=============================================
[2019-03-26 02:15:53,867] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[56.947666]
 [56.977577]
 [56.995525]
 [56.886635]
 [56.50115 ]], R is [[57.21487808]
 [57.33279419]
 [57.45065689]
 [57.55657959]
 [57.67708206]].
[2019-03-26 02:15:53,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-26 02:15:55,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:55,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:56,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-26 02:15:56,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:56,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:56,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-26 02:15:57,803] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:15:57,804] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:15:57,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-26 02:15:58,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:15:58,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7393
[2019-03-26 02:15:58,486] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.61666666666667, 91.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7219102234000182, 6.9112, 6.9112, 168.912956510431, 624520.2171965144, 624520.2171965144, 186694.6079686998], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 105000.0000, 
sim time next is 105600.0000, 
raw observation next is [22.63333333333334, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6606862370843574, 6.9112, 6.9112, 168.912956510431, 571415.3344075446, 571415.3344075446, 175864.8517446204], 
processed observation next is [1.0, 0.21739130434782608, 0.27172195892575074, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5862027281516553, 0.0, 0.0, 0.8294399451523027, 0.1587264817798735, 0.1587264817798735, 0.2624848533501797], 
reward next is 0.7375, 
noisyNet noise sample is [array([0.7618852], dtype=float32), 1.4836254]. 
=============================================
[2019-03-26 02:16:01,694] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:01,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:01,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-26 02:16:03,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:03,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:03,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-26 02:16:04,158] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:04,166] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2589
[2019-03-26 02:16:04,177] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.96666666666667, 95.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5178758073415661, 6.9112, 6.9112, 168.912956510431, 457631.5982186428, 457631.5982186428, 154306.5335363729], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 193800.0000, 
sim time next is 194400.0000, 
raw observation next is [20.0, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5181062271789852, 6.911199999999999, 6.9112, 168.912956510431, 457777.1928452512, 457777.1928452519, 154338.7473575221], 
processed observation next is [0.0, 0.2608695652173913, 0.1469194312796209, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4123246672914454, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1271603313459031, 0.1271603313459033, 0.23035633933958524], 
reward next is 0.7696, 
noisyNet noise sample is [array([-0.56942177], dtype=float32), 0.10851595]. 
=============================================
[2019-03-26 02:16:05,345] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:05,354] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7020
[2019-03-26 02:16:05,359] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.46666666666667, 64.0, 1.0, 2.0, 0.2354741541656717, 0.0, 1.0, 0.0, 1.0, 2.0, 0.3986716694057393, 6.9112, 6.9112, 168.9129564982, 658064.516660751, 658064.516660751, 202754.2694748975], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7924800.0000, 
sim time next is 7925400.0000, 
raw observation next is [30.3, 65.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.785386543057966, 6.911200000000001, 6.9112, 168.9129565104279, 649459.0851150278, 649459.0851150273, 198608.511752489], 
processed observation next is [1.0, 0.7391304347826086, 0.6350710900473934, 0.65, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7382762720219096, 8.881784197001253e-17, 0.0, 0.8294399451522875, 0.18040530142084107, 0.1804053014208409, 0.2964306145559537], 
reward next is 0.7036, 
noisyNet noise sample is [array([-1.7809538], dtype=float32), -0.09242324]. 
=============================================
[2019-03-26 02:16:06,568] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:06,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:06,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-26 02:16:07,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:07,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:07,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-26 02:16:09,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:09,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:09,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-26 02:16:09,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:09,337] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:09,425] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-26 02:16:09,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:09,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:09,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-26 02:16:10,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:10,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:10,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-26 02:16:10,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:16:10,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:10,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-26 02:16:11,833] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:11,840] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3772
[2019-03-26 02:16:11,845] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.73333333333333, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6547124679182821, 6.911200000000001, 6.9112, 168.912956510431, 575655.3478477785, 575655.3478477779, 174657.0799073337], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 22800.0000, 
sim time next is 23400.0000, 
raw observation next is [21.8, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6690959515840954, 6.9112, 6.9112, 168.912956510431, 588175.5020911928, 588175.5020911928, 177085.1854044202], 
processed observation next is [1.0, 0.2608695652173913, 0.23222748815165886, 0.85, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5964584775415798, 0.0, 0.0, 0.8294399451523027, 0.1633820839142202, 0.1633820839142202, 0.26430624687226895], 
reward next is 0.7357, 
noisyNet noise sample is [array([-0.88677776], dtype=float32), -1.7357314]. 
=============================================
[2019-03-26 02:16:21,719] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:21,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1341
[2019-03-26 02:16:21,745] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.4, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6189474282681109, 6.9112, 6.9112, 168.912956510431, 538335.8262998111, 538335.8262998111, 169020.8816922606], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 164400.0000, 
sim time next is 165000.0000, 
raw observation next is [21.3, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6126902527597909, 6.9112, 6.9112, 168.912956510431, 533454.9806005908, 533454.9806005908, 168032.1687860991], 
processed observation next is [1.0, 0.9130434782608695, 0.2085308056872039, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5276710399509645, 0.0, 0.0, 0.8294399451523027, 0.14818193905571966, 0.14818193905571966, 0.25079428177029717], 
reward next is 0.7492, 
noisyNet noise sample is [array([0.6466147], dtype=float32), -2.531688]. 
=============================================
[2019-03-26 02:16:21,763] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[75.01276 ]
 [74.82291 ]
 [74.686295]
 [74.81488 ]
 [74.57625 ]], R is [[75.0226593 ]
 [75.02015686]
 [75.01617432]
 [75.01081848]
 [75.00423431]].
[2019-03-26 02:16:21,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:22,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9225
[2019-03-26 02:16:22,008] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.06666666666667, 75.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7460177028481824, 6.9112, 6.9112, 168.912956510431, 666519.4414477205, 666519.4414477205, 190398.2385007164], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 463800.0000, 
sim time next is 464400.0000, 
raw observation next is [21.2, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7782571062045694, 6.9112, 6.9112, 168.912956510431, 695236.5461233738, 695236.5461233738, 196668.0936588665], 
processed observation next is [1.0, 0.391304347826087, 0.20379146919431282, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7295818368348407, 0.0, 0.0, 0.8294399451523027, 0.19312126281204828, 0.19312126281204828, 0.29353446814756196], 
reward next is 0.7065, 
noisyNet noise sample is [array([1.5065081], dtype=float32), 0.13133283]. 
=============================================
[2019-03-26 02:16:22,759] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2970421e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 02:16:22,773] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9444
[2019-03-26 02:16:22,778] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.33333333333334, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5088814308517914, 6.911200000000001, 6.9112, 168.912956510431, 450883.3626340893, 450883.3626340887, 153100.7348705109], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 411600.0000, 
sim time next is 412200.0000, 
raw observation next is [21.25, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5022181456483042, 6.9112, 6.9112, 168.912956510431, 445290.5144321945, 445290.5144321945, 152246.5126267576], 
processed observation next is [1.0, 0.782608695652174, 0.20616113744075834, 0.81, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3929489581076881, 0.0, 0.0, 0.8294399451523027, 0.12369180956449848, 0.12369180956449848, 0.22723360093545913], 
reward next is 0.7728, 
noisyNet noise sample is [array([0.20353064], dtype=float32), -0.35314023]. 
=============================================
[2019-03-26 02:16:34,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.2912704e-36], sum to 1.0000
[2019-03-26 02:16:34,949] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9507
[2019-03-26 02:16:34,958] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.63333333333333, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4886165861233705, 6.9112, 6.9112, 168.912956510431, 433317.0466089039, 433317.0466089039, 150566.0378231654], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 344400.0000, 
sim time next is 345000.0000, 
raw observation next is [20.61666666666667, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4885273604859793, 6.9112, 6.9112, 168.912956510431, 433298.7257239028, 433298.7257239028, 150552.2940780312], 
processed observation next is [0.0, 1.0, 0.1761453396524489, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.37625287864143814, 0.0, 0.0, 0.8294399451523027, 0.12036075714552856, 0.12036075714552856, 0.22470491653437494], 
reward next is 0.7753, 
noisyNet noise sample is [array([-0.5385686], dtype=float32), -1.3973868]. 
=============================================
[2019-03-26 02:16:34,977] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[87.3809  ]
 [87.348366]
 [87.314606]
 [87.286385]
 [87.245804]], R is [[87.29605103]
 [87.19836426]
 [87.10151672]
 [87.00547791]
 [86.91018677]].
[2019-03-26 02:16:37,276] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:37,288] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7037
[2019-03-26 02:16:37,296] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.76666666666667, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4088621614366962, 6.9112, 6.9112, 168.912956510431, 368300.1255819054, 368300.1255819054, 141355.6710214405], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 697800.0000, 
sim time next is 698400.0000, 
raw observation next is [17.7, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4068144151621802, 6.9112, 6.9112, 168.912956510431, 366528.4819288982, 366528.4819288982, 141146.7247696934], 
processed observation next is [1.0, 0.08695652173913043, 0.03791469194312799, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.27660294531973195, 0.0, 0.0, 0.8294399451523027, 0.10181346720247172, 0.10181346720247172, 0.21066675338760207], 
reward next is 0.7893, 
noisyNet noise sample is [array([-0.918274], dtype=float32), -1.388259]. 
=============================================
[2019-03-26 02:16:38,683] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:38,695] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8817
[2019-03-26 02:16:38,705] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.23333333333333, 72.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4483966253316066, 6.911200000000001, 6.9112, 168.912956510431, 401205.2548112686, 401205.254811268, 145695.624582516], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 675600.0000, 
sim time next is 676200.0000, 
raw observation next is [21.01666666666667, 73.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4467853405441971, 6.9112, 6.9112, 168.912956510431, 399902.8632683962, 399902.8632683962, 145508.875773144], 
processed observation next is [1.0, 0.8260869565217391, 0.1951026856240128, 0.7383333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3253479762734111, 0.0, 0.0, 0.8294399451523027, 0.1110841286856656, 0.1110841286856656, 0.2171774265270806], 
reward next is 0.7828, 
noisyNet noise sample is [array([1.1566027], dtype=float32), -0.9509867]. 
=============================================
[2019-03-26 02:16:39,745] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 9.7768527e-38 2.1203606e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 02:16:39,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1754
[2019-03-26 02:16:39,764] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.86666666666666, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9683875847094235, 6.9112, 6.9112, 168.912956510431, 855930.9029342425, 855930.9029342425, 239946.2004694596], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 395400.0000, 
sim time next is 396000.0000, 
raw observation next is [22.9, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9840014837316896, 6.911200000000001, 6.9112, 168.9128929857789, 869495.7771212879, 869495.7771212872, 243908.055294982], 
processed observation next is [1.0, 0.6086956521739131, 0.2843601895734597, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9804896143069384, 8.881784197001253e-17, 0.0, 0.8294396332171792, 0.2415266047559133, 0.24152660475591312, 0.3640418735746], 
reward next is 0.6360, 
noisyNet noise sample is [array([-0.23346351], dtype=float32), 2.7566524]. 
=============================================
[2019-03-26 02:16:39,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[68.84909]
 [69.68714]
 [70.83488]
 [71.79583]
 [72.87996]], R is [[68.19902039]
 [68.1588974 ]
 [68.12748718]
 [68.12160492]
 [68.1314621 ]].
[2019-03-26 02:16:40,555] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 02:16:40,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6891
[2019-03-26 02:16:40,572] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.05, 81.5, 1.0, 2.0, 0.2988071159916584, 1.0, 2.0, 0.2988071159916584, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 945452.2581054736, 945452.2581054736, 262614.4710677043], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 406200.0000, 
sim time next is 406800.0000, 
raw observation next is [22.0, 82.0, 1.0, 2.0, 0.2994482828671537, 1.0, 2.0, 0.2994482828671537, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 947139.0515835128, 947139.0515835128, 262725.6523186242], 
processed observation next is [1.0, 0.7391304347826086, 0.2417061611374408, 0.82, 1.0, 1.0, 0.15596178658693216, 1.0, 1.0, 0.15596178658693216, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2630941809954202, 0.2630941809954202, 0.39212783928152867], 
reward next is 0.6079, 
noisyNet noise sample is [array([1.494948], dtype=float32), -0.15933482]. 
=============================================
[2019-03-26 02:16:44,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:16:44,826] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2102
[2019-03-26 02:16:44,832] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.6, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4565343884158075, 6.911199999999999, 6.9112, 168.912956510431, 408707.3088508554, 408707.308850856, 146585.0171681458], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 545400.0000, 
sim time next is 546000.0000, 
raw observation next is [20.8, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4494010088947544, 6.9112, 6.9112, 168.912956510431, 402064.8347357412, 402064.8347357412, 145809.1115374012], 
processed observation next is [1.0, 0.30434782608695654, 0.1848341232227489, 0.7666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3285378157253102, 0.0, 0.0, 0.8294399451523027, 0.11168467631548366, 0.11168467631548366, 0.21762553960806147], 
reward next is 0.7824, 
noisyNet noise sample is [array([1.776981], dtype=float32), 1.4266437]. 
=============================================
[2019-03-26 02:16:44,842] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.71306 ]
 [67.838104]
 [67.925514]
 [68.0281  ]
 [67.94511 ]], R is [[67.84181213]
 [67.9446106 ]
 [68.04851532]
 [68.15221405]
 [68.25655365]].
[2019-03-26 02:16:46,967] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 02:16:46,971] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:16:46,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:46,975] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:16:46,976] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:16:46,976] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:46,977] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:16:46,977] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:46,979] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:46,979] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:16:46,985] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:16:47,014] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-26 02:16:47,038] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-26 02:16:47,065] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-26 02:16:47,090] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-26 02:16:47,090] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-26 02:17:22,207] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6797038], dtype=float32), -0.05456752]
[2019-03-26 02:17:22,209] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.0, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7273393719419822, 6.9112, 6.9112, 168.912956510431, 620969.1439803627, 620969.1439803627, 187728.5850194677]
[2019-03-26 02:17:22,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:17:22,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 9.7461635e-24 8.5903575e-19], sampled 0.7541023479828444
[2019-03-26 02:17:28,549] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.6797038], dtype=float32), -0.05456752]
[2019-03-26 02:17:28,550] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.21666666666667, 88.16666666666667, 1.0, 2.0, 0.7043713979864201, 1.0, 2.0, 0.7043713979864201, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1969659.530241124, 1969659.530241124, 376000.4537871446]
[2019-03-26 02:17:28,551] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:17:28,553] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.56283886e-20 2.34033487e-34 0.00000000e+00 1.00000000e+00
 1.28608725e-33], sampled 0.6578288702412022
[2019-03-26 02:17:30,762] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6797038], dtype=float32), -0.05456752]
[2019-03-26 02:17:30,763] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.33333333333334, 88.66666666666666, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2752999736990267, 6.9112, 6.9112, 178.6582176852504, 690081.4116638327, 690081.4116638327, 260369.0143752334]
[2019-03-26 02:17:30,764] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:17:30,769] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.0576476e-01 3.6680314e-25 6.3080884e-28 9.4218992e-02 1.6270780e-05], sampled 0.8522918362179864
[2019-03-26 02:17:47,347] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.6797038], dtype=float32), -0.05456752]
[2019-03-26 02:17:47,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.15719484666667, 68.63272514333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7661082782918548, 6.9112, 6.9112, 168.912956510431, 649879.2284588525, 649879.2284588525, 195137.1706871976]
[2019-03-26 02:17:47,351] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:17:47,358] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 7.9205737e-34 1.8593686e-27], sampled 0.23204648222434576
[2019-03-26 02:18:56,050] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7822.2535 3138880470.9917 933.0000
[2019-03-26 02:18:56,413] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8295.6983 2991218577.4133 671.0000
[2019-03-26 02:18:56,587] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7768.4923 3339369820.6129 809.0000
[2019-03-26 02:18:56,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7777.6430 3193153970.3846 804.0000
[2019-03-26 02:18:56,892] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8264.7946 3027825059.5262 587.0000
[2019-03-26 02:18:57,904] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1925000, evaluation results [1925000.0, 7768.492303975174, 3339369820.61285, 809.0, 7822.253476161097, 3138880470.991739, 933.0, 8295.69831874075, 2991218577.413319, 671.0, 7777.643029670393, 3193153970.3845525, 804.0, 8264.794621066534, 3027825059.526185, 587.0]
[2019-03-26 02:19:00,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 4.367711e-38 4.436443e-30], sum to 1.0000
[2019-03-26 02:19:00,652] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4343
[2019-03-26 02:19:00,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [18.66666666666667, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4340099645660251, 6.9112, 6.9112, 168.912956510431, 389762.4796214334, 389762.4796214334, 144032.7719559615], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 688800.0000, 
sim time next is 689400.0000, 
raw observation next is [18.6, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4317874791514963, 6.911199999999999, 6.9112, 168.912956510431, 387843.0326727108, 387843.0326727114, 143791.7062588017], 
processed observation next is [1.0, 1.0, 0.08056872037914704, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.30705790140426376, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10773417574241965, 0.10773417574241984, 0.2146144869534354], 
reward next is 0.7854, 
noisyNet noise sample is [array([-0.31008887], dtype=float32), 0.8446387]. 
=============================================
[2019-03-26 02:19:02,171] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:19:02,182] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1716
[2019-03-26 02:19:02,189] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.9, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5027484609318698, 6.9112, 6.9112, 168.912956510431, 445417.9380949528, 445417.9380949528, 152329.00464799], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 801600.0000, 
sim time next is 802200.0000, 
raw observation next is [21.05, 84.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5057920930164184, 6.911200000000001, 6.9112, 168.912956510431, 447865.6756368405, 447865.6756368399, 152722.8137939695], 
processed observation next is [0.0, 0.2608695652173913, 0.1966824644549764, 0.8466666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.39730743050782724, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12440713212134459, 0.12440713212134441, 0.22794449819995447], 
reward next is 0.7721, 
noisyNet noise sample is [array([1.6549567], dtype=float32), 1.111461]. 
=============================================
[2019-03-26 02:19:15,262] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:19:15,269] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3258
[2019-03-26 02:19:15,278] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.3, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5423928817000558, 6.911200000000001, 6.9112, 168.912956510431, 476804.7620459746, 476804.762045974, 157652.8994294876], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1053600.0000, 
sim time next is 1054200.0000, 
raw observation next is [20.15, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5343768896945525, 6.911199999999999, 6.9112, 168.912956510431, 470426.6481691363, 470426.6481691369, 156546.9488277313], 
processed observation next is [1.0, 0.17391304347826086, 0.15402843601895733, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4321669386518933, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1306740689358712, 0.13067406893587136, 0.23365216242944972], 
reward next is 0.7663, 
noisyNet noise sample is [array([0.669232], dtype=float32), -0.59800833]. 
=============================================
[2019-03-26 02:19:23,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 2.4875483e-36 1.1576408e-17 9.6770766e-23], sum to 1.0000
[2019-03-26 02:19:23,274] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1974
[2019-03-26 02:19:23,280] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.93333333333333, 74.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5699661657385768, 6.911199999999999, 6.9112, 168.912956510431, 497794.5797429986, 497794.5797429992, 161602.3491272733], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 925800.0000, 
sim time next is 926400.0000, 
raw observation next is [23.86666666666667, 75.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5767204453737864, 6.9112, 6.9112, 168.912956510431, 503540.6873273705, 503540.6873273705, 162584.0264821013], 
processed observation next is [0.0, 0.7391304347826086, 0.33017377567140627, 0.7533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4838054211875444, 0.0, 0.0, 0.8294399451523027, 0.13987241314649182, 0.13987241314649182, 0.2426627260926885], 
reward next is 0.7573, 
noisyNet noise sample is [array([-0.5157569], dtype=float32), -0.6070699]. 
=============================================
[2019-03-26 02:19:36,730] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.8700026e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 02:19:36,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4613
[2019-03-26 02:19:36,747] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.16666666666667, 78.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5735492841253108, 6.9112, 6.9112, 168.912956510431, 501965.7990923684, 501965.7990923684, 162094.6804343805], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1110000.0000, 
sim time next is 1110600.0000, 
raw observation next is [23.05, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5720319389649744, 6.9112, 6.9112, 168.912956510431, 500848.2221666477, 500848.2221666477, 161869.7785527896], 
processed observation next is [1.0, 0.8695652173913043, 0.2914691943127963, 0.785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4780877304450907, 0.0, 0.0, 0.8294399451523027, 0.13912450615740216, 0.13912450615740216, 0.24159668440714865], 
reward next is 0.7584, 
noisyNet noise sample is [array([-1.649126], dtype=float32), 0.34008452]. 
=============================================
[2019-03-26 02:19:37,448] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.5862672e-17 2.9901548e-38 1.3826172e-38 1.0000000e+00 7.1066654e-22], sum to 1.0000
[2019-03-26 02:19:37,458] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4364
[2019-03-26 02:19:37,466] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.53333333333333, 93.33333333333334, 1.0, 2.0, 0.2363714478710916, 1.0, 2.0, 0.2363714478710916, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 660570.9254043932, 660570.9254043932, 238760.8324274095], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1290000.0000, 
sim time next is 1290600.0000, 
raw observation next is [24.5, 93.5, 1.0, 2.0, 0.2359230342795235, 1.0, 2.0, 0.2359230342795235, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 659317.3898457156, 659317.3898457156, 238690.9564444546], 
processed observation next is [1.0, 0.9565217391304348, 0.3601895734597157, 0.935, 1.0, 1.0, 0.07942534250544998, 1.0, 1.0, 0.07942534250544998, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18314371940158766, 0.18314371940158766, 0.35625515887232034], 
reward next is 0.6437, 
noisyNet noise sample is [array([0.5745434], dtype=float32), -0.29369307]. 
=============================================
[2019-03-26 02:19:39,479] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 6.184173e-33 0.000000e+00], sum to 1.0000
[2019-03-26 02:19:39,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8548
[2019-03-26 02:19:39,500] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.68333333333333, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8952464643702164, 6.911199999999999, 6.9112, 168.912956510431, 761068.9233658643, 761068.923365865, 222865.2708464423], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1320600.0000, 
sim time next is 1321200.0000, 
raw observation next is [23.6, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7982863779526564, 6.9112, 6.9112, 168.912956510431, 679141.4078438018, 679141.4078438018, 201639.2982282709], 
processed observation next is [1.0, 0.30434782608695654, 0.3175355450236968, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7540077779910443, 0.0, 0.0, 0.8294399451523027, 0.18865039106772272, 0.18865039106772272, 0.3009541764601058], 
reward next is 0.6990, 
noisyNet noise sample is [array([0.4831016], dtype=float32), -1.6037312]. 
=============================================
[2019-03-26 02:19:45,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:19:45,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7948
[2019-03-26 02:19:45,082] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.8, 94.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4955762217723247, 6.911200000000001, 6.9112, 168.912956510431, 438710.0800277437, 438710.0800277431, 151453.3807288423], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1138800.0000, 
sim time next is 1139400.0000, 
raw observation next is [19.75, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.499785594328309, 6.9112, 6.9112, 168.912956510431, 442485.7190894212, 442485.7190894212, 151972.8930747661], 
processed observation next is [1.0, 0.17391304347826086, 0.13507109004739343, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.38998243210769384, 0.0, 0.0, 0.8294399451523027, 0.12291269974706144, 0.12291269974706144, 0.226825213544427], 
reward next is 0.7732, 
noisyNet noise sample is [array([1.072885], dtype=float32), 0.41527492]. 
=============================================
[2019-03-26 02:19:53,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:19:53,709] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5063
[2019-03-26 02:19:53,715] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.15, 83.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7904124799435971, 6.911200000000001, 6.9112, 168.912956510431, 663133.1128216296, 663133.112821629, 199884.85281984], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1277400.0000, 
sim time next is 1278000.0000, 
raw observation next is [26.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7872203378232958, 6.9112, 6.9112, 168.912956510431, 660905.2686308747, 660905.2686308747, 199246.3986579613], 
processed observation next is [1.0, 0.8260869565217391, 0.4312796208530806, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7405126071015803, 0.0, 0.0, 0.8294399451523027, 0.18358479684190965, 0.18358479684190965, 0.29738268456412137], 
reward next is 0.7026, 
noisyNet noise sample is [array([0.6581259], dtype=float32), 0.78296036]. 
=============================================
[2019-03-26 02:19:53,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.7502  ]
 [68.95721 ]
 [69.43266 ]
 [69.45339 ]
 [68.975334]], R is [[68.89198303]
 [68.90473175]
 [68.91616058]
 [68.92624664]
 [68.93547058]].
[2019-03-26 02:20:01,529] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:20:01,538] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4235
[2019-03-26 02:20:01,547] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.15, 96.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7167448958463613, 6.9112, 6.9112, 168.912956510431, 611177.3138799083, 611177.3138799083, 185761.9162544412], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1632600.0000, 
sim time next is 1633200.0000, 
raw observation next is [23.13333333333333, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7161820286826319, 6.911199999999999, 6.9112, 168.912956510431, 610666.0307506204, 610666.030750621, 185658.3080911199], 
processed observation next is [1.0, 0.9130434782608695, 0.29541864139020524, 0.9666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6538805227836975, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16962945298628346, 0.1696294529862836, 0.2771019523748058], 
reward next is 0.7229, 
noisyNet noise sample is [array([0.6442287], dtype=float32), -1.0374304]. 
=============================================
[2019-03-26 02:20:03,763] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:20:03,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8479
[2019-03-26 02:20:03,779] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.85, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6152164547728143, 6.9112, 6.9112, 168.912956510431, 534249.9698801828, 534249.9698801828, 168449.7064626429], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1414200.0000, 
sim time next is 1414800.0000, 
raw observation next is [23.0, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6174036724439194, 6.9112, 6.9112, 168.912956510431, 535800.503940334, 535800.503940334, 168797.275251156], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 0.86, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.533419112736487, 0.0, 0.0, 0.8294399451523027, 0.14883347331675945, 0.14883347331675945, 0.2519362317181433], 
reward next is 0.7481, 
noisyNet noise sample is [array([0.69091785], dtype=float32), 0.008862651]. 
=============================================
[2019-03-26 02:20:05,565] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 02:20:05,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:20:05,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:20:05,568] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:20:05,568] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:20:05,568] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:20:05,572] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:20:05,574] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:20:05,573] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:20:05,578] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:20:05,578] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:20:05,613] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-26 02:20:05,640] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-26 02:20:05,668] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-26 02:20:05,669] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-26 02:20:05,720] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-26 02:20:28,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7449827], dtype=float32), -0.02014368]
[2019-03-26 02:20:28,219] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.13333333333333, 46.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6579471923615863, 6.9112, 6.9112, 168.912956510431, 587285.1097181031, 587285.1097181031, 174748.1905772691]
[2019-03-26 02:20:28,220] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:20:28,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.25231515768424295
[2019-03-26 02:22:14,302] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7482.7941 3306343301.6905 1516.0000
[2019-03-26 02:22:14,492] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7357.1714 3169696813.5901 1650.0000
[2019-03-26 02:22:14,814] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7539.4604 3105304001.8783 1571.0000
[2019-03-26 02:22:14,972] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8172.9606 2941165135.9745 1079.0000
[2019-03-26 02:22:15,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8081.3835 2984638681.2738 1098.0000
[2019-03-26 02:22:16,039] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1950000, evaluation results [1950000.0, 7482.794058683007, 3306343301.6905117, 1516.0, 7539.460365214855, 3105304001.87831, 1571.0, 8172.960554205212, 2941165135.974521, 1079.0, 7357.171376506148, 3169696813.590131, 1650.0, 8081.383511572063, 2984638681.273792, 1098.0]
[2019-03-26 02:22:21,527] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:21,536] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0410
[2019-03-26 02:22:21,543] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.7, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.62025672705759, 6.911199999999999, 6.9112, 168.912956510431, 537146.5565886591, 537146.5565886598, 169260.5567827524], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1494000.0000, 
sim time next is 1494600.0000, 
raw observation next is [22.91666666666667, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6241430388354651, 6.9112, 6.9112, 168.912956510431, 539921.8645940092, 539921.8645940092, 169881.8591343207], 
processed observation next is [0.0, 0.30434782608695654, 0.28515007898894185, 0.8916666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.541637852238372, 0.0, 0.0, 0.8294399451523027, 0.14997829572055812, 0.14997829572055812, 0.25355501363331445], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.39270902], dtype=float32), 1.1469885]. 
=============================================
[2019-03-26 02:22:25,520] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:25,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3233
[2019-03-26 02:22:25,535] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.23333333333333, 95.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7796673763622061, 6.9112, 6.9112, 168.912956510431, 656519.0869793519, 656519.0869793519, 197762.1464520841], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1906800.0000, 
sim time next is 1907400.0000, 
raw observation next is [24.21666666666667, 95.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7806545893003548, 6.9112, 6.9112, 168.912956510431, 657309.3593871044, 657309.3593871044, 197959.5944467228], 
processed observation next is [1.0, 0.043478260869565216, 0.34676145339652464, 0.9583333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7325055967077496, 0.0, 0.0, 0.8294399451523027, 0.18258593316308455, 0.18258593316308455, 0.2954620812637654], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.8725952], dtype=float32), 0.7930363]. 
=============================================
[2019-03-26 02:22:28,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:28,255] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0176
[2019-03-26 02:22:28,266] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.1, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7420057286999903, 6.9112, 6.9112, 168.912956510431, 631949.131524256, 631949.131524256, 190486.9802578159], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1922400.0000, 
sim time next is 1923000.0000, 
raw observation next is [24.21666666666667, 89.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7142497613389116, 6.9112, 6.9112, 168.912956510431, 608019.4277061047, 608019.4277061047, 185297.3870919685], 
processed observation next is [1.0, 0.2608695652173913, 0.34676145339652464, 0.895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6515240991937946, 0.0, 0.0, 0.8294399451523027, 0.16889428547391797, 0.16889428547391797, 0.2765632643163709], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.484304], dtype=float32), -1.954099]. 
=============================================
[2019-03-26 02:22:28,282] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.277084]
 [61.337364]
 [61.445618]
 [61.452583]
 [61.57817 ]], R is [[61.34977722]
 [61.45196915]
 [61.55076599]
 [61.64933777]
 [61.74859238]].
[2019-03-26 02:22:30,473] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 2.434964e-35 0.000000e+00], sum to 1.0000
[2019-03-26 02:22:30,481] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3191
[2019-03-26 02:22:30,489] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1615645.99720381 W.
[2019-03-26 02:22:30,495] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 86.0, 1.0, 2.0, 0.3852451524144117, 1.0, 1.0, 0.3852451524144117, 1.0, 2.0, 0.6487709276817665, 6.9112, 6.9112, 170.5573041426782, 1615645.99720381, 1615645.99720381, 342091.1740609413], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1857600.0000, 
sim time next is 1858200.0000, 
raw observation next is [26.0, 85.83333333333334, 1.0, 2.0, 0.3677751867525125, 1.0, 2.0, 0.3677751867525125, 1.0, 2.0, 0.6203499497631222, 6.9112, 6.9112, 170.5573041426782, 1542327.515926713, 1542327.515926713, 333390.4431191715], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.8583333333333334, 1.0, 1.0, 0.23828335753314756, 1.0, 1.0, 0.23828335753314756, 1.0, 1.0, 0.537012133857466, 0.0, 0.0, 0.8375144448122397, 0.4284243099796425, 0.4284243099796425, 0.49759767629727086], 
reward next is 0.5024, 
noisyNet noise sample is [array([0.43079743], dtype=float32), -0.14031292]. 
=============================================
[2019-03-26 02:22:30,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:30,721] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2327
[2019-03-26 02:22:30,727] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.15, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8320038420199666, 6.911200000000001, 6.9112, 168.912956510431, 692782.1274833999, 692782.1274833992, 208445.5857429653], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1881000.0000, 
sim time next is 1881600.0000, 
raw observation next is [26.06666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8261547481347725, 6.911199999999999, 6.9112, 168.912956510431, 688709.9329947372, 688709.9329947378, 207218.5592274587], 
processed observation next is [1.0, 0.782608695652174, 0.4344391785150081, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7879935952863079, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19130831472076032, 0.1913083147207605, 0.30928143168277417], 
reward next is 0.6907, 
noisyNet noise sample is [array([-1.5733232], dtype=float32), -0.10233959]. 
=============================================
[2019-03-26 02:22:32,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:32,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1371
[2019-03-26 02:22:32,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1242469.220233168 W.
[2019-03-26 02:22:32,878] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.1, 87.33333333333334, 1.0, 2.0, 0.4444684845901865, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7462316273568886, 6.911200000000001, 6.9112, 168.912956510431, 1242469.220233168, 1242469.220233167, 275612.4955021125], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1682400.0000, 
sim time next is 1683000.0000, 
raw observation next is [26.2, 87.0, 1.0, 2.0, 0.3055028897398958, 1.0, 1.0, 0.3055028897398958, 1.0, 2.0, 0.5165700087069519, 6.911199999999999, 6.9112, 170.5573041426782, 1281022.046005656, 1281022.046005657, 305316.3620168408], 
processed observation next is [1.0, 0.4782608695652174, 0.44075829383886256, 0.87, 1.0, 1.0, 0.16325649366252507, 1.0, 0.5, 0.16325649366252507, 1.0, 1.0, 0.4104512301304291, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.35583945722379334, 0.3558394572237936, 0.45569606271170265], 
reward next is 0.5443, 
noisyNet noise sample is [array([0.44264063], dtype=float32), 0.22978944]. 
=============================================
[2019-03-26 02:22:32,894] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[57.16068 ]
 [57.03438 ]
 [55.78652 ]
 [55.861294]
 [55.304596]], R is [[57.18349457]
 [57.20029831]
 [57.24062729]
 [56.66822052]
 [56.64498901]].
[2019-03-26 02:22:34,230] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:34,241] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3004
[2019-03-26 02:22:34,248] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.3, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7031516086331772, 6.911200000000001, 6.9112, 168.912956510431, 601360.3309604963, 601360.3309604957, 183295.2961486259], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1969200.0000, 
sim time next is 1969800.0000, 
raw observation next is [23.2, 93.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.701087871768882, 6.911200000000001, 6.9112, 168.912956510431, 599988.9851594074, 599988.9851594067, 182924.920357457], 
processed observation next is [1.0, 0.8260869565217391, 0.29857819905213273, 0.9333333333333332, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6354730143522951, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16666360698872426, 0.16666360698872407, 0.27302226919023437], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.6471821], dtype=float32), -0.4380357]. 
=============================================
[2019-03-26 02:22:39,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:39,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4340
[2019-03-26 02:22:39,934] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.86666666666667, 97.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7657703139196964, 6.9112, 6.9112, 168.912956510431, 645689.8343248379, 645689.8343248379, 195015.2413966245], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2090400.0000, 
sim time next is 2091000.0000, 
raw observation next is [23.83333333333334, 97.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7648054150691853, 6.9112, 6.9112, 168.912956510431, 644983.9256269227, 644983.9256269227, 194827.171626894], 
processed observation next is [0.0, 0.17391304347826086, 0.32859399684044266, 0.9783333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7131773354502259, 0.0, 0.0, 0.8294399451523027, 0.17916220156303408, 0.17916220156303408, 0.2907868233237224], 
reward next is 0.7092, 
noisyNet noise sample is [array([-1.1266904], dtype=float32), 0.10747685]. 
=============================================
[2019-03-26 02:22:39,959] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.81465]
 [72.06525]
 [72.27427]
 [72.53485]
 [72.7406 ]], R is [[71.86849213]
 [71.85874176]
 [71.84869385]
 [71.83816528]
 [71.827034  ]].
[2019-03-26 02:22:49,421] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 7.911621e-15 0.000000e+00], sum to 1.0000
[2019-03-26 02:22:49,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5694
[2019-03-26 02:22:49,441] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1046792.780266696 W.
[2019-03-26 02:22:49,446] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.86666666666667, 83.66666666666667, 1.0, 2.0, 0.3745036436395714, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6396965146275517, 6.9112, 6.9112, 168.912956510431, 1046792.780266696, 1046792.780266696, 247514.7160000795], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2187600.0000, 
sim time next is 2188200.0000, 
raw observation next is [28.08333333333334, 82.83333333333334, 1.0, 2.0, 0.3806952630544941, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6513053116853442, 6.911200000000001, 6.9112, 168.912956510431, 1064107.938572963, 1064107.938572963, 250121.191386832], 
processed observation next is [1.0, 0.30434782608695654, 0.53001579778831, 0.8283333333333335, 1.0, 1.0, 0.25384971452348687, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5747625752260295, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2955855384924897, 0.2955855384924897, 0.3733152110251224], 
reward next is 0.6267, 
noisyNet noise sample is [array([-0.9957889], dtype=float32), -1.997911]. 
=============================================
[2019-03-26 02:22:52,008] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5566565e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 02:22:52,018] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3206
[2019-03-26 02:22:52,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1378017.966534992 W.
[2019-03-26 02:22:52,038] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.1, 80.0, 1.0, 2.0, 0.4855528677137741, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8165373292646022, 6.9112, 6.9112, 168.9127917303988, 1378017.966534992, 1378017.966534992, 297108.1177436989], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1936800.0000, 
sim time next is 1937400.0000, 
raw observation next is [26.15, 79.83333333333334, 1.0, 2.0, 0.456173360358134, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7663052472454202, 6.9112, 6.9112, 168.9129564692849, 1292592.391926098, 1292592.391926098, 282368.4289902918], 
processed observation next is [1.0, 0.43478260869565216, 0.43838862559241704, 0.7983333333333335, 1.0, 1.0, 0.34478718115437834, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7150063990797807, 0.0, 0.0, 0.8294399449502564, 0.3590534422016939, 0.3590534422016939, 0.4214454164034206], 
reward next is 0.5786, 
noisyNet noise sample is [array([-2.109636], dtype=float32), 0.6861159]. 
=============================================
[2019-03-26 02:22:54,496] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:54,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6332
[2019-03-26 02:22:54,517] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.81666666666667, 67.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9463555396629265, 6.9112, 6.9112, 168.912956510431, 761534.574390876, 761534.574390876, 233549.0382609005], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2310600.0000, 
sim time next is 2311200.0000, 
raw observation next is [31.7, 68.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9582002572303899, 6.911199999999999, 6.9112, 168.912956510431, 771005.4887456789, 771005.4887456795, 236443.3110295302], 
processed observation next is [1.0, 0.782608695652174, 0.7014218009478673, 0.68, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9490247039394998, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21416819131824413, 0.2141681913182443, 0.3529004642231794], 
reward next is 0.6471, 
noisyNet noise sample is [array([-1.5822316], dtype=float32), -0.5612007]. 
=============================================
[2019-03-26 02:22:56,014] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5959908e-08 0.0000000e+00 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 02:22:56,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1118
[2019-03-26 02:22:56,034] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.01666666666667, 65.83333333333333, 1.0, 2.0, 0.818530251169251, 1.0, 2.0, 0.818530251169251, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2289215.528374047, 2289215.528374047, 428996.8992358373], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2213400.0000, 
sim time next is 2214000.0000, 
raw observation next is [32.0, 66.0, 1.0, 2.0, 0.8332922512037103, 1.0, 2.0, 0.8332922512037103, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2330539.486534208, 2330539.486534207, 436407.9629394049], 
processed observation next is [1.0, 0.6521739130434783, 0.7156398104265403, 0.66, 1.0, 1.0, 0.7991472906068798, 1.0, 1.0, 0.7991472906068798, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6473720795928356, 0.6473720795928353, 0.651355168566276], 
reward next is 0.3486, 
noisyNet noise sample is [array([0.5749155], dtype=float32), -0.21250391]. 
=============================================
[2019-03-26 02:22:56,049] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[58.97304 ]
 [58.688362]
 [58.30862 ]
 [57.83622 ]
 [57.099323]], R is [[59.30197906]
 [59.06866837]
 [58.83999252]
 [58.60953903]
 [58.38806915]].
[2019-03-26 02:22:56,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:22:56,843] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5582
[2019-03-26 02:22:56,848] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.15, 96.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7778705889309141, 6.911199999999999, 6.9112, 168.912956510431, 654897.4471729493, 654897.4471729499, 197400.0955191996], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [24.1, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7799608265248898, 6.9112, 6.9112, 168.912956510431, 656733.6792914135, 656733.6792914135, 197820.4296024464], 
processed observation next is [0.0, 0.17391304347826086, 0.3412322274881518, 0.9666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7316595445425487, 0.0, 0.0, 0.8294399451523027, 0.18242602202539265, 0.18242602202539265, 0.2952543725409648], 
reward next is 0.7047, 
noisyNet noise sample is [array([0.26608783], dtype=float32), -0.623696]. 
=============================================
[2019-03-26 02:22:57,988] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 1.584009e-34 0.000000e+00], sum to 1.0000
[2019-03-26 02:22:57,997] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2557
[2019-03-26 02:22:58,003] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 938366.5711014862 W.
[2019-03-26 02:22:58,012] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.3, 82.0, 1.0, 2.0, 0.2238208839396086, 1.0, 1.0, 0.2238208839396086, 1.0, 2.0, 0.3797863439552859, 6.911199999999999, 6.9112, 170.5573041426782, 938366.5711014862, 938366.5711014868, 276244.150757051], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2347200.0000, 
sim time next is 2347800.0000, 
raw observation next is [27.26666666666667, 82.00000000000001, 1.0, 2.0, 0.3472516839759525, 0.0, 1.0, 0.0, 1.0, 2.0, 0.5869069555278623, 6.9112, 6.9112, 168.912956510431, 970584.7341972485, 970584.7341972485, 236371.7667705143], 
processed observation next is [1.0, 0.17391304347826086, 0.4913112164297, 0.8200000000000002, 1.0, 1.0, 0.21355624575415966, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.4962279945461735, 0.0, 0.0, 0.8294399451523027, 0.26960687061034677, 0.26960687061034677, 0.3527936817470363], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.84286135], dtype=float32), -1.1429932]. 
=============================================
[2019-03-26 02:23:06,468] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.077250e-05 0.000000e+00 0.000000e+00 0.000000e+00 9.999893e-01], sum to 1.0000
[2019-03-26 02:23:06,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3523
[2019-03-26 02:23:06,494] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.2, 80.0, 1.0, 2.0, 0.1893592350809233, 1.0, 2.0, 0.1893592350809233, 1.0, 2.0, 0.3288543692796014, 6.911199999999999, 6.9112, 170.5573041426782, 793833.0183334319, 793833.0183334326, 266976.9828463271], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2417400.0000, 
sim time next is 2418000.0000, 
raw observation next is [29.16666666666666, 80.0, 1.0, 2.0, 0.1884308474113175, 1.0, 2.0, 0.1884308474113175, 1.0, 2.0, 0.327021383428097, 6.9112, 6.9112, 170.5573041426782, 789939.5922252738, 789939.5922252738, 266724.4394253935], 
processed observation next is [1.0, 1.0, 0.5813586097946285, 0.8, 1.0, 1.0, 0.022205840254599374, 1.0, 1.0, 0.022205840254599374, 1.0, 1.0, 0.17929437003426463, 0.0, 0.0, 0.8375144448122397, 0.2194276645070205, 0.2194276645070205, 0.39809617824685595], 
reward next is 0.6019, 
noisyNet noise sample is [array([2.299685], dtype=float32), -2.1433008]. 
=============================================
[2019-03-26 02:23:06,505] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[60.322117]
 [61.99573 ]
 [65.81721 ]
 [65.4148  ]
 [64.665504]], R is [[58.8275528 ]
 [58.84080505]
 [58.85339355]
 [58.89637756]
 [58.94148636]].
[2019-03-26 02:23:16,625] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 1. 0.], sum to 1.0000
[2019-03-26 02:23:16,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0296
[2019-03-26 02:23:16,644] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.98333333333333, 63.16666666666666, 1.0, 2.0, 0.8784524101105522, 1.0, 2.0, 0.8784524101105522, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2456966.920044079, 2456966.92004408, 459870.5915652485], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [31.96666666666667, 63.33333333333334, 1.0, 2.0, 0.67381664028585, 1.0, 2.0, 0.67381664028585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1884143.013920815, 1884143.013920815, 363119.9373638147], 
processed observation next is [1.0, 0.5217391304347826, 0.7140600315955767, 0.6333333333333334, 1.0, 1.0, 0.6070080003443975, 1.0, 1.0, 0.6070080003443975, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5233730594224486, 0.5233730594224486, 0.5419700557668876], 
reward next is 0.4580, 
noisyNet noise sample is [array([-0.5726497], dtype=float32), 0.49057457]. 
=============================================
[2019-03-26 02:23:17,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:23:17,172] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7230
[2019-03-26 02:23:17,178] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.16666666666667, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7736039088787285, 6.911200000000001, 6.9112, 168.912956510431, 652397.9849566373, 652397.9849566367, 196567.8398945526], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2617800.0000, 
sim time next is 2618400.0000, 
raw observation next is [25.33333333333334, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7765763517586345, 6.911200000000001, 6.9112, 168.912956510431, 654358.2785045952, 654358.2785045946, 197151.2238759159], 
processed observation next is [0.0, 0.30434782608695654, 0.3996840442338076, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7275321362910175, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18176618847349865, 0.18176618847349849, 0.2942555580237551], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.41267955], dtype=float32), -0.98476]. 
=============================================
[2019-03-26 02:23:22,296] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0000000e+00 1.0459703e-37 0.0000000e+00 8.9415043e-16 2.0299530e-28], sum to 1.0000
[2019-03-26 02:23:22,304] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9021
[2019-03-26 02:23:22,314] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1014023.837965975 W.
[2019-03-26 02:23:22,320] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 79.5, 1.0, 2.0, 0.3627857257091975, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6131056554810023, 6.911200000000001, 6.9112, 168.912956510431, 1014023.837965975, 1014023.837965974, 242096.5338001842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2353800.0000, 
sim time next is 2354400.0000, 
raw observation next is [27.9, 79.0, 1.0, 2.0, 0.7330480107280376, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1024477.758532907, 1024477.758532907, 228138.8708180117], 
processed observation next is [1.0, 0.2608695652173913, 0.5213270142180094, 0.79, 1.0, 1.0, 0.6783710972626958, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2845771551480297, 0.2845771551480297, 0.34050577734031595], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5228008], dtype=float32), -0.6604925]. 
=============================================
[2019-03-26 02:23:23,731] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 02:23:23,734] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:23:23,735] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:23:23,736] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:23:23,736] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:23:23,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:23:23,736] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:23:23,739] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:23:23,739] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:23:23,738] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:23:23,740] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:23:23,771] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-26 02:23:23,799] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-26 02:23:23,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-26 02:23:23,860] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-26 02:23:23,861] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-26 02:23:55,167] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8606527], dtype=float32), -0.012722707]
[2019-03-26 02:23:55,168] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7625662225229773, 6.9112, 6.9112, 168.912956510431, 644460.7817445411, 644460.7817445411, 194408.9175393443]
[2019-03-26 02:23:55,169] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:23:55,171] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.0012236e-38], sampled 0.13994001142249102
[2019-03-26 02:24:01,588] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8606527], dtype=float32), -0.012722707]
[2019-03-26 02:24:01,589] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6834591761109262, 6.911200000000001, 6.9112, 168.912956510431, 587064.0676119039, 587064.0676119033, 179800.319807959]
[2019-03-26 02:24:01,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:24:01,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.20630564674542518
[2019-03-26 02:24:51,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8606527], dtype=float32), -0.012722707]
[2019-03-26 02:24:51,591] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.61666666666667, 61.0, 1.0, 2.0, 0.6949413638530675, 1.0, 1.0, 0.6949413638530675, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1943281.881317553, 1943281.881317553, 371689.2512446874]
[2019-03-26 02:24:51,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:24:51,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4199509e-04 4.4620561e-26 6.3599417e-31 9.9975806e-01 6.1281771e-18], sampled 0.8829430408068346
[2019-03-26 02:25:07,654] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8606527], dtype=float32), -0.012722707]
[2019-03-26 02:25:07,655] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.35, 85.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9918222658252296, 6.9112, 6.9112, 168.912956510431, 802879.502399237, 802879.502399237, 245144.1405797926]
[2019-03-26 02:25:07,656] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:25:07,660] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6052737e-31], sampled 0.3919132598023307
[2019-03-26 02:25:16,525] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8606527], dtype=float32), -0.012722707]
[2019-03-26 02:25:16,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.45, 79.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7160340559298913, 6.9112, 6.9112, 168.912956510431, 610821.5112986965, 610821.5112986965, 185632.6909305305]
[2019-03-26 02:25:16,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:25:16,531] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.19245553251336545
[2019-03-26 02:25:33,122] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8138.3134 2940782426.9032 1178.0000
[2019-03-26 02:25:33,303] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8045.0454 2985723990.8677 1203.0000
[2019-03-26 02:25:33,450] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7462.9853 3309029740.7346 1617.0000
[2019-03-26 02:25:33,713] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7303.1783 3171838456.6170 1791.0000
[2019-03-26 02:25:33,752] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7478.3111 3107586109.7814 1728.0000
[2019-03-26 02:25:34,768] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1975000, evaluation results [1975000.0, 7462.985301329866, 3309029740.7345862, 1617.0, 7478.311110774816, 3107586109.7814417, 1728.0, 8138.313416787352, 2940782426.903197, 1178.0, 7303.178294498578, 3171838456.616969, 1791.0, 8045.0453854007055, 2985723990.8677235, 1203.0]
[2019-03-26 02:25:45,117] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:25:45,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8765
[2019-03-26 02:25:45,145] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6916440000335307, 6.9112, 6.9112, 168.912956510431, 593043.692255293, 593043.692255293, 181241.7148442036], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2744400.0000, 
sim time next is 2745000.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6859729115678705, 6.9112, 6.9112, 168.912956510431, 588179.7242589301, 588179.7242589301, 180241.9250721155], 
processed observation next is [0.0, 0.782608695652174, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6170401360583787, 0.0, 0.0, 0.8294399451523027, 0.1633832567385917, 0.1633832567385917, 0.2690177986150978], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.5864004], dtype=float32), 0.21119386]. 
=============================================
[2019-03-26 02:25:45,163] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[84.33711]
 [84.27007]
 [84.16111]
 [83.85582]
 [83.80032]], R is [[84.32981873]
 [84.21601105]
 [84.10583496]
 [83.99761963]
 [83.88928986]].
[2019-03-26 02:25:47,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:25:47,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1386
[2019-03-26 02:25:47,656] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 90.66666666666667, 1.0, 2.0, 0.2839900335624375, 1.0, 2.0, 0.2839900335624375, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 847235.0511738079, 847235.0511738079, 254012.3208254566], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2895600.0000, 
sim time next is 2896200.0000, 
raw observation next is [23.0, 91.5, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9264635280428913, 6.9112, 6.9112, 168.912956510431, 795784.5252913028, 795784.5252913028, 230275.7152998009], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.915, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.9103213756620625, 0.0, 0.0, 0.8294399451523027, 0.2210512570253619, 0.2210512570253619, 0.3436950974623894], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5230448], dtype=float32), 1.1820762]. 
=============================================
[2019-03-26 02:25:53,859] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:25:53,873] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0555
[2019-03-26 02:25:53,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 951797.3565674245 W.
[2019-03-26 02:25:53,896] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.6228801362112099, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 951797.3565674245, 951797.3565674245, 215105.4314883523], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2889600.0000, 
sim time next is 2890200.0000, 
raw observation next is [22.63333333333333, 90.33333333333333, 1.0, 2.0, 0.2156432359390706, 1.0, 1.0, 0.2156432359390706, 1.0, 1.0, 0.3775531857810365, 6.9112, 6.9112, 170.5573041426782, 974029.2655004227, 974029.2655004227, 282099.9345612745], 
processed observation next is [1.0, 0.43478260869565216, 0.27172195892575024, 0.9033333333333333, 1.0, 1.0, 0.0549918505290007, 1.0, 0.5, 0.0549918505290007, 1.0, 0.5, 0.2409185192451665, 0.0, 0.0, 0.8375144448122397, 0.2705636848612285, 0.2705636848612285, 0.4210446784496634], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.71737885], dtype=float32), 0.6492017]. 
=============================================
[2019-03-26 02:25:53,903] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:25:53,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7058
[2019-03-26 02:25:53,921] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5643483850893828, 6.911199999999999, 6.9112, 168.912956510431, 495274.6924825286, 495274.6924825293, 160735.9720342559], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [20.58333333333334, 96.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5652141157950548, 6.9112, 6.9112, 168.912956510431, 495910.2715004249, 495910.2715004249, 160862.9109233582], 
processed observation next is [1.0, 0.9130434782608695, 0.17456556082148533, 0.9650000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.46977331194518873, 0.0, 0.0, 0.8294399451523027, 0.13775285319456246, 0.13775285319456246, 0.24009389690053462], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.03079337], dtype=float32), 1.2201017]. 
=============================================
[2019-03-26 02:25:57,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:25:57,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8776
[2019-03-26 02:25:57,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1217806.696754182 W.
[2019-03-26 02:25:57,049] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4202227026487476, 0.0, 1.0, 0.0, 1.0, 2.0, 0.7170094459305001, 6.9112, 6.9112, 168.912956510431, 1217806.696754182, 1217806.696754182, 269640.7651610827], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2818800.0000, 
sim time next is 2819400.0000, 
raw observation next is [24.83333333333334, 84.0, 1.0, 2.0, 0.2737408268845808, 1.0, 1.0, 0.2737408268845808, 1.0, 2.0, 0.4629559997864026, 6.9112, 6.9112, 170.5573041426782, 1174738.022872715, 1174738.022872715, 295558.3183347889], 
processed observation next is [1.0, 0.6521739130434783, 0.3759873617693526, 0.84, 1.0, 1.0, 0.12498894805371179, 1.0, 0.5, 0.12498894805371179, 1.0, 1.0, 0.3450682924224422, 0.0, 0.0, 0.8375144448122397, 0.32631611746464306, 0.32631611746464306, 0.44113181841013266], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.095417], dtype=float32), -1.3789984]. 
=============================================
[2019-03-26 02:26:04,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:26:04,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7119
[2019-03-26 02:26:04,204] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.682637249262444, 6.9112, 6.9112, 168.912956510431, 586936.0112060449, 586936.0112060449, 179655.0908198257], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3099600.0000, 
sim time next is 3100200.0000, 
raw observation next is [22.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6742183052046246, 6.911199999999999, 6.9112, 168.912956510431, 579883.4486975231, 579883.4486975238, 178193.9350974869], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6027052502495421, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16107873574931197, 0.16107873574931217, 0.2659610971604282], 
reward next is 0.7340, 
noisyNet noise sample is [array([-0.49817428], dtype=float32), -0.22458814]. 
=============================================
[2019-03-26 02:26:11,354] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:26:11,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5706
[2019-03-26 02:26:11,369] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6729833480930978, 6.9112, 6.9112, 168.912956510431, 583606.3098482878, 583606.3098482878, 177940.3273940858], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3136800.0000, 
sim time next is 3137400.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6877783041488011, 6.9112, 6.9112, 168.912956510431, 596439.9825081596, 596439.9825081596, 180513.43149738], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6192418343278062, 0.0, 0.0, 0.8294399451523027, 0.1656777729189332, 0.1656777729189332, 0.2694230320856418], 
reward next is 0.7306, 
noisyNet noise sample is [array([1.2084299], dtype=float32), -0.89632833]. 
=============================================
[2019-03-26 02:26:24,683] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:26:24,694] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9120
[2019-03-26 02:26:24,706] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1047909.083930202 W.
[2019-03-26 02:26:24,716] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.3749028187456196, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6378348292395227, 6.9112, 6.9112, 168.912956510431, 1047909.083930202, 1047909.083930202, 247325.1960224624], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.3678173489844817, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6249281263853903, 6.9112, 6.9112, 168.912956510431, 1028094.556176724, 1028094.556176724, 244453.517491021], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.23833415540299, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5425952760797442, 0.0, 0.0, 0.8294399451523027, 0.28558182116020114, 0.28558182116020114, 0.3648559962552552], 
reward next is 0.6351, 
noisyNet noise sample is [array([-1.8789966], dtype=float32), -0.29006553]. 
=============================================
[2019-03-26 02:26:24,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[62.688766]
 [64.16562 ]
 [64.163055]
 [65.12615 ]
 [63.455956]], R is [[61.6398201 ]
 [61.02342224]
 [60.41318893]
 [60.43764114]
 [60.41982651]].
[2019-03-26 02:26:25,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:26:25,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6388
[2019-03-26 02:26:25,967] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2521770.97297433 W.
[2019-03-26 02:26:25,971] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.83333333333334, 63.66666666666666, 1.0, 2.0, 0.6010658599776922, 1.0, 2.0, 0.6010658599776922, 1.0, 1.0, 1.03, 6.926772473833378, 6.9112, 170.5573041426782, 2521770.97297433, 2510615.776519746, 488366.4123544523], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3419400.0000, 
sim time next is 3420000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.9445044806540572, 1.0, 2.0, 0.9445044806540572, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2641905.0261669, 2641905.0261669, 496362.7088326751], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.63, 1.0, 1.0, 0.9331379284988641, 1.0, 1.0, 0.9331379284988641, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7338625072685834, 0.7338625072685834, 0.7408398639293658], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8763721], dtype=float32), -0.3786697]. 
=============================================
[2019-03-26 02:26:25,990] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[58.045265]
 [59.97829 ]
 [58.151115]
 [55.964333]
 [60.34752 ]], R is [[59.53205109]
 [59.12996292]
 [58.82550812]
 [58.2372551 ]
 [57.65488434]].
[2019-03-26 02:26:27,770] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9581942e-11 4.0470648e-36 9.9236333e-28 2.7308718e-20 1.0000000e+00], sum to 1.0000
[2019-03-26 02:26:27,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9474
[2019-03-26 02:26:27,790] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 71.0, 1.0, 1.0, 0.3034446379679833, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5269829854751629, 6.911200000000001, 6.9112, 168.912889588129, 848093.0996980005, 848093.0996979999, 223079.2242775351], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3258000.0000, 
sim time next is 3258600.0000, 
raw observation next is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.2003919759665277, 1.0, 1.0, 0.2003919759665277, 1.0, 2.0, 0.3480145916147319, 6.9112, 6.9112, 170.5573041426782, 840102.6612512566, 840102.6612512566, 269952.5296364406], 
processed observation next is [0.0, 0.7391304347826086, 0.7077409162717223, 0.7166666666666667, 1.0, 1.0, 0.03661683851388877, 1.0, 0.5, 0.03661683851388877, 1.0, 1.0, 0.2048958434325999, 0.0, 0.0, 0.8375144448122397, 0.2333618503475713, 0.2333618503475713, 0.40291422333797106], 
reward next is 0.5971, 
noisyNet noise sample is [array([-0.3669886], dtype=float32), 0.24398486]. 
=============================================
[2019-03-26 02:26:31,720] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.575442e-17 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 02:26:31,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0054
[2019-03-26 02:26:31,737] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.5, 81.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2644496068220184, 6.9112, 6.9112, 170.5573041426782, 661866.083237877, 661866.083237877, 254225.7282372163], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3223800.0000, 
sim time next is 3224400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.265181557581287, 6.9112, 6.9112, 170.5573041426782, 663358.2739889976, 663358.2739889976, 254450.518253865], 
processed observation next is [0.0, 0.30434782608695654, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.10387994826986216, 0.0, 0.0, 0.8375144448122397, 0.184266187219166, 0.184266187219166, 0.3797768929162164], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2181545], dtype=float32), -0.43837824]. 
=============================================
[2019-03-26 02:26:33,006] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0697068e-02 1.8336290e-27 1.3526065e-26 9.8930293e-01 1.1907845e-31], sum to 1.0000
[2019-03-26 02:26:33,016] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5829
[2019-03-26 02:26:33,022] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 65.33333333333334, 1.0, 2.0, 0.599466392386074, 1.0, 2.0, 0.599466392386074, 1.0, 1.0, 1.03, 6.923649827446702, 6.9112, 170.5573041426782, 2515053.655251057, 2506135.337424383, 487785.0191413717], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3428400.0000, 
sim time next is 3429000.0000, 
raw observation next is [32.0, 66.5, 1.0, 2.0, 0.8859917364065081, 1.0, 2.0, 0.8859917364065081, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2478074.771927631, 2478074.771927631, 463909.3262072458], 
processed observation next is [1.0, 0.6956521739130435, 0.7156398104265403, 0.665, 1.0, 1.0, 0.8626406462729013, 1.0, 1.0, 0.8626406462729013, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6883541033132309, 0.6883541033132309, 0.6924019794137998], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16290277], dtype=float32), 0.9236601]. 
=============================================
[2019-03-26 02:26:33,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[37.359673]
 [38.06433 ]
 [37.03949 ]
 [36.035583]
 [34.986835]], R is [[37.9611969 ]
 [37.79129791]
 [37.69937897]
 [37.57587433]
 [37.4524231 ]].
[2019-03-26 02:26:38,696] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.5632378e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 02:26:38,714] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4494
[2019-03-26 02:26:38,726] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1176163.451823492 W.
[2019-03-26 02:26:38,733] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 75.66666666666666, 1.0, 2.0, 0.4207620540498946, 0.0, 2.0, 0.0, 1.0, 1.0, 0.7080765090694591, 6.911200000000001, 6.9112, 168.912956510431, 1176163.451823492, 1176163.451823491, 265267.0100124073], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3483600.0000, 
sim time next is 3484200.0000, 
raw observation next is [28.0, 74.83333333333334, 1.0, 2.0, 0.405960086786429, 1.0, 1.0, 0.405960086786429, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1134759.301230324, 1134759.301230324, 273631.1831716895], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.7483333333333334, 1.0, 1.0, 0.28428926118846864, 1.0, 0.5, 0.28428926118846864, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3152109170084233, 0.3152109170084233, 0.40840475100252166], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.082673], dtype=float32), 0.6850365]. 
=============================================
[2019-03-26 02:26:40,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.0981208e-32 6.8093754e-23], sum to 1.0000
[2019-03-26 02:26:40,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2725
[2019-03-26 02:26:40,391] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 897681.0969369805 W.
[2019-03-26 02:26:40,396] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 75.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.008254206580084, 6.9112, 168.9122704178878, 897681.0969369805, 828827.7821113762, 254812.5274729879], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3261600.0000, 
sim time next is 3262200.0000, 
raw observation next is [30.66666666666667, 75.66666666666667, 1.0, 1.0, 0.205652195675858, 1.0, 1.0, 0.205652195675858, 1.0, 2.0, 0.3571498536685982, 6.9112, 6.9112, 170.5573041426782, 862163.9281446008, 862163.9281446008, 271429.6758956955], 
processed observation next is [0.0, 0.782608695652174, 0.6524486571879939, 0.7566666666666667, 1.0, 0.5, 0.04295445262151564, 1.0, 0.5, 0.04295445262151564, 1.0, 1.0, 0.2160364069129246, 0.0, 0.0, 0.8375144448122397, 0.2394899800401669, 0.2394899800401669, 0.40511891924730675], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.532509], dtype=float32), 0.10766984]. 
=============================================
[2019-03-26 02:26:42,540] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 02:26:42,541] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:26:42,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:26:42,542] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:26:42,543] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:26:42,545] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:26:42,548] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:26:42,545] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:26:42,548] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:26:42,580] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:26:42,552] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:26:43,313] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-26 02:26:43,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-26 02:26:43,339] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-26 02:26:43,340] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-26 02:26:43,356] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-26 02:27:23,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.1237895], dtype=float32), -0.009353971]
[2019-03-26 02:27:23,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.86666666666667, 88.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7052157245563329, 6.911199999999999, 6.9112, 168.912956510431, 603937.9589075651, 603937.9589075657, 183669.377037374]
[2019-03-26 02:27:23,480] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:27:23,482] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.689384e-37], sampled 0.2877761263325995
[2019-03-26 02:27:31,207] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1237895], dtype=float32), -0.009353971]
[2019-03-26 02:27:31,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.85, 89.5, 1.0, 2.0, 0.70607876523772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1078598.675603483, 1078598.675603484, 233837.9636241932]
[2019-03-26 02:27:31,211] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:27:31,214] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3768166e-09 1.5069022e-32 3.0196562e-33 7.2803828e-23 1.0000000e+00], sampled 0.0620736411029057
[2019-03-26 02:27:35,922] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1237895], dtype=float32), -0.009353971]
[2019-03-26 02:27:35,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.91262072666667, 85.15995744166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977020727306719, 6.9112, 168.9123479075691, 875514.4574358888, 828819.1363563692, 254812.0330131004]
[2019-03-26 02:27:35,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:27:35,928] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5575486e-05 0.0000000e+00 1.8166878e-36 2.0760569e-32 9.9996448e-01], sampled 0.39239374336130095
[2019-03-26 02:27:59,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1237895], dtype=float32), -0.009353971]
[2019-03-26 02:27:59,420] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.57334659, 80.55136890333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 14.91004334061629, 6.9112, 169.8912280128125, 6549193.420764139, 841673.4859226177, 251499.79212458]
[2019-03-26 02:27:59,422] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:27:59,427] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 0.0000000e+00 0.0000000e+00 3.8242934e-22 1.0000000e+00], sampled 0.6029071882611247
[2019-03-26 02:28:17,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1237895], dtype=float32), -0.009353971]
[2019-03-26 02:28:17,128] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.27866834, 61.89137057000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9770046089887453, 6.9112, 6.9112, 168.912956510431, 790766.2314562714, 790766.2314562714, 241378.2278234597]
[2019-03-26 02:28:17,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:28:17,133] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.1923155e-01 2.0755026e-32 3.9463292e-31 4.5441751e-15 2.8076851e-01], sampled 0.7248843490490136
[2019-03-26 02:28:51,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7273.0127 3261674069.3263 841.0000
[2019-03-26 02:28:52,773] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7179.5536 3300033362.9974 760.0000
[2019-03-26 02:28:52,974] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7054.7108 3434853112.3243 805.0000
[2019-03-26 02:28:53,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7564.9101 3112364910.8381 557.0000
[2019-03-26 02:28:53,239] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7515.5780 3130928067.1701 530.0000
[2019-03-26 02:28:54,257] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2000000, evaluation results [2000000.0, 7054.710789683173, 3434853112.3242807, 805.0, 7273.012724782028, 3261674069.3263, 841.0, 7564.91010252219, 3112364910.8380737, 557.0, 7179.55358921634, 3300033362.997358, 760.0, 7515.577963151602, 3130928067.170054, 530.0]
[2019-03-26 02:29:04,103] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 9.4443627e-31 1.6061023e-26 7.1097510e-16 1.2671391e-25], sum to 1.0000
[2019-03-26 02:29:04,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3297
[2019-03-26 02:29:04,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2637744.878322423 W.
[2019-03-26 02:29:04,129] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.16666666666667, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 9.420315056606414, 6.9112, 168.8986865820307, 2637744.878322423, 857842.6806375347, 256164.2801339852], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3463800.0000, 
sim time next is 3464400.0000, 
raw observation next is [26.33333333333334, 85.66666666666667, 1.0, 1.0, 0.3820643019449809, 1.0, 1.0, 0.3820643019449809, 1.0, 2.0, 0.6478739535796305, 6.9112, 6.9112, 170.5573041426782, 1602296.134071771, 1602296.134071771, 341075.0782739817], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271754, 0.8566666666666667, 1.0, 0.5, 0.25549915896985653, 1.0, 0.5, 0.25549915896985653, 1.0, 1.0, 0.570577992170281, 0.0, 0.0, 0.8375144448122397, 0.44508225946438085, 0.44508225946438085, 0.5090672810059428], 
reward next is 0.4909, 
noisyNet noise sample is [array([1.5676451], dtype=float32), 0.25528198]. 
=============================================
[2019-03-26 02:29:33,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.527752e-30], sum to 1.0000
[2019-03-26 02:29:33,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3986
[2019-03-26 02:29:33,703] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 68.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9692259189055561, 6.911200000000001, 6.9112, 168.912956510431, 784667.4686306181, 784667.4686306175, 239438.929173892], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3871800.0000, 
sim time next is 3872400.0000, 
raw observation next is [30.66666666666667, 69.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9544277181609631, 6.9112, 6.9112, 168.912956510431, 775374.9933313566, 775374.9933313566, 235912.5649524186], 
processed observation next is [0.0, 0.8260869565217391, 0.6524486571879939, 0.69, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9444240465377598, 0.0, 0.0, 0.8294399451523027, 0.2153819425920435, 0.2153819425920435, 0.3521083058991322], 
reward next is 0.6479, 
noisyNet noise sample is [array([1.2004519], dtype=float32), -0.09897813]. 
=============================================
[2019-03-26 02:29:36,302] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:36,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6633
[2019-03-26 02:29:36,324] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9588896800518035, 6.911199999999999, 6.9112, 168.912956510431, 771560.4268489094, 771560.42684891, 236613.4189107081], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4038000.0000, 
sim time next is 4038600.0000, 
raw observation next is [30.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9698388860546231, 6.9112, 6.9112, 168.912956510431, 780373.8258510361, 780373.8258510361, 239327.5266620899], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.96321815372515, 0.0, 0.0, 0.8294399451523027, 0.21677050718084337, 0.21677050718084337, 0.357205263674761], 
reward next is 0.6428, 
noisyNet noise sample is [array([-0.10886823], dtype=float32), -1.6469495]. 
=============================================
[2019-03-26 02:29:45,482] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.3693166e-18 6.1029688e-18 7.3059391e-26 3.3319726e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 02:29:45,491] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5299
[2019-03-26 02:29:45,496] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.5722759777847611, 1.0, 2.0, 0.5722759777847611, 1.0, 2.0, 0.9938541188542975, 6.9112, 6.9112, 170.5573041426782, 2400867.022987765, 2400867.022987765, 468725.1770458458], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4328400.0000, 
sim time next is 4329000.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.5198364098467846, 1.0, 2.0, 0.5198364098467846, 1.0, 2.0, 0.902783931376151, 6.9112, 6.9112, 170.5573041426782, 2180670.933930949, 2180670.933930949, 429135.9106516755], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.84, 1.0, 1.0, 0.4214896504178127, 1.0, 1.0, 0.4214896504178127, 1.0, 1.0, 0.8814438187514037, 0.0, 0.0, 0.8375144448122397, 0.6057419260919302, 0.6057419260919302, 0.6405013591816052], 
reward next is 0.3595, 
noisyNet noise sample is [array([1.8870118], dtype=float32), 0.07726268]. 
=============================================
[2019-03-26 02:29:45,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[15.708533]
 [15.111543]
 [16.209362]
 [17.327286]
 [16.591434]], R is [[16.72955132]
 [16.86266518]
 [16.69403839]
 [16.5270977 ]
 [16.36182594]].
[2019-03-26 02:29:46,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:46,842] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6373
[2019-03-26 02:29:46,854] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3403852.456780068 W.
[2019-03-26 02:29:46,863] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 51.66666666666667, 1.0, 2.0, 0.9808689396119709, 1.0, 2.0, 0.8110245093202478, 1.0, 2.0, 1.03, 7.005119888869074, 6.9112, 170.5573041426782, 3403852.456780068, 3336573.819966333, 624737.9869610448], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4285200.0000, 
sim time next is 4285800.0000, 
raw observation next is [38.0, 51.5, 1.0, 2.0, 0.9881256916019573, 1.0, 2.0, 0.8146528853152413, 1.0, 2.0, 1.03, 7.0051204616048, 6.9112, 170.5573041426782, 3419101.522946735, 3351822.475859103, 627800.0404850192], 
processed observation next is [1.0, 0.6086956521739131, 1.0, 0.515, 1.0, 1.0, 0.9856936043397075, 1.0, 1.0, 0.776690223271375, 1.0, 1.0, 1.0365853658536586, 0.009392046160480038, 0.0, 0.8375144448122397, 0.9497504230407597, 0.9310617988497508, 0.9370149857985361], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.93210256], dtype=float32), 2.250478]. 
=============================================
[2019-03-26 02:29:50,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5485432e-27 3.5832466e-35 7.9912442e-37 3.8686625e-38 1.0000000e+00], sum to 1.0000
[2019-03-26 02:29:50,967] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5751
[2019-03-26 02:29:50,972] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 14.37079474744479, 6.9112, 169.6335937030769, 6145277.880913357, 830606.8496871983, 253746.8810124964], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4241400.0000, 
sim time next is 4242000.0000, 
raw observation next is [30.0, 77.66666666666667, 1.0, 1.0, 1.04, 1.0, 1.0, 0.940349794770232, 1.0, 2.0, 1.03, 7.385097933221007, 6.9112, 170.5573041426782, 3940468.011687461, 3600995.645185821, 674954.0622201556], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.7766666666666667, 1.0, 0.5, 1.0481927710843375, 1.0, 0.5, 0.9281322828557012, 1.0, 1.0, 1.0365853658536586, 0.047389793322100714, 0.0, 0.8375144448122397, 1.0945744476909613, 1.0002765681071726, 1.00739412271665], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.9633427], dtype=float32), 0.1717715]. 
=============================================
[2019-03-26 02:29:50,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[ 8.798995]
 [12.337342]
 [11.117317]
 [11.903297]
 [11.308547]], R is [[11.69522381]
 [11.57827187]
 [11.46248913]
 [11.34786415]
 [11.23438549]].
[2019-03-26 02:29:51,705] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:51,711] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0229
[2019-03-26 02:29:51,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1270203.77279028 W.
[2019-03-26 02:29:51,728] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.9087680681200019, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1270203.77279028, 1270203.77279028, 272362.4176827943], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4251000.0000, 
sim time next is 4251600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.4353686574829924, 1.0, 1.0, 0.4353686574829924, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1217010.218732456, 1217010.218732456, 281407.2996849761], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.79, 1.0, 1.0, 0.3197212740758945, 1.0, 0.5, 0.3197212740758945, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3380583940923489, 0.3380583940923489, 0.4200108950522031], 
reward next is 0.5800, 
noisyNet noise sample is [array([4.343309], dtype=float32), 0.64648235]. 
=============================================
[2019-03-26 02:29:51,742] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:51,749] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9828
[2019-03-26 02:29:51,756] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9687610835094013, 6.9112, 6.9112, 168.912956510431, 779506.2616273132, 779506.2616273132, 239058.772686248], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4384800.0000, 
sim time next is 4385400.0000, 
raw observation next is [32.0, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9702149386570234, 6.9112, 6.9112, 168.912956510431, 780676.5250469598, 780676.5250469598, 239421.0102528523], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.6633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9636767544597846, 0.0, 0.0, 0.8294399451523027, 0.21685459029082216, 0.21685459029082216, 0.35734479142216763], 
reward next is 0.6427, 
noisyNet noise sample is [array([-0.22418891], dtype=float32), -0.081207804]. 
=============================================
[2019-03-26 02:29:53,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.4637087e-24], sum to 1.0000
[2019-03-26 02:29:53,190] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5456
[2019-03-26 02:29:53,200] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 872605.094317438 W.
[2019-03-26 02:29:53,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97292133852984, 6.9112, 168.9123813776201, 872605.094317438, 828818.0015713018, 254811.9312985826], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4140000.0000, 
sim time next is 4140600.0000, 
raw observation next is [29.0, 84.0, 1.0, 1.0, 0.295699109789038, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5135315645136453, 6.9112, 6.9112, 168.9129062469572, 826436.8111602493, 826436.8111602493, 220581.0269049851], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 0.5, 0.15144471058920242, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4067458103824943, 0.0, 0.0, 0.8294396983356356, 0.22956578087784701, 0.22956578087784701, 0.32922541329102256], 
reward next is 0.6708, 
noisyNet noise sample is [array([-0.433451], dtype=float32), -0.66157484]. 
=============================================
[2019-03-26 02:29:53,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:53,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6885
[2019-03-26 02:29:53,414] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.33333333333333, 66.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9245534539168123, 6.911200000000001, 6.9112, 168.912956510431, 757011.0513080831, 757011.0513080824, 228960.0605868003], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4459800.0000, 
sim time next is 4460400.0000, 
raw observation next is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8924184521851801, 6.9112, 6.9112, 168.912956510431, 735659.043754213, 735659.043754213, 221636.734954489], 
processed observation next is [0.0, 0.6521739130434783, 0.6208530805687204, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8688029904697319, 0.0, 0.0, 0.8294399451523027, 0.2043497343761703, 0.2043497343761703, 0.3308010969469985], 
reward next is 0.6692, 
noisyNet noise sample is [array([-0.2707487], dtype=float32), 0.18746196]. 
=============================================
[2019-03-26 02:29:55,408] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 7.1388847e-16 3.7337681e-21 4.0648404e-37 1.5792390e-12], sum to 1.0000
[2019-03-26 02:29:55,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3193
[2019-03-26 02:29:55,433] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3197718.606835007 W.
[2019-03-26 02:29:55,439] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 75.0, 1.0, 2.0, 0.8827649101061995, 1.0, 1.0, 0.7619724945673625, 1.0, 2.0, 1.03, 7.005112147426569, 6.9112, 170.5573041426782, 3197718.606835007, 3130445.51553144, 585273.7827274714], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4177800.0000, 
sim time next is 4178400.0000, 
raw observation next is [32.66666666666667, 73.66666666666666, 1.0, 2.0, 0.7688276358921091, 1.0, 2.0, 0.7050038574603171, 1.0, 2.0, 1.03, 7.005103159778765, 6.9112, 170.5573041426782, 2958359.017387407, 2891092.364301558, 543950.7868533934], 
processed observation next is [1.0, 0.34782608695652173, 0.7472353870458138, 0.7366666666666666, 1.0, 1.0, 0.7214790793880833, 1.0, 1.0, 0.6445829607955628, 1.0, 1.0, 1.0365853658536586, 0.00939031597787654, 0.0, 0.8375144448122397, 0.8217663937187242, 0.8030812123059884, 0.8118668460498409], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07895063], dtype=float32), 1.5832052]. 
=============================================
[2019-03-26 02:29:57,252] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:29:57,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3987
[2019-03-26 02:29:57,275] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 901737.0804231154 W.
[2019-03-26 02:29:57,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 1.0, 0.2150876236143925, 1.0, 1.0, 0.2150876236143925, 1.0, 2.0, 0.3735360716541318, 6.9112, 6.9112, 170.5573041426782, 901737.0804231154, 901737.0804231154, 274176.8689280642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4324800.0000, 
sim time next is 4325400.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6195950557394172, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564676037, 865855.9385089725, 865855.9385089725, 204453.4304200427], 
processed observation next is [1.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5416807900474906, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.829439944942001, 0.2405155384747146, 0.2405155384747146, 0.30515437376125776], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.29407546], dtype=float32), -0.18099317]. 
=============================================
[2019-03-26 02:30:01,674] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:30:01,684] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9555
[2019-03-26 02:30:01,690] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333334, 69.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9778667903410477, 6.9112, 6.9112, 168.912956510431, 792625.1883574052, 792625.1883574052, 241657.954061947], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4462800.0000, 
sim time next is 4463400.0000, 
raw observation next is [31.66666666666667, 70.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.920647124683104, 6.9112, 168.9127045201245, 835505.6369897414, 828803.5315930616, 254812.2731802212], 
processed observation next is [0.0, 0.6521739130434783, 0.6998420221169038, 0.7016666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0009447124683103602, 0.0, 0.8294387077645361, 0.23208489916381705, 0.2302232032202949, 0.3803168256421212], 
reward next is 0.5724, 
noisyNet noise sample is [array([-0.5302023], dtype=float32), 0.38379458]. 
=============================================
[2019-03-26 02:30:02,042] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 02:30:02,044] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:30:02,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:30:02,045] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:30:02,045] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:30:02,047] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:30:02,046] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:30:02,047] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:30:02,047] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:30:02,049] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:30:02,050] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:30:02,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-26 02:30:02,120] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-26 02:30:02,120] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-26 02:30:02,172] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-26 02:30:02,198] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-26 02:30:11,638] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:30:11,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.07010903666667, 94.92323239666668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4418130464876823, 6.911200000000001, 6.9112, 168.912956510431, 396224.2475429479, 396224.2475429473, 144910.5922244711]
[2019-03-26 02:30:11,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:30:11,644] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5131445613003066
[2019-03-26 02:30:49,813] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:30:49,815] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.024675585, 86.81858025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6608396922943006, 6.9112, 6.9112, 168.912956510431, 569697.5343560023, 569697.5343560023, 175905.5162029242]
[2019-03-26 02:30:49,817] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:30:49,821] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9585866198314749
[2019-03-26 02:30:49,897] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:30:49,898] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.96587916, 93.69078590000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6854674977275586, 6.911200000000001, 6.9112, 168.912956510431, 587149.6714653565, 587149.6714653558, 180152.8829063483]
[2019-03-26 02:30:49,900] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:30:49,904] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8599905194002482
[2019-03-26 02:30:52,534] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:30:52,535] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.85, 77.5, 1.0, 2.0, 0.1938214997550166, 1.0, 2.0, 0.1938214997550166, 1.0, 2.0, 0.3366038473250239, 6.911200000000001, 6.9112, 178.6582176852504, 812532.8834533329, 812532.8834533324, 270166.5573168405]
[2019-03-26 02:30:52,539] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:30:52,542] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.598598e-20 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.23323513228378923
[2019-03-26 02:30:57,588] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:30:57,591] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.1794927976920301, 1.0, 2.0, 0.1794927976920301, 1.0, 2.0, 0.3076242160677526, 6.9112, 6.9112, 170.5573041426782, 752456.3661752787, 752456.3661752787, 264286.6683130555]
[2019-03-26 02:30:57,593] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:30:57,601] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.768727e-16 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.75582429034452
[2019-03-26 02:31:04,355] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:04,356] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.05, 66.5, 1.0, 2.0, 0.934819191905254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1306638.36972699, 1306638.36972699, 279698.539619702]
[2019-03-26 02:31:04,357] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:31:04,359] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0300944e-26], sampled 0.27210284087937187
[2019-03-26 02:31:04,360] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1306638.36972699 W.
[2019-03-26 02:31:07,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:07,695] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.52237578833333, 84.49762241333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8247212736895185, 6.9112, 6.9112, 168.912956510431, 692364.6718247776, 692364.6718247776, 207032.6985356545]
[2019-03-26 02:31:07,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:31:07,700] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7404141288998185
[2019-03-26 02:31:33,270] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:33,270] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.5202796, 51.81063725, 1.0, 2.0, 0.6560771903486133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 985557.968766219, 985557.9687662183, 220368.108523681]
[2019-03-26 02:31:33,270] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:31:33,272] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9999678e-01 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.1753439e-06], sampled 0.6274289996656323
[2019-03-26 02:31:33,273] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 985557.968766219 W.
[2019-03-26 02:31:44,041] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:44,043] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.92729244, 85.77943507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8784655414328087, 6.9112, 6.9112, 168.912956510431, 727563.9650294246, 727563.9650294246, 218573.4281287448]
[2019-03-26 02:31:44,043] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:31:44,046] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5962582e-16], sampled 0.02882735421587468
[2019-03-26 02:31:47,178] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:47,182] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.6, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.715217208258674, 6.9112, 168.9085145789111, 2024528.12131352, 1454145.641473526, 311351.2594441452]
[2019-03-26 02:31:47,184] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:31:47,187] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 5.773671e-21], sampled 0.06688883396740208
[2019-03-26 02:31:47,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2024528.12131352 W.
[2019-03-26 02:31:57,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0503123], dtype=float32), 0.037395045]
[2019-03-26 02:31:57,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.57140501666667, 88.37561930166666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8612059018850063, 6.9112, 6.9112, 168.912956510431, 716253.438883083, 716253.438883083, 214796.6583174519]
[2019-03-26 02:31:57,863] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:31:57,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.2185869e-24], sampled 0.3542090051490283
[2019-03-26 02:32:10,384] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7502.6914 3158951194.3508 1474.0000
[2019-03-26 02:32:11,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8074.3965 2994588852.7443 988.0000
[2019-03-26 02:32:11,461] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7991.8470 3032175984.9623 1028.0000
[2019-03-26 02:32:11,607] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7334.5407 3357252100.3956 1519.0000
[2019-03-26 02:32:11,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7319.8515 3223619047.6295 1493.0000
[2019-03-26 02:32:12,753] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2025000, evaluation results [2025000.0, 7334.540698851957, 3357252100.395606, 1519.0, 7502.691360055027, 3158951194.350783, 1474.0, 8074.39646272581, 2994588852.7442713, 988.0, 7319.85152786885, 3223619047.6294665, 1493.0, 7991.846958351988, 3032175984.9623027, 1028.0]
[2019-03-26 02:32:20,609] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:20,617] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1529
[2019-03-26 02:32:20,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2705938.927163974 W.
[2019-03-26 02:32:20,637] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.9673724035324492, 1.0, 2.0, 0.9673724035324492, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2705938.927163974, 2705938.927163974, 509591.6335176394], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4614600.0000, 
sim time next is 4615200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.942715333288308, 1.0, 2.0, 0.942715333288308, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2636895.261165785, 2636895.261165785, 495343.0563955469], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.9309823292630217, 1.0, 1.0, 0.9309823292630217, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7324709058793848, 0.7324709058793848, 0.7393179946202192], 
reward next is 0.2607, 
noisyNet noise sample is [array([0.23020047], dtype=float32), -0.8308882]. 
=============================================
[2019-03-26 02:32:21,830] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:21,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2415
[2019-03-26 02:32:21,853] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3111625.223141582 W.
[2019-03-26 02:32:21,859] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 56.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.193272662149723, 6.9112, 170.5573041426782, 3111625.223141582, 2909565.10126144, 552169.662397791], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4376400.0000, 
sim time next is 4377000.0000, 
raw observation next is [35.66666666666666, 56.83333333333333, 1.0, 2.0, 0.8754861817695089, 1.0, 2.0, 0.7583331303990171, 1.0, 1.0, 1.03, 7.005111573159915, 6.9112, 170.5573041426782, 3182426.072095957, 3115153.392162954, 582488.534445593], 
processed observation next is [1.0, 0.6521739130434783, 0.889415481832543, 0.5683333333333332, 1.0, 1.0, 0.8499833515295288, 1.0, 1.0, 0.7088350968662857, 1.0, 0.5, 1.0365853658536586, 0.00939115731599154, 0.0, 0.8375144448122397, 0.884007242248877, 0.8653203867119317, 0.8693858723068553], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10204107], dtype=float32), -0.87976176]. 
=============================================
[2019-03-26 02:32:21,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[36.544304]
 [37.304363]
 [37.766808]
 [38.16137 ]
 [38.566128]], R is [[35.5473938 ]
 [35.19192123]
 [35.0521965 ]
 [34.92295074]
 [34.57372284]].
[2019-03-26 02:32:28,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:28,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1594
[2019-03-26 02:32:28,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1094946.145830395 W.
[2019-03-26 02:32:28,248] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 72.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.286209566651512, 6.9112, 168.9108740482489, 1094946.145830395, 828904.7323762346, 254812.8199207959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4468200.0000, 
sim time next is 4468800.0000, 
raw observation next is [31.33333333333334, 73.66666666666667, 1.0, 1.0, 0.6459716320371938, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9126511613977, 902731.6709927157, 902731.6709927157, 209620.6176316363], 
processed observation next is [0.0, 0.7391304347826086, 0.6840442338072673, 0.7366666666666667, 1.0, 0.5, 0.5734597976351732, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294384457487597, 0.2507587974979766, 0.2507587974979766, 0.3128665934800542], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3265572], dtype=float32), 0.13804965]. 
=============================================
[2019-03-26 02:32:33,027] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:33,034] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1200
[2019-03-26 02:32:33,047] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1321596.498947001 W.
[2019-03-26 02:32:33,056] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.4727599123330165, 1.0, 1.0, 0.4727599123330165, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1321596.498947001, 1321596.498947001, 292014.4613980449], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4596600.0000, 
sim time next is 4597200.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.4609466761744764, 1.0, 2.0, 0.4609466761744764, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1288552.84585522, 1288552.84585522, 288574.8900980323], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.94, 1.0, 1.0, 0.35053816406563415, 1.0, 1.0, 0.35053816406563415, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.35793134607089444, 0.35793134607089444, 0.430708791191093], 
reward next is 0.5693, 
noisyNet noise sample is [array([-0.7932907], dtype=float32), -1.5931535]. 
=============================================
[2019-03-26 02:32:39,290] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:39,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7254
[2019-03-26 02:32:39,309] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.83333333333334, 74.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8240582080891342, 6.9112, 6.9112, 168.912956510431, 688062.4498286162, 688062.4498286162, 206802.4616992174], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4752600.0000, 
sim time next is 4753200.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8204180248428954, 6.911200000000001, 6.9112, 168.912956510431, 685402.6333898442, 685402.6333898436, 206041.0832667093], 
processed observation next is [1.0, 0.0, 0.5102685624012641, 0.7566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7809975912718237, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19038962038606783, 0.1903896203860677, 0.3075240048756855], 
reward next is 0.6925, 
noisyNet noise sample is [array([0.75388867], dtype=float32), -0.8695313]. 
=============================================
[2019-03-26 02:32:39,782] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:39,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9887
[2019-03-26 02:32:39,807] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2168149.464040354 W.
[2019-03-26 02:32:39,813] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.9093681956877819, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.986675358257092, 6.9112, 168.912507351289, 2168149.464040354, 2114604.789613297, 437374.794607375], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873800.0000, 
sim time next is 4874400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.7342720845175039, 1.0, 1.0, 0.7342720845175039, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2053352.044123584, 2053352.044123584, 389138.9782074791], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.7, 1.0, 1.0, 0.6798458849608481, 1.0, 0.5, 0.6798458849608481, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5703755678121066, 0.5703755678121066, 0.5808044450857898], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7044634], dtype=float32), -0.6811507]. 
=============================================
[2019-03-26 02:32:44,564] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:44,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0290
[2019-03-26 02:32:44,585] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2143756.689799715 W.
[2019-03-26 02:32:44,590] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.511045452591588, 1.0, 1.0, 0.511045452591588, 1.0, 2.0, 0.8875169458378637, 6.9112, 6.9112, 170.5573041426782, 2143756.689799715, 2143756.689799715, 422868.1706935889], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4698000.0000, 
sim time next is 4698600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.029760097313366, 6.9112, 168.9122838035689, 2369199.494139777, 2285089.215558046, 475866.2176667057], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.01185600973133658, 0.0, 0.8294366418536432, 0.6581109705943825, 0.6347470043216794, 0.71024808606971], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22251976], dtype=float32), -0.3670715]. 
=============================================
[2019-03-26 02:32:45,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:45,274] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6052
[2019-03-26 02:32:45,284] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1203658.108332601 W.
[2019-03-26 02:32:45,293] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.33333333333334, 82.33333333333334, 1.0, 2.0, 0.8611849036303105, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1203658.108332601, 1203658.108332602, 259484.6417312889], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4692000.0000, 
sim time next is 4692600.0000, 
raw observation next is [28.5, 81.5, 1.0, 2.0, 0.4323154574712653, 1.0, 1.0, 0.4323154574712653, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1208470.628763246, 1208470.628763246, 280576.7850351524], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.815, 1.0, 1.0, 0.31604271984489796, 1.0, 0.5, 0.31604271984489796, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3356862857675683, 0.3356862857675683, 0.41877132094798863], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6423085], dtype=float32), -0.3296887]. 
=============================================
[2019-03-26 02:32:50,649] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:50,653] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9014
[2019-03-26 02:32:50,661] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333333, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9070191972306431, 6.911199999999999, 6.9112, 168.912956510431, 742720.5266115637, 742720.5266115644, 224823.5452717287], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5060400.0000, 
sim time next is 5061000.0000, 
raw observation next is [31.16666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8973871229977365, 6.9112, 6.9112, 168.912956510431, 736515.5528379027, 736515.5528379027, 222655.4387624505], 
processed observation next is [0.0, 0.5652173913043478, 0.6761453396524489, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8748623451191907, 0.0, 0.0, 0.8294399451523027, 0.20458765356608408, 0.20458765356608408, 0.3323215503917172], 
reward next is 0.6677, 
noisyNet noise sample is [array([0.48524702], dtype=float32), -0.38619754]. 
=============================================
[2019-03-26 02:32:50,688] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[70.98113 ]
 [70.846176]
 [70.72736 ]
 [70.65154 ]
 [70.60495 ]], R is [[71.07481384]
 [71.02851105]
 [70.97943115]
 [70.92764282]
 [70.87487793]].
[2019-03-26 02:32:56,223] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:56,232] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8370
[2019-03-26 02:32:56,248] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1151679.69359876 W.
[2019-03-26 02:32:56,253] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.2746734046862369, 1.0, 2.0, 0.2746734046862369, 1.0, 1.0, 0.4649347759760152, 6.9112, 6.9112, 170.5573041426782, 1151679.69359876, 1151679.69359876, 293296.9436458251], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5198400.0000, 
sim time next is 5199000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.2680890842758105, 1.0, 2.0, 0.2680890842758105, 1.0, 2.0, 0.4538870628408041, 6.9112, 6.9112, 170.5573041426782, 1124057.787136518, 1124057.787136518, 290892.4662854125], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.11817961960941026, 1.0, 1.0, 0.11817961960941026, 1.0, 1.0, 0.3340086132204928, 0.0, 0.0, 0.8375144448122397, 0.31223827420458833, 0.31223827420458833, 0.4341678601274813], 
reward next is 0.5658, 
noisyNet noise sample is [array([-0.06063041], dtype=float32), -0.5964269]. 
=============================================
[2019-03-26 02:32:56,266] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[38.665726]
 [37.25615 ]
 [37.366287]
 [37.816544]
 [37.956337]], R is [[40.1109848 ]
 [39.70987701]
 [39.89874649]
 [40.0939827 ]
 [39.69304276]].
[2019-03-26 02:32:59,798] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:32:59,806] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3986
[2019-03-26 02:32:59,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2042818.389349294 W.
[2019-03-26 02:32:59,827] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.4870059143543725, 1.0, 2.0, 0.4870059143543725, 1.0, 1.0, 0.8403999484834705, 6.911200000000001, 6.9112, 170.5573041426782, 2042818.389349294, 2042818.389349293, 405334.796870511], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4875600.0000, 
sim time next is 4876200.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.9028782221928564, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990730841829238, 6.9112, 168.9124832658376, 2159065.224608866, 2102643.46625255, 435422.674058446], 
processed observation next is [1.0, 0.43478260869565216, 0.6445497630331753, 0.68, 1.0, 1.0, 0.8829858098709114, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007953084182923842, 0.0, 0.8294376213046968, 0.5997403401691295, 0.5840676295145972, 0.6498845881469344], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9615985], dtype=float32), 2.0735137]. 
=============================================
[2019-03-26 02:33:04,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:33:04,780] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8067
[2019-03-26 02:33:04,789] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1910994.340140412 W.
[2019-03-26 02:33:04,796] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.16666666666667, 65.16666666666667, 1.0, 2.0, 0.4556071930835914, 1.0, 2.0, 0.4556071930835914, 1.0, 1.0, 0.7773418447505046, 6.9112, 6.9112, 170.5573041426782, 1910994.340140412, 1910994.340140412, 383522.4404624472], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4967400.0000, 
sim time next is 4968000.0000, 
raw observation next is [30.2, 65.0, 1.0, 2.0, 0.7510667477757486, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.976730562626127, 6.9112, 168.9125660807225, 1946591.655359893, 1900102.126302344, 397131.0470921863], 
processed observation next is [1.0, 0.5217391304347826, 0.6303317535545023, 0.65, 1.0, 1.0, 0.700080419006926, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.006553056262612688, 0.0, 0.8294380279636945, 0.540719904266637, 0.5278061461950956, 0.5927329061077408], 
reward next is 0.0796, 
noisyNet noise sample is [array([-0.07326763], dtype=float32), 2.2408133]. 
=============================================
[2019-03-26 02:33:04,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[49.311134]
 [49.332703]
 [48.26557 ]
 [48.174274]
 [48.864716]], R is [[48.63106155]
 [48.57233047]
 [48.54471207]
 [48.53616333]
 [48.17495728]].
[2019-03-26 02:33:06,788] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:33:06,797] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3435
[2019-03-26 02:33:06,806] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.81666666666667, 78.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9330594828901653, 6.9112, 6.9112, 168.912956510431, 761900.6960647892, 761900.6960647892, 230904.6987452998], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5255400.0000, 
sim time next is 5256000.0000, 
raw observation next is [28.8, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9368969846809737, 6.9112, 6.9112, 168.912956510431, 764918.5746976368, 764918.5746976368, 231824.4491511564], 
processed observation next is [1.0, 0.8695652173913043, 0.5639810426540285, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.92304510326948, 0.0, 0.0, 0.8294399451523027, 0.21247738186045464, 0.21247738186045464, 0.34600664052411406], 
reward next is 0.6540, 
noisyNet noise sample is [array([0.3525102], dtype=float32), 1.2217534]. 
=============================================
[2019-03-26 02:33:06,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.23111]
 [69.60154]
 [69.58056]
 [69.61152]
 [69.38378]], R is [[69.02175903]
 [68.98690796]
 [68.95424652]
 [68.9213562 ]
 [68.88796234]].
[2019-03-26 02:33:17,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:33:17,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4254
[2019-03-26 02:33:17,698] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1191638.098127058 W.
[2019-03-26 02:33:17,701] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8525897338478473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191638.098127058, 1191638.098127057, 257233.0642912946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5287200.0000, 
sim time next is 5287800.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.8453244220013031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1181477.948575945, 1181477.948575945, 255344.3761271308], 
processed observation next is [1.0, 0.17391304347826086, 0.5545023696682465, 0.88, 1.0, 1.0, 0.8136438819292808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32818831904887363, 0.32818831904887363, 0.3811110091449713], 
reward next is 0.6189, 
noisyNet noise sample is [array([0.00485601], dtype=float32), -0.13572657]. 
=============================================
[2019-03-26 02:33:20,390] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 02:33:20,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:33:20,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:33:20,396] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:33:20,396] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:33:20,398] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:33:20,399] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:33:20,399] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:33:20,398] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:33:20,403] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:33:20,404] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:33:20,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-26 02:33:20,463] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-26 02:33:20,488] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-26 02:33:20,514] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-26 02:33:20,537] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-26 02:33:41,269] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1898303], dtype=float32), 0.03220943]
[2019-03-26 02:33:41,270] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.59415652, 70.71415712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6196638866009201, 6.911199999999999, 6.9112, 168.912956510431, 539207.4491340343, 539207.449134035, 169129.0562045433]
[2019-03-26 02:33:41,273] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:33:41,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.1858996679470154
[2019-03-26 02:34:12,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1898303], dtype=float32), 0.03220943]
[2019-03-26 02:34:12,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.16666666666667, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8258396166075013, 6.911199999999999, 6.9112, 168.912956510431, 689468.4503746668, 689468.4503746675, 207179.1728128956]
[2019-03-26 02:34:12,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:34:12,555] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.31260218916969307
[2019-03-26 02:34:56,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1898303], dtype=float32), 0.03220943]
[2019-03-26 02:34:56,937] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.95, 74.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.3304695230621, 6.9112, 168.9105798981263, 2581392.099987051, 2283952.055236124, 475182.1486445133]
[2019-03-26 02:34:56,937] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:34:56,939] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.4218504e-35], sampled 0.6716506659021616
[2019-03-26 02:34:56,941] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2581392.099987051 W.
[2019-03-26 02:35:03,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1898303], dtype=float32), 0.03220943]
[2019-03-26 02:35:03,944] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 84.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8869827839058181, 6.9112, 6.9112, 168.912956510431, 731295.1431127662, 731295.1431127662, 220394.2306291193]
[2019-03-26 02:35:03,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:35:03,947] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.13779654068704017
[2019-03-26 02:35:30,180] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7361.8221 3107573617.3968 1979.0000
[2019-03-26 02:35:30,350] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7317.7387 3319159761.9734 2043.0000
[2019-03-26 02:35:30,352] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8069.6116 2938203644.8819 1351.0000
[2019-03-26 02:35:30,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7097.8947 3183991640.5255 2278.0000
[2019-03-26 02:35:30,635] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7946.8008 2988005245.6755 1479.0000
[2019-03-26 02:35:31,651] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2050000, evaluation results [2050000.0, 7317.7387174861215, 3319159761.9733872, 2043.0, 7361.822133711847, 3107573617.3967547, 1979.0, 8069.611590998333, 2938203644.881877, 1351.0, 7097.8946715342845, 3183991640.525541, 2278.0, 7946.8007662642, 2988005245.6754575, 1479.0]
[2019-03-26 02:35:31,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:35:31,954] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7928
[2019-03-26 02:35:31,968] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1062170.780942481 W.
[2019-03-26 02:35:31,978] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.33333333333334, 77.33333333333334, 1.0, 2.0, 0.3800025719605797, 0.0, 1.0, 0.0, 1.0, 1.0, 0.6449782368466024, 6.9112, 6.9112, 168.912956510431, 1062170.780942481, 1062170.780942481, 249133.0876143232], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5210400.0000, 
sim time next is 5211000.0000, 
raw observation next is [28.5, 76.5, 1.0, 2.0, 0.3783836509665088, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6419948744408729, 6.911200000000001, 6.9112, 168.912956510431, 1057643.372349331, 1057643.37234933, 248459.3044478507], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.765, 1.0, 1.0, 0.25106463971868526, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5634083834644791, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29378982565259193, 0.29378982565259165, 0.37083478275798615], 
reward next is 0.6292, 
noisyNet noise sample is [array([-1.1710695], dtype=float32), -1.7521411]. 
=============================================
[2019-03-26 02:35:31,989] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[10.626698]
 [10.061941]
 [11.025202]
 [11.161773]
 [12.383327]], R is [[12.11737823]
 [11.99620438]
 [11.87624264]
 [11.75748062]
 [12.23807812]].
[2019-03-26 02:35:34,782] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:35:34,793] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7386
[2019-03-26 02:35:34,808] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1211220.981374685 W.
[2019-03-26 02:35:34,812] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.61666666666667, 82.66666666666667, 1.0, 2.0, 0.4332988067103095, 1.0, 1.0, 0.4332988067103095, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1211220.981374685, 1211220.981374685, 280845.5795444631], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5557800.0000, 
sim time next is 5558400.0000, 
raw observation next is [28.8, 82.0, 1.0, 2.0, 0.5040233165585758, 1.0, 2.0, 0.5040233165585758, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1409050.532095159, 1409050.532095159, 301512.5123565309], 
processed observation next is [1.0, 0.34782608695652173, 0.5639810426540285, 0.82, 1.0, 1.0, 0.4024377307934648, 1.0, 1.0, 0.4024377307934648, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3914029255819886, 0.3914029255819886, 0.4500186751590013], 
reward next is 0.5500, 
noisyNet noise sample is [array([-0.50040215], dtype=float32), -0.39218816]. 
=============================================
[2019-03-26 02:35:47,134] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:35:47,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1010
[2019-03-26 02:35:47,157] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.3, 61.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8807749703116223, 6.9112, 6.9112, 168.912956510431, 726191.7161211467, 726191.7161211467, 218979.9847989066], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5740800.0000, 
sim time next is 5741400.0000, 
raw observation next is [31.45, 60.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8812504946156359, 6.911199999999999, 6.9112, 168.912956510431, 726444.820945256, 726444.8209452567, 219082.7754998079], 
processed observation next is [0.0, 0.43478260869565216, 0.6895734597156398, 0.6066666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8551835300190681, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017902280403489, 0.2017902280403491, 0.3269892171638924], 
reward next is 0.6730, 
noisyNet noise sample is [array([1.6416354], dtype=float32), -0.03284044]. 
=============================================
[2019-03-26 02:36:03,178] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:36:03,190] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2293
[2019-03-26 02:36:03,197] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.75, 92.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8659755632154844, 6.911200000000001, 6.9112, 168.912956510431, 717350.7608672505, 717350.7608672499, 215767.4462293044], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5622600.0000, 
sim time next is 5623200.0000, 
raw observation next is [25.7, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.863159734157759, 6.911200000000001, 6.9112, 168.912956510431, 715305.0353794238, 715305.0353794231, 215149.2199093625], 
processed observation next is [0.0, 0.08695652173913043, 0.4170616113744076, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8331216270216574, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19869584316095104, 0.19869584316095085, 0.3211182386706903], 
reward next is 0.6789, 
noisyNet noise sample is [array([1.0373476], dtype=float32), -0.97801626]. 
=============================================
[2019-03-26 02:36:08,733] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 3.0912651e-33 7.0949981e-35 0.0000000e+00 2.5709014e-36], sum to 1.0000
[2019-03-26 02:36:08,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2474
[2019-03-26 02:36:08,749] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.7, 62.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 9.729494105977963, 6.9112, 168.8968342018454, 4284727.867674105, 2285523.867806633, 467945.5821212297], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5836800.0000, 
sim time next is 5837400.0000, 
raw observation next is [32.75, 62.0, 1.0, 2.0, 0.9794985502071398, 1.0, 1.0, 0.8103393146178327, 1.0, 2.0, 1.03, 7.005119780713308, 6.9112, 170.5573041426782, 3400972.792843174, 3333694.233505814, 624159.2828588902], 
processed observation next is [1.0, 0.5652173913043478, 0.7511848341232228, 0.62, 1.0, 1.0, 0.9752994580808914, 1.0, 0.5, 0.7714931501419671, 1.0, 1.0, 1.0365853658536586, 0.009391978071330787, 0.0, 0.8375144448122397, 0.9447146646786595, 0.9260261759738372, 0.9315810191923735], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.111767], dtype=float32), -1.7132549]. 
=============================================
[2019-03-26 02:36:13,334] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:36:13,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5078
[2019-03-26 02:36:13,349] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [33.8, 53.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9154578914372504, 6.911199999999999, 6.9112, 168.912956510431, 748202.109028389, 748202.1090283897, 226742.6702667851], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5748600.0000, 
sim time next is 5749200.0000, 
raw observation next is [34.0, 53.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9223761330071919, 6.911199999999999, 6.9112, 168.912956510431, 752631.2448169832, 752631.2448169838, 228325.073772697], 
processed observation next is [0.0, 0.5652173913043478, 0.8104265402843602, 0.53, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9053367475697461, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20906423467138424, 0.20906423467138438, 0.34078369219805527], 
reward next is 0.6592, 
noisyNet noise sample is [array([1.1341698], dtype=float32), -0.01902157]. 
=============================================
[2019-03-26 02:36:18,666] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:36:18,678] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0429
[2019-03-26 02:36:18,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2332832.19797835 W.
[2019-03-26 02:36:18,698] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.8, 75.33333333333333, 1.0, 2.0, 0.8341112534799425, 1.0, 2.0, 0.8341112534799425, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2332832.19797835, 2332832.197978349, 436819.6999606001], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5820000.0000, 
sim time next is 5820600.0000, 
raw observation next is [29.95, 74.66666666666667, 1.0, 2.0, 0.5620217342978995, 1.0, 2.0, 0.5620217342978995, 1.0, 1.0, 0.9760458890477594, 6.9112, 6.9112, 170.5573041426782, 2357806.853513224, 2357806.853513224, 460684.4290850396], 
processed observation next is [1.0, 0.34782608695652173, 0.6184834123222749, 0.7466666666666667, 1.0, 1.0, 0.4723153425275897, 1.0, 1.0, 0.4723153425275897, 1.0, 0.5, 0.970787669570438, 0.0, 0.0, 0.8375144448122397, 0.6549463481981178, 0.6549463481981178, 0.6875887001269247], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1622882], dtype=float32), -1.2555224]. 
=============================================
[2019-03-26 02:36:24,126] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:36:24,139] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5113
[2019-03-26 02:36:24,144] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.53333333333333, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8823023963705315, 6.9112, 6.9112, 168.912956510431, 727041.0059077957, 727041.0059077957, 219311.6493460972], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6223800.0000, 
sim time next is 6224400.0000, 
raw observation next is [26.5, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8817091036996166, 6.911200000000001, 6.9112, 168.912956510431, 726659.8264886804, 726659.8264886797, 219180.8263120756], 
processed observation next is [0.0, 0.043478260869565216, 0.4549763033175356, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8557428093897763, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20184995180241122, 0.20184995180241103, 0.32713556165981433], 
reward next is 0.6729, 
noisyNet noise sample is [array([0.506771], dtype=float32), 1.273093]. 
=============================================
[2019-03-26 02:36:39,562] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 02:36:39,565] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:36:39,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:36:39,568] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:36:39,570] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:36:39,572] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:36:39,573] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:36:39,573] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:36:39,575] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:36:39,576] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:36:39,576] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:36:39,608] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-26 02:36:39,637] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-26 02:36:39,638] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-26 02:36:39,694] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-26 02:36:39,719] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-26 02:36:52,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:36:52,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.502945965, 73.45047272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4653729592979034, 6.911200000000001, 6.9112, 168.912956510431, 416520.9836292092, 416520.9836292085, 147594.6778525936]
[2019-03-26 02:36:52,938] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:36:52,941] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24046682450141466
[2019-03-26 02:37:22,721] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:37:22,722] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.034717855, 86.941225645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5343696120470133, 6.9112, 6.9112, 168.912956510431, 473690.9778375734, 473690.9778375734, 156412.536176981]
[2019-03-26 02:37:22,724] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:37:22,729] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6657420173970442
[2019-03-26 02:37:27,427] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:37:27,428] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.06725850333333, 90.70240110833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6683631167225494, 6.9112, 6.9112, 168.912956510431, 576401.7882990994, 576401.7882990994, 177181.5951286819]
[2019-03-26 02:37:27,428] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:37:27,430] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8696227127411263
[2019-03-26 02:38:08,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:08,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.5, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.224300631438072, 6.9112, 168.9109835756649, 2506004.316772275, 2283882.547191935, 475427.14237926]
[2019-03-26 02:38:08,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:38:08,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.12582027503895532
[2019-03-26 02:38:08,477] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2506004.316772275 W.
[2019-03-26 02:38:14,318] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:14,322] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.82783736, 86.84650922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9004233142467796, 6.9112, 6.9112, 168.912956510431, 743379.103415698, 743379.103415698, 223530.622763722]
[2019-03-26 02:38:14,324] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:38:14,327] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.23843037733890404
[2019-03-26 02:38:19,651] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:19,651] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.52793514, 71.596088415, 1.0, 2.0, 0.6355811323935641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 888205.0954667621, 888205.0954667621, 207562.236945433]
[2019-03-26 02:38:19,652] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:38:19,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.24474322196366372
[2019-03-26 02:38:19,653] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 888205.0954667621 W.
[2019-03-26 02:38:20,053] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:20,057] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.296157795, 83.23808363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9705713437257345, 6.9112, 6.9112, 168.912956510431, 788934.7358537159, 788934.7358537159, 239941.6211459275]
[2019-03-26 02:38:20,058] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:38:20,062] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2987885555183539
[2019-03-26 02:38:33,091] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:33,095] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.61103383666666, 74.00547613666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7232849281435053, 6.911199999999999, 6.9112, 168.912956510431, 618450.8664980531, 618450.8664980537, 186976.8334322271]
[2019-03-26 02:38:33,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:38:33,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6777376993071882
[2019-03-26 02:38:43,260] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:43,261] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.96356601333333, 60.28310833166667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7331695908158232, 6.9112, 6.9112, 168.912956510431, 623948.4888012647, 623948.4888012647, 188809.4329135976]
[2019-03-26 02:38:43,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:38:43,267] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.855280448557999
[2019-03-26 02:38:44,988] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1874774], dtype=float32), 0.0017624578]
[2019-03-26 02:38:44,989] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.6, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.921624682429665, 6.9112, 168.9126933876242, 837820.139549466, 830424.5225644633, 254908.0016492978]
[2019-03-26 02:38:44,991] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:38:44,993] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 5.59152e-25], sampled 0.7443071711767455
[2019-03-26 02:38:49,183] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7091.0018 3183421464.1129 2288.0000
[2019-03-26 02:38:49,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7942.8478 2988228327.4544 1491.0000
[2019-03-26 02:38:49,480] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7354.7863 3107020070.6825 1987.0000
[2019-03-26 02:38:49,552] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7318.2080 3319244664.6473 2036.0000
[2019-03-26 02:38:49,583] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8070.7838 2937981174.7835 1356.0000
[2019-03-26 02:38:50,602] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2075000, evaluation results [2075000.0, 7318.20800641662, 3319244664.6473308, 2036.0, 7354.786265172497, 3107020070.6824813, 1987.0, 8070.783847272133, 2937981174.783516, 1356.0, 7091.001759899151, 3183421464.11293, 2288.0, 7942.847813388286, 2988228327.4543724, 1491.0]
[2019-03-26 02:38:52,505] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:38:52,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7735
[2019-03-26 02:38:52,520] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.45, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.883588502876805, 6.9112, 6.9112, 168.912956510431, 730465.7556748218, 730465.7556748218, 219691.331015498], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6384600.0000, 
sim time next is 6385200.0000, 
raw observation next is [27.43333333333333, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8812605262877738, 6.9112, 6.9112, 168.912956510431, 728715.8697678183, 728715.8697678183, 219167.535571856], 
processed observation next is [0.0, 0.9130434782608695, 0.49921011058451803, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8551957637655777, 0.0, 0.0, 0.8294399451523027, 0.20242107493550507, 0.20242107493550507, 0.32711572473411343], 
reward next is 0.6729, 
noisyNet noise sample is [array([1.5854889], dtype=float32), -0.33020765]. 
=============================================
[2019-03-26 02:38:54,519] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:38:54,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9142
[2019-03-26 02:38:54,534] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.3, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9097988088267673, 6.911200000000001, 6.9112, 168.912956510431, 747387.7525456371, 747387.7525456365, 225575.3221918042], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6314400.0000, 
sim time next is 6315000.0000, 
raw observation next is [27.26666666666667, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9083927436552462, 6.9112, 6.9112, 168.912956510431, 746274.5013215193, 746274.5013215193, 225247.3228605063], 
processed observation next is [0.0, 0.08695652173913043, 0.4913112164297, 0.8616666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8882838337259099, 0.0, 0.0, 0.8294399451523027, 0.20729847258931092, 0.20729847258931092, 0.3361900341201587], 
reward next is 0.6638, 
noisyNet noise sample is [array([1.703421], dtype=float32), 0.33952004]. 
=============================================
[2019-03-26 02:38:54,566] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.93163 ]
 [58.120445]
 [58.280403]
 [58.622883]
 [58.93436 ]], R is [[61.60250854]
 [61.64980316]
 [61.69691849]
 [61.74396133]
 [61.79085922]].
[2019-03-26 02:38:56,746] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:38:56,756] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5857
[2019-03-26 02:38:56,762] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.915120168964873, 6.9112, 6.9112, 168.912956510431, 749521.4657144416, 749521.4657144416, 226733.4403119813], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6361200.0000, 
sim time next is 6361800.0000, 
raw observation next is [31.0, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9492920738529647, 6.911200000000001, 6.9112, 168.912956510431, 776852.0980397784, 776852.0980397778, 234922.81906777], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9381610656743471, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.215792249455494, 0.21579224945549383, 0.3506310732354776], 
reward next is 0.6494, 
noisyNet noise sample is [array([-1.6293076], dtype=float32), -1.0546134]. 
=============================================
[2019-03-26 02:39:00,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:00,131] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3273
[2019-03-26 02:39:00,135] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.4, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8851704278517187, 6.911200000000001, 6.9112, 168.912956510431, 729235.516811192, 729235.5168111913, 219958.6869179909], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6228000.0000, 
sim time next is 6228600.0000, 
raw observation next is [26.41666666666666, 91.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8850720455036252, 6.911200000000001, 6.9112, 168.912956510431, 729037.1732101982, 729037.1732101976, 219931.7577841239], 
processed observation next is [0.0, 0.08695652173913043, 0.4510268562401261, 0.9100000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8598439579312502, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20251032589172172, 0.20251032589172155, 0.3282563549016775], 
reward next is 0.6717, 
noisyNet noise sample is [array([-0.3952921], dtype=float32), -1.3520241]. 
=============================================
[2019-03-26 02:39:02,860] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.1659447e-32], sum to 1.0000
[2019-03-26 02:39:02,874] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1495
[2019-03-26 02:39:02,883] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.63333333333334, 83.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9090017973687147, 6.911199999999999, 6.9112, 168.912956510431, 746530.7973011857, 746530.7973011864, 225380.0437189411], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6295800.0000, 
sim time next is 6296400.0000, 
raw observation next is [27.6, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9099048855851405, 6.9112, 6.9112, 168.912956510431, 747363.2004427084, 747363.2004427084, 225595.61897593], 
processed observation next is [0.0, 0.9130434782608695, 0.5071090047393366, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8901279092501712, 0.0, 0.0, 0.8294399451523027, 0.20760088901186347, 0.20760088901186347, 0.33670987906855226], 
reward next is 0.6633, 
noisyNet noise sample is [array([-0.06639925], dtype=float32), 0.024229387]. 
=============================================
[2019-03-26 02:39:03,689] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:03,696] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6418
[2019-03-26 02:39:03,703] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.4, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8455472710640556, 6.9112, 6.9112, 168.912956510431, 702040.278220611, 702040.278220611, 211313.2500498309], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6282000.0000, 
sim time next is 6282600.0000, 
raw observation next is [30.25, 64.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8460755783924613, 6.911199999999998, 6.9112, 168.912956510431, 702476.4107185948, 702476.4107185961, 211428.3800239218], 
processed observation next is [0.0, 0.7391304347826086, 0.6327014218009479, 0.64, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8122872907225136, -1.7763568394002506e-16, 0.0, 0.8294399451523027, 0.19513233631072077, 0.19513233631072113, 0.31556474630436093], 
reward next is 0.6844, 
noisyNet noise sample is [array([1.0484476], dtype=float32), -0.088568285]. 
=============================================
[2019-03-26 02:39:12,791] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:12,800] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1189
[2019-03-26 02:39:12,810] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1408902.594044565 W.
[2019-03-26 02:39:12,816] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 84.0, 1.0, 2.0, 0.5039704332487548, 1.0, 2.0, 0.5039704332487548, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1408902.594044565, 1408902.594044565, 301486.968969441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6685200.0000, 
sim time next is 6685800.0000, 
raw observation next is [27.36666666666667, 83.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.383809647115427, 6.9112, 168.9107091930355, 1789265.654280927, 1453984.565484831, 311349.2539683933], 
processed observation next is [1.0, 0.391304347826087, 0.49605055292259104, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04726096471154273, 0.0, 0.8294289097950645, 0.4970182373002575, 0.4038846015235642, 0.46470037905730344], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0334817], dtype=float32), 0.109165]. 
=============================================
[2019-03-26 02:39:19,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:19,584] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8081
[2019-03-26 02:39:19,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.4, 82.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6237036866049371, 6.9112, 6.9112, 168.912956510431, 541509.2451905177, 541509.2451905177, 169787.4739629782], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6846000.0000, 
sim time next is 6846600.0000, 
raw observation next is [23.5, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6256410836388676, 6.911199999999999, 6.9112, 168.912956510431, 542888.3395401764, 542888.339540177, 170099.6038367427], 
processed observation next is [0.0, 0.21739130434782608, 0.31279620853080575, 0.8233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5434647361449605, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15080231653893791, 0.15080231653893805, 0.2538800057264816], 
reward next is 0.7461, 
noisyNet noise sample is [array([-0.52442414], dtype=float32), 1.6739941]. 
=============================================
[2019-03-26 02:39:35,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:35,832] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3088
[2019-03-26 02:39:35,839] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.6, 39.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5090486092353919, 6.9112, 6.9112, 168.912956510431, 449936.831319606, 449936.831319606, 153170.034363171], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6880800.0000, 
sim time next is 6881400.0000, 
raw observation next is [29.55, 40.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5209175459165685, 6.911200000000001, 6.9112, 168.912956510431, 459727.3100937979, 459727.3100937972, 154725.2865187401], 
processed observation next is [0.0, 0.6521739130434783, 0.5995260663507109, 0.4, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.415753104776303, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12770203058161053, 0.12770203058161034, 0.23093326346080612], 
reward next is 0.7691, 
noisyNet noise sample is [array([0.7048763], dtype=float32), -0.36975592]. 
=============================================
[2019-03-26 02:39:42,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:39:42,120] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0923
[2019-03-26 02:39:42,127] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.01666666666667, 83.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.610749015622026, 6.911200000000001, 6.9112, 168.912956510431, 531489.2801868112, 531489.2801868106, 167735.8493590766], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6843000.0000, 
sim time next is 6843600.0000, 
raw observation next is [23.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.611539835283763, 6.911199999999999, 6.9112, 168.912956510431, 532022.6739180152, 532022.6739180159, 167861.2661056527], 
processed observation next is [0.0, 0.21739130434782608, 0.28909952606635075, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.526268091809467, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14778407608833755, 0.14778407608833774, 0.25053920314276523], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.42494693], dtype=float32), -0.35956857]. 
=============================================
[2019-03-26 02:39:47,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7971392e-02 1.6069679e-38 0.0000000e+00 9.4202858e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 02:39:47,267] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2253
[2019-03-26 02:39:47,275] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.1, 51.0, 1.0, 2.0, 0.7023641242740891, 1.0, 2.0, 0.7023641242740891, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1971030.270504059, 1971030.270504059, 376006.9783437019], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7041600.0000, 
sim time next is 7042200.0000, 
raw observation next is [31.2, 50.33333333333334, 1.0, 2.0, 0.712292386555526, 1.0, 2.0, 0.712292386555526, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2006233.838853647, 2006233.838853647, 381243.7818816257], 
processed observation next is [1.0, 0.5217391304347826, 0.6777251184834123, 0.5033333333333334, 1.0, 1.0, 0.6533643211512361, 1.0, 1.0, 0.6533643211512361, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5572871774593464, 0.5572871774593464, 0.5690205699725757], 
reward next is 0.4310, 
noisyNet noise sample is [array([-1.127059], dtype=float32), 0.035803154]. 
=============================================
[2019-03-26 02:39:51,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.2316464e-02 1.2099040e-27 1.5677693e-28 9.4768351e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 02:39:51,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5608
[2019-03-26 02:39:51,630] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.25, 72.0, 1.0, 2.0, 0.3850654091363155, 1.0, 2.0, 0.3850654091363155, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1085203.787732051, 1085203.787732051, 269527.0937305139], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7025400.0000, 
sim time next is 7026000.0000, 
raw observation next is [27.4, 71.0, 1.0, 2.0, 0.35860905273079, 1.0, 2.0, 0.35860905273079, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1012011.846706819, 1012011.846706819, 263423.9317171268], 
processed observation next is [1.0, 0.30434782608695654, 0.4976303317535545, 0.71, 1.0, 1.0, 0.2272398225672169, 1.0, 1.0, 0.2272398225672169, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2811144018630053, 0.2811144018630053, 0.39317004733899524], 
reward next is 0.6068, 
noisyNet noise sample is [array([1.7360417], dtype=float32), -0.6653751]. 
=============================================
[2019-03-26 02:39:51,644] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.93159 ]
 [69.02907 ]
 [68.49298 ]
 [72.660995]
 [75.0854  ]], R is [[71.77409363]
 [71.65406799]
 [71.53496552]
 [71.41077423]
 [70.69667053]].
[2019-03-26 02:39:58,292] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 02:39:58,296] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:39:58,297] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:39:58,298] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:39:58,300] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:39:58,301] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:39:58,303] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:39:58,299] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:39:58,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:39:58,308] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:39:58,311] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:39:58,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-26 02:39:58,369] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-26 02:39:58,396] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-26 02:39:58,398] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-26 02:39:58,398] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-26 02:40:00,188] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1964145], dtype=float32), 0.05267737]
[2019-03-26 02:40:00,188] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.24643392333333, 83.84049092000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7713221713167017, 6.9112, 6.9112, 168.912956510431, 654906.6440610585, 654906.6440610585, 196174.7403003852]
[2019-03-26 02:40:00,189] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:40:00,193] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8098817776254543
[2019-03-26 02:40:47,709] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1964145], dtype=float32), 0.05267737]
[2019-03-26 02:40:47,710] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.78887401, 99.27478302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6641582893250443, 6.9112, 6.9112, 168.912956510431, 569927.3963517692, 569927.3963517692, 176477.4043201045]
[2019-03-26 02:40:47,710] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:40:47,715] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2504207501946376
[2019-03-26 02:41:26,869] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1964145], dtype=float32), 0.05267737]
[2019-03-26 02:41:26,870] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.98333333333333, 51.0, 1.0, 2.0, 0.7533693952902121, 1.0, 1.0, 0.7533693952902121, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2106827.745965692, 2106827.745965692, 397530.16980826]
[2019-03-26 02:41:26,871] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:41:26,874] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8222511e-08 1.0833279e-27 7.7145930e-30 1.0000000e+00 9.0190274e-31], sampled 0.7857665107538852
[2019-03-26 02:41:46,386] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1964145], dtype=float32), 0.05267737]
[2019-03-26 02:41:46,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.1530791, 52.8640176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.861535622984479, 6.9112, 6.9112, 168.912956510431, 744013.6456476514, 744013.6456476514, 215202.0416245988]
[2019-03-26 02:41:46,389] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:41:46,394] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6286951968198667
[2019-03-26 02:42:07,836] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7427.3858 3165392738.0476 1412.0000
[2019-03-26 02:42:07,981] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7616.1406 3102309059.5164 1401.0000
[2019-03-26 02:42:08,071] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8104.0177 2983492103.3951 984.0000
[2019-03-26 02:42:08,244] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7550.7085 3300885162.4802 1342.0000
[2019-03-26 02:42:08,442] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8216.5750 2938881946.8847 956.0000
[2019-03-26 02:42:09,461] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2100000, evaluation results [2100000.0, 7550.70851582539, 3300885162.4802437, 1342.0, 7616.140638181008, 3102309059.51636, 1401.0, 8216.574966109192, 2938881946.8846793, 956.0, 7427.385778418494, 3165392738.04763, 1412.0, 8104.017716040067, 2983492103.3950553, 984.0]
[2019-03-26 02:42:10,382] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 2.2659273e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:10,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4480
[2019-03-26 02:42:10,401] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.56666666666667, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8006568106823921, 6.9112, 6.9112, 168.912956510431, 672256.6350135135, 672256.6350135135, 201994.4002602639], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7078200.0000, 
sim time next is 7078800.0000, 
raw observation next is [25.53333333333333, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8020238344604571, 6.911200000000001, 6.9112, 168.912956510431, 673518.5485279667, 673518.548527966, 202278.9647772838], 
processed observation next is [1.0, 0.9565217391304348, 0.4091627172195892, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7585656517810453, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18708848570221298, 0.1870884857022128, 0.3019089026526624], 
reward next is 0.6981, 
noisyNet noise sample is [array([-1.4030659], dtype=float32), -0.48396537]. 
=============================================
[2019-03-26 02:42:14,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.7103743e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:14,706] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5530
[2019-03-26 02:42:14,716] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1009843.023150352 W.
[2019-03-26 02:42:14,727] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 93.0, 1.0, 2.0, 0.3225250763315905, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5787482406291322, 6.9112, 6.9112, 168.912956510431, 1009843.023150352, 1009843.023150352, 238965.9448933253], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7384800.0000, 
sim time next is 7385400.0000, 
raw observation next is [21.25, 93.0, 1.0, 2.0, 0.6067188883620378, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954801.1727154275, 954801.1727154268, 214541.8749702706], 
processed observation next is [1.0, 0.4782608695652174, 0.20616113744075834, 0.93, 1.0, 1.0, 0.5261673353759492, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2652225479765076, 0.26522254797650746, 0.3202117536869711], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0359144], dtype=float32), -0.53561896]. 
=============================================
[2019-03-26 02:42:17,214] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.9624001e-17 2.8628912e-34 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:17,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.0178929e-08 4.9209552e-36 3.9059583e-37 9.9999988e-01 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:17,227] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3664
[2019-03-26 02:42:17,234] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4995
[2019-03-26 02:42:17,235] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.86666666666667, 84.0, 1.0, 2.0, 0.6762944865245327, 1.0, 2.0, 0.6762944865245327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1891077.744341633, 1891077.744341633, 364147.8125890098], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7203000.0000, 
sim time next is 7203600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.6844274594791306, 1.0, 2.0, 0.6844274594791306, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1913839.752616839, 1913839.752616838, 367536.5019972873], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.84, 1.0, 1.0, 0.6197921198543742, 1.0, 1.0, 0.6197921198543742, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5316221535046775, 0.5316221535046772, 0.5485619432795333], 
reward next is 0.4514, 
noisyNet noise sample is [array([-1.807191], dtype=float32), -0.8444113]. 
=============================================
[2019-03-26 02:42:17,241] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.75, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7888740523833199, 6.9112, 6.9112, 168.912956510431, 662441.5942000849, 662441.5942000849, 199584.9348450296], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7173000.0000, 
sim time next is 7173600.0000, 
raw observation next is [25.76666666666667, 86.0, 1.0, 1.0, 0.2377871357475558, 1.0, 1.0, 0.2377871357475558, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 664528.4758381078, 664528.4758381078, 238984.2053888025], 
processed observation next is [1.0, 0.0, 0.42022116903633505, 0.86, 1.0, 0.5, 0.08167124788862144, 1.0, 0.5, 0.08167124788862144, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18459124328836327, 0.18459124328836327, 0.35669284386388433], 
reward next is 0.6433, 
noisyNet noise sample is [array([-0.3938175], dtype=float32), -1.3699695]. 
=============================================
[2019-03-26 02:42:18,412] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:42:18,426] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9487
[2019-03-26 02:42:18,432] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.1, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7891326668832369, 6.911199999999999, 6.9112, 168.912956510431, 663084.7484409822, 663084.7484409829, 199646.1533555184], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7156800.0000, 
sim time next is 7157400.0000, 
raw observation next is [26.08333333333334, 83.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7899544020046693, 6.911199999999999, 6.9112, 168.912956510431, 663868.1781886118, 663868.1781886123, 199815.0574957939], 
processed observation next is [1.0, 0.8695652173913043, 0.43522906793049004, 0.8316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7438468317130111, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18440782727461438, 0.18440782727461455, 0.2982314290981999], 
reward next is 0.7018, 
noisyNet noise sample is [array([1.2240026], dtype=float32), 0.24639544]. 
=============================================
[2019-03-26 02:42:26,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:42:26,494] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8278
[2019-03-26 02:42:26,501] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.3, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6852341824369153, 6.9112, 6.9112, 168.912956510431, 586920.5135751214, 586920.5135751214, 180112.0559486472], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7527600.0000, 
sim time next is 7528200.0000, 
raw observation next is [23.28333333333333, 91.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.682335146882981, 6.9112, 6.9112, 168.912956510431, 584765.3535057231, 584765.3535057231, 179605.1288565265], 
processed observation next is [0.0, 0.13043478260869565, 0.3025276461295418, 0.9183333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.612603837662172, 0.0, 0.0, 0.8294399451523027, 0.1624348204182564, 0.1624348204182564, 0.26806735650227836], 
reward next is 0.7319, 
noisyNet noise sample is [array([1.1080513], dtype=float32), 0.71183294]. 
=============================================
[2019-03-26 02:42:39,518] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0355986e-11 4.3840520e-16 2.2337526e-25 1.0000000e+00 1.3250155e-25], sum to 1.0000
[2019-03-26 02:42:39,528] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2987
[2019-03-26 02:42:39,537] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.63333333333333, 65.33333333333334, 1.0, 2.0, 0.8680673081688052, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.984091660266975, 6.9112, 168.9125233322559, 2110340.514724452, 2058628.794809048, 426146.9495225764], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7834800.0000, 
sim time next is 7835400.0000, 
raw observation next is [30.45, 66.5, 1.0, 2.0, 0.7307577827319359, 1.0, 1.0, 0.7307577827319359, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2043515.116488057, 2043515.116488057, 387566.349540904], 
processed observation next is [1.0, 0.6956521739130435, 0.6421800947867299, 0.665, 1.0, 1.0, 0.6756117864240192, 1.0, 0.5, 0.6756117864240192, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5676430879133492, 0.5676430879133492, 0.5784572381207522], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.47489044], dtype=float32), -1.1400003]. 
=============================================
[2019-03-26 02:42:44,792] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:42:44,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5066
[2019-03-26 02:42:44,811] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.76666666666667, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8149554366325361, 6.9112, 6.9112, 168.912956510431, 681213.063239944, 681213.063239944, 204899.8356812124], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7681200.0000, 
sim time next is 7681800.0000, 
raw observation next is [25.73333333333333, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8145384811522504, 6.9112, 6.9112, 168.912956510431, 681175.3280614462, 681175.3280614462, 204820.0679061235], 
processed observation next is [1.0, 0.9130434782608695, 0.41864139020537117, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7738274160393296, 0.0, 0.0, 0.8294399451523027, 0.18921536890595728, 0.18921536890595728, 0.30570159388973656], 
reward next is 0.6943, 
noisyNet noise sample is [array([0.4328951], dtype=float32), -1.8511316]. 
=============================================
[2019-03-26 02:42:47,605] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 6.5318896e-25 3.5390157e-35 2.6295264e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:47,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6680
[2019-03-26 02:42:47,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2301200.910049324 W.
[2019-03-26 02:42:47,627] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 64.66666666666667, 1.0, 2.0, 0.5485411957535119, 1.0, 2.0, 0.5485411957535119, 1.0, 1.0, 0.9489562282798981, 6.9112, 6.9112, 170.5573041426782, 2301200.910049324, 2301200.910049324, 449600.4964428329], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7729800.0000, 
sim time next is 7730400.0000, 
raw observation next is [31.23333333333334, 64.33333333333334, 1.0, 2.0, 0.8057815507299966, 1.0, 2.0, 0.8057815507299966, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2253528.593646677, 2253528.593646676, 422692.8577996784], 
processed observation next is [1.0, 0.4782608695652174, 0.6793048973143764, 0.6433333333333334, 1.0, 1.0, 0.7660018683493934, 1.0, 1.0, 0.7660018683493934, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6259801649018548, 0.6259801649018544, 0.6308848623875797], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1431249], dtype=float32), -0.963716]. 
=============================================
[2019-03-26 02:42:49,166] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:49,167] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:49,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-26 02:42:49,370] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:49,371] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:49,450] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-26 02:42:50,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:50,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:50,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-26 02:42:50,891] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2115387: loss 5.8242
[2019-03-26 02:42:50,902] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2115390: learning rate 0.0010
[2019-03-26 02:42:51,139] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2115488: loss 5.8329
[2019-03-26 02:42:51,140] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2115488: learning rate 0.0010
[2019-03-26 02:42:52,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:52,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:52,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-26 02:42:52,601] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2116089: loss 9.3337
[2019-03-26 02:42:52,602] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2116089: learning rate 0.0010
[2019-03-26 02:42:54,120] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2116733: loss 41.5076
[2019-03-26 02:42:54,122] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2116733: learning rate 0.0010
[2019-03-26 02:42:54,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:54,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:54,344] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-26 02:42:54,886] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:54,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:54,968] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-26 02:42:55,864] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2117492: loss 26.7383
[2019-03-26 02:42:55,865] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2117493: learning rate 0.0010
[2019-03-26 02:42:56,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:42:56,416] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:42:56,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-26 02:42:56,655] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2117780: loss 24.8454
[2019-03-26 02:42:56,658] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2117780: learning rate 0.0010
[2019-03-26 02:42:58,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8379523e-09 9.7536170e-36 2.0661871e-36 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 02:42:58,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2638
[2019-03-26 02:42:58,047] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.6, 72.0, 1.0, 2.0, 0.4271228710208353, 1.0, 2.0, 0.4271228710208353, 1.0, 1.0, 0.7351120637067972, 6.9112, 6.9112, 170.5573041426782, 1791420.102347434, 1791420.102347434, 367403.5838236914], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7902000.0000, 
sim time next is 7902600.0000, 
raw observation next is [29.63333333333333, 72.0, 1.0, 2.0, 0.6789294177278722, 1.0, 2.0, 0.6789294177278722, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1898452.156565024, 1898452.156565024, 365233.0473186241], 
processed observation next is [1.0, 0.4782608695652174, 0.6034755134281199, 0.72, 1.0, 1.0, 0.613167973166111, 1.0, 1.0, 0.613167973166111, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5273478212680622, 0.5273478212680622, 0.545123951221827], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5927216], dtype=float32), 0.92275184]. 
=============================================
[2019-03-26 02:42:58,247] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2118446: loss 17.7549
[2019-03-26 02:42:58,249] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2118446: learning rate 0.0010
[2019-03-26 02:43:02,352] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:02,353] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:02,450] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-26 02:43:02,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:02,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:03,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-26 02:43:03,993] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2120687: loss 10.5830
[2019-03-26 02:43:03,997] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2120687: learning rate 0.0010
[2019-03-26 02:43:04,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2120994: loss 5.1656
[2019-03-26 02:43:04,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2120996: learning rate 0.0010
[2019-03-26 02:43:07,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:07,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:07,839] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-26 02:43:08,965] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2122614: loss 0.0024
[2019-03-26 02:43:08,969] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2122615: learning rate 0.0010
[2019-03-26 02:43:09,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:09,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:09,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-26 02:43:09,480] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2122799: loss 0.0206
[2019-03-26 02:43:09,486] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2122801: learning rate 0.0010
[2019-03-26 02:43:09,587] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2122845: loss 8.0407
[2019-03-26 02:43:09,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2122845: learning rate 0.0010
[2019-03-26 02:43:10,566] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.5186546e-38], sum to 1.0000
[2019-03-26 02:43:10,574] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2391
[2019-03-26 02:43:10,581] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.41666666666666, 92.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.905916588606538, 6.9112, 6.9112, 168.912956510431, 743780.161061396, 743780.161061396, 224649.0964791683], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7948200.0000, 
sim time next is 7948800.0000, 
raw observation next is [26.4, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9066508780258289, 6.9112, 6.9112, 168.912956510431, 744056.553743155, 744056.553743155, 224807.1488025224], 
processed observation next is [1.0, 0.0, 0.45023696682464454, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8861596073485718, 0.0, 0.0, 0.8294399451523027, 0.20668237603976528, 0.20668237603976528, 0.3355330579142125], 
reward next is 0.6645, 
noisyNet noise sample is [array([1.2943856], dtype=float32), -0.27785957]. 
=============================================
[2019-03-26 02:43:10,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:10,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:10,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-26 02:43:10,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:10,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:10,935] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2123413: loss 0.0213
[2019-03-26 02:43:10,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2123414: learning rate 0.0010
[2019-03-26 02:43:10,973] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2123428: loss 5.8366
[2019-03-26 02:43:10,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2123428: learning rate 0.0010
[2019-03-26 02:43:10,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-26 02:43:11,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:11,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:11,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-26 02:43:11,888] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2123859: loss 0.0027
[2019-03-26 02:43:11,891] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2123859: learning rate 0.0010
[2019-03-26 02:43:12,181] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2124011: loss 2.2756
[2019-03-26 02:43:12,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2124011: learning rate 0.0010
[2019-03-26 02:43:12,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:12,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:12,425] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-26 02:43:12,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 02:43:12,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:12,544] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2124171: loss 10.3637
[2019-03-26 02:43:12,546] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2124171: learning rate 0.0010
[2019-03-26 02:43:12,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-26 02:43:12,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.59993 ]
 [71.58943 ]
 [71.558075]], R is [[0.66446694]
 [1.32252511]
 [1.97461658]].
[2019-03-26 02:43:13,176] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2124480: loss 2.2769
[2019-03-26 02:43:13,177] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2124480: learning rate 0.0010
[2019-03-26 02:43:13,318] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:43:13,324] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0577
[2019-03-26 02:43:13,329] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.51666666666667, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6428876145712171, 6.9112, 6.9112, 168.912956510431, 562941.8508880638, 562941.8508880638, 172778.3825773168], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [22.73333333333333, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9758863872692861, 6.911199999999999, 6.9112, 168.912956510431, 854020.5062737424, 854020.5062737431, 242222.5726084513], 
processed observation next is [1.0, 0.34782608695652173, 0.27646129541864134, 0.82, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9705931552064464, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2372279184093729, 0.2372279184093731, 0.3615262277738079], 
reward next is 0.6385, 
noisyNet noise sample is [array([0.35561195], dtype=float32), -1.4185469]. 
=============================================
[2019-03-26 02:43:13,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.29514 ]
 [63.54838 ]
 [63.190662]
 [63.252815]
 [63.196644]], R is [[60.61688995]
 [60.75284576]
 [60.89883804]
 [61.03862381]
 [61.18221664]].
[2019-03-26 02:43:13,646] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2124715: loss 0.0211
[2019-03-26 02:43:13,648] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2124715: learning rate 0.0010
[2019-03-26 02:43:14,155] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2124903: loss 2.3094
[2019-03-26 02:43:14,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2124903: learning rate 0.0010
[2019-03-26 02:43:14,306] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2124958: loss 1.4857
[2019-03-26 02:43:14,310] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2124959: learning rate 0.0010
[2019-03-26 02:43:14,356] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2124978: loss 0.0052
[2019-03-26 02:43:14,361] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2124979: learning rate 0.0010
[2019-03-26 02:43:14,417] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 02:43:14,419] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:43:14,420] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:14,421] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:43:14,422] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:14,424] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:43:14,425] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:14,426] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:43:14,426] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:43:14,427] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:14,429] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:43:14,457] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-26 02:43:14,486] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-26 02:43:14,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-26 02:43:14,531] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-26 02:43:14,562] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-26 02:43:55,532] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:43:55,533] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.93333333333333, 83.33333333333334, 1.0, 2.0, 0.4655936429567699, 1.0, 2.0, 0.4655936429567699, 1.0, 2.0, 0.8046125608389112, 6.911200000000001, 6.9112, 170.5573041426782, 1952919.583429263, 1952919.583429263, 391507.8139036353]
[2019-03-26 02:43:55,533] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:43:55,537] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.4749498e-01 3.0885567e-21 1.7288218e-26 1.4646463e-14 5.2505020e-02], sampled 0.41680152370981294
[2019-03-26 02:43:55,538] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1952919.583429263 W.
[2019-03-26 02:44:00,445] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:44:00,448] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6197591954724252, 6.9112, 6.9112, 168.912956510431, 537651.3392306102, 537651.3392306102, 169170.2589030449]
[2019-03-26 02:44:00,451] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:44:00,455] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2365662026336962
[2019-03-26 02:44:09,530] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:44:09,534] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.680782115, 68.81510189000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.010841655544662, 6.911200000000001, 6.9112, 168.912847887126, 816046.3071734676, 816046.3071734669, 249930.9624291796]
[2019-03-26 02:44:09,536] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:44:09,537] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.869427464151794
[2019-03-26 02:44:12,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:44:12,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.5, 75.0, 1.0, 2.0, 0.1972973456223985, 1.0, 2.0, 0.1972973456223985, 1.0, 2.0, 0.3426402421168727, 6.9112, 6.9112, 178.6582176852504, 827109.587695087, 827109.587695087, 271107.4143199729]
[2019-03-26 02:44:12,018] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:44:12,020] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.020798e-01 1.267309e-38 0.000000e+00 0.000000e+00 5.979202e-01], sampled 0.062168638824297795
[2019-03-26 02:44:41,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:44:41,150] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.57837455666667, 91.77511308333334, 1.0, 2.0, 0.5376791622180781, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9337708915567109, 6.9112, 6.9112, 168.912945048395, 1503215.249164363, 1503215.249164363, 329370.8910450641]
[2019-03-26 02:44:41,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:44:41,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 1.9025958e-37 0.0000000e+00 0.0000000e+00 5.0821206e-15], sampled 0.5601611742803939
[2019-03-26 02:44:41,155] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1503215.249164363 W.
[2019-03-26 02:45:06,035] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-1.0966544], dtype=float32), 0.14558241]
[2019-03-26 02:45:06,037] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.25, 86.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9174453454842904, 6.9112, 6.9112, 168.912956510431, 754419.6204531722, 754419.6204531722, 227408.4218927747]
[2019-03-26 02:45:06,039] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:45:06,042] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9579712055100638
[2019-03-26 02:45:24,014] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7542.6030 3120036930.1226 1585.0000
[2019-03-26 02:45:24,223] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8078.8800 2992507391.4628 1073.0000
[2019-03-26 02:45:24,305] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7372.8365 3186139143.5569 1548.0000
[2019-03-26 02:45:24,383] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8142.4754 2949120948.1099 1110.0000
[2019-03-26 02:45:24,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7475.3660 3318652486.4960 1513.0000
[2019-03-26 02:45:25,423] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2125000, evaluation results [2125000.0, 7475.365979151781, 3318652486.4960113, 1513.0, 7542.603001964852, 3120036930.1225934, 1585.0, 8142.475391637691, 2949120948.109908, 1110.0, 7372.836504022302, 3186139143.5569468, 1548.0, 8078.880004688094, 2992507391.462849, 1073.0]
[2019-03-26 02:45:26,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:45:26,833] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3400
[2019-03-26 02:45:26,838] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.6, 84.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4781835247019263, 6.911200000000001, 6.9112, 168.912956510431, 428018.0119751459, 428018.0119751453, 149082.4711382512], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 439800.0000, 
sim time next is 440400.0000, 
raw observation next is [19.6, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.444091283895806, 6.9112, 6.9112, 168.912956510431, 397546.711401831, 397546.711401831, 145210.6079233933], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.8466666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.32206254133634876, 0.0, 0.0, 0.8294399451523027, 0.11042964205606418, 0.11042964205606418, 0.21673225063193033], 
reward next is 0.7833, 
noisyNet noise sample is [array([0.7850186], dtype=float32), -0.4069161]. 
=============================================
[2019-03-26 02:45:27,410] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2125732: loss 0.0085
[2019-03-26 02:45:27,411] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2125733: learning rate 0.0010
[2019-03-26 02:45:34,421] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2128260: loss 0.0046
[2019-03-26 02:45:34,425] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2128262: learning rate 0.0010
[2019-03-26 02:45:35,159] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2128535: loss 0.0052
[2019-03-26 02:45:35,163] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2128535: learning rate 0.0010
[2019-03-26 02:45:36,101] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9828293e-11 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 02:45:36,112] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6618
[2019-03-26 02:45:36,118] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.2, 81.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 470588.4206839675, 470588.4206839668, 225792.2247449822], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 324000.0000, 
sim time next is 324600.0000, 
raw observation next is [22.11666666666667, 81.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 469318.3960460681, 469318.3960460681, 225558.7004260933], 
processed observation next is [0.0, 0.782608695652174, 0.24723538704581383, 0.815, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.1303662211239078, 0.1303662211239078, 0.33665477675536315], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0239502], dtype=float32), -0.8629207]. 
=============================================
[2019-03-26 02:45:39,470] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2130124: loss 0.7998
[2019-03-26 02:45:39,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2130125: learning rate 0.0010
[2019-03-26 02:45:39,755] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2130229: loss 0.5073
[2019-03-26 02:45:39,765] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2130230: learning rate 0.0010
[2019-03-26 02:45:40,216] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.0779968e-28], sum to 1.0000
[2019-03-26 02:45:40,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9416
[2019-03-26 02:45:40,232] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.75, 92.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5486437934608952, 6.9112, 6.9112, 168.912956510431, 482675.1279515905, 482675.1279515905, 158497.7582798681], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 210600.0000, 
sim time next is 211200.0000, 
raw observation next is [20.76666666666667, 92.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.547414837579121, 6.9112, 6.9112, 168.912956510431, 481606.5066932103, 481606.5066932103, 158328.0242059373], 
processed observation next is [0.0, 0.43478260869565216, 0.18325434439178534, 0.9233333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.448066875096489, 0.0, 0.0, 0.8294399451523027, 0.13377958519255842, 0.13377958519255842, 0.23631048388945866], 
reward next is 0.7637, 
noisyNet noise sample is [array([-1.0444722], dtype=float32), -0.8345664]. 
=============================================
[2019-03-26 02:45:40,676] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2130565: loss 0.0474
[2019-03-26 02:45:40,680] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2130565: learning rate 0.0010
[2019-03-26 02:45:41,907] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2131024: loss 0.4442
[2019-03-26 02:45:41,908] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2131025: learning rate 0.0010
[2019-03-26 02:45:42,751] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2131340: loss 0.0711
[2019-03-26 02:45:42,753] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2131340: learning rate 0.0010
[2019-03-26 02:45:43,613] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2131652: loss 0.1623
[2019-03-26 02:45:43,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2131652: learning rate 0.0010
[2019-03-26 02:45:44,717] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2132060: loss 0.0013
[2019-03-26 02:45:44,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2132060: learning rate 0.0010
[2019-03-26 02:45:45,193] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2132235: loss 0.0029
[2019-03-26 02:45:45,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2132236: learning rate 0.0010
[2019-03-26 02:45:46,078] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2132559: loss 0.0014
[2019-03-26 02:45:46,085] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2132559: learning rate 0.0010
[2019-03-26 02:45:46,149] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2132588: loss 0.3622
[2019-03-26 02:45:46,154] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2132589: learning rate 0.0010
[2019-03-26 02:45:46,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.5644151e-33], sum to 1.0000
[2019-03-26 02:45:46,846] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0279
[2019-03-26 02:45:46,855] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.56666666666667, 76.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.447870054624815, 6.9112, 6.9112, 168.912956510431, 401142.3041803128, 401142.3041803128, 145608.872584677], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 678000.0000, 
sim time next is 678600.0000, 
raw observation next is [20.45, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4505314518753746, 6.911200000000001, 6.9112, 168.912956510431, 403544.4234412894, 403544.4234412888, 145900.93273195], 
processed observation next is [1.0, 0.8695652173913043, 0.16824644549763035, 0.775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.32991640472606654, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11209567317813594, 0.11209567317813578, 0.21776258616708954], 
reward next is 0.7822, 
noisyNet noise sample is [array([-1.0373389], dtype=float32), 1.6587086]. 
=============================================
[2019-03-26 02:45:46,873] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2132854: loss 0.1267
[2019-03-26 02:45:46,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2132855: learning rate 0.0010
[2019-03-26 02:45:47,265] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2132996: loss 0.0010
[2019-03-26 02:45:47,269] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2132996: learning rate 0.0010
[2019-03-26 02:45:47,404] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2133044: loss 0.0085
[2019-03-26 02:45:47,411] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2133045: learning rate 0.0010
[2019-03-26 02:45:48,303] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.7631735e-14], sum to 1.0000
[2019-03-26 02:45:48,312] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5110
[2019-03-26 02:45:48,320] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [19.0, 82.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4184209553018997, 6.911199999999999, 6.9112, 168.912956510431, 376717.4235899877, 376717.4235899884, 142331.9455336961], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 599400.0000, 
sim time next is 600000.0000, 
raw observation next is [18.86666666666667, 83.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4140527249716222, 6.9112, 6.9112, 168.912956510431, 372957.7030028827, 372957.7030028827, 141874.910118256], 
processed observation next is [1.0, 0.9565217391304348, 0.09320695102685649, 0.83, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2854301524044173, 0.0, 0.0, 0.8294399451523027, 0.1035993619452452, 0.1035993619452452, 0.21175359719142686], 
reward next is 0.7882, 
noisyNet noise sample is [array([0.6030724], dtype=float32), -0.48852932]. 
=============================================
[2019-03-26 02:45:48,348] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.68566]
 [80.69729]
 [80.83315]
 [80.97536]
 [80.82745]], R is [[80.75579071]
 [80.7358017 ]
 [80.71546173]
 [80.69512939]
 [80.67492676]].
[2019-03-26 02:45:49,242] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2133719: loss 1.2312
[2019-03-26 02:45:49,247] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2133719: learning rate 0.0010
[2019-03-26 02:45:50,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:45:50,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3477
[2019-03-26 02:45:50,845] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4504697291018697, 6.911200000000001, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770875, 145892.5188688431], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [21.16666666666667, 73.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5619969306624618, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 159906.3756388777], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4658499154420266, 0.0, 0.0, 0.8294399451523027, 0.13976901878634496, 0.13976901878634496, 0.2386662322968324], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.84671295], dtype=float32), -0.9061167]. 
=============================================
[2019-03-26 02:45:56,024] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2136234: loss 0.0431
[2019-03-26 02:45:56,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2136235: learning rate 0.0010
[2019-03-26 02:45:56,644] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:45:56,655] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7162
[2019-03-26 02:45:56,663] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [16.96666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3705435247216051, 6.9112, 6.9112, 168.912956510431, 335655.8152497928, 335655.8152497928, 137560.5876900406], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 620400.0000, 
sim time next is 621000.0000, 
raw observation next is [16.95, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3713659748597411, 6.911199999999999, 6.9112, 168.912956510431, 336434.2421623027, 336434.2421623032, 137629.3697902915], 
processed observation next is [1.0, 0.17391304347826086, 0.002369668246445531, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.23337314007285498, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.0934539561561952, 0.09345395615619533, 0.20541696983625596], 
reward next is 0.7946, 
noisyNet noise sample is [array([-2.5161088], dtype=float32), -1.5020087]. 
=============================================
[2019-03-26 02:45:56,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[81.11669 ]
 [81.12518 ]
 [81.197716]
 [81.10286 ]
 [81.140915]], R is [[81.01571655]
 [81.00024414]
 [80.98370361]
 [80.96838379]
 [80.95313263]].
[2019-03-26 02:45:57,094] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2136627: loss 0.0128
[2019-03-26 02:45:57,098] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2136629: learning rate 0.0010
[2019-03-26 02:46:01,104] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2138105: loss 0.0439
[2019-03-26 02:46:01,109] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2138107: learning rate 0.0010
[2019-03-26 02:46:01,431] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2138225: loss 0.0053
[2019-03-26 02:46:01,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2138225: learning rate 0.0010
[2019-03-26 02:46:02,235] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2138527: loss 1.0944
[2019-03-26 02:46:02,239] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2138528: learning rate 0.0010
[2019-03-26 02:46:03,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:46:03,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5557
[2019-03-26 02:46:03,130] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [17.53333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4012861846051065, 6.911200000000001, 6.9112, 168.912956510431, 362233.1496192599, 362233.1496192593, 140540.3484297487], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 708000.0000, 
sim time next is 708600.0000, 
raw observation next is [17.51666666666667, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3955899810124917, 6.9112, 6.9112, 168.912956510431, 357128.8032411602, 357128.8032411602, 139989.2957334586], 
processed observation next is [1.0, 0.17391304347826086, 0.029225908372827993, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.2629146109908435, 0.0, 0.0, 0.8294399451523027, 0.09920244534476672, 0.09920244534476672, 0.20893924736337105], 
reward next is 0.7911, 
noisyNet noise sample is [array([0.7417121], dtype=float32), -1.3579905]. 
=============================================
[2019-03-26 02:46:03,653] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2139046: loss 0.0378
[2019-03-26 02:46:03,656] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2139046: learning rate 0.0010
[2019-03-26 02:46:04,329] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2139293: loss 0.2459
[2019-03-26 02:46:04,330] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2139293: learning rate 0.0010
[2019-03-26 02:46:05,331] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2139666: loss 0.0008
[2019-03-26 02:46:05,333] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2139666: learning rate 0.0010
[2019-03-26 02:46:06,527] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2140103: loss 0.3179
[2019-03-26 02:46:06,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2140103: learning rate 0.0010
[2019-03-26 02:46:06,702] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2140166: loss 0.2751
[2019-03-26 02:46:06,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2140167: learning rate 0.0010
[2019-03-26 02:46:07,689] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2140540: loss 0.0291
[2019-03-26 02:46:07,693] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2140541: learning rate 0.0010
[2019-03-26 02:46:07,785] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2140569: loss 0.0231
[2019-03-26 02:46:07,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2140570: learning rate 0.0010
[2019-03-26 02:46:08,600] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2140870: loss 0.0016
[2019-03-26 02:46:08,603] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2140872: learning rate 0.0010
[2019-03-26 02:46:08,936] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2140989: loss 0.2397
[2019-03-26 02:46:08,938] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2140989: learning rate 0.0010
[2019-03-26 02:46:09,064] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2141035: loss 0.0074
[2019-03-26 02:46:09,069] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2141035: learning rate 0.0010
[2019-03-26 02:46:10,181] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.7036523e-16], sum to 1.0000
[2019-03-26 02:46:10,192] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5385
[2019-03-26 02:46:10,198] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.533367042309381, 6.9112, 6.9112, 168.912956510431, 469208.0941894515, 469208.0941894515, 156422.9415735155], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 900600.0000, 
sim time next is 901200.0000, 
raw observation next is [22.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5331688700776678, 6.9112, 6.9112, 168.912956510431, 469033.7216901088, 469033.7216901088, 156396.4293437205], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4306937439971558, 0.0, 0.0, 0.8294399451523027, 0.13028714491391913, 0.13028714491391913, 0.2334275064831649], 
reward next is 0.7666, 
noisyNet noise sample is [array([-0.6686872], dtype=float32), 0.76989585]. 
=============================================
[2019-03-26 02:46:10,942] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2141728: loss 0.0258
[2019-03-26 02:46:10,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2141730: learning rate 0.0010
[2019-03-26 02:46:11,617] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.045925e-36 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 02:46:11,633] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7391
[2019-03-26 02:46:11,638] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.8, 94.83333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2037200681933921, 6.9112, 6.9112, 170.5573041426782, 528216.0917288136, 528216.0917288136, 234546.5790386823], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [21.8, 95.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2045259098247234, 6.9112, 6.9112, 170.5573041426782, 530185.1834655844, 530185.1834655844, 234830.0587695297], 
processed observation next is [1.0, 0.043478260869565216, 0.23222748815165886, 0.95, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.02990964612771148, 0.0, 0.0, 0.8375144448122397, 0.14727366207377346, 0.14727366207377346, 0.35049262502914885], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20487295], dtype=float32), 0.23190099]. 
=============================================
[2019-03-26 02:46:11,660] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[60.73687 ]
 [60.11173 ]
 [59.469418]
 [59.135612]
 [59.561253]], R is [[60.43947983]
 [59.83508682]
 [59.2367363 ]
 [58.64437103]
 [58.05792618]].
[2019-03-26 02:46:12,417] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9695687e-14 1.5733049e-30 0.0000000e+00 2.3459022e-35 1.0000000e+00], sum to 1.0000
[2019-03-26 02:46:12,429] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7381
[2019-03-26 02:46:12,434] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 53.0, 1.0, 2.0, 0.2103778018974673, 1.0, 2.0, 0.2103778018974673, 1.0, 2.0, 0.3854359491576024, 6.911199999999999, 6.9112, 170.5573041426782, 1025740.190388081, 1025740.190388082, 286659.7132848473], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 657600.0000, 
sim time next is 658200.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.2111135700204603, 1.0, 2.0, 0.2111135700204603, 1.0, 2.0, 0.3867757160591049, 6.9112, 6.9112, 170.5573041426782, 1029284.14367817, 1029284.14367817, 286898.4430169841], 
processed observation next is [1.0, 0.6086956521739131, 0.3696682464454976, 0.53, 1.0, 1.0, 0.04953442171139794, 1.0, 1.0, 0.04953442171139794, 1.0, 1.0, 0.2521655073891523, 0.0, 0.0, 0.8375144448122397, 0.285912262132825, 0.285912262132825, 0.42820663136863296], 
reward next is 0.5718, 
noisyNet noise sample is [array([-0.91459644], dtype=float32), -0.12810206]. 
=============================================
[2019-03-26 02:46:17,737] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2144230: loss 0.0010
[2019-03-26 02:46:17,739] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2144230: learning rate 0.0010
[2019-03-26 02:46:17,825] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.3966206e-02 5.9420294e-31 1.3681937e-31 2.8816570e-29 9.4603378e-01], sum to 1.0000
[2019-03-26 02:46:17,834] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9991
[2019-03-26 02:46:17,842] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.33333333333333, 97.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2227004677135825, 6.911199999999999, 6.9112, 170.5573041426782, 572194.7838976802, 572194.7838976809, 241024.460322552], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1039200.0000, 
sim time next is 1039800.0000, 
raw observation next is [22.36666666666667, 97.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2231464938499655, 6.9112, 6.9112, 170.5573041426782, 573115.4450653072, 573115.4450653072, 241164.7400801029], 
processed observation next is [1.0, 0.0, 0.2590837282780413, 0.97, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.052617675426787185, 0.0, 0.0, 0.8375144448122397, 0.1591987347403631, 0.1591987347403631, 0.35994737325388493], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5865707], dtype=float32), -2.1817887]. 
=============================================
[2019-03-26 02:46:18,961] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2144675: loss 0.0013
[2019-03-26 02:46:18,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2144675: learning rate 0.0010
[2019-03-26 02:46:19,253] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:46:19,261] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2593
[2019-03-26 02:46:19,269] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.45, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5356096664678136, 6.9112, 6.9112, 168.912956510431, 471407.6856369834, 471407.6856369834, 156716.1153879083], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1060200.0000, 
sim time next is 1060800.0000, 
raw observation next is [20.5, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.538230740217674, 6.9112, 6.9112, 168.912956510431, 473492.4332114068, 473492.4332114068, 157076.8164104944], 
processed observation next is [1.0, 0.2608695652173913, 0.1706161137440759, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4368667563630171, 0.0, 0.0, 0.8294399451523027, 0.13152567589205744, 0.13152567589205744, 0.23444300956790207], 
reward next is 0.7656, 
noisyNet noise sample is [array([-0.80173826], dtype=float32), -0.6777295]. 
=============================================
[2019-03-26 02:46:22,639] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2146056: loss 4.5183
[2019-03-26 02:46:22,640] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2146056: learning rate 0.0010
[2019-03-26 02:46:23,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 1.4543197e-23 2.0142756e-31 7.3244147e-18 2.8067423e-11], sum to 1.0000
[2019-03-26 02:46:23,119] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6095
[2019-03-26 02:46:23,128] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 955318.5555562115 W.
[2019-03-26 02:46:23,132] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3137955946786588, 1.0, 1.0, 0.3137955946786588, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 955318.5555562115, 955318.5555562115, 262328.3015178528], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 988200.0000, 
sim time next is 988800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6537007959285636, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1002520.754180983, 1002520.754180983, 222198.4298617518], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5827720432874259, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2784779872724953, 0.2784779872724953, 0.3316394475548534], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7217051], dtype=float32), 0.69209534]. 
=============================================
[2019-03-26 02:46:23,226] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2146321: loss 3.0305
[2019-03-26 02:46:23,230] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2146323: learning rate 0.0010
[2019-03-26 02:46:23,479] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2146435: loss 0.0073
[2019-03-26 02:46:23,482] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2146435: learning rate 0.0010
[2019-03-26 02:46:24,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:46:24,719] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1393
[2019-03-26 02:46:24,726] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5335135178698118, 6.911199999999999, 6.9112, 168.912956510431, 469337.0086301719, 469337.0086301725, 156442.543440791], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 897000.0000, 
sim time next is 897600.0000, 
raw observation next is [22.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.532848901381859, 6.911199999999999, 6.9112, 168.912956510431, 468752.1887341646, 468752.1887341653, 156353.6447878174], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4303035382705598, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1302089413150457, 0.1302089413150459, 0.2333636489370409], 
reward next is 0.7666, 
noisyNet noise sample is [array([-1.2139965], dtype=float32), 1.0850999]. 
=============================================
[2019-03-26 02:46:25,212] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2147093: loss 1.5447
[2019-03-26 02:46:25,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2147093: learning rate 0.0010
[2019-03-26 02:46:25,722] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2147279: loss 0.0033
[2019-03-26 02:46:25,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2147279: learning rate 0.0010
[2019-03-26 02:46:26,693] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2147635: loss 6.7765
[2019-03-26 02:46:26,696] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2147635: learning rate 0.0010
[2019-03-26 02:46:27,944] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2148090: loss 0.0026
[2019-03-26 02:46:27,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2148090: learning rate 0.0010
[2019-03-26 02:46:28,210] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2148188: loss 0.0013
[2019-03-26 02:46:28,214] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2148188: learning rate 0.0010
[2019-03-26 02:46:29,097] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2148511: loss 0.0034
[2019-03-26 02:46:29,100] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2148511: learning rate 0.0010
[2019-03-26 02:46:29,374] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2148611: loss 2.7158
[2019-03-26 02:46:29,379] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2148612: learning rate 0.0010
[2019-03-26 02:46:29,654] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 1.216099e-15], sum to 1.0000
[2019-03-26 02:46:29,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8711
[2019-03-26 02:46:29,661] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.85, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6319850142813768, 6.911200000000001, 6.9112, 168.912956510431, 547949.5020356743, 547949.5020356737, 171120.4405658639], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1200600.0000, 
sim time next is 1201200.0000, 
raw observation next is [23.73333333333333, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6325144337161253, 6.911199999999999, 6.9112, 168.912956510431, 548524.3494125624, 548524.3494125631, 171204.0894456537], 
processed observation next is [1.0, 0.9130434782608695, 0.3238546603475513, 0.8066666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5518468703855186, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1523678748368229, 0.1523678748368231, 0.25552849170993086], 
reward next is 0.7445, 
noisyNet noise sample is [array([1.2557079], dtype=float32), -0.21301775]. 
=============================================
[2019-03-26 02:46:30,244] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2148925: loss 0.0055
[2019-03-26 02:46:30,246] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2148925: learning rate 0.0010
[2019-03-26 02:46:30,398] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2148983: loss 0.0421
[2019-03-26 02:46:30,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2148985: learning rate 0.0010
[2019-03-26 02:46:30,447] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2149000: loss 2.0651
[2019-03-26 02:46:30,450] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2149000: learning rate 0.0010
[2019-03-26 02:46:30,944] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 1.9350798e-30 1.1632933e-33 1.2454706e-14 6.0119166e-14], sum to 1.0000
[2019-03-26 02:46:30,958] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4879
[2019-03-26 02:46:30,967] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 903903.4264064509 W.
[2019-03-26 02:46:30,972] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.95, 93.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.924750350699819, 6.9112, 168.9126977721049, 903903.4264064509, 894290.3559780999, 256529.7582104096], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 984600.0000, 
sim time next is 985200.0000, 
raw observation next is [21.96666666666667, 93.66666666666667, 1.0, 1.0, 0.2958161179540738, 1.0, 1.0, 0.2958161179540738, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 899858.9527263594, 899858.9527263594, 258414.8089535659], 
processed observation next is [1.0, 0.391304347826087, 0.24012638230647723, 0.9366666666666668, 1.0, 0.5, 0.15158568428201663, 1.0, 0.5, 0.15158568428201663, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2499608202017665, 0.2499608202017665, 0.3856937447068148], 
reward next is 0.6143, 
noisyNet noise sample is [array([-0.10596961], dtype=float32), 0.51668483]. 
=============================================
[2019-03-26 02:46:31,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:46:31,205] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5962
[2019-03-26 02:46:31,213] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.05, 73.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5691759704565186, 6.9112, 6.9112, 168.912956510431, 497365.1924567967, 497365.1924567967, 161482.7508256612], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 924600.0000, 
sim time next is 925200.0000, 
raw observation next is [24.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5704203678157002, 6.9112, 6.9112, 168.912956510431, 498329.8766802831, 498329.8766802831, 161664.6028694742], 
processed observation next is [0.0, 0.7391304347826086, 0.3364928909952607, 0.74, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47612239977524407, 0.0, 0.0, 0.8294399451523027, 0.1384249657445231, 0.1384249657445231, 0.24129045204399135], 
reward next is 0.7587, 
noisyNet noise sample is [array([-0.28512746], dtype=float32), -1.1359596]. 
=============================================
[2019-03-26 02:46:31,937] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 02:46:31,949] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6036
[2019-03-26 02:46:31,953] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.8, 94.16666666666667, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.204063553073027, 6.911200000000001, 6.9112, 170.5573041426782, 529153.4456683717, 529153.445668371, 234673.72557286], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 951000.0000, 
sim time next is 951600.0000, 
raw observation next is [21.8, 94.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2033036373125497, 6.9112, 6.9112, 170.5573041426782, 527279.9787713811, 527279.9787713811, 234405.1866831638], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.9433333333333335, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.028419069893353285, 0.0, 0.0, 0.8375144448122397, 0.1464666607698281, 0.1464666607698281, 0.34985848758681165], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1917961], dtype=float32), 0.2158962]. 
=============================================
[2019-03-26 02:46:32,666] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2149818: loss 0.9043
[2019-03-26 02:46:32,669] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2149819: learning rate 0.0010
[2019-03-26 02:46:33,154] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 02:46:33,156] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:46:33,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:46:33,159] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:46:33,160] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:46:33,160] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:46:33,161] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:46:33,162] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:46:33,164] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:46:33,164] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:46:33,165] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:46:33,187] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-26 02:46:33,215] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-26 02:46:33,254] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-26 02:46:33,284] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-26 02:46:33,311] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-26 02:46:57,714] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.9318531], dtype=float32), 0.16296278]
[2019-03-26 02:46:57,715] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.95341154333333, 91.05955045166667, 1.0, 2.0, 0.2066613558148771, 1.0, 2.0, 0.2066613558148771, 1.0, 2.0, 0.3589024310957167, 6.911199999999999, 6.9112, 184.5923449428631, 866369.7722482336, 866369.7722482343, 275777.6860492713]
[2019-03-26 02:46:57,715] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:46:57,718] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4522121e-13 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.49334152947792154
[2019-03-26 02:47:22,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9318531], dtype=float32), 0.16296278]
[2019-03-26 02:47:22,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.0, 74.5, 1.0, 2.0, 0.3244467427662276, 1.0, 2.0, 0.3244467427662276, 1.0, 2.0, 0.5493555398915149, 6.9112, 6.9112, 178.6582176852504, 1360467.886006314, 1360467.886006314, 315472.5479379596]
[2019-03-26 02:47:22,300] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:47:22,302] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0803379e-11 5.5944233e-32 5.7928512e-33 1.0610751e-18 1.0000000e+00], sampled 0.12323465923184651
[2019-03-26 02:47:30,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9318531], dtype=float32), 0.16296278]
[2019-03-26 02:47:30,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.35, 49.5, 1.0, 2.0, 0.3661230789650773, 1.0, 2.0, 0.3661230789650773, 1.0, 2.0, 0.6297351543221381, 6.9112, 6.9112, 169.0403247858759, 1535404.029081031, 1535404.029081031, 333866.2918720508]
[2019-03-26 02:47:30,245] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:47:30,250] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1583144e-21 0.0000000e+00 0.0000000e+00 3.5736721e-38 1.0000000e+00], sampled 0.8351832529957773
[2019-03-26 02:47:34,825] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9318531], dtype=float32), 0.16296278]
[2019-03-26 02:47:34,826] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.53333333333334, 77.33333333333334, 1.0, 2.0, 0.1893190897776851, 1.0, 2.0, 0.1893190897776851, 1.0, 2.0, 0.3282197816870772, 6.911200000000001, 6.9112, 169.0403247858759, 793667.2939642507, 793667.29396425, 266650.9351339435]
[2019-03-26 02:47:34,827] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:47:34,831] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4530093e-08 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.9920174095548115
[2019-03-26 02:48:41,928] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7118.2699 3409221704.2957 280.0000
[2019-03-26 02:48:42,626] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6660.6120 3546250539.9120 762.0000
[2019-03-26 02:48:42,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7082.9370 3240303666.9075 309.0000
[2019-03-26 02:48:43,013] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7028.4259 3366420707.0223 469.0000
[2019-03-26 02:48:43,197] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 7136.5964 3225057949.1778 349.0000
[2019-03-26 02:48:44,213] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2150000, evaluation results [2150000.0, 6660.611953255384, 3546250539.9119654, 762.0, 7028.425908138296, 3366420707.0223494, 469.0, 7136.596387157578, 3225057949.1778197, 349.0, 7118.2698846366475, 3409221704.2957144, 280.0, 7082.936987645469, 3240303666.907514, 309.0]
[2019-03-26 02:48:50,296] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2577729e-24 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 02:48:50,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2199
[2019-03-26 02:48:50,312] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666667, 69.16666666666666, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2472349982087065, 6.911200000000001, 6.9112, 170.5573041426782, 624995.09352545, 624995.0935254494, 248788.4523632403], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1443000.0000, 
sim time next is 1443600.0000, 
raw observation next is [27.6, 69.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.245422902836648, 6.911200000000001, 6.9112, 170.5573041426782, 621124.5033786983, 621124.5033786976, 248227.7000435844], 
processed observation next is [0.0, 0.7391304347826086, 0.5071090047393366, 0.69, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.07978402784957073, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.17253458427186064, 0.17253458427186044, 0.37048910454266326], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3979821], dtype=float32), 0.8923889]. 
=============================================
[2019-03-26 02:48:50,457] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2152312: loss 7.2182
[2019-03-26 02:48:50,462] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2152312: learning rate 0.0010
[2019-03-26 02:48:51,504] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2152689: loss 1.1744
[2019-03-26 02:48:51,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2152689: learning rate 0.0010
[2019-03-26 02:48:55,180] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2154045: loss 0.1439
[2019-03-26 02:48:55,183] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2154045: learning rate 0.0010
[2019-03-26 02:48:55,844] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2154289: loss 0.0874
[2019-03-26 02:48:55,846] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2154289: learning rate 0.0010
[2019-03-26 02:48:56,477] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2154528: loss 3.1836
[2019-03-26 02:48:56,479] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2154529: learning rate 0.0010
[2019-03-26 02:48:57,885] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0218473e-34], sum to 1.0000
[2019-03-26 02:48:57,896] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7538
[2019-03-26 02:48:57,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 968182.8512264325 W.
[2019-03-26 02:48:57,916] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.65, 64.5, 1.0, 2.0, 0.2158451227473028, 1.0, 2.0, 0.2158451227473028, 1.0, 2.0, 0.3761833644241565, 6.911200000000001, 6.9112, 170.5573041426782, 968182.8512264325, 968182.8512264319, 281386.5119804448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1168200.0000, 
sim time next is 1168800.0000, 
raw observation next is [26.76666666666667, 64.0, 1.0, 2.0, 0.5442695674436145, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825012.9266890734, 825012.9266890734, 198703.0860916184], 
processed observation next is [1.0, 0.5217391304347826, 0.46761453396524505, 0.64, 1.0, 1.0, 0.45092718969110185, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2291702574136315, 0.2291702574136315, 0.2965717702859976], 
reward next is 0.7034, 
noisyNet noise sample is [array([-1.072042], dtype=float32), 1.4504377]. 
=============================================
[2019-03-26 02:48:57,984] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2155085: loss 0.0199
[2019-03-26 02:48:57,985] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2155085: learning rate 0.0010
[2019-03-26 02:48:58,472] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2155261: loss 2.2013
[2019-03-26 02:48:58,474] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2155261: learning rate 0.0010
[2019-03-26 02:48:59,562] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2155668: loss 0.0767
[2019-03-26 02:48:59,565] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2155668: learning rate 0.0010
[2019-03-26 02:49:00,614] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2156055: loss 21.8561
[2019-03-26 02:49:00,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2156055: learning rate 0.0010
[2019-03-26 02:49:00,969] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2156183: loss 7.8048
[2019-03-26 02:49:00,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2156184: learning rate 0.0010
[2019-03-26 02:49:01,705] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2156454: loss 6.2795
[2019-03-26 02:49:01,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2156454: learning rate 0.0010
[2019-03-26 02:49:01,915] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2156530: loss 0.1349
[2019-03-26 02:49:01,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2156530: learning rate 0.0010
[2019-03-26 02:49:02,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:02,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8913
[2019-03-26 02:49:02,842] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.6, 55.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6420785678182614, 6.911199999999999, 6.9112, 168.912956510431, 553951.0811976786, 553951.0811976792, 172785.4693406732], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1524000.0000, 
sim time next is 1524600.0000, 
raw observation next is [28.45, 55.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6421099761948695, 6.9112, 6.9112, 168.912956510431, 554235.9988482147, 554235.9988482147, 172788.706681928], 
processed observation next is [0.0, 0.6521739130434783, 0.54739336492891, 0.555, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5635487514571579, 0.0, 0.0, 0.8294399451523027, 0.1539544441245041, 0.1539544441245041, 0.25789359206257906], 
reward next is 0.7421, 
noisyNet noise sample is [array([1.4290729], dtype=float32), -0.7154268]. 
=============================================
[2019-03-26 02:49:02,894] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2156894: loss 0.1341
[2019-03-26 02:49:02,900] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2156895: learning rate 0.0010
[2019-03-26 02:49:03,007] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2156935: loss 6.1888
[2019-03-26 02:49:03,009] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2156935: learning rate 0.0010
[2019-03-26 02:49:03,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2156994: loss 4.5476
[2019-03-26 02:49:03,165] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2156994: learning rate 0.0010
[2019-03-26 02:49:04,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:04,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3190
[2019-03-26 02:49:04,177] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6621053481689498, 6.911199999999999, 6.9112, 168.912956510431, 569426.0801235291, 569426.0801235298, 176126.4283823338], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1497600.0000, 
sim time next is 1498200.0000, 
raw observation next is [24.26666666666667, 82.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6621588623367899, 6.9112, 6.9112, 168.912956510431, 569188.2520316958, 569188.2520316958, 176136.3203570169], 
processed observation next is [0.0, 0.34782608695652173, 0.34913112164297017, 0.8283333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5879986126058413, 0.0, 0.0, 0.8294399451523027, 0.15810784778658216, 0.15810784778658216, 0.2628900303836073], 
reward next is 0.7371, 
noisyNet noise sample is [array([-0.54756707], dtype=float32), -1.1003217]. 
=============================================
[2019-03-26 02:49:05,045] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2157699: loss 0.1664
[2019-03-26 02:49:05,048] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2157699: learning rate 0.0010
[2019-03-26 02:49:05,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.9203406e-06 1.0032944e-30 0.0000000e+00 1.3732135e-32 9.9999309e-01], sum to 1.0000
[2019-03-26 02:49:05,600] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4062
[2019-03-26 02:49:05,611] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.6, 75.83333333333333, 1.0, 2.0, 0.3372388264551894, 1.0, 2.0, 0.3372388264551894, 1.0, 1.0, 0.5616577138735673, 6.911199999999999, 6.9112, 170.5573041426782, 1414183.637787781, 1414183.637787781, 318017.6533294356], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1245000.0000, 
sim time next is 1245600.0000, 
raw observation next is [26.8, 75.0, 1.0, 2.0, 0.3269028413044815, 1.0, 2.0, 0.3269028413044815, 1.0, 2.0, 0.5450635281160106, 6.911199999999999, 6.9112, 170.5573041426782, 1370812.808886243, 1370812.808886244, 313532.737666285], 
processed observation next is [1.0, 0.43478260869565216, 0.4691943127962086, 0.75, 1.0, 1.0, 0.18903956783672468, 1.0, 1.0, 0.18903956783672468, 1.0, 1.0, 0.4451994245317202, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3807813358017342, 0.38078133580173446, 0.4679593099496791], 
reward next is 0.5320, 
noisyNet noise sample is [array([-1.3273174], dtype=float32), -0.14988315]. 
=============================================
[2019-03-26 02:49:08,696] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.690459e-14], sum to 1.0000
[2019-03-26 02:49:08,708] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3882
[2019-03-26 02:49:08,717] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.8, 91.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7878777598917149, 6.911200000000001, 6.9112, 168.912956510431, 662967.2775267791, 662967.2775267785, 199409.8207840321], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1286400.0000, 
sim time next is 1287000.0000, 
raw observation next is [24.75, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7868916476040839, 6.9112, 6.9112, 168.912956510431, 662180.2061036015, 662180.2061036015, 199210.745149271], 
processed observation next is [1.0, 0.9130434782608695, 0.3720379146919432, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7401117653708341, 0.0, 0.0, 0.8294399451523027, 0.18393894613988931, 0.18393894613988931, 0.29732947037204627], 
reward next is 0.7027, 
noisyNet noise sample is [array([-0.53377616], dtype=float32), -0.5016322]. 
=============================================
[2019-03-26 02:49:08,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.704704]
 [73.43762 ]
 [73.09448 ]
 [72.763565]
 [72.68468 ]], R is [[73.79650879]
 [73.76091766]
 [73.72592926]
 [73.69177246]
 [73.65853119]].
[2019-03-26 02:49:09,964] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0383672e-21 1.0018330e-37 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 02:49:09,975] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9267
[2019-03-26 02:49:09,980] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.33333333333334, 92.66666666666667, 1.0, 2.0, 0.292295585125021, 1.0, 2.0, 0.292295585125021, 1.0, 2.0, 0.5091585251177129, 6.9112, 6.9112, 170.5573041426782, 1310279.11553774, 1310279.11553774, 309227.0956288228], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1339800.0000, 
sim time next is 1340400.0000, 
raw observation next is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.2222337055094335, 1.0, 2.0, 0.2222337055094335, 1.0, 2.0, 0.3885802135490797, 6.9112, 6.9112, 170.5573041426782, 1001795.457580783, 1001795.457580783, 283965.1495962141], 
processed observation next is [1.0, 0.5217391304347826, 0.2543443917851502, 0.9233333333333335, 1.0, 1.0, 0.06293217531257046, 1.0, 1.0, 0.06293217531257046, 1.0, 1.0, 0.2543661140842435, 0.0, 0.0, 0.8375144448122397, 0.27827651599466197, 0.27827651599466197, 0.42382858148688674], 
reward next is 0.5762, 
noisyNet noise sample is [array([-0.45523328], dtype=float32), 0.63986737]. 
=============================================
[2019-03-26 02:49:10,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.23132765e-20 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.00000000e+00], sum to 1.0000
[2019-03-26 02:49:10,422] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5081
[2019-03-26 02:49:10,427] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.95, 91.50000000000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2883919473159149, 6.9112, 6.9112, 170.5573041426782, 712574.0265389761, 712574.0265389761, 261889.0268959395], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1721400.0000, 
sim time next is 1722000.0000, 
raw observation next is [25.9, 92.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2883660141438794, 6.911199999999999, 6.9112, 170.5573041426782, 712637.7582846946, 712637.7582846952, 261896.3579836226], 
processed observation next is [1.0, 0.9565217391304348, 0.42654028436018954, 0.92, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.1321536757852188, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1979549328568596, 0.19795493285685978, 0.39089008654272034], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00405023], dtype=float32), 0.12409935]. 
=============================================
[2019-03-26 02:49:10,457] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.359184]
 [68.96193 ]
 [70.209915]
 [70.17758 ]
 [69.94911 ]], R is [[68.96212006]
 [68.27249908]
 [68.19880676]
 [68.19648743]
 [68.19371796]].
[2019-03-26 02:49:11,879] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2160229: loss 0.2235
[2019-03-26 02:49:11,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2160230: learning rate 0.0010
[2019-03-26 02:49:12,866] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2160590: loss 0.0850
[2019-03-26 02:49:12,869] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2160591: learning rate 0.0010
[2019-03-26 02:49:17,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2162145: loss 15.9022
[2019-03-26 02:49:17,063] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2162145: learning rate 0.0010
[2019-03-26 02:49:17,698] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2162379: loss 0.0368
[2019-03-26 02:49:17,699] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2162379: learning rate 0.0010
[2019-03-26 02:49:17,775] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2162403: loss 0.1136
[2019-03-26 02:49:17,780] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2162405: learning rate 0.0010
[2019-03-26 02:49:19,850] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:19,859] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9265
[2019-03-26 02:49:19,865] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.7, 86.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8546006952902093, 6.9112, 6.9112, 168.912956510431, 708056.8846347855, 708056.8846347855, 213248.8687802215], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1713600.0000, 
sim time next is 1714200.0000, 
raw observation next is [26.65, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.857249883777081, 6.911199999999999, 6.9112, 168.912956510431, 710266.2688861516, 710266.2688861522, 213833.8479591855], 
processed observation next is [1.0, 0.8695652173913043, 0.462085308056872, 0.865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8259144924110744, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19729618580170877, 0.19729618580170893, 0.31915499695400823], 
reward next is 0.6808, 
noisyNet noise sample is [array([-0.5406604], dtype=float32), 2.0172799]. 
=============================================
[2019-03-26 02:49:19,964] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2163207: loss 0.0364
[2019-03-26 02:49:19,970] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2163208: learning rate 0.0010
[2019-03-26 02:49:19,997] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2163215: loss 0.0704
[2019-03-26 02:49:19,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2163215: learning rate 0.0010
[2019-03-26 02:49:20,917] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8850399e-20 6.0808824e-36 5.9287198e-37 1.1737676e-27 1.0000000e+00], sum to 1.0000
[2019-03-26 02:49:20,929] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2928
[2019-03-26 02:49:20,935] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.5, 87.33333333333334, 1.0, 2.0, 0.3758920248944449, 1.0, 1.0, 0.3758920248944449, 1.0, 2.0, 0.630908825786688, 6.911200000000001, 6.9112, 170.5573041426782, 1576391.88437077, 1576391.884370769, 337016.5943078385], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1855200.0000, 
sim time next is 1855800.0000, 
raw observation next is [25.6, 87.0, 1.0, 2.0, 0.3895354281488586, 1.0, 2.0, 0.3895354281488586, 1.0, 2.0, 0.6551931556352284, 6.9112, 6.9112, 170.5573041426782, 1633652.323591105, 1633652.323591105, 344207.4173239801], 
processed observation next is [1.0, 0.4782608695652174, 0.4123222748815167, 0.87, 1.0, 1.0, 0.2645005158419983, 1.0, 1.0, 0.2645005158419983, 1.0, 1.0, 0.5795038483356444, 0.0, 0.0, 0.8375144448122397, 0.45379231210864024, 0.45379231210864024, 0.5137424139163882], 
reward next is 0.4863, 
noisyNet noise sample is [array([-0.80792296], dtype=float32), -0.7239502]. 
=============================================
[2019-03-26 02:49:21,709] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2163845: loss 0.0663
[2019-03-26 02:49:21,710] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2163845: learning rate 0.0010
[2019-03-26 02:49:22,106] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2163993: loss 0.4215
[2019-03-26 02:49:22,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2163993: learning rate 0.0010
[2019-03-26 02:49:22,172] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:22,184] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6983
[2019-03-26 02:49:22,189] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.26666666666667, 96.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5986698462614509, 6.9112, 6.9112, 168.912956510431, 521447.604229324, 521447.604229324, 165874.2704173857], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1477200.0000, 
sim time next is 1477800.0000, 
raw observation next is [21.2, 96.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.597172884762986, 6.9112, 6.9112, 168.912956510431, 520395.3048499784, 520395.3048499784, 165642.5687994323], 
processed observation next is [0.0, 0.08695652173913043, 0.20379146919431282, 0.965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5087474204426659, 0.0, 0.0, 0.8294399451523027, 0.14455425134721622, 0.14455425134721622, 0.24722771462601836], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.74188656], dtype=float32), -2.3082404]. 
=============================================
[2019-03-26 02:49:22,479] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2164128: loss 0.1716
[2019-03-26 02:49:22,486] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2164128: learning rate 0.0010
[2019-03-26 02:49:23,420] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2164482: loss 0.0291
[2019-03-26 02:49:23,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2164482: learning rate 0.0010
[2019-03-26 02:49:23,857] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2164645: loss 0.0699
[2019-03-26 02:49:23,863] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2164645: learning rate 0.0010
[2019-03-26 02:49:24,523] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2164898: loss 0.3987
[2019-03-26 02:49:24,526] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2164898: learning rate 0.0010
[2019-03-26 02:49:24,542] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2164901: loss 0.1296
[2019-03-26 02:49:24,545] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2164902: learning rate 0.0010
[2019-03-26 02:49:24,553] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2164906: loss 0.3094
[2019-03-26 02:49:24,557] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2164906: learning rate 0.0010
[2019-03-26 02:49:27,056] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2165829: loss 0.1072
[2019-03-26 02:49:27,059] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2165829: learning rate 0.0010
[2019-03-26 02:49:32,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:32,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7777
[2019-03-26 02:49:32,517] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.93333333333333, 84.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8590468652771404, 6.9112, 6.9112, 168.912956510431, 711368.316647948, 711368.316647948, 214218.449718196], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1712400.0000, 
sim time next is 1713000.0000, 
raw observation next is [26.81666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8575061408685778, 6.9112, 6.9112, 168.912956510431, 710274.4841059294, 710274.4841059294, 213883.6777635921], 
processed observation next is [1.0, 0.8260869565217391, 0.46998420221169057, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8262270010592412, 0.0, 0.0, 0.8294399451523027, 0.19729846780720262, 0.19729846780720262, 0.31922936979640615], 
reward next is 0.6808, 
noisyNet noise sample is [array([-0.02378595], dtype=float32), -0.15817663]. 
=============================================
[2019-03-26 02:49:32,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.752014]
 [68.24848 ]
 [68.33137 ]
 [68.48103 ]
 [68.659065]], R is [[67.43673706]
 [67.44263458]
 [67.44886017]
 [67.45554352]
 [67.46266937]].
[2019-03-26 02:49:33,921] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2168350: loss 0.0364
[2019-03-26 02:49:33,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2168350: learning rate 0.0010
[2019-03-26 02:49:34,785] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2168669: loss 0.0602
[2019-03-26 02:49:34,786] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2168669: learning rate 0.0010
[2019-03-26 02:49:38,299] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2169966: loss 0.0060
[2019-03-26 02:49:38,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2169967: learning rate 0.0010
[2019-03-26 02:49:38,866] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2170180: loss 0.1009
[2019-03-26 02:49:38,870] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2170181: learning rate 0.0010
[2019-03-26 02:49:39,760] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2170510: loss 0.0472
[2019-03-26 02:49:39,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2170510: learning rate 0.0010
[2019-03-26 02:49:41,263] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2171064: loss 0.1389
[2019-03-26 02:49:41,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2171066: learning rate 0.0010
[2019-03-26 02:49:42,037] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2171349: loss 0.0917
[2019-03-26 02:49:42,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2171350: learning rate 0.0010
[2019-03-26 02:49:43,115] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2171749: loss 0.0860
[2019-03-26 02:49:43,120] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2171750: learning rate 0.0010
[2019-03-26 02:49:43,982] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2172073: loss 0.1876
[2019-03-26 02:49:43,988] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2172073: learning rate 0.0010
[2019-03-26 02:49:44,223] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2172162: loss 0.0949
[2019-03-26 02:49:44,225] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2172162: learning rate 0.0010
[2019-03-26 02:49:45,063] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:49:45,072] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4682
[2019-03-26 02:49:45,077] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.9, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8368661871509396, 6.9112, 6.9112, 168.912956510431, 695657.049610034, 695657.049610034, 209456.8347938028], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2100600.0000, 
sim time next is 2101200.0000, 
raw observation next is [26.03333333333333, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8431298103828127, 6.911199999999999, 6.9112, 168.912956510431, 700215.9651104757, 700215.9651104764, 210792.9665972836], 
processed observation next is [0.0, 0.30434782608695654, 0.4328593996840442, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8086948907107472, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1945044347529099, 0.1945044347529101, 0.3146163680556472], 
reward next is 0.6854, 
noisyNet noise sample is [array([-0.7410075], dtype=float32), -0.8426189]. 
=============================================
[2019-03-26 02:49:45,227] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2172533: loss 0.0538
[2019-03-26 02:49:45,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2172534: learning rate 0.0010
[2019-03-26 02:49:45,280] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2172553: loss 0.0760
[2019-03-26 02:49:45,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2172554: learning rate 0.0010
[2019-03-26 02:49:45,816] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2172752: loss 0.0259
[2019-03-26 02:49:45,818] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2172752: learning rate 0.0010
[2019-03-26 02:49:46,371] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2172958: loss 0.0721
[2019-03-26 02:49:46,375] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2172959: learning rate 0.0010
[2019-03-26 02:49:46,417] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2172976: loss 0.0504
[2019-03-26 02:49:46,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2172976: learning rate 0.0010
[2019-03-26 02:49:48,581] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2173770: loss 0.2085
[2019-03-26 02:49:48,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2173770: learning rate 0.0010
[2019-03-26 02:49:51,918] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 02:49:51,921] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:49:51,924] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:49:51,924] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:49:51,926] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:49:51,927] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:49:51,929] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:49:51,929] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:49:51,929] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:49:51,931] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:49:51,931] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:49:51,966] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-26 02:49:51,993] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-26 02:49:51,994] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-26 02:49:52,041] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-26 02:49:52,065] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-26 02:49:55,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:49:55,255] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.07938641166667, 98.22256974999999, 1.0, 2.0, 0.2652702509747423, 1.0, 2.0, 0.2652702509747423, 1.0, 2.0, 0.4450933068065317, 6.9112, 6.9112, 171.5212843490159, 1112229.464708741, 1112229.464708741, 289712.6598424765]
[2019-03-26 02:49:55,258] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:49:55,265] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2488969e-09 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sampled 0.8576472378958211
[2019-03-26 02:50:19,662] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:50:19,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.17666098, 88.50809555000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7428073708525407, 6.911200000000001, 6.9112, 168.912956510431, 632074.4946977295, 632074.4946977289, 190635.5213907578]
[2019-03-26 02:50:19,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:50:19,671] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7401087976643417
[2019-03-26 02:50:22,996] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:50:22,998] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.23333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8366312705936271, 6.9112, 6.9112, 168.912956510431, 695816.3081932284, 695816.3081932284, 209416.6530196774]
[2019-03-26 02:50:23,000] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:50:23,002] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6303953898494977
[2019-03-26 02:50:41,044] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:50:41,044] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.66666666666667, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1909938052161895, 6.911199999999999, 6.9112, 170.5573041426782, 500152.380936214, 500152.3809362146, 230170.2069675689]
[2019-03-26 02:50:41,047] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:50:41,051] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00278324 0.         0.         0.         0.9972167 ], sampled 0.5297441857672576
[2019-03-26 02:51:02,430] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:02,432] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.83333333333334, 63.66666666666666, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.205757157679647, 6.9112, 168.9112470473963, 1037849.328889564, 828882.4582018603, 254813.2897987697]
[2019-03-26 02:51:02,433] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:51:02,440] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.10057987268568735
[2019-03-26 02:51:02,440] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1037849.328889564 W.
[2019-03-26 02:51:29,999] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:30,001] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.25, 85.5, 1.0, 2.0, 0.7465990207154669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043425.397041194, 1043425.397041194, 231227.2041670242]
[2019-03-26 02:51:30,001] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:51:30,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8249245e-32], sampled 0.6902370737521405
[2019-03-26 02:51:30,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1043425.397041194 W.
[2019-03-26 02:51:30,591] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:30,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.83333333333334, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9828030130975534, 6.9112, 6.9112, 168.912956510431, 811772.170000697, 811772.170000697, 243606.8251532203]
[2019-03-26 02:51:30,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:51:30,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4588942784285336
[2019-03-26 02:51:44,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:44,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.45909946, 54.52113839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5542956155548932, 6.9112, 6.9112, 168.912956510431, 488899.9916839051, 488899.9916839051, 159237.5458311812]
[2019-03-26 02:51:44,138] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:51:44,140] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2960796042324222
[2019-03-26 02:51:49,879] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:49,880] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.43333333333333, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.031763093597037, 6.9112, 168.9120603057053, 914365.4442802635, 828834.2900277817, 254812.3428438498]
[2019-03-26 02:51:49,882] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:51:49,885] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.983897e-38], sampled 0.868123215627086
[2019-03-26 02:51:49,887] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 914365.4442802635 W.
[2019-03-26 02:51:57,838] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-1.1032193], dtype=float32), 0.21933375]
[2019-03-26 02:51:57,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.2514825, 67.86751466333334, 1.0, 2.0, 0.8892380013254898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564412172, 1242890.219820221, 1242890.21982022, 266991.801349604]
[2019-03-26 02:51:57,840] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:51:57,842] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.7685742e-18], sampled 0.49769628283406886
[2019-03-26 02:51:57,846] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1242890.219820221 W.
[2019-03-26 02:52:01,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8109.7245 2997573305.4843 990.0000
[2019-03-26 02:52:01,853] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8176.8285 2954490702.6407 990.0000
[2019-03-26 02:52:01,882] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7515.9438 3320276143.4106 1367.0000
[2019-03-26 02:52:01,952] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7555.0959 3117416625.5949 1519.0000
[2019-03-26 02:52:02,035] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7380.9606 3185037494.4121 1458.0000
[2019-03-26 02:52:03,056] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2175000, evaluation results [2175000.0, 7515.943755460031, 3320276143.410574, 1367.0, 7555.095852442949, 3117416625.594898, 1519.0, 8176.828498718063, 2954490702.6407237, 990.0, 7380.960581964153, 3185037494.412149, 1458.0, 8109.724476491966, 2997573305.4842877, 990.0]
[2019-03-26 02:52:06,432] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2176264: loss 0.0288
[2019-03-26 02:52:06,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2176264: learning rate 0.0010
[2019-03-26 02:52:07,289] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2176580: loss 0.1925
[2019-03-26 02:52:07,292] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2176582: learning rate 0.0010
[2019-03-26 02:52:07,618] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:07,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4632
[2019-03-26 02:52:07,637] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.88333333333333, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7238401369032755, 6.9112, 6.9112, 168.912956510431, 617110.0769192945, 617110.0769192945, 187072.3137020698], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1966200.0000, 
sim time next is 1966800.0000, 
raw observation next is [23.76666666666667, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7191629217033502, 6.9112, 6.9112, 168.912956510431, 613512.9860393716, 613512.9860393716, 186208.6667111474], 
processed observation next is [1.0, 0.782608695652174, 0.32543443917851517, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6575157581748173, 0.0, 0.0, 0.8294399451523027, 0.17042027389982545, 0.17042027389982545, 0.27792338315096626], 
reward next is 0.7221, 
noisyNet noise sample is [array([-0.05181957], dtype=float32), 0.63333774]. 
=============================================
[2019-03-26 02:52:10,599] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 2.8909832e-36 6.4462194e-32 7.1541517e-31 1.1355665e-09], sum to 1.0000
[2019-03-26 02:52:10,610] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0965
[2019-03-26 02:52:10,617] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.9, 87.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8166778574262253, 6.9112, 6.9112, 168.912956510431, 681922.010016076, 681922.010016076, 205243.1436553423], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2054400.0000, 
sim time next is 2055000.0000, 
raw observation next is [25.85, 87.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8146149823989239, 6.911199999999998, 6.9112, 168.912956510431, 680448.1610901399, 680448.1610901412, 204816.1845568896], 
processed observation next is [0.0, 0.782608695652174, 0.4241706161137442, 0.8783333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7739207102425902, -1.7763568394002506e-16, 0.0, 0.8294399451523027, 0.1890133780805944, 0.18901337808059476, 0.3056957978461039], 
reward next is 0.6943, 
noisyNet noise sample is [array([-0.26449242], dtype=float32), 0.7597128]. 
=============================================
[2019-03-26 02:52:10,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.125786]
 [77.02409 ]
 [76.9148  ]
 [76.803635]
 [76.681656]], R is [[77.1183548 ]
 [77.04084015]
 [76.96334839]
 [76.88574219]
 [76.80791473]].
[2019-03-26 02:52:11,439] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2178111: loss 7.2531
[2019-03-26 02:52:11,443] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2178112: learning rate 0.0010
[2019-03-26 02:52:12,127] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2178361: loss -25.1575
[2019-03-26 02:52:12,130] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2178361: learning rate 0.0010
[2019-03-26 02:52:12,406] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2178467: loss 0.0335
[2019-03-26 02:52:12,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2178467: learning rate 0.0010
[2019-03-26 02:52:14,357] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2179189: loss 10.3416
[2019-03-26 02:52:14,360] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2179190: learning rate 0.0010
[2019-03-26 02:52:14,489] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2179234: loss 0.0051
[2019-03-26 02:52:14,497] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2179234: learning rate 0.0010
[2019-03-26 02:52:14,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.922085e-11 0.000000e+00 0.000000e+00 1.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-26 02:52:14,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5654
[2019-03-26 02:52:14,580] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.03333333333333, 61.33333333333333, 1.0, 2.0, 0.9515661998243694, 1.0, 2.0, 0.9515661998243694, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2661678.6373925, 2661678.6373925, 500406.0203716779], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2389200.0000, 
sim time next is 2389800.0000, 
raw observation next is [33.06666666666667, 61.16666666666666, 1.0, 2.0, 0.9520769074572752, 1.0, 2.0, 0.9520769074572752, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2663108.688578923, 2663108.688578923, 500700.1664838643], 
processed observation next is [1.0, 0.6521739130434783, 0.7661927330173778, 0.6116666666666666, 1.0, 1.0, 0.9422613342858738, 1.0, 1.0, 0.9422613342858738, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7397524134941452, 0.7397524134941452, 0.7473136813192005], 
reward next is 0.2527, 
noisyNet noise sample is [array([0.35034126], dtype=float32), 0.9210869]. 
=============================================
[2019-03-26 02:52:16,195] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2179856: loss 22.9494
[2019-03-26 02:52:16,202] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2179858: learning rate 0.0010
[2019-03-26 02:52:16,364] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2179919: loss 0.0061
[2019-03-26 02:52:16,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2179919: learning rate 0.0010
[2019-03-26 02:52:16,371] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0000000e+00 1.0089302e-33 2.0751757e-29 1.5420353e-27 1.3647301e-08], sum to 1.0000
[2019-03-26 02:52:16,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5795
[2019-03-26 02:52:16,391] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.46666666666667, 79.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9921444908532424, 6.9112, 6.9112, 168.912956510431, 800735.0482298566, 800735.0482298566, 245093.3727765085], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2414400.0000, 
sim time next is 2415000.0000, 
raw observation next is [29.38333333333333, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.002931910702419, 6.9112, 6.9112, 168.9128488619138, 809813.2317085193, 809813.2317085193, 247886.5662070102], 
processed observation next is [1.0, 0.9565217391304348, 0.5916271721958924, 0.7966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0035755008566085, 0.0, 0.0, 0.8294394165488015, 0.22494811991903313, 0.22494811991903313, 0.3699799495627018], 
reward next is 0.6300, 
noisyNet noise sample is [array([0.88511664], dtype=float32), -0.8200072]. 
=============================================
[2019-03-26 02:52:16,413] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[64.192055]
 [63.74441 ]
 [63.446957]
 [63.056873]
 [62.731823]], R is [[64.20076752]
 [64.19294739]
 [64.18086243]
 [64.17340088]
 [64.16449738]].
[2019-03-26 02:52:16,672] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2180030: loss 0.0032
[2019-03-26 02:52:16,673] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2180030: learning rate 0.0010
[2019-03-26 02:52:17,651] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2180387: loss 0.0050
[2019-03-26 02:52:17,654] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2180387: learning rate 0.0010
[2019-03-26 02:52:18,416] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2180679: loss -74.9273
[2019-03-26 02:52:18,419] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2180679: learning rate 0.0010
[2019-03-26 02:52:18,625] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2180756: loss 0.0061
[2019-03-26 02:52:18,632] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2180757: learning rate 0.0010
[2019-03-26 02:52:18,713] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2180782: loss 0.0376
[2019-03-26 02:52:18,716] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2180782: learning rate 0.0010
[2019-03-26 02:52:19,215] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2180971: loss 4.6397
[2019-03-26 02:52:19,219] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2180971: learning rate 0.0010
[2019-03-26 02:52:21,793] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2181915: loss 10.7065
[2019-03-26 02:52:21,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2181915: learning rate 0.0010
[2019-03-26 02:52:28,490] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2184406: loss 8.8946
[2019-03-26 02:52:28,490] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2184406: learning rate 0.0010
[2019-03-26 02:52:28,532] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 8.29302e-29], sum to 1.0000
[2019-03-26 02:52:28,544] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0018
[2019-03-26 02:52:28,555] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.8, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9511110017806479, 6.9112, 6.9112, 168.912956510431, 773845.0740716335, 773845.0740716335, 235156.5221265816], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2502000.0000, 
sim time next is 2502600.0000, 
raw observation next is [26.78333333333333, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9490855486515815, 6.9112, 6.9112, 168.912956510431, 772241.7899304081, 772241.7899304081, 234662.705944564], 
processed observation next is [1.0, 1.0, 0.46840442338072663, 0.9400000000000002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9379092056726603, 0.0, 0.0, 0.8294399451523027, 0.21451160831400226, 0.21451160831400226, 0.3502428446933791], 
reward next is 0.6498, 
noisyNet noise sample is [array([-0.8398458], dtype=float32), -1.397629]. 
=============================================
[2019-03-26 02:52:29,217] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2184671: loss -1.3537
[2019-03-26 02:52:29,218] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2184671: learning rate 0.0010
[2019-03-26 02:52:32,830] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:32,840] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6805
[2019-03-26 02:52:32,847] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.738612331635632, 6.9112, 6.9112, 168.912956510431, 627312.4138143113, 627312.4138143113, 189826.7958957655], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2711400.0000, 
sim time next is 2712000.0000, 
raw observation next is [23.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7348586822919936, 6.911199999999999, 6.9112, 168.912956510431, 624343.1823901994, 624343.1823902001, 189118.7577811736], 
processed observation next is [0.0, 0.391304347826087, 0.28909952606635075, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6766569296243824, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1734286617750554, 0.1734286617750556, 0.28226680265846804], 
reward next is 0.7177, 
noisyNet noise sample is [array([0.43415353], dtype=float32), 1.3351134]. 
=============================================
[2019-03-26 02:52:32,867] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.103584]
 [76.9497  ]
 [76.589905]
 [76.47396 ]
 [76.32166 ]], R is [[77.16400909]
 [77.10904694]
 [77.05322266]
 [76.99550629]
 [76.93572235]].
[2019-03-26 02:52:32,868] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2186005: loss 0.2232
[2019-03-26 02:52:32,873] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2186005: learning rate 0.0010
[2019-03-26 02:52:33,570] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2186266: loss 0.0570
[2019-03-26 02:52:33,571] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2186266: learning rate 0.0010
[2019-03-26 02:52:34,266] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2186515: loss 3.9094
[2019-03-26 02:52:34,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2186515: learning rate 0.0010
[2019-03-26 02:52:35,630] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2187022: loss 0.0741
[2019-03-26 02:52:35,634] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2187022: learning rate 0.0010
[2019-03-26 02:52:36,312] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2187276: loss -109.8387
[2019-03-26 02:52:36,314] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2187276: learning rate 0.0010
[2019-03-26 02:52:37,710] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2187799: loss 0.3790
[2019-03-26 02:52:37,715] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2187800: learning rate 0.0010
[2019-03-26 02:52:37,793] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.9962735e-01 1.0698538e-21 1.1031311e-24 3.7258794e-04 7.3901845e-17], sum to 1.0000
[2019-03-26 02:52:37,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9971
[2019-03-26 02:52:37,813] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1788622.154783553 W.
[2019-03-26 02:52:37,822] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.81666666666667, 88.83333333333334, 1.0, 2.0, 0.6396844817511671, 1.0, 1.0, 0.6396844817511671, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1788622.154783553, 1788622.154783552, 349363.019488224], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2458200.0000, 
sim time next is 2458800.0000, 
raw observation next is [25.7, 89.0, 1.0, 2.0, 0.4301198567738913, 1.0, 2.0, 0.4301198567738913, 1.0, 1.0, 0.7277390150591727, 6.9112, 6.9112, 170.5573041426782, 1804000.510425789, 1804000.510425789, 367204.5929178707], 
processed observation next is [1.0, 0.4782608695652174, 0.4170616113744076, 0.89, 1.0, 1.0, 0.313397417799869, 1.0, 1.0, 0.313397417799869, 1.0, 0.5, 0.6679744086087472, 0.0, 0.0, 0.8375144448122397, 0.5011112528960525, 0.5011112528960525, 0.5480665565938369], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4694843], dtype=float32), -0.03218683]. 
=============================================
[2019-03-26 02:52:38,188] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2187970: loss 7.3882
[2019-03-26 02:52:38,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2187971: learning rate 0.0010
[2019-03-26 02:52:38,487] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2188083: loss 16.0142
[2019-03-26 02:52:38,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2188083: learning rate 0.0010
[2019-03-26 02:52:39,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:39,040] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3710
[2019-03-26 02:52:39,043] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333333, 80.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8106651031163903, 6.9112, 6.9112, 168.912956510431, 678122.6819250647, 678122.6819250647, 204013.9203761732], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2646600.0000, 
sim time next is 2647200.0000, 
raw observation next is [26.66666666666667, 82.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8139361788416736, 6.9112, 6.9112, 168.912956510431, 680389.353542873, 680389.353542873, 204686.7823872991], 
processed observation next is [0.0, 0.6521739130434783, 0.4628751974723541, 0.8233333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.773092901026431, 0.0, 0.0, 0.8294399451523027, 0.18899704265079806, 0.18899704265079806, 0.3055026602795509], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.01807114], dtype=float32), 0.45552385]. 
=============================================
[2019-03-26 02:52:39,284] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2188377: loss 23.1097
[2019-03-26 02:52:39,288] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2188377: learning rate 0.0010
[2019-03-26 02:52:39,803] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2188563: loss 0.2266
[2019-03-26 02:52:39,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2188563: learning rate 0.0010
[2019-03-26 02:52:40,605] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2188857: loss 0.1220
[2019-03-26 02:52:40,607] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2188858: learning rate 0.0010
[2019-03-26 02:52:40,697] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2188898: loss -94.5708
[2019-03-26 02:52:40,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2188899: learning rate 0.0010
[2019-03-26 02:52:40,757] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2188917: loss -88.1148
[2019-03-26 02:52:40,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2188917: learning rate 0.0010
[2019-03-26 02:52:43,372] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2189877: loss 0.2026
[2019-03-26 02:52:43,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2189878: learning rate 0.0010
[2019-03-26 02:52:48,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:48,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1011
[2019-03-26 02:52:48,390] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7151950840123337, 6.911199999999999, 6.9112, 168.912956510431, 610664.6346321506, 610664.6346321513, 185481.3822709611], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2832600.0000, 
sim time next is 2833200.0000, 
raw observation next is [24.0, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7140108911954084, 6.9112, 6.9112, 168.912956510431, 609653.2496629966, 609653.2496629966, 185264.3658041304], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.651232794140742, 0.0, 0.0, 0.8294399451523027, 0.16934812490638795, 0.16934812490638795, 0.2765139788121349], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.9031764], dtype=float32), 1.3966368]. 
=============================================
[2019-03-26 02:52:49,914] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2192300: loss 0.1704
[2019-03-26 02:52:49,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2192300: learning rate 0.0010
[2019-03-26 02:52:50,909] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2192664: loss 0.1746
[2019-03-26 02:52:50,910] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2192664: learning rate 0.0010
[2019-03-26 02:52:52,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:52,906] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1190
[2019-03-26 02:52:52,910] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.95, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7361049684276473, 6.9112, 6.9112, 168.912956510431, 625771.8717359477, 625771.8717359477, 189357.5114457821], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2608200.0000, 
sim time next is 2608800.0000, 
raw observation next is [23.93333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7370847818167513, 6.9112, 6.9112, 168.912956510431, 626735.3034365972, 626735.3034365972, 189544.0038313558], 
processed observation next is [0.0, 0.17391304347826086, 0.3333333333333332, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6793716851423796, 0.0, 0.0, 0.8294399451523027, 0.17409313984349922, 0.17409313984349922, 0.28290149825575495], 
reward next is 0.7171, 
noisyNet noise sample is [array([1.0789113], dtype=float32), 0.8987131]. 
=============================================
[2019-03-26 02:52:54,119] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2193844: loss 0.0305
[2019-03-26 02:52:54,121] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2193844: learning rate 0.0010
[2019-03-26 02:52:54,971] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2194157: loss 0.0482
[2019-03-26 02:52:54,975] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2194158: learning rate 0.0010
[2019-03-26 02:52:55,586] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:52:55,600] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9773
[2019-03-26 02:52:55,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7950551663478183, 6.9112, 6.9112, 168.912956510431, 666642.3273552274, 666642.3273552274, 200823.8258842817], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2704200.0000, 
sim time next is 2704800.0000, 
raw observation next is [24.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7977753240464865, 6.9112, 6.9112, 168.912956510431, 668923.8227857094, 668923.8227857094, 201381.5698934319], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7533845415201055, 0.0, 0.0, 0.8294399451523027, 0.1858121729960304, 0.1858121729960304, 0.3005695073036297], 
reward next is 0.6994, 
noisyNet noise sample is [array([-1.0263976], dtype=float32), 0.9515419]. 
=============================================
[2019-03-26 02:52:56,117] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2194581: loss 0.5334
[2019-03-26 02:52:56,118] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2194582: learning rate 0.0010
[2019-03-26 02:52:56,851] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2194856: loss 0.4065
[2019-03-26 02:52:56,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2194857: learning rate 0.0010
[2019-03-26 02:52:58,156] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2195336: loss 0.7567
[2019-03-26 02:52:58,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2195336: learning rate 0.0010
[2019-03-26 02:52:59,041] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2195661: loss 0.0620
[2019-03-26 02:52:59,049] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2195662: learning rate 0.0010
[2019-03-26 02:53:00,335] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2196141: loss 0.7565
[2019-03-26 02:53:00,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2196141: learning rate 0.0010
[2019-03-26 02:53:00,483] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2196192: loss 0.5677
[2019-03-26 02:53:00,491] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2196192: learning rate 0.0010
[2019-03-26 02:53:00,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:53:00,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1068
[2019-03-26 02:53:00,979] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6029641722795747, 6.9112, 6.9112, 168.912956510431, 522871.2346575569, 522871.2346575569, 166565.5341407446], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2863200.0000, 
sim time next is 2863800.0000, 
raw observation next is [22.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6024579492831807, 6.9112, 6.9112, 168.912956510431, 522432.1466490824, 522432.1466490824, 166488.1230320765], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5151926210770497, 0.0, 0.0, 0.8294399451523027, 0.14512004073585624, 0.14512004073585624, 0.24848973586877088], 
reward next is 0.7515, 
noisyNet noise sample is [array([-0.47504812], dtype=float32), -0.73651326]. 
=============================================
[2019-03-26 02:53:01,334] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2196503: loss 0.2855
[2019-03-26 02:53:01,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2196503: learning rate 0.0010
[2019-03-26 02:53:01,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:53:01,514] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5716
[2019-03-26 02:53:01,521] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.55203059111013, 6.911200000000001, 6.9112, 168.912956510431, 485115.6906017719, 485115.6906017712, 158984.092066474], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3013800.0000, 
sim time next is 3014400.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5517909787766158, 6.9112, 6.9112, 168.912956510431, 484905.0750497521, 484905.0750497521, 158950.8217100307], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4534036326544094, 0.0, 0.0, 0.8294399451523027, 0.13469585418048668, 0.13469585418048668, 0.23724003240303088], 
reward next is 0.7628, 
noisyNet noise sample is [array([-0.00206627], dtype=float32), -0.8419181]. 
=============================================
[2019-03-26 02:53:01,603] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2196600: loss 0.6207
[2019-03-26 02:53:01,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2196600: learning rate 0.0010
[2019-03-26 02:53:02,173] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2196806: loss 0.0211
[2019-03-26 02:53:02,176] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2196806: learning rate 0.0010
[2019-03-26 02:53:02,995] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2197109: loss 0.4224
[2019-03-26 02:53:02,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2197109: learning rate 0.0010
[2019-03-26 02:53:03,126] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2197155: loss 0.4923
[2019-03-26 02:53:03,136] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2197155: learning rate 0.0010
[2019-03-26 02:53:03,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.9999869e-01 9.3525714e-38 0.0000000e+00 0.0000000e+00 1.3605894e-06], sum to 1.0000
[2019-03-26 02:53:03,634] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2323
[2019-03-26 02:53:03,648] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 988381.0972253977 W.
[2019-03-26 02:53:03,657] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 1.0, 0.3139583045346284, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5652128277062062, 6.9112, 6.9112, 168.9127568567122, 988381.0972253977, 988381.0972253977, 236246.0821506111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2995800.0000, 
sim time next is 2996400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5241091770718961, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564605768, 827157.6254106076, 827157.6254106081, 198357.6307765395], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.42663756273722414, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399449074957, 0.2297660070585021, 0.22976600705850225, 0.2960561653381187], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.18190801], dtype=float32), -0.4064758]. 
=============================================
[2019-03-26 02:53:04,741] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2197744: loss 0.0925
[2019-03-26 02:53:04,742] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2197744: learning rate 0.0010
[2019-03-26 02:53:10,824] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 02:53:10,826] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:53:10,827] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:53:10,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:53:10,827] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:53:10,828] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:53:10,830] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:53:10,831] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:53:10,831] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:53:10,833] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:53:10,837] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:53:10,862] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-26 02:53:10,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-26 02:53:10,915] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-26 02:53:10,957] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-26 02:53:10,983] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-26 02:53:15,285] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8781134], dtype=float32), 0.20582433]
[2019-03-26 02:53:15,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.063163985, 82.16086174166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6013862489900524, 6.9112, 6.9112, 168.912956510431, 524900.6808511463, 524900.6808511463, 166265.4252564709]
[2019-03-26 02:53:15,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:53:15,288] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.46451201002039444
[2019-03-26 02:53:22,740] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8781134], dtype=float32), 0.20582433]
[2019-03-26 02:53:22,741] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.22784627166666, 77.39705909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3373172005867098, 6.9112, 6.9112, 168.912956510431, 305816.0672057525, 305816.0672057525, 109378.5313878679]
[2019-03-26 02:53:22,742] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:53:22,746] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.21201904674934524
[2019-03-26 02:54:34,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8781134], dtype=float32), 0.20582433]
[2019-03-26 02:54:34,632] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.33333333333333, 60.66666666666667, 1.0, 2.0, 0.6217334898780938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868845.5253646072, 868845.5253646072, 204864.7912215832]
[2019-03-26 02:54:34,633] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:54:34,636] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 5.56964e-16], sampled 0.9306761928686778
[2019-03-26 02:54:34,640] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 868845.5253646072 W.
[2019-03-26 02:54:43,927] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8781134], dtype=float32), 0.20582433]
[2019-03-26 02:54:43,928] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.48939347333333, 62.49729054333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8787135092355476, 6.9112, 6.9112, 168.912956510431, 728150.9704369215, 728150.9704369215, 218642.2286008673]
[2019-03-26 02:54:43,934] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:54:43,938] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7383250603917922
[2019-03-26 02:55:07,156] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8781134], dtype=float32), 0.20582433]
[2019-03-26 02:55:07,158] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.21920833, 90.020591955, 1.0, 2.0, 0.787225460548872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104265, 1131403.481173458, 1131403.481173458, 245124.0344162348]
[2019-03-26 02:55:07,159] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:55:07,161] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9219201e-10], sampled 0.3279018486333588
[2019-03-26 02:55:07,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1131403.481173458 W.
[2019-03-26 02:55:18,948] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7634.1942 3133288551.0974 1360.0000
[2019-03-26 02:55:20,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8146.1748 3009622992.4665 873.0000
[2019-03-26 02:55:20,327] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8206.1444 2971457824.2123 895.0000
[2019-03-26 02:55:20,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7479.7405 3197069782.7790 1274.0000
[2019-03-26 02:55:20,590] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7572.1902 3330349738.4563 1287.0000
[2019-03-26 02:55:21,607] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2200000, evaluation results [2200000.0, 7572.190189463994, 3330349738.456278, 1287.0, 7634.194243249724, 3133288551.09735, 1360.0, 8206.144433067004, 2971457824.212254, 895.0, 7479.740519037855, 3197069782.779045, 1274.0, 8146.1747991376415, 3009622992.466461, 873.0]
[2019-03-26 02:55:22,342] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2200277: loss 0.3568
[2019-03-26 02:55:22,345] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2200278: learning rate 0.0010
[2019-03-26 02:55:23,543] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2200715: loss 0.1367
[2019-03-26 02:55:23,546] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2200716: learning rate 0.0010
[2019-03-26 02:55:26,778] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2201907: loss 0.0903
[2019-03-26 02:55:26,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2201908: learning rate 0.0010
[2019-03-26 02:55:27,310] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2202108: loss 0.2356
[2019-03-26 02:55:27,315] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2202109: learning rate 0.0010
[2019-03-26 02:55:27,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9999774e-01 3.8220980e-29 1.7441739e-35 0.0000000e+00 2.3013558e-06], sum to 1.0000
[2019-03-26 02:55:27,649] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0055
[2019-03-26 02:55:27,662] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 877165.409530787 W.
[2019-03-26 02:55:27,670] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.66666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.009945757355326, 6.911200000000001, 6.9112, 168.9128220750547, 877165.409530787, 877165.4095307864, 251195.4654395208], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2968800.0000, 
sim time next is 2969400.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 1.0, 0.1954871209531799, 1.0, 1.0, 0.1954871209531799, 1.0, 2.0, 0.3429467571367611, 6.911200000000001, 6.9112, 170.5573041426782, 885654.1127857962, 885654.1127857955, 276407.4724798365], 
processed observation next is [1.0, 0.34782608695652173, 0.23380726698262277, 0.95, 1.0, 0.5, 0.03070737464238542, 1.0, 0.5, 0.03070737464238542, 1.0, 1.0, 0.198715557483855, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.24601503132938782, 0.24601503132938762, 0.41254846638781567], 
reward next is 0.5875, 
noisyNet noise sample is [array([-1.1279173], dtype=float32), -0.47099128]. 
=============================================
[2019-03-26 02:55:28,663] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2202604: loss 0.0884
[2019-03-26 02:55:28,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2202606: learning rate 0.0010
[2019-03-26 02:55:29,023] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2202735: loss 0.0738
[2019-03-26 02:55:29,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2202735: learning rate 0.0010
[2019-03-26 02:55:30,746] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2203373: loss 0.0571
[2019-03-26 02:55:30,748] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2203373: learning rate 0.0010
[2019-03-26 02:55:31,636] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2203705: loss 0.0367
[2019-03-26 02:55:31,640] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2203705: learning rate 0.0010
[2019-03-26 02:55:32,882] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2204161: loss 2.5017
[2019-03-26 02:55:32,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2204162: learning rate 0.0010
[2019-03-26 02:55:33,187] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2204274: loss 0.3918
[2019-03-26 02:55:33,196] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2204276: learning rate 0.0010
[2019-03-26 02:55:33,747] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2204481: loss 0.0151
[2019-03-26 02:55:33,749] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2204481: learning rate 0.0010
[2019-03-26 02:55:33,955] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2204554: loss 5.0915
[2019-03-26 02:55:33,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2204555: learning rate 0.0010
[2019-03-26 02:55:34,252] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5785971e-35 1.6315980e-36 0.0000000e+00 1.9007907e-36 1.0000000e+00], sum to 1.0000
[2019-03-26 02:55:34,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8459
[2019-03-26 02:55:34,273] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.3658274610215766, 1.0, 2.0, 0.3658274610215766, 1.0, 2.0, 0.6149175040640618, 6.9112, 6.9112, 170.5573041426782, 1534153.552322907, 1534153.552322907, 332141.2121531426], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3160200.0000, 
sim time next is 3160800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.3707298163263714, 1.0, 2.0, 0.3707298163263714, 1.0, 2.0, 0.6232106584503531, 6.911199999999999, 6.9112, 170.5573041426782, 1554727.242447863, 1554727.242447864, 334569.1151949639], 
processed observation next is [1.0, 0.6086956521739131, 0.4312796208530806, 0.84, 1.0, 1.0, 0.2418431522004475, 1.0, 1.0, 0.2418431522004475, 1.0, 1.0, 0.5405008029882354, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.43186867845773974, 0.43186867845774, 0.49935688835069236], 
reward next is 0.5006, 
noisyNet noise sample is [array([-0.09334886], dtype=float32), -0.021709852]. 
=============================================
[2019-03-26 02:55:34,786] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2204862: loss 0.0210
[2019-03-26 02:55:34,789] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2204862: learning rate 0.0010
[2019-03-26 02:55:35,373] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2205075: loss 12.8030
[2019-03-26 02:55:35,378] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2205076: learning rate 0.0010
[2019-03-26 02:55:35,551] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2205140: loss 2.2234
[2019-03-26 02:55:35,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2205142: learning rate 0.0010
[2019-03-26 02:55:36,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3157852e-14 7.5723007e-32 0.0000000e+00 2.0479887e-37 1.0000000e+00], sum to 1.0000
[2019-03-26 02:55:36,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1307
[2019-03-26 02:55:36,209] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.205915463671423, 1.0, 2.0, 0.205915463671423, 1.0, 2.0, 0.3560319045221495, 6.9112, 6.9112, 170.5573041426782, 912640.8142013536, 912640.8142013536, 277046.6845006265], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.2111456411885566, 1.0, 2.0, 0.2111456411885566, 1.0, 2.0, 0.3649432523979682, 6.9112, 6.9112, 170.5573041426782, 935328.2532569695, 935328.2532569695, 278534.0935276487], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.04957306167295976, 1.0, 1.0, 0.04957306167295976, 1.0, 1.0, 0.22554055170483928, 0.0, 0.0, 0.8375144448122397, 0.2598134036824915, 0.2598134036824915, 0.41572252765320705], 
reward next is 0.5843, 
noisyNet noise sample is [array([1.2612559], dtype=float32), -0.650236]. 
=============================================
[2019-03-26 02:55:37,156] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2205730: loss 0.0143
[2019-03-26 02:55:37,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2205731: learning rate 0.0010
[2019-03-26 02:55:43,699] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2208141: loss 0.0945
[2019-03-26 02:55:43,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2208141: learning rate 0.0010
[2019-03-26 02:55:44,927] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2208598: loss 0.0341
[2019-03-26 02:55:44,930] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2208599: learning rate 0.0010
[2019-03-26 02:55:48,030] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.8127095e-33], sum to 1.0000
[2019-03-26 02:55:48,046] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9744
[2019-03-26 02:55:48,056] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.16666666666667, 83.16666666666666, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9443955318360541, 6.911199999999999, 6.9112, 168.912956510431, 768009.2018588639, 768009.2018588645, 233497.6907910267], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3354600.0000, 
sim time next is 3355200.0000, 
raw observation next is [28.0, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9407658961963881, 6.9112, 6.9112, 168.912956510431, 767159.3037648113, 767159.3037648113, 232717.9096610161], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9277632880443757, 0.0, 0.0, 0.8294399451523027, 0.21309980660133646, 0.21309980660133646, 0.34734016367315834], 
reward next is 0.6527, 
noisyNet noise sample is [array([0.05148179], dtype=float32), -1.1980431]. 
=============================================
[2019-03-26 02:55:48,719] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2210000: loss 0.3727
[2019-03-26 02:55:48,723] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2210001: learning rate 0.0010
[2019-03-26 02:55:48,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:55:48,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2122
[2019-03-26 02:55:48,801] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.5, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7728708314747301, 6.9112, 6.9112, 168.912956510431, 650751.4531206937, 650751.4531206937, 196404.0142972263], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3220200.0000, 
sim time next is 3220800.0000, 
raw observation next is [25.66666666666666, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7753636858403277, 6.911200000000001, 6.9112, 168.912956510431, 652432.7671732802, 652432.7671732797, 196892.6904741923], 
processed observation next is [0.0, 0.2608695652173913, 0.4154818325434437, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7260532754150337, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18123132421480007, 0.1812313242147999, 0.29386968727491386], 
reward next is 0.7061, 
noisyNet noise sample is [array([-1.2285669], dtype=float32), 1.2908862]. 
=============================================
[2019-03-26 02:55:49,503] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2210293: loss 0.4925
[2019-03-26 02:55:49,510] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2210293: learning rate 0.0010
[2019-03-26 02:55:50,047] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2210496: loss 0.0268
[2019-03-26 02:55:50,049] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2210496: learning rate 0.0010
[2019-03-26 02:55:50,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:55:50,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1853
[2019-03-26 02:55:50,205] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9823030709709644, 6.9112, 6.9112, 168.912956510431, 791172.1943385428, 791172.1943385428, 242500.0958507184], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3240000.0000, 
sim time next is 3240600.0000, 
raw observation next is [32.0, 66.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.989165861368632, 6.9112, 6.9112, 168.9129318977196, 795930.9630050459, 795930.9630050459, 244196.2354702146], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.6633333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.986787635815405, 0.0, 0.0, 0.829439824292622, 0.2210919341680683, 0.2210919341680683, 0.36447199323912627], 
reward next is 0.6355, 
noisyNet noise sample is [array([-0.6862177], dtype=float32), -0.6681946]. 
=============================================
[2019-03-26 02:55:51,552] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2211050: loss 0.0776
[2019-03-26 02:55:51,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2211050: learning rate 0.0010
[2019-03-26 02:55:51,704] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2211108: loss 0.0404
[2019-03-26 02:55:51,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2211108: learning rate 0.0010
[2019-03-26 02:55:53,959] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2211937: loss 0.0448
[2019-03-26 02:55:53,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2211938: learning rate 0.0010
[2019-03-26 02:55:54,078] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2211983: loss 0.7709
[2019-03-26 02:55:54,081] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2211984: learning rate 0.0010
[2019-03-26 02:55:54,343] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2212079: loss 0.0437
[2019-03-26 02:55:54,346] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2212081: learning rate 0.0010
[2019-03-26 02:55:55,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.8381256e-27 0.0000000e+00 0.0000000e+00 1.4726899e-34 1.0000000e+00], sum to 1.0000
[2019-03-26 02:55:55,023] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6911
[2019-03-26 02:55:55,030] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.1731141813480441, 1.0, 2.0, 0.1731141813480441, 1.0, 2.0, 0.2946383066643412, 6.9112, 6.9112, 170.5573041426782, 725707.3586495541, 725707.3586495541, 262663.8669675326], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3625200.0000, 
sim time next is 3625800.0000, 
raw observation next is [28.0, 78.16666666666667, 1.0, 2.0, 0.1721411306984787, 1.0, 2.0, 0.1721411306984787, 1.0, 2.0, 0.292418949967044, 6.911200000000001, 6.9112, 170.5573041426782, 721626.8866974559, 721626.8866974553, 262411.5746950567], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.7816666666666667, 1.0, 1.0, 0.0025796755403357623, 1.0, 1.0, 0.0025796755403357623, 1.0, 1.0, 0.1370962804476146, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20045191297151552, 0.20045191297151535, 0.3916590667090399], 
reward next is 0.6083, 
noisyNet noise sample is [array([0.5284116], dtype=float32), 0.9706456]. 
=============================================
[2019-03-26 02:55:55,135] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2212369: loss 0.0471
[2019-03-26 02:55:55,139] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2212370: learning rate 0.0010
[2019-03-26 02:55:56,118] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2212730: loss 2.1017
[2019-03-26 02:55:56,120] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2212731: learning rate 0.0010
[2019-03-26 02:55:56,744] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2212954: loss 0.0436
[2019-03-26 02:55:56,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2212955: learning rate 0.0010
[2019-03-26 02:55:56,793] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2212975: loss 0.0192
[2019-03-26 02:55:56,795] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2212975: learning rate 0.0010
[2019-03-26 02:55:56,896] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2213013: loss 0.0137
[2019-03-26 02:55:56,898] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2213013: learning rate 0.0010
[2019-03-26 02:55:59,357] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2213926: loss 1.1664
[2019-03-26 02:55:59,359] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2213927: learning rate 0.0010
[2019-03-26 02:56:05,753] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2216288: loss 0.9171
[2019-03-26 02:56:05,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2216288: learning rate 0.0010
[2019-03-26 02:56:07,147] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2216797: loss 3.3474
[2019-03-26 02:56:07,150] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2216797: learning rate 0.0010
[2019-03-26 02:56:10,219] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2217931: loss 0.0538
[2019-03-26 02:56:10,222] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2217931: learning rate 0.0010
[2019-03-26 02:56:10,868] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2218169: loss 0.0636
[2019-03-26 02:56:10,871] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2218169: learning rate 0.0010
[2019-03-26 02:56:11,948] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2218572: loss 3.3458
[2019-03-26 02:56:11,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2218573: learning rate 0.0010
[2019-03-26 02:56:12,852] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2218906: loss 0.0519
[2019-03-26 02:56:12,856] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2218908: learning rate 0.0010
[2019-03-26 02:56:13,769] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2219245: loss 1.4069
[2019-03-26 02:56:13,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2219245: learning rate 0.0010
[2019-03-26 02:56:15,483] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2219872: loss 0.0340
[2019-03-26 02:56:15,485] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2219873: learning rate 0.0010
[2019-03-26 02:56:15,969] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2220052: loss 4.3931
[2019-03-26 02:56:15,972] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2220053: learning rate 0.0010
[2019-03-26 02:56:16,095] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2220092: loss 0.2412
[2019-03-26 02:56:16,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2220093: learning rate 0.0010
[2019-03-26 02:56:16,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:56:16,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9012
[2019-03-26 02:56:16,680] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 871455.2770216538 W.
[2019-03-26 02:56:16,686] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.6236002251814692, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104199, 871455.2770216538, 871455.2770216533, 205226.2232348358], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3852000.0000, 
sim time next is 3852600.0000, 
raw observation next is [35.0, 59.33333333333333, 1.0, 2.0, 0.2089794476845648, 1.0, 1.0, 0.2089794476845648, 1.0, 1.0, 0.3629281900686684, 6.911200000000001, 6.9112, 170.5573041426782, 876118.5975680414, 876118.5975680408, 272385.1421139166], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5933333333333333, 1.0, 1.0, 0.04696318998140337, 1.0, 0.5, 0.04696318998140337, 1.0, 0.5, 0.2230831586203273, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2433662771022337, 0.24336627710223357, 0.40654498822972623], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13453259], dtype=float32), 0.8055571]. 
=============================================
[2019-03-26 02:56:16,866] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2220379: loss 2.3666
[2019-03-26 02:56:16,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2220379: learning rate 0.0010
[2019-03-26 02:56:17,282] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2220533: loss 0.0673
[2019-03-26 02:56:17,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2220534: learning rate 0.0010
[2019-03-26 02:56:18,254] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2220891: loss 0.0621
[2019-03-26 02:56:18,258] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2220891: learning rate 0.0010
[2019-03-26 02:56:18,361] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2220932: loss 0.1908
[2019-03-26 02:56:18,365] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2220933: learning rate 0.0010
[2019-03-26 02:56:18,666] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2221043: loss 0.6671
[2019-03-26 02:56:18,669] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2221045: learning rate 0.0010
[2019-03-26 02:56:19,929] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:56:19,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5997
[2019-03-26 02:56:19,942] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.33333333333334, 77.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9532552163734513, 6.9112, 6.9112, 168.912956510431, 773936.4954474329, 773936.4954474329, 235599.9599944961], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3696000.0000, 
sim time next is 3696600.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.950031110829056, 6.911199999999999, 6.9112, 168.912956510431, 771983.658723076, 771983.6587230766, 234843.0757446111], 
processed observation next is [1.0, 0.782608695652174, 0.581358609794629, 0.7833333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9390623302793366, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21443990520085443, 0.2144399052008546, 0.35051205335016583], 
reward next is 0.6495, 
noisyNet noise sample is [array([1.2699025], dtype=float32), 0.3837156]. 
=============================================
[2019-03-26 02:56:20,734] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2221809: loss 0.0840
[2019-03-26 02:56:20,737] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2221809: learning rate 0.0010
[2019-03-26 02:56:25,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0000000e+00 2.9974183e-38 0.0000000e+00 0.0000000e+00 3.8550573e-12], sum to 1.0000
[2019-03-26 02:56:25,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4057
[2019-03-26 02:56:25,631] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8772299792600364, 6.911199999999999, 6.9112, 168.912956510431, 724514.3625625372, 724514.3625625379, 218222.4202083238], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3800400.0000, 
sim time next is 3801000.0000, 
raw observation next is [28.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8772467121052163, 6.911199999999999, 6.9112, 168.912956510431, 724528.1909779891, 724528.1909779897, 218226.2056723965], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8503008684209955, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2012578308272192, 0.20125783082721935, 0.3257107547349201], 
reward next is 0.6743, 
noisyNet noise sample is [array([0.21048304], dtype=float32), 1.5270176]. 
=============================================
[2019-03-26 02:56:25,652] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[53.669704]
 [53.63819 ]
 [53.60681 ]
 [53.583145]
 [53.549988]], R is [[53.81834412]
 [53.95445633]
 [54.08856201]
 [54.22077179]
 [54.35223389]].
[2019-03-26 02:56:26,937] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2224078: loss 0.1672
[2019-03-26 02:56:26,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2224080: learning rate 0.0010
[2019-03-26 02:56:28,536] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2224673: loss 0.1323
[2019-03-26 02:56:28,540] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2224673: learning rate 0.0010
[2019-03-26 02:56:29,431] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 02:56:29,432] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:56:29,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:56:29,435] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:56:29,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:56:29,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.920655e-11 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 02:56:29,440] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:56:29,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:56:29,443] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:56:29,444] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:56:29,445] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:56:29,447] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:56:29,452] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5099
[2019-03-26 02:56:29,458] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.1733352653978841, 1.0, 2.0, 0.1733352653978841, 1.0, 2.0, 0.2948777708169963, 6.9112, 6.9112, 170.5573041426782, 726634.4731535198, 726634.4731535198, 262709.8885341877], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3796200.0000, 
sim time next is 3796800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.173271754252991, 1.0, 2.0, 0.173271754252991, 1.0, 2.0, 0.2947691082986381, 6.9112, 6.9112, 170.5573041426782, 726368.1395045116, 726368.1395045116, 262694.933790163], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.0039418725939650325, 1.0, 1.0, 0.0039418725939650325, 1.0, 1.0, 0.1399623271934611, 0.0, 0.0, 0.8375144448122397, 0.20176892764014212, 0.20176892764014212, 0.3920819907315866], 
reward next is 0.6079, 
noisyNet noise sample is [array([-1.3938068], dtype=float32), 0.94987047]. 
=============================================
[2019-03-26 02:56:29,481] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-26 02:56:29,507] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-26 02:56:29,531] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-26 02:56:29,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-26 02:56:29,557] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-26 02:56:38,126] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:56:38,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.56666666666667, 70.83333333333334, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9866833602560787, 6.9112, 6.9112, 168.9129564946223, 833660.115903482, 833660.115903482, 245140.912083236]
[2019-03-26 02:56:38,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:56:38,130] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7935974e-05 0.0000000e+00 0.0000000e+00 0.0000000e+00 9.9998212e-01], sampled 0.34396665791651015
[2019-03-26 02:56:45,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:56:45,300] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6251075619851028, 6.911199999999999, 6.9112, 168.912956510431, 543088.8518842998, 543088.8518843005, 170004.7884142995]
[2019-03-26 02:56:45,301] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:56:45,306] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6740386e-21], sampled 0.7816224631118414
[2019-03-26 02:56:54,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:56:54,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.92800183333333, 66.53659780833334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9183453018762121, 6.9112, 6.9112, 168.912956510431, 749985.8842154369, 749985.8842154369, 227398.7069976078]
[2019-03-26 02:56:54,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 02:56:54,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 6.374328e-32], sampled 0.9336029022858964
[2019-03-26 02:57:01,542] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:01,545] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.86666666666667, 91.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7476361014999627, 6.911200000000001, 6.9112, 168.912956510431, 637203.9112639973, 637203.9112639967, 191567.798887478]
[2019-03-26 02:57:01,548] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:57:01,550] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6920319507323074
[2019-03-26 02:57:05,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:05,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6925400305700932, 6.911200000000001, 6.9112, 168.912956510431, 594687.2963151644, 594687.2963151637, 181399.5610594157]
[2019-03-26 02:57:05,074] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:57:05,076] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.6128719  0.         0.         0.         0.38712814], sampled 0.8927193719241379
[2019-03-26 02:57:09,847] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:09,848] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.70815525, 77.671145505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9558446405371026, 6.9112, 6.9112, 168.9129565104249, 778575.4367604126, 778575.4367604126, 236362.6192680234]
[2019-03-26 02:57:09,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:57:09,851] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.9929462  0.         0.         0.         0.00705378], sampled 0.8731398915879371
[2019-03-26 02:57:39,948] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:39,949] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.8, 51.5, 1.0, 2.0, 0.3780250892651318, 1.0, 2.0, 0.3780250892651318, 1.0, 2.0, 0.6565045652461756, 6.9112, 6.9112, 178.6582176852504, 1585290.845752959, 1585290.845752959, 343088.1441290468]
[2019-03-26 02:57:39,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 02:57:39,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9968874e-01 2.5136588e-37 0.0000000e+00 0.0000000e+00 3.1124605e-04], sampled 0.708656777903033
[2019-03-26 02:57:39,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1585290.845752959 W.
[2019-03-26 02:57:44,387] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:44,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.53786427333333, 60.15239531666667, 1.0, 1.0, 0.5886885988334107, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129036598545, 822648.8431681894, 822648.8431681894, 198657.48282451]
[2019-03-26 02:57:44,390] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:57:44,391] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.164487e-09 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.14821456928411403
[2019-03-26 02:57:47,644] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:57:47,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.64813904666667, 86.00020117333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9341804588284549, 6.9112, 6.9112, 168.912956510431, 764344.8330875557, 764344.8330875557, 231244.1054389346]
[2019-03-26 02:57:47,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:57:47,653] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.48861570672272303
[2019-03-26 02:58:06,152] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:58:06,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.89294518833333, 74.65290520833332, 1.0, 2.0, 0.8161352819838447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564503341, 1140659.479271893, 1140659.479271894, 247914.309378075]
[2019-03-26 02:58:06,155] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 02:58:06,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.6534557e-11], sampled 0.9247283627072843
[2019-03-26 02:58:06,160] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1140659.479271893 W.
[2019-03-26 02:58:22,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8077668], dtype=float32), 0.2656002]
[2019-03-26 02:58:22,751] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.26666666666667, 54.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6237201490527859, 6.9112, 6.9112, 168.912956510431, 540343.179096837, 540343.179096837, 169805.9335433972]
[2019-03-26 02:58:22,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 02:58:22,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6005037309730774
[2019-03-26 02:58:38,428] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7543.4732 3250717988.8218 868.0000
[2019-03-26 02:58:39,007] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7637.2594 3193923439.7530 1032.0000
[2019-03-26 02:58:39,126] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8107.9691 3029426956.9916 707.0000
[2019-03-26 02:58:39,220] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7466.5342 3381108849.4872 1117.0000
[2019-03-26 02:58:39,372] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8065.7292 3060968797.4048 676.0000
[2019-03-26 02:58:40,389] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2225000, evaluation results [2225000.0, 7466.534210349233, 3381108849.487248, 1117.0, 7637.259431234568, 3193923439.7530375, 1032.0, 8107.969139049092, 3029426956.9915667, 707.0, 7543.473244043655, 3250717988.821845, 868.0, 8065.729155583729, 3060968797.4047656, 676.0]
[2019-03-26 02:58:43,176] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2226039: loss 144.0316
[2019-03-26 02:58:43,178] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2226039: learning rate 0.0010
[2019-03-26 02:58:44,035] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2226358: loss 26.7878
[2019-03-26 02:58:44,038] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2226358: learning rate 0.0010
[2019-03-26 02:58:44,227] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2226430: loss 0.0603
[2019-03-26 02:58:44,229] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2226430: learning rate 0.0010
[2019-03-26 02:58:45,873] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2227032: loss 44.7338
[2019-03-26 02:58:45,875] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2227032: learning rate 0.0010
[2019-03-26 02:58:46,232] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2227167: loss 0.0390
[2019-03-26 02:58:46,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2227167: learning rate 0.0010
[2019-03-26 02:58:48,355] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2227948: loss 0.1404
[2019-03-26 02:58:48,358] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2227948: learning rate 0.0010
[2019-03-26 02:58:48,472] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2227990: loss 0.0363
[2019-03-26 02:58:48,476] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2227990: learning rate 0.0010
[2019-03-26 02:58:48,644] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2228052: loss 223.2019
[2019-03-26 02:58:48,646] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2228053: learning rate 0.0010
[2019-03-26 02:58:49,180] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2228258: loss 0.0512
[2019-03-26 02:58:49,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2228259: learning rate 0.0010
[2019-03-26 02:58:50,438] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2228721: loss 145.1913
[2019-03-26 02:58:50,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2228721: learning rate 0.0010
[2019-03-26 02:58:50,590] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2228777: loss 0.1082
[2019-03-26 02:58:50,593] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2228777: learning rate 0.0010
[2019-03-26 02:58:50,980] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2228917: loss 0.1689
[2019-03-26 02:58:50,983] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2228918: learning rate 0.0010
[2019-03-26 02:58:51,133] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:58:51,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5209
[2019-03-26 02:58:51,149] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1247328.739292446 W.
[2019-03-26 02:58:51,157] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.4, 87.0, 1.0, 2.0, 0.4462058596790464, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7630879330281855, 6.9112, 6.9112, 168.912956510431, 1247328.739292446, 1247328.739292446, 278744.4742652206], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4071600.0000, 
sim time next is 4072200.0000, 
raw observation next is [27.36666666666667, 87.16666666666667, 1.0, 2.0, 0.4814632861269303, 1.0, 1.0, 0.4814632861269303, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1345941.98385042, 1345941.98385042, 294596.5500365932], 
processed observation next is [1.0, 0.13043478260869565, 0.49605055292259104, 0.8716666666666667, 1.0, 1.0, 0.3752569712372654, 1.0, 0.5, 0.3752569712372654, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3738727732917833, 0.3738727732917833, 0.43969634333819885], 
reward next is 0.5603, 
noisyNet noise sample is [array([-0.5636919], dtype=float32), 0.9781696]. 
=============================================
[2019-03-26 02:58:51,298] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2229036: loss 175.0574
[2019-03-26 02:58:51,300] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2229037: learning rate 0.0010
[2019-03-26 02:58:53,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 6.7319196e-30 1.6296573e-36 1.9618949e-25 1.0000000e+00], sum to 1.0000
[2019-03-26 02:58:53,398] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3320
[2019-03-26 02:58:53,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 82.33333333333334, 1.0, 1.0, 1.04, 1.0, 1.0, 1.04, 1.0, 2.0, 1.03, 8.700335495282683, 6.9112, 170.5573041426782, 5023687.352335338, 3742056.819685501, 695053.2945877387], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3982800.0000, 
sim time next is 3983400.0000, 
raw observation next is [29.5, 81.5, 1.0, 2.0, 0.6709602213666469, 1.0, 2.0, 0.656070150197586, 1.0, 2.0, 1.03, 7.005095442508193, 6.9112, 170.5573041426782, 2752795.627326333, 2685534.502435312, 512313.1837075749], 
processed observation next is [1.0, 0.08695652173913043, 0.5971563981042655, 0.815, 1.0, 1.0, 0.6035665317670444, 1.0, 1.0, 0.5856266869850433, 1.0, 1.0, 1.0365853658536586, 0.009389544250819259, 0.0, 0.8375144448122397, 0.7646654520350925, 0.7459818062320311, 0.7646465428471267], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.53476936], dtype=float32), 1.5595423]. 
=============================================
[2019-03-26 02:58:53,791] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2229957: loss 191.0833
[2019-03-26 02:58:53,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2229958: learning rate 0.0010
[2019-03-26 02:58:55,125] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:58:55,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8366
[2019-03-26 02:58:55,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2992029.973035831 W.
[2019-03-26 02:58:55,164] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.026513850721453, 6.9112, 170.5573041426782, 2992029.973035831, 2909425.97109651, 553046.397732809], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4032000.0000, 
sim time next is 4032600.0000, 
raw observation next is [33.33333333333334, 63.16666666666666, 1.0, 2.0, 0.9208963167380619, 1.0, 2.0, 0.9208963167380619, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2575801.772753411, 2575801.77275341, 483026.1525343108], 
processed observation next is [1.0, 0.6956521739130435, 0.7788309636650873, 0.6316666666666666, 1.0, 1.0, 0.9046943575157372, 1.0, 1.0, 0.9046943575157372, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.715500492431503, 0.7155004924315028, 0.7209345560213595], 
reward next is 0.2791, 
noisyNet noise sample is [array([-0.29436928], dtype=float32), -1.255529]. 
=============================================
[2019-03-26 02:58:57,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:58:57,448] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8697
[2019-03-26 02:58:57,459] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3395379.7646527 W.
[2019-03-26 02:58:57,466] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.33333333333334, 61.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.588928519288367, 6.9112, 170.5573041426782, 3395379.7646527, 2909895.258052882, 549912.9545531349], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4368000.0000, 
sim time next is 4368600.0000, 
raw observation next is [34.5, 64.5, 1.0, 2.0, 0.931730267076893, 1.0, 2.0, 0.7864551730527093, 1.0, 1.0, 1.03, 7.005116010990339, 6.9112, 170.5573041426782, 3300599.195066199, 3233323.336134744, 604521.0492520418], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.645, 1.0, 1.0, 0.9177473097311963, 1.0, 1.0, 0.7427170759671197, 1.0, 0.5, 1.0365853658536586, 0.009391601099033942, 0.0, 0.8375144448122397, 0.9168331097406108, 0.89814537114854, 0.9022702227642415], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.71649766], dtype=float32), -0.7070047]. 
=============================================
[2019-03-26 02:58:59,767] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2232159: loss 226.8951
[2019-03-26 02:58:59,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2232159: learning rate 0.0010
[2019-03-26 02:59:01,421] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2232768: loss 313.9912
[2019-03-26 02:59:01,424] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2232769: learning rate 0.0010
[2019-03-26 02:59:04,913] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2234059: loss 0.5990
[2019-03-26 02:59:04,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2234059: learning rate 0.0010
[2019-03-26 02:59:05,926] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2234445: loss 0.7779
[2019-03-26 02:59:05,927] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2234445: learning rate 0.0010
[2019-03-26 02:59:05,953] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2234453: loss 47.9282
[2019-03-26 02:59:05,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2234453: learning rate 0.0010
[2019-03-26 02:59:07,795] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2235135: loss 0.4047
[2019-03-26 02:59:07,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2235135: learning rate 0.0010
[2019-03-26 02:59:07,882] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2235163: loss 308.1186
[2019-03-26 02:59:07,885] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2235164: learning rate 0.0010
[2019-03-26 02:59:09,707] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2235839: loss 60.0122
[2019-03-26 02:59:09,709] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2235839: learning rate 0.0010
[2019-03-26 02:59:09,863] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2235899: loss 46.2317
[2019-03-26 02:59:09,869] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2235900: learning rate 0.0010
[2019-03-26 02:59:10,488] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2236124: loss 0.3399
[2019-03-26 02:59:10,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2236125: learning rate 0.0010
[2019-03-26 02:59:10,807] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2236243: loss 34.5071
[2019-03-26 02:59:10,809] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2236243: learning rate 0.0010
[2019-03-26 02:59:12,200] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2236759: loss 45.8545
[2019-03-26 02:59:12,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2236759: learning rate 0.0010
[2019-03-26 02:59:12,230] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2236771: loss 0.1391
[2019-03-26 02:59:12,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2236771: learning rate 0.0010
[2019-03-26 02:59:12,532] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2236879: loss 62.5296
[2019-03-26 02:59:12,535] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2236879: learning rate 0.0010
[2019-03-26 02:59:13,306] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2237164: loss 0.5123
[2019-03-26 02:59:13,311] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2237164: learning rate 0.0010
[2019-03-26 02:59:14,843] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.2959963e-27], sum to 1.0000
[2019-03-26 02:59:14,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5389
[2019-03-26 02:59:14,862] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 968392.6382949733 W.
[2019-03-26 02:59:14,867] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 88.16666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.10788947262353, 6.9112, 168.9117374438924, 968392.6382949733, 828855.3639523673, 254812.8868810618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4416600.0000, 
sim time next is 4417200.0000, 
raw observation next is [29.0, 89.0, 1.0, 1.0, 0.6323869284502617, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9127963574782, 883739.4368642427, 883739.4368642421, 206933.5178375478], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.89, 1.0, 0.5, 0.5570926848798333, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439158727986, 0.24548317690673407, 0.2454831769067339, 0.3088559967724594], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.85801494], dtype=float32), 0.034259632]. 
=============================================
[2019-03-26 02:59:15,023] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.52269e-36 0.00000e+00 0.00000e+00 0.00000e+00 1.00000e+00], sum to 1.0000
[2019-03-26 02:59:15,034] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0707
[2019-03-26 02:59:15,039] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.2056040528620313, 1.0, 2.0, 0.2056040528620313, 1.0, 2.0, 0.3570662455220531, 6.911199999999999, 6.9112, 170.5573041426782, 861962.0159911927, 861962.0159911932, 271416.8275869164], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4398000.0000, 
sim time next is 4398600.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.2058743974422063, 1.0, 2.0, 0.2058743974422063, 1.0, 2.0, 0.3575357446534982, 6.9112, 6.9112, 170.5573041426782, 863095.8483479451, 863095.8483479451, 271493.7817987771], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.04322216559301961, 1.0, 1.0, 0.04322216559301961, 1.0, 1.0, 0.21650700567499778, 0.0, 0.0, 0.8375144448122397, 0.2397488467633181, 0.2397488467633181, 0.40521459969966733], 
reward next is 0.5948, 
noisyNet noise sample is [array([-0.01094226], dtype=float32), 2.0753171]. 
=============================================
[2019-03-26 02:59:15,568] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2237992: loss 0.0359
[2019-03-26 02:59:15,571] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2237992: learning rate 0.0010
[2019-03-26 02:59:21,827] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2240294: loss 0.0526
[2019-03-26 02:59:21,831] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2240295: learning rate 0.0010
[2019-03-26 02:59:22,329] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0613142e-25 8.1971100e-32 1.7108770e-38 9.6098358e-37 1.0000000e+00], sum to 1.0000
[2019-03-26 02:59:22,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0553
[2019-03-26 02:59:22,349] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 7.344961e-38], sum to 1.0000
[2019-03-26 02:59:22,355] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 84.0, 1.0, 2.0, 0.5283948593367576, 1.0, 1.0, 0.5283948593367576, 1.0, 1.0, 0.917647127817738, 6.9112, 6.9112, 170.5573041426782, 2216609.485513588, 2216609.485513588, 435338.9946829009], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4346400.0000, 
sim time next is 4347000.0000, 
raw observation next is [30.5, 84.0, 1.0, 2.0, 0.4947335175166463, 1.0, 2.0, 0.4947335175166463, 1.0, 2.0, 0.8591885090520512, 6.911200000000001, 6.9112, 170.5573041426782, 2075264.361749998, 2075264.361749998, 411525.163603899], 
processed observation next is [1.0, 0.30434782608695654, 0.6445497630331753, 0.84, 1.0, 1.0, 0.39124520182728467, 1.0, 1.0, 0.39124520182728467, 1.0, 1.0, 0.8282786695756723, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5764623227083328, 0.5764623227083328, 0.6142166620953716], 
reward next is 0.3858, 
noisyNet noise sample is [array([-0.16440476], dtype=float32), 0.39646223]. 
=============================================
[2019-03-26 02:59:22,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[32.490234]
 [32.34821 ]
 [33.781612]
 [33.16772 ]
 [32.466496]], R is [[32.31664276]
 [31.99347687]
 [31.67354202]
 [31.35680771]
 [31.04323959]].
[2019-03-26 02:59:22,366] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2148
[2019-03-26 02:59:22,377] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.16666666666666, 66.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9631587900982628, 6.9112, 6.9112, 168.912956510431, 774996.7741950011, 774996.7741950011, 237667.7152429466], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4384200.0000, 
sim time next is 4384800.0000, 
raw observation next is [32.0, 67.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9687610835094013, 6.9112, 6.9112, 168.912956510431, 779506.2616273132, 779506.2616273132, 239058.772686248], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.67, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9619037603773185, 0.0, 0.0, 0.8294399451523027, 0.2165295171186981, 0.2165295171186981, 0.3568041383376836], 
reward next is 0.6432, 
noisyNet noise sample is [array([0.5174123], dtype=float32), -1.9796412]. 
=============================================
[2019-03-26 02:59:23,429] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2240878: loss 0.0286
[2019-03-26 02:59:23,431] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2240879: learning rate 0.0010
[2019-03-26 02:59:26,271] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2241928: loss 0.0326
[2019-03-26 02:59:26,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2241928: learning rate 0.0010
[2019-03-26 02:59:27,549] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2242397: loss 0.0835
[2019-03-26 02:59:27,552] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2242398: learning rate 0.0010
[2019-03-26 02:59:27,699] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2242453: loss 0.0169
[2019-03-26 02:59:27,701] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2242453: learning rate 0.0010
[2019-03-26 02:59:29,319] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2243039: loss 0.3664
[2019-03-26 02:59:29,321] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2243039: learning rate 0.0010
[2019-03-26 02:59:29,963] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2243281: loss 0.0403
[2019-03-26 02:59:29,965] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2243282: learning rate 0.0010
[2019-03-26 02:59:31,484] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2243838: loss 0.0228
[2019-03-26 02:59:31,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2243839: learning rate 0.0010
[2019-03-26 02:59:31,581] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2243878: loss 0.0413
[2019-03-26 02:59:31,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2243878: learning rate 0.0010
[2019-03-26 02:59:32,206] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2244109: loss 0.1548
[2019-03-26 02:59:32,210] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2244110: learning rate 0.0010
[2019-03-26 02:59:32,670] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2244276: loss 0.0153
[2019-03-26 02:59:32,673] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2244276: learning rate 0.0010
[2019-03-26 02:59:34,048] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2244786: loss 1.2934
[2019-03-26 02:59:34,049] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2244786: learning rate 0.0010
[2019-03-26 02:59:34,235] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2244855: loss 0.0139
[2019-03-26 02:59:34,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2244855: learning rate 0.0010
[2019-03-26 02:59:34,367] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2244895: loss 0.0395
[2019-03-26 02:59:34,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2244895: learning rate 0.0010
[2019-03-26 02:59:35,103] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2245167: loss -7.0971
[2019-03-26 02:59:35,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2245168: learning rate 0.0010
[2019-03-26 02:59:37,468] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2246040: loss 0.2137
[2019-03-26 02:59:37,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2246042: learning rate 0.0010
[2019-03-26 02:59:40,379] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0000000e+00 7.2317757e-16 5.5357757e-29 3.3837360e-12 3.1216929e-09], sum to 1.0000
[2019-03-26 02:59:40,389] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6252
[2019-03-26 02:59:40,402] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1776615.588155795 W.
[2019-03-26 02:59:40,412] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.6353888741196771, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979968345808233, 6.9112, 168.9125295522782, 1776615.588155795, 1727829.080132699, 371836.6305951838], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4883400.0000, 
sim time next is 4884000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.6590218795968611, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989943071738095, 6.9112, 168.9124321040647, 1817787.45021719, 1761924.578430958, 376620.9778672452], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5891829874660977, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007874307173809481, 0.0, 0.8294373700769698, 0.5049409583936639, 0.48942349400859947, 0.5621208624884256], 
reward next is 0.0442, 
noisyNet noise sample is [array([-0.1041937], dtype=float32), 1.3674431]. 
=============================================
[2019-03-26 02:59:40,426] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[40.669197]
 [41.551823]
 [41.331257]
 [40.74504 ]
 [40.105488]], R is [[40.14762497]
 [39.84732819]
 [39.91241455]
 [39.8795929 ]
 [39.82607651]].
[2019-03-26 02:59:42,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 02:59:42,588] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9151
[2019-03-26 02:59:42,596] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8499529968044814, 6.911200000000001, 6.9112, 168.912956510431, 706012.3749141812, 706012.3749141805, 212285.820696267], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5034600.0000, 
sim time next is 5035200.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8539647278073613, 6.911199999999999, 6.9112, 168.912956510431, 708880.172886724, 708880.1728867247, 213152.5304202729], 
processed observation next is [0.0, 0.2608695652173913, 0.4628751974723541, 0.8566666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8219082046431234, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1969111591352011, 0.1969111591352013, 0.31813810510488494], 
reward next is 0.6819, 
noisyNet noise sample is [array([0.886889], dtype=float32), 1.3479584]. 
=============================================
[2019-03-26 02:59:43,906] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2248424: loss 0.0138
[2019-03-26 02:59:43,907] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2248424: learning rate 0.0010
[2019-03-26 02:59:45,328] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2248950: loss 0.0097
[2019-03-26 02:59:45,333] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2248952: learning rate 0.0010
[2019-03-26 02:59:47,638] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2249798: loss 0.2513
[2019-03-26 02:59:47,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2249798: learning rate 0.0010
[2019-03-26 02:59:48,181] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 02:59:48,185] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 02:59:48,187] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 02:59:48,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:59:48,189] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 02:59:48,190] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:59:48,191] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 02:59:48,188] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 02:59:48,193] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:59:48,195] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:59:48,195] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 02:59:48,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-26 02:59:48,247] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-26 02:59:48,276] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-26 02:59:48,305] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-26 02:59:48,306] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-26 02:59:57,936] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 02:59:57,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.26666666666667, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4074099094645764, 6.9112, 6.9112, 168.912956510431, 366964.4319369723, 366964.4319369723, 141214.6980911088]
[2019-03-26 02:59:57,939] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 02:59:57,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8316904881472681
[2019-03-26 03:00:02,552] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:00:02,554] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.02805647, 78.88302824333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6959925365068126, 6.911200000000001, 6.9112, 168.912956510431, 596446.0473630598, 596446.0473630592, 182014.1311315351]
[2019-03-26 03:00:02,556] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:00:02,560] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.04970371802668627
[2019-03-26 03:00:13,420] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:00:13,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.3, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5560592707694743, 6.911200000000001, 6.9112, 168.912956510431, 489782.4894617391, 489782.4894617384, 159507.7261928047]
[2019-03-26 03:00:13,424] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:00:13,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.265781795450457
[2019-03-26 03:00:30,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:00:30,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.4, 81.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9221695600746618, 6.9112, 6.9112, 168.912956510431, 753780.0319965587, 753780.0319965587, 228335.9929155416]
[2019-03-26 03:00:30,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:00:30,475] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.03100868840352211
[2019-03-26 03:01:05,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:01:05,241] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.28542296, 65.12834553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8769167902706336, 6.9112, 6.9112, 168.912956510431, 728488.5389252696, 728488.5389252696, 218295.1320691069]
[2019-03-26 03:01:05,243] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:01:05,246] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4091746940211095
[2019-03-26 03:01:16,832] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:01:16,834] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.83333333333333, 74.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.969159873246347, 6.9112, 6.9112, 168.912956510431, 788070.7511735666, 788070.7511735666, 239601.8980340631]
[2019-03-26 03:01:16,835] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:01:16,842] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5001452359447137
[2019-03-26 03:01:42,940] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8836791], dtype=float32), 0.24419515]
[2019-03-26 03:01:42,941] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.11666666666667, 50.66666666666667, 1.0, 2.0, 0.4527599173603853, 1.0, 2.0, 0.4527599173603853, 1.0, 2.0, 0.7542989436209705, 6.911200000000001, 6.9112, 170.5573041426782, 1899041.170851052, 1899041.170851051, 378661.8147816351]
[2019-03-26 03:01:42,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:01:42,945] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9666435e-01 7.3148219e-23 3.8689683e-26 4.5491584e-12 3.3356098e-03], sampled 0.8975590882159186
[2019-03-26 03:01:42,946] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1899041.170851052 W.
[2019-03-26 03:01:57,018] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8192.3909 2947273475.9902 1013.0000
[2019-03-26 03:01:57,298] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7536.0087 3316501230.0213 1407.0000
[2019-03-26 03:01:57,597] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7538.6775 3115927021.2328 1580.0000
[2019-03-26 03:01:57,861] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8121.2883 2990726723.1256 1012.0000
[2019-03-26 03:01:57,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7386.4690 3180191963.0732 1508.0000
[2019-03-26 03:01:58,922] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 2250000, evaluation results [2250000.0, 7536.0087286325825, 3316501230.021323, 1407.0, 7538.677524872136, 3115927021.2327743, 1580.0, 8192.390889216746, 2947273475.990243, 1013.0, 7386.468981649993, 3180191963.0732236, 1508.0, 8121.288257539541, 2990726723.1256347, 1012.0]
[2019-03-26 03:01:59,555] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2250237: loss 0.2283
[2019-03-26 03:01:59,557] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2250237: learning rate 0.0010
[2019-03-26 03:02:00,441] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2250558: loss 0.3355
[2019-03-26 03:02:00,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2250558: learning rate 0.0010
[2019-03-26 03:02:01,366] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2250902: loss 0.2132
[2019-03-26 03:02:01,370] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2250903: learning rate 0.0010
[2019-03-26 03:02:02,346] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2251265: loss 0.1390
[2019-03-26 03:02:02,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2251265: learning rate 0.0010
[2019-03-26 03:02:04,123] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2251924: loss -5.2448
[2019-03-26 03:02:04,125] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2251925: learning rate 0.0010
[2019-03-26 03:02:04,182] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2251946: loss -40.3464
[2019-03-26 03:02:04,184] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2251946: learning rate 0.0010
[2019-03-26 03:02:04,417] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2252033: loss 0.3460
[2019-03-26 03:02:04,420] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2252033: learning rate 0.0010
[2019-03-26 03:02:04,592] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:02:04,601] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1722
[2019-03-26 03:02:04,605] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.16666666666667, 65.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.931266580816203, 6.9112, 6.9112, 168.912956510431, 759881.1769350927, 759881.1769350927, 230448.0481031945], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5158200.0000, 
sim time next is 5158800.0000, 
raw observation next is [31.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9272235978710114, 6.9112, 6.9112, 168.912956510431, 757327.4045932451, 757327.4045932451, 229514.1502672735], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9112482900865991, 0.0, 0.0, 0.8294399451523027, 0.21036872349812363, 0.21036872349812363, 0.34255843323473656], 
reward next is 0.6574, 
noisyNet noise sample is [array([0.4445966], dtype=float32), -0.62781733]. 
=============================================
[2019-03-26 03:02:05,138] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2252300: loss 1.3992
[2019-03-26 03:02:05,140] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2252300: learning rate 0.0010
[2019-03-26 03:02:06,140] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2252666: loss 0.6465
[2019-03-26 03:02:06,143] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2252667: learning rate 0.0010
[2019-03-26 03:02:06,546] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2252819: loss 2.8940
[2019-03-26 03:02:06,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2252819: learning rate 0.0010
[2019-03-26 03:02:06,674] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2252870: loss 0.2497
[2019-03-26 03:02:06,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2252870: learning rate 0.0010
[2019-03-26 03:02:07,168] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2253053: loss 0.5369
[2019-03-26 03:02:07,170] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2253053: learning rate 0.0010
[2019-03-26 03:02:07,201] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5123981e-01 4.9426702e-15 3.0107650e-21 2.4240758e-02 3.2451937e-01], sum to 1.0000
[2019-03-26 03:02:07,214] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7487
[2019-03-26 03:02:07,218] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.9544053001375141, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005990875815419, 6.9112, 168.9123929860509, 2231185.603700744, 2163937.918298394, 449581.1122196271], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5236200.0000, 
sim time next is 5236800.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6007477733698486, 1.0, 1.0, 0.6007477733698486, 1.0, 2.0, 1.03, 6.92615147085797, 6.9112, 170.5573041426782, 2520435.095072931, 2509724.748310417, 488249.7541219486], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5189732209275284, 1.0, 0.5, 0.5189732209275284, 1.0, 1.0, 1.0365853658536586, 0.0014951470857970327, 0.0, 0.8375144448122397, 0.7001208597424808, 0.6971457634195602, 0.7287309763014158], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0239506], dtype=float32), -0.4187054]. 
=============================================
[2019-03-26 03:02:07,546] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7767005e-02 8.8667186e-19 3.3740426e-32 7.4688177e-23 9.8223305e-01], sum to 1.0000
[2019-03-26 03:02:07,556] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8401
[2019-03-26 03:02:07,563] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.2515362030903055, 1.0, 2.0, 0.2515362030903055, 1.0, 2.0, 0.4256758422981889, 6.9112, 6.9112, 170.5573041426782, 1054619.878135367, 1054619.878135367, 285067.7946480951], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5201400.0000, 
sim time next is 5202000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.2454122070943991, 1.0, 2.0, 0.2454122070943991, 1.0, 2.0, 0.4152655382206306, 6.9112, 6.9112, 170.5573041426782, 1028931.382609454, 1028931.382609454, 283009.5285835302], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.0908580808366254, 1.0, 1.0, 0.0908580808366254, 1.0, 1.0, 0.2869091929519885, 0.0, 0.0, 0.8375144448122397, 0.28581427294707057, 0.28581427294707057, 0.42240228146795555], 
reward next is 0.5776, 
noisyNet noise sample is [array([0.20977348], dtype=float32), -1.6303422]. 
=============================================
[2019-03-26 03:02:07,584] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[51.969627]
 [51.727295]
 [51.428246]
 [51.07093 ]
 [50.632866]], R is [[52.13040161]
 [52.18362045]
 [52.23514938]
 [52.2856102 ]
 [51.76275635]].
[2019-03-26 03:02:07,947] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:02:07,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1635
[2019-03-26 03:02:07,968] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.5, 76.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8176168566411297, 6.9112, 6.9112, 168.912956510431, 683380.2914506069, 683380.2914506069, 205458.0801890971], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4912200.0000, 
sim time next is 4912800.0000, 
raw observation next is [27.33333333333334, 77.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8138702582363103, 6.911200000000001, 6.9112, 168.912956510431, 680539.9540463309, 680539.9540463302, 204678.0128299078], 
processed observation next is [1.0, 0.8695652173913043, 0.4944707740916275, 0.7733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7730125100442807, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1890388761239808, 0.1890388761239806, 0.3054895713879221], 
reward next is 0.6945, 
noisyNet noise sample is [array([-0.25513396], dtype=float32), -0.583318]. 
=============================================
[2019-03-26 03:02:09,743] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2253994: loss 0.0526
[2019-03-26 03:02:09,747] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2253995: learning rate 0.0010
[2019-03-26 03:02:16,413] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2256456: loss 0.4984
[2019-03-26 03:02:16,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2256457: learning rate 0.0010
[2019-03-26 03:02:17,545] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2256880: loss 0.1102
[2019-03-26 03:02:17,549] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2256880: learning rate 0.0010
[2019-03-26 03:02:20,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:02:20,445] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6178
[2019-03-26 03:02:20,451] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8157692213545391, 6.9112, 6.9112, 168.912956510431, 681741.7675323692, 681741.7675323692, 205067.0659627843], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5020800.0000, 
sim time next is 5021400.0000, 
raw observation next is [26.0, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.823854866335233, 6.911199999999999, 6.9112, 168.912956510431, 687391.9707137512, 687391.9707137519, 206746.1471338063], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7851888613844304, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1909422140871531, 0.19094221408715328, 0.30857633900568104], 
reward next is 0.6914, 
noisyNet noise sample is [array([-0.48862708], dtype=float32), 1.6674839]. 
=============================================
[2019-03-26 03:02:20,472] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2257952: loss 25.2730
[2019-03-26 03:02:20,476] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2257954: learning rate 0.0010
[2019-03-26 03:02:21,767] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2258435: loss -43.7929
[2019-03-26 03:02:21,771] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2258435: learning rate 0.0010
[2019-03-26 03:02:21,779] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2258437: loss 0.1182
[2019-03-26 03:02:21,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2258438: learning rate 0.0010
[2019-03-26 03:02:23,572] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2259101: loss 40.8637
[2019-03-26 03:02:23,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2259101: learning rate 0.0010
[2019-03-26 03:02:23,726] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2259157: loss 0.1879
[2019-03-26 03:02:23,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2259157: learning rate 0.0010
[2019-03-26 03:02:24,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:02:24,679] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8461
[2019-03-26 03:02:24,689] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.947131055123442, 6.9112, 6.9112, 168.912956510431, 769718.4977741437, 769718.4977741437, 234138.9571091012], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5154600.0000, 
sim time next is 5155200.0000, 
raw observation next is [32.0, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9484951232183607, 6.911199999999999, 6.9112, 168.912956510431, 770827.4566769394, 770827.45667694, 234472.0666952817], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9371891746565375, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21411873796581649, 0.21411873796581665, 0.3499583085004205], 
reward next is 0.6500, 
noisyNet noise sample is [array([2.957013], dtype=float32), 0.41952434]. 
=============================================
[2019-03-26 03:02:25,498] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2259798: loss 0.1790
[2019-03-26 03:02:25,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2259799: learning rate 0.0010
[2019-03-26 03:02:25,590] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2259835: loss 0.2060
[2019-03-26 03:02:25,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2259835: learning rate 0.0010
[2019-03-26 03:02:26,395] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2260133: loss 0.0599
[2019-03-26 03:02:26,397] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2260133: learning rate 0.0010
[2019-03-26 03:02:26,646] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2260223: loss -31.5694
[2019-03-26 03:02:26,649] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2260223: learning rate 0.0010
[2019-03-26 03:02:27,537] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2260554: loss 0.0809
[2019-03-26 03:02:27,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2260554: learning rate 0.0010
[2019-03-26 03:02:27,785] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2260642: loss 0.0450
[2019-03-26 03:02:27,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2260643: learning rate 0.0010
[2019-03-26 03:02:28,543] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2260925: loss 112.0300
[2019-03-26 03:02:28,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2260925: learning rate 0.0010
[2019-03-26 03:02:29,819] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2261392: loss -13.6740
[2019-03-26 03:02:29,824] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2261392: learning rate 0.0010
[2019-03-26 03:02:32,489] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2262376: loss -10.5397
[2019-03-26 03:02:32,492] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2262377: learning rate 0.0010
[2019-03-26 03:02:37,384] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:02:37,398] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9515
[2019-03-26 03:02:37,404] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.96666666666667, 78.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.926095016556647, 6.9112, 168.9126923377479, 839372.0604201736, 828805.0395543796, 254812.0310056191], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5515800.0000, 
sim time next is 5516400.0000, 
raw observation next is [29.83333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.945291411364861, 6.9112, 168.9125555715127, 852995.916436522, 828810.3532864732, 254811.9736350152], 
processed observation next is [1.0, 0.8695652173913043, 0.6129541864139023, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.003409141136486138, 0.0, 0.8294379763586632, 0.23694331012125613, 0.23022509813513145, 0.38031637855972417], 
reward next is 0.4492, 
noisyNet noise sample is [array([-0.63682234], dtype=float32), 0.5753267]. 
=============================================
[2019-03-26 03:02:38,558] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2264621: loss 9.1767
[2019-03-26 03:02:38,563] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2264621: learning rate 0.0010
[2019-03-26 03:02:39,578] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2264994: loss 47.2530
[2019-03-26 03:02:39,580] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2264994: learning rate 0.0010
[2019-03-26 03:02:41,839] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2265834: loss 23.7170
[2019-03-26 03:02:41,842] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2265834: learning rate 0.0010
[2019-03-26 03:02:43,068] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2266280: loss 17.9690
[2019-03-26 03:02:43,071] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2266280: learning rate 0.0010
[2019-03-26 03:02:43,696] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2266513: loss -50.1573
[2019-03-26 03:02:43,700] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2266513: learning rate 0.0010
[2019-03-26 03:02:44,783] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2266910: loss 16.9650
[2019-03-26 03:02:44,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2266911: learning rate 0.0010
[2019-03-26 03:02:45,442] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2267151: loss -50.2436
[2019-03-26 03:02:45,447] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2267153: learning rate 0.0010
[2019-03-26 03:02:47,128] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2267768: loss 1.5847
[2019-03-26 03:02:47,132] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2267770: learning rate 0.0010
[2019-03-26 03:02:47,243] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2267812: loss -9.7605
[2019-03-26 03:02:47,245] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2267812: learning rate 0.0010
[2019-03-26 03:02:47,572] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.8912886e-19], sum to 1.0000
[2019-03-26 03:02:47,582] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3551
[2019-03-26 03:02:47,588] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.78333333333333, 91.66666666666667, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 0.9330718825864773, 6.9112, 6.9112, 168.912956510431, 760467.5199307594, 760467.5199307594, 230839.9089444483], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5532600.0000, 
sim time next is 5533200.0000, 
raw observation next is [26.7, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9288307316488573, 6.9112, 6.9112, 168.912956510431, 759076.3129868737, 759076.3129868737, 229918.4591819449], 
processed observation next is [1.0, 0.043478260869565216, 0.46445497630331756, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9132082093278748, 0.0, 0.0, 0.8294399451523027, 0.21085453138524268, 0.21085453138524268, 0.34316187937603715], 
reward next is 0.6568, 
noisyNet noise sample is [array([0.0956516], dtype=float32), 0.70246977]. 
=============================================
[2019-03-26 03:02:47,931] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2268062: loss 18.0795
[2019-03-26 03:02:47,937] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2268064: learning rate 0.0010
[2019-03-26 03:02:48,080] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2268117: loss -0.4235
[2019-03-26 03:02:48,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2268117: learning rate 0.0010
[2019-03-26 03:02:49,254] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2268551: loss 0.1566
[2019-03-26 03:02:49,260] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2268552: learning rate 0.0010
[2019-03-26 03:02:49,315] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2268571: loss 1.7156
[2019-03-26 03:02:49,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2268572: learning rate 0.0010
[2019-03-26 03:02:50,116] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2268868: loss 86.5128
[2019-03-26 03:02:50,118] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2268868: learning rate 0.0010
[2019-03-26 03:02:51,268] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2269289: loss 41.3202
[2019-03-26 03:02:51,271] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2269290: learning rate 0.0010
[2019-03-26 03:02:53,008] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 6.3377306e-35], sum to 1.0000
[2019-03-26 03:02:53,017] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7586
[2019-03-26 03:02:53,030] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.1, 71.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9889160853127178, 6.9112, 6.9112, 168.912956510431, 795729.9057151967, 795729.9057151967, 244132.7351148071], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5512800.0000, 
sim time next is 5513400.0000, 
raw observation next is [30.85, 73.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9988876843005822, 6.9112, 6.9112, 168.9128768433796, 803756.5744391159, 803756.5744391159, 246683.2889503516], 
processed observation next is [1.0, 0.8260869565217391, 0.661137440758294, 0.73, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9986435174397342, 0.0, 0.0, 0.8294395539506085, 0.22326571512197665, 0.22326571512197665, 0.36818401335873374], 
reward next is 0.6318, 
noisyNet noise sample is [array([0.2601727], dtype=float32), -0.19456165]. 
=============================================
[2019-03-26 03:02:53,903] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.8911818e-36], sum to 1.0000
[2019-03-26 03:02:53,917] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1083
[2019-03-26 03:02:53,924] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.15, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9765639597646443, 6.9112, 6.9112, 168.912956510431, 790290.1059167754, 790290.1059167754, 241261.0818549802], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5862600.0000, 
sim time next is 5863200.0000, 
raw observation next is [28.06666666666666, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9705989318847396, 6.9112, 6.9112, 168.912956510431, 786186.2365035978, 786186.2365035978, 239803.7572117984], 
processed observation next is [1.0, 0.8695652173913043, 0.5292259083728275, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9641450388838286, 0.0, 0.0, 0.8294399451523027, 0.21838506569544383, 0.21838506569544383, 0.3579160555399976], 
reward next is 0.6421, 
noisyNet noise sample is [array([-0.76524323], dtype=float32), -0.24790093]. 
=============================================
[2019-03-26 03:02:54,258] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2270389: loss 17.6429
[2019-03-26 03:02:54,260] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2270389: learning rate 0.0010
[2019-03-26 03:03:00,047] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2272516: loss 14.7027
[2019-03-26 03:03:00,050] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2272516: learning rate 0.0010
[2019-03-26 03:03:01,193] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2272934: loss 16.1061
[2019-03-26 03:03:01,197] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2272934: learning rate 0.0010
[2019-03-26 03:03:03,269] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2273708: loss -160.6447
[2019-03-26 03:03:03,274] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2273709: learning rate 0.0010
[2019-03-26 03:03:04,957] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2274329: loss -113.8422
[2019-03-26 03:03:04,960] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2274329: learning rate 0.0010
[2019-03-26 03:03:05,531] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2274541: loss 13.3676
[2019-03-26 03:03:05,535] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2274541: learning rate 0.0010
[2019-03-26 03:03:05,798] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1838082e-16 1.8285819e-33 1.9328986e-35 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 03:03:05,810] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7670
[2019-03-26 03:03:05,818] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333333, 67.66666666666667, 1.0, 2.0, 0.8411502123195247, 1.0, 2.0, 0.8411502123195247, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2352537.202462593, 2352537.202462593, 440403.0082632223], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6021600.0000, 
sim time next is 6022200.0000, 
raw observation next is [31.16666666666667, 66.83333333333333, 1.0, 2.0, 0.8258281161339748, 1.0, 2.0, 0.8258281161339748, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2309644.612822159, 2309644.612822159, 432640.5750508489], 
processed observation next is [1.0, 0.6956521739130435, 0.6761453396524489, 0.6683333333333333, 1.0, 1.0, 0.7901543567879215, 1.0, 1.0, 0.7901543567879215, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6415679480061554, 0.6415679480061554, 0.6457322015684311], 
reward next is 0.3543, 
noisyNet noise sample is [array([-0.7394474], dtype=float32), 1.2681726]. 
=============================================
[2019-03-26 03:03:06,425] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2274866: loss -54.8673
[2019-03-26 03:03:06,428] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2274866: learning rate 0.0010
[2019-03-26 03:03:06,779] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 03:03:06,783] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:03:06,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:03:06,785] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:03:06,786] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:03:06,787] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:03:06,789] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:03:06,790] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:03:06,792] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:03:06,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:03:06,796] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:03:06,833] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-26 03:03:06,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-26 03:03:06,888] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-26 03:03:06,918] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-26 03:03:06,919] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-26 03:03:18,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:03:18,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.8, 59.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4789776633166212, 6.911200000000001, 6.9112, 168.912956510431, 427280.1186530145, 427280.1186530138, 149271.9635630158]
[2019-03-26 03:03:18,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:03:18,740] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.401204559974891
[2019-03-26 03:03:26,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:03:26,278] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.53333333333333, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.519484306506227, 6.9112, 6.9112, 168.912956510431, 460038.8700304418, 460038.8700304418, 154472.4828185282]
[2019-03-26 03:03:26,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:03:26,286] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7416994799653435
[2019-03-26 03:04:47,482] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:04:47,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.08333333333333, 65.0, 1.0, 2.0, 0.7176445611228975, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.988009338266526, 6.9112, 168.9115554520739, 1899820.07765563, 1845329.341496288, 389188.879264807]
[2019-03-26 03:04:47,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:04:47,489] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 2.0086634e-34 9.5957567e-32 2.6183876e-15 4.3117875e-24], sampled 0.7884452328457249
[2019-03-26 03:04:47,493] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1899820.07765563 W.
[2019-03-26 03:04:58,073] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:04:58,076] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.631165714558774, 6.9112, 6.9112, 168.912956510431, 547326.3541794426, 547326.3541794426, 170987.5684242611]
[2019-03-26 03:04:58,076] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:04:58,081] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.03391437470661551
[2019-03-26 03:05:00,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:05:00,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.71163974, 86.82250326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6248889426789515, 6.9112, 6.9112, 168.912956510431, 544038.568743808, 544038.568743808, 169950.6478610327]
[2019-03-26 03:05:00,127] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:05:00,129] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.16532296061236196
[2019-03-26 03:05:09,949] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:05:09,951] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.43333333333333, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6270357225006589, 6.9112, 6.9112, 168.912956510431, 542917.393203109, 542917.393203109, 170336.8363246744]
[2019-03-26 03:05:09,953] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:05:09,956] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.09578597670680633
[2019-03-26 03:05:15,190] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.65400237], dtype=float32), 0.24305104]
[2019-03-26 03:05:15,191] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.6, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5917053981446007, 6.911199999999999, 6.9112, 168.912956510431, 515483.2401744344, 515483.240174435, 164822.130318689]
[2019-03-26 03:05:15,192] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:05:15,195] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3466221190018688
[2019-03-26 03:05:16,227] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8248.3311 2950743483.5522 858.0000
[2019-03-26 03:05:16,233] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7488.6598 3171174298.6667 1290.0000
[2019-03-26 03:05:16,515] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8173.1134 2993315854.1703 858.0000
[2019-03-26 03:05:16,732] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7668.3327 3109824456.9423 1315.0000
[2019-03-26 03:05:16,781] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7602.6072 3311682599.4748 1167.0000
[2019-03-26 03:05:17,801] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2275000, evaluation results [2275000.0, 7602.607168416947, 3311682599.474823, 1167.0, 7668.332697617111, 3109824456.9423227, 1315.0, 8248.3311349599, 2950743483.5522237, 858.0, 7488.659787677618, 3171174298.666662, 1290.0, 8173.113361709362, 2993315854.1702867, 858.0]
[2019-03-26 03:05:18,229] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2275162: loss 14.3114
[2019-03-26 03:05:18,234] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2275162: learning rate 0.0010
[2019-03-26 03:05:19,834] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2275755: loss 17.3920
[2019-03-26 03:05:19,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2275756: learning rate 0.0010
[2019-03-26 03:05:20,129] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2275864: loss 15.7839
[2019-03-26 03:05:20,131] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2275864: learning rate 0.0010
[2019-03-26 03:05:20,642] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2276056: loss 18.7695
[2019-03-26 03:05:20,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2276056: learning rate 0.0010
[2019-03-26 03:05:21,177] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2276252: loss -149.9383
[2019-03-26 03:05:21,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2276252: learning rate 0.0010
[2019-03-26 03:05:21,793] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2276476: loss 20.8666
[2019-03-26 03:05:21,798] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2276478: learning rate 0.0010
[2019-03-26 03:05:21,954] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2276540: loss 20.8288
[2019-03-26 03:05:21,961] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2276541: learning rate 0.0010
[2019-03-26 03:05:23,359] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2277054: loss -213.0049
[2019-03-26 03:05:23,365] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2277057: learning rate 0.0010
[2019-03-26 03:05:24,389] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2277441: loss -120.2900
[2019-03-26 03:05:24,391] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2277441: learning rate 0.0010
[2019-03-26 03:05:27,279] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2278501: loss -140.5377
[2019-03-26 03:05:27,282] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2278502: learning rate 0.0010
[2019-03-26 03:05:32,397] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 3.3538390e-23 1.6648959e-36 3.4789333e-10 2.5643234e-18], sum to 1.0000
[2019-03-26 03:05:32,409] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9417
[2019-03-26 03:05:32,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1967720.992613962 W.
[2019-03-26 03:05:32,424] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.8, 73.0, 1.0, 2.0, 0.7036787921631141, 1.0, 2.0, 0.7036787921631141, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1967720.992613962, 1967720.992613963, 375703.6917310159], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6082200.0000, 
sim time next is 6082800.0000, 
raw observation next is [29.93333333333333, 72.0, 1.0, 2.0, 0.8389918281442533, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.992582603566376, 6.9112, 168.9124723670309, 2069645.387160034, 2011909.932700572, 418203.9694227622], 
processed observation next is [1.0, 0.391304347826087, 0.6176935229067929, 0.72, 1.0, 1.0, 0.8060142507762088, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008138260356637606, 0.0, 0.8294375677865663, 0.5749014964333428, 0.5588638701946034, 0.6241850289891974], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.65954125], dtype=float32), 1.4067783]. 
=============================================
[2019-03-26 03:05:32,858] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2280553: loss 10.2271
[2019-03-26 03:05:32,864] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2280554: learning rate 0.0010
[2019-03-26 03:05:34,216] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2281057: loss -2.1845
[2019-03-26 03:05:34,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2281057: learning rate 0.0010
[2019-03-26 03:05:34,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.0975804e-09 8.8322744e-24 3.2338512e-26 1.0000000e+00 1.7841527e-18], sum to 1.0000
[2019-03-26 03:05:34,257] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9476
[2019-03-26 03:05:34,268] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.65, 76.0, 1.0, 2.0, 0.8874392057886283, 1.0, 2.0, 0.8874392057886283, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2482127.292766787, 2482127.292766787, 464690.3272073591], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5909400.0000, 
sim time next is 5910000.0000, 
raw observation next is [30.8, 75.33333333333333, 1.0, 2.0, 0.852626335414149, 1.0, 2.0, 0.852626335414149, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2384664.361036218, 2384664.361036218, 446311.3829485683], 
processed observation next is [1.0, 0.391304347826087, 0.6587677725118484, 0.7533333333333333, 1.0, 1.0, 0.8224413679688543, 1.0, 1.0, 0.8224413679688543, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.662406766954505, 0.662406766954505, 0.6661363924605498], 
reward next is 0.3339, 
noisyNet noise sample is [array([0.6192903], dtype=float32), -0.7759948]. 
=============================================
[2019-03-26 03:05:34,286] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[32.592155]
 [31.581047]
 [31.004274]
 [31.17121 ]
 [31.055487]], R is [[33.22408295]
 [33.19827271]
 [32.86629105]
 [32.53762817]
 [32.60503006]].
[2019-03-26 03:05:35,808] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2281646: loss 0.8610
[2019-03-26 03:05:35,811] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2281646: learning rate 0.0010
[2019-03-26 03:05:37,592] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2282304: loss 0.8645
[2019-03-26 03:05:37,594] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2282304: learning rate 0.0010
[2019-03-26 03:05:38,397] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2282601: loss -56.9094
[2019-03-26 03:05:38,398] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2282601: learning rate 0.0010
[2019-03-26 03:05:38,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:05:38,701] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4429
[2019-03-26 03:05:38,707] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [29.1, 82.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9826341904801045, 6.9112, 6.9112, 168.912956510431, 791824.1445646493, 791824.1445646493, 242605.5220796459], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5943600.0000, 
sim time next is 5944200.0000, 
raw observation next is [29.0, 82.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9821120473517954, 6.9112, 6.9112, 168.912956510431, 792268.2266041666, 792268.2266041666, 242522.4151337024], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.8216666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9781854235997504, 0.0, 0.0, 0.8294399451523027, 0.22007450739004628, 0.22007450739004628, 0.3619737539308991], 
reward next is 0.6380, 
noisyNet noise sample is [array([-0.34009048], dtype=float32), 0.2775914]. 
=============================================
[2019-03-26 03:05:38,712] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2282717: loss 1.6316
[2019-03-26 03:05:38,715] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2282718: learning rate 0.0010
[2019-03-26 03:05:39,872] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2283141: loss -45.0061
[2019-03-26 03:05:39,874] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2283141: learning rate 0.0010
[2019-03-26 03:05:41,635] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2283791: loss -145.4427
[2019-03-26 03:05:41,639] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2283793: learning rate 0.0010
[2019-03-26 03:05:41,964] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2283917: loss -69.2782
[2019-03-26 03:05:41,966] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2283917: learning rate 0.0010
[2019-03-26 03:05:42,293] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2284038: loss -136.8225
[2019-03-26 03:05:42,297] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2284039: learning rate 0.0010
[2019-03-26 03:05:42,463] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2284097: loss 0.4371
[2019-03-26 03:05:42,464] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2284097: learning rate 0.0010
[2019-03-26 03:05:43,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1568124e-10 2.2910286e-21 5.6990806e-22 1.7411660e-01 8.2588339e-01], sum to 1.0000
[2019-03-26 03:05:43,233] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3457
[2019-03-26 03:05:43,239] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.590384035761639, 1.0, 2.0, 0.590384035761639, 1.0, 2.0, 1.025301827133853, 6.911199999999999, 6.9112, 170.5573041426782, 2476910.975602532, 2476910.975602532, 483274.6993628861], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6004800.0000, 
sim time next is 6005400.0000, 
raw observation next is [32.0, 68.16666666666667, 1.0, 2.0, 0.6099897555118943, 1.0, 2.0, 0.6099897555118943, 1.0, 2.0, 1.03, 6.944194935304375, 6.9112, 170.5573041426782, 2559249.507777505, 2535613.893470824, 491629.5106779084], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.6816666666666668, 1.0, 1.0, 0.5301081391709569, 1.0, 1.0, 0.5301081391709569, 1.0, 1.0, 1.0365853658536586, 0.0032994935304374805, 0.0, 0.8375144448122397, 0.710902641049307, 0.7043371926307845, 0.7337753890715051], 
reward next is 0.1012, 
noisyNet noise sample is [array([1.2179155], dtype=float32), -0.6822161]. 
=============================================
[2019-03-26 03:05:43,689] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2284546: loss -153.8243
[2019-03-26 03:05:43,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2284546: learning rate 0.0010
[2019-03-26 03:05:43,776] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2284582: loss -271.3225
[2019-03-26 03:05:43,780] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2284583: learning rate 0.0010
[2019-03-26 03:05:43,799] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:05:43,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5493
[2019-03-26 03:05:43,814] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.66666666666667, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8912235277635934, 6.9112, 6.9112, 168.912956510431, 733278.3325510502, 733278.3325510502, 221308.3766753096], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6239400.0000, 
sim time next is 6240000.0000, 
raw observation next is [26.73333333333334, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8944886974673109, 6.9112, 6.9112, 168.912956510431, 735579.0213199387, 735579.0213199387, 222044.7849633827], 
processed observation next is [0.0, 0.21739130434782608, 0.4660347551342816, 0.8966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8713276798381838, 0.0, 0.0, 0.8294399451523027, 0.2043275059222052, 0.2043275059222052, 0.33141012681101895], 
reward next is 0.6686, 
noisyNet noise sample is [array([0.26091945], dtype=float32), 0.5574957]. 
=============================================
[2019-03-26 03:05:43,823] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.67335]
 [69.67109]
 [69.47149]
 [69.49597]
 [69.66953]], R is [[69.62439728]
 [69.59783936]
 [69.57228088]
 [69.54670715]
 [69.52110291]].
[2019-03-26 03:05:44,908] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2284999: loss 0.0598
[2019-03-26 03:05:44,912] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2284999: learning rate 0.0010
[2019-03-26 03:05:45,989] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2285397: loss 0.0057
[2019-03-26 03:05:45,992] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2285397: learning rate 0.0010
[2019-03-26 03:05:46,932] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.0578668e-20 2.0736654e-28 1.0633870e-29 6.4518925e-20 1.0000000e+00], sum to 1.0000
[2019-03-26 03:05:46,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4745
[2019-03-26 03:05:46,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.8, 68.0, 1.0, 2.0, 0.5327147099862385, 1.0, 2.0, 0.5327147099862385, 1.0, 2.0, 0.9251492797994463, 6.9112, 6.9112, 170.5573041426782, 2234747.403872366, 2234747.403872366, 438505.7016466676], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6184800.0000, 
sim time next is 6185400.0000, 
raw observation next is [30.7, 68.5, 1.0, 2.0, 0.567872091060716, 1.0, 2.0, 0.567872091060716, 1.0, 2.0, 0.9862060240022257, 6.911199999999999, 6.9112, 170.5573041426782, 2382373.803769802, 2382373.803769803, 465252.888236765], 
processed observation next is [1.0, 0.6086956521739131, 0.6540284360189573, 0.685, 1.0, 1.0, 0.47936396513339274, 1.0, 1.0, 0.47936396513339274, 1.0, 1.0, 0.9831780780514947, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6617705010471673, 0.6617705010471675, 0.6944072958757687], 
reward next is 0.3056, 
noisyNet noise sample is [array([0.54896426], dtype=float32), 1.1376]. 
=============================================
[2019-03-26 03:05:47,934] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.37720474e-14 3.31016018e-34 1.05178665e-32 3.82988278e-23
 1.00000000e+00], sum to 1.0000
[2019-03-26 03:05:47,944] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5197
[2019-03-26 03:05:47,952] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.96666666666667, 68.33333333333333, 1.0, 2.0, 0.548133413064176, 1.0, 1.0, 0.548133413064176, 1.0, 2.0, 0.941434465495896, 6.9112, 6.9112, 170.5573041426782, 2299488.635335841, 2299488.635335841, 447918.9237448641], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6442800.0000, 
sim time next is 6443400.0000, 
raw observation next is [29.98333333333333, 68.16666666666667, 1.0, 2.0, 0.545594643611955, 1.0, 2.0, 0.545594643611955, 1.0, 2.0, 0.9380462716857967, 6.9112, 6.9112, 170.5573041426782, 2288828.425053081, 2288828.425053081, 446215.7488482381], 
processed observation next is [1.0, 0.5652173913043478, 0.6200631911532385, 0.6816666666666668, 1.0, 1.0, 0.45252366700235536, 1.0, 1.0, 0.45252366700235536, 1.0, 1.0, 0.924446672787557, 0.0, 0.0, 0.8375144448122397, 0.6357856736258558, 0.6357856736258558, 0.6659936549973703], 
reward next is 0.3340, 
noisyNet noise sample is [array([0.29847044], dtype=float32), 0.37523678]. 
=============================================
[2019-03-26 03:05:48,778] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2286420: loss 0.3416
[2019-03-26 03:05:48,787] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2286423: learning rate 0.0010
[2019-03-26 03:05:54,644] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2288581: loss 1.1897
[2019-03-26 03:05:54,645] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2288581: learning rate 0.0010
[2019-03-26 03:05:55,498] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2288896: loss 0.7154
[2019-03-26 03:05:55,500] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2288897: learning rate 0.0010
[2019-03-26 03:05:57,804] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2289744: loss 16.9021
[2019-03-26 03:05:57,807] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2289744: learning rate 0.0010
[2019-03-26 03:05:59,416] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2290339: loss 152.4617
[2019-03-26 03:05:59,417] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2290339: learning rate 0.0010
[2019-03-26 03:05:59,595] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2290406: loss 1.9375
[2019-03-26 03:05:59,597] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2290406: learning rate 0.0010
[2019-03-26 03:06:00,535] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:06:00,545] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3292
[2019-03-26 03:06:00,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2273038.024784493 W.
[2019-03-26 03:06:00,557] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.91666666666667, 84.16666666666667, 1.0, 2.0, 0.9843077198627057, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.993274767326391, 6.9112, 168.912468668949, 2273038.024784493, 2214811.528213466, 458991.8654571146], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6624600.0000, 
sim time next is 6625200.0000, 
raw observation next is [27.83333333333334, 84.33333333333334, 1.0, 2.0, 0.5593117278232739, 1.0, 1.0, 0.5593117278232739, 1.0, 2.0, 0.9678172705400072, 6.9112, 6.9112, 170.5573041426782, 2346427.096788088, 2346427.096788088, 457864.5273490206], 
processed observation next is [1.0, 0.6956521739130435, 0.5181674565560824, 0.8433333333333334, 1.0, 1.0, 0.4690502744858721, 1.0, 0.5, 0.4690502744858721, 1.0, 1.0, 0.9607527689512283, 0.0, 0.0, 0.8375144448122397, 0.6517853046633577, 0.6517853046633577, 0.6833798915657023], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.45612988], dtype=float32), -0.64923745]. 
=============================================
[2019-03-26 03:06:00,628] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2290788: loss 42.4042
[2019-03-26 03:06:00,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2290789: learning rate 0.0010
[2019-03-26 03:06:01,327] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2291046: loss 0.6742
[2019-03-26 03:06:01,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2291049: learning rate 0.0010
[2019-03-26 03:06:03,259] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2291757: loss 0.0381
[2019-03-26 03:06:03,263] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2291758: learning rate 0.0010
[2019-03-26 03:06:03,295] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2291768: loss 0.0105
[2019-03-26 03:06:03,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2291768: learning rate 0.0010
[2019-03-26 03:06:03,918] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2292000: loss 0.0082
[2019-03-26 03:06:03,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2292000: learning rate 0.0010
[2019-03-26 03:06:04,672] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2292281: loss 71.2600
[2019-03-26 03:06:04,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2292281: learning rate 0.0010
[2019-03-26 03:06:05,211] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2292486: loss 0.1011
[2019-03-26 03:06:05,214] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2292487: learning rate 0.0010
[2019-03-26 03:06:05,274] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2292505: loss 0.0480
[2019-03-26 03:06:05,275] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2292506: learning rate 0.0010
[2019-03-26 03:06:07,393] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2293285: loss 116.1434
[2019-03-26 03:06:07,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2293285: learning rate 0.0010
[2019-03-26 03:06:07,605] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:06:07,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8958
[2019-03-26 03:06:07,627] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2000287.28361961 W.
[2019-03-26 03:06:07,635] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 64.0, 1.0, 2.0, 0.7894349195696719, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.989124556501045, 6.9112, 168.9124934810528, 2000287.28361961, 1945005.072823224, 405870.5513622444], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6614400.0000, 
sim time next is 6615000.0000, 
raw observation next is [31.25, 63.0, 1.0, 2.0, 0.7579500769080907, 1.0, 1.0, 0.7579500769080907, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2119631.773921946, 2119631.773921947, 399915.7346160283], 
processed observation next is [1.0, 0.5652173913043478, 0.6800947867298578, 0.63, 1.0, 1.0, 0.7083735866362538, 1.0, 0.5, 0.7083735866362538, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5887866038672073, 0.5887866038672075, 0.5968891561433258], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22569427], dtype=float32), 0.5604068]. 
=============================================
[2019-03-26 03:06:07,659] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[36.187122]
 [37.16805 ]
 [37.357986]
 [37.912937]
 [37.37855 ]], R is [[35.13816833]
 [34.78678513]
 [34.92029572]
 [34.57109451]
 [34.78087616]].
[2019-03-26 03:06:08,371] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2293644: loss 225.7848
[2019-03-26 03:06:08,372] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2293644: learning rate 0.0010
[2019-03-26 03:06:09,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.000000e+00 1.328346e-32 0.000000e+00 0.000000e+00 8.487287e-29], sum to 1.0000
[2019-03-26 03:06:09,438] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9588
[2019-03-26 03:06:09,447] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1186182.724253717 W.
[2019-03-26 03:06:09,456] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.73333333333333, 70.66666666666666, 1.0, 2.0, 0.2592993811000323, 1.0, 2.0, 0.2592993811000323, 1.0, 2.0, 0.4577051318313916, 6.911200000000001, 6.9112, 170.5573041426782, 1186182.724253717, 1186182.724253716, 298698.3737295754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6770400.0000, 
sim time next is 6771000.0000, 
raw observation next is [24.96666666666667, 69.33333333333334, 1.0, 2.0, 0.774520519214343, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510412, 1200542.14689221, 1200542.14689221, 252943.5306796803], 
processed observation next is [1.0, 0.34782608695652173, 0.3823064770932071, 0.6933333333333335, 1.0, 1.0, 0.7283379749570397, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522093, 0.3334839296922806, 0.3334839296922806, 0.3775276577308661], 
reward next is 0.6225, 
noisyNet noise sample is [array([0.1941699], dtype=float32), -0.7869581]. 
=============================================
[2019-03-26 03:06:09,474] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[47.163734]
 [50.91479 ]
 [58.06085 ]
 [59.84193 ]
 [60.170135]], R is [[44.09733963]
 [44.2105484 ]
 [44.31588364]
 [43.87272644]
 [44.14988708]].
[2019-03-26 03:06:11,030] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2294617: loss 34.4568
[2019-03-26 03:06:11,031] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2294617: learning rate 0.0010
[2019-03-26 03:06:13,756] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.132488e-24], sum to 1.0000
[2019-03-26 03:06:13,765] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4417
[2019-03-26 03:06:13,771] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.0, 72.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.631165714558774, 6.9112, 6.9112, 168.912956510431, 547326.3541794426, 547326.3541794426, 170987.5684242611], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6735600.0000, 
sim time next is 6736200.0000, 
raw observation next is [24.85, 72.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.62620051345053, 6.9112, 6.9112, 168.912956510431, 543323.6003411306, 543323.6003411306, 170189.285223731], 
processed observation next is [1.0, 1.0, 0.37677725118483424, 0.7266666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5441469676225975, 0.0, 0.0, 0.8294399451523027, 0.15092322231698074, 0.15092322231698074, 0.2540138585428821], 
reward next is 0.7460, 
noisyNet noise sample is [array([-1.4063265], dtype=float32), 0.045779128]. 
=============================================
[2019-03-26 03:06:15,905] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 1.2993649e-29 6.5565606e-38 1.1222856e-31 8.1726829e-27], sum to 1.0000
[2019-03-26 03:06:15,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5673
[2019-03-26 03:06:15,930] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 977188.6768227 W.
[2019-03-26 03:06:15,939] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.31666666666667, 91.0, 1.0, 2.0, 0.6992266559837106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104309, 977188.6768227, 977188.6768226993, 220677.4771199963], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6490200.0000, 
sim time next is 6490800.0000, 
raw observation next is [26.3, 91.0, 1.0, 2.0, 0.3416153823479987, 0.0, 2.0, 0.0, 1.0, 1.0, 0.579348166700809, 6.9112, 6.9112, 168.912956510431, 954823.9250350922, 954823.9250350922, 234601.6445089065], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.91, 1.0, 1.0, 0.20676552090120326, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.4870099593912305, 0.0, 0.0, 0.8294399451523027, 0.2652288680653034, 0.2652288680653034, 0.3501517082222485], 
reward next is 0.6498, 
noisyNet noise sample is [array([1.819331], dtype=float32), 1.2341493]. 
=============================================
[2019-03-26 03:06:16,590] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2296670: loss 13.4125
[2019-03-26 03:06:16,593] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2296671: learning rate 0.0010
[2019-03-26 03:06:17,568] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2297028: loss 27.4278
[2019-03-26 03:06:17,572] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2297029: learning rate 0.0010
[2019-03-26 03:06:19,225] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2297636: loss 55.2636
[2019-03-26 03:06:19,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2297637: learning rate 0.0010
[2019-03-26 03:06:20,756] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2298202: loss 63.4721
[2019-03-26 03:06:20,759] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2298202: learning rate 0.0010
[2019-03-26 03:06:21,089] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:06:21,097] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7708
[2019-03-26 03:06:21,101] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.6, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8406921453752844, 6.911200000000001, 6.9112, 168.912956510431, 698780.5219603968, 698780.5219603961, 210282.1012484613], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6557400.0000, 
sim time next is 6558000.0000, 
raw observation next is [27.56666666666667, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8456362469298735, 6.9112, 6.9112, 168.912956510431, 702200.4437346288, 702200.4437346288, 211335.4080104029], 
processed observation next is [1.0, 0.9130434782608695, 0.505529225908373, 0.7966666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8117515206461872, 0.0, 0.0, 0.8294399451523027, 0.1950556788151747, 0.1950556788151747, 0.31542598210507894], 
reward next is 0.6846, 
noisyNet noise sample is [array([-2.0654445], dtype=float32), 0.27749002]. 
=============================================
[2019-03-26 03:06:21,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.59096 ]
 [64.544945]
 [64.67732 ]
 [64.961784]
 [64.83585 ]], R is [[64.56221008]
 [64.60273743]
 [64.64424133]
 [64.6864624 ]
 [64.72930908]].
[2019-03-26 03:06:21,349] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2298418: loss -47.9221
[2019-03-26 03:06:21,353] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2298419: learning rate 0.0010
[2019-03-26 03:06:22,130] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2298703: loss 51.4621
[2019-03-26 03:06:22,135] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2298704: learning rate 0.0010
[2019-03-26 03:06:22,138] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 9.1362984e-34 0.0000000e+00 1.0073521e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 03:06:22,148] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1689
[2019-03-26 03:06:22,155] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.9, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8813259553825179, 6.9112, 6.9112, 168.912956510431, 726725.2371117882, 726725.2371117882, 219108.0923769634], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6566400.0000, 
sim time next is 6567000.0000, 
raw observation next is [26.83333333333334, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.880566303128017, 6.911199999999999, 6.9112, 168.912956510431, 726193.3998804901, 726193.3998804907, 218939.0745339513], 
processed observation next is [1.0, 0.0, 0.4707740916271725, 0.8733333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8543491501561181, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2017203888556917, 0.20172038885569188, 0.32677473811037505], 
reward next is 0.6732, 
noisyNet noise sample is [array([-1.3137378], dtype=float32), -1.5656136]. 
=============================================
[2019-03-26 03:06:22,171] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[57.445133]
 [63.38971 ]
 [63.455063]
 [63.54111 ]
 [63.642567]], R is [[54.839077  ]
 [54.96366119]
 [55.08694077]
 [55.20891953]
 [55.32963562]].
[2019-03-26 03:06:23,246] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2299116: loss 8.2817
[2019-03-26 03:06:23,249] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2299116: learning rate 0.0010
[2019-03-26 03:06:23,684] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:06:23,695] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7486
[2019-03-26 03:06:23,699] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.7, 86.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8649798793418046, 6.9112, 6.9112, 168.912956510431, 716001.5143104362, 716001.5143104362, 215527.5235396067], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6643800.0000, 
sim time next is 6644400.0000, 
raw observation next is [26.66666666666667, 86.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8655824455412443, 6.9112, 6.9112, 168.912956510431, 716594.6577153513, 716594.6577153513, 215665.1670146906], 
processed observation next is [1.0, 0.9130434782608695, 0.4628751974723541, 0.8666666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8360761530990783, 0.0, 0.0, 0.8294399451523027, 0.19905407158759758, 0.19905407158759758, 0.32188830897715015], 
reward next is 0.6781, 
noisyNet noise sample is [array([-0.63303244], dtype=float32), 0.33036748]. 
=============================================
[2019-03-26 03:06:25,046] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2299779: loss 13.5950
[2019-03-26 03:06:25,051] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2299781: learning rate 0.0010
[2019-03-26 03:06:25,268] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2299859: loss 31.9233
[2019-03-26 03:06:25,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2299860: learning rate 0.0010
[2019-03-26 03:06:25,543] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 5.4097967e-26 1.3038168e-32 1.1578818e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 03:06:25,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6322
[2019-03-26 03:06:25,565] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2315960.338830856 W.
[2019-03-26 03:06:25,574] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.8280842535780971, 1.0, 2.0, 0.8280842535780971, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2315960.338830856, 2315960.338830856, 433772.350686298], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6609600.0000, 
sim time next is 6610200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5282771764842566, 1.0, 2.0, 0.5282771764842566, 1.0, 1.0, 0.9148209109729365, 6.911200000000001, 6.9112, 170.5573041426782, 2216115.369806046, 2216115.369806045, 434748.4415953745], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4316592487762128, 1.0, 1.0, 0.4316592487762128, 1.0, 0.5, 0.8961230621621176, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6155876027239017, 0.6155876027239015, 0.6488782710378723], 
reward next is 0.3511, 
noisyNet noise sample is [array([1.5423636], dtype=float32), 0.0027635721]. 
=============================================
[2019-03-26 03:06:25,650] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 03:06:25,652] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:06:25,652] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:06:25,657] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:06:25,658] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:06:25,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:06:25,659] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:06:25,660] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:06:25,660] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:06:25,659] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:06:25,666] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:06:25,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-26 03:06:25,718] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-26 03:06:25,719] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-26 03:06:25,767] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-26 03:06:25,790] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-26 03:07:12,321] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8081935], dtype=float32), 0.23216966]
[2019-03-26 03:07:12,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.39976571, 92.61503483499999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7926809855866043, 6.911199999999999, 6.9112, 168.912956510431, 673472.4917221628, 673472.4917221635, 200481.8326285881]
[2019-03-26 03:07:12,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:07:12,327] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.6055493698651309
[2019-03-26 03:07:19,211] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8081935], dtype=float32), 0.23216966]
[2019-03-26 03:07:19,214] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 71.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.012683642998475, 6.9112, 168.9122414332506, 900824.6926272222, 828829.0082749485, 254812.8273073338]
[2019-03-26 03:07:19,217] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:07:19,220] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 4.6212576e-14], sampled 0.8797665099155686
[2019-03-26 03:07:19,220] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 900824.6926272222 W.
[2019-03-26 03:07:52,712] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8081935], dtype=float32), 0.23216966]
[2019-03-26 03:07:52,714] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.0, 64.33333333333333, 1.0, 2.0, 0.972197514020155, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991871817304, 6.9112, 168.9123159570826, 2256088.008925974, 2188839.64759519, 454865.5400333242]
[2019-03-26 03:07:52,717] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:07:52,719] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.5251091e-03 1.9688520e-22 2.3960355e-23 4.6680681e-05 9.9342817e-01], sampled 0.715782788229121
[2019-03-26 03:08:11,965] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8081935], dtype=float32), 0.23216966]
[2019-03-26 03:08:11,965] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.06666666666667, 63.16666666666666, 1.0, 2.0, 0.6274121474841825, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876784.4780650168, 876784.4780650168, 205959.4044056158]
[2019-03-26 03:08:11,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:08:11,969] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00000e+00 0.00000e+00 0.00000e+00 0.00000e+00 1.80642e-22], sampled 0.341260673270701
[2019-03-26 03:08:11,972] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 876784.4780650168 W.
[2019-03-26 03:08:14,275] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8081935], dtype=float32), 0.23216966]
[2019-03-26 03:08:14,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.41666666666666, 93.50000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8479053685454554, 6.9112, 6.9112, 168.912956510431, 706178.4726697917, 706178.4726697917, 211893.5228438482]
[2019-03-26 03:08:14,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:08:14,282] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.27658843950853496
[2019-03-26 03:08:34,909] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7371.7132 3182772839.4265 1600.0000
[2019-03-26 03:08:34,952] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7514.1492 3320269591.4669 1461.0000
[2019-03-26 03:08:35,187] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8193.9892 2950029642.9859 1014.0000
[2019-03-26 03:08:35,362] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8103.8376 2993887676.9737 1063.0000
[2019-03-26 03:08:35,369] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7544.3778 3116475506.9901 1591.0000
[2019-03-26 03:08:36,386] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2300000, evaluation results [2300000.0, 7514.149167674597, 3320269591.46688, 1461.0, 7544.377815239227, 3116475506.990126, 1591.0, 8193.989185099026, 2950029642.985868, 1014.0, 7371.713164860401, 3182772839.4264517, 1600.0, 8103.8375636375995, 2993887676.9737086, 1063.0]
[2019-03-26 03:08:36,434] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2300023: loss -76.6630
[2019-03-26 03:08:36,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2300023: learning rate 0.0010
[2019-03-26 03:08:36,646] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2300100: loss 42.4928
[2019-03-26 03:08:36,651] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2300102: learning rate 0.0010
[2019-03-26 03:08:37,832] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0007306e-10 6.3331023e-31 2.8977401e-36 9.4177949e-08 9.9999988e-01], sum to 1.0000
[2019-03-26 03:08:37,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0650
[2019-03-26 03:08:37,845] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5282770853390629, 1.0, 2.0, 0.5282770853390629, 1.0, 2.0, 0.9154226115486902, 6.911199999999999, 6.9112, 170.5573041426782, 2216114.987114243, 2216114.987114243, 434863.8056652212], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6610200.0000, 
sim time next is 6610800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4193544442909883, 1.0, 2.0, 0.4193544442909883, 1.0, 2.0, 0.7254956551726047, 6.9112, 6.9112, 170.5573041426782, 1758811.366204315, 1758811.366204315, 363450.6722639581], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3004270413144437, 1.0, 1.0, 0.3004270413144437, 1.0, 1.0, 0.66523860386903, 0.0, 0.0, 0.8375144448122397, 0.48855871283453195, 0.48855871283453195, 0.5424636899462061], 
reward next is 0.4575, 
noisyNet noise sample is [array([1.2785432], dtype=float32), 2.2762606]. 
=============================================
[2019-03-26 03:08:37,949] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2300582: loss 28.2641
[2019-03-26 03:08:37,953] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2300583: learning rate 0.0010
[2019-03-26 03:08:38,124] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2300648: loss 105.9746
[2019-03-26 03:08:38,124] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2300648: learning rate 0.0010
[2019-03-26 03:08:39,500] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2301160: loss 45.0675
[2019-03-26 03:08:39,501] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2301160: learning rate 0.0010
[2019-03-26 03:08:40,431] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2301503: loss 40.4138
[2019-03-26 03:08:40,438] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2301503: learning rate 0.0010
[2019-03-26 03:08:41,043] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:08:41,052] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8197
[2019-03-26 03:08:41,065] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1027420.421854046 W.
[2019-03-26 03:08:41,071] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.2, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.147776115220196, 6.9112, 168.9118789774643, 1027420.421854046, 859586.2540533098, 256241.6105847949], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6675000.0000, 
sim time next is 6675600.0000, 
raw observation next is [25.3, 93.33333333333334, 1.0, 1.0, 0.2004848896832811, 1.0, 1.0, 0.2004848896832811, 1.0, 2.0, 0.3387779516505316, 6.9112, 6.9112, 170.5573041426782, 840492.3357717941, 840492.3357717941, 269460.4800085066], 
processed observation next is [1.0, 0.2608695652173913, 0.39810426540284366, 0.9333333333333335, 1.0, 0.5, 0.03672878275094107, 1.0, 0.5, 0.03672878275094107, 1.0, 1.0, 0.19363164835430682, 0.0, 0.0, 0.8375144448122397, 0.2334700932699428, 0.2334700932699428, 0.40217982090821885], 
reward next is 0.5978, 
noisyNet noise sample is [array([2.543748], dtype=float32), 0.21247233]. 
=============================================
[2019-03-26 03:08:42,802] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2302372: loss 43.9788
[2019-03-26 03:08:42,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2302372: learning rate 0.0010
[2019-03-26 03:08:45,871] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 2.3825223e-31 6.0700024e-32 3.2733206e-25 4.0097665e-11], sum to 1.0000
[2019-03-26 03:08:45,881] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9337
[2019-03-26 03:08:45,891] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1825532.570712227 W.
[2019-03-26 03:08:45,896] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.48333333333333, 81.33333333333333, 1.0, 2.0, 0.4352492810590289, 1.0, 1.0, 0.4352492810590289, 1.0, 2.0, 0.7325759508762315, 6.911200000000001, 6.9112, 170.5573041426782, 1825532.570712227, 1825532.570712226, 369573.5511844084], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7135800.0000, 
sim time next is 7136400.0000, 
raw observation next is [26.46666666666667, 81.66666666666667, 1.0, 2.0, 0.4416139156072401, 1.0, 2.0, 0.4416139156072401, 1.0, 2.0, 0.7447304603152998, 6.9112, 6.9112, 170.5573041426782, 1852250.34112758, 1852250.34112758, 373557.4087972657], 
processed observation next is [1.0, 0.6086956521739131, 0.45339652448657203, 0.8166666666666668, 1.0, 1.0, 0.3272456814545062, 1.0, 1.0, 0.3272456814545062, 1.0, 1.0, 0.6886956833113412, 0.0, 0.0, 0.8375144448122397, 0.51451398364655, 0.51451398364655, 0.5575483713392025], 
reward next is 0.4425, 
noisyNet noise sample is [array([-0.5813586], dtype=float32), 0.4178234]. 
=============================================
[2019-03-26 03:08:48,855] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2304608: loss 39.0778
[2019-03-26 03:08:48,859] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2304609: learning rate 0.0010
[2019-03-26 03:08:50,004] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2305036: loss 42.1229
[2019-03-26 03:08:50,011] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2305039: learning rate 0.0010
[2019-03-26 03:08:51,307] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2305511: loss -83.5920
[2019-03-26 03:08:51,309] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2305511: learning rate 0.0010
[2019-03-26 03:08:53,054] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2306161: loss -27.9585
[2019-03-26 03:08:53,059] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2306161: learning rate 0.0010
[2019-03-26 03:08:53,635] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2306373: loss 55.9557
[2019-03-26 03:08:53,638] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2306374: learning rate 0.0010
[2019-03-26 03:08:54,395] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2306654: loss 27.6163
[2019-03-26 03:08:54,404] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2306655: learning rate 0.0010
[2019-03-26 03:08:55,375] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:08:55,386] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9372
[2019-03-26 03:08:55,392] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.4, 79.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7168516311636545, 6.9112, 6.9112, 168.912956510431, 611451.8569809191, 611451.8569809191, 185782.5548484894], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6909600.0000, 
sim time next is 6910200.0000, 
raw observation next is [25.35, 80.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7175161318298285, 6.911199999999999, 6.9112, 168.912956510431, 612002.4918850659, 612002.4918850666, 185904.7004233413], 
processed observation next is [0.0, 1.0, 0.4004739336492892, 0.8, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6555074778412542, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1700006921902961, 0.1700006921902963, 0.27746970212439], 
reward next is 0.7225, 
noisyNet noise sample is [array([-1.6950217], dtype=float32), -0.8966874]. 
=============================================
[2019-03-26 03:08:55,504] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2307062: loss 38.7145
[2019-03-26 03:08:55,509] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2307063: learning rate 0.0010
[2019-03-26 03:08:57,381] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2307762: loss 41.7044
[2019-03-26 03:08:57,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2307762: learning rate 0.0010
[2019-03-26 03:08:57,661] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2307861: loss 35.5736
[2019-03-26 03:08:57,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2307861: learning rate 0.0010
[2019-03-26 03:08:58,240] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2308081: loss 44.2641
[2019-03-26 03:08:58,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2308081: learning rate 0.0010
[2019-03-26 03:08:58,573] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2308200: loss -288.8011
[2019-03-26 03:08:58,577] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2308201: learning rate 0.0010
[2019-03-26 03:08:59,718] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2308619: loss 31.2122
[2019-03-26 03:08:59,721] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2308620: learning rate 0.0010
[2019-03-26 03:09:00,002] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2308729: loss 36.9442
[2019-03-26 03:09:00,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2308731: learning rate 0.0010
[2019-03-26 03:09:00,422] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0000000e+00 2.7373547e-38 9.1663421e-37 7.9413942e-25 7.0605626e-23], sum to 1.0000
[2019-03-26 03:09:00,432] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2632
[2019-03-26 03:09:00,443] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1990544.18264112 W.
[2019-03-26 03:09:00,448] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 55.0, 1.0, 2.0, 0.7761150000035264, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.951951754095052, 6.9112, 168.9127122975344, 1990544.18264112, 1961633.527517794, 404793.8295906803], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7053600.0000, 
sim time next is 7054200.0000, 
raw observation next is [30.3, 56.5, 1.0, 2.0, 0.4854068136770323, 1.0, 1.0, 0.4854068136770323, 1.0, 2.0, 0.8123834066432196, 6.9112, 6.9112, 170.5573041426782, 2036104.350461964, 2036104.350461964, 399692.2543261813], 
processed observation next is [1.0, 0.6521739130434783, 0.6350710900473934, 0.565, 1.0, 1.0, 0.3800082092494365, 1.0, 0.5, 0.3800082092494365, 1.0, 1.0, 0.7711992763941703, 0.0, 0.0, 0.8375144448122397, 0.56558454179499, 0.56558454179499, 0.5965556034719124], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.11434749], dtype=float32), 0.056893032]. 
=============================================
[2019-03-26 03:09:01,420] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2309249: loss 47.4619
[2019-03-26 03:09:01,421] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2309249: learning rate 0.0010
[2019-03-26 03:09:02,201] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2309533: loss -40.8875
[2019-03-26 03:09:02,206] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2309536: learning rate 0.0010
[2019-03-26 03:09:03,942] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:09:03,957] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7401
[2019-03-26 03:09:03,964] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.83333333333334, 73.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7458900173741994, 6.911199999999999, 6.9112, 168.912956510431, 632901.887851938, 632901.8878519386, 191208.5347829018], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6993600.0000, 
sim time next is 6994200.0000, 
raw observation next is [26.71666666666667, 74.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7504178982130156, 6.9112, 6.9112, 168.912956510431, 636186.8733373553, 636186.8733373553, 192072.8452786282], 
processed observation next is [0.0, 0.9565217391304348, 0.46524486571879947, 0.7483333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6956315831866045, 0.0, 0.0, 0.8294399451523027, 0.17671857592704315, 0.17671857592704315, 0.28667588847556447], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.79183424], dtype=float32), -0.24496208]. 
=============================================
[2019-03-26 03:09:04,673] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2310446: loss -38.2172
[2019-03-26 03:09:04,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2310446: learning rate 0.0010
[2019-03-26 03:09:10,599] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2312654: loss -9.8665
[2019-03-26 03:09:10,602] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2312654: learning rate 0.0010
[2019-03-26 03:09:11,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2313088: loss 0.1536
[2019-03-26 03:09:11,811] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2313089: learning rate 0.0010
[2019-03-26 03:09:12,943] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2313511: loss 25.2152
[2019-03-26 03:09:12,947] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2313512: learning rate 0.0010
[2019-03-26 03:09:14,132] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.9984109e-01 3.5736722e-33 6.6268156e-33 1.4322853e-19 1.5883039e-04], sum to 1.0000
[2019-03-26 03:09:14,145] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5141
[2019-03-26 03:09:14,153] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1685280.762717368 W.
[2019-03-26 03:09:14,158] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.23333333333333, 83.83333333333333, 1.0, 2.0, 0.4018362738995289, 1.0, 2.0, 0.4018362738995289, 1.0, 1.0, 0.6759287978647562, 6.9112, 6.9112, 170.5573041426782, 1685280.762717368, 1685280.762717368, 350743.4885942101], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7141800.0000, 
sim time next is 7142400.0000, 
raw observation next is [26.2, 84.0, 1.0, 2.0, 0.5913522467961683, 1.0, 2.0, 0.5913522467961683, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1653376.108246829, 1653376.108246829, 331069.0310292212], 
processed observation next is [1.0, 0.6956521739130435, 0.44075829383886256, 0.84, 1.0, 1.0, 0.5076533093929738, 1.0, 1.0, 0.5076533093929738, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4592711411796747, 0.4592711411796747, 0.494132882133166], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10159188], dtype=float32), 1.1326742]. 
=============================================
[2019-03-26 03:09:14,722] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2314163: loss 0.0136
[2019-03-26 03:09:14,724] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2314164: learning rate 0.0010
[2019-03-26 03:09:15,231] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2314351: loss 0.0096
[2019-03-26 03:09:15,235] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2314352: learning rate 0.0010
[2019-03-26 03:09:16,247] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2314730: loss 0.0494
[2019-03-26 03:09:16,249] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2314730: learning rate 0.0010
[2019-03-26 03:09:16,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 5.4807199e-38 0.0000000e+00 0.0000000e+00 1.1258212e-12], sum to 1.0000
[2019-03-26 03:09:16,935] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3383
[2019-03-26 03:09:16,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1101032.916219182 W.
[2019-03-26 03:09:16,955] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.1, 93.0, 1.0, 2.0, 0.6967388288533848, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1101032.916219182, 1101032.916219182, 235424.5847228622], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7381800.0000, 
sim time next is 7382400.0000, 
raw observation next is [21.16666666666666, 93.0, 1.0, 2.0, 0.3532320858825915, 1.0, 1.0, 0.3532320858825915, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1102857.515104162, 1102857.515104162, 274119.6067790971], 
processed observation next is [1.0, 0.43478260869565216, 0.2022116903633489, 0.93, 1.0, 1.0, 0.2207615492561343, 1.0, 0.5, 0.2207615492561343, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3063493097511561, 0.3063493097511561, 0.40913374146133896], 
reward next is 0.5909, 
noisyNet noise sample is [array([-0.3472283], dtype=float32), -0.15344492]. 
=============================================
[2019-03-26 03:09:17,058] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2315024: loss -132.8022
[2019-03-26 03:09:17,060] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2315024: learning rate 0.0010
[2019-03-26 03:09:18,909] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2315712: loss 0.2380
[2019-03-26 03:09:18,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2315712: learning rate 0.0010
[2019-03-26 03:09:19,283] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2315848: loss -365.3831
[2019-03-26 03:09:19,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2315849: learning rate 0.0010
[2019-03-26 03:09:19,876] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2316074: loss 8.7275
[2019-03-26 03:09:19,885] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2316075: learning rate 0.0010
[2019-03-26 03:09:20,215] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2316197: loss 0.0168
[2019-03-26 03:09:20,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2316198: learning rate 0.0010
[2019-03-26 03:09:21,238] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2316575: loss -113.0658
[2019-03-26 03:09:21,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2316578: learning rate 0.0010
[2019-03-26 03:09:21,505] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2316670: loss 7.4756
[2019-03-26 03:09:21,508] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2316670: learning rate 0.0010
[2019-03-26 03:09:22,853] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2317171: loss 0.0119
[2019-03-26 03:09:22,855] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2317171: learning rate 0.0010
[2019-03-26 03:09:23,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:09:23,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7285
[2019-03-26 03:09:23,253] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.26666666666667, 90.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6252525980123753, 6.911200000000001, 6.9112, 168.912956510431, 542896.435930813, 542896.4359308124, 170032.7169469236], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7255200.0000, 
sim time next is 7255800.0000, 
raw observation next is [22.25, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6224056060167671, 6.911200000000001, 6.9112, 168.912956510431, 540633.8206291805, 540633.8206291798, 169577.9724331652], 
processed observation next is [1.0, 1.0, 0.2535545023696683, 0.905, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5395190317277647, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15017606128588348, 0.1501760612858833, 0.2531014513927839], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.7984436], dtype=float32), -0.50638133]. 
=============================================
[2019-03-26 03:09:23,665] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2317469: loss 0.0054
[2019-03-26 03:09:23,668] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2317469: learning rate 0.0010
[2019-03-26 03:09:26,011] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2318330: loss 0.0048
[2019-03-26 03:09:26,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2318330: learning rate 0.0010
[2019-03-26 03:09:32,186] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2320615: loss 0.0100
[2019-03-26 03:09:32,190] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2320616: learning rate 0.0010
[2019-03-26 03:09:33,524] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2321107: loss 0.0040
[2019-03-26 03:09:33,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2321108: learning rate 0.0010
[2019-03-26 03:09:34,646] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2321523: loss 1.1419
[2019-03-26 03:09:34,649] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2321523: learning rate 0.0010
[2019-03-26 03:09:36,247] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2322117: loss -44.2471
[2019-03-26 03:09:36,250] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2322117: learning rate 0.0010
[2019-03-26 03:09:36,878] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2322348: loss 0.0182
[2019-03-26 03:09:36,883] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2322350: learning rate 0.0010
[2019-03-26 03:09:37,855] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2322705: loss -142.0132
[2019-03-26 03:09:37,860] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2322705: learning rate 0.0010
[2019-03-26 03:09:38,714] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2323026: loss 0.0249
[2019-03-26 03:09:38,715] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2323026: learning rate 0.0010
[2019-03-26 03:09:40,173] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.4824736e-09 1.6235267e-36 1.8154771e-29 5.9587060e-18 1.0000000e+00], sum to 1.0000
[2019-03-26 03:09:40,181] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2539
[2019-03-26 03:09:40,188] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2584268896187393, 6.911200000000001, 6.9112, 170.5573041426782, 650639.4180091578, 650639.4180091571, 252515.4190283149], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7606800.0000, 
sim time next is 7607400.0000, 
raw observation next is [24.23333333333333, 94.16666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2582552675032348, 6.9112, 6.9112, 170.5573041426782, 650561.7220635476, 650561.7220635476, 252497.8388073754], 
processed observation next is [1.0, 0.043478260869565216, 0.3475513428120062, 0.9416666666666668, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.09543325305272536, 0.0, 0.0, 0.8375144448122397, 0.18071158946209656, 0.18071158946209656, 0.3768624459811573], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.79488206], dtype=float32), 0.9002561]. 
=============================================
[2019-03-26 03:09:40,496] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2323683: loss 0.0418
[2019-03-26 03:09:40,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2323684: learning rate 0.0010
[2019-03-26 03:09:41,279] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2323968: loss 0.0056
[2019-03-26 03:09:41,284] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2323968: learning rate 0.0010
[2019-03-26 03:09:41,960] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2324223: loss 3.0275
[2019-03-26 03:09:41,964] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2324224: learning rate 0.0010
[2019-03-26 03:09:42,066] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2324261: loss 0.0065
[2019-03-26 03:09:42,067] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2324261: learning rate 0.0010
[2019-03-26 03:09:43,012] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2324612: loss 0.0061
[2019-03-26 03:09:43,015] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2324612: learning rate 0.0010
[2019-03-26 03:09:43,248] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2324700: loss 0.0042
[2019-03-26 03:09:43,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2324700: learning rate 0.0010
[2019-03-26 03:09:44,068] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 03:09:44,069] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:09:44,070] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:09:44,072] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:09:44,073] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:09:44,077] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:09:44,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:09:44,079] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:09:44,081] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:09:44,082] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:09:44,079] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:09:44,114] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-26 03:09:44,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-26 03:09:44,167] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-26 03:09:44,195] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-26 03:09:44,224] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-26 03:09:57,678] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82922137], dtype=float32), 0.20715466]
[2019-03-26 03:09:57,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.05, 84.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5057920930164184, 6.911200000000001, 6.9112, 168.912956510431, 447865.6756368405, 447865.6756368399, 152722.8137939695]
[2019-03-26 03:09:57,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:09:57,681] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5769413553870213
[2019-03-26 03:10:50,363] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82922137], dtype=float32), 0.20715466]
[2019-03-26 03:10:50,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.26666666666667, 87.66666666666667, 1.0, 2.0, 0.7948406715529139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1110881.811720385, 1110881.811720385, 242648.6013289097]
[2019-03-26 03:10:50,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:10:50,368] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 1.9390327e-38 1.3512382e-38 2.2700124e-09], sampled 0.43919771261294516
[2019-03-26 03:10:50,369] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1110881.811720385 W.
[2019-03-26 03:11:04,568] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.82922137], dtype=float32), 0.20715466]
[2019-03-26 03:11:04,569] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.2, 62.83333333333334, 1.0, 2.0, 0.6216202852956395, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868687.2621865702, 868687.2621865702, 204841.3408814677]
[2019-03-26 03:11:04,572] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:11:04,575] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.5547066e-26 0.0000000e+00], sampled 0.1378024967113718
[2019-03-26 03:11:04,576] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 868687.2621865702 W.
[2019-03-26 03:11:17,524] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82922137], dtype=float32), 0.20715466]
[2019-03-26 03:11:17,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.06666666666666, 63.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9608225481687146, 6.911200000000001, 6.9112, 168.912956510431, 778718.9762077103, 778718.9762077096, 237395.7545016727]
[2019-03-26 03:11:17,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:11:17,531] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8595656770846074
[2019-03-26 03:11:52,741] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7520.2304 3314570213.2204 1317.0000
[2019-03-26 03:11:53,387] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8194.4170 2950814911.0334 946.0000
[2019-03-26 03:11:53,405] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7390.8102 3176184434.5237 1443.0000
[2019-03-26 03:11:53,501] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8122.1902 2992705496.1069 939.0000
[2019-03-26 03:11:53,745] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7599.2731 3116013781.6653 1430.0000
[2019-03-26 03:11:54,762] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2325000, evaluation results [2325000.0, 7520.230419022725, 3314570213.220443, 1317.0, 7599.273099561271, 3116013781.665277, 1430.0, 8194.416990318086, 2950814911.0333624, 946.0, 7390.810169771418, 3176184434.5237207, 1443.0, 8122.1902031909485, 2992705496.106868, 939.0]
[2019-03-26 03:11:55,305] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2325205: loss -9.9937
[2019-03-26 03:11:55,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2325209: learning rate 0.0010
[2019-03-26 03:11:56,050] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2325480: loss -44.7470
[2019-03-26 03:11:56,054] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2325481: learning rate 0.0010
[2019-03-26 03:11:56,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:11:56,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:11:56,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-26 03:11:57,984] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2326274: loss -15.1691
[2019-03-26 03:11:57,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2326274: learning rate 0.0010
[2019-03-26 03:11:58,156] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:11:58,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:11:58,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-26 03:11:58,881] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 6.6351265e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 03:11:58,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3382
[2019-03-26 03:11:58,901] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2004207.926483305 W.
[2019-03-26 03:11:58,905] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.00000000000001, 59.33333333333334, 1.0, 2.0, 0.4778098197950772, 1.0, 1.0, 0.4778098197950772, 1.0, 2.0, 0.8226593488462405, 6.9112, 6.9112, 170.5573041426782, 2004207.926483305, 2004207.926483305, 398912.4404405301], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7741200.0000, 
sim time next is 7741800.0000, 
raw observation next is [31.9, 59.5, 1.0, 2.0, 0.7048356306582619, 1.0, 2.0, 0.7048356306582619, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1970958.875166847, 1970958.875166847, 376198.6773721696], 
processed observation next is [1.0, 0.6086956521739131, 0.7109004739336492, 0.595, 1.0, 1.0, 0.6443802779015204, 1.0, 1.0, 0.6443802779015204, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5474885764352353, 0.5474885764352353, 0.5614905632420442], 
reward next is 0.4385, 
noisyNet noise sample is [array([-0.27421844], dtype=float32), -0.97785425]. 
=============================================
[2019-03-26 03:11:59,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:11:59,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:11:59,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-26 03:12:00,928] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:00,936] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7326
[2019-03-26 03:12:00,942] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.46666666666667, 84.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.48882004808151, 6.9112, 6.9112, 168.912956510431, 434981.5357246876, 434981.5357246876, 150514.3404186126], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6000.0000, 
sim time next is 6600.0000, 
raw observation next is [20.53333333333333, 84.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4915238283580591, 6.9112, 6.9112, 168.912956510431, 437087.9319102297, 437087.9319102297, 150859.6267231566], 
processed observation next is [1.0, 0.043478260869565216, 0.17219589257503945, 0.8483333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.37990710775373054, 0.0, 0.0, 0.8294399451523027, 0.12141331441950824, 0.12141331441950824, 0.2251636219748606], 
reward next is 0.7748, 
noisyNet noise sample is [array([-1.0407524], dtype=float32), 0.8323429]. 
=============================================
[2019-03-26 03:12:02,385] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 4.3724545e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 03:12:02,394] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6702
[2019-03-26 03:12:02,406] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1912341.751608788 W.
[2019-03-26 03:12:02,411] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.75, 59.0, 1.0, 2.0, 0.6838922219623477, 1.0, 2.0, 0.6838922219623477, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1912341.751608788, 1912341.751608788, 367294.7607057533], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7651800.0000, 
sim time next is 7652400.0000, 
raw observation next is [30.83333333333333, 58.66666666666667, 1.0, 2.0, 0.7828916173555706, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.966177960296633, 6.9112, 168.9126290914814, 1991129.845724039, 1952126.664097715, 404950.3436309791], 
processed observation next is [1.0, 0.5652173913043478, 0.6603475513428118, 0.5866666666666667, 1.0, 1.0, 0.7384236353681572, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005497796029663338, 0.0, 0.8294383373753671, 0.5530916238122331, 0.5422574066938097, 0.6044034979566852], 
reward next is 0.1207, 
noisyNet noise sample is [array([0.9276465], dtype=float32), 0.845431]. 
=============================================
[2019-03-26 03:12:02,812] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:02,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:02,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-26 03:12:03,316] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2328390: loss 8.2428
[2019-03-26 03:12:03,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2328390: learning rate 0.0010
[2019-03-26 03:12:04,423] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2328858: loss -33.2656
[2019-03-26 03:12:04,425] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2328858: learning rate 0.0010
[2019-03-26 03:12:05,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:05,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:05,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-26 03:12:05,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:05,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:05,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-26 03:12:07,051] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2329958: loss 3.6550
[2019-03-26 03:12:07,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2329958: learning rate 0.0010
[2019-03-26 03:12:07,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:07,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:07,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-26 03:12:08,586] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2330581: loss 111.1574
[2019-03-26 03:12:08,589] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2330582: learning rate 0.0010
[2019-03-26 03:12:10,067] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2331140: loss -23.2199
[2019-03-26 03:12:10,069] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2331140: learning rate 0.0010
[2019-03-26 03:12:11,076] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2331507: loss 17.3477
[2019-03-26 03:12:11,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2331507: learning rate 0.0010
[2019-03-26 03:12:11,619] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2331709: loss 24.5926
[2019-03-26 03:12:11,623] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2331710: learning rate 0.0010
[2019-03-26 03:12:11,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:11,691] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0459
[2019-03-26 03:12:11,697] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.28333333333333, 88.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6148001068221433, 6.911199999999999, 6.9112, 168.912956510431, 535122.9142332589, 535122.9142332596, 168364.0993336726], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 82200.0000, 
sim time next is 82800.0000, 
raw observation next is [22.2, 89.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6119749775316332, 6.911200000000001, 6.9112, 168.912956510431, 532885.1997642361, 532885.1997642354, 167920.0427131327], 
processed observation next is [1.0, 1.0, 0.2511848341232228, 0.89, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5267987530873576, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1480236666011767, 0.1480236666011765, 0.2506269294225861], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.3155251], dtype=float32), -1.4479386]. 
=============================================
[2019-03-26 03:12:11,875] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:11,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0337
[2019-03-26 03:12:11,893] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.75, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8805944641105334, 6.911199999999999, 6.9112, 168.912956510431, 725727.4146969982, 725727.4146969989, 218927.0753904031], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7854600.0000, 
sim time next is 7855200.0000, 
raw observation next is [26.7, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8782670139818406, 6.9112, 6.9112, 168.912956510431, 724342.2111914434, 724342.2111914434, 218419.4184627537], 
processed observation next is [1.0, 0.9565217391304348, 0.46445497630331756, 0.88, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8515451390022445, 0.0, 0.0, 0.8294399451523027, 0.20120616977540093, 0.20120616977540093, 0.32599913203396075], 
reward next is 0.6740, 
noisyNet noise sample is [array([-0.40869516], dtype=float32), 0.7544032]. 
=============================================
[2019-03-26 03:12:12,250] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:12,262] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9955
[2019-03-26 03:12:12,265] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.93333333333334, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8622458667244856, 6.9112, 6.9112, 168.912956510431, 711262.8172811773, 711262.8172811773, 214831.5218363689], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 7929600.0000, 
sim time next is 7930200.0000, 
raw observation next is [28.71666666666667, 75.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8683519456801607, 6.9112, 6.9112, 168.912956510431, 715910.761143711, 715910.761143711, 216178.4084840213], 
processed observation next is [1.0, 0.782608695652174, 0.5600315955766194, 0.755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.839453592292879, 0.0, 0.0, 0.8294399451523027, 0.19886410031769752, 0.19886410031769752, 0.32265434102092727], 
reward next is 0.6773, 
noisyNet noise sample is [array([1.4774183], dtype=float32), -1.0459536]. 
=============================================
[2019-03-26 03:12:12,558] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2332055: loss 61.8596
[2019-03-26 03:12:12,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2332057: learning rate 0.0010
[2019-03-26 03:12:12,669] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2332091: loss 69.8486
[2019-03-26 03:12:12,672] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2332091: learning rate 0.0010
[2019-03-26 03:12:13,261] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:13,262] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:13,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-26 03:12:14,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:14,471] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:14,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-26 03:12:17,363] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:17,363] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:17,444] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-26 03:12:19,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:19,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:19,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-26 03:12:19,653] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:19,659] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7834
[2019-03-26 03:12:19,665] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.23333333333333, 93.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5271920265338547, 6.911200000000001, 6.9112, 168.912956510431, 465394.6007317224, 465394.6007317218, 155543.1930015133], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 198600.0000, 
sim time next is 199200.0000, 
raw observation next is [20.26666666666667, 93.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5288443731972283, 6.9112, 6.9112, 168.912956510431, 466795.059711286, 466795.059711286, 155763.9742882101], 
processed observation next is [0.0, 0.30434782608695654, 0.15955766192733034, 0.9366666666666668, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.425419967313693, 0.0, 0.0, 0.8294399451523027, 0.12966529436424612, 0.12966529436424612, 0.2324835437137464], 
reward next is 0.7675, 
noisyNet noise sample is [array([-0.74009275], dtype=float32), -0.34909514]. 
=============================================
[2019-03-26 03:12:20,154] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:20,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:20,233] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-26 03:12:21,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:21,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:21,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-26 03:12:21,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:21,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:21,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-26 03:12:21,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:21,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:22,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 03:12:22,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:22,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-26 03:12:22,124] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-26 03:12:23,545] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6741202e-21], sum to 1.0000
[2019-03-26 03:12:23,557] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7512
[2019-03-26 03:12:23,568] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.63333333333333, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5242433220881926, 6.9112, 6.9112, 168.912956510431, 462374.4486972084, 462374.4486972084, 155171.1158504856], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 253200.0000, 
sim time next is 253800.0000, 
raw observation next is [20.6, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5221845831601394, 6.911199999999999, 6.9112, 168.912956510431, 460693.7622599908, 460693.7622599914, 154896.351811156], 
processed observation next is [0.0, 0.9565217391304348, 0.17535545023696694, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.41729827214651144, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12797048951666412, 0.12797048951666426, 0.23118858479277016], 
reward next is 0.7688, 
noisyNet noise sample is [array([-0.34198684], dtype=float32), 0.98140264]. 
=============================================
[2019-03-26 03:12:29,747] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:29,756] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9432
[2019-03-26 03:12:29,763] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.76666666666667, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.560685159498048, 6.911200000000001, 6.9112, 168.912956510431, 491622.1101814043, 491622.1101814037, 160229.3138237913], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 231000.0000, 
sim time next is 231600.0000, 
raw observation next is [21.73333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5586565362406493, 6.9112, 6.9112, 168.912956510431, 489982.389484485, 489982.389484485, 159939.2539728316], 
processed observation next is [0.0, 0.6956521739130435, 0.22906793048973137, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4617762637081088, 0.0, 0.0, 0.8294399451523027, 0.13610621930124583, 0.13610621930124583, 0.23871530443706207], 
reward next is 0.7613, 
noisyNet noise sample is [array([-2.193707], dtype=float32), -0.023286395]. 
=============================================
[2019-03-26 03:12:35,033] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:35,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5833
[2019-03-26 03:12:35,053] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.2, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6715993600777949, 6.911200000000001, 6.9112, 168.912956510431, 599861.8445047349, 599861.8445047343, 177017.3708834142], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 549600.0000, 
sim time next is 550200.0000, 
raw observation next is [21.2, 75.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6725663336134642, 6.9112, 6.9112, 168.912956510431, 600724.2853475936, 600724.2853475936, 177181.9993499423], 
processed observation next is [1.0, 0.34782608695652173, 0.20379146919431282, 0.75, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.600690650748127, 0.0, 0.0, 0.8294399451523027, 0.1668678570409982, 0.1668678570409982, 0.2644507452984214], 
reward next is 0.7355, 
noisyNet noise sample is [array([0.47036448], dtype=float32), 1.1520346]. 
=============================================
[2019-03-26 03:12:44,673] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:44,687] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3161
[2019-03-26 03:12:44,695] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 74.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4504697291018697, 6.911200000000001, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770875, 145892.5188688431], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [21.16666666666667, 73.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5619969306624618, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 159906.3756388777], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7316666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4658499154420266, 0.0, 0.0, 0.8294399451523027, 0.13976901878634496, 0.13976901878634496, 0.2386662322968324], 
reward next is 0.7613, 
noisyNet noise sample is [array([0.32721406], dtype=float32), 0.8439441]. 
=============================================
[2019-03-26 03:12:46,082] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:12:46,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3770
[2019-03-26 03:12:46,107] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.03333333333333, 89.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4714067393520501, 6.9112, 6.9112, 168.912956510431, 418631.7409646709, 418631.7409646709, 148488.1653282566], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [20.01666666666667, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4722024487335745, 6.9112, 6.9112, 168.912956510431, 419326.6400830902, 419326.6400830902, 148581.7131041938], 
processed observation next is [1.0, 0.17391304347826086, 0.14770932069510287, 0.8983333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.3563444496750908, 0.0, 0.0, 0.8294399451523027, 0.11647962224530284, 0.11647962224530284, 0.2217637509017818], 
reward next is 0.7782, 
noisyNet noise sample is [array([-1.942308], dtype=float32), -0.60403234]. 
=============================================
[2019-03-26 03:12:46,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[68.139206]
 [68.420586]
 [68.91941 ]
 [69.28469 ]
 [69.411736]], R is [[68.07476044]
 [68.17238617]
 [68.26921082]
 [68.36560059]
 [68.46094513]].
[2019-03-26 03:12:49,736] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:12:49,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3219
[2019-03-26 03:12:49,754] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.7, 64.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 466946.734370199, 466946.7343701984, 225101.6541983634], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 815400.0000, 
sim time next is 816000.0000, 
raw observation next is [24.86666666666667, 63.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 466768.592692835, 466768.5926928343, 225068.5775956833], 
processed observation next is [0.0, 0.43478260869565216, 0.3775671406003162, 0.63, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1296579424146764, 0.1296579424146762, 0.33592325014281094], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.652722], dtype=float32), -0.6357995]. 
=============================================
[2019-03-26 03:12:49,772] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[19.688667]
 [19.94103 ]
 [20.034071]
 [20.003532]
 [20.010632]], R is [[19.47930336]
 [19.28450966]
 [19.09166527]
 [18.90074921]
 [18.7117424 ]].
[2019-03-26 03:12:51,893] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:12:51,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9296
[2019-03-26 03:12:51,911] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.0, 81.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 436016.1223071401, 436016.1223071401, 218840.0092649166], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 414000.0000, 
sim time next is 414600.0000, 
raw observation next is [20.91666666666667, 80.83333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 431465.175621719, 431465.1756217183, 217971.2972763887], 
processed observation next is [1.0, 0.8260869565217391, 0.19036334913112193, 0.8083333333333332, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.11985143767269972, 0.11985143767269953, 0.3253302944423712], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0947426], dtype=float32), 0.46563664]. 
=============================================
[2019-03-26 03:12:55,032] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:12:55,045] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3549
[2019-03-26 03:12:55,052] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.26666666666667, 80.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999998, 6.9112, 170.5573041426782, 401757.6585987958, 401757.658598797, 212261.0302387493], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 458400.0000, 
sim time next is 459000.0000, 
raw observation next is [20.3, 80.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 401587.5809757782, 401587.5809757782, 212232.8115702998], 
processed observation next is [1.0, 0.30434782608695654, 0.16113744075829392, 0.805, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.11155210582660506, 0.11155210582660506, 0.31676539040343255], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.43500254], dtype=float32), -0.3335372]. 
=============================================
[2019-03-26 03:12:55,075] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[-3.8115964]
 [-3.7905045]
 [-3.6651578]
 [-3.712583 ]
 [-3.6848407]], R is [[-3.84452486]
 [-3.80607963]
 [-3.76801896]
 [-3.73033881]
 [-3.69303536]].
[2019-03-26 03:12:55,627] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:12:55,636] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7852
[2019-03-26 03:12:55,641] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.43333333333333, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1857099491187954, 6.9112, 6.9112, 170.5573041426782, 486898.0587140038, 486898.0587140038, 228284.2205706182], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 863400.0000, 
sim time next is 864000.0000, 
raw observation next is [21.4, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.1847738933130962, 6.911200000000001, 6.9112, 170.5573041426782, 484604.7661319155, 484604.7661319149, 227950.8082628403], 
processed observation next is [0.0, 0.0, 0.21327014218009477, 0.89, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.005821821113531972, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1346124350366432, 0.13461243503664302, 0.34022508695946313], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17133331], dtype=float32), -0.70045364]. 
=============================================
[2019-03-26 03:12:55,661] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[3.266905 ]
 [3.3489664]
 [3.4565058]
 [3.5815835]
 [3.7134054]], R is [[2.14908433]
 [2.12759352]
 [2.10631752]
 [2.08525443]
 [2.06440187]].
[2019-03-26 03:12:59,667] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 03:12:59,668] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:12:59,670] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:12:59,671] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:59,671] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:59,674] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:12:59,675] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:12:59,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:12:59,677] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:59,679] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:59,680] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:12:59,705] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-26 03:12:59,734] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-26 03:12:59,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-26 03:12:59,785] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-26 03:12:59,807] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-26 03:13:04,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.6997132], dtype=float32), 0.20837125]
[2019-03-26 03:13:04,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.95394008, 88.69551045, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.2078258240086678, 6.911199999999999, 6.9112, 184.5923449428631, 541497.561514604, 541497.5615146047, 240225.4882224471]
[2019-03-26 03:13:04,218] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:13:04,224] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.26916500213410477
[2019-03-26 03:14:13,329] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.6997132], dtype=float32), 0.20837125]
[2019-03-26 03:14:13,329] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.46666666666667, 56.16666666666667, 1.0, 2.0, 0.2488794749966475, 1.0, 2.0, 0.2488794749966475, 1.0, 2.0, 0.4322213423690904, 6.911200000000001, 6.9112, 178.6582176852504, 1043452.530810226, 1043452.530810225, 287038.4253907195]
[2019-03-26 03:14:13,329] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:14:13,330] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.49969094425301785
[2019-03-26 03:14:30,826] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.6997132], dtype=float32), 0.20837125]
[2019-03-26 03:14:30,827] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 90.0, 1.0, 2.0, 0.1832663484857613, 1.0, 2.0, 0.1832663484857613, 1.0, 2.0, 0.3157729116437306, 6.9112, 6.9112, 170.5573041426782, 768281.2310774713, 768281.2310774713, 265297.9883717773]
[2019-03-26 03:14:30,828] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:14:30,830] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.42048746694209727
[2019-03-26 03:14:57,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.6997132], dtype=float32), 0.20837125]
[2019-03-26 03:14:57,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.32583818, 89.498558265, 1.0, 2.0, 0.2455549782964566, 1.0, 2.0, 0.2455549782964566, 1.0, 2.0, 0.4264477910287179, 6.9112, 6.9112, 171.5212843490159, 1029527.48421444, 1029527.48421444, 284084.7043965759]
[2019-03-26 03:14:57,275] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:14:57,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 0. 0. 0. 1.], sampled 0.7755234925817069
[2019-03-26 03:15:09,058] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 4011.1699 3612680055.9438 60.0000
[2019-03-26 03:15:09,510] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3956.4743 3594729768.9573 102.0000
[2019-03-26 03:15:09,749] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 4847.9030 3698588572.4946 16.0000
[2019-03-26 03:15:09,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3999.6064 3854438861.9596 568.0000
[2019-03-26 03:15:09,863] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 4733.4612 3681369035.8995 80.0000
[2019-03-26 03:15:10,880] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2350000, evaluation results [2350000.0, 3999.6064331448074, 3854438861.9595537, 568.0, 4733.461229752753, 3681369035.8995056, 80.0, 4011.169895716826, 3612680055.9438186, 60.0, 4847.902998952604, 3698588572.494561, 16.0, 3956.474253349884, 3594729768.9573374, 102.0]
[2019-03-26 03:15:14,555] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.5259595e-30 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 03:15:14,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1322
[2019-03-26 03:15:14,571] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [21.35, 83.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 451594.951807551, 451594.9518075516, 222048.6982252532], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 803400.0000, 
sim time next is 804000.0000, 
raw observation next is [21.5, 82.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 453230.3658306527, 453230.3658306533, 222383.7226238696], 
processed observation next is [0.0, 0.30434782608695654, 0.21800947867298584, 0.8266666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.12589732384184799, 0.12589732384184812, 0.3319160039162233], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.41389772], dtype=float32), -0.36692637]. 
=============================================
[2019-03-26 03:15:14,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[-9.286891]
 [-9.226442]
 [-9.2385  ]
 [-9.253839]
 [-9.232707]], R is [[-9.23611641]
 [-9.14375496]
 [-9.05231762]
 [-8.96179485]
 [-8.87217712]].
[2019-03-26 03:15:17,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:15:17,872] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8830
[2019-03-26 03:15:17,879] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.2242073722529284, 1.0, 2.0, 0.2242073722529284, 1.0, 2.0, 0.3932444528494866, 6.911200000000001, 6.9112, 170.5573041426782, 1015488.397508107, 1015488.397508107, 285138.4706250373], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 989400.0000, 
sim time next is 990000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.2232087899985492, 1.0, 2.0, 0.2232087899985492, 1.0, 2.0, 0.3914752275473354, 6.9112, 6.9112, 170.5573041426782, 1010892.91828077, 1010892.91828077, 284805.7182574827], 
processed observation next is [1.0, 0.4782608695652174, 0.2417061611374408, 0.94, 1.0, 1.0, 0.06410697590186651, 1.0, 1.0, 0.06410697590186651, 1.0, 1.0, 0.2578966189601651, 0.0, 0.0, 0.8375144448122397, 0.28080358841132497, 0.28080358841132497, 0.42508316157833237], 
reward next is 0.5749, 
noisyNet noise sample is [array([-0.52078676], dtype=float32), 0.18366347]. 
=============================================
[2019-03-26 03:15:17,899] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[1.6607492]
 [1.5634289]
 [1.5376053]
 [1.5981314]
 [1.4017098]], R is [[2.26918745]
 [2.8209157 ]
 [3.36832857]
 [3.91535234]
 [4.46226931]].
[2019-03-26 03:15:18,105] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:15:18,117] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6544
[2019-03-26 03:15:18,123] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [18.43333333333334, 85.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911199999999999, 6.9112, 170.5573041426782, 364968.3939619528, 364968.3939619534, 204786.8539996626], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 602400.0000, 
sim time next is 603000.0000, 
raw observation next is [18.35, 85.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 362933.1726331983, 362933.1726331983, 204422.2035247394], 
processed observation next is [1.0, 1.0, 0.06872037914691956, 0.855, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.10081477017588843, 0.10081477017588843, 0.3051077664548349], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.986587], dtype=float32), -0.50729036]. 
=============================================
[2019-03-26 03:15:18,145] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[-5.8027496]
 [-5.626642 ]
 [-5.436739 ]
 [-5.3126535]
 [-5.231324 ]], R is [[-5.89866018]
 [-5.83967352]
 [-5.7812767 ]
 [-5.72346401]
 [-5.66622925]].
[2019-03-26 03:15:30,003] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 0. 0. 0. 1.], sum to 1.0000
[2019-03-26 03:15:30,017] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8838
[2019-03-26 03:15:30,024] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.43333333333333, 66.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 469407.8708663483, 469407.8708663476, 225598.9175634097], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 831000.0000, 
sim time next is 831600.0000, 
raw observation next is [24.4, 67.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.9112, 6.9112, 170.5573041426782, 472334.2994887853, 472334.2994887853, 226161.8638212499], 
processed observation next is [0.0, 0.6521739130434783, 0.3554502369668246, 0.67, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.8375144448122397, 0.13120397208021814, 0.13120397208021814, 0.33755502062873116], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6709992], dtype=float32), -1.7777442]. 
=============================================
[2019-03-26 03:15:32,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.999833e-01 0.000000e+00 0.000000e+00 0.000000e+00 1.663745e-05], sum to 1.0000
[2019-03-26 03:15:32,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5870
[2019-03-26 03:15:32,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [22.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5331688700776678, 6.9112, 6.9112, 168.912956510431, 469033.7216901088, 469033.7216901088, 156396.4293437205], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [22.5, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5331877321621858, 6.911200000000001, 6.9112, 168.912956510431, 469050.3185091248, 469050.3185091242, 156398.9523286584], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.430716746539251, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13029175514142355, 0.13029175514142338, 0.23343127213232598], 
reward next is 0.7666, 
noisyNet noise sample is [array([-1.443401], dtype=float32), -0.23976693]. 
=============================================
[2019-03-26 03:15:41,101] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:15:41,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0962
[2019-03-26 03:15:41,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 972351.7614153804 W.
[2019-03-26 03:15:41,133] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [24.93333333333334, 71.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.029651366916946, 7.018732198794264, 6.9112, 168.9124786288865, 972351.7614153804, 896064.9344462168, 256391.0352377478], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1084800.0000, 
sim time next is 1085400.0000, 
raw observation next is [25.1, 70.5, 1.0, 1.0, 0.2328501583243602, 1.0, 1.0, 0.2328501583243602, 1.0, 2.0, 0.4078677228512196, 6.911199999999999, 6.9112, 170.5573041426782, 1052532.102841158, 1052532.102841159, 287778.1357788562], 
processed observation next is [1.0, 0.5652173913043478, 0.38862559241706174, 0.705, 1.0, 0.5, 0.07572308231850626, 1.0, 0.5, 0.07572308231850626, 1.0, 1.0, 0.27788746689173116, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2923700285669883, 0.2923700285669886, 0.42951960564008385], 
reward next is 0.5705, 
noisyNet noise sample is [array([-1.6532553], dtype=float32), -1.5704805]. 
=============================================
[2019-03-26 03:15:50,482] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8856052e-03 0.0000000e+00 3.7305524e-31 0.0000000e+00 9.9711442e-01], sum to 1.0000
[2019-03-26 03:15:50,491] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8196
[2019-03-26 03:15:50,497] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.7, 67.33333333333333, 1.0, 2.0, 0.2736541613573066, 1.0, 2.0, 0.2736541613573066, 1.0, 2.0, 0.4796186759161298, 6.911199999999999, 6.9112, 170.5573041426782, 1238178.571292461, 1238178.571292461, 302892.5388389313], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1093200.0000, 
sim time next is 1093800.0000, 
raw observation next is [25.7, 67.16666666666667, 1.0, 2.0, 0.2721344513609464, 1.0, 2.0, 0.2721344513609464, 1.0, 2.0, 0.4771718796573117, 6.9112, 6.9112, 170.5573041426782, 1232157.172452227, 1232157.172452227, 302381.6260052455], 
processed observation next is [1.0, 0.6521739130434783, 0.4170616113744076, 0.6716666666666667, 1.0, 1.0, 0.1230535558565619, 1.0, 1.0, 0.1230535558565619, 1.0, 1.0, 0.36240473128940454, 0.0, 0.0, 0.8375144448122397, 0.34226588123672974, 0.34226588123672974, 0.45131585970932164], 
reward next is 0.5487, 
noisyNet noise sample is [array([0.15084897], dtype=float32), -1.2094946]. 
=============================================
[2019-03-26 03:15:54,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:15:54,219] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7196
[2019-03-26 03:15:54,227] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.96666666666667, 89.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5344489350472523, 6.9112, 6.9112, 168.912956510431, 470953.5056266566, 470953.5056266566, 156540.3192926283], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1123800.0000, 
sim time next is 1124400.0000, 
raw observation next is [20.93333333333334, 89.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5335488651761672, 6.911200000000001, 6.9112, 168.912956510431, 470226.5675257269, 470226.5675257262, 156417.3793267443], 
processed observation next is [1.0, 0.0, 0.19115323854660388, 0.8933333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4311571526538624, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1306184909793686, 0.1306184909793684, 0.23345877511454372], 
reward next is 0.7665, 
noisyNet noise sample is [array([2.141258], dtype=float32), 0.41374734]. 
=============================================
[2019-03-26 03:15:56,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:15:56,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9515
[2019-03-26 03:15:56,227] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.63333333333334, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.618734890087569, 6.911200000000001, 6.9112, 168.912956510431, 537115.7424958579, 537115.7424958573, 169003.9764164822], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1471800.0000, 
sim time next is 1472400.0000, 
raw observation next is [21.6, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6161377246014939, 6.911200000000001, 6.9112, 168.912956510431, 535052.6042527283, 535052.6042527277, 168593.6811803683], 
processed observation next is [0.0, 0.043478260869565216, 0.22274881516587688, 0.96, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5318752739042608, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14862572340353564, 0.14862572340353547, 0.25163235997069894], 
reward next is 0.7484, 
noisyNet noise sample is [array([0.28640074], dtype=float32), 1.4897739]. 
=============================================
[2019-03-26 03:16:06,583] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00000e+00 0.00000e+00 0.00000e+00 1.45628e-37 1.00000e+00], sum to 1.0000
[2019-03-26 03:16:06,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5742
[2019-03-26 03:16:06,600] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.4696955436811484, 1.0, 2.0, 0.4696955436811484, 1.0, 2.0, 0.7978843814285156, 6.9112, 6.9112, 170.5573041426782, 1970140.711565472, 1970140.711565472, 391778.8276276179], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.438860197222972, 1.0, 2.0, 0.438860197222972, 1.0, 2.0, 0.744956424284806, 6.911200000000001, 6.9112, 170.5573041426782, 1840690.567280397, 1840690.567280397, 372722.2414996067], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.323927948461412, 1.0, 1.0, 0.323927948461412, 1.0, 1.0, 0.6889712491278122, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5113029353556658, 0.5113029353556658, 0.5563018529844876], 
reward next is 0.4437, 
noisyNet noise sample is [array([-0.77402836], dtype=float32), -0.67275435]. 
=============================================
[2019-03-26 03:16:09,773] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1858306e-17 1.2648519e-37 2.2257859e-37 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 03:16:09,782] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3767
[2019-03-26 03:16:09,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.3, 86.0, 1.0, 2.0, 0.2713456958202761, 1.0, 2.0, 0.2713456958202761, 1.0, 2.0, 0.4721894666252109, 6.9112, 6.9112, 170.5573041426782, 1214463.107958674, 1214463.107958674, 300533.4785831353], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.2591142604986391, 1.0, 2.0, 0.2591142604986391, 1.0, 2.0, 0.4516071869308442, 6.9112, 6.9112, 170.5573041426782, 1162423.075403234, 1162423.075403234, 296181.1839672424], 
processed observation next is [1.0, 0.5217391304347826, 0.3001579778830963, 0.8583333333333334, 1.0, 1.0, 0.10736657891402301, 1.0, 1.0, 0.10736657891402301, 1.0, 1.0, 0.3312282767449319, 0.0, 0.0, 0.8375144448122397, 0.32289529872312056, 0.32289529872312056, 0.4420614686078245], 
reward next is 0.5579, 
noisyNet noise sample is [array([0.49493214], dtype=float32), -0.1905484]. 
=============================================
[2019-03-26 03:16:12,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3820515e-04 4.7065238e-38 5.0571167e-36 1.4448998e-33 9.9946183e-01], sum to 1.0000
[2019-03-26 03:16:12,854] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0886
[2019-03-26 03:16:12,859] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.55, 90.5, 1.0, 2.0, 0.2516584414865147, 1.0, 2.0, 0.2516584414865147, 1.0, 2.0, 0.4300802632609735, 6.9112, 6.9112, 170.5573041426782, 1096444.545254898, 1096444.545254898, 289654.5133428623], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1611000.0000, 
sim time next is 1611600.0000, 
raw observation next is [23.5, 91.0, 1.0, 2.0, 0.2596230839778647, 1.0, 2.0, 0.2596230839778647, 1.0, 2.0, 0.4433381734556385, 6.9112, 6.9112, 170.5573041426782, 1129845.66200843, 1129845.66200843, 292329.9841831286], 
processed observation next is [1.0, 0.6521739130434783, 0.31279620853080575, 0.91, 1.0, 1.0, 0.1079796192504394, 1.0, 1.0, 0.1079796192504394, 1.0, 1.0, 0.3211441139702908, 0.0, 0.0, 0.8375144448122397, 0.3138460172245639, 0.3138460172245639, 0.4363134092285501], 
reward next is 0.5637, 
noisyNet noise sample is [array([-0.6086901], dtype=float32), -0.9704035]. 
=============================================
[2019-03-26 03:16:17,410] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.631473e-09 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 03:16:17,420] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9235
[2019-03-26 03:16:17,425] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.71666666666667, 82.66666666666667, 1.0, 2.0, 0.265907420811034, 1.0, 2.0, 0.265907420811034, 1.0, 2.0, 0.4630773532508684, 6.9112, 6.9112, 170.5573041426782, 1191475.664831722, 1191475.664831722, 298588.3588098829], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1768200.0000, 
sim time next is 1768800.0000, 
raw observation next is [23.63333333333334, 83.33333333333334, 1.0, 2.0, 0.2597605160440348, 1.0, 2.0, 0.2597605160440348, 1.0, 2.0, 0.4524103496872035, 6.9112, 6.9112, 170.5573041426782, 1164064.89472538, 1164064.89472538, 296282.6166359183], 
processed observation next is [1.0, 0.4782608695652174, 0.3191153238546607, 0.8333333333333335, 1.0, 1.0, 0.10814520005305393, 1.0, 1.0, 0.10814520005305393, 1.0, 1.0, 0.33220774352097987, 0.0, 0.0, 0.8375144448122397, 0.32335135964593886, 0.32335135964593886, 0.4422128606506244], 
reward next is 0.5578, 
noisyNet noise sample is [array([-0.325285], dtype=float32), -0.44119456]. 
=============================================
[2019-03-26 03:16:18,445] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 03:16:18,450] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:16:18,451] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:16:18,453] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:16:18,454] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:16:18,454] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:16:18,455] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:16:18,455] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:16:18,456] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:16:18,457] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:16:18,458] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:16:18,482] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-26 03:16:18,506] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-26 03:16:18,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-26 03:16:18,562] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-26 03:16:18,563] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-26 03:17:04,943] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7361899], dtype=float32), 0.12824182]
[2019-03-26 03:17:04,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.3, 54.5, 1.0, 2.0, 0.3426809708493699, 1.0, 2.0, 0.3426809708493699, 1.0, 2.0, 0.5831653477512099, 6.911199999999999, 6.9112, 169.0403247858759, 1437028.770578675, 1437028.770578676, 321730.5673233687]
[2019-03-26 03:17:04,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:17:04,952] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.6671754e-16], sampled 0.6291066030553442
[2019-03-26 03:17:04,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1437028.770578675 W.
[2019-03-26 03:17:09,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7361899], dtype=float32), 0.12824182]
[2019-03-26 03:17:09,515] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.66666666666667, 61.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9811757388473532, 6.9112, 6.9112, 168.912956510431, 793379.2689508296, 793379.2689508296, 242389.2368500547]
[2019-03-26 03:17:09,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:17:09,518] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4043283144225127
[2019-03-26 03:17:24,337] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.7361899], dtype=float32), 0.12824182]
[2019-03-26 03:17:24,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 1.0, 0.5987295147790566, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912900603278, 836685.8082525656, 836685.8082525656, 200508.2787007806]
[2019-03-26 03:17:24,340] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:17:24,345] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.4827708e-24], sampled 0.09195098932633183
[2019-03-26 03:17:59,401] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.7361899], dtype=float32), 0.12824182]
[2019-03-26 03:17:59,402] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.6, 55.0, 1.0, 2.0, 0.883382806920445, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1234701.641132193, 1234701.641132193, 265411.2012913963]
[2019-03-26 03:17:59,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:17:59,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.47373948 0.         0.         0.         0.5262605 ], sampled 0.5197666104057914
[2019-03-26 03:18:08,885] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.7361899], dtype=float32), 0.12824182]
[2019-03-26 03:18:08,886] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.28333333333333, 79.5, 1.0, 2.0, 0.8568580099308754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1197607.098994347, 1197607.098994347, 258345.6511902685]
[2019-03-26 03:18:08,887] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:18:08,890] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.088705e-10 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sampled 0.20528846981507265
[2019-03-26 03:18:27,382] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8191.8806 3006847040.0294 788.0000
[2019-03-26 03:18:27,790] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7480.1264 3243065815.9438 1175.0000
[2019-03-26 03:18:28,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7650.1032 3178386338.5990 1193.0000
[2019-03-26 03:18:28,132] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8095.4039 3046082650.7607 809.0000
[2019-03-26 03:18:28,212] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7561.9191 3366204534.0735 1192.0000
[2019-03-26 03:18:29,231] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2375000, evaluation results [2375000.0, 7561.919133752468, 3366204534.0734644, 1192.0, 7650.103224808277, 3178386338.5989747, 1193.0, 8191.880641544623, 3006847040.0293636, 788.0, 7480.126381946966, 3243065815.9438143, 1175.0, 8095.4039007524125, 3046082650.760661, 809.0]
[2019-03-26 03:18:41,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0448190e-02 0.0000000e+00 0.0000000e+00 2.5650803e-31 9.4955188e-01], sum to 1.0000
[2019-03-26 03:18:41,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1991
[2019-03-26 03:18:41,375] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.76666666666667, 75.0, 1.0, 2.0, 0.4335965224226583, 1.0, 2.0, 0.4335965224226583, 1.0, 2.0, 0.7435795319210704, 6.9112, 6.9112, 170.5573041426782, 1818594.649617133, 1818594.649617133, 370808.7655201676], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1701600.0000, 
sim time next is 1702200.0000, 
raw observation next is [28.83333333333333, 74.5, 1.0, 2.0, 0.42466934771628, 1.0, 2.0, 0.42466934771628, 1.0, 2.0, 0.7278410750157783, 6.911200000000001, 6.9112, 170.5573041426782, 1781121.089468031, 1781121.089468031, 365506.4383931686], 
processed observation next is [1.0, 0.6956521739130435, 0.5655608214849919, 0.745, 1.0, 1.0, 0.30683053941720484, 1.0, 1.0, 0.30683053941720484, 1.0, 1.0, 0.6680988719704612, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49475585818556417, 0.49475585818556417, 0.5455319976017442], 
reward next is 0.4545, 
noisyNet noise sample is [array([-1.5898244], dtype=float32), 0.25959873]. 
=============================================
[2019-03-26 03:18:45,325] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.3830235e-36], sum to 1.0000
[2019-03-26 03:18:45,335] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0787
[2019-03-26 03:18:45,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1650114.81056642 W.
[2019-03-26 03:18:45,354] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.43333333333333, 78.83333333333333, 1.0, 2.0, 0.3934577980462134, 1.0, 2.0, 0.3934577980462134, 1.0, 2.0, 0.6590440292774633, 6.9112, 6.9112, 170.5573041426782, 1650114.81056642, 1650114.81056642, 345862.9986403739], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1955400.0000, 
sim time next is 1956000.0000, 
raw observation next is [26.26666666666667, 79.66666666666667, 1.0, 2.0, 0.3890134601404955, 1.0, 2.0, 0.3890134601404955, 1.0, 2.0, 0.6512893070545265, 6.911199999999999, 6.9112, 170.5573041426782, 1631461.60293443, 1631461.602934431, 343494.7990350513], 
processed observation next is [1.0, 0.6521739130434783, 0.44391785150079005, 0.7966666666666667, 1.0, 1.0, 0.2638716387234885, 1.0, 1.0, 0.2638716387234885, 1.0, 1.0, 0.5747430573835688, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4531837785928972, 0.4531837785928975, 0.5126788045299273], 
reward next is 0.4873, 
noisyNet noise sample is [array([-0.08279797], dtype=float32), 1.0364021]. 
=============================================
[2019-03-26 03:18:45,375] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[56.35302 ]
 [56.11088 ]
 [55.588184]
 [55.500572]
 [55.85517 ]], R is [[56.22844315]
 [56.14994812]
 [55.58844757]
 [55.53927612]
 [54.9838829 ]].
[2019-03-26 03:18:47,061] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:47,071] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1910
[2019-03-26 03:18:47,080] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.88333333333333, 96.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6418071237322004, 6.9112, 6.9112, 168.912956510431, 555663.2742153206, 555663.2742153206, 172723.1675326844], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1829400.0000, 
sim time next is 1830000.0000, 
raw observation next is [21.86666666666667, 96.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6205052091592389, 6.9112, 6.9112, 168.912956510431, 537080.5314178695, 537080.5314178695, 169302.8495659686], 
processed observation next is [1.0, 0.17391304347826086, 0.23538704581358633, 0.9666666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5372014745844377, 0.0, 0.0, 0.8294399451523027, 0.14918903650496376, 0.14918903650496376, 0.25269082024771433], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.1742651], dtype=float32), 1.0852363]. 
=============================================
[2019-03-26 03:18:47,093] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[68.646576]
 [68.73915 ]
 [68.646614]
 [68.67025 ]
 [68.73426 ]], R is [[68.63573456]
 [68.69158173]
 [68.75234985]
 [68.81276703]
 [68.87293243]].
[2019-03-26 03:18:47,287] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:47,306] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0331
[2019-03-26 03:18:47,313] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.35, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9319216133017346, 6.9112, 6.9112, 168.912956510431, 761301.3853222206, 761301.3853222206, 230646.2589403971], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2147400.0000, 
sim time next is 2148000.0000, 
raw observation next is [27.26666666666667, 88.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.927529800242145, 6.9112, 6.9112, 168.912956510431, 758125.3869766641, 758125.3869766641, 229612.2971582096], 
processed observation next is [0.0, 0.8695652173913043, 0.4913112164297, 0.8833333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.911621707612372, 0.0, 0.0, 0.8294399451523027, 0.2105903852712956, 0.2105903852712956, 0.3427049211316561], 
reward next is 0.6573, 
noisyNet noise sample is [array([1.1884735], dtype=float32), 0.43966484]. 
=============================================
[2019-03-26 03:18:47,330] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.264885]
 [77.17275 ]
 [77.07286 ]
 [76.95915 ]
 [76.67373 ]], R is [[77.25550079]
 [77.13870239]
 [77.0221405 ]
 [76.90668488]
 [76.79178619]].
[2019-03-26 03:18:49,736] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:49,748] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4297
[2019-03-26 03:18:49,756] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.23333333333333, 95.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7750332526601105, 6.9112, 6.9112, 168.912956510431, 653122.8952995116, 653122.8952995116, 196844.3248558331], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1990200.0000, 
sim time next is 1990800.0000, 
raw observation next is [24.3, 95.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7750821243056282, 6.911200000000001, 6.9112, 168.912956510431, 652952.9311250603, 652952.9311250596, 196850.3517934249], 
processed observation next is [0.0, 0.043478260869565216, 0.3507109004739337, 0.95, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7257099076897904, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18137581420140564, 0.18137581420140544, 0.293806495214067], 
reward next is 0.7062, 
noisyNet noise sample is [array([-0.7433673], dtype=float32), 0.618377]. 
=============================================
[2019-03-26 03:18:51,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:51,390] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7109
[2019-03-26 03:18:51,394] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [24.45, 93.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8784423864054577, 6.9112, 6.9112, 168.912956510431, 740943.7219133448, 740943.7219133448, 218921.1640465687], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1749000.0000, 
sim time next is 1749600.0000, 
raw observation next is [24.5, 93.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8171296819080524, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 205484.053853957], 
processed observation next is [1.0, 0.2608695652173913, 0.3601895734597157, 0.93, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7769874169610393, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1913762893031219, 0.19137628930312206, 0.30669261769247314], 
reward next is 0.6933, 
noisyNet noise sample is [array([0.13528216], dtype=float32), -1.0209832]. 
=============================================
[2019-03-26 03:18:51,475] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:51,489] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3271
[2019-03-26 03:18:51,493] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.35, 87.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7773643121221813, 6.911200000000001, 6.9112, 168.912956510431, 654385.5020715554, 654385.5020715548, 197297.2008846526], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1887000.0000, 
sim time next is 1887600.0000, 
raw observation next is [25.3, 87.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7812579523447722, 6.9112, 6.9112, 168.912956510431, 657973.7384515428, 657973.7384515428, 198083.6733550827], 
processed observation next is [1.0, 0.8695652173913043, 0.39810426540284366, 0.8733333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7332414052985028, 0.0, 0.0, 0.8294399451523027, 0.18277048290320633, 0.18277048290320633, 0.29564727366430255], 
reward next is 0.7044, 
noisyNet noise sample is [array([1.048143], dtype=float32), 0.63676673]. 
=============================================
[2019-03-26 03:18:57,244] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:18:57,254] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3622
[2019-03-26 03:18:57,261] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.1, 88.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.780547305575322, 6.9112, 6.9112, 168.912956510431, 658324.6432128607, 658324.6432128607, 197957.4114601501], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 1890000.0000, 
sim time next is 1890600.0000, 
raw observation next is [25.03333333333333, 88.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7768839094678908, 6.911199999999999, 6.9112, 168.912956510431, 655440.5529497878, 655440.5529497884, 197226.635157531], 
processed observation next is [1.0, 0.9130434782608695, 0.38546603475513425, 0.8833333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7279072066681594, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18206682026382995, 0.18206682026383011, 0.2943681121754194], 
reward next is 0.7056, 
noisyNet noise sample is [array([0.69269335], dtype=float32), -0.728337]. 
=============================================
[2019-03-26 03:19:21,749] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:19:21,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9746
[2019-03-26 03:19:21,767] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.56666666666667, 68.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9759984345075887, 6.9112, 6.9112, 168.912956510431, 785331.8940519581, 785331.8940519581, 240868.1599440656], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2311800.0000, 
sim time next is 2312400.0000, 
raw observation next is [31.43333333333334, 69.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9909276546043022, 6.9112, 6.9112, 168.9129304389064, 797349.1196256437, 797349.1196256437, 244645.049822333], 
processed observation next is [1.0, 0.782608695652174, 0.6887835703001584, 0.6933333333333335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.988936164151588, 0.0, 0.0, 0.8294398171291812, 0.2214858665626788, 0.2214858665626788, 0.36514186540646715], 
reward next is 0.6349, 
noisyNet noise sample is [array([-2.4307377], dtype=float32), 0.10342198]. 
=============================================
[2019-03-26 03:19:21,923] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:19:21,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4125
[2019-03-26 03:19:21,948] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.76666666666667, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9479290903973229, 6.911200000000001, 6.9112, 168.912956510431, 771459.6826564639, 771459.6826564634, 234387.7852654763], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [26.75, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9467729672899879, 6.9112, 6.9112, 168.912956510431, 770710.0982300517, 770710.0982300517, 234114.8446042195], 
processed observation next is [1.0, 1.0, 0.4668246445497631, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9350889844999851, 0.0, 0.0, 0.8294399451523027, 0.21408613839723659, 0.21408613839723659, 0.34942514120032764], 
reward next is 0.6506, 
noisyNet noise sample is [array([1.0691469], dtype=float32), 1.0325543]. 
=============================================
[2019-03-26 03:19:30,674] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 8.148733e-37 0.000000e+00], sum to 1.0000
[2019-03-26 03:19:30,686] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3290
[2019-03-26 03:19:30,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 1114457.706674716 W.
[2019-03-26 03:19:30,702] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 81.83333333333334, 1.0, 2.0, 0.3986989488404353, 0.0, 2.0, 0.0, 1.0, 1.0, 0.6796082843154632, 6.9112, 6.9112, 168.912956510431, 1114457.706674716, 1114457.706674716, 257177.3108013649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2433000.0000, 
sim time next is 2433600.0000, 
raw observation next is [27.9, 82.0, 1.0, 2.0, 0.8139016518802309, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137536.006800764, 1137536.006800764, 247351.4414106397], 
processed observation next is [1.0, 0.17391304347826086, 0.5213270142180094, 0.82, 1.0, 1.0, 0.7757851227472661, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3159822241113233, 0.3159822241113233, 0.3691812558367757], 
reward next is 0.0000, 
noisyNet noise sample is [array([5.9290796e-05], dtype=float32), 2.437886]. 
=============================================
[2019-03-26 03:19:35,477] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:19:35,488] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4123
[2019-03-26 03:19:35,496] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.5, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8176525601240145, 6.911199999999999, 6.9112, 168.912956510431, 682820.8564156874, 682820.856415688, 205450.60420364], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2647800.0000, 
sim time next is 2648400.0000, 
raw observation next is [26.33333333333334, 85.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8216010820957289, 6.911199999999999, 6.9112, 168.912956510431, 685404.9337410438, 685404.9337410444, 206265.5242521928], 
processed observation next is [0.0, 0.6521739130434783, 0.44707740916271754, 0.8566666666666666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7824403440191816, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19039025937251217, 0.19039025937251233, 0.3078589914211833], 
reward next is 0.6921, 
noisyNet noise sample is [array([-0.03880867], dtype=float32), 0.6682511]. 
=============================================
[2019-03-26 03:19:37,023] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 03:19:37,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:19:37,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:19:37,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:19:37,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:19:37,030] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:19:37,032] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:19:37,030] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:19:37,032] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:19:37,033] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:19:37,036] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:19:37,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-26 03:19:37,094] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-26 03:19:37,123] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-26 03:19:37,124] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-26 03:19:37,124] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-26 03:19:54,813] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:19:54,816] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.7, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.549589975116366, 6.911200000000001, 6.9112, 168.912956510431, 483459.6441880774, 483459.6441880768, 158630.0001746365]
[2019-03-26 03:19:54,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:19:54,823] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3778822402102361
[2019-03-26 03:20:06,176] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:20:06,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.73333333333333, 92.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7145576274483624, 6.911200000000001, 6.9112, 168.912956510431, 609000.0666001562, 609000.0666001557, 185358.6410621222]
[2019-03-26 03:20:06,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:20:06,182] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32195897529909334
[2019-03-26 03:20:19,585] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:20:19,588] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.63333333333333, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8222866331880508, 6.9112, 6.9112, 168.912956510431, 690252.151756295, 690252.151756295, 206514.6050705907]
[2019-03-26 03:20:19,591] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:20:19,594] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.06429480926859366
[2019-03-26 03:20:22,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:20:22,037] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.45336155333334, 98.09168293333335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8767134622286062, 6.911199999999999, 6.9112, 168.912956510431, 725549.8460305573, 725549.8460305579, 218157.5822994751]
[2019-03-26 03:20:22,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:20:22,043] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.3681504824910127
[2019-03-26 03:20:31,325] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:20:31,327] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.128258155, 78.770811235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9581053705625335, 6.9112, 6.9112, 168.912956510431, 779327.3163717677, 779327.3163717677, 236867.4811871788]
[2019-03-26 03:20:31,331] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:20:31,334] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.34478120317628624
[2019-03-26 03:20:53,607] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:20:53,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.267998025, 77.849898585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7489549334980857, 6.9112, 6.9112, 168.912956510431, 635517.2244264332, 635517.2244264332, 191797.4597036898]
[2019-03-26 03:20:53,609] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:20:53,612] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9327741894081214
[2019-03-26 03:21:01,973] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:21:01,973] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.08333333333334, 79.33333333333334, 1.0, 2.0, 0.9655972280782834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349685.546083875, 1349685.546083875, 288623.29756321]
[2019-03-26 03:21:01,976] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:21:01,980] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00
 1.31291185e-11], sampled 0.5877199471191084
[2019-03-26 03:21:01,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1349685.546083875 W.
[2019-03-26 03:21:21,560] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:21:21,561] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.19829029, 59.06878267833333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8504646228613778, 6.9112, 6.9112, 168.912956510431, 708960.6066287545, 708960.6066287545, 212472.7768637394]
[2019-03-26 03:21:21,563] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:21:21,566] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9068141140111203
[2019-03-26 03:21:33,859] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:21:33,859] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.23333333333333, 81.83333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994779336875503, 6.9112, 168.9122383515195, 888496.363480705, 829202.5574200819, 254834.4897829266]
[2019-03-26 03:21:33,860] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:21:33,864] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32489365833639594
[2019-03-26 03:21:33,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 888496.363480705 W.
[2019-03-26 03:21:36,619] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8375738], dtype=float32), 0.17184319]
[2019-03-26 03:21:36,620] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.63333333333334, 61.66666666666667, 1.0, 2.0, 0.8903013113439863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340734.917294742, 1340734.917294742, 280916.9559604069]
[2019-03-26 03:21:36,620] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:21:36,623] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 3.5219059e-12], sampled 0.5209716101524217
[2019-03-26 03:21:36,624] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1340734.917294742 W.
[2019-03-26 03:21:46,394] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7180.3398 3189979974.8435 1946.0000
[2019-03-26 03:21:46,563] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7417.9305 3123804628.6448 1804.0000
[2019-03-26 03:21:46,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7399.6808 3324568397.7075 1653.0000
[2019-03-26 03:21:46,902] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8027.3067 2997555779.9113 1208.0000
[2019-03-26 03:21:46,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8099.9835 2950623198.5027 1185.0000
[2019-03-26 03:21:47,983] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2400000, evaluation results [2400000.0, 7399.680801661366, 3324568397.707531, 1653.0, 7417.930487781399, 3123804628.644808, 1804.0, 8099.983543509287, 2950623198.502728, 1185.0, 7180.339848116769, 3189979974.8434677, 1946.0, 8027.30673666536, 2997555779.9113045, 1208.0]
[2019-03-26 03:21:48,223] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:21:48,235] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6184
[2019-03-26 03:21:48,240] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.66666666666667, 96.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5998027549939359, 6.9112, 6.9112, 168.912956510431, 520451.6342583667, 520451.6342583667, 166078.8468562597], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2776800.0000, 
sim time next is 2777400.0000, 
raw observation next is [21.5, 97.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5938185543401078, 6.911200000000001, 6.9112, 168.912956510431, 515505.4628424659, 515505.4628424653, 165169.8534091654], 
processed observation next is [1.0, 0.13043478260869565, 0.21800947867298584, 0.97, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5046567735854972, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14319596190068498, 0.14319596190068482, 0.24652216926741105], 
reward next is 0.7535, 
noisyNet noise sample is [array([-0.11465714], dtype=float32), 1.2179048]. 
=============================================
[2019-03-26 03:21:50,308] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0000000e+00 2.3138074e-29 1.3380203e-30 1.0599925e-15 8.7047218e-12], sum to 1.0000
[2019-03-26 03:21:50,316] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4721
[2019-03-26 03:21:50,329] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1114742.726591281 W.
[2019-03-26 03:21:50,334] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.96666666666667, 81.83333333333334, 1.0, 2.0, 0.3988028787785089, 1.0, 2.0, 0.3988028787785089, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1114742.726591281, 1114742.726591281, 271825.4760035898], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2433000.0000, 
sim time next is 2433600.0000, 
raw observation next is [27.9, 82.0, 1.0, 2.0, 0.4070761915552442, 1.0, 2.0, 0.4070761915552442, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1137880.746833767, 1137880.746833767, 273923.4129286318], 
processed observation next is [1.0, 0.17391304347826086, 0.5213270142180094, 0.82, 1.0, 1.0, 0.2856339657292099, 1.0, 1.0, 0.2856339657292099, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.31607798523160197, 0.31607798523160197, 0.4088409148188535], 
reward next is 0.5912, 
noisyNet noise sample is [array([-0.34366167], dtype=float32), -0.9538007]. 
=============================================
[2019-03-26 03:21:53,760] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:21:53,769] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5968
[2019-03-26 03:21:53,774] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6852418744755675, 6.911199999999999, 6.9112, 168.912956510431, 587552.7706215467, 587552.7706215472, 180113.67923386], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2736600.0000, 
sim time next is 2737200.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6844010565086958, 6.911200000000001, 6.9112, 168.912956510431, 586831.5939398808, 586831.5939398803, 179966.353301128], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6151232396447509, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16300877609441133, 0.1630087760944112, 0.26860649746437015], 
reward next is 0.7314, 
noisyNet noise sample is [array([-1.7521847], dtype=float32), 1.6691085]. 
=============================================
[2019-03-26 03:21:57,807] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:21:57,820] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6212
[2019-03-26 03:21:57,829] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6887749523509226, 6.911199999999999, 6.9112, 168.912956510431, 590582.9688820627, 590582.9688820634, 180734.8260902976], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2670000.0000, 
sim time next is 2670600.0000, 
raw observation next is [23.0, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6886049693292974, 6.9112, 6.9112, 168.912956510431, 590437.1791572652, 590437.1791572652, 180704.8642253236], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.6202499625967041, 0.0, 0.0, 0.8294399451523027, 0.16401032754368478, 0.16401032754368478, 0.26970875257510984], 
reward next is 0.7303, 
noisyNet noise sample is [array([-2.6209047], dtype=float32), 0.44804627]. 
=============================================
[2019-03-26 03:22:06,282] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.04273525e-07 0.00000000e+00 0.00000000e+00 0.00000000e+00
 9.99999881e-01], sum to 1.0000
[2019-03-26 03:22:06,291] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4263
[2019-03-26 03:22:06,302] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.2068590815842133, 1.0, 2.0, 0.2068590815842133, 1.0, 2.0, 0.3628786379281531, 6.9112, 6.9112, 170.5573041426782, 937125.1765215546, 937125.1765215546, 279727.7405314107], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2911200.0000, 
sim time next is 2911800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.1981804149489828, 1.0, 2.0, 0.1981804149489828, 1.0, 2.0, 0.3479610375564882, 6.911199999999999, 6.9112, 170.5573041426782, 899011.2554334349, 899011.2554334354, 277311.1703584705], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.03395230716744915, 1.0, 1.0, 0.03395230716744915, 1.0, 1.0, 0.20483053360547343, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.24972534873150967, 0.24972534873150984, 0.4138972691917471], 
reward next is 0.5861, 
noisyNet noise sample is [array([0.5548328], dtype=float32), -1.3887457]. 
=============================================
[2019-03-26 03:22:28,118] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 9.375698e-30], sum to 1.0000
[2019-03-26 03:22:28,130] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7349
[2019-03-26 03:22:28,137] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8437810175796359, 6.911200000000001, 6.9112, 168.912956510431, 702070.5489921827, 702070.548992182, 210974.0469073773], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3278400.0000, 
sim time next is 3279000.0000, 
raw observation next is [27.16666666666666, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8321156431084469, 6.9112, 6.9112, 168.912956510431, 693920.1639008125, 693920.1639008125, 208498.7691843932], 
processed observation next is [0.0, 0.9565217391304348, 0.4865718799368086, 0.79, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.795262979400545, 0.0, 0.0, 0.8294399451523027, 0.19275560108355902, 0.19275560108355902, 0.31119219281252714], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.7130202], dtype=float32), -0.8650801]. 
=============================================
[2019-03-26 03:22:28,156] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.259254]
 [79.0143  ]
 [78.87393 ]
 [78.77679 ]
 [78.60916 ]], R is [[79.42378998]
 [79.31466675]
 [79.20332336]
 [79.09025574]
 [78.97608948]].
[2019-03-26 03:22:29,140] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:22:29,149] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8229
[2019-03-26 03:22:29,158] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6013050316191789, 6.9112, 6.9112, 168.912956510431, 523209.2688497411, 523209.2688497411, 166284.8994432739], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 2963400.0000, 
sim time next is 2964000.0000, 
raw observation next is [21.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5990819265202487, 6.911200000000001, 6.9112, 168.912956510431, 521168.9336773183, 521168.9336773177, 165948.41503255], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.5110755201466447, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14476914824369952, 0.14476914824369935, 0.2476842015411194], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.44967854], dtype=float32), -1.4401373]. 
=============================================
[2019-03-26 03:22:29,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[75.2816  ]
 [75.34177 ]
 [75.285645]
 [75.29594 ]
 [75.32358 ]], R is [[75.2427063 ]
 [75.24209595]
 [75.24347687]
 [75.24715424]
 [75.25234985]].
[2019-03-26 03:22:29,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:22:29,438] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2241
[2019-03-26 03:22:29,444] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.16666666666667, 99.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.558591463171263, 6.911200000000001, 6.9112, 168.912956510431, 490771.874960068, 490771.8749600674, 159904.6385600406], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [20.33333333333334, 98.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5452446368230801, 6.9112, 6.9112, 168.912956510431, 478834.0511005365, 478834.0511005365, 158057.7576267705], 
processed observation next is [1.0, 0.13043478260869565, 0.16271721958925783, 0.98, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.44542028880863427, 0.0, 0.0, 0.8294399451523027, 0.1330094586390379, 0.1330094586390379, 0.23590710093547837], 
reward next is 0.7641, 
noisyNet noise sample is [array([1.3702221], dtype=float32), 1.0494064]. 
=============================================
[2019-03-26 03:22:29,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.018906]
 [79.37304 ]
 [79.51242 ]
 [79.93886 ]
 [80.265945]], R is [[78.80639648]
 [78.77967072]
 [78.7520752 ]
 [78.72451019]
 [78.69589996]].
[2019-03-26 03:22:33,172] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:22:33,184] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7188
[2019-03-26 03:22:33,193] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.553751683877052, 6.9112, 6.9112, 168.912956510431, 486628.5057558087, 486628.5057558087, 159223.522197194], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3018600.0000, 
sim time next is 3019200.0000, 
raw observation next is [20.0, 100.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5527194962731952, 6.9112, 6.9112, 168.912956510431, 485721.2277670297, 485721.2277670297, 159079.8333416576], 
processed observation next is [1.0, 0.9565217391304348, 0.1469194312796209, 1.0, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.45453597106487215, 0.0, 0.0, 0.8294399451523027, 0.13492256326861937, 0.13492256326861937, 0.23743258707710088], 
reward next is 0.7626, 
noisyNet noise sample is [array([-2.0103414], dtype=float32), -0.09563776]. 
=============================================
[2019-03-26 03:22:39,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.9738636e-13 1.6315092e-38 1.2618828e-30 1.2767254e-27 1.0000000e+00], sum to 1.0000
[2019-03-26 03:22:39,137] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1923
[2019-03-26 03:22:39,143] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [23.5, 97.0, 1.0, 2.0, 0.2600181828834158, 1.0, 2.0, 0.2600181828834158, 1.0, 2.0, 0.4346998324484037, 6.9112, 6.9112, 170.5573041426782, 1097512.302697714, 1097512.302697714, 288265.8811666738], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3083400.0000, 
sim time next is 3084000.0000, 
raw observation next is [23.33333333333333, 98.0, 1.0, 2.0, 0.2799279784085133, 1.0, 2.0, 0.2799279784085133, 1.0, 2.0, 0.4683009935783421, 6.9112, 6.9112, 170.5573041426782, 1182731.48612582, 1182731.48612582, 295604.8495032457], 
processed observation next is [1.0, 0.6956521739130435, 0.30489731437598716, 0.98, 1.0, 1.0, 0.1324433474801365, 1.0, 1.0, 0.1324433474801365, 1.0, 1.0, 0.3515865775345635, 0.0, 0.0, 0.8375144448122397, 0.3285365239238389, 0.3285365239238389, 0.44120126791529213], 
reward next is 0.5588, 
noisyNet noise sample is [array([1.3694278], dtype=float32), 2.129293]. 
=============================================
[2019-03-26 03:22:39,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[63.705887]
 [63.131496]
 [62.065556]
 [61.437763]
 [61.309143]], R is [[63.88598633]
 [63.81687927]
 [63.74725342]
 [63.66581345]
 [63.63481903]].
[2019-03-26 03:22:39,368] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:22:39,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7891
[2019-03-26 03:22:39,385] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.5, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9678383275616655, 6.9112, 6.9112, 168.912956510431, 782286.2972326354, 782286.2972326354, 239025.4521806863], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3245400.0000, 
sim time next is 3246000.0000, 
raw observation next is [32.66666666666667, 63.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9774270165777346, 6.911199999999999, 6.9112, 168.912956510431, 788129.6249392093, 788129.6249392099, 241320.9435436652], 
processed observation next is [0.0, 0.5652173913043478, 0.7472353870458138, 0.63, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9724719714362616, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21892489581644703, 0.2189248958164472, 0.3601805127517391], 
reward next is 0.6398, 
noisyNet noise sample is [array([0.9534823], dtype=float32), 0.41392168]. 
=============================================
[2019-03-26 03:22:39,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.20615 ]
 [64.22562 ]
 [64.217804]
 [64.23882 ]
 [64.03826 ]], R is [[64.17023468]
 [64.17177582]
 [64.17557526]
 [64.17902374]
 [64.18659973]].
[2019-03-26 03:22:47,309] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2792578e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 03:22:47,319] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1474
[2019-03-26 03:22:47,328] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 77.66666666666667, 1.0, 2.0, 0.1967113530724814, 1.0, 1.0, 0.1967113530724814, 1.0, 1.0, 0.3416225668483642, 6.9112, 6.9112, 170.5573041426782, 824666.4648578722, 824666.4648578722, 268941.077683458], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3343200.0000, 
sim time next is 3343800.0000, 
raw observation next is [30.16666666666666, 78.33333333333334, 1.0, 2.0, 0.1958063284755038, 1.0, 2.0, 0.1958063284755038, 1.0, 2.0, 0.340050838419622, 6.9112, 6.9112, 170.5573041426782, 820870.9083537626, 820870.9083537626, 268695.2555085746], 
processed observation next is [0.0, 0.6956521739130435, 0.6287519747235385, 0.7833333333333334, 1.0, 1.0, 0.03109196201867927, 1.0, 1.0, 0.03109196201867927, 1.0, 1.0, 0.19518394929222196, 0.0, 0.0, 0.8375144448122397, 0.22801969676493405, 0.22801969676493405, 0.4010376947889173], 
reward next is 0.5990, 
noisyNet noise sample is [array([-0.03886908], dtype=float32), 0.13578193]. 
=============================================
[2019-03-26 03:22:49,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:22:49,024] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8073
[2019-03-26 03:22:49,029] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [32.0, 65.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9835696307238004, 6.911200000000001, 6.9112, 168.9129462910714, 792069.3816977444, 792069.3816977437, 242813.0853593596], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3241200.0000, 
sim time next is 3241800.0000, 
raw observation next is [32.0, 65.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9733335454871083, 6.9112, 6.9112, 168.912956510431, 785180.5101975994, 785180.5101975994, 240313.0008900828], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.65, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9674799335208636, 0.0, 0.0, 0.8294399451523027, 0.21810569727711096, 0.21810569727711096, 0.3586761207314669], 
reward next is 0.6413, 
noisyNet noise sample is [array([-1.2911087], dtype=float32), -0.47650695]. 
=============================================
[2019-03-26 03:22:55,852] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 03:22:55,856] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:22:55,857] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:22:55,857] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:22:55,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:22:55,859] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:22:55,861] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:22:55,862] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:22:55,863] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:22:55,870] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:22:55,870] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:22:55,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-26 03:22:55,918] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-26 03:22:55,945] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-26 03:22:55,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-26 03:22:56,002] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-26 03:23:04,229] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:23:04,232] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.6, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6838152442040352, 6.9112, 6.9112, 168.912956510431, 585022.3965038194, 585022.3965038194, 179862.6111776201]
[2019-03-26 03:23:04,237] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:23:04,241] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.4004420709443943
[2019-03-26 03:23:12,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:23:12,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.05, 77.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.489940672224265, 6.911199999999999, 6.9112, 168.912956510431, 433649.596285699, 433649.5962856996, 150765.5783602308]
[2019-03-26 03:23:12,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:23:12,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.2537807263760352
[2019-03-26 03:23:28,997] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:23:29,000] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.46666666666667, 94.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6879345260952142, 6.911200000000001, 6.9112, 168.912956510431, 592883.8028149763, 592883.8028149757, 180577.4567046519]
[2019-03-26 03:23:29,001] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:23:29,004] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7670080575205193
[2019-03-26 03:23:34,652] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:23:34,654] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 81.5, 1.0, 2.0, 0.4579970665661136, 0.0, 1.0, 0.0, 1.0, 1.0, 0.7784689965462731, 6.9112, 6.9112, 168.9129558797871, 1280309.88199038, 1280309.88199038, 283579.579308097]
[2019-03-26 03:23:34,655] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:23:34,657] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.00000e+00 0.00000e+00 0.00000e+00 1.66254e-30 0.00000e+00], sampled 0.8618678895240617
[2019-03-26 03:23:34,658] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1280309.88199038 W.
[2019-03-26 03:23:57,141] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:23:57,142] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.30091898833333, 78.614258855, 1.0, 2.0, 0.6495593372688578, 1.0, 1.0, 0.6495593372688578, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 1816236.119003967, 1816236.119003966, 353754.3707699638]
[2019-03-26 03:23:57,145] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:23:57,148] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.99976516e-01 1.08587255e-33 5.04002203e-33 2.34758827e-05
 5.19366565e-18], sampled 0.5676752956446041
[2019-03-26 03:23:57,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1816236.119003967 W.
[2019-03-26 03:24:05,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:24:05,589] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.56666666666667, 63.66666666666667, 1.0, 1.0, 1.04, 1.0, 1.0, 1.04, 1.0, 2.0, 1.03, 11.9753994937029, 6.9112, 178.6582176852504, 7544613.903833747, 3744618.310534215, 658068.4653517558]
[2019-03-26 03:24:05,589] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:24:05,593] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 0. 0. 1. 0.], sampled 0.41417700996017615
[2019-03-26 03:24:05,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 7544613.903833747 W.
[2019-03-26 03:25:02,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.82601166], dtype=float32), 0.095981896]
[2019-03-26 03:25:02,892] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.7993927, 65.71534862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8296061647505923, 6.9112, 6.9112, 168.912956510431, 691578.4838002466, 691578.4838002466, 207954.8488265701]
[2019-03-26 03:25:02,893] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:25:02,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9960978890594578
[2019-03-26 03:25:04,669] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7306.1674 3185836304.5549 1525.0000
[2019-03-26 03:25:05,356] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7529.2157 3126049437.4620 1476.0000
[2019-03-26 03:25:05,536] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8131.8386 2959922609.0911 968.0000
[2019-03-26 03:25:05,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7436.6156 3321733279.4549 1354.0000
[2019-03-26 03:25:05,703] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8068.8176 3002567209.9849 978.0000
[2019-03-26 03:25:06,721] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2425000, evaluation results [2425000.0, 7436.615604712198, 3321733279.4548655, 1354.0, 7529.2156915226105, 3126049437.4619637, 1476.0, 8131.8385802305975, 2959922609.091063, 968.0, 7306.167404264374, 3185836304.5549006, 1525.0, 8068.817611067298, 3002567209.9848757, 978.0]
[2019-03-26 03:25:09,373] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:25:09,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0916
[2019-03-26 03:25:09,401] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2053214.644997831 W.
[2019-03-26 03:25:09,406] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.4894819987150272, 1.0, 2.0, 0.4894819987150272, 1.0, 2.0, 0.8500683575974448, 6.9112, 6.9112, 170.5573041426782, 2053214.644997831, 2053214.644997831, 407950.1724986901], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3416400.0000, 
sim time next is 3417000.0000, 
raw observation next is [33.16666666666666, 66.33333333333334, 1.0, 2.0, 0.4404338189301903, 1.0, 2.0, 0.4404338189301903, 1.0, 2.0, 0.7648878897921015, 6.9112, 6.9112, 170.5573041426782, 1847296.423967939, 1847296.423967939, 376401.5935752608], 
processed observation next is [1.0, 0.5652173913043478, 0.7709320695102682, 0.6633333333333334, 1.0, 1.0, 0.3258238782291449, 1.0, 1.0, 0.3258238782291449, 1.0, 1.0, 0.7132779143806115, 0.0, 0.0, 0.8375144448122397, 0.5131378955466498, 0.5131378955466498, 0.5617934232466579], 
reward next is 0.4382, 
noisyNet noise sample is [array([-0.78614557], dtype=float32), -0.44501916]. 
=============================================
[2019-03-26 03:25:09,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[40.935425]
 [39.93298 ]
 [40.31323 ]
 [38.12325 ]
 [36.865543]], R is [[42.45588684]
 [42.42245102]
 [42.35296249]
 [42.30210114]
 [42.20582581]].
[2019-03-26 03:25:10,205] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 9.5408195e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 03:25:10,216] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5328
[2019-03-26 03:25:10,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 1219356.7586232 W.
[2019-03-26 03:25:10,233] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.436207622734698, 1.0, 1.0, 0.436207622734698, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1219356.7586232, 1219356.7586232, 281640.5834652384], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3396000.0000, 
sim time next is 3396600.0000, 
raw observation next is [28.5, 86.5, 1.0, 2.0, 0.444720443482496, 1.0, 2.0, 0.444720443482496, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1243166.942788008, 1243166.942788008, 283987.2401293319], 
processed observation next is [1.0, 0.30434782608695654, 0.5497630331753555, 0.865, 1.0, 1.0, 0.3309884861234892, 1.0, 1.0, 0.3309884861234892, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3453241507744466, 0.3453241507744466, 0.42386155243183865], 
reward next is 0.5761, 
noisyNet noise sample is [array([-0.16410129], dtype=float32), 0.45180276]. 
=============================================
[2019-03-26 03:25:13,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.7217043e-22 1.0670395e-30 7.6951198e-34 1.0000000e+00 3.0226850e-12], sum to 1.0000
[2019-03-26 03:25:13,539] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1678
[2019-03-26 03:25:13,549] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.8372341078322794, 1.0, 1.0, 0.8372341078322794, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2341574.3415973, 2341574.3415973, 438394.062841801], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3745800.0000, 
sim time next is 3746400.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.7377131042291774, 1.0, 2.0, 0.7377131042291774, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2062983.934480418, 2062983.934480418, 390678.1509915289], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.6839916918423823, 1.0, 1.0, 0.6839916918423823, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5730510929112272, 0.5730510929112272, 0.5831017178978043], 
reward next is 0.4169, 
noisyNet noise sample is [array([0.32564053], dtype=float32), -0.10440396]. 
=============================================
[2019-03-26 03:25:17,769] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5134641e-13 3.5656806e-22 5.1346281e-26 1.0000000e+00 1.4593629e-08], sum to 1.0000
[2019-03-26 03:25:17,781] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6184
[2019-03-26 03:25:17,787] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333333, 66.83333333333333, 1.0, 2.0, 0.8642411261692159, 1.0, 2.0, 0.8642411261692159, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2417180.557747778, 2417180.557747778, 452360.6195343782], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3498600.0000, 
sim time next is 3499200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.8110709563245065, 1.0, 2.0, 0.8110709563245065, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268334.894586516, 2268334.894586516, 425302.1699648786], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.7723746461741042, 1.0, 1.0, 0.7723746461741042, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6300930262740323, 0.6300930262740323, 0.6347793581565352], 
reward next is 0.3652, 
noisyNet noise sample is [array([-0.62107664], dtype=float32), -0.7812105]. 
=============================================
[2019-03-26 03:25:18,038] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.0000000e+00 1.6932659e-26 1.6861794e-29 1.2231795e-16 1.3780863e-19], sum to 1.0000
[2019-03-26 03:25:18,045] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5281
[2019-03-26 03:25:18,055] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 1045778.347248348 W.
[2019-03-26 03:25:18,060] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.7482817914715639, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1045778.347248348, 1045778.347248348, 231609.2523354227], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3478800.0000, 
sim time next is 3479400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.2654287279847461, 1.0, 1.0, 0.2654287279847461, 1.0, 1.0, 0.4489711243259621, 6.911199999999999, 6.9112, 170.5573041426782, 1112897.518641244, 1112897.518641244, 289897.4383207829], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.79, 1.0, 1.0, 0.11497437106595913, 1.0, 0.5, 0.11497437106595913, 1.0, 0.5, 0.3280135662511733, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.3091381996225678, 0.3091381996225678, 0.43268274376236254], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5352317], dtype=float32), 1.6764839]. 
=============================================
[2019-03-26 03:25:21,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7337846e-35 3.5132112e-34 1.1966695e-34 8.7775785e-01 1.2224215e-01], sum to 1.0000
[2019-03-26 03:25:21,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2542
[2019-03-26 03:25:21,267] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5608704842380974, 1.0, 2.0, 0.5608704842380974, 1.0, 2.0, 0.9740465484180957, 6.9112, 6.9112, 170.5573041426782, 2352972.555129437, 2352972.555129437, 459790.7667201828], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3668400.0000, 
sim time next is 3669000.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.8150478593303485, 1.0, 2.0, 0.8150478593303485, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2279467.304973441, 2279467.30497344, 427263.5766779101], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.63, 1.0, 1.0, 0.7771660955787332, 1.0, 1.0, 0.7771660955787332, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6331853624926226, 0.6331853624926221, 0.6377068308625524], 
reward next is 0.3623, 
noisyNet noise sample is [array([0.44369113], dtype=float32), 1.6926117]. 
=============================================
[2019-03-26 03:25:21,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[47.366734]
 [47.43582 ]
 [47.875736]
 [48.161926]
 [48.728226]], R is [[47.80294418]
 [47.63866043]
 [47.45804596]
 [47.28526688]
 [47.14065552]].
[2019-03-26 03:25:25,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2554202e-08 8.7417671e-25 9.0098084e-24 9.9999988e-01 2.6517993e-38], sum to 1.0000
[2019-03-26 03:25:25,396] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3431
[2019-03-26 03:25:25,401] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 61.66666666666667, 1.0, 2.0, 0.6570482271692284, 1.0, 2.0, 0.6491141530988768, 1.0, 1.0, 1.03, 7.00509434569309, 6.9112, 170.5573041426782, 2723577.240622964, 2656316.901425217, 508104.7692045119], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3594000.0000, 
sim time next is 3594600.0000, 
raw observation next is [33.0, 61.0, 1.0, 2.0, 0.9716570737878357, 1.0, 2.0, 0.9716570737878357, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2717937.065395884, 2717937.065395883, 512092.8870905788], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.61, 1.0, 1.0, 0.9658518961299225, 1.0, 1.0, 0.9658518961299225, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7549825181655234, 0.754982518165523, 0.764317741926237], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.360134], dtype=float32), 0.57265115]. 
=============================================
[2019-03-26 03:25:30,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 1.1238829e-25 2.5291020e-37 4.6154471e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 03:25:30,727] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4212
[2019-03-26 03:25:30,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 2323869.361033925 W.
[2019-03-26 03:25:30,748] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.8309095407359655, 1.0, 2.0, 0.8309095407359655, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2323869.361033925, 2323869.361033925, 435199.2781765345], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3672000.0000, 
sim time next is 3672600.0000, 
raw observation next is [32.16666666666667, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.244811568918596, 6.9112, 168.9054004666492, 3234235.391203229, 2288167.767519591, 473026.5944878118], 
processed observation next is [1.0, 0.5217391304347826, 0.7235387045813588, 0.63, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.13336115689185962, 0.0, 0.8294028415181622, 0.8983987197786747, 0.6356021576443308, 0.706009842519122], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.68402636], dtype=float32), -0.11835426]. 
=============================================
[2019-03-26 03:25:34,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.9996912e-01 1.6485819e-23 8.7046767e-29 3.0851115e-05 0.0000000e+00], sum to 1.0000
[2019-03-26 03:25:34,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4424
[2019-03-26 03:25:34,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2608587.700618733 W.
[2019-03-26 03:25:34,784] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 67.0, 1.0, 2.0, 0.9326056629405096, 1.0, 1.0, 0.9326056629405096, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2608587.700618733, 2608587.700618733, 489607.3591929553], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3762000.0000, 
sim time next is 3762600.0000, 
raw observation next is [34.16666666666667, 65.83333333333334, 1.0, 2.0, 0.5704638794884397, 1.0, 2.0, 0.5704638794884397, 1.0, 1.0, 0.990707103383651, 6.911199999999999, 6.9112, 170.5573041426782, 2393257.454651009, 2393257.45465101, 467294.9850002512], 
processed observation next is [1.0, 0.5652173913043478, 0.8183254344391787, 0.6583333333333334, 1.0, 1.0, 0.4824866017933008, 1.0, 1.0, 0.4824866017933008, 1.0, 0.5, 0.9886671992483549, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.664793737403058, 0.6647937374030584, 0.6974552014929123], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.784655], dtype=float32), 0.7102574]. 
=============================================
[2019-03-26 03:25:44,579] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:25:44,590] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3677
[2019-03-26 03:25:44,596] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.75, 90.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.025540332756329, 6.9112, 6.9112, 168.9127722461745, 827995.8541563777, 827995.8541563777, 253794.1403169562], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3893400.0000, 
sim time next is 3894000.0000, 
raw observation next is [27.66666666666666, 91.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.015285699525447, 6.9112, 6.9112, 168.9128443175578, 819881.673579733, 819881.673579733, 251105.2762708637], 
processed observation next is [0.0, 0.043478260869565216, 0.5102685624012636, 0.91, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0186410969822524, 0.0, 0.0, 0.8294393942339332, 0.2277449093277036, 0.2277449093277036, 0.3747839944341249], 
reward next is 0.6252, 
noisyNet noise sample is [array([0.05579061], dtype=float32), -0.051873356]. 
=============================================
[2019-03-26 03:25:44,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[52.999916]
 [52.774536]
 [52.290283]
 [51.674248]
 [50.392906]], R is [[53.19570541]
 [53.28495407]
 [53.34859085]
 [53.40214539]
 [53.48794174]].
[2019-03-26 03:25:44,898] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.254556e-24 0.000000e+00 0.000000e+00 0.000000e+00 1.000000e+00], sum to 1.0000
[2019-03-26 03:25:44,914] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9904
[2019-03-26 03:25:44,918] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.75, 60.5, 1.0, 2.0, 0.2079742374353858, 1.0, 1.0, 0.2079742374353858, 1.0, 2.0, 0.3611824722939538, 6.9112, 6.9112, 170.5573041426782, 871902.6739222336, 871902.6739222336, 272095.1752449629], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3850200.0000, 
sim time next is 3850800.0000, 
raw observation next is [34.83333333333334, 60.33333333333334, 1.0, 2.0, 0.207284195530915, 1.0, 2.0, 0.207284195530915, 1.0, 2.0, 0.3599840977062329, 6.9112, 6.9112, 170.5573041426782, 869008.5984384523, 869008.5984384523, 271896.9709987016], 
processed observation next is [0.0, 0.5652173913043478, 0.8499210110584523, 0.6033333333333334, 1.0, 1.0, 0.044920717507126506, 1.0, 1.0, 0.044920717507126506, 1.0, 1.0, 0.21949280208077182, 0.0, 0.0, 0.8375144448122397, 0.24139127734401455, 0.24139127734401455, 0.40581637462492776], 
reward next is 0.5942, 
noisyNet noise sample is [array([1.2198672], dtype=float32), 0.461728]. 
=============================================
[2019-03-26 03:25:47,360] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3058949e-02 9.7245789e-20 2.8545052e-21 2.4699904e-03 9.8447108e-01], sum to 1.0000
[2019-03-26 03:25:47,372] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4052
[2019-03-26 03:25:47,381] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.66666666666666, 58.0, 1.0, 2.0, 0.7536043768310873, 1.0, 2.0, 0.6973922279298062, 1.0, 2.0, 1.03, 7.005101959191959, 6.9112, 170.5573041426782, 2926381.446501399, 2859115.653444715, 538796.6878835781], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4275600.0000, 
sim time next is 4276200.0000, 
raw observation next is [36.83333333333334, 57.5, 1.0, 2.0, 0.7315179005278883, 1.0, 2.0, 0.6863489897782068, 1.0, 2.0, 1.03, 7.005100217444723, 6.9112, 170.5573041426782, 2879988.640418373, 2812724.095046083, 531471.1961153896], 
processed observation next is [1.0, 0.4782608695652174, 0.9447077409162722, 0.575, 1.0, 1.0, 0.6765275909974559, 1.0, 1.0, 0.6221072166002491, 1.0, 1.0, 1.0365853658536586, 0.00939002174447232, 0.0, 0.8375144448122397, 0.7999968445606592, 0.7813122486239119, 0.7932405912169994], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1195848], dtype=float32), 2.5675557]. 
=============================================
[2019-03-26 03:25:47,669] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:25:47,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4919
[2019-03-26 03:25:47,696] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 79.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.982045399235081, 6.9112, 6.9112, 168.912956510431, 790199.3615984996, 790199.3615984996, 242391.077553913], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4039200.0000, 
sim time next is 4039800.0000, 
raw observation next is [29.83333333333333, 79.00000000000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9983769836129206, 6.9112, 6.9112, 168.9128838434998, 803345.4828989182, 803345.4828989182, 246552.0240314902], 
processed observation next is [1.0, 0.782608695652174, 0.6129541864139019, 0.7900000000000001, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9980207117230739, 0.0, 0.0, 0.8294395883244035, 0.2231515230274773, 0.2231515230274773, 0.3679880955693884], 
reward next is 0.6320, 
noisyNet noise sample is [array([-1.2665442], dtype=float32), -0.45604366]. 
=============================================
[2019-03-26 03:25:47,747] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4887835e-10 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.0000000e+00], sum to 1.0000
[2019-03-26 03:25:47,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5870
[2019-03-26 03:25:47,764] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 84.0, 1.0, 1.0, 0.3232801286481158, 1.0, 1.0, 0.3232801286481158, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 903550.9060282306, 903550.9060282306, 254547.4964907856], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3974400.0000, 
sim time next is 3975000.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 0.2129230582301208, 1.0, 2.0, 0.2129230582301208, 1.0, 1.0, 0.3697769374143629, 6.9112, 6.9112, 170.5573041426782, 892658.542173916, 892658.542173916, 273535.5983379748], 
processed observation next is [1.0, 0.0, 0.6129541864139019, 0.84, 1.0, 1.0, 0.05171452798809735, 1.0, 1.0, 0.05171452798809735, 1.0, 0.5, 0.23143528952971085, 0.0, 0.0, 0.8375144448122397, 0.24796070615942112, 0.24796070615942112, 0.40826208707160416], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.23964167], dtype=float32), -0.5722772]. 
=============================================
[2019-03-26 03:25:47,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[32.924458]
 [44.597466]
 [42.297295]
 [44.06958 ]
 [44.090736]], R is [[27.01264572]
 [26.74251938]
 [26.47509384]
 [26.83424759]
 [27.16009903]].
[2019-03-26 03:25:49,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 8.800918e-19], sum to 1.0000
[2019-03-26 03:25:49,752] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9390
[2019-03-26 03:25:49,757] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [31.33333333333334, 73.66666666666666, 1.0, 1.0, 0.2047872516927511, 1.0, 1.0, 0.2047872516927511, 1.0, 2.0, 0.3556477320112876, 6.9112, 6.9112, 170.5573041426782, 858536.3374535043, 858536.3374535043, 271184.417064975], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 3919200.0000, 
sim time next is 3919800.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.050000991603626, 6.9112, 168.9120460450044, 927309.0084092219, 828839.3385728775, 254812.7434711641], 
processed observation next is [0.0, 0.34782608695652173, 0.6998420221169038, 0.7233333333333334, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.013880099160362569, 0.0, 0.8294354743502451, 0.2575858356692283, 0.23023314960357708, 0.38031752756890164], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.13741428], dtype=float32), -1.2771333]. 
=============================================
[2019-03-26 03:25:59,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:25:59,673] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1692
[2019-03-26 03:25:59,678] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.33333333333334, 87.33333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9224596131077653, 6.911200000000001, 6.9112, 168.912956510431, 754801.8454693147, 754801.8454693141, 228439.9509790625], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4056000.0000, 
sim time next is 4056600.0000, 
raw observation next is [27.16666666666666, 88.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9235273704601907, 6.911200000000001, 6.9112, 168.912956510431, 756193.4880246463, 756193.4880246457, 228716.8980875319], 
processed observation next is [1.0, 0.9565217391304348, 0.4865718799368086, 0.8816666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9067406956831594, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21005374667351287, 0.2100537466735127, 0.3413685046082566], 
reward next is 0.6586, 
noisyNet noise sample is [array([0.49432528], dtype=float32), -0.39661887]. 
=============================================
[2019-03-26 03:26:01,409] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:26:01,417] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0648
[2019-03-26 03:26:01,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1007472.997110602 W.
[2019-03-26 03:26:01,434] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.0, 71.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.162955437019765, 6.9112, 168.9114592336574, 1007472.997110602, 828870.6085151361, 254813.9463481841], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4125600.0000, 
sim time next is 4126200.0000, 
raw observation next is [32.66666666666666, 72.33333333333334, 1.0, 1.0, 0.327000085602889, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5678910081114243, 6.9112, 6.9112, 168.9127515203691, 913956.2117439493, 913956.2117439493, 231054.7152325744], 
processed observation next is [1.0, 0.782608695652174, 0.7472353870458132, 0.7233333333333334, 1.0, 0.5, 0.18915672964203495, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.47303781477002965, 0.0, 0.0, 0.8294389385572539, 0.25387672548443035, 0.25387672548443035, 0.3448577839292155], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.0231109], dtype=float32), 1.3272408]. 
=============================================
[2019-03-26 03:26:08,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:26:08,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9994
[2019-03-26 03:26:08,230] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 910640.9942753892 W.
[2019-03-26 03:26:08,237] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.33333333333334, 60.83333333333333, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.026515147396769, 6.9112, 168.9121776200561, 910640.9942753892, 828832.8371060519, 254813.1272174171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4305000.0000, 
sim time next is 4305600.0000, 
raw observation next is [34.0, 63.0, 1.0, 1.0, 0.6206224839631425, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9128626159542, 867292.3091841729, 867292.3091841729, 204651.3078729753], 
processed observation next is [1.0, 0.8695652173913043, 0.8104265402843602, 0.63, 1.0, 0.5, 0.54291865537728, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294394840874365, 0.24091453032893692, 0.24091453032893692, 0.3054497132432467], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1830416], dtype=float32), -1.0156533]. 
=============================================
[2019-03-26 03:26:11,563] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:26:11,572] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8222
[2019-03-26 03:26:11,576] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.66666666666667, 67.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.958263831663111, 6.9112, 6.9112, 168.9129565104244, 778371.4890788099, 778371.4890788099, 236851.628449608], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4459200.0000, 
sim time next is 4459800.0000, 
raw observation next is [30.33333333333333, 66.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9248404668318172, 6.9112, 6.9112, 168.912956510431, 757011.0774279987, 757011.0774279987, 229018.139547549], 
processed observation next is [0.0, 0.6086956521739131, 0.6366508688783569, 0.6683333333333333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9083420327217282, 0.0, 0.0, 0.8294399451523027, 0.21028085484111073, 0.21028085484111073, 0.3418181187276851], 
reward next is 0.6582, 
noisyNet noise sample is [array([-1.2337763], dtype=float32), -0.83738554]. 
=============================================
[2019-03-26 03:26:14,610] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 03:26:14,612] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:26:14,614] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:26:14,614] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:26:14,614] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:26:14,615] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:26:14,616] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:26:14,619] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:26:14,620] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:26:14,620] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:26:14,622] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:26:14,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-26 03:26:14,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-26 03:26:14,707] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-26 03:26:14,737] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-26 03:26:14,738] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-26 03:26:50,407] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:26:50,408] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.73333333333333, 94.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6844702877426302, 6.9112, 6.9112, 168.912956510431, 588537.9566894369, 588537.9566894369, 179975.5604943095]
[2019-03-26 03:26:50,410] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:26:50,413] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.5697571671515823
[2019-03-26 03:26:58,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:26:58,481] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.68715617, 70.66382621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8981086526435316, 6.911199999999999, 6.9112, 168.912956510431, 739671.9911065217, 739671.9911065223, 222925.3690291792]
[2019-03-26 03:26:58,482] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:26:58,486] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.07451021805607172
[2019-03-26 03:26:58,879] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:26:58,880] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.37895627333333, 84.57268416000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7374803296028961, 6.911200000000001, 6.9112, 168.912956510431, 629080.9550815937, 629080.9550815931, 189632.804623585]
[2019-03-26 03:26:58,880] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:26:58,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.547636774758549
[2019-03-26 03:27:13,940] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:27:13,941] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.88045458, 75.92460869, 1.0, 2.0, 0.7185048596775345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1004143.239363397, 1004143.239363397, 224887.6276400895]
[2019-03-26 03:27:13,942] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:27:13,945] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.7608950000468077
[2019-03-26 03:27:13,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1004143.239363397 W.
[2019-03-26 03:27:23,996] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:27:23,997] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.5, 58.66666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9381437570146378, 6.9112, 6.9112, 168.912956510431, 766122.3959276016, 766122.3959276016, 232134.3879037693]
[2019-03-26 03:27:23,998] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:27:24,000] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8475917680695881
[2019-03-26 03:28:05,284] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:28:05,286] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.6, 87.5, 1.0, 2.0, 0.8987183266899959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1256148.733362568, 1256148.733362569, 269581.5281795163]
[2019-03-26 03:28:05,287] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:28:05,289] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 5.8484675e-12], sampled 0.10805412872391507
[2019-03-26 03:28:05,292] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1256148.733362568 W.
[2019-03-26 03:28:07,578] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.9021916], dtype=float32), 0.14684728]
[2019-03-26 03:28:07,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.7, 70.0, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9959013288986742, 6.911200000000001, 6.9112, 168.9129564951231, 806766.1850022671, 806766.1850022664, 246220.871236268]
[2019-03-26 03:28:07,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:28:07,581] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.000000e+00 0.000000e+00 0.000000e+00 0.000000e+00 4.437893e-25], sampled 0.6683639174449088
[2019-03-26 03:28:23,119] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7960.7364 2992692035.4358 1423.0000
[2019-03-26 03:28:24,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7050.8176 3189835156.2466 2365.0000
[2019-03-26 03:28:24,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7361.2386 3110988337.0616 1957.0000
[2019-03-26 03:28:24,288] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7308.3932 3325030220.7123 2016.0000
[2019-03-26 03:28:24,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8063.7643 2943890883.8286 1316.0000
[2019-03-26 03:28:25,342] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 2450000, evaluation results [2450000.0, 7308.393225263055, 3325030220.7123194, 2016.0, 7361.2385638301, 3110988337.061556, 1957.0, 8063.764317684974, 2943890883.8286157, 1316.0, 7050.817634817087, 3189835156.2465672, 2365.0, 7960.736365673496, 2992692035.435787, 1423.0]
[2019-03-26 03:28:25,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:25,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3464
[2019-03-26 03:28:25,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 877126.6040291282 W.
[2019-03-26 03:28:25,847] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.16666666666666, 66.33333333333333, 1.0, 1.0, 0.3138284328868842, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5450162032758398, 6.911199999999999, 6.9112, 168.9128162058755, 877126.6040291282, 877126.6040291288, 226517.2416203965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4308600.0000, 
sim time next is 4309200.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.6096693204909136, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564753966, 851979.611204626, 851979.611204626, 202561.8953837135], 
processed observation next is [1.0, 0.9130434782608695, 0.7630331753554502, 0.67, 1.0, 1.0, 0.5297220728806188, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399449802677, 0.23666100311239613, 0.23666100311239613, 0.3023311871398709], 
reward next is 0.6977, 
noisyNet noise sample is [array([-0.907386], dtype=float32), 0.27021542]. 
=============================================
[2019-03-26 03:28:37,577] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:37,586] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5374
[2019-03-26 03:28:37,596] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 891933.826256136 W.
[2019-03-26 03:28:37,603] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 85.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.000156116052296, 6.9112, 168.9123019871118, 891933.826256136, 828825.5404355866, 254812.2391424684], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4419600.0000, 
sim time next is 4420200.0000, 
raw observation next is [29.0, 84.83333333333333, 1.0, 1.0, 0.2007637797427158, 1.0, 1.0, 0.2007637797427158, 1.0, 2.0, 0.3486602918165828, 6.9112, 6.9112, 170.5573041426782, 841661.9846998038, 841661.9846998038, 270055.1579933779], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.8483333333333333, 1.0, 0.5, 0.03706479487074193, 1.0, 0.5, 0.03706479487074193, 1.0, 1.0, 0.20568328270314976, 0.0, 0.0, 0.8375144448122397, 0.2337949957499455, 0.2337949957499455, 0.40306739999011626], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.49345028], dtype=float32), 1.1150906]. 
=============================================
[2019-03-26 03:28:40,754] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:40,763] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8749
[2019-03-26 03:28:40,775] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 983550.8277419556 W.
[2019-03-26 03:28:40,779] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.3518884906903207, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5882749545415429, 6.911199999999999, 6.9112, 168.912956510431, 983550.8277419556, 983550.8277419562, 237211.2053058997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4763400.0000, 
sim time next is 4764000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6615471552118452, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 924507.6077073995, 924507.6077073995, 212760.7957889485], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5922254882070423, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25680766880761097, 0.25680766880761097, 0.3175534265506694], 
reward next is 0.6824, 
noisyNet noise sample is [array([0.18943249], dtype=float32), -0.5670734]. 
=============================================
[2019-03-26 03:28:40,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[41.65582 ]
 [41.3031  ]
 [41.882534]
 [42.434185]
 [44.47454 ]], R is [[41.5865593 ]
 [41.81664658]
 [42.03086853]
 [42.18539047]
 [41.76353836]].
[2019-03-26 03:28:43,554] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:43,564] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9384
[2019-03-26 03:28:43,570] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [28.33333333333333, 72.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8268209883272016, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 207356.2052961754], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 4909200.0000, 
sim time next is 4909800.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8234874813422559, 6.9112, 6.9112, 168.912956510431, 686810.6418548356, 686810.6418548356, 206660.6411943133], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7333333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7847408309051902, 0.0, 0.0, 0.8294399451523027, 0.19078073384856542, 0.19078073384856542, 0.3084487182004676], 
reward next is 0.6916, 
noisyNet noise sample is [array([-0.8371502], dtype=float32), 1.3563771]. 
=============================================
[2019-03-26 03:28:46,537] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:46,545] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4092
[2019-03-26 03:28:46,555] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 982003.9503229399 W.
[2019-03-26 03:28:46,562] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.2342245866225346, 1.0, 1.0, 0.2342245866225346, 1.0, 1.0, 0.3975027316605932, 6.911200000000001, 6.9112, 170.5573041426782, 982003.9503229399, 982003.9503229392, 279474.1125710643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4950000.0000, 
sim time next is 4950600.0000, 
raw observation next is [27.33333333333333, 82.33333333333333, 1.0, 2.0, 0.6154522036737121, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860064.1469095564, 860064.1469095564, 203652.6903187393], 
processed observation next is [1.0, 0.30434782608695654, 0.494470774091627, 0.8233333333333333, 1.0, 1.0, 0.5366894020165206, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23890670747487677, 0.23890670747487677, 0.30395923928170043], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.0438247], dtype=float32), 0.5609387]. 
=============================================
[2019-03-26 03:28:57,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:28:57,488] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0682
[2019-03-26 03:28:57,498] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8549327959628396, 6.911200000000001, 6.9112, 168.912956510431, 708355.0793879715, 708355.079387971, 213322.7986113017], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5044800.0000, 
sim time next is 5045400.0000, 
raw observation next is [30.0, 66.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8560261496107959, 6.911200000000001, 6.9112, 168.912956510431, 709357.6101144654, 709357.6101144648, 213567.0910885875], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8244221336717024, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19704378058735147, 0.19704378058735134, 0.31875685237102613], 
reward next is 0.6812, 
noisyNet noise sample is [array([-0.00904149], dtype=float32), -0.629923]. 
=============================================
[2019-03-26 03:29:18,677] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:29:18,685] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5416
[2019-03-26 03:29:18,692] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 3337262.493180242 W.
[2019-03-26 03:29:18,702] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.6, 62.0, 1.0, 2.0, 0.9491789540097199, 1.0, 2.0, 0.7951795165191223, 1.0, 2.0, 1.03, 7.005117387915733, 6.9112, 170.5573041426782, 3337262.493180242, 3269985.647901118, 611596.9054002634], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5398200.0000, 
sim time next is 5398800.0000, 
raw observation next is [35.73333333333333, 62.66666666666667, 1.0, 2.0, 0.9406063506029563, 1.0, 2.0, 0.7908932148157406, 1.0, 2.0, 1.03, 7.005116711417177, 6.9112, 170.5573041426782, 3319249.546835862, 3251973.186160172, 608106.8520212197], 
processed observation next is [1.0, 0.4782608695652174, 0.8925750394944705, 0.6266666666666667, 1.0, 1.0, 0.928441386268622, 1.0, 1.0, 0.7480641142358321, 1.0, 1.0, 1.0365853658536586, 0.009391671141717683, 0.0, 0.8375144448122397, 0.9220137630099617, 0.9033258850444922, 0.9076221671958503], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.87283224], dtype=float32), 0.06791298]. 
=============================================
[2019-03-26 03:29:32,978] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:29:32,991] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5313
[2019-03-26 03:29:33,001] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2033613.668274486 W.
[2019-03-26 03:29:33,006] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.8132472869835399, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.990201507569141, 6.9112, 168.9124861365693, 2033613.668274486, 1977567.435855916, 411726.0491849904], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5218800.0000, 
sim time next is 5219400.0000, 
raw observation next is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.4764140270022353, 1.0, 1.0, 0.4764140270022353, 1.0, 2.0, 0.8231944866937718, 6.9112, 6.9112, 170.5573041426782, 1998347.711921022, 1998347.711921022, 398504.6359753687], 
processed observation next is [1.0, 0.391304347826087, 0.6603475513428123, 0.6666666666666665, 1.0, 1.0, 0.36917352650871726, 1.0, 0.5, 0.36917352650871726, 1.0, 1.0, 0.7843835203582582, 0.0, 0.0, 0.8375144448122397, 0.5550965866447284, 0.5550965866447284, 0.594783038769207], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.68325186], dtype=float32), 0.13620031]. 
=============================================
[2019-03-26 03:29:33,127] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 03:29:33,129] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:29:33,130] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:29:33,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:29:33,131] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:29:33,131] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:29:33,133] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:29:33,134] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:29:33,135] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:29:33,138] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:29:33,142] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:29:33,165] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-26 03:29:33,194] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-26 03:29:33,217] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-26 03:29:33,248] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-26 03:29:33,273] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-26 03:29:49,302] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:29:49,303] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.20946699666667, 97.91221734166668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7105209495340002, 6.911199999999999, 6.9112, 168.912956510431, 605592.5950711229, 605592.5950711234, 184621.5698997827]
[2019-03-26 03:29:49,305] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:29:49,308] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.32082787427629345
[2019-03-26 03:30:16,468] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:30:16,471] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.41666666666667, 79.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.008083045756128, 6.9112, 6.9112, 168.9128212284018, 815407.2720937681, 815407.2720937681, 249302.0954477392]
[2019-03-26 03:30:16,474] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:30:16,479] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.45030879001375257
[2019-03-26 03:30:34,692] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:30:34,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.0, 58.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.918210299647222, 6.9112, 168.9127324323242, 833776.1982934864, 828802.8570454436, 254812.1120125378]
[2019-03-26 03:30:34,694] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:30:34,698] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.8076308583448358
[2019-03-26 03:30:48,965] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:30:48,967] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.88333333333333, 43.66666666666667, 1.0, 2.0, 0.5614676860860076, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9750836905302467, 6.911199999999999, 6.9112, 168.912907636389, 1569771.13796569, 1569771.13796569, 343510.4292143839]
[2019-03-26 03:30:48,969] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:30:48,971] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9999952e-01 2.1581021e-38 1.4576021e-37 0.0000000e+00 4.1948385e-07], sampled 0.5229455690078977
[2019-03-26 03:30:48,971] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1569771.13796569 W.
[2019-03-26 03:31:16,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:31:16,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.846976415, 66.686214055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8723953782760453, 6.9112, 6.9112, 168.912956510431, 722200.5655576795, 722200.5655576795, 217190.7380925213]
[2019-03-26 03:31:16,495] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:31:16,497] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.09176665632851622
[2019-03-26 03:31:32,483] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.92390877], dtype=float32), 0.17540272]
[2019-03-26 03:31:32,485] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.86666666666667, 57.33333333333334, 1.0, 2.0, 0.5908654567609877, 0.0, 2.0, 0.0, 1.0, 2.0, 1.026137896201364, 6.911199999999999, 6.9112, 168.9128477753026, 1652026.427806512, 1652026.427806513, 361870.2880118802]
[2019-03-26 03:31:32,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:31:32,488] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.9827356e-25], sampled 0.821194502610651
[2019-03-26 03:31:32,489] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1652026.427806512 W.
[2019-03-26 03:31:42,307] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7367.6580 3117009408.1108 1909.0000
[2019-03-26 03:31:42,426] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8075.4664 2948055237.0161 1273.0000
[2019-03-26 03:31:42,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 7962.8154 2995448954.6425 1341.0000
[2019-03-26 03:31:42,925] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7335.5903 3328251223.4108 1922.0000
[2019-03-26 03:31:43,011] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7110.3101 3191783708.5864 2211.0000
[2019-03-26 03:31:44,030] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2475000, evaluation results [2475000.0, 7335.590321915298, 3328251223.4108195, 1922.0, 7367.65796135608, 3117009408.1107883, 1909.0, 8075.466385632722, 2948055237.0161133, 1273.0, 7110.310086934657, 3191783708.5863724, 2211.0, 7962.8154495229455, 2995448954.642464, 1341.0]
[2019-03-26 03:31:45,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:31:45,716] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6037
[2019-03-26 03:31:45,726] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 3 for the demand 2455506.42709781 W.
[2019-03-26 03:31:45,734] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5852871632567228, 1.0, 2.0, 0.5852871632567228, 1.0, 1.0, 1.016450245154308, 6.911199999999999, 6.9112, 170.5573041426782, 2455506.42709781, 2455506.427097811, 479132.861659389], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5241600.0000, 
sim time next is 5242200.0000, 
raw observation next is [31.83333333333334, 67.5, 1.0, 2.0, 0.936238152135501, 1.0, 2.0, 0.936238152135501, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2618758.770293789, 2618758.770293789, 491649.0505956605], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.675, 1.0, 1.0, 0.9231784965487964, 1.0, 1.0, 0.9231784965487964, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7274329917482747, 0.7274329917482747, 0.7338045531278515], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0419098], dtype=float32), 1.0745593]. 
=============================================
[2019-03-26 03:31:51,373] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:31:51,384] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1958
[2019-03-26 03:31:51,396] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2176830.18347101 W.
[2019-03-26 03:31:51,402] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.43333333333334, 53.66666666666667, 1.0, 2.0, 0.91556983355761, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990709113297768, 6.9112, 168.9124832783223, 2176830.18347101, 2120423.840035399, 438977.952496451], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5575200.0000, 
sim time next is 5575800.0000, 
raw observation next is [33.51666666666667, 52.83333333333334, 1.0, 2.0, 0.515409413354472, 1.0, 1.0, 0.515409413354472, 1.0, 2.0, 0.8888687711995532, 6.911200000000001, 6.9112, 170.5573041426782, 2162081.305703803, 2162081.305703803, 424800.83796598], 
processed observation next is [1.0, 0.5217391304347826, 0.7875197472353873, 0.5283333333333334, 1.0, 1.0, 0.4161559197041831, 1.0, 0.5, 0.4161559197041831, 1.0, 1.0, 0.8644741112189672, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6005781404732786, 0.6005781404732786, 0.6340311014417612], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18599576], dtype=float32), -1.971475]. 
=============================================
[2019-03-26 03:31:55,263] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:31:55,274] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8347
[2019-03-26 03:31:55,284] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.0, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8765735914147726, 6.911199999999999, 6.9112, 168.912956510431, 724675.4562992429, 724675.4562992435, 218099.0793024531], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5619600.0000, 
sim time next is 5620200.0000, 
raw observation next is [25.95, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8740834856069766, 6.9112, 6.9112, 168.912956510431, 722906.3343201919, 722906.3343201919, 217547.1150597096], 
processed observation next is [0.0, 0.043478260869565216, 0.42890995260663506, 0.9216666666666667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8464432751304592, 0.0, 0.0, 0.8294399451523027, 0.2008073150889422, 0.2008073150889422, 0.324697186656283], 
reward next is 0.6753, 
noisyNet noise sample is [array([0.8294338], dtype=float32), 1.0775766]. 
=============================================
[2019-03-26 03:31:56,923] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:31:56,932] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1522
[2019-03-26 03:31:56,942] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.15, 91.5, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8838825103490255, 6.9112, 6.9112, 168.912956510431, 729840.9116384251, 729840.9116384251, 219727.1107332793], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5617800.0000, 
sim time next is 5618400.0000, 
raw observation next is [26.1, 91.66666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8833966453732549, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 219626.9823085113], 
processed observation next is [0.0, 0.0, 0.4360189573459717, 0.9166666666666665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8578007870405547, 0.0, 0.0, 0.8294399451523027, 0.2027033896409691, 0.2027033896409691, 0.3278014661321064], 
reward next is 0.6722, 
noisyNet noise sample is [array([-0.52786547], dtype=float32), 0.8702565]. 
=============================================
[2019-03-26 03:31:57,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:31:57,517] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9896
[2019-03-26 03:31:57,522] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [25.7, 92.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8578625383999007, 6.9112, 6.9112, 168.912956510431, 712109.2127325597, 712109.2127325597, 214012.5487020431], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5626200.0000, 
sim time next is 5626800.0000, 
raw observation next is [25.7, 92.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.854911502384327, 6.911200000000001, 6.9112, 168.912956510431, 709887.8341999974, 709887.8341999968, 213368.0425379974], 
processed observation next is [0.0, 0.13043478260869565, 0.4170616113744076, 0.92, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8230628077857647, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1971910650555548, 0.19719106505555467, 0.31845976498208567], 
reward next is 0.6815, 
noisyNet noise sample is [array([1.2902743], dtype=float32), 0.43418187]. 
=============================================
[2019-03-26 03:32:00,129] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:00,138] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5699
[2019-03-26 03:32:00,147] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.43333333333333, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8609849500040665, 6.911199999999999, 6.9112, 168.912956510431, 714585.1043146537, 714585.1043146544, 214700.9557364608], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5708400.0000, 
sim time next is 5709000.0000, 
raw observation next is [26.41666666666666, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8594657081398175, 6.9112, 6.9112, 168.912956510431, 713491.6336039457, 713491.6336039457, 214369.2114082841], 
processed observation next is [0.0, 0.043478260869565216, 0.4510268562401261, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8286167172436797, 0.0, 0.0, 0.8294399451523027, 0.19819212044554047, 0.19819212044554047, 0.31995404687803597], 
reward next is 0.6800, 
noisyNet noise sample is [array([-0.2867054], dtype=float32), 1.477949]. 
=============================================
[2019-03-26 03:32:00,166] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[50.971592]
 [51.25647 ]
 [51.405155]
 [51.57419 ]
 [51.737316]], R is [[51.08068085]
 [51.24942398]
 [51.41596222]
 [51.58019638]
 [51.74186325]].
[2019-03-26 03:32:00,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:00,399] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6158
[2019-03-26 03:32:00,403] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [30.2, 82.0, 1.0, 2.0, 0.3079525201177865, 1.0, 2.0, 0.3079525201177865, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 860693.8579183081, 860693.8579183081, 251445.0162802577], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5434200.0000, 
sim time next is 5434800.0000, 
raw observation next is [30.13333333333333, 82.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.102144024599381, 6.9112, 168.9117751224021, 964315.0887929419, 828853.7733877341, 254812.9016094049], 
processed observation next is [1.0, 0.9130434782608695, 0.6271721958925749, 0.82, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.01909440245993812, 0.0, 0.829434143996239, 0.26786530244248385, 0.23023715927437058, 0.3803177635961267], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9842304], dtype=float32), 0.82216656]. 
=============================================
[2019-03-26 03:32:03,922] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:03,930] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1245
[2019-03-26 03:32:03,944] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [27.98333333333333, 86.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9676955194034988, 6.9112, 6.9112, 168.912956510431, 784563.0240367375, 784563.0240367375, 239117.2152773291], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5863800.0000, 
sim time next is 5864400.0000, 
raw observation next is [27.9, 87.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9662335082807503, 6.911199999999999, 6.9112, 168.912956510431, 784106.6393279284, 784106.639327929, 238790.8180260917], 
processed observation next is [1.0, 0.9130434782608695, 0.5213270142180094, 0.87, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9588213515618906, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21780739981331346, 0.21780739981331362, 0.35640420600909206], 
reward next is 0.6436, 
noisyNet noise sample is [array([1.9189596], dtype=float32), -1.6156697]. 
=============================================
[2019-03-26 03:32:19,108] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:19,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2727
[2019-03-26 03:32:19,127] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.88333333333333, 85.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.865922522700517, 6.9112, 6.9112, 168.912956510431, 717457.2643590256, 717457.2643590256, 215760.6794265448], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 5724600.0000, 
sim time next is 5725200.0000, 
raw observation next is [27.06666666666667, 84.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8685576208519183, 6.911200000000001, 6.9112, 168.912956510431, 719288.0111071462, 719288.0111071455, 216338.233822428], 
processed observation next is [0.0, 0.2608695652173913, 0.48183254344391807, 0.84, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8397044156730712, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1998022253075406, 0.1998022253075404, 0.32289288630213137], 
reward next is 0.6771, 
noisyNet noise sample is [array([0.5035197], dtype=float32), -0.4602142]. 
=============================================
[2019-03-26 03:32:21,618] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 2.6831886e-25], sum to 1.0000
[2019-03-26 03:32:21,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5618
[2019-03-26 03:32:21,644] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1009771.888934333 W.
[2019-03-26 03:32:21,650] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 88.0, 1.0, 2.0, 0.3612652354286857, 0.0, 2.0, 0.0, 1.0, 2.0, 0.6209191432628144, 6.911200000000001, 6.9112, 168.912956510431, 1009771.888934333, 1009771.888934332, 242872.7296559624], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6161400.0000, 
sim time next is 6162000.0000, 
raw observation next is [27.66666666666666, 87.66666666666667, 1.0, 2.0, 0.4428051521650672, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7613826898627603, 6.9112, 6.9112, 168.912956510431, 1237816.822793957, 1237816.822793957, 277807.4886161792], 
processed observation next is [1.0, 0.30434782608695654, 0.5102685624012636, 0.8766666666666667, 1.0, 1.0, 0.3286809062229725, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.7090032803204394, 0.0, 0.0, 0.8294399451523027, 0.34383800633165473, 0.34383800633165473, 0.41463804271071525], 
reward next is 0.5854, 
noisyNet noise sample is [array([-0.9156655], dtype=float32), -0.57190794]. 
=============================================
[2019-03-26 03:32:21,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[37.973606]
 [37.721405]
 [37.666   ]
 [37.469406]
 [37.834026]], R is [[37.6521225 ]
 [37.91310501]
 [38.17509842]
 [38.47089767]
 [38.08618927]].
[2019-03-26 03:32:27,339] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:27,350] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3553
[2019-03-26 03:32:27,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 0 has been changed to 4 for the demand 2003740.204894333 W.
[2019-03-26 03:32:27,375] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.53333333333333, 63.33333333333333, 1.0, 2.0, 0.7165476263964088, 1.0, 2.0, 0.7165476263964088, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2003740.204894333, 2003740.204894333, 381295.0029810405], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5835000.0000, 
sim time next is 5835600.0000, 
raw observation next is [32.6, 63.0, 1.0, 2.0, 0.4622330530066003, 1.0, 2.0, 0.4622330530066003, 1.0, 1.0, 0.8027459502659547, 6.911199999999999, 6.9112, 170.5573041426782, 1938810.927991579, 1938810.927991579, 390012.3665087503], 
processed observation next is [1.0, 0.5652173913043478, 0.7440758293838864, 0.63, 1.0, 1.0, 0.35208801567060277, 1.0, 1.0, 0.35208801567060277, 1.0, 0.5, 0.7594462808121398, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5385585911087719, 0.5385585911087719, 0.5821080097145527], 
reward next is 0.4179, 
noisyNet noise sample is [array([1.5447646], dtype=float32), -1.7374972]. 
=============================================
[2019-03-26 03:32:31,039] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1. 0. 0. 0. 0.], sum to 1.0000
[2019-03-26 03:32:31,052] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1689
[2019-03-26 03:32:31,058] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [26.93333333333333, 89.83333333333333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9145564441662922, 6.9112, 6.9112, 168.912956510431, 749104.3168114576, 749104.3168114576, 226602.2537390891], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 6137400.0000, 
sim time next is 6138000.0000, 
raw observation next is [26.9, 90.0, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9142649177786246, 6.911200000000001, 6.9112, 168.912956510431, 748977.5873112564, 748977.5873112559, 226538.3731737766], 
processed observation next is [1.0, 0.043478260869565216, 0.4739336492890995, 0.9, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.8954450216812495, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20804932980868235, 0.20804932980868218, 0.33811697488623377], 
reward next is 0.6619, 
noisyNet noise sample is [array([-0.52064663], dtype=float32), -0.9728038]. 
=============================================
[2019-03-26 03:32:31,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[48.063915]
 [48.656197]
 [49.021053]
 [49.193497]
 [49.914387]], R is [[47.79436111]
 [47.97820663]
 [48.16007233]
 [48.33988571]
 [48.51764297]].
[2019-03-26 03:32:34,981] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.9999988e-01 3.3306638e-28 7.5316397e-35 0.0000000e+00 6.2270033e-08], sum to 1.0000
[2019-03-26 03:32:34,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7181
[2019-03-26 03:32:35,000] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 0 has been changed to 1 for the demand 900465.2345451849 W.
[2019-03-26 03:32:35,006] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.33333333333333, 1.0, 2.0, 0.2147843827769708, 1.0, 2.0, 0.2147843827769708, 1.0, 2.0, 0.3669165395813568, 6.9112, 6.9112, 170.5573041426782, 900465.2345451849, 900465.2345451849, 273718.3769237296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5894400.0000, 
sim time next is 5895000.0000, 
raw observation next is [26.2, 93.5, 1.0, 2.0, 0.6629684366058558, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 926494.7050637115, 926494.7050637122, 213054.7873653413], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.935, 1.0, 1.0, 0.5939378754287419, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2573596402954754, 0.2573596402954756, 0.3179922199482706], 
reward next is 0.6820, 
noisyNet noise sample is [array([0.1182652], dtype=float32), -1.725521]. 
=============================================
[2019-03-26 03:32:35,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[42.649437]
 [42.680374]
 [42.912487]
 [42.351254]
 [42.54351 ]], R is [[41.84514618]
 [42.01816177]
 [42.18370438]
 [41.76186752]
 [41.34424973]].
[2019-03-26 03:32:40,223] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0000000e+00 2.5148521e-31 0.0000000e+00 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 03:32:40,236] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9157
[2019-03-26 03:32:40,249] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 0 has been changed to 2 for the demand 1010310.886187742 W.
[2019-03-26 03:32:40,255] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.8, 90.0, 1.0, 1.0, 0.2409730914812159, 1.0, 1.0, 0.2409730914812159, 1.0, 2.0, 0.4135489228107568, 6.911200000000001, 6.9112, 170.5573041426782, 1010310.886187742, 1010310.886187741, 281989.318169393], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5983200.0000, 
sim time next is 5983800.0000, 
raw observation next is [26.96666666666667, 89.5, 1.0, 2.0, 0.4082664365930261, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6982664961706447, 6.911199999999999, 6.9112, 168.9129564778803, 1141215.465388039, 1141215.46538804, 261590.8513470069], 
processed observation next is [1.0, 0.2608695652173913, 0.47709320695102697, 0.895, 1.0, 1.0, 0.28706799589521215, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 0.6320323124032252, -8.881784197001253e-17, 0.0, 0.8294399449924639, 0.3170042959411219, 0.3170042959411222, 0.39043410648807], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8388539], dtype=float32), -1.727559]. 
=============================================
[2019-03-26 03:32:40,591] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8180776e-10 1.9736740e-01 7.0937501e-17 8.0263263e-01 1.3880517e-09], sum to 1.0000
[2019-03-26 03:32:40,607] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8820
[2019-03-26 03:32:40,617] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.4, 71.66666666666667, 1.0, 2.0, 0.8193515616015532, 1.0, 2.0, 0.8193515616015532, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2291514.625240809, 2291514.625240809, 429395.9795404865], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6430800.0000, 
sim time next is 6431400.0000, 
raw observation next is [29.5, 71.0, 1.0, 2.0, 0.7997299217678309, 1.0, 2.0, 0.7997299217678309, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2236588.863846308, 2236588.863846308, 419733.8719348417], 
processed observation next is [1.0, 0.43478260869565216, 0.5971563981042655, 0.71, 1.0, 1.0, 0.7587107491178685, 1.0, 1.0, 0.7587107491178685, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6212746844017523, 0.6212746844017523, 0.6264684655743905], 
reward next is 0.3735, 
noisyNet noise sample is [array([-2.3024392], dtype=float32), 0.56938046]. 
=============================================
[2019-03-26 03:32:51,954] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 03:32:51,955] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 03:32:51,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:32:51,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 03:32:51,957] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 03:32:51,960] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:32:51,962] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 03:32:51,962] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:32:51,961] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 03:32:51,966] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:32:51,984] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 03:32:53,294] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-26 03:32:53,613] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-26 03:32:53,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-26 03:32:53,668] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-26 03:32:53,668] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/1/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-26 03:32:57,008] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:32:57,009] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.69213462833333, 100.0, 1.0, 2.0, 0.2676692619735486, 1.0, 1.0, 0.2676692619735486, 1.0, 1.0, 0.4530416893991531, 6.9112, 6.9112, 171.5212843490159, 1122293.312093782, 1122293.312093782, 290926.6129192669]
[2019-03-26 03:32:57,011] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:32:57,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1262253e-19 5.5325262e-36 7.3927060e-34 0.0000000e+00 1.0000000e+00], sampled 0.2447903627647433
[2019-03-26 03:33:14,481] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:33:14,483] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [18.51666666666667, 83.16666666666666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.4057001766543437, 6.9112, 6.9112, 168.912956510431, 366385.1858228149, 366385.1858228149, 140953.3695500868]
[2019-03-26 03:33:14,484] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 03:33:14,490] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.46830992585483977
[2019-03-26 03:33:16,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:33:16,693] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.96666666666667, 52.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5117247007389957, 6.911199999999999, 6.9112, 168.912956510431, 453629.3422191397, 453629.3422191403, 153452.0772610959]
[2019-03-26 03:33:16,694] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 03:33:16,697] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9165488272440879
[2019-03-26 03:34:13,695] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:34:13,695] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 87.33333333333333, 1.0, 2.0, 0.6417696448468935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896857.0028543914, 896857.0028543914, 208777.9244044103]
[2019-03-26 03:34:13,696] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 03:34:13,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0000000e+00 0.0000000e+00 0.0000000e+00 0.0000000e+00 1.2041889e-30], sampled 0.1676056010461593
[2019-03-26 03:34:13,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 896857.0028543914 W.
[2019-03-26 03:34:27,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:34:27,852] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.06071635333333, 85.39438695999999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8911637973776378, 6.911199999999999, 6.9112, 168.912956510431, 735550.3838121564, 735550.383812157, 221383.1646429209]
[2019-03-26 03:34:27,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:34:27,855] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.9714593029519226
[2019-03-26 03:34:42,281] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:34:42,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.84738405, 80.52568004333334, 1.0, 2.0, 0.3143829256788777, 1.0, 2.0, 0.3143829256788777, 1.0, 2.0, 0.545979173881964, 6.911199999999999, 6.9112, 171.5212843490159, 1318275.783445988, 1318275.783445989, 310760.2323669857]
[2019-03-26 03:34:42,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 03:34:42,286] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9314157e-14 2.6354419e-31 1.3691954e-32 2.4956515e-32 1.0000000e+00], sampled 0.3079329923166426
[2019-03-26 03:34:42,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:34:42,389] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.912127125, 91.719815065, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.406450220572768, 6.9112, 168.9102440917427, 1180279.934457569, 828938.02475514, 254812.1382024024]
[2019-03-26 03:34:42,393] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:34:42,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0000000e+00 2.2826102e-33 6.3096726e-36 7.8612315e-37 2.3023871e-22], sampled 0.7093880144222774
[2019-03-26 03:34:42,397] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [1, 0, 0, 0, 0] has been changed to [0, 1, 0, 0, 0] for the demand 1180279.934457569 W.
[2019-03-26 03:34:49,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.8659634], dtype=float32), 0.20604417]
[2019-03-26 03:34:49,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.54191283, 79.14052391000001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7696087150090662, 6.9112, 6.9112, 168.912956510431, 651626.6552776151, 651626.6552776151, 195813.9323721769]
[2019-03-26 03:34:49,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 03:34:49,927] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1. 0. 0. 0. 0.], sampled 0.962444727128158
[2019-03-26 03:35:02,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8131.9176 2981741562.2583 870.0000
[2019-03-26 03:35:02,844] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 7557.4929 3135288359.7165 1286.0000
[2019-03-26 03:35:03,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7327.1304 3201788754.3636 1340.0000
[2019-03-26 03:35:03,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7470.6100 3337449172.3227 1196.0000
[2019-03-26 03:35:03,327] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8024.9521 3023732824.0243 871.0000
[2019-03-26 03:35:04,346] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2500000, evaluation results [2500000.0, 7470.609993336325, 3337449172.3227024, 1196.0, 7557.492885354777, 3135288359.716471, 1286.0, 8131.917618648468, 2981741562.2582984, 870.0, 7327.13035228017, 3201788754.363589, 1340.0, 8024.952089158066, 3023732824.024259, 871.0]
