Using TensorFlow backend.
[2019-03-26 19:07:59,345] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=50, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_warm_start=False, job_mode='Train', learning_rate=0.001, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[64, 2], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-03-26 19:07:59,345] A3C_AGENT_MAIN INFO:Start compiling...
2019-03-26 19:07:59.389540: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-03-26 19:08:18,288] A3C_AGENT_MAIN INFO:Start the learning...
[2019-03-26 19:08:18,288] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-03-26 19:08:18,298] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-03-26 19:08:18,303] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-03-26 19:08:18,309] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-03-26 19:08:18,313] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-03-26 19:08:18,316] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-03-26 19:08:18,316] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:18,317] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-03-26 19:08:18,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:18,378] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-03-26 19:08:19,318] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:19,319] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-03-26 19:08:19,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-03-26 19:08:19,634] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 19:08:19,635] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:08:19,636] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:08:19,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,636] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:08:19,636] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:08:19,638] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,638] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:08:19,638] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,638] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,638] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:19,642] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-03-26 19:08:19,643] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-03-26 19:08:19,659] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-03-26 19:08:19,667] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-03-26 19:08:19,668] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-03-26 19:08:20,320] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:20,321] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-03-26 19:08:20,406] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:20,407] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-03-26 19:08:21,322] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:21,324] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-03-26 19:08:21,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:21,386] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-03-26 19:08:22,325] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:22,325] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-03-26 19:08:22,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:22,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-03-26 19:08:23,326] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:23,330] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-03-26 19:08:23,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:23,394] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-03-26 19:08:24,330] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:24,334] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-03-26 19:08:24,393] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:24,394] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-03-26 19:08:25,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:25,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.65, 66.66666666666667, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4403941368238054, 6.911199999999999, 6.9112, 168.912956510431, 395318.8824685939, 395318.8824685945, 144728.9452203237]
[2019-03-26 19:08:25,244] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:08:25,249] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.26884967 0.1828419  0.2422561  0.15992466 0.14612757], sampled 0.8909300904040194
[2019-03-26 19:08:25,333] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:25,337] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-03-26 19:08:25,401] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:25,403] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-03-26 19:08:26,337] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:26,344] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-03-26 19:08:26,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:26,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-03-26 19:08:27,345] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:27,350] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-03-26 19:08:27,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:27,412] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-03-26 19:08:28,349] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:28,355] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-03-26 19:08:28,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:28,421] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-03-26 19:08:29,353] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:29,357] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-03-26 19:08:29,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:29,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-03-26 19:08:30,356] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:30,360] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-03-26 19:08:30,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:30,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-03-26 19:08:31,361] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:31,365] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-03-26 19:08:31,425] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:31,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-03-26 19:08:32,366] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:32,370] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-03-26 19:08:32,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:32,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-03-26 19:08:33,371] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-03-26 19:08:33,374] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-03-26 19:08:33,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:08:33,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-03-26 19:08:41,964] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:41,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.45, 43.16666666666667, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 1.0, 2.0, 0.2741664473661291, 6.911200000000001, 6.9112, 168.912956510431, 490085.5394163093, 490085.5394163087, 187646.4802182293]
[2019-03-26 19:08:41,966] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:08:41,968] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.29886618 0.17230324 0.2587801  0.15207897 0.11797158], sampled 0.24740816055317305
[2019-03-26 19:08:44,309] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:44,310] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.55, 83.0, 1.0, 1.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.1945833033931314, 6.911199999999999, 6.9112, 169.0403247858759, 508392.974165481, 508392.9741654816, 231146.2187936095]
[2019-03-26 19:08:44,311] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:08:44,316] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.28150743 0.1302618  0.3158566  0.14394407 0.12843014], sampled 0.4527547426506141
[2019-03-26 19:08:44,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:44,330] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.85, 70.0, 1.0, 2.0, 0.3399570737873855, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 528993.0575815059, 528993.0575815066, 169074.5487990631]
[2019-03-26 19:08:44,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:08:44,334] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.25584605 0.1437564  0.27600485 0.18874773 0.13564487], sampled 0.043723217444844864
[2019-03-26 19:08:49,519] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:49,520] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.41666666666666, 84.0, 1.0, 2.0, 0.255795269182497, 0.0, 1.0, 0.0, 1.0, 1.0, 0.4520028510438948, 6.911199999999999, 6.9112, 168.912956510431, 781262.6364814884, 781262.6364814889, 214659.8256315466]
[2019-03-26 19:08:49,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:08:49,525] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.25260693 0.13752308 0.23034863 0.2167094  0.16281193], sampled 0.6990218295321218
[2019-03-26 19:08:50,342] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:50,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.5, 92.33333333333333, 1.0, 2.0, 0.2075515562628742, 0.0, 1.0, 0.0, 1.0, 2.0, 0.3575181605414197, 6.911200000000001, 6.9112, 168.912956510431, 609771.0122369053, 609771.0122369047, 199494.5265620593]
[2019-03-26 19:08:50,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:08:50,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.30339864 0.12284066 0.33226684 0.12553912 0.11595477], sampled 0.1658289677167113
[2019-03-26 19:08:54,814] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:08:54,815] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.16666666666667, 88.0, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.2197309504253196, 6.9112, 6.9112, 169.0403247858759, 566768.4284202516, 566768.4284202516, 239855.3656947324]
[2019-03-26 19:08:54,817] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:08:54,820] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.30480865 0.14813551 0.3445915  0.09156043 0.1109039 ], sampled 0.3373785210965109
[2019-03-26 19:09:12,909] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:09:12,910] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.0, 66.83333333333334, 1.0, 2.0, 0.1976490801654119, 1.0, 1.0, 0.1976490801654119, 1.0, 2.0, 0.3432510886976962, 6.9112, 6.9112, 178.6582176852504, 828584.6723876457, 828584.6723876457, 271203.54218425]
[2019-03-26 19:09:12,910] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:09:12,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.26886988 0.17074768 0.30696473 0.15987228 0.09354543], sampled 0.6882842544784874
[2019-03-26 19:09:31,457] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-03-26 19:09:31,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.60704414, 69.47950730666666, 1.0, 2.0, 0.1965948624173629, 1.0, 1.0, 0.1965948624173629, 1.0, 2.0, 0.3414202611044718, 6.9112, 6.9112, 171.5212843490159, 824176.1375310642, 824176.1375310642, 269106.2287540176]
[2019-03-26 19:09:31,463] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:09:31,465] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.24473026 0.14296196 0.2935599  0.19145617 0.12729177], sampled 0.06908056591290046
[2019-03-26 19:10:16,552] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3649.0532 3434508141.8486 1539.0000
[2019-03-26 19:10:16,702] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 4037.8077 3081312454.5720 783.0000
[2019-03-26 19:10:16,780] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3929.7202 3220153028.6300 1135.0000
[2019-03-26 19:10:16,785] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3821.1437 3280749079.0314 1468.0000
[2019-03-26 19:10:16,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3861.0138 3128180290.6489 941.0000
[2019-03-26 19:10:17,856] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3649.0532015586355, 3434508141.848573, 1539.0, 3929.7201560572607, 3220153028.6299767, 1135.0, 4037.8076669385746, 3081312454.571956, 783.0, 3821.1437033731295, 3280749079.031449, 1468.0, 3861.0137562310483, 3128180290.6489344, 941.0]
[2019-03-26 19:10:20,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.26559538 0.22761662 0.26042295 0.122627   0.12373812], sum to 1.0000
[2019-03-26 19:10:20,895] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1008
[2019-03-26 19:10:21,013] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.23333333333333, 85.16666666666667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5469136460556863, 6.9112, 6.9112, 168.912956510431, 483166.3040385656, 483166.3040385656, 158184.4145506776], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 15000.0000, 
sim time next is 15600.0000, 
raw observation next is [21.26666666666667, 85.33333333333334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5173659901891493, 6.911200000000001, 6.9112, 168.912956510431, 456876.1405156444, 456876.1405156438, 154252.9221733297], 
processed observation next is [1.0, 0.17391304347826086, 0.2069510268562403, 0.8533333333333334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4114219392550601, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12691003903212345, 0.12691003903212328, 0.2302282420497458], 
reward next is 0.7698, 
noisyNet noise sample is [array([-1.1968371], dtype=float32), -0.78272414]. 
=============================================
[2019-03-26 19:10:23,228] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0974589e-04 1.5540201e-03 9.9822801e-01 5.8780893e-06 2.2490049e-06], sum to 1.0000
[2019-03-26 19:10:23,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2167
[2019-03-26 19:10:23,341] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.43333333333333, 65.5, 1.0, 2.0, 0.503916484433464, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8688333782903443, 6.9112, 6.9112, 168.9129367532876, 1483130.573628132, 1483130.573628132, 314479.8417497822], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 54600.0000, 
sim time next is 55200.0000, 
raw observation next is [27.36666666666667, 66.0, 1.0, 2.0, 0.5521677024353089, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9516945268986255, 6.911200000000001, 6.9112, 168.9129565054976, 1624412.170501107, 1624412.170501106, 342310.181347033], 
processed observation next is [1.0, 0.6521739130434783, 0.49605055292259104, 0.66, 1.0, 1.0, 0.46044301498229984, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9410908864617383, 8.881784197001253e-17, 0.0, 0.8294399451280774, 0.4512256029169741, 0.4512256029169739, 0.5109107184284075], 
reward next is 0.4891, 
noisyNet noise sample is [array([0.35742226], dtype=float32), -0.8026543]. 
=============================================
[2019-03-26 19:10:25,058] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0949357e-06 3.0827494e-05 9.9996805e-01 3.1168339e-09 2.5171679e-11], sum to 1.0000
[2019-03-26 19:10:25,068] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1516
[2019-03-26 19:10:25,181] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [22.28333333333333, 89.0, 1.0, 2.0, 0.1720473210267907, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3070518891563538, 6.911199999999999, 6.9112, 168.912956510431, 533788.4978333927, 533788.4978333933, 195325.5563023438], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 85800.0000, 
sim time next is 86400.0000, 
raw observation next is [22.3, 89.0, 1.0, 2.0, 0.1727013775469943, 0.0, 2.0, 0.0, 1.0, 2.0, 0.3081326802494744, 6.911200000000001, 6.9112, 168.912956510431, 535574.7656444098, 535574.7656444091, 195431.1278274775], 
processed observation next is [1.0, 0.0, 0.25592417061611383, 0.89, 1.0, 1.0, 0.0032546717433666124, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.15625936615789557, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1487707682345583, 0.1487707682345581, 0.2916882504887724], 
reward next is 0.7083, 
noisyNet noise sample is [array([0.61123276], dtype=float32), 1.3396088]. 
=============================================
[2019-03-26 19:10:34,439] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0912272e-08 1.0000000e+00 1.6804994e-18 8.0192150e-15 2.8710363e-21], sum to 1.0000
[2019-03-26 19:10:34,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2277
[2019-03-26 19:10:34,553] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 89.0, 1.0, 2.0, 0.3006138163358041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 478712.2646796132, 478712.2646796139, 165529.8364474007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 241200.0000, 
sim time next is 241800.0000, 
raw observation next is [21.26666666666667, 89.0, 1.0, 2.0, 0.3001949018991961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478264.1305440522, 478264.1305440522, 165501.1985853083], 
processed observation next is [0.0, 0.8260869565217391, 0.2069510268562403, 0.89, 1.0, 1.0, 0.15686132758939292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13285114737334783, 0.13285114737334783, 0.2470167143064303], 
reward next is 0.7530, 
noisyNet noise sample is [array([0.29912773], dtype=float32), -0.22965519]. 
=============================================
[2019-03-26 19:10:34,619] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0002783e-08 1.0000000e+00 2.7901116e-14 4.2139032e-14 1.5005865e-17], sum to 1.0000
[2019-03-26 19:10:34,628] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4363
[2019-03-26 19:10:34,731] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 89.0, 1.0, 2.0, 0.2985970192955962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476385.0334150187, 476385.0334150187, 165377.1718629935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 243000.0000, 
sim time next is 243600.0000, 
raw observation next is [21.16666666666667, 89.0, 1.0, 2.0, 0.2972226638213607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474524.5789758748, 474524.5789758748, 165249.8959981345], 
processed observation next is [0.0, 0.8260869565217391, 0.2022116903633494, 0.89, 1.0, 1.0, 0.1532803178570611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13181238304885412, 0.13181238304885412, 0.24664163581811122], 
reward next is 0.7534, 
noisyNet noise sample is [array([0.12873885], dtype=float32), -0.48392844]. 
=============================================
[2019-03-26 19:10:37,828] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7936: loss 0.6312
[2019-03-26 19:10:37,889] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7936: learning rate 0.0010
[2019-03-26 19:10:37,896] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7943: loss 0.7407
[2019-03-26 19:10:37,897] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7943: learning rate 0.0010
[2019-03-26 19:10:37,912] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7951: loss 1.1845
[2019-03-26 19:10:37,914] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7951: learning rate 0.0010
[2019-03-26 19:10:37,925] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7958: loss 0.6817
[2019-03-26 19:10:37,929] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7959: learning rate 0.0010
[2019-03-26 19:10:37,937] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7961: loss 0.7266
[2019-03-26 19:10:37,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7962: learning rate 0.0010
[2019-03-26 19:10:37,952] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 7969: loss 0.6089
[2019-03-26 19:10:37,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 7970: loss 0.6690
[2019-03-26 19:10:37,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 7970: learning rate 0.0010
[2019-03-26 19:10:37,958] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 7970: learning rate 0.0010
[2019-03-26 19:10:37,963] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7974: loss 0.5578
[2019-03-26 19:10:37,965] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7974: learning rate 0.0010
[2019-03-26 19:10:37,989] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7982: loss 0.3655
[2019-03-26 19:10:37,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7986: learning rate 0.0010
[2019-03-26 19:10:38,003] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 7989: loss 0.3724
[2019-03-26 19:10:38,006] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 7991: learning rate 0.0010
[2019-03-26 19:10:38,016] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7994: loss 0.1775
[2019-03-26 19:10:38,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7996: learning rate 0.0010
[2019-03-26 19:10:38,022] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 7996: loss 0.3412
[2019-03-26 19:10:38,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 7998: learning rate 0.0010
[2019-03-26 19:10:38,039] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8003: loss 0.0251
[2019-03-26 19:10:38,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8003: learning rate 0.0010
[2019-03-26 19:10:38,075] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8019: loss 0.0739
[2019-03-26 19:10:38,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8021: learning rate 0.0010
[2019-03-26 19:10:38,132] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8048: loss 0.0065
[2019-03-26 19:10:38,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8049: learning rate 0.0010
[2019-03-26 19:10:38,229] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8084: loss 0.2695
[2019-03-26 19:10:38,231] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8086: learning rate 0.0010
[2019-03-26 19:10:40,894] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9043472e-08 1.0000000e+00 9.9221367e-17 6.9424049e-16 1.7183968e-18], sum to 1.0000
[2019-03-26 19:10:40,906] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8391
[2019-03-26 19:10:40,912] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 86.33333333333334, 1.0, 2.0, 0.2654771746370886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 431528.6926698316, 431528.692669831, 162369.0579879229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 346800.0000, 
sim time next is 347400.0000, 
raw observation next is [20.5, 86.5, 1.0, 2.0, 0.2644502284675434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 429946.2735186359, 429946.2735186366, 162267.0934253527], 
processed observation next is [1.0, 0.0, 0.1706161137440759, 0.865, 1.0, 1.0, 0.11379545598499204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11942952042184331, 0.1194295204218435, 0.24218969167963092], 
reward next is 0.7578, 
noisyNet noise sample is [array([-0.38471615], dtype=float32), 0.36496332]. 
=============================================
[2019-03-26 19:10:41,472] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1241438e-11 1.0000000e+00 9.9374139e-22 7.0038137e-19 3.7300590e-25], sum to 1.0000
[2019-03-26 19:10:41,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4058
[2019-03-26 19:10:41,596] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.08333333333334, 89.16666666666667, 1.0, 2.0, 0.25517796528193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 415501.0883076111, 415501.0883076117, 161354.2605694449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 360600.0000, 
sim time next is 361200.0000, 
raw observation next is [20.06666666666667, 89.33333333333334, 1.0, 2.0, 0.2549050236881701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415034.4809460548, 415034.4809460548, 161326.1703894624], 
processed observation next is [1.0, 0.17391304347826086, 0.1500789889415484, 0.8933333333333334, 1.0, 1.0, 0.10229520926285551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11528735581834855, 0.11528735581834855, 0.24078532893949614], 
reward next is 0.7592, 
noisyNet noise sample is [array([2.2045372], dtype=float32), 0.76013863]. 
=============================================
[2019-03-26 19:10:41,891] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.2093226e-11 1.0000000e+00 3.0515027e-24 8.8457506e-19 3.5667089e-25], sum to 1.0000
[2019-03-26 19:10:41,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5941
[2019-03-26 19:10:42,015] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([-0.5935098], dtype=float32), 0.10359613]. 
=============================================
[2019-03-26 19:10:47,696] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0615178e-08 1.0000000e+00 3.8107849e-19 3.2686408e-18 3.8786722e-20], sum to 1.0000
[2019-03-26 19:10:47,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5739
[2019-03-26 19:10:47,808] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 77.5, 1.0, 2.0, 0.3972677704483104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653737.6474729812, 653737.6474729812, 179071.1754221473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 462600.0000, 
sim time next is 463200.0000, 
raw observation next is [20.93333333333333, 76.66666666666667, 1.0, 2.0, 0.3979314826951001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654638.7772868029, 654638.7772868029, 179169.1155806829], 
processed observation next is [1.0, 0.34782608695652173, 0.19115323854660338, 0.7666666666666667, 1.0, 1.0, 0.274616244210964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1818441048018897, 0.1818441048018897, 0.26741659041892973], 
reward next is 0.7326, 
noisyNet noise sample is [array([0.17169447], dtype=float32), -1.6032827]. 
=============================================
[2019-03-26 19:10:51,149] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.6938552e-11 1.0000000e+00 3.3526222e-24 5.3784663e-23 2.7230717e-26], sum to 1.0000
[2019-03-26 19:10:51,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5954
[2019-03-26 19:10:51,274] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.81666666666667, 87.0, 1.0, 2.0, 0.2311405328509986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383424.2429438421, 383424.2429438421, 158904.2438024592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 517800.0000, 
sim time next is 518400.0000, 
raw observation next is [18.8, 87.0, 1.0, 2.0, 0.2299358285746499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381481.9020643638, 381481.9020643638, 158786.4280343929], 
processed observation next is [1.0, 0.0, 0.09004739336492901, 0.87, 1.0, 1.0, 0.07221184165620467, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10596719501787884, 0.10596719501787884, 0.2369946687080491], 
reward next is 0.7630, 
noisyNet noise sample is [array([-0.0671357], dtype=float32), -1.3823328]. 
=============================================
[2019-03-26 19:10:55,582] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6476119e-07 9.9999988e-01 5.8117371e-16 5.7942460e-17 8.8721821e-18], sum to 1.0000
[2019-03-26 19:10:55,593] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7906
[2019-03-26 19:10:55,699] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 76.33333333333334, 1.0, 2.0, 0.2369782164181012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392469.1225712495, 392469.1225712489, 159516.5214232711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 593400.0000, 
sim time next is 594000.0000, 
raw observation next is [20.2, 77.0, 1.0, 2.0, 0.235071197854184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 389467.2819713699, 389467.2819713705, 159322.4572576308], 
processed observation next is [1.0, 0.9130434782608695, 0.15639810426540288, 0.77, 1.0, 1.0, 0.07839903355925783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10818535610315831, 0.10818535610315848, 0.23779471232482208], 
reward next is 0.7622, 
noisyNet noise sample is [array([0.12249722], dtype=float32), 1.0260224]. 
=============================================
[2019-03-26 19:10:55,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[56.488174]
 [56.58768 ]
 [56.644703]
 [56.76649 ]
 [56.831852]], R is [[56.6337204 ]
 [56.82929993]
 [57.02272034]
 [57.21414948]
 [57.40355301]].
[2019-03-26 19:10:55,918] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 15903: loss 0.6515
[2019-03-26 19:10:55,922] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 15903: learning rate 0.0010
[2019-03-26 19:10:55,927] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15906: loss 0.3977
[2019-03-26 19:10:55,928] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15906: learning rate 0.0010
[2019-03-26 19:10:55,976] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 15927: loss 0.2445
[2019-03-26 19:10:55,978] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 15927: learning rate 0.0010
[2019-03-26 19:10:56,034] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 15955: loss 0.1550
[2019-03-26 19:10:56,036] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15956: loss 0.1387
[2019-03-26 19:10:56,038] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 15956: learning rate 0.0010
[2019-03-26 19:10:56,039] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15956: learning rate 0.0010
[2019-03-26 19:10:56,047] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 15958: loss 0.1295
[2019-03-26 19:10:56,052] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 15958: learning rate 0.0010
[2019-03-26 19:10:56,057] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15961: loss 0.1371
[2019-03-26 19:10:56,060] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15962: learning rate 0.0010
[2019-03-26 19:10:56,072] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15971: loss 0.0039
[2019-03-26 19:10:56,073] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15971: loss 0.0019
[2019-03-26 19:10:56,074] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15971: learning rate 0.0010
[2019-03-26 19:10:56,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15971: learning rate 0.0010
[2019-03-26 19:10:56,157] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16004: loss 0.0401
[2019-03-26 19:10:56,161] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16005: learning rate 0.0010
[2019-03-26 19:10:56,177] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 16011: loss 0.0829
[2019-03-26 19:10:56,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 16012: learning rate 0.0010
[2019-03-26 19:10:56,195] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 16019: loss 0.1102
[2019-03-26 19:10:56,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 16019: learning rate 0.0010
[2019-03-26 19:10:56,271] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16058: loss 0.3784
[2019-03-26 19:10:56,274] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16058: learning rate 0.0010
[2019-03-26 19:10:56,286] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16065: loss 0.1807
[2019-03-26 19:10:56,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16065: learning rate 0.0010
[2019-03-26 19:10:56,313] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16076: loss 0.1746
[2019-03-26 19:10:56,315] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16076: learning rate 0.0010
[2019-03-26 19:10:56,344] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16087: loss 0.0407
[2019-03-26 19:10:56,345] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16087: learning rate 0.0010
[2019-03-26 19:10:57,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.7865363e-10 1.0000000e+00 2.6771094e-22 3.4682980e-22 4.7333301e-25], sum to 1.0000
[2019-03-26 19:10:57,614] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2516
[2019-03-26 19:10:57,622] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 86.0, 1.0, 2.0, 0.2135148031529875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 355961.443750149, 355961.4437501484, 156949.4386614617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 626400.0000, 
sim time next is 627000.0000, 
raw observation next is [18.51666666666667, 85.0, 1.0, 2.0, 0.2146485999096978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 357625.0040675116, 357625.0040675116, 157118.3436851489], 
processed observation next is [1.0, 0.2608695652173913, 0.07661927330173794, 0.85, 1.0, 1.0, 0.05379349386710579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.0993402789076421, 0.0993402789076421, 0.2345049905748491], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.5678711], dtype=float32), 0.65468323]. 
=============================================
[2019-03-26 19:10:57,633] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[88.622246]
 [88.603966]
 [88.58368 ]
 [88.55984 ]
 [88.51759 ]], R is [[88.52315521]
 [88.40367126]
 [88.28578186]
 [88.16903687]
 [88.05269623]].
[2019-03-26 19:11:00,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.3632862e-08 9.9999988e-01 3.4921384e-16 2.1899633e-17 6.8126880e-19], sum to 1.0000
[2019-03-26 19:11:00,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6764
[2019-03-26 19:11:00,677] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.68333333333334, 75.83333333333333, 1.0, 2.0, 0.2418179191474085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399429.7051594391, 399429.7051594391, 160052.8851681269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 677400.0000, 
sim time next is 678000.0000, 
raw observation next is [20.56666666666667, 76.66666666666667, 1.0, 2.0, 0.2428439831853793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401142.3041803128, 401142.3041803128, 160150.2860547056], 
processed observation next is [1.0, 0.8695652173913043, 0.17377567140600336, 0.7666666666666667, 1.0, 1.0, 0.0877638351631076, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11142841782786467, 0.11142841782786467, 0.23903027769359042], 
reward next is 0.7610, 
noisyNet noise sample is [array([0.9004513], dtype=float32), 0.51709217]. 
=============================================
[2019-03-26 19:11:00,702] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[58.31142 ]
 [58.29862 ]
 [58.315804]
 [58.318188]
 [58.37209 ]], R is [[58.47256088]
 [58.64895248]
 [58.82363892]
 [58.99642944]
 [59.16734695]].
[2019-03-26 19:11:05,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3568269e-08 1.0000000e+00 1.2147584e-16 1.9619607e-19 7.1349465e-21], sum to 1.0000
[2019-03-26 19:11:05,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7539
[2019-03-26 19:11:05,299] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 50.0, 1.0, 2.0, 0.5632090907821332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 932419.665441059, 932419.6654410597, 207775.8769637052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 752400.0000, 
sim time next is 753000.0000, 
raw observation next is [24.38333333333334, 51.5, 1.0, 2.0, 0.2965636525602371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490864.6678054858, 490864.6678054858, 165764.7785930676], 
processed observation next is [1.0, 0.7391304347826086, 0.3546603475513432, 0.515, 1.0, 1.0, 0.15248632838582782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13635129661263493, 0.13635129661263493, 0.24741011730308599], 
reward next is 0.7526, 
noisyNet noise sample is [array([1.2872628], dtype=float32), -0.80376136]. 
=============================================
[2019-03-26 19:11:05,310] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[71.32658]
 [71.32359]
 [71.289  ]
 [71.27162]
 [71.25956]], R is [[71.23854065]
 [71.21604156]
 [71.18738556]
 [71.15844727]
 [71.13257599]].
[2019-03-26 19:11:06,868] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.2766211e-08 1.0000000e+00 7.1402193e-16 1.2745415e-18 1.0099188e-20], sum to 1.0000
[2019-03-26 19:11:06,877] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9239
[2019-03-26 19:11:06,883] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 89.33333333333334, 1.0, 2.0, 0.2540526982891675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417111.0264477726, 417111.0264477726, 161313.052710072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 778200.0000, 
sim time next is 778800.0000, 
raw observation next is [19.5, 89.66666666666667, 1.0, 2.0, 0.2547393510533554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418024.3709610438, 418024.3709610438, 161381.6543271054], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.8966666666666667, 1.0, 1.0, 0.1020956036787414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11611788082251216, 0.11611788082251216, 0.2408681407867245], 
reward next is 0.7591, 
noisyNet noise sample is [array([0.23060623], dtype=float32), 0.9744793]. 
=============================================
[2019-03-26 19:11:13,584] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23856: loss 0.2240
[2019-03-26 19:11:13,587] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23856: learning rate 0.0010
[2019-03-26 19:11:13,690] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 23906: loss 0.0297
[2019-03-26 19:11:13,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 23906: learning rate 0.0010
[2019-03-26 19:11:13,712] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 23917: loss 0.0046
[2019-03-26 19:11:13,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 23918: learning rate 0.0010
[2019-03-26 19:11:13,735] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23927: loss 0.0020
[2019-03-26 19:11:13,739] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23928: learning rate 0.0010
[2019-03-26 19:11:13,759] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23937: loss 0.0017
[2019-03-26 19:11:13,764] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23938: loss 0.0326
[2019-03-26 19:11:13,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23938: learning rate 0.0010
[2019-03-26 19:11:13,767] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23938: learning rate 0.0010
[2019-03-26 19:11:13,830] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23967: loss 0.1747
[2019-03-26 19:11:13,835] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23969: learning rate 0.0010
[2019-03-26 19:11:13,847] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23974: loss 0.0953
[2019-03-26 19:11:13,852] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23974: learning rate 0.0010
[2019-03-26 19:11:13,878] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23991: loss 0.2573
[2019-03-26 19:11:13,879] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23991: learning rate 0.0010
[2019-03-26 19:11:13,896] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 23998: loss 0.1578
[2019-03-26 19:11:13,898] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 23998: learning rate 0.0010
[2019-03-26 19:11:13,969] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 24032: loss 0.1155
[2019-03-26 19:11:13,971] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 24032: learning rate 0.0010
[2019-03-26 19:11:13,973] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24033: loss 0.3075
[2019-03-26 19:11:13,975] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24033: learning rate 0.0010
[2019-03-26 19:11:13,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24034: loss 0.2179
[2019-03-26 19:11:13,980] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24034: learning rate 0.0010
[2019-03-26 19:11:14,002] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 24045: loss 0.1075
[2019-03-26 19:11:14,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 24048: learning rate 0.0010
[2019-03-26 19:11:14,027] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 24057: loss 0.0862
[2019-03-26 19:11:14,028] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 24057: learning rate 0.0010
[2019-03-26 19:11:14,267] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24167: loss 0.2960
[2019-03-26 19:11:14,269] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24168: learning rate 0.0010
[2019-03-26 19:11:16,047] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 19:11:16,049] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:11:16,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:11:16,054] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:11:16,055] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:11:16,055] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:11:16,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:11:16,051] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:11:16,059] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:11:16,060] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:11:16,061] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:11:16,079] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-03-26 19:11:16,081] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-03-26 19:11:16,101] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-03-26 19:11:16,102] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-03-26 19:11:16,140] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-03-26 19:11:45,452] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17164642], dtype=float32), -0.16791591]
[2019-03-26 19:11:45,453] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.96666666666667, 86.66666666666667, 1.0, 2.0, 0.5002663313458996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699044.5951192325, 699044.5951192325, 183636.9525629862]
[2019-03-26 19:11:45,454] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:11:45,456] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0554883e-10 1.0000000e+00 1.2363428e-16 1.9613835e-20 8.1304802e-21], sampled 0.7457087831693776
[2019-03-26 19:12:04,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.17164642], dtype=float32), -0.16791591]
[2019-03-26 19:12:04,602] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.1, 81.66666666666666, 1.0, 2.0, 0.5068988926549669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708315.6606207802, 708315.6606207802, 184683.6909122212]
[2019-03-26 19:12:04,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:12:04,607] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6908148e-10 1.0000000e+00 3.0260920e-17 3.8777234e-21 1.4287894e-21], sampled 0.2285614068165951
[2019-03-26 19:12:28,555] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.17164642], dtype=float32), -0.16791591]
[2019-03-26 19:12:28,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.47044963166667, 63.07709232666667, 1.0, 2.0, 0.5342661920449485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746570.821700006, 746570.8217000067, 189139.474337094]
[2019-03-26 19:12:28,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:12:28,560] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.82945089e-10 1.00000000e+00 1.51937660e-16 2.48473278e-20
 1.07366355e-20], sampled 0.13724971074971115
[2019-03-26 19:12:37,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.17164642], dtype=float32), -0.16791591]
[2019-03-26 19:12:37,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.55492989833333, 60.65381927333333, 1.0, 2.0, 0.5539273405147019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774054.8516623371, 774054.8516623377, 192477.0156040193]
[2019-03-26 19:12:37,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:12:37,021] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.2521855e-10 1.0000000e+00 1.6694409e-16 2.8718153e-20 1.2214705e-20], sampled 0.8232489793649236
[2019-03-26 19:13:09,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.17164642], dtype=float32), -0.16791591]
[2019-03-26 19:13:09,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.4, 88.33333333333334, 1.0, 2.0, 0.5889801326749963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 823056.3979289265, 823056.3979289272, 198705.2032854495]
[2019-03-26 19:13:09,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:13:09,532] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4178255e-10 1.0000000e+00 4.8201561e-17 5.5013577e-21 2.4914135e-21], sampled 0.18313742490662477
[2019-03-26 19:13:09,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 19:13:09,887] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3130 3007656030.3370 1766.0000
[2019-03-26 19:13:09,943] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 19:13:10,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1746 3163992302.2432 1778.0000
[2019-03-26 19:13:10,137] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 19:13:11,154] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 25000, evaluation results [25000.0, 7884.174552150108, 3163992302.2431507, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.312951428424, 3007656030.3370423, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 19:13:11,444] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.653149e-11 1.000000e+00 5.657001e-17 1.002426e-21 3.632363e-21], sum to 1.0000
[2019-03-26 19:13:11,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5041
[2019-03-26 19:13:11,465] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.95, 93.0, 1.0, 2.0, 0.3407761778038421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527468.8128107061, 527468.8128107061, 168874.2556418068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 945000.0000, 
sim time next is 945600.0000, 
raw observation next is [21.9, 93.33333333333334, 1.0, 2.0, 0.3397946603984714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526082.1326077123, 526082.132607713, 168767.1717017065], 
processed observation next is [0.0, 0.9565217391304348, 0.23696682464454974, 0.9333333333333335, 1.0, 1.0, 0.2045718799981583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14613392572436454, 0.14613392572436473, 0.25189130104732316], 
reward next is 0.7481, 
noisyNet noise sample is [array([1.241544], dtype=float32), 0.27691117]. 
=============================================
[2019-03-26 19:13:11,931] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6360876e-11 1.0000000e+00 8.7528100e-18 2.3884391e-22 2.5897990e-22], sum to 1.0000
[2019-03-26 19:13:11,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8080
[2019-03-26 19:13:11,942] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.83333333333333, 1.0, 2.0, 0.341666743957812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527903.3872326761, 527903.3872326761, 168880.8325417688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 953400.0000, 
sim time next is 954000.0000, 
raw observation next is [21.8, 95.0, 1.0, 2.0, 0.3431374915803272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529872.0780038765, 529872.0780038759, 169030.1340209157], 
processed observation next is [1.0, 0.043478260869565216, 0.23222748815165886, 0.95, 1.0, 1.0, 0.20859938744617734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14718668833441015, 0.14718668833440998, 0.2522837821207697], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.33053133], dtype=float32), 0.116481744]. 
=============================================
[2019-03-26 19:13:11,955] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.0388 ]
 [80.9643 ]
 [80.84289]
 [80.75032]
 [80.69134]], R is [[81.33744049]
 [81.27200317]
 [81.20738983]
 [81.14344025]
 [81.08001709]].
[2019-03-26 19:13:13,691] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.11385914e-10 1.00000000e+00 3.38859542e-18 3.02991786e-23
 2.05783045e-24], sum to 1.0000
[2019-03-26 19:13:13,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6530
[2019-03-26 19:13:13,827] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.4961679268021104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769069.1211663409, 769069.1211663409, 191988.1119634396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 981000.0000, 
sim time next is 981600.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.5464192945959767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846897.5477797518, 846897.5477797525, 201097.2470592801], 
processed observation next is [1.0, 0.34782608695652173, 0.23696682464454974, 0.93, 1.0, 1.0, 0.4535172224047912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23524931882770883, 0.23524931882770903, 0.3001451448645972], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.43111423], dtype=float32), 1.326509]. 
=============================================
[2019-03-26 19:13:16,000] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.67598643e-10 1.00000000e+00 6.64665637e-18 1.22453096e-20
 5.10597040e-20], sum to 1.0000
[2019-03-26 19:13:16,009] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5245
[2019-03-26 19:13:16,015] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3544994367054833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545390.8104076107, 545390.8104076107, 170241.8525517737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1018800.0000, 
sim time next is 1019400.0000, 
raw observation next is [21.8, 96.0, 1.0, 2.0, 0.3532030349694134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 543413.2838072306, 543413.28380723, 170078.231363019], 
processed observation next is [1.0, 0.8260869565217391, 0.23222748815165886, 0.96, 1.0, 1.0, 0.22072654815591977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1509481343908974, 0.15094813439089722, 0.25384810651196865], 
reward next is 0.7462, 
noisyNet noise sample is [array([-0.28281105], dtype=float32), -0.5369991]. 
=============================================
[2019-03-26 19:13:19,475] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7651271e-10 1.0000000e+00 1.9782117e-16 1.7146683e-21 3.7624801e-20], sum to 1.0000
[2019-03-26 19:13:19,492] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5249
[2019-03-26 19:13:19,496] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 79.0, 1.0, 2.0, 0.5602844662877203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892231.1014334069, 892231.1014334069, 205940.6190739584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1076400.0000, 
sim time next is 1077000.0000, 
raw observation next is [22.76666666666667, 78.50000000000001, 1.0, 2.0, 0.5762714572907568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916527.6782513242, 916527.6782513235, 209064.3108221342], 
processed observation next is [1.0, 0.4782608695652174, 0.2780410742496052, 0.7850000000000001, 1.0, 1.0, 0.4894836834828395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25459102173647896, 0.25459102173647874, 0.3120362848091555], 
reward next is 0.6880, 
noisyNet noise sample is [array([1.279014], dtype=float32), -0.30569884]. 
=============================================
[2019-03-26 19:13:19,512] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.37298]
 [71.3168 ]
 [71.24012]
 [71.1598 ]
 [71.08325]], R is [[71.43598938]
 [71.41426086]
 [71.3921814 ]
 [71.37171936]
 [71.35614014]].
[2019-03-26 19:13:24,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7843249e-12 1.0000000e+00 8.1384365e-18 3.9860936e-24 2.3335704e-25], sum to 1.0000
[2019-03-26 19:13:24,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9587
[2019-03-26 19:13:24,327] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 78.0, 1.0, 2.0, 0.6821492543553236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1071185.225632032, 1071185.225632032, 231280.6963054911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1155600.0000, 
sim time next is 1156200.0000, 
raw observation next is [23.58333333333333, 77.16666666666667, 1.0, 2.0, 0.6333834938920408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 992957.2007269862, 992957.2007269862, 219962.0367019851], 
processed observation next is [1.0, 0.391304347826087, 0.31674565560821466, 0.7716666666666667, 1.0, 1.0, 0.5582933661349889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27582144464638503, 0.27582144464638503, 0.3283015473163957], 
reward next is 0.6717, 
noisyNet noise sample is [array([1.4897087], dtype=float32), -0.34216145]. 
=============================================
[2019-03-26 19:13:26,465] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31849: loss 0.0276
[2019-03-26 19:13:26,468] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31849: learning rate 0.0010
[2019-03-26 19:13:26,516] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31872: loss 0.1255
[2019-03-26 19:13:26,518] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31872: learning rate 0.0010
[2019-03-26 19:13:26,625] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 31919: loss 0.0388
[2019-03-26 19:13:26,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 31920: learning rate 0.0010
[2019-03-26 19:13:26,676] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31943: loss 0.0432
[2019-03-26 19:13:26,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31944: learning rate 0.0010
[2019-03-26 19:13:26,690] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31947: loss 0.0890
[2019-03-26 19:13:26,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31947: learning rate 0.0010
[2019-03-26 19:13:26,710] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31956: loss 0.2642
[2019-03-26 19:13:26,712] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31956: learning rate 0.0010
[2019-03-26 19:13:26,743] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 31967: loss 0.2809
[2019-03-26 19:13:26,752] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 31970: learning rate 0.0010
[2019-03-26 19:13:26,762] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 31974: loss 0.2508
[2019-03-26 19:13:26,764] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 31974: learning rate 0.0010
[2019-03-26 19:13:26,780] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31984: loss 0.3607
[2019-03-26 19:13:26,787] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31985: learning rate 0.0010
[2019-03-26 19:13:26,839] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 32014: loss 0.3913
[2019-03-26 19:13:26,841] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32015: loss 0.3804
[2019-03-26 19:13:26,842] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 32015: learning rate 0.0010
[2019-03-26 19:13:26,846] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32015: learning rate 0.0010
[2019-03-26 19:13:26,870] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32028: loss 0.3184
[2019-03-26 19:13:26,873] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32029: learning rate 0.0010
[2019-03-26 19:13:26,882] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 32032: loss 0.2415
[2019-03-26 19:13:26,885] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 32032: learning rate 0.0010
[2019-03-26 19:13:26,910] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 32043: loss 0.1330
[2019-03-26 19:13:26,914] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 32043: learning rate 0.0010
[2019-03-26 19:13:27,019] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32091: loss 0.0233
[2019-03-26 19:13:27,024] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32091: learning rate 0.0010
[2019-03-26 19:13:27,137] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1157610e-11 1.0000000e+00 1.3241519e-17 2.9568198e-22 6.3391714e-23], sum to 1.0000
[2019-03-26 19:13:27,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2374
[2019-03-26 19:13:27,150] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 83.66666666666667, 1.0, 2.0, 0.3541673993488564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 546192.685384067, 546192.6853840663, 170345.9873198644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1203600.0000, 
sim time next is 1204200.0000, 
raw observation next is [23.2, 84.5, 1.0, 2.0, 0.354696437771207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546798.935816469, 546798.935816469, 170390.5290413429], 
processed observation next is [1.0, 0.9565217391304348, 0.29857819905213273, 0.845, 1.0, 1.0, 0.22252582864000844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15188859328235252, 0.15188859328235252, 0.25431422244976554], 
reward next is 0.7457, 
noisyNet noise sample is [array([2.3891542], dtype=float32), 0.4410525]. 
=============================================
[2019-03-26 19:13:27,192] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32152: loss 0.0584
[2019-03-26 19:13:27,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32152: learning rate 0.0010
[2019-03-26 19:13:29,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3971190e-10 1.0000000e+00 4.5712459e-17 2.2003534e-19 1.9445764e-20], sum to 1.0000
[2019-03-26 19:13:29,229] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3221
[2019-03-26 19:13:29,346] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 88.0, 1.0, 2.0, 0.3865048400023153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584469.8819849034, 584469.8819849034, 173343.6761196216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1234800.0000, 
sim time next is 1235400.0000, 
raw observation next is [23.55, 87.33333333333334, 1.0, 2.0, 0.4150440338059782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626248.9733533538, 626248.9733533538, 177151.9139097347], 
processed observation next is [1.0, 0.30434782608695654, 0.3151658767772513, 0.8733333333333334, 1.0, 1.0, 0.29523377566985326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17395804815370938, 0.17395804815370938, 0.2644058416563204], 
reward next is 0.7356, 
noisyNet noise sample is [array([0.9080101], dtype=float32), -0.8419166]. 
=============================================
[2019-03-26 19:13:38,701] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0218811e-13 1.0000000e+00 5.9371422e-21 7.5008106e-22 7.3900371e-24], sum to 1.0000
[2019-03-26 19:13:38,712] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5865
[2019-03-26 19:13:38,719] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 98.0, 1.0, 2.0, 0.3052578858737106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485545.4604064184, 485545.460406419, 166012.4568929832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1392600.0000, 
sim time next is 1393200.0000, 
raw observation next is [20.3, 98.0, 1.0, 2.0, 0.3055627625221013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485848.896974797, 485848.8969747977, 166031.5411313304], 
processed observation next is [0.0, 0.13043478260869565, 0.16113744075829392, 0.98, 1.0, 1.0, 0.16332862954470034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1349580269374436, 0.1349580269374438, 0.24780827034526925], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.9757441], dtype=float32), 0.11381899]. 
=============================================
[2019-03-26 19:13:39,189] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1760063e-16 1.0000000e+00 3.2769014e-28 3.1408638e-28 3.9902998e-28], sum to 1.0000
[2019-03-26 19:13:39,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0454
[2019-03-26 19:13:39,209] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 93.66666666666667, 1.0, 2.0, 0.3272875263374947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512773.1299449203, 512773.1299449197, 167891.275647502], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1406400.0000, 
sim time next is 1407000.0000, 
raw observation next is [21.51666666666667, 93.33333333333333, 1.0, 2.0, 0.3283729307895555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513909.2603748629, 513909.2603748622, 167964.9492477559], 
processed observation next is [0.0, 0.2608695652173913, 0.21879936808846778, 0.9333333333333332, 1.0, 1.0, 0.19081075998741623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1427525723263508, 0.1427525723263506, 0.2506939541011282], 
reward next is 0.7493, 
noisyNet noise sample is [array([1.3484377], dtype=float32), 0.5051519]. 
=============================================
[2019-03-26 19:13:39,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.56093]
 [70.53892]
 [70.51898]
 [70.49063]
 [70.46105]], R is [[70.62610626]
 [70.66925812]
 [70.71199036]
 [70.75437927]
 [70.79658508]].
[2019-03-26 19:13:39,434] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1534447e-19 1.0000000e+00 4.3554455e-32 9.1629038e-32 3.0053933e-34], sum to 1.0000
[2019-03-26 19:13:39,444] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1418
[2019-03-26 19:13:39,451] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.76666666666667, 92.0, 1.0, 2.0, 0.3310319007821896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516816.3737775118, 516816.3737775118, 168158.4414166933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1408800.0000, 
sim time next is 1409400.0000, 
raw observation next is [21.85, 91.5, 1.0, 2.0, 0.3324168960917796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518687.3201661177, 518687.3201661177, 168297.0415119994], 
processed observation next is [0.0, 0.30434782608695654, 0.23459715639810438, 0.915, 1.0, 1.0, 0.19568300733949348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1440798111572549, 0.1440798111572549, 0.2511896141970141], 
reward next is 0.7488, 
noisyNet noise sample is [array([0.37298846], dtype=float32), 2.1960816]. 
=============================================
[2019-03-26 19:13:44,175] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 39789: loss 0.0119
[2019-03-26 19:13:44,178] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 39790: learning rate 0.0010
[2019-03-26 19:13:44,355] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39870: loss 0.1089
[2019-03-26 19:13:44,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39872: learning rate 0.0010
[2019-03-26 19:13:44,422] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39895: loss 0.1902
[2019-03-26 19:13:44,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39895: learning rate 0.0010
[2019-03-26 19:13:44,524] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39943: loss 0.0913
[2019-03-26 19:13:44,529] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39947: learning rate 0.0010
[2019-03-26 19:13:44,559] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 39961: loss 0.0005
[2019-03-26 19:13:44,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 39961: learning rate 0.0010
[2019-03-26 19:13:44,565] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39962: loss 0.0010
[2019-03-26 19:13:44,568] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39962: learning rate 0.0010
[2019-03-26 19:13:44,597] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39973: loss 0.0079
[2019-03-26 19:13:44,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39976: learning rate 0.0010
[2019-03-26 19:13:44,602] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 39976: loss 0.0043
[2019-03-26 19:13:44,606] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 39977: learning rate 0.0010
[2019-03-26 19:13:44,618] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 39980: loss 0.0404
[2019-03-26 19:13:44,624] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 39981: learning rate 0.0010
[2019-03-26 19:13:44,625] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 39981: loss 0.0174
[2019-03-26 19:13:44,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 39982: learning rate 0.0010
[2019-03-26 19:13:44,641] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39990: loss 0.0058
[2019-03-26 19:13:44,644] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39990: learning rate 0.0010
[2019-03-26 19:13:44,746] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40041: loss 0.0900
[2019-03-26 19:13:44,753] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40042: learning rate 0.0010
[2019-03-26 19:13:44,771] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 40049: loss 0.0856
[2019-03-26 19:13:44,774] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 40050: learning rate 0.0010
[2019-03-26 19:13:44,846] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 40080: loss 0.0217
[2019-03-26 19:13:44,847] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 40080: learning rate 0.0010
[2019-03-26 19:13:44,941] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40123: loss 0.0006
[2019-03-26 19:13:44,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40123: learning rate 0.0010
[2019-03-26 19:13:45,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40170: loss 0.0276
[2019-03-26 19:13:45,052] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40171: learning rate 0.0010
[2019-03-26 19:13:46,193] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4201052e-18 1.0000000e+00 3.8380067e-27 1.1224249e-27 1.4546425e-29], sum to 1.0000
[2019-03-26 19:13:46,200] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2670
[2019-03-26 19:13:46,208] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [27.1, 59.0, 1.0, 2.0, 0.3437258197622903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531896.1454583206, 531896.14545832, 169226.5476227], 
processed observation next is [0.0, 0.7391304347826086, 0.4834123222748816, 0.59, 1.0, 1.0, 0.20930821658107263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14774892929397795, 0.1477489292939778, 0.25257693675029846], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.58599734], dtype=float32), -0.04482774]. 
=============================================
[2019-03-26 19:13:46,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[70.619606]
 [70.59309 ]
 [70.56223 ]
 [70.53082 ]
 [70.491806]], R is [[70.69033051]
 [70.7305603 ]
 [70.77004242]
 [70.80873108]
 [70.84672546]].
[2019-03-26 19:13:47,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.9046971e-17 1.0000000e+00 1.0688948e-27 4.7768252e-26 2.7451369e-29], sum to 1.0000
[2019-03-26 19:13:47,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4186
[2019-03-26 19:13:47,417] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333334, 85.5, 1.0, 2.0, 0.3573930548972085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548950.372092193, 548950.3720921937, 170512.9621013683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1543800.0000, 
sim time next is 1544400.0000, 
raw observation next is [23.1, 86.0, 1.0, 2.0, 0.3561579235170261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547306.5807380031, 547306.5807380031, 170382.9207291479], 
processed observation next is [0.0, 0.9130434782608695, 0.2938388625592418, 0.86, 1.0, 1.0, 0.22428665483979046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15202960576055644, 0.15202960576055644, 0.25430286675992225], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.7910089], dtype=float32), 1.2174318]. 
=============================================
[2019-03-26 19:13:55,490] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6077971e-15 1.0000000e+00 3.1031551e-23 2.6111356e-23 2.7855960e-26], sum to 1.0000
[2019-03-26 19:13:55,498] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9493
[2019-03-26 19:13:55,504] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([1.1730765], dtype=float32), -0.7256914]. 
=============================================
[2019-03-26 19:13:56,112] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9750870e-13 1.0000000e+00 4.4278258e-20 3.6535314e-19 1.9909175e-21], sum to 1.0000
[2019-03-26 19:13:56,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5465
[2019-03-26 19:13:56,121] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 79.0, 1.0, 2.0, 0.5805050214866686, 1.0, 2.0, 0.5805050214866686, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1623025.122089181, 1623025.122089181, 327160.3476170787], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1695600.0000, 
sim time next is 1696200.0000, 
raw observation next is [28.25, 78.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 8.472683201245568, 6.9112, 168.9040860259215, 2562189.346915605, 1454477.596520511, 310531.117300093], 
processed observation next is [1.0, 0.6521739130434783, 0.537914691943128, 0.7866666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.15614832012455676, 0.0, 0.8293963870124356, 0.7117192630321125, 0.40402155458903083, 0.4634792795523776], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03432526], dtype=float32), -1.5329956]. 
=============================================
[2019-03-26 19:13:56,634] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.9481929e-08 9.9999988e-01 2.0075242e-13 1.4092600e-12 9.9100034e-15], sum to 1.0000
[2019-03-26 19:13:56,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2818
[2019-03-26 19:13:56,649] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357678926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [28.06666666666667, 78.66666666666667, 1.0, 2.0, 0.5064215132278067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707648.3718230554, 707648.371823056, 184609.1211600403], 
processed observation next is [1.0, 0.782608695652174, 0.529225908372828, 0.7866666666666667, 1.0, 1.0, 0.40532712437085144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19656899217307094, 0.1965689921730711, 0.2755360017314034], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.27852786], dtype=float32), -1.3522679]. 
=============================================
[2019-03-26 19:13:56,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[23.99162 ]
 [23.704084]
 [23.377922]
 [23.312016]
 [22.934172]], R is [[24.88661003]
 [25.36264801]
 [25.83553696]
 [26.3058548 ]
 [26.77353668]].
[2019-03-26 19:13:57,692] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.2392850e-16 1.0000000e+00 2.1255911e-25 4.7942723e-28 1.4920284e-27], sum to 1.0000
[2019-03-26 19:13:57,705] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0719
[2019-03-26 19:13:57,710] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 94.0, 1.0, 2.0, 0.503261366934298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703231.0780435816, 703231.0780435816, 184108.6900725193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1726800.0000, 
sim time next is 1727400.0000, 
raw observation next is [25.36666666666667, 94.0, 1.0, 2.0, 0.5013646783190098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700579.8707358699, 700579.8707358692, 183810.1755656344], 
processed observation next is [1.0, 1.0, 0.40126382306477115, 0.94, 1.0, 1.0, 0.399234552191578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19460551964885273, 0.19460551964885253, 0.27434354562034985], 
reward next is 0.7257, 
noisyNet noise sample is [array([-0.3135451], dtype=float32), 0.49923846]. 
=============================================
[2019-03-26 19:13:57,795] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3830314e-16 1.0000000e+00 2.3758783e-24 8.5740148e-28 8.5089135e-28], sum to 1.0000
[2019-03-26 19:13:57,803] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9138
[2019-03-26 19:13:57,923] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 92.0, 1.0, 2.0, 0.5097685494312868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712326.9227911418, 712326.9227911412, 185140.8630280854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1722000.0000, 
sim time next is 1722600.0000, 
raw observation next is [25.85, 92.5, 1.0, 2.0, 0.5100168059782505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712673.9413427863, 712673.941342787, 185180.5133621522], 
processed observation next is [1.0, 0.9565217391304348, 0.4241706161137442, 0.925, 1.0, 1.0, 0.40965880238343433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19796498370632953, 0.19796498370632973, 0.27638882591366004], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.98849237], dtype=float32), 0.32915464]. 
=============================================
[2019-03-26 19:13:58,904] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.9478619e-18 1.0000000e+00 3.4689773e-27 3.8997773e-31 1.4922773e-29], sum to 1.0000
[2019-03-26 19:13:58,911] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8350
[2019-03-26 19:13:59,036] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 94.0, 1.0, 2.0, 0.4775462822977513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679988.2797720652, 679988.2797720659, 181780.5287206051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1746000.0000, 
sim time next is 1746600.0000, 
raw observation next is [24.25, 93.83333333333334, 1.0, 2.0, 0.5569936859691708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792635.6746041526, 792635.674604152, 194838.0820007253], 
processed observation next is [1.0, 0.21739130434782608, 0.3483412322274882, 0.9383333333333335, 1.0, 1.0, 0.4662574529749045, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22017657627893128, 0.2201765762789311, 0.2908031074637691], 
reward next is 0.7092, 
noisyNet noise sample is [array([-2.2083375], dtype=float32), -1.2512681]. 
=============================================
[2019-03-26 19:13:59,096] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3614575e-17 1.0000000e+00 3.9115344e-30 2.5868154e-31 8.6473201e-30], sum to 1.0000
[2019-03-26 19:13:59,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8988
[2019-03-26 19:13:59,108] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 94.0, 1.0, 2.0, 0.4768157272293644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666265.7373571822, 666265.7373571828, 180042.9938791809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1734600.0000, 
sim time next is 1735200.0000, 
raw observation next is [24.6, 94.0, 1.0, 2.0, 0.4742197296898489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662885.1596225568, 662885.1596225568, 179687.1728541443], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.94, 1.0, 1.0, 0.3665297948070469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18413476656182132, 0.18413476656182132, 0.2681898102300661], 
reward next is 0.7318, 
noisyNet noise sample is [array([1.0644157], dtype=float32), -0.19987178]. 
=============================================
[2019-03-26 19:13:59,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.7370568e-15 1.0000000e+00 1.7693699e-26 1.6322085e-28 1.5634773e-27], sum to 1.0000
[2019-03-26 19:13:59,185] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-26 19:13:59,194] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.35, 93.5, 1.0, 2.0, 0.4873468549343948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691831.4654513524, 691831.465451353, 183027.3932835453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1747800.0000, 
sim time next is 1748400.0000, 
raw observation next is [24.4, 93.33333333333334, 1.0, 2.0, 0.5124576535202783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726592.295516293, 726592.295516293, 186915.6330045822], 
processed observation next is [1.0, 0.21739130434782608, 0.3554502369668246, 0.9333333333333335, 1.0, 1.0, 0.4125995825545521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20183119319897028, 0.20183119319897028, 0.278978556723257], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.030893], dtype=float32), 0.118661664]. 
=============================================
[2019-03-26 19:14:01,758] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6226588e-15 1.0000000e+00 1.7784892e-29 2.7404915e-29 2.0712146e-28], sum to 1.0000
[2019-03-26 19:14:01,767] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6338
[2019-03-26 19:14:01,772] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 93.0, 1.0, 2.0, 0.5709548202188928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 905379.7015858737, 905379.7015858737, 207764.8903681867], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1783800.0000, 
sim time next is 1784400.0000, 
raw observation next is [21.0, 93.33333333333334, 1.0, 2.0, 0.5371679581402501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851030.6025076078, 851030.6025076078, 201084.4899458381], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.9333333333333335, 1.0, 1.0, 0.44237103390391574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2363973895854466, 0.2363973895854466, 0.3001261043967733], 
reward next is 0.6999, 
noisyNet noise sample is [array([-0.21741223], dtype=float32), 0.5926274]. 
=============================================
[2019-03-26 19:14:02,223] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 47820: loss 0.2795
[2019-03-26 19:14:02,226] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 47821: learning rate 0.0010
[2019-03-26 19:14:02,298] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47853: loss 0.0662
[2019-03-26 19:14:02,300] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47854: learning rate 0.0010
[2019-03-26 19:14:02,311] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47858: loss 0.0893
[2019-03-26 19:14:02,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47858: learning rate 0.0010
[2019-03-26 19:14:02,345] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47874: loss 0.0251
[2019-03-26 19:14:02,347] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47876: learning rate 0.0010
[2019-03-26 19:14:02,356] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47881: loss 0.0670
[2019-03-26 19:14:02,360] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47882: learning rate 0.0010
[2019-03-26 19:14:02,387] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 47895: loss 0.0106
[2019-03-26 19:14:02,390] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 47895: learning rate 0.0010
[2019-03-26 19:14:02,560] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47977: loss 0.1836
[2019-03-26 19:14:02,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47977: learning rate 0.0010
[2019-03-26 19:14:02,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 47980: loss 0.2065
[2019-03-26 19:14:02,572] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 47981: learning rate 0.0010
[2019-03-26 19:14:02,596] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 47992: loss 0.1492
[2019-03-26 19:14:02,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 47993: learning rate 0.0010
[2019-03-26 19:14:02,607] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47996: loss 0.1119
[2019-03-26 19:14:02,609] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47997: learning rate 0.0010
[2019-03-26 19:14:02,693] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48037: loss 0.0173
[2019-03-26 19:14:02,696] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48037: learning rate 0.0010
[2019-03-26 19:14:02,706] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 48045: loss 0.0281
[2019-03-26 19:14:02,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 48046: learning rate 0.0010
[2019-03-26 19:14:02,854] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 48108: loss 0.0817
[2019-03-26 19:14:02,860] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 48110: learning rate 0.0010
[2019-03-26 19:14:02,911] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48135: loss 0.0964
[2019-03-26 19:14:02,913] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48135: learning rate 0.0010
[2019-03-26 19:14:02,955] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48153: loss 0.0364
[2019-03-26 19:14:02,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48155: learning rate 0.0010
[2019-03-26 19:14:02,979] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48165: loss 0.0097
[2019-03-26 19:14:02,982] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48165: learning rate 0.0010
[2019-03-26 19:14:06,894] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 19:14:06,898] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:14:06,899] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:14:06,901] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:14:06,901] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:14:06,903] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:14:06,905] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:14:06,904] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:14:06,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:14:06,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:14:06,910] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:14:06,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-03-26 19:14:06,919] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-03-26 19:14:06,957] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-03-26 19:14:06,958] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-03-26 19:14:06,975] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-03-26 19:14:43,877] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21669233], dtype=float32), -0.3860929]
[2019-03-26 19:14:43,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.6, 94.0, 1.0, 2.0, 0.4914845726403022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687460.6183830122, 687460.6183830122, 182360.6946388401]
[2019-03-26 19:14:43,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:14:43,883] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1965684e-11 1.0000000e+00 2.9782665e-18 5.3911487e-20 2.3050535e-19], sampled 0.08736347958407487
[2019-03-26 19:14:53,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.21669233], dtype=float32), -0.3860929]
[2019-03-26 19:14:53,748] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.7, 71.0, 1.0, 2.0, 0.5392050694011464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753474.7387567776, 753474.7387567776, 189966.0617758326]
[2019-03-26 19:14:53,749] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:14:53,751] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.5216556e-11 1.0000000e+00 1.0048669e-17 2.0621576e-19 8.3947241e-19], sampled 0.38267653036149063
[2019-03-26 19:15:41,357] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.21669233], dtype=float32), -0.3860929]
[2019-03-26 19:15:41,359] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.83333333333334, 78.66666666666667, 1.0, 2.0, 0.6268411952826682, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.963127142298999, 6.9112, 168.9125643078965, 1752695.622048812, 1715856.805644611, 370183.3249687987]
[2019-03-26 19:15:41,359] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:15:41,363] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.6854557e-11 1.0000000e+00 6.2171457e-18 1.1274417e-19 6.4687525e-19], sampled 0.8793053236253673
[2019-03-26 19:15:41,367] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1752695.622048812 W.
[2019-03-26 19:15:51,135] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.21669233], dtype=float32), -0.3860929]
[2019-03-26 19:15:51,138] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.46666666666667, 90.66666666666667, 1.0, 2.0, 0.3311335112969416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522813.9779869437, 522813.9779869437, 168755.0481316826]
[2019-03-26 19:15:51,140] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:15:51,142] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.7257727e-11 1.0000000e+00 8.2501216e-18 1.6586472e-19 6.4529792e-19], sampled 0.6717822462039713
[2019-03-26 19:16:00,362] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 19:16:00,508] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-03-26 19:16:00,524] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-26 19:16:00,636] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1844 3164017094.3913 1778.0000
[2019-03-26 19:16:00,843] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 19:16:01,856] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 50000, evaluation results [50000.0, 7884.184409332408, 3164017094.3913355, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 19:16:04,554] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0912447e-14 1.0000000e+00 1.0594916e-22 4.5255949e-25 1.0211243e-23], sum to 1.0000
[2019-03-26 19:16:04,565] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8436
[2019-03-26 19:16:04,573] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 93.33333333333334, 1.0, 2.0, 0.4608808574322754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677225.1522928596, 677225.1522928596, 181884.2629197574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1919400.0000, 
sim time next is 1920000.0000, 
raw observation next is [23.63333333333333, 92.66666666666667, 1.0, 2.0, 0.4303234485036004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631597.265746425, 631597.2657464244, 177259.1514627667], 
processed observation next is [1.0, 0.21739130434782608, 0.3191153238546602, 0.9266666666666667, 1.0, 1.0, 0.31364270904048236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1754436849295625, 0.17544368492956233, 0.26456589770562194], 
reward next is 0.7354, 
noisyNet noise sample is [array([0.5406619], dtype=float32), -0.24436374]. 
=============================================
[2019-03-26 19:16:04,589] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[61.59017 ]
 [61.595234]
 [61.570618]
 [61.565956]
 [61.604694]], R is [[61.73215866]
 [61.84336853]
 [61.96304321]
 [62.08113861]
 [62.19718552]].
[2019-03-26 19:16:07,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6859140e-16 1.0000000e+00 4.3030556e-27 9.3383355e-28 1.1538477e-27], sum to 1.0000
[2019-03-26 19:16:07,994] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1897
[2019-03-26 19:16:07,999] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 98.0, 1.0, 2.0, 0.4427695656611675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 636530.7489122464, 636530.7489122457, 177409.2802272631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1983600.0000, 
sim time next is 1984200.0000, 
raw observation next is [23.56666666666667, 97.83333333333334, 1.0, 2.0, 0.4458249436070488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 639476.8617077165, 639476.8617077158, 177668.092980041], 
processed observation next is [1.0, 1.0, 0.31595576619273325, 0.9783333333333334, 1.0, 1.0, 0.33231920916511903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1776324615854768, 0.1776324615854766, 0.2651762581791657], 
reward next is 0.7348, 
noisyNet noise sample is [array([-1.2691076], dtype=float32), 0.717039]. 
=============================================
[2019-03-26 19:16:14,751] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55822: loss 0.1524
[2019-03-26 19:16:14,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55822: learning rate 0.0010
[2019-03-26 19:16:14,800] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 55842: loss 0.0892
[2019-03-26 19:16:14,801] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55842: loss 0.0353
[2019-03-26 19:16:14,802] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 55842: learning rate 0.0010
[2019-03-26 19:16:14,803] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55842: learning rate 0.0010
[2019-03-26 19:16:14,843] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55858: loss 0.0033
[2019-03-26 19:16:14,845] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55859: learning rate 0.0010
[2019-03-26 19:16:14,891] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 55882: loss 0.0053
[2019-03-26 19:16:14,895] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 55883: learning rate 0.0010
[2019-03-26 19:16:14,941] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55904: loss 0.0045
[2019-03-26 19:16:14,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55905: learning rate 0.0010
[2019-03-26 19:16:15,078] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 55965: loss 0.2581
[2019-03-26 19:16:15,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 55966: learning rate 0.0010
[2019-03-26 19:16:15,151] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 55997: loss 0.1294
[2019-03-26 19:16:15,152] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 55997: learning rate 0.0010
[2019-03-26 19:16:15,178] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 56007: loss 0.0446
[2019-03-26 19:16:15,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 56009: learning rate 0.0010
[2019-03-26 19:16:15,247] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56041: loss 0.0007
[2019-03-26 19:16:15,248] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56041: learning rate 0.0010
[2019-03-26 19:16:15,262] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 56047: loss 0.0249
[2019-03-26 19:16:15,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 56047: learning rate 0.0010
[2019-03-26 19:16:15,286] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56057: loss 0.0005
[2019-03-26 19:16:15,288] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56057: learning rate 0.0010
[2019-03-26 19:16:15,335] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56077: loss 0.0120
[2019-03-26 19:16:15,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56077: learning rate 0.0010
[2019-03-26 19:16:15,361] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56087: loss 0.0724
[2019-03-26 19:16:15,364] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56088: learning rate 0.0010
[2019-03-26 19:16:15,493] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56150: loss 0.0891
[2019-03-26 19:16:15,495] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56150: learning rate 0.0010
[2019-03-26 19:16:15,656] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56224: loss 0.0023
[2019-03-26 19:16:15,660] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56224: learning rate 0.0010
[2019-03-26 19:16:19,363] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4759866e-14 1.0000000e+00 1.9745084e-25 1.6317362e-27 1.4275560e-24], sum to 1.0000
[2019-03-26 19:16:19,372] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9188
[2019-03-26 19:16:19,378] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 94.0, 1.0, 2.0, 0.5063296143861639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707519.9141621374, 707519.9141621374, 184593.4038149681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2166600.0000, 
sim time next is 2167200.0000, 
raw observation next is [25.3, 94.0, 1.0, 2.0, 0.5066144268144847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707918.0295500153, 707918.029550016, 184638.4407051234], 
processed observation next is [1.0, 0.08695652173913043, 0.39810426540284366, 0.94, 1.0, 1.0, 0.4055595503788971, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1966438970972265, 0.19664389709722668, 0.2755797622464528], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.99617547], dtype=float32), 0.1905427]. 
=============================================
[2019-03-26 19:16:25,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1417459e-19 1.0000000e+00 1.2367882e-28 2.4273691e-27 3.3153009e-28], sum to 1.0000
[2019-03-26 19:16:25,480] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9082
[2019-03-26 19:16:25,486] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 86.33333333333334, 1.0, 2.0, 0.5980096843371455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835679.4958897944, 835679.4958897944, 200367.9193378125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2265600.0000, 
sim time next is 2266200.0000, 
raw observation next is [26.2, 85.5, 1.0, 2.0, 0.5981101342203867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835819.9232756051, 835819.9232756051, 200386.6752005977], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.855, 1.0, 1.0, 0.5157953424342009, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2321722009098903, 0.2321722009098903, 0.2990845898516384], 
reward next is 0.7009, 
noisyNet noise sample is [array([-0.08285844], dtype=float32), 0.48254943]. 
=============================================
[2019-03-26 19:16:27,739] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2166369e-08 1.0000000e+00 1.7604239e-13 4.9643301e-10 4.3574402e-13], sum to 1.0000
[2019-03-26 19:16:27,748] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8288
[2019-03-26 19:16:27,757] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.55, 74.5, 1.0, 2.0, 0.5714326050483157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798525.8016781245, 798525.8016781245, 195545.2215697849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2316600.0000, 
sim time next is 2317200.0000, 
raw observation next is [30.43333333333333, 75.33333333333333, 1.0, 2.0, 0.5727263156361257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800334.3272621585, 800334.3272621585, 195775.5309704226], 
processed observation next is [1.0, 0.8260869565217391, 0.6413902053712479, 0.7533333333333333, 1.0, 1.0, 0.4852124284772599, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22231509090615514, 0.22231509090615514, 0.2922022850304815], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.8234263], dtype=float32), -0.4951442]. 
=============================================
[2019-03-26 19:16:28,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3353568e-10 1.0000000e+00 8.5308712e-16 8.9129338e-12 5.4221379e-16], sum to 1.0000
[2019-03-26 19:16:28,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0914
[2019-03-26 19:16:28,187] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 78.0, 1.0, 2.0, 0.5704048126491973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797089.0150994578, 797089.0150994578, 195362.0085995898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2322000.0000, 
sim time next is 2322600.0000, 
raw observation next is [29.6, 78.33333333333333, 1.0, 2.0, 0.5689007990590577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794986.5059331937, 794986.5059331937, 195095.3099613687], 
processed observation next is [1.0, 0.9130434782608695, 0.6018957345971565, 0.7833333333333333, 1.0, 1.0, 0.4806033723603104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2208295849814427, 0.2208295849814427, 0.29118702979308764], 
reward next is 0.7088, 
noisyNet noise sample is [array([1.2348287], dtype=float32), 0.86505723]. 
=============================================
[2019-03-26 19:16:30,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2903474e-10 1.0000000e+00 4.2737789e-17 9.4846664e-10 3.8272658e-17], sum to 1.0000
[2019-03-26 19:16:30,427] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9599
[2019-03-26 19:16:30,432] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 75.0, 1.0, 2.0, 0.7020375815863508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981118.8380463118, 981118.8380463118, 221284.6010492296], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2358000.0000, 
sim time next is 2358600.0000, 
raw observation next is [28.83333333333334, 74.5, 1.0, 2.0, 0.781209157663075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1091820.399190727, 1091820.399190727, 239349.6336851854], 
processed observation next is [1.0, 0.30434782608695654, 0.5655608214849924, 0.745, 1.0, 1.0, 0.7363965754976807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30328344421964637, 0.30328344421964637, 0.35723825923162], 
reward next is 0.6428, 
noisyNet noise sample is [array([0.01002074], dtype=float32), 1.1241266]. 
=============================================
[2019-03-26 19:16:32,371] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63761: loss 7.8022
[2019-03-26 19:16:32,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63761: learning rate 0.0010
[2019-03-26 19:16:32,508] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63826: loss 7.8567
[2019-03-26 19:16:32,511] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63827: learning rate 0.0010
[2019-03-26 19:16:32,701] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63912: loss 6.1329
[2019-03-26 19:16:32,702] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63912: learning rate 0.0010
[2019-03-26 19:16:32,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 63915: loss 11.3320
[2019-03-26 19:16:32,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 63915: learning rate 0.0010
[2019-03-26 19:16:32,723] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63917: loss 8.7516
[2019-03-26 19:16:32,725] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63917: learning rate 0.0010
[2019-03-26 19:16:32,821] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 63964: loss 7.5314
[2019-03-26 19:16:32,824] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 63965: learning rate 0.0010
[2019-03-26 19:16:32,847] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63978: loss 11.0987
[2019-03-26 19:16:32,849] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63978: learning rate 0.0010
[2019-03-26 19:16:32,875] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 63990: loss 6.1516
[2019-03-26 19:16:32,877] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 63991: learning rate 0.0010
[2019-03-26 19:16:32,883] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 63992: loss 6.4814
[2019-03-26 19:16:32,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 63993: learning rate 0.0010
[2019-03-26 19:16:32,901] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63998: loss 7.0977
[2019-03-26 19:16:32,904] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63998: learning rate 0.0010
[2019-03-26 19:16:32,955] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64019: loss 7.1172
[2019-03-26 19:16:32,956] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64019: learning rate 0.0010
[2019-03-26 19:16:32,975] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64027: loss 7.8567
[2019-03-26 19:16:32,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64031: learning rate 0.0010
[2019-03-26 19:16:33,027] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64050: loss 4.1594
[2019-03-26 19:16:33,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64053: learning rate 0.0010
[2019-03-26 19:16:33,060] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64066: loss 4.1010
[2019-03-26 19:16:33,067] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64067: learning rate 0.0010
[2019-03-26 19:16:33,277] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2536710e-08 9.9976128e-01 9.7458491e-12 2.3876016e-04 5.6233707e-13], sum to 1.0000
[2019-03-26 19:16:33,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64163: loss 3.3494
[2019-03-26 19:16:33,280] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64163: learning rate 0.0010
[2019-03-26 19:16:33,286] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5549
[2019-03-26 19:16:33,291] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412000.0000, 
sim time next is 2412600.0000, 
raw observation next is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
processed observation next is [1.0, 0.9565217391304348, 0.6074249605055293, 0.7833333333333333, 1.0, 1.0, 0.4869236659138897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22286662728617787, 0.22286662728617787, 0.29257959931056793], 
reward next is 0.7074, 
noisyNet noise sample is [array([0.5304458], dtype=float32), -0.37896737]. 
=============================================
[2019-03-26 19:16:33,566] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64296: loss 3.3595
[2019-03-26 19:16:33,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64297: learning rate 0.0010
[2019-03-26 19:16:36,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1195700e-11 1.0000000e+00 1.1206264e-15 3.5640397e-12 1.6298728e-16], sum to 1.0000
[2019-03-26 19:16:36,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5806
[2019-03-26 19:16:36,205] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1896748.309031399 W.
[2019-03-26 19:16:36,211] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.5, 87.83333333333334, 1.0, 2.0, 0.4522137481361139, 1.0, 2.0, 0.4522137481361139, 1.0, 2.0, 0.7726980317292991, 6.9112, 6.9112, 170.5573041426782, 1896748.309031399, 1896748.309031399, 381617.2010286258], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2454600.0000, 
sim time next is 2455200.0000, 
raw observation next is [26.4, 88.0, 1.0, 2.0, 0.6685825210345306, 1.0, 2.0, 0.6685825210345306, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1869494.469615545, 1869494.469615544, 360958.2940204999], 
processed observation next is [1.0, 0.43478260869565216, 0.45023696682464454, 0.88, 1.0, 1.0, 0.6007018325717236, 1.0, 1.0, 0.6007018325717236, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5193040193376514, 0.5193040193376511, 0.5387437224186566], 
reward next is 0.4613, 
noisyNet noise sample is [array([0.15548071], dtype=float32), 1.769529]. 
=============================================
[2019-03-26 19:16:37,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7140892e-06 9.9999785e-01 7.6464324e-10 4.9882362e-07 2.2267522e-12], sum to 1.0000
[2019-03-26 19:16:37,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1274
[2019-03-26 19:16:37,314] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1941034.85627031 W.
[2019-03-26 19:16:37,319] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.2, 85.66666666666667, 1.0, 2.0, 0.747096029483112, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981409883323709, 6.9112, 168.9124799914019, 1941034.85627031, 1891225.689119429, 396075.1359294918], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [27.3, 85.33333333333334, 1.0, 2.0, 0.7167683843069328, 1.0, 1.0, 0.7167683843069328, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2004358.105619629, 2004358.105619629, 381382.1577506366], 
processed observation next is [1.0, 0.6521739130434783, 0.4928909952606636, 0.8533333333333334, 1.0, 1.0, 0.6587570895264251, 1.0, 0.5, 0.6587570895264251, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5567661404498969, 0.5567661404498969, 0.5692271011203531], 
reward next is 0.4308, 
noisyNet noise sample is [array([-0.44951612], dtype=float32), -0.17780456]. 
=============================================
[2019-03-26 19:16:38,122] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.6283567e-12 1.0000000e+00 2.8275846e-19 2.3308521e-14 5.0367167e-19], sum to 1.0000
[2019-03-26 19:16:38,138] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0867
[2019-03-26 19:16:38,142] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 91.66666666666667, 1.0, 2.0, 0.5531454007898564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772961.776055016, 772961.776055016, 192341.8934636629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2490000.0000, 
sim time next is 2490600.0000, 
raw observation next is [27.08333333333333, 92.33333333333333, 1.0, 2.0, 0.55378696632678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773858.6221313174, 773858.6221313174, 192452.5599444059], 
processed observation next is [1.0, 0.8260869565217391, 0.4826224328593995, 0.9233333333333333, 1.0, 1.0, 0.4623939353334699, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2149607283698104, 0.2149607283698104, 0.28724262678269535], 
reward next is 0.7128, 
noisyNet noise sample is [array([0.96225244], dtype=float32), 0.5753159]. 
=============================================
[2019-03-26 19:16:39,232] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.22822168e-13 1.00000000e+00 4.61932228e-20 3.91126076e-10
 1.33913004e-20], sum to 1.0000
[2019-03-26 19:16:39,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4886
[2019-03-26 19:16:39,247] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.691170751088145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965925.2059823722, 965925.2059823722, 218951.8724616951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2517600.0000, 
sim time next is 2518200.0000, 
raw observation next is [26.3, 96.0, 1.0, 2.0, 0.6858248433928761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 958450.8197995113, 958450.8197995106, 217816.1786139738], 
processed observation next is [1.0, 0.13043478260869565, 0.4454976303317536, 0.96, 1.0, 1.0, 0.621475714931176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2662363388331976, 0.2662363388331974, 0.3250987740507072], 
reward next is 0.6749, 
noisyNet noise sample is [array([-0.79948133], dtype=float32), 0.8040496]. 
=============================================
[2019-03-26 19:16:44,243] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3477815e-13 9.9999988e-01 1.0655723e-20 1.1875103e-07 8.0160047e-22], sum to 1.0000
[2019-03-26 19:16:44,254] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1659
[2019-03-26 19:16:44,258] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 92.0, 1.0, 2.0, 0.4399230405821846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 634846.1989082854, 634846.1989082847, 177304.0313787904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2602200.0000, 
sim time next is 2602800.0000, 
raw observation next is [24.1, 92.0, 1.0, 2.0, 0.4379643431404646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 632869.7156598038, 632869.7156598032, 177129.8350404189], 
processed observation next is [0.0, 0.13043478260869565, 0.3412322274881518, 0.92, 1.0, 1.0, 0.32284860619333083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17579714323883439, 0.17579714323883425, 0.2643728881200282], 
reward next is 0.7356, 
noisyNet noise sample is [array([-1.136041], dtype=float32), 0.42184865]. 
=============================================
[2019-03-26 19:16:49,993] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71704: loss 0.2107
[2019-03-26 19:16:49,995] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71704: learning rate 0.0010
[2019-03-26 19:16:50,114] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71756: loss 0.0554
[2019-03-26 19:16:50,117] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71756: learning rate 0.0010
[2019-03-26 19:16:50,220] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6037351e-16 1.0000000e+00 1.4841184e-23 3.7180378e-19 1.5200339e-28], sum to 1.0000
[2019-03-26 19:16:50,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9659
[2019-03-26 19:16:50,235] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 97.66666666666667, 1.0, 2.0, 0.462076859757189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650915.6321728296, 650915.632172829, 178546.2408703369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2695800.0000, 
sim time next is 2696400.0000, 
raw observation next is [24.0, 98.0, 1.0, 2.0, 0.4638759431771568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 652221.4458027497, 652221.4458027504, 178652.3929667745], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.98, 1.0, 1.0, 0.35406740141826126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18117262383409716, 0.18117262383409732, 0.26664536263697686], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.42641708], dtype=float32), -1.1637938]. 
=============================================
[2019-03-26 19:16:50,378] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71881: loss 0.2043
[2019-03-26 19:16:50,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71881: learning rate 0.0010
[2019-03-26 19:16:50,451] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71910: loss 0.2661
[2019-03-26 19:16:50,452] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71911: learning rate 0.0010
[2019-03-26 19:16:50,525] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 71947: loss 0.1462
[2019-03-26 19:16:50,527] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 71947: loss 0.1989
[2019-03-26 19:16:50,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 71947: learning rate 0.0010
[2019-03-26 19:16:50,534] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 71948: learning rate 0.0010
[2019-03-26 19:16:50,560] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71961: loss 0.1444
[2019-03-26 19:16:50,566] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71962: learning rate 0.0010
[2019-03-26 19:16:50,572] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 71966: loss 0.1803
[2019-03-26 19:16:50,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 71966: learning rate 0.0010
[2019-03-26 19:16:50,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 71972: loss 0.2213
[2019-03-26 19:16:50,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 71972: learning rate 0.0010
[2019-03-26 19:16:50,608] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 71978: loss 0.2268
[2019-03-26 19:16:50,610] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 71979: learning rate 0.0010
[2019-03-26 19:16:50,630] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 71990: loss 0.1392
[2019-03-26 19:16:50,632] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 71991: learning rate 0.0010
[2019-03-26 19:16:50,757] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72047: loss 0.0014
[2019-03-26 19:16:50,761] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72049: learning rate 0.0010
[2019-03-26 19:16:50,765] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 72051: loss 0.0002
[2019-03-26 19:16:50,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 72052: learning rate 0.0010
[2019-03-26 19:16:50,917] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 72121: loss 0.0056
[2019-03-26 19:16:50,919] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 72121: learning rate 0.0010
[2019-03-26 19:16:51,025] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8734028e-18 1.0000000e+00 1.1437553e-27 1.4317516e-14 1.0115605e-29], sum to 1.0000
[2019-03-26 19:16:51,032] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3427
[2019-03-26 19:16:51,036] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 100.0, 1.0, 2.0, 0.4767107555814358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666119.0120372076, 666119.0120372076, 180027.0951194802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2707800.0000, 
sim time next is 2708400.0000, 
raw observation next is [23.66666666666666, 100.0, 1.0, 2.0, 0.4681866242906509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658817.9564229068, 658817.9564229074, 179357.1657760982], 
processed observation next is [0.0, 0.34782608695652173, 0.3206951026856238, 1.0, 1.0, 1.0, 0.3592609931212662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1830049878952519, 0.18300498789525205, 0.2676972623523854], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.13150214], dtype=float32), 0.2637968]. 
=============================================
[2019-03-26 19:16:51,174] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72236: loss 0.0879
[2019-03-26 19:16:51,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72236: learning rate 0.0010
[2019-03-26 19:16:51,368] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72322: loss 0.0039
[2019-03-26 19:16:51,371] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72323: learning rate 0.0010
[2019-03-26 19:16:55,163] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.9154848e-15 1.0000000e+00 1.6448118e-25 1.7589211e-16 1.6145558e-25], sum to 1.0000
[2019-03-26 19:16:55,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6053
[2019-03-26 19:16:55,178] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.453426618058298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698485.0806789106, 698485.0806789106, 184441.2760997399], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2772600.0000, 
sim time next is 2773200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3890464400127625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599267.4437834781, 599267.4437834781, 174934.6697144199], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2639113735093524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16646317882874392, 0.16646317882874392, 0.26109652196182076], 
reward next is 0.7389, 
noisyNet noise sample is [array([-0.72410476], dtype=float32), -0.14603454]. 
=============================================
[2019-03-26 19:16:55,561] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5425934e-13 1.0000000e+00 2.5445481e-21 5.7775250e-14 5.9864611e-24], sum to 1.0000
[2019-03-26 19:16:55,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7057
[2019-03-26 19:16:55,575] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 98.0, 1.0, 2.0, 0.3415665411043997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529250.0020647112, 529250.0020647112, 169033.531024559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2780400.0000, 
sim time next is 2781000.0000, 
raw observation next is [21.5, 97.0, 1.0, 2.0, 0.3434163734979812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531381.400996574, 531381.400996574, 169184.0196654944], 
processed observation next is [1.0, 0.17391304347826086, 0.21800947867298584, 0.97, 1.0, 1.0, 0.20893538975660386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14760594472127056, 0.14760594472127056, 0.2525134621873051], 
reward next is 0.7475, 
noisyNet noise sample is [array([-0.16263846], dtype=float32), -0.7431323]. 
=============================================
[2019-03-26 19:16:55,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.543365]
 [71.674614]
 [71.78126 ]
 [71.95612 ]
 [72.10276 ]], R is [[71.49798584]
 [71.53071594]
 [71.55981445]
 [71.59374237]
 [71.62717438]].
[2019-03-26 19:16:57,303] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 19:16:57,304] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:16:57,305] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:16:57,305] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:16:57,306] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:16:57,309] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:16:57,308] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:16:57,310] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:16:57,309] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:16:57,312] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:16:57,313] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:16:57,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-03-26 19:16:57,342] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-03-26 19:16:57,359] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-03-26 19:16:57,378] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-03-26 19:16:57,378] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-03-26 19:17:14,142] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:14,143] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.0, 89.0, 1.0, 2.0, 0.3004546686436339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481467.423517141, 481467.423517141, 165759.9005148803]
[2019-03-26 19:17:14,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:17:14,150] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.10716574e-14 1.00000000e+00 1.16883272e-23 1.22624621e-13
 2.03113342e-25], sampled 0.5184741390942013
[2019-03-26 19:17:14,764] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:14,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.8, 72.0, 1.0, 2.0, 0.281720104992064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 455521.5915965082, 455521.5915965089, 163965.1424269354]
[2019-03-26 19:17:14,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:17:14,770] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0652967e-14 1.0000000e+00 6.7313785e-23 3.2042294e-13 1.2158151e-24], sampled 0.7271551875374119
[2019-03-26 19:17:14,960] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:14,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.0803689, 57.44756689333333, 1.0, 2.0, 0.3730984615201574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578139.9044554799, 578139.9044554799, 173130.093887092]
[2019-03-26 19:17:14,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:17:14,969] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3555578e-14 1.0000000e+00 1.8124796e-23 1.4954208e-13 2.8711618e-25], sampled 0.8674180133853343
[2019-03-26 19:17:17,088] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:17,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.2, 47.0, 1.0, 2.0, 0.3441306530754073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564856.1773124506, 564856.1773124506, 171735.3854635477]
[2019-03-26 19:17:17,092] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:17:17,095] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.8982211e-14 1.0000000e+00 3.0481666e-22 7.6905799e-13 6.5415431e-24], sampled 0.81605038239907
[2019-03-26 19:17:33,264] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:33,266] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 76.0, 1.0, 2.0, 0.7870082751547443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1123252.819943018, 1123252.819943018, 243994.2101263765]
[2019-03-26 19:17:33,267] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:17:33,269] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8945862e-14 1.0000000e+00 3.4541026e-23 2.0612871e-13 5.1361195e-25], sampled 0.5194649772420157
[2019-03-26 19:17:50,015] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:17:50,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.16666666666667, 62.16666666666667, 1.0, 2.0, 0.5583365707365252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780218.5472938264, 780218.5472938264, 193240.6921484281]
[2019-03-26 19:17:50,020] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:17:50,024] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.1565376e-15 1.0000000e+00 7.6838074e-24 9.2900981e-14 1.1720328e-25], sampled 0.7517662775766736
[2019-03-26 19:18:15,797] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:18:15,799] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.30712333666666, 85.69330979666667, 1.0, 2.0, 0.5666047055582574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791776.7307080439, 791776.7307080439, 194689.2966570453]
[2019-03-26 19:18:15,801] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:18:15,805] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4770429e-15 1.0000000e+00 4.0365744e-24 6.4132402e-14 5.7868864e-26], sampled 0.12223387306262523
[2019-03-26 19:18:22,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:18:22,745] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.6, 91.0, 1.0, 2.0, 0.5130791033812663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716954.4984407043, 716954.4984407043, 185669.5528703295]
[2019-03-26 19:18:22,746] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:18:22,749] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.9302154e-15 1.0000000e+00 3.2919430e-24 5.7974614e-14 4.8212556e-26], sampled 0.2458685856402273
[2019-03-26 19:18:30,004] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:18:30,004] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.26666666666667, 63.16666666666666, 1.0, 2.0, 0.9952599575268863, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129564254823, 1391174.469515594, 1391174.469515594, 297487.3094692345]
[2019-03-26 19:18:30,005] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:18:30,010] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6979372e-14 1.0000000e+00 6.1507251e-23 2.8655715e-13 9.6028215e-25], sampled 0.5984811966442622
[2019-03-26 19:18:42,728] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:18:42,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.25291629833333, 93.81103313333332, 1.0, 2.0, 0.3676106595438436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563004.2656610734, 563004.2656610734, 171659.6026221493]
[2019-03-26 19:18:42,732] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:18:42,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.4683957e-15 1.0000000e+00 8.2124483e-24 9.6253016e-14 1.2517747e-25], sampled 0.4018260464855895
[2019-03-26 19:18:49,235] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5373098]
[2019-03-26 19:18:49,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.4, 61.0, 1.0, 2.0, 0.3136543251102086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499704.7484001722, 499704.7484001728, 167061.3718151623]
[2019-03-26 19:18:49,239] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:18:49,242] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6720139e-14 1.0000000e+00 5.3049476e-23 2.8061025e-13 9.5815617e-25], sampled 0.1863316287147787
[2019-03-26 19:18:50,687] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 19:18:50,693] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1685 3164067279.4129 1778.0000
[2019-03-26 19:18:51,210] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4067 2927264152.9822 1338.0000
[2019-03-26 19:18:51,426] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-03-26 19:18:51,549] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 19:18:52,564] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 75000, evaluation results [75000.0, 7884.168455680242, 3164067279.4128904, 1778.0, 8254.406660643592, 2927264152.9821725, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 19:18:57,832] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3312641e-15 1.0000000e+00 4.2424095e-26 2.5907133e-18 5.8767552e-28], sum to 1.0000
[2019-03-26 19:18:57,840] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6512
[2019-03-26 19:18:57,846] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.526962387468613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787175.6370659193, 787175.6370659193, 194221.3347785668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2898000.0000, 
sim time next is 2898600.0000, 
raw observation next is [22.83333333333334, 94.00000000000001, 1.0, 2.0, 0.5686072264897417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851744.2705237466, 851744.2705237466, 202139.1769588858], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115327, 0.9400000000000002, 1.0, 1.0, 0.4802496704695683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23659563070104073, 0.23659563070104073, 0.30170026411774], 
reward next is 0.6983, 
noisyNet noise sample is [array([0.41415453], dtype=float32), 1.368165]. 
=============================================
[2019-03-26 19:18:58,177] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0701407e-13 1.0000000e+00 1.6203466e-21 2.2836188e-12 8.6946842e-23], sum to 1.0000
[2019-03-26 19:18:58,184] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4296
[2019-03-26 19:18:58,189] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 97.0, 1.0, 2.0, 0.3315803871043239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519262.2577181674, 519262.2577181674, 168389.7613645521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2917800.0000, 
sim time next is 2918400.0000, 
raw observation next is [21.0, 96.0, 1.0, 2.0, 0.327929369601104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515079.3029153354, 515079.3029153354, 168100.4300197184], 
processed observation next is [1.0, 0.782608695652174, 0.19431279620853087, 0.96, 1.0, 1.0, 0.1902763489169928, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14307758414314872, 0.14307758414314872, 0.2508961642085349], 
reward next is 0.7491, 
noisyNet noise sample is [array([-0.3935744], dtype=float32), -0.04074355]. 
=============================================
[2019-03-26 19:19:00,451] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9233133e-14 1.0000000e+00 7.5099149e-22 2.0417926e-13 8.4970230e-25], sum to 1.0000
[2019-03-26 19:19:00,461] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3647
[2019-03-26 19:19:00,471] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3573354535617388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 568563.9135277172, 568563.9135277172, 172496.4618637447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2945400.0000, 
sim time next is 2946000.0000, 
raw observation next is [20.33333333333334, 98.0, 1.0, 2.0, 0.3173203459314302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504281.4369892242, 504281.4369892236, 167387.4508776125], 
processed observation next is [1.0, 0.08695652173913043, 0.16271721958925783, 0.98, 1.0, 1.0, 0.17749439268847014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14007817694145117, 0.140078176941451, 0.2498320162352425], 
reward next is 0.7502, 
noisyNet noise sample is [array([0.7025302], dtype=float32), -1.1724048]. 
=============================================
[2019-03-26 19:19:00,484] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9671691e-15 1.0000000e+00 1.7055289e-21 2.4398250e-14 2.8625100e-26], sum to 1.0000
[2019-03-26 19:19:00,488] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[64.157196]
 [64.197556]
 [64.28602 ]
 [64.30006 ]
 [64.34059 ]], R is [[64.27997589]
 [64.3797226 ]
 [64.48848724]
 [64.59615326]
 [64.70275116]].
[2019-03-26 19:19:00,491] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3535
[2019-03-26 19:19:00,497] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.3110690049642909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491912.914583405, 491912.914583405, 166424.9596614277], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2949000.0000, 
sim time next is 2949600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3081062899933056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487217.4252841523, 487217.425284153, 166080.1343062757], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16639312047386215, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1353381736900423, 0.1353381736900425, 0.24788079747205327], 
reward next is 0.7521, 
noisyNet noise sample is [array([1.8283119], dtype=float32), -0.26155585]. 
=============================================
[2019-03-26 19:19:02,983] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79711: loss 0.0385
[2019-03-26 19:19:02,985] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79712: learning rate 0.0010
[2019-03-26 19:19:03,104] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79726: loss 0.0677
[2019-03-26 19:19:03,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79728: learning rate 0.0010
[2019-03-26 19:19:03,479] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 79870: loss 0.2883
[2019-03-26 19:19:03,483] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 79871: learning rate 0.0010
[2019-03-26 19:19:03,585] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79882: loss 0.2457
[2019-03-26 19:19:03,589] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79883: learning rate 0.0010
[2019-03-26 19:19:03,720] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 79910: loss 0.1347
[2019-03-26 19:19:03,723] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 79911: learning rate 0.0010
[2019-03-26 19:19:03,820] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 79924: loss 0.1384
[2019-03-26 19:19:03,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 79924: learning rate 0.0010
[2019-03-26 19:19:03,910] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79937: loss 0.1088
[2019-03-26 19:19:03,910] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79937: loss 0.0819
[2019-03-26 19:19:03,912] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79937: learning rate 0.0010
[2019-03-26 19:19:03,914] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79937: learning rate 0.0010
[2019-03-26 19:19:04,141] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 79974: loss 0.0212
[2019-03-26 19:19:04,142] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 79974: learning rate 0.0010
[2019-03-26 19:19:04,276] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80008: loss 0.0195
[2019-03-26 19:19:04,278] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80009: learning rate 0.0010
[2019-03-26 19:19:04,278] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80010: loss 0.0190
[2019-03-26 19:19:04,362] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80013: learning rate 0.0010
[2019-03-26 19:19:04,582] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 80085: loss 0.0357
[2019-03-26 19:19:04,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 80087: learning rate 0.0010
[2019-03-26 19:19:04,701] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 80108: loss 0.0281
[2019-03-26 19:19:04,705] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 80110: learning rate 0.0010
[2019-03-26 19:19:04,709] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80112: loss 0.0193
[2019-03-26 19:19:04,793] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80112: learning rate 0.0010
[2019-03-26 19:19:05,202] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80267: loss 0.1412
[2019-03-26 19:19:05,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80269: learning rate 0.0010
[2019-03-26 19:19:05,330] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80292: loss 0.1574
[2019-03-26 19:19:05,333] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80292: learning rate 0.0010
[2019-03-26 19:19:09,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2348570e-14 1.0000000e+00 3.8488074e-22 1.9475071e-14 1.5745842e-22], sum to 1.0000
[2019-03-26 19:19:09,070] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5498
[2019-03-26 19:19:09,073] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8235318903294749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1195045.921058673, 1195045.921058672, 255959.9240597442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3076200.0000, 
sim time next is 3076800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.8334669727279125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1209470.981654011, 1209470.981654011, 258589.5951404254], 
processed observation next is [1.0, 0.6086956521739131, 0.28909952606635075, 1.0, 1.0, 1.0, 0.7993577984673644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3359641615705586, 0.3359641615705586, 0.3859546196125752], 
reward next is 0.6140, 
noisyNet noise sample is [array([-1.418774], dtype=float32), 1.9593931]. 
=============================================
[2019-03-26 19:19:10,687] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.62676479e-16 1.00000000e+00 8.57182307e-28 1.10066646e-13
 1.49007671e-28], sum to 1.0000
[2019-03-26 19:19:10,699] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4013
[2019-03-26 19:19:10,707] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3837926002778032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578065.3288533741, 578065.3288533748, 172702.1588169946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3103200.0000, 
sim time next is 3103800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3841203323757227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578558.9167785364, 578558.9167785364, 172746.2695860706], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2579763040671358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1607108102162601, 0.1607108102162601, 0.2578302531135382], 
reward next is 0.7422, 
noisyNet noise sample is [array([-0.37334028], dtype=float32), -2.5522285]. 
=============================================
[2019-03-26 19:19:14,868] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0251888e-09 1.0000000e+00 1.1632102e-16 8.1217491e-11 4.3366949e-17], sum to 1.0000
[2019-03-26 19:19:14,877] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6861
[2019-03-26 19:19:14,884] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.4806063155426771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.91295651043, 671564.0890506982, 671564.0890506975, 180614.9459829721], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3172800.0000, 
sim time next is 3173400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.4777586850197099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667583.7682886572, 667583.7682886572, 180186.501913135], 
processed observation next is [1.0, 0.7391304347826086, 0.4786729857819906, 0.84, 1.0, 1.0, 0.3707935964092891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1854399356357381, 0.1854399356357381, 0.26893507748229106], 
reward next is 0.7311, 
noisyNet noise sample is [array([0.27737], dtype=float32), 2.1810036]. 
=============================================
[2019-03-26 19:19:19,272] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.80767070e-17 1.00000000e+00 1.10984409e-26 1.30225395e-20
 1.70092355e-27], sum to 1.0000
[2019-03-26 19:19:19,282] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5692
[2019-03-26 19:19:19,290] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.5996724398922827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838004.0048726617, 838004.0048726617, 200685.6172654667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3258600.0000, 
sim time next is 3259200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.598822191377483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836815.3687192408, 836815.3687192402, 200527.3486329291], 
processed observation next is [0.0, 0.7391304347826086, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5166532426234736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23244871353312244, 0.23244871353312227, 0.2992945501984016], 
reward next is 0.7007, 
noisyNet noise sample is [array([-1.305434], dtype=float32), -0.18259472]. 
=============================================
[2019-03-26 19:19:20,568] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2426829e-19 1.0000000e+00 1.5039305e-28 5.9075240e-21 3.8862245e-28], sum to 1.0000
[2019-03-26 19:19:20,576] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8943
[2019-03-26 19:19:20,580] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 81.5, 1.0, 2.0, 0.5350485252847587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747664.420231969, 747664.4202319696, 189269.4674526293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3270600.0000, 
sim time next is 3271200.0000, 
raw observation next is [28.0, 80.66666666666667, 1.0, 2.0, 0.530733519724695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741632.6264427055, 741632.6264427049, 188551.2806606337], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8066666666666668, 1.0, 1.0, 0.4346186984634879, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20600906290075152, 0.20600906290075138, 0.28141982188154285], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.00397734], dtype=float32), 1.6837182]. 
=============================================
[2019-03-26 19:19:21,682] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87666: loss 0.2360
[2019-03-26 19:19:21,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87666: learning rate 0.0010
[2019-03-26 19:19:21,811] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87729: loss 0.2039
[2019-03-26 19:19:21,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87730: learning rate 0.0010
[2019-03-26 19:19:22,128] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 87872: loss 0.1418
[2019-03-26 19:19:22,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 87872: learning rate 0.0010
[2019-03-26 19:19:22,160] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 87886: loss 0.2256
[2019-03-26 19:19:22,164] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 87886: learning rate 0.0010
[2019-03-26 19:19:22,194] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87897: loss 0.2211
[2019-03-26 19:19:22,196] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87898: learning rate 0.0010
[2019-03-26 19:19:22,244] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 87919: loss 0.1229
[2019-03-26 19:19:22,247] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 87919: learning rate 0.0010
[2019-03-26 19:19:22,334] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87960: loss 0.0339
[2019-03-26 19:19:22,337] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87962: learning rate 0.0010
[2019-03-26 19:19:22,337] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87963: loss 0.0326
[2019-03-26 19:19:22,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87963: learning rate 0.0010
[2019-03-26 19:19:22,347] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 87969: loss 0.0114
[2019-03-26 19:19:22,349] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 87969: learning rate 0.0010
[2019-03-26 19:19:22,450] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88012: loss 0.0456
[2019-03-26 19:19:22,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88013: learning rate 0.0010
[2019-03-26 19:19:22,480] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88026: loss 0.0631
[2019-03-26 19:19:22,483] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88027: learning rate 0.0010
[2019-03-26 19:19:22,524] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 88048: loss 0.0531
[2019-03-26 19:19:22,527] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 88048: learning rate 0.0010
[2019-03-26 19:19:22,565] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88062: loss 0.0256
[2019-03-26 19:19:22,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88063: learning rate 0.0010
[2019-03-26 19:19:22,720] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88137: loss 0.0128
[2019-03-26 19:19:22,722] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88137: learning rate 0.0010
[2019-03-26 19:19:23,112] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88311: loss 0.0057
[2019-03-26 19:19:23,113] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88312: learning rate 0.0010
[2019-03-26 19:19:23,179] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 88336: loss 0.0527
[2019-03-26 19:19:23,182] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 88336: learning rate 0.0010
[2019-03-26 19:19:23,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6477718e-15 1.0000000e+00 1.3540954e-25 1.2479649e-18 2.5870119e-24], sum to 1.0000
[2019-03-26 19:19:23,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9625
[2019-03-26 19:19:23,919] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.4878208114317547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681648.3319470783, 681648.331947079, 181710.0035530568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3312600.0000, 
sim time next is 3313200.0000, 
raw observation next is [28.33333333333334, 72.66666666666667, 1.0, 2.0, 0.4904327901074785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685299.3143714768, 685299.3143714775, 182110.7996345521], 
processed observation next is [0.0, 0.34782608695652173, 0.5418641390205374, 0.7266666666666667, 1.0, 1.0, 0.38606360253913075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19036092065874355, 0.19036092065874374, 0.27180716363365987], 
reward next is 0.7282, 
noisyNet noise sample is [array([2.1274915], dtype=float32), 1.2347459]. 
=============================================
[2019-03-26 19:19:27,838] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.1432233e-14 1.0000000e+00 2.9021543e-21 8.1604155e-18 6.3463544e-24], sum to 1.0000
[2019-03-26 19:19:27,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5378
[2019-03-26 19:19:27,853] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7724873724178252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079624.607801491, 1079624.607801491, 237268.8230786504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3385800.0000, 
sim time next is 3386400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7506801962894162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1049131.948596385, 1049131.948596384, 232164.0864793828], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.6996146943245978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2914255412767736, 0.2914255412767733, 0.34651356190952654], 
reward next is 0.6535, 
noisyNet noise sample is [array([1.5670362], dtype=float32), -0.0075722183]. 
=============================================
[2019-03-26 19:19:39,674] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95776: loss 1.8285
[2019-03-26 19:19:39,675] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95776: learning rate 0.0010
[2019-03-26 19:19:39,817] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95840: loss 2.5466
[2019-03-26 19:19:39,821] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95842: learning rate 0.0010
[2019-03-26 19:19:39,948] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95898: loss 0.0488
[2019-03-26 19:19:39,953] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95899: learning rate 0.0010
[2019-03-26 19:19:39,955] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 95902: loss 1.4190
[2019-03-26 19:19:39,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 95903: learning rate 0.0010
[2019-03-26 19:19:39,973] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 95910: loss 0.0186
[2019-03-26 19:19:39,976] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 95910: learning rate 0.0010
[2019-03-26 19:19:40,057] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95942: loss 0.4963
[2019-03-26 19:19:40,060] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95942: learning rate 0.0010
[2019-03-26 19:19:40,075] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 95951: loss 0.0181
[2019-03-26 19:19:40,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 95953: learning rate 0.0010
[2019-03-26 19:19:40,082] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 95955: loss 0.4606
[2019-03-26 19:19:40,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 95956: learning rate 0.0010
[2019-03-26 19:19:40,099] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95963: loss 0.0124
[2019-03-26 19:19:40,102] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95963: learning rate 0.0010
[2019-03-26 19:19:40,124] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 95976: loss 0.0199
[2019-03-26 19:19:40,125] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 95976: learning rate 0.0010
[2019-03-26 19:19:40,172] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95997: loss 0.0239
[2019-03-26 19:19:40,174] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95997: learning rate 0.0010
[2019-03-26 19:19:40,179] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 95999: loss 0.0207
[2019-03-26 19:19:40,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 95999: learning rate 0.0010
[2019-03-26 19:19:40,279] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96042: loss 0.1673
[2019-03-26 19:19:40,283] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96043: learning rate 0.0010
[2019-03-26 19:19:40,394] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 96094: loss 0.0084
[2019-03-26 19:19:40,397] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 96094: learning rate 0.0010
[2019-03-26 19:19:40,677] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96222: loss 0.0163
[2019-03-26 19:19:40,679] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96222: learning rate 0.0010
[2019-03-26 19:19:40,910] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 96327: loss 0.1289
[2019-03-26 19:19:40,911] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 96327: learning rate 0.0010
[2019-03-26 19:19:41,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8831175e-16 1.0000000e+00 8.4777390e-22 8.3490532e-12 5.1847567e-27], sum to 1.0000
[2019-03-26 19:19:41,490] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0200
[2019-03-26 19:19:41,496] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5243107355649014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732654.5068667321, 732654.5068667321, 187492.39677599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5219144311735413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729304.8407424163, 729304.8407424163, 187100.5978990221], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42399329057053164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20258467798400454, 0.20258467798400454, 0.27925462372988374], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.8581563], dtype=float32), -0.43993786]. 
=============================================
[2019-03-26 19:19:43,951] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8019874e-15 1.0000000e+00 3.6268247e-21 2.7053086e-12 1.2667084e-26], sum to 1.0000
[2019-03-26 19:19:43,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4287
[2019-03-26 19:19:43,963] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6136135355518414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857493.6615310544, 857493.6615310544, 203300.2973600092], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3649200.0000, 
sim time next is 3649800.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6202235523851146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866734.5923844696, 866734.5923844696, 204564.513677921], 
processed observation next is [1.0, 0.21739130434782608, 0.470774091627172, 0.7983333333333335, 1.0, 1.0, 0.5424380149218249, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.240759608995686, 0.240759608995686, 0.3053201696685388], 
reward next is 0.6947, 
noisyNet noise sample is [array([0.75408506], dtype=float32), 1.07006]. 
=============================================
[2019-03-26 19:19:45,976] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7807927e-13 3.4448978e-02 2.4624507e-17 9.6555102e-01 2.5254668e-24], sum to 1.0000
[2019-03-26 19:19:45,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6520
[2019-03-26 19:19:45,991] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 76.5, 1.0, 2.0, 0.5413980553911583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756540.2662723038, 756540.2662723038, 190336.320302138], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3699000.0000, 
sim time next is 3699600.0000, 
raw observation next is [29.0, 75.66666666666666, 1.0, 2.0, 0.2684032224934635, 1.0, 1.0, 0.2684032224934635, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 750119.210353317, 750119.210353317, 244064.748108392], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7566666666666666, 1.0, 1.0, 0.11855809938971504, 1.0, 0.5, 0.11855809938971504, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20836644732036583, 0.20836644732036583, 0.3642757434453612], 
reward next is 0.6357, 
noisyNet noise sample is [array([0.92087805], dtype=float32), -0.40759072]. 
=============================================
[2019-03-26 19:19:49,057] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 19:19:49,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:19:49,060] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:19:49,061] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:19:49,062] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:19:49,063] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:19:49,064] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:19:49,065] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:19:49,064] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:19:49,068] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:19:49,071] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:19:49,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-03-26 19:19:49,101] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-03-26 19:19:49,102] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-03-26 19:19:49,137] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-03-26 19:19:49,157] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-03-26 19:20:10,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5392285]
[2019-03-26 19:20:10,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.31666666666667, 72.33333333333333, 1.0, 2.0, 0.2749089355963183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 447565.1987122355, 447565.1987122361, 163388.4925926762]
[2019-03-26 19:20:10,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:20:10,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1140156e-15 1.0000000e+00 7.1282265e-21 6.6051100e-16 1.2018463e-26], sampled 0.039725511472409436
[2019-03-26 19:20:44,508] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.18126081], dtype=float32), -0.5392285]
[2019-03-26 19:20:44,508] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.73333333333333, 69.66666666666667, 1.0, 2.0, 0.533771340681171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745879.0854389833, 745879.0854389827, 189055.8117113524]
[2019-03-26 19:20:44,509] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:20:44,512] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.8591562e-15 1.0000000e+00 3.4409437e-20 1.3743931e-15 6.9491793e-26], sampled 0.7240565646422127
[2019-03-26 19:21:42,613] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-03-26 19:21:42,622] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9262 3163995690.6303 1778.0000
[2019-03-26 19:21:42,868] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 19:21:42,870] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0142 3007608811.4011 1766.0000
[2019-03-26 19:21:43,003] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2912 2927339788.0998 1338.0000
[2019-03-26 19:21:44,018] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 100000, evaluation results [100000.0, 7884.926202492573, 3163995690.6303034, 1778.0, 8254.291174362113, 2927339788.099768, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.014205657292, 3007608811.4011106, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 19:21:45,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.9481512e-11 3.4315315e-06 1.2978714e-11 9.9999654e-01 2.0966427e-12], sum to 1.0000
[2019-03-26 19:21:45,285] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1234
[2019-03-26 19:21:45,292] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.8986699690531377, 1.0, 2.0, 0.8986699690531377, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2513570.82393344, 2513570.82393344, 470772.5481577385], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3769800.0000, 
sim time next is 3770400.0000, 
raw observation next is [33.66666666666667, 61.00000000000001, 1.0, 2.0, 0.9770631991962063, 1.0, 2.0, 0.9770631991962063, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2733075.720311314, 2733075.720311314, 515282.4154681229], 
processed observation next is [1.0, 0.6521739130434783, 0.7946287519747238, 0.6100000000000001, 1.0, 1.0, 0.9723653002363931, 1.0, 1.0, 0.9723653002363931, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7591877000864761, 0.7591877000864761, 0.7690782320419746], 
reward next is 0.2309, 
noisyNet noise sample is [array([-1.5944349], dtype=float32), 0.598402]. 
=============================================
[2019-03-26 19:21:52,453] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103810: loss 0.0066
[2019-03-26 19:21:52,457] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103811: learning rate 0.0010
[2019-03-26 19:21:52,655] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103899: loss 0.0259
[2019-03-26 19:21:52,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103900: learning rate 0.0010
[2019-03-26 19:21:52,662] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 103900: loss 0.0174
[2019-03-26 19:21:52,665] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 103901: learning rate 0.0010
[2019-03-26 19:21:52,686] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 103913: loss 0.0314
[2019-03-26 19:21:52,688] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 103914: learning rate 0.0010
[2019-03-26 19:21:52,700] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103918: loss 0.0036
[2019-03-26 19:21:52,702] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103918: learning rate 0.0010
[2019-03-26 19:21:52,708] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 103921: loss 0.0026
[2019-03-26 19:21:52,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 103921: learning rate 0.0010
[2019-03-26 19:21:52,722] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 103926: loss 0.0142
[2019-03-26 19:21:52,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 103927: learning rate 0.0010
[2019-03-26 19:21:52,741] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 103935: loss 0.0154
[2019-03-26 19:21:52,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 103935: learning rate 0.0010
[2019-03-26 19:21:52,756] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103940: loss 0.0584
[2019-03-26 19:21:52,757] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103940: learning rate 0.0010
[2019-03-26 19:21:52,807] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103965: loss 0.0292
[2019-03-26 19:21:52,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103965: learning rate 0.0010
[2019-03-26 19:21:52,866] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103991: loss 0.0620
[2019-03-26 19:21:52,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103993: learning rate 0.0010
[2019-03-26 19:21:53,017] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104059: loss 0.0021
[2019-03-26 19:21:53,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104059: learning rate 0.0010
[2019-03-26 19:21:53,032] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104065: loss 0.0046
[2019-03-26 19:21:53,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104066: learning rate 0.0010
[2019-03-26 19:21:53,078] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 104086: loss 0.0376
[2019-03-26 19:21:53,079] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 104086: learning rate 0.0010
[2019-03-26 19:21:53,319] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104194: loss 0.0735
[2019-03-26 19:21:53,320] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104194: learning rate 0.0010
[2019-03-26 19:21:53,537] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 104290: loss 0.0052
[2019-03-26 19:21:53,539] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 104290: learning rate 0.0010
[2019-03-26 19:21:53,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0783229e-13 1.0000000e+00 1.3838064e-20 1.1772464e-10 4.9343674e-25], sum to 1.0000
[2019-03-26 19:21:53,978] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8588
[2019-03-26 19:21:53,986] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.596136839469091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833061.2910813255, 833061.2910813255, 200028.4995750689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3918000.0000, 
sim time next is 3918600.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.598966487685373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837017.0929512661, 837017.0929512654, 200553.6879196656], 
processed observation next is [0.0, 0.34782608695652173, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5168270935968349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23250474804201837, 0.23250474804201818, 0.29933386256666505], 
reward next is 0.7007, 
noisyNet noise sample is [array([-0.16036104], dtype=float32), -0.57872736]. 
=============================================
[2019-03-26 19:21:57,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9787401e-17 1.0000000e+00 4.1495622e-25 4.5386863e-16 4.5163569e-30], sum to 1.0000
[2019-03-26 19:21:57,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9505
[2019-03-26 19:21:57,317] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5909416752724208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825798.5749622694, 825798.5749622694, 199069.8323283244], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3978600.0000, 
sim time next is 3979200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5900555595911859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 824559.8112500493, 824559.8112500499, 198907.3005377516], 
processed observation next is [1.0, 0.043478260869565216, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5060910356520313, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2290443920139026, 0.22904439201390273, 0.29687656796679346], 
reward next is 0.7031, 
noisyNet noise sample is [array([0.25366217], dtype=float32), 0.28896296]. 
=============================================
[2019-03-26 19:21:58,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1107899e-11 1.0000000e+00 3.1146018e-14 1.3469113e-10 1.9967791e-16], sum to 1.0000
[2019-03-26 19:21:58,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0483
[2019-03-26 19:21:58,335] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1706452.88074072 W.
[2019-03-26 19:21:58,338] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.4068805114629439, 1.0, 1.0, 0.4068805114629439, 1.0, 2.0, 0.7066168909698345, 6.9112, 6.9112, 170.5573041426782, 1706452.88074072, 1706452.88074072, 356730.5058650076], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.3925149674050764, 1.0, 2.0, 0.3925149674050764, 1.0, 2.0, 0.6816686916993343, 6.9112, 6.9112, 170.5573041426782, 1646157.65504171, 1646157.65504171, 348784.2075296319], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.26809032217479084, 1.0, 1.0, 0.26809032217479084, 1.0, 1.0, 0.6117910874382124, 0.0, 0.0, 0.8375144448122397, 0.4572660152893639, 0.4572660152893639, 0.5205734440740775], 
reward next is 0.4794, 
noisyNet noise sample is [array([-1.4271414], dtype=float32), 0.50300884]. 
=============================================
[2019-03-26 19:21:58,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4899733e-14 1.0000000e+00 1.4005838e-19 2.2610854e-16 5.1179890e-25], sum to 1.0000
[2019-03-26 19:21:58,385] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1191
[2019-03-26 19:21:58,388] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.9114081067797528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1273896.022769302, 1273896.022769301, 273098.4465384181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3994800.0000, 
sim time next is 3995400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.9021881975391663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1261001.491076004, 1261001.491076004, 270543.7144273416], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.8821544548664655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35027819196555665, 0.35027819196555665, 0.40379658869752477], 
reward next is 0.5962, 
noisyNet noise sample is [array([2.162536], dtype=float32), -0.10209968]. 
=============================================
[2019-03-26 19:22:04,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2445828e-15 9.9997401e-01 2.1232916e-17 2.5967778e-05 7.9932767e-26], sum to 1.0000
[2019-03-26 19:22:04,333] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9007
[2019-03-26 19:22:04,348] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2535776.863792947 W.
[2019-03-26 19:22:04,353] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.5, 79.0, 1.0, 2.0, 0.6044007897231782, 1.0, 2.0, 0.6044007897231782, 1.0, 1.0, 1.03, 6.933283323222656, 6.9112, 170.5573041426782, 2535776.863792947, 2519957.681076403, 489580.4100236094], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4095000.0000, 
sim time next is 4095600.0000, 
raw observation next is [30.66666666666666, 79.0, 1.0, 2.0, 0.659358014452563, 1.0, 2.0, 0.6502690467405441, 1.0, 2.0, 1.03, 7.005094527792086, 6.9112, 170.5573041426782, 2728428.277714526, 2661167.808071861, 508799.3940679212], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.79, 1.0, 1.0, 0.5895879692199555, 1.0, 1.0, 0.5786374057114989, 1.0, 1.0, 1.0365853658536586, 0.009389452779208617, 0.0, 0.8375144448122397, 0.7578967438095905, 0.7392132800199614, 0.7594020806983899], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8290498], dtype=float32), -1.7106256]. 
=============================================
[2019-03-26 19:22:10,420] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111867: loss -143.2179
[2019-03-26 19:22:10,422] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111867: learning rate 0.0010
[2019-03-26 19:22:10,520] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111910: loss -135.1826
[2019-03-26 19:22:10,523] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111912: learning rate 0.0010
[2019-03-26 19:22:10,548] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 111924: loss -150.1043
[2019-03-26 19:22:10,549] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 111924: learning rate 0.0010
[2019-03-26 19:22:10,553] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 111925: loss -159.2719
[2019-03-26 19:22:10,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 111925: learning rate 0.0010
[2019-03-26 19:22:10,567] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111931: loss -107.5540
[2019-03-26 19:22:10,569] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111931: learning rate 0.0010
[2019-03-26 19:22:10,601] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111946: loss -87.1565
[2019-03-26 19:22:10,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111946: learning rate 0.0010
[2019-03-26 19:22:10,639] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 111960: loss -64.3757
[2019-03-26 19:22:10,643] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 111961: learning rate 0.0010
[2019-03-26 19:22:10,659] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111970: loss -177.0599
[2019-03-26 19:22:10,661] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111970: learning rate 0.0010
[2019-03-26 19:22:10,673] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 111973: loss -177.4924
[2019-03-26 19:22:10,674] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 111973: learning rate 0.0010
[2019-03-26 19:22:10,694] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111984: loss -124.1172
[2019-03-26 19:22:10,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111984: learning rate 0.0010
[2019-03-26 19:22:10,727] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112000: loss -160.1354
[2019-03-26 19:22:10,729] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112000: learning rate 0.0010
[2019-03-26 19:22:10,776] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112021: loss -103.0322
[2019-03-26 19:22:10,779] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112022: learning rate 0.0010
[2019-03-26 19:22:10,782] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112023: loss -106.4695
[2019-03-26 19:22:10,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112023: learning rate 0.0010
[2019-03-26 19:22:10,814] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112034: loss -145.8741
[2019-03-26 19:22:10,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112034: learning rate 0.0010
[2019-03-26 19:22:10,985] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112113: loss -121.2027
[2019-03-26 19:22:10,987] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112114: learning rate 0.0010
[2019-03-26 19:22:11,184] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 112201: loss -90.3904
[2019-03-26 19:22:11,186] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 112201: learning rate 0.0010
[2019-03-26 19:22:12,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.6591917e-21 1.0000000e+00 6.9782938e-26 2.2018318e-11 4.3036104e-36], sum to 1.0000
[2019-03-26 19:22:12,449] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8991
[2019-03-26 19:22:12,456] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5988962928921273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836918.9616015643, 836918.9616015643, 200540.7644253055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4229400.0000, 
sim time next is 4230000.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5977174704536253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835270.9854980216, 835270.9854980216, 200321.6919845118], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.5153222535585847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23201971819389489, 0.23201971819389489, 0.2989875999768833], 
reward next is 0.7010, 
noisyNet noise sample is [array([-0.10147031], dtype=float32), -0.4007391]. 
=============================================
[2019-03-26 19:22:12,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.769222]
 [62.94407 ]
 [63.162235]
 [63.31164 ]
 [63.537136]], R is [[62.72632599]
 [62.79974747]
 [62.87202454]
 [62.94306183]
 [63.01285553]].
[2019-03-26 19:22:20,150] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.6369159e-07 1.4562926e-02 2.3962792e-07 9.8543614e-01 5.1351395e-11], sum to 1.0000
[2019-03-26 19:22:20,158] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2794
[2019-03-26 19:22:20,165] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3367835.047087727 W.
[2019-03-26 19:22:20,170] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.83333333333334, 71.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.550521283657646, 6.9112, 170.5573041426782, 3367835.047087727, 2909863.205676652, 550147.9579150773], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4355400.0000, 
sim time next is 4356000.0000, 
raw observation next is [34.0, 71.0, 1.0, 2.0, 1.033690623181432, 1.0, 2.0, 0.8374353511049786, 1.0, 1.0, 1.03, 7.005124058115024, 6.9112, 170.5573041426782, 3514854.215605319, 3447572.592191116, 647477.6768283382], 
processed observation next is [1.0, 0.43478260869565216, 0.8104265402843602, 0.71, 1.0, 1.0, 1.0405911122667855, 1.0, 1.0, 0.8041389772349139, 1.0, 0.5, 1.0365853658536586, 0.009392405811502424, 0.0, 0.8375144448122397, 0.9763483932236997, 0.9576590533864211, 0.9663845922811017], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.51136655], dtype=float32), 0.79201293]. 
=============================================
[2019-03-26 19:22:20,185] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[13.054336]
 [12.957072]
 [12.579801]
 [12.143346]
 [11.784018]], R is [[12.34060478]
 [12.21719933]
 [12.09502792]
 [12.1901722 ]
 [12.06827068]].
[2019-03-26 19:22:20,596] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1374699e-03 9.6835572e-01 7.9696171e-04 2.3541301e-02 1.6850277e-04], sum to 1.0000
[2019-03-26 19:22:20,604] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5711
[2019-03-26 19:22:20,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2645741.053771333 W.
[2019-03-26 19:22:20,618] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.0, 1.0, 2.0, 0.6305829626618417, 1.0, 2.0, 0.6305829626618417, 1.0, 2.0, 1.03, 6.984401831363543, 6.9112, 170.5573041426782, 2645741.053771333, 2593303.604305508, 499336.0598252359], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4365600.0000, 
sim time next is 4366200.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.9501606933856225, 1.0, 2.0, 0.9501606933856225, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2657743.035680955, 2657743.035680955, 499611.4081809989], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.93995264263328, 1.0, 1.0, 0.93995264263328, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7382619543558208, 0.7382619543558208, 0.745688668926864], 
reward next is 0.2543, 
noisyNet noise sample is [array([-0.06635771], dtype=float32), 0.70992893]. 
=============================================
[2019-03-26 19:22:28,185] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119865: loss 0.1736
[2019-03-26 19:22:28,187] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119865: learning rate 0.0010
[2019-03-26 19:22:28,198] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119871: loss 0.1751
[2019-03-26 19:22:28,203] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119873: learning rate 0.0010
[2019-03-26 19:22:28,270] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 119902: loss 0.0999
[2019-03-26 19:22:28,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 119902: learning rate 0.0010
[2019-03-26 19:22:28,299] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119917: loss 0.0150
[2019-03-26 19:22:28,305] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119917: learning rate 0.0010
[2019-03-26 19:22:28,320] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 119928: loss 0.0346
[2019-03-26 19:22:28,322] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 119929: learning rate 0.0010
[2019-03-26 19:22:28,334] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 119933: loss 0.0698
[2019-03-26 19:22:28,339] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 119934: learning rate 0.0010
[2019-03-26 19:22:28,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 119934: loss 0.0478
[2019-03-26 19:22:28,343] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 119934: learning rate 0.0010
[2019-03-26 19:22:28,475] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119996: loss 0.0567
[2019-03-26 19:22:28,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119996: learning rate 0.0010
[2019-03-26 19:22:28,485] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 120000: loss 0.0216
[2019-03-26 19:22:28,486] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 120000: learning rate 0.0010
[2019-03-26 19:22:28,501] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120006: loss 0.0346
[2019-03-26 19:22:28,510] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120011: learning rate 0.0010
[2019-03-26 19:22:28,542] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 120023: loss 0.1551
[2019-03-26 19:22:28,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 120023: learning rate 0.0010
[2019-03-26 19:22:28,584] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 120040: loss 0.1507
[2019-03-26 19:22:28,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 120043: learning rate 0.0010
[2019-03-26 19:22:28,601] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120049: loss 0.2974
[2019-03-26 19:22:28,602] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120049: learning rate 0.0010
[2019-03-26 19:22:28,678] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120085: loss 0.0256
[2019-03-26 19:22:28,680] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120086: learning rate 0.0010
[2019-03-26 19:22:28,694] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120092: loss 0.0032
[2019-03-26 19:22:28,697] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120092: learning rate 0.0010
[2019-03-26 19:22:28,795] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 120134: loss 0.0063
[2019-03-26 19:22:28,797] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 120136: learning rate 0.0010
[2019-03-26 19:22:32,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6862145e-30 9.9997711e-01 6.6939690e-30 2.2899865e-05 1.8397590e-37], sum to 1.0000
[2019-03-26 19:22:32,129] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8174
[2019-03-26 19:22:32,133] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5285019838757005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738513.2547740127, 738513.2547740127, 188182.1144252415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4560600.0000, 
sim time next is 4561200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5273121755060175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736850.0746077342, 736850.0746077342, 187985.7987345446], 
processed observation next is [0.0, 0.8260869565217391, 0.5734597156398105, 0.74, 1.0, 1.0, 0.43049659699520176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20468057627992617, 0.20468057627992617, 0.28057581900678297], 
reward next is 0.7194, 
noisyNet noise sample is [array([-1.2672473], dtype=float32), -0.43795896]. 
=============================================
[2019-03-26 19:22:39,544] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 19:22:39,549] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:22:39,550] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:22:39,550] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:22:39,551] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:22:39,553] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:22:39,554] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:22:39,554] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:22:39,558] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:22:39,559] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:22:39,559] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:22:39,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-03-26 19:22:39,578] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-03-26 19:22:39,595] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-03-26 19:22:39,636] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-03-26 19:22:39,637] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-03-26 19:22:44,423] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:22:44,424] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.88138579, 97.55397287000001, 1.0, 2.0, 0.2730984842536156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445400.8605887628, 445400.8605887628, 163224.3072725939]
[2019-03-26 19:22:44,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:22:44,427] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4091482e-21 1.0000000e+00 1.3996629e-22 1.7012504e-13 1.3622406e-24], sampled 0.5613292146235208
[2019-03-26 19:22:51,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:22:51,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 82.0, 1.0, 2.0, 0.4312494919988522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 630161.650939557, 630161.650939557, 177047.4515845739]
[2019-03-26 19:22:51,601] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:22:51,604] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1436286e-20 1.0000000e+00 8.6196905e-22 1.4353214e-12 1.1763385e-23], sampled 0.9702421201446121
[2019-03-26 19:23:13,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:13,064] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.76513844666667, 67.01846169833334, 1.0, 2.0, 0.8330994700017748, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005983595589171, 6.9112, 168.912315985339, 2061398.414632088, 1994155.924708872, 416200.8662000271]
[2019-03-26 19:23:13,065] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:23:13,067] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.1278377e-16 9.9945742e-01 8.2203399e-16 5.4255279e-04 2.4757942e-16], sampled 0.8663908091624895
[2019-03-26 19:23:13,068] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2061398.414632088 W.
[2019-03-26 19:23:16,799] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:16,799] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.8, 92.5, 1.0, 2.0, 0.4289859438025845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 625845.1367824614, 625845.1367824607, 176599.1175567735]
[2019-03-26 19:23:16,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:23:16,810] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6808503e-20 1.0000000e+00 2.6180202e-21 1.2359304e-12 3.4922064e-23], sampled 0.11047775330849696
[2019-03-26 19:23:18,222] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:18,223] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.03333333333333, 83.66666666666666, 1.0, 2.0, 0.492803915151457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688613.6462699765, 688613.6462699765, 182475.02472677]
[2019-03-26 19:23:18,223] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:23:18,225] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3038365e-21 1.0000000e+00 2.9476190e-22 3.5258042e-12 4.6277787e-24], sampled 0.20424379560203554
[2019-03-26 19:23:31,944] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:31,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.90000000000001, 51.66666666666667, 1.0, 2.0, 0.7008030845475833, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00276239083709, 6.9112, 168.9123371229272, 1876252.596380222, 1811295.323179027, 384942.1310306253]
[2019-03-26 19:23:31,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:23:31,950] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4276588e-16 9.9975365e-01 2.8850350e-16 2.4637682e-04 7.6596733e-17], sampled 0.9255962200316817
[2019-03-26 19:23:31,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1876252.596380222 W.
[2019-03-26 19:23:41,947] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:41,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.76666666666667, 59.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.950475185128887, 6.9112, 168.9125401415302, 1481637.108744108, 1453774.009348582, 311354.5496980809]
[2019-03-26 19:23:41,949] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:23:41,952] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8191287e-16 1.0000000e+00 3.3493999e-17 5.4953508e-09 1.6727158e-18], sampled 0.8140229599634363
[2019-03-26 19:23:42,000] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:23:42,002] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.16666666666667, 59.83333333333333, 1.0, 2.0, 0.6964648996900635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 973327.2769809135, 973327.2769809135, 220089.3975259496]
[2019-03-26 19:23:42,003] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:23:42,005] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.4718729e-22 9.9999928e-01 3.6927705e-22 7.7416303e-07 3.9295522e-23], sampled 0.9375859953570975
[2019-03-26 19:24:03,612] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:24:03,615] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.85773461, 78.346958255, 1.0, 2.0, 0.6319018931204741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 883061.3341917244, 883061.3341917244, 206841.6793527208]
[2019-03-26 19:24:03,616] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:24:03,619] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.3210621e-21 9.9996841e-01 7.9916665e-21 3.1537889e-05 1.5220628e-21], sampled 0.8980433615583524
[2019-03-26 19:24:06,380] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:24:06,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.94584753, 88.74904985, 1.0, 2.0, 0.9552080451606435, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565018987, 1335154.697097743, 1335154.697097744, 285570.4286452624]
[2019-03-26 19:24:06,384] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:24:06,389] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.57343598e-20 9.93136287e-01 2.58288083e-20 6.86370302e-03
 1.08908926e-20], sampled 0.1320450031356386
[2019-03-26 19:24:08,350] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:24:08,351] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.488189655, 84.766089535, 1.0, 2.0, 0.5122564076404725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715804.5117416723, 715804.5117416716, 185537.6479441572]
[2019-03-26 19:24:08,352] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:24:08,353] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8241658e-20 1.0000000e+00 4.7295675e-21 5.6991904e-11 1.1496855e-22], sampled 0.9242376240437791
[2019-03-26 19:24:25,936] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:24:25,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.13333333333333, 82.33333333333334, 1.0, 2.0, 0.8900136466947111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1243974.97643733, 1243974.976437329, 267208.5251794899]
[2019-03-26 19:24:25,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:24:25,941] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.2349309e-19 1.0000000e+00 1.1783351e-19 3.0184305e-10 3.7249050e-21], sampled 0.34893712996362447
[2019-03-26 19:24:32,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56736976]
[2019-03-26 19:24:32,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.06666666666667, 74.66666666666667, 1.0, 2.0, 0.739970497434048, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984023818104038, 6.9112, 168.912464191928, 1931063.142546823, 1879399.570161988, 394354.2892235941]
[2019-03-26 19:24:32,491] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:24:32,495] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1863457e-18 9.9945503e-01 6.2974931e-18 5.4491032e-04 1.8933333e-18], sampled 0.8786274733714496
[2019-03-26 19:24:32,496] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1931063.142546823 W.
[2019-03-26 19:24:33,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8001.4692 3155919684.0055 1397.0000
[2019-03-26 19:24:33,652] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.8663 2778007932.4087 894.0000
[2019-03-26 19:24:33,822] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8540.4632 2837189531.7748 976.0000
[2019-03-26 19:24:33,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8317.6320 2921326684.6099 1151.0000
[2019-03-26 19:24:33,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8111.3647 2994935184.3581 1400.0000
[2019-03-26 19:24:34,958] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 125000, evaluation results [125000.0, 8001.469154466778, 3155919684.005519, 1397.0, 8317.632045022483, 2921326684.609876, 1151.0, 8668.866328672348, 2778007932.408708, 894.0, 8111.364749496653, 2994935184.358118, 1400.0, 8540.463224235242, 2837189531.77485, 976.0]
[2019-03-26 19:24:41,285] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127858: loss 4.7942
[2019-03-26 19:24:41,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127859: learning rate 0.0010
[2019-03-26 19:24:41,307] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 127867: loss 4.7879
[2019-03-26 19:24:41,309] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 127868: learning rate 0.0010
[2019-03-26 19:24:41,440] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127929: loss 4.5747
[2019-03-26 19:24:41,441] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127929: learning rate 0.0010
[2019-03-26 19:24:41,453] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127934: loss 2.9437
[2019-03-26 19:24:41,455] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127935: learning rate 0.0010
[2019-03-26 19:24:41,477] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 127942: loss 4.1365
[2019-03-26 19:24:41,480] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 127942: learning rate 0.0010
[2019-03-26 19:24:41,533] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 127969: loss 2.5297
[2019-03-26 19:24:41,542] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 127970: learning rate 0.0010
[2019-03-26 19:24:41,560] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 127980: loss 2.3876
[2019-03-26 19:24:41,562] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127980: loss 4.0342
[2019-03-26 19:24:41,562] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 127980: learning rate 0.0010
[2019-03-26 19:24:41,566] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127981: loss 3.4165
[2019-03-26 19:24:41,569] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127981: learning rate 0.0010
[2019-03-26 19:24:41,570] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127981: learning rate 0.0010
[2019-03-26 19:24:41,591] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 127994: loss 2.3143
[2019-03-26 19:24:41,592] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 127994: learning rate 0.0010
[2019-03-26 19:24:41,682] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 128031: loss 1.5629
[2019-03-26 19:24:41,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 128032: learning rate 0.0010
[2019-03-26 19:24:41,688] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128035: loss 2.4996
[2019-03-26 19:24:41,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128036: learning rate 0.0010
[2019-03-26 19:24:41,722] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128049: loss 2.4156
[2019-03-26 19:24:41,724] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128050: learning rate 0.0010
[2019-03-26 19:24:41,780] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 128075: loss 1.7769
[2019-03-26 19:24:41,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 128075: learning rate 0.0010
[2019-03-26 19:24:41,793] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 128082: loss 1.7287
[2019-03-26 19:24:41,794] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 128082: learning rate 0.0010
[2019-03-26 19:24:41,814] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128088: loss 1.5830
[2019-03-26 19:24:41,816] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128089: learning rate 0.0010
[2019-03-26 19:24:43,006] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.0652311e-28 2.8542391e-07 7.7946974e-30 9.9999976e-01 9.7185103e-30], sum to 1.0000
[2019-03-26 19:24:43,017] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2410
[2019-03-26 19:24:43,023] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.2472412284529573, 1.0, 2.0, 0.2472412284529573, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 690957.7327918367, 690957.7327918367, 240492.7642915406], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4827600.0000, 
sim time next is 4828200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.2468717146924758, 1.0, 2.0, 0.2468717146924758, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 689924.731525478, 689924.731525478, 240432.8801944461], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 1.0, 1.0, 0.09261652372587444, 1.0, 1.0, 0.09261652372587444, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1916457587570772, 0.1916457587570772, 0.3588550450663375], 
reward next is 0.6411, 
noisyNet noise sample is [array([-1.5229441], dtype=float32), -0.7774435]. 
=============================================
[2019-03-26 19:24:48,415] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0809132e-24 6.0391653e-04 3.2001397e-24 9.9939609e-01 2.9612941e-24], sum to 1.0000
[2019-03-26 19:24:48,429] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0934
[2019-03-26 19:24:48,436] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 81.5, 1.0, 2.0, 0.2498322559555383, 1.0, 2.0, 0.2498322559555383, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 698201.1572695323, 698201.157269533, 240916.0390648369], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4915800.0000, 
sim time next is 4916400.0000, 
raw observation next is [27.0, 82.33333333333334, 1.0, 2.0, 0.2516560932861675, 1.0, 2.0, 0.2516560932861675, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 703299.8696362243, 703299.8696362243, 241216.3367878796], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.8233333333333335, 1.0, 1.0, 0.09838083528453916, 1.0, 1.0, 0.09838083528453916, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19536107489895121, 0.19536107489895121, 0.36002438326549197], 
reward next is 0.6400, 
noisyNet noise sample is [array([0.9627204], dtype=float32), -0.9169819]. 
=============================================
[2019-03-26 19:24:48,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.2682932e-25 1.4245522e-04 2.3174684e-23 9.9985754e-01 7.8278021e-28], sum to 1.0000
[2019-03-26 19:24:48,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7945
[2019-03-26 19:24:48,943] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.2547950332166585, 1.0, 2.0, 0.2547950332166585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 712075.1343383405, 712075.1343383405, 241738.0504635476], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4924200.0000, 
sim time next is 4924800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.2549214989229626, 1.0, 2.0, 0.2549214989229626, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 712428.6851534894, 712428.68515349, 241759.1253860083], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.10231505894332844, 1.0, 1.0, 0.10231505894332844, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19789685698708037, 0.19789685698708057, 0.3608345155015049], 
reward next is 0.6392, 
noisyNet noise sample is [array([-0.47658265], dtype=float32), 0.21849285]. 
=============================================
[2019-03-26 19:24:49,918] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6703455e-16 1.0000000e+00 1.7410912e-15 6.5721137e-11 1.1456220e-16], sum to 1.0000
[2019-03-26 19:24:49,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9162
[2019-03-26 19:24:49,930] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.6151211958055472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859601.392528048, 859601.392528048, 203588.7240195037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4940400.0000, 
sim time next is 4941000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.6053667032932727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845964.5439142855, 845964.5439142855, 201742.5127341375], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5245381967388827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23499015108730154, 0.23499015108730154, 0.30110822796139924], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.3711675], dtype=float32), 0.13209395]. 
=============================================
[2019-03-26 19:24:49,949] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[35.887794]
 [36.131313]
 [36.339317]
 [36.59304 ]
 [36.873997]], R is [[36.05044174]
 [36.38607407]
 [36.70934296]
 [37.01757812]
 [37.32071304]].
[2019-03-26 19:24:53,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2164426e-37 1.4570443e-18 2.0769956e-38 1.0000000e+00 2.7101146e-36], sum to 1.0000
[2019-03-26 19:24:53,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3761
[2019-03-26 19:24:53,076] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.2586010186843372, 1.0, 2.0, 0.2586010186843372, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 722715.2972435225, 722715.2972435225, 242379.7073330061], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4992600.0000, 
sim time next is 4993200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.258749322954503, 1.0, 2.0, 0.258749322954503, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 723129.9046400326, 723129.9046400319, 242404.3884696466], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.74, 1.0, 1.0, 0.1069268951259072, 1.0, 1.0, 0.1069268951259072, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.2008694179555646, 0.2008694179555644, 0.3617975947308158], 
reward next is 0.6382, 
noisyNet noise sample is [array([-2.0252662], dtype=float32), 0.115672775]. 
=============================================
[2019-03-26 19:24:59,127] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135880: loss 1.0492
[2019-03-26 19:24:59,128] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135880: learning rate 0.0010
[2019-03-26 19:24:59,154] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 135892: loss 0.9088
[2019-03-26 19:24:59,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 135893: learning rate 0.0010
[2019-03-26 19:24:59,248] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 135935: loss 1.5594
[2019-03-26 19:24:59,250] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 135935: learning rate 0.0010
[2019-03-26 19:24:59,259] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 135938: loss 1.9243
[2019-03-26 19:24:59,264] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 135941: learning rate 0.0010
[2019-03-26 19:24:59,281] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135948: loss 1.7957
[2019-03-26 19:24:59,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135949: learning rate 0.0010
[2019-03-26 19:24:59,293] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 135953: loss 2.2932
[2019-03-26 19:24:59,295] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 135953: learning rate 0.0010
[2019-03-26 19:24:59,366] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135985: loss 2.1963
[2019-03-26 19:24:59,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135986: learning rate 0.0010
[2019-03-26 19:24:59,371] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 135987: loss 1.4166
[2019-03-26 19:24:59,376] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 135988: learning rate 0.0010
[2019-03-26 19:24:59,391] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 135993: loss 1.9031
[2019-03-26 19:24:59,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 135994: learning rate 0.0010
[2019-03-26 19:24:59,413] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 136002: loss 1.4302
[2019-03-26 19:24:59,415] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 136002: learning rate 0.0010
[2019-03-26 19:24:59,441] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136015: loss 1.5318
[2019-03-26 19:24:59,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136017: learning rate 0.0010
[2019-03-26 19:24:59,469] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 136029: loss 1.0758
[2019-03-26 19:24:59,474] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 136030: learning rate 0.0010
[2019-03-26 19:24:59,501] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 136047: loss 0.6649
[2019-03-26 19:24:59,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 136048: learning rate 0.0010
[2019-03-26 19:24:59,528] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 136060: loss 0.4551
[2019-03-26 19:24:59,531] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 136061: learning rate 0.0010
[2019-03-26 19:24:59,531] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 136061: loss 0.5107
[2019-03-26 19:24:59,535] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 136061: learning rate 0.0010
[2019-03-26 19:24:59,580] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136079: loss 0.1461
[2019-03-26 19:24:59,585] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136080: learning rate 0.0010
[2019-03-26 19:25:00,235] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2242698e-27 1.0000000e+00 2.3640379e-29 5.7533356e-13 1.4995279e-32], sum to 1.0000
[2019-03-26 19:25:00,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1544
[2019-03-26 19:25:00,253] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4835689770511917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675829.8095865631, 675829.8095865624, 181077.0919183193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5112000.0000, 
sim time next is 5112600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4830505893968829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675104.7403194243, 675104.740319425, 180998.4027749045], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3771693848155216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18752909453317343, 0.18752909453317362, 0.2701468698132903], 
reward next is 0.7299, 
noisyNet noise sample is [array([-2.2427504], dtype=float32), 0.57606304]. 
=============================================
[2019-03-26 19:25:02,462] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3018497e-31 1.0000000e+00 3.3533213e-34 2.3137323e-14 1.6832069e-33], sum to 1.0000
[2019-03-26 19:25:02,474] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8584
[2019-03-26 19:25:02,482] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.00000000000001, 1.0, 2.0, 0.5525213899601028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772089.4704744226, 772089.4704744231, 192234.4598290622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5149200.0000, 
sim time next is 5149800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5509749634307706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769927.7218306224, 769927.7218306231, 191968.559560129], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45900598003707294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21386881161961735, 0.21386881161961754, 0.28652023814944627], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.09958028], dtype=float32), 2.5319612]. 
=============================================
[2019-03-26 19:25:02,903] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1040293e-38 1.0000000e+00 0.0000000e+00 4.1214982e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 19:25:02,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2004
[2019-03-26 19:25:02,916] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5427956296079233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758493.9095505218, 758493.9095505225, 190572.9430491542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5160000.0000, 
sim time next is 5160600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5417049376440679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756969.2510182518, 756969.2510182523, 190388.4717676915], 
processed observation next is [0.0, 0.7391304347826086, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4478372742699613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102692363939588, 0.21026923639395897, 0.28416189816073356], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.66688526], dtype=float32), 0.48132116]. 
=============================================
[2019-03-26 19:25:03,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0955299e-26 1.0000000e+00 1.1050282e-26 1.0980533e-10 6.3943048e-30], sum to 1.0000
[2019-03-26 19:25:03,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7069
[2019-03-26 19:25:03,415] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5227833462935365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730519.4496325778, 730519.4496325784, 187242.60886876], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5168400.0000, 
sim time next is 5169000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5220866714079402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729545.6058289391, 729545.6058289384, 187128.7836591793], 
processed observation next is [0.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.4242008089252291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20265155717470532, 0.20265155717470512, 0.2792966920286258], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.6606498], dtype=float32), 0.63422173]. 
=============================================
[2019-03-26 19:25:03,423] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.48299 ]
 [73.42867 ]
 [73.38329 ]
 [73.32673 ]
 [73.277954]], R is [[73.51229858]
 [73.49771118]
 [73.48310852]
 [73.46827698]
 [73.45207214]].
[2019-03-26 19:25:04,608] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0177920e-22 1.0000000e+00 2.6556640e-25 6.7675721e-10 7.5431974e-28], sum to 1.0000
[2019-03-26 19:25:04,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7663
[2019-03-26 19:25:04,619] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5163661980249745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721549.3020272445, 721549.3020272438, 186199.6390236945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5190000.0000, 
sim time next is 5190600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5161590341472515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721259.7213000394, 721259.7213000394, 186166.1952553471], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4170590772858452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20034992258334428, 0.20034992258334428, 0.2778599929184285], 
reward next is 0.7221, 
noisyNet noise sample is [array([1.2136033], dtype=float32), 0.26704907]. 
=============================================
[2019-03-26 19:25:04,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9737432e-20 1.0000000e+00 2.0632157e-23 4.0040589e-14 9.4744239e-23], sum to 1.0000
[2019-03-26 19:25:04,840] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9664
[2019-03-26 19:25:04,845] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5156102579911713, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720492.6236715536, 720492.623671553, 186077.6642661265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5188800.0000, 
sim time next is 5189400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5159126785277046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720915.3571234473, 720915.3571234473, 186126.441042029], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4167622632863911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20025426586762426, 0.20025426586762426, 0.2778006582716851], 
reward next is 0.7222, 
noisyNet noise sample is [array([2.0236895], dtype=float32), -0.9099606]. 
=============================================
[2019-03-26 19:25:07,194] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.3700499e-17 5.7099539e-01 9.3135309e-17 4.2900461e-01 1.2589754e-15], sum to 1.0000
[2019-03-26 19:25:07,203] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7001
[2019-03-26 19:25:07,209] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8837098603219602, 1.0, 2.0, 0.8837098603219602, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2471686.165809982, 2471686.165809982, 462685.2982689391], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5232000.0000, 
sim time next is 5232600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.9310296446545802, 1.0, 2.0, 0.9310296446545802, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2604174.831817282, 2604174.831817282, 488705.1689742079], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.9169031863308195, 1.0, 1.0, 0.9169031863308195, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7233818977270228, 0.7233818977270228, 0.7294106999615043], 
reward next is 0.2706, 
noisyNet noise sample is [array([-1.3919963], dtype=float32), -1.1849135]. 
=============================================
[2019-03-26 19:25:09,394] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5700386e-28 1.0000000e+00 7.9273609e-30 6.1016125e-20 2.5052113e-30], sum to 1.0000
[2019-03-26 19:25:09,403] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6442
[2019-03-26 19:25:09,409] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 82.0, 1.0, 2.0, 0.5512454862350286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770305.8851659923, 770305.885165993, 192014.7141803915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5265000.0000, 
sim time next is 5265600.0000, 
raw observation next is [28.5, 82.33333333333333, 1.0, 2.0, 0.5520011049038989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771362.1638402259, 771362.1638402259, 192144.7478012505], 
processed observation next is [1.0, 0.9565217391304348, 0.5497630331753555, 0.8233333333333333, 1.0, 1.0, 0.46024229506493836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21426726773339608, 0.21426726773339608, 0.2867832056735082], 
reward next is 0.7132, 
noisyNet noise sample is [array([1.0137739], dtype=float32), 0.2594661]. 
=============================================
[2019-03-26 19:25:10,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0217386e-20 1.0000000e+00 3.1001040e-22 2.3994465e-12 2.8549286e-20], sum to 1.0000
[2019-03-26 19:25:10,544] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1736
[2019-03-26 19:25:10,549] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.8453244220013031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1181477.948575945, 1181477.948575945, 255344.3761271651], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5287800.0000, 
sim time next is 5288400.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.8386196192004669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1172101.727168837, 1172101.727168838, 253615.2669527114], 
processed observation next is [1.0, 0.21739130434782608, 0.5545023696682465, 0.88, 1.0, 1.0, 0.8055658062656228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32558381310245477, 0.325583813102455, 0.3785302491831513], 
reward next is 0.6215, 
noisyNet noise sample is [array([-0.61355096], dtype=float32), 0.6540861]. 
=============================================
[2019-03-26 19:25:12,669] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.3567870e-14 9.9997604e-01 1.2412805e-15 2.4020053e-05 3.1297431e-17], sum to 1.0000
[2019-03-26 19:25:12,674] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4011
[2019-03-26 19:25:12,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3102431.809198635 W.
[2019-03-26 19:25:12,691] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.11666666666667, 52.83333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.180453735796751, 6.9112, 170.5573041426782, 3102431.809198635, 2909554.405705713, 552238.129599302], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5320200.0000, 
sim time next is 5320800.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 0.726033835886696, 1.0, 2.0, 0.6836069574576106, 1.0, 1.0, 1.03, 7.005099784989508, 6.9112, 170.5573041426782, 2868469.589289329, 2801205.3537023, 529678.6517973167], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 0.6699202842008386, 1.0, 1.0, 0.6188035632019404, 1.0, 0.5, 1.0365853658536586, 0.009389978498950758, 0.0, 0.8375144448122397, 0.7967971081359247, 0.7781125982506389, 0.7905651519362936], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.81417894], dtype=float32), -1.3206657]. 
=============================================
[2019-03-26 19:25:12,714] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.5086377e-18 9.9999988e-01 2.7578117e-17 1.4003999e-07 3.6448724e-19], sum to 1.0000
[2019-03-26 19:25:12,719] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7268
[2019-03-26 19:25:12,727] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3197025.475442735 W.
[2019-03-26 19:25:12,731] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.31235133122056, 6.9112, 170.5573041426782, 3197025.475442735, 2909664.459096589, 551513.8260665198], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5323200.0000, 
sim time next is 5323800.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.644671422077757, 6.9112, 170.5573041426782, 3435357.194339374, 2909941.778995204, 549567.6850369088], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.07334714220777575, 0.0, 0.8375144448122397, 0.9542658873164928, 0.808317160832001, 0.8202502761744908], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.24210317], dtype=float32), 0.54244304]. 
=============================================
[2019-03-26 19:25:15,163] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8236447e-33 1.0000000e+00 7.6090013e-36 1.2758410e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 19:25:15,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4244
[2019-03-26 19:25:15,176] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 86.0, 1.0, 2.0, 0.6006835132853975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 839417.4738056152, 839417.4738056146, 200873.6446078068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5364000.0000, 
sim time next is 5364600.0000, 
raw observation next is [29.25, 86.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 11.41401588712133, 6.9112, 170.3810597889865, 4677914.128340551, 1455697.048234397, 301122.304110757], 
processed observation next is [1.0, 0.08695652173913043, 0.5853080568720379, 0.865, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.45028158871213303, 0.0, 0.8366490043505993, 1.2994205912057086, 0.4043602911762214, 0.44943627479217463], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8878948], dtype=float32), -1.4452494]. 
=============================================
[2019-03-26 19:25:16,985] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 143893: loss -14.8055
[2019-03-26 19:25:16,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 143893: learning rate 0.0010
[2019-03-26 19:25:17,012] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 143904: loss -30.7580
[2019-03-26 19:25:17,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 143904: learning rate 0.0010
[2019-03-26 19:25:17,041] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143917: loss -1.2929
[2019-03-26 19:25:17,043] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143918: learning rate 0.0010
[2019-03-26 19:25:17,125] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143956: loss 37.9170
[2019-03-26 19:25:17,127] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143957: learning rate 0.0010
[2019-03-26 19:25:17,142] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 143961: loss -10.3286
[2019-03-26 19:25:17,144] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 143961: learning rate 0.0010
[2019-03-26 19:25:17,170] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143974: loss -48.7283
[2019-03-26 19:25:17,172] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143975: learning rate 0.0010
[2019-03-26 19:25:17,201] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 143987: loss -18.6086
[2019-03-26 19:25:17,206] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 143987: learning rate 0.0010
[2019-03-26 19:25:17,211] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 143992: loss -28.7177
[2019-03-26 19:25:17,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 143992: learning rate 0.0010
[2019-03-26 19:25:17,228] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 144000: loss -66.7245
[2019-03-26 19:25:17,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 144000: learning rate 0.0010
[2019-03-26 19:25:17,242] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 144005: loss -21.6559
[2019-03-26 19:25:17,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 144005: learning rate 0.0010
[2019-03-26 19:25:17,254] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 144012: loss 3.2096
[2019-03-26 19:25:17,255] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 144012: learning rate 0.0010
[2019-03-26 19:25:17,283] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144024: loss -32.9089
[2019-03-26 19:25:17,287] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144024: learning rate 0.0010
[2019-03-26 19:25:17,288] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 144025: loss -27.0323
[2019-03-26 19:25:17,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 144025: learning rate 0.0010
[2019-03-26 19:25:17,301] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144028: loss 5.6575
[2019-03-26 19:25:17,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144030: learning rate 0.0010
[2019-03-26 19:25:17,336] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144044: loss -1.5912
[2019-03-26 19:25:17,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144045: learning rate 0.0010
[2019-03-26 19:25:17,463] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 144101: loss -51.8671
[2019-03-26 19:25:17,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 144103: learning rate 0.0010
[2019-03-26 19:25:20,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4986126e-25 1.0000000e+00 1.5447923e-25 1.2374970e-19 6.2236728e-25], sum to 1.0000
[2019-03-26 19:25:20,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6497
[2019-03-26 19:25:20,294] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 92.0, 1.0, 2.0, 1.020220102079698, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912856602807, 1426087.197637082, 1426087.197637082, 305150.9151187706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5454000.0000, 
sim time next is 5454600.0000, 
raw observation next is [27.86666666666667, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.945481483920778, 6.9112, 168.9125790459621, 1478091.993395194, 1453771.583172353, 311353.9108126464], 
processed observation next is [1.0, 0.13043478260869565, 0.519747235387046, 0.92, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0034281483920778123, 0.0, 0.8294380916289564, 0.41058110927644276, 0.40382543977009805, 0.464707329571114], 
reward next is 0.3639, 
noisyNet noise sample is [array([1.6087307], dtype=float32), 1.2408113]. 
=============================================
[2019-03-26 19:25:21,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.7542498e-09 9.9999583e-01 8.6178709e-09 4.1438511e-06 1.8004162e-08], sum to 1.0000
[2019-03-26 19:25:21,874] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0215
[2019-03-26 19:25:21,881] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2810881.447837377 W.
[2019-03-26 19:25:21,888] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.46666666666667, 68.66666666666667, 1.0, 2.0, 1.004847165207756, 1.0, 2.0, 1.004847165207756, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2810881.447837377, 2810881.447837377, 531932.836165648], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5478600.0000, 
sim time next is 5479200.0000, 
raw observation next is [33.7, 68.0, 1.0, 2.0, 0.9879810473190169, 1.0, 2.0, 0.9879810473190169, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2763649.291323984, 2763649.291323983, 521777.2788563376], 
processed observation next is [1.0, 0.43478260869565216, 0.7962085308056873, 0.68, 1.0, 1.0, 0.9855193341192975, 1.0, 1.0, 0.9855193341192975, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7676803587011066, 0.7676803587011064, 0.7787720579945338], 
reward next is 0.2212, 
noisyNet noise sample is [array([-1.1338032], dtype=float32), -0.6844846]. 
=============================================
[2019-03-26 19:25:24,193] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9514276e-21 1.0000000e+00 7.7466683e-25 1.4589243e-14 1.6624081e-21], sum to 1.0000
[2019-03-26 19:25:24,201] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2011
[2019-03-26 19:25:24,207] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.96666666666667, 78.5, 1.0, 2.0, 0.5822848562401479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813696.6579454262, 813696.6579454262, 197492.1938883586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5515800.0000, 
sim time next is 5516400.0000, 
raw observation next is [29.83333333333334, 79.0, 1.0, 2.0, 0.5815143960302137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812619.5893772576, 812619.589377257, 197352.7054057249], 
processed observation next is [1.0, 0.8695652173913043, 0.6129541864139023, 0.79, 1.0, 1.0, 0.49580047714483577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22572766371590489, 0.22572766371590475, 0.29455627672496254], 
reward next is 0.7054, 
noisyNet noise sample is [array([0.61243176], dtype=float32), 0.07125934]. 
=============================================
[2019-03-26 19:25:26,365] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2463640e-26 1.0000000e+00 2.2064359e-29 2.2065324e-18 2.8217143e-25], sum to 1.0000
[2019-03-26 19:25:26,373] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9307
[2019-03-26 19:25:26,382] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 90.0, 1.0, 2.0, 0.6987487340141891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 976520.4605154968, 976520.4605154968, 220575.1261582956], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5551200.0000, 
sim time next is 5551800.0000, 
raw observation next is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.8164064319881933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141038.651709217, 1141038.651709217, 247977.6576206631], 
processed observation next is [1.0, 0.2608695652173913, 0.4723538704581361, 0.8933333333333333, 1.0, 1.0, 0.778802930106257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3169551810303381, 0.3169551810303381, 0.37011590689651214], 
reward next is 0.6299, 
noisyNet noise sample is [array([0.10348429], dtype=float32), 0.1066212]. 
=============================================
[2019-03-26 19:25:30,637] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 19:25:30,638] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:25:30,640] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:25:30,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:25:30,641] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:25:30,643] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:25:30,642] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:25:30,647] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:25:30,648] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:25:30,644] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:25:30,652] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:25:30,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-03-26 19:25:30,666] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-03-26 19:25:30,704] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-03-26 19:25:30,708] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-03-26 19:25:30,752] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-03-26 19:25:32,023] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:25:32,024] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.93333333333333, 66.16666666666667, 1.0, 2.0, 0.4426115663496044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643186.69618311, 643186.6961831107, 178250.1281482361]
[2019-03-26 19:25:32,026] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:25:32,028] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.3912147e-26 1.0000000e+00 3.0965959e-28 2.6942356e-17 2.6504884e-28], sampled 0.9238977609707422
[2019-03-26 19:25:39,207] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:25:39,208] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.2, 67.5, 1.0, 2.0, 0.4726403767214698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777850.2612768414, 777850.2612768414, 191015.0072088683]
[2019-03-26 19:25:39,209] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:25:39,211] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7269042e-26 1.0000000e+00 2.4685203e-28 5.1917077e-18 1.3987833e-28], sampled 0.9343290911873436
[2019-03-26 19:25:43,729] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:25:43,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.970704455, 79.67751479833333, 1.0, 2.0, 0.36919094165473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563658.368834658, 563658.3688346585, 171665.5067063825]
[2019-03-26 19:25:43,733] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:25:43,737] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.9345167e-28 1.0000000e+00 2.2368679e-30 7.7291581e-19 1.5893506e-30], sampled 0.002109355985278838
[2019-03-26 19:25:44,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:25:44,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.55, 73.0, 1.0, 2.0, 0.344679662748107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535731.3156585245, 535731.3156585252, 169601.5698973982]
[2019-03-26 19:25:44,749] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:25:44,751] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.8281642e-28 1.0000000e+00 1.4035925e-30 7.6865826e-19 1.0880351e-30], sampled 0.2024733542991699
[2019-03-26 19:26:12,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:26:12,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.05, 53.5, 1.0, 2.0, 0.8551549308359105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195225.411968925, 1195225.411968925, 257898.832423442]
[2019-03-26 19:26:12,245] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:26:12,248] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0973795e-24 1.0000000e+00 1.2193538e-25 2.6953103e-13 5.2572772e-25], sampled 0.5233914052216927
[2019-03-26 19:26:14,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:26:14,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 95.0, 1.0, 2.0, 0.5296480638879271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787887.8744350799, 787887.8744350792, 194310.1199839892]
[2019-03-26 19:26:14,490] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:26:14,492] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2359094e-25 1.0000000e+00 1.8533070e-27 1.4728558e-15 3.8805824e-27], sampled 0.4479623766077472
[2019-03-26 19:26:23,608] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:26:23,609] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.6, 68.0, 1.0, 2.0, 0.8903823568360509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1244490.625834642, 1244490.625834642, 267308.8385902786]
[2019-03-26 19:26:23,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:26:23,616] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.7724766e-23 1.0000000e+00 1.6845570e-24 1.7450635e-12 7.8164890e-24], sampled 0.8692059630880682
[2019-03-26 19:26:55,558] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:26:55,559] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.64795648, 75.71977861, 1.0, 2.0, 0.5830357713800746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814746.4044396526, 814746.4044396526, 197628.7282836535]
[2019-03-26 19:26:55,560] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:26:55,565] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4045288e-24 1.0000000e+00 3.9883559e-26 5.0498028e-10 1.5176789e-24], sampled 0.4875081472579512
[2019-03-26 19:27:18,427] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:27:18,427] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.25, 91.5, 1.0, 2.0, 0.3877300353579577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579652.4909455976, 579652.4909455976, 172711.0493729905]
[2019-03-26 19:27:18,430] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:27:18,433] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.0507489e-28 1.0000000e+00 3.3453042e-30 3.7600715e-19 1.8463253e-30], sampled 0.7364978144787466
[2019-03-26 19:27:23,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6180507]
[2019-03-26 19:27:23,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 61.0, 1.0, 2.0, 0.2995045996463072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480240.1397591506, 480240.1397591506, 165674.2809422359]
[2019-03-26 19:27:23,130] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:27:23,133] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5571324e-27 1.0000000e+00 6.7271966e-30 5.3882963e-18 7.1726002e-30], sampled 0.01902652399198279
[2019-03-26 19:27:24,471] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8178.8538 3149490261.2334 966.0000
[2019-03-26 19:27:24,557] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8352.0360 2980471846.5509 864.0000
[2019-03-26 19:27:24,648] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8724.5129 2775064959.9458 763.0000
[2019-03-26 19:27:24,789] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8432.1622 2916114844.0785 897.0000
[2019-03-26 19:27:24,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8639.9625 2832936991.7925 751.0000
[2019-03-26 19:27:25,993] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 150000, evaluation results [150000.0, 8178.8538107675295, 3149490261.2334013, 966.0, 8432.162233338542, 2916114844.0784554, 897.0, 8724.512926229789, 2775064959.945827, 763.0, 8352.036029982624, 2980471846.5508895, 864.0, 8639.962517331642, 2832936991.7924705, 751.0]
[2019-03-26 19:27:30,070] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 151851: loss 0.0001
[2019-03-26 19:27:30,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 151852: learning rate 0.0010
[2019-03-26 19:27:30,184] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 151907: loss 0.0649
[2019-03-26 19:27:30,187] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 151908: learning rate 0.0010
[2019-03-26 19:27:30,207] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151917: loss 0.1350
[2019-03-26 19:27:30,210] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151917: learning rate 0.0010
[2019-03-26 19:27:30,284] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151947: loss 0.0978
[2019-03-26 19:27:30,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151947: learning rate 0.0010
[2019-03-26 19:27:30,338] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151971: loss 0.0913
[2019-03-26 19:27:30,345] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151972: learning rate 0.0010
[2019-03-26 19:27:30,364] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151980: loss 0.0884
[2019-03-26 19:27:30,365] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151980: learning rate 0.0010
[2019-03-26 19:27:30,393] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 151992: loss 0.0289
[2019-03-26 19:27:30,395] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 151992: learning rate 0.0010
[2019-03-26 19:27:30,409] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 152000: loss 0.0006
[2019-03-26 19:27:30,409] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152000: loss 0.0028
[2019-03-26 19:27:30,410] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 152000: learning rate 0.0010
[2019-03-26 19:27:30,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152000: learning rate 0.0010
[2019-03-26 19:27:30,415] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 152000: loss 0.0230
[2019-03-26 19:27:30,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 152002: learning rate 0.0010
[2019-03-26 19:27:30,448] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152020: loss 0.0004
[2019-03-26 19:27:30,450] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152020: loss 0.0019
[2019-03-26 19:27:30,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152020: learning rate 0.0010
[2019-03-26 19:27:30,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152021: learning rate 0.0010
[2019-03-26 19:27:30,482] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 152034: loss 0.0002
[2019-03-26 19:27:30,491] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 152034: learning rate 0.0010
[2019-03-26 19:27:30,502] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152038: loss 0.0007
[2019-03-26 19:27:30,505] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152038: learning rate 0.0010
[2019-03-26 19:27:30,513] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152042: loss 0.0561
[2019-03-26 19:27:30,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152045: learning rate 0.0010
[2019-03-26 19:27:30,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 152098: loss 0.0134
[2019-03-26 19:27:30,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 152101: learning rate 0.0010
[2019-03-26 19:27:36,338] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1574480e-20 1.0000000e+00 1.9213496e-20 2.6429692e-11 3.3129266e-19], sum to 1.0000
[2019-03-26 19:27:36,349] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1794
[2019-03-26 19:27:36,353] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 91.66666666666667, 1.0, 2.0, 0.3819106484658206, 1.0, 1.0, 0.3819106484658206, 1.0, 1.0, 0.655074150687757, 6.9112, 6.9112, 170.5573041426782, 1601651.262662901, 1601651.262662901, 342013.8145316627], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5800800.0000, 
sim time next is 5801400.0000, 
raw observation next is [26.35, 92.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.128495392506945, 6.9112, 168.9115555400722, 1608016.054500173, 1453860.50290801, 311349.8381904884], 
processed observation next is [1.0, 0.13043478260869565, 0.4478672985781992, 0.92, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.02172953925069452, 0.0, 0.8294330657464739, 0.44667112625004807, 0.40385013969666944, 0.4647012510305797], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.8552046], dtype=float32), -0.25394264]. 
=============================================
[2019-03-26 19:27:37,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1379842e-20 1.0000000e+00 1.6113694e-24 3.5707696e-08 2.0467245e-21], sum to 1.0000
[2019-03-26 19:27:37,225] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5395
[2019-03-26 19:27:37,240] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 86.33333333333334, 1.0, 2.0, 0.8800687012622707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1230066.84180666, 1230066.841806659, 264513.9030617427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5811600.0000, 
sim time next is 5812200.0000, 
raw observation next is [27.55, 85.5, 1.0, 2.0, 0.956055345539492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1336339.767816107, 1336339.767816107, 285822.5644311356], 
processed observation next is [1.0, 0.2608695652173913, 0.504739336492891, 0.855, 1.0, 1.0, 0.9470546331801108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3712054910600297, 0.3712054910600297, 0.4266008424345307], 
reward next is 0.5734, 
noisyNet noise sample is [array([0.541348], dtype=float32), 0.52700686]. 
=============================================
[2019-03-26 19:27:38,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 7.6684865e-33 0.0000000e+00 1.0000000e+00 1.2136113e-36], sum to 1.0000
[2019-03-26 19:27:38,980] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1719
[2019-03-26 19:27:38,986] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.55, 62.16666666666667, 1.0, 2.0, 0.8877548980970035, 1.0, 2.0, 0.8877548980970035, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2483011.147169054, 2483011.147169055, 464855.7422388227], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5843400.0000, 
sim time next is 5844000.0000, 
raw observation next is [32.5, 62.33333333333334, 1.0, 2.0, 0.8453105903781825, 1.0, 2.0, 0.8453105903781825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2364183.99548175, 2364183.99548175, 442534.6685552174], 
processed observation next is [1.0, 0.6521739130434783, 0.7393364928909952, 0.6233333333333334, 1.0, 1.0, 0.8136272173231114, 1.0, 1.0, 0.8136272173231114, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6567177765227084, 0.6567177765227084, 0.6604995053062946], 
reward next is 0.3395, 
noisyNet noise sample is [array([0.9768446], dtype=float32), -0.0826679]. 
=============================================
[2019-03-26 19:27:39,000] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[101.7844  ]
 [100.881294]
 [100.620605]
 [100.49774 ]
 [100.33252 ]], R is [[101.89527893]
 [101.18251038]
 [100.3740387 ]
 [ 99.56916809]
 [ 98.77226257]].
[2019-03-26 19:27:39,437] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9682125e-33 7.5114162e-19 1.4300021e-33 1.0000000e+00 1.1111469e-22], sum to 1.0000
[2019-03-26 19:27:39,448] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1449
[2019-03-26 19:27:39,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.8, 76.83333333333333, 1.0, 2.0, 0.2793049630917407, 1.0, 2.0, 0.2793049630917407, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 780597.9087455474, 780597.9087455474, 246009.8859832747], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5856600.0000, 
sim time next is 5857200.0000, 
raw observation next is [29.6, 78.0, 1.0, 2.0, 0.2796494235827841, 1.0, 2.0, 0.2796494235827841, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 781560.9532435107, 781560.9532435107, 246072.2300622698], 
processed observation next is [1.0, 0.8260869565217391, 0.6018957345971565, 0.78, 1.0, 1.0, 0.13210773925636637, 1.0, 1.0, 0.13210773925636637, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21710026478986408, 0.21710026478986408, 0.3672719851675669], 
reward next is 0.6327, 
noisyNet noise sample is [array([-1.0132409], dtype=float32), -0.038215797]. 
=============================================
[2019-03-26 19:27:41,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5549432e-18 1.0000000e+00 2.2008727e-20 2.0853007e-11 1.7605410e-21], sum to 1.0000
[2019-03-26 19:27:41,282] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1113
[2019-03-26 19:27:41,287] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 0.5262040121936707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735301.0247756034, 735301.0247756029, 187803.544846052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5880000.0000, 
sim time next is 5880600.0000, 
raw observation next is [26.15, 93.0, 1.0, 2.0, 0.5257780160128267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734705.5451753035, 734705.545175304, 187733.5330950924], 
processed observation next is [1.0, 0.043478260869565216, 0.43838862559241704, 0.93, 1.0, 1.0, 0.4286482120636466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20408487365980651, 0.20408487365980668, 0.2801993031270036], 
reward next is 0.7198, 
noisyNet noise sample is [array([1.3311099], dtype=float32), -0.4885078]. 
=============================================
[2019-03-26 19:27:43,476] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.4934524e-34 0.0000000e+00 1.0000000e+00 3.6763898e-34], sum to 1.0000
[2019-03-26 19:27:43,484] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4353
[2019-03-26 19:27:43,488] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 72.16666666666667, 1.0, 2.0, 0.8985581483871018, 1.0, 2.0, 0.8985581483871018, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2513257.748115304, 2513257.748115305, 470709.4290765268], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5919000.0000, 
sim time next is 5919600.0000, 
raw observation next is [31.03333333333334, 72.33333333333334, 1.0, 2.0, 0.7938100133190397, 1.0, 2.0, 0.7938100133190397, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2220018.068483935, 2220018.068483935, 416874.9461326379], 
processed observation next is [1.0, 0.5217391304347826, 0.6698262243285943, 0.7233333333333334, 1.0, 1.0, 0.7515783293000478, 1.0, 1.0, 0.7515783293000478, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.616671685689982, 0.616671685689982, 0.6222014121382655], 
reward next is 0.3778, 
noisyNet noise sample is [array([-0.26056388], dtype=float32), 1.3071833]. 
=============================================
[2019-03-26 19:27:47,864] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 159842: loss 0.1349
[2019-03-26 19:27:47,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 159844: learning rate 0.0010
[2019-03-26 19:27:47,967] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 159891: loss 0.1770
[2019-03-26 19:27:47,969] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 159891: learning rate 0.0010
[2019-03-26 19:27:47,995] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159903: loss 0.1763
[2019-03-26 19:27:47,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159903: learning rate 0.0010
[2019-03-26 19:27:48,122] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159958: loss 0.2466
[2019-03-26 19:27:48,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159960: learning rate 0.0010
[2019-03-26 19:27:48,129] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159962: loss 0.1077
[2019-03-26 19:27:48,132] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159963: learning rate 0.0010
[2019-03-26 19:27:48,167] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159977: loss 0.2892
[2019-03-26 19:27:48,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159977: learning rate 0.0010
[2019-03-26 19:27:48,190] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159987: loss 0.2547
[2019-03-26 19:27:48,191] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159987: learning rate 0.0010
[2019-03-26 19:27:48,215] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 159993: loss 0.1687
[2019-03-26 19:27:48,218] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 159993: learning rate 0.0010
[2019-03-26 19:27:48,233] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160000: loss 0.2884
[2019-03-26 19:27:48,238] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160000: learning rate 0.0010
[2019-03-26 19:27:48,262] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 160012: loss 0.3199
[2019-03-26 19:27:48,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 160012: learning rate 0.0010
[2019-03-26 19:27:48,297] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 160026: loss 0.2956
[2019-03-26 19:27:48,297] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160026: loss 0.3363
[2019-03-26 19:27:48,300] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 160026: learning rate 0.0010
[2019-03-26 19:27:48,300] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160026: learning rate 0.0010
[2019-03-26 19:27:48,344] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 160050: loss 0.2517
[2019-03-26 19:27:48,344] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160050: loss 0.3946
[2019-03-26 19:27:48,346] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 160050: learning rate 0.0010
[2019-03-26 19:27:48,347] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160051: learning rate 0.0010
[2019-03-26 19:27:48,384] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 160065: loss 0.1517
[2019-03-26 19:27:48,386] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160065: loss 0.2638
[2019-03-26 19:27:48,387] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 160065: learning rate 0.0010
[2019-03-26 19:27:48,388] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160066: learning rate 0.0010
[2019-03-26 19:27:53,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 2.0073894e-29 0.0000000e+00 1.0000000e+00 4.8890985e-33], sum to 1.0000
[2019-03-26 19:27:53,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3329
[2019-03-26 19:27:53,443] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.46666666666667, 68.33333333333334, 1.0, 2.0, 0.664300702823733, 1.0, 2.0, 0.664300702823733, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1857511.245457109, 1857511.245457109, 359215.8143287394], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6085200.0000, 
sim time next is 6085800.0000, 
raw observation next is [30.6, 67.5, 1.0, 2.0, 0.670234487580457, 1.0, 2.0, 0.670234487580457, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1874117.748292412, 1874117.748292412, 361641.7309858361], 
processed observation next is [1.0, 0.43478260869565216, 0.6492890995260664, 0.675, 1.0, 1.0, 0.602692153711394, 1.0, 1.0, 0.602692153711394, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5205882634145589, 0.5205882634145589, 0.5397637775908002], 
reward next is 0.4602, 
noisyNet noise sample is [array([-1.1704465], dtype=float32), -1.4519635]. 
=============================================
[2019-03-26 19:27:57,869] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.0641227e-35 6.3205915e-19 1.1038692e-34 1.0000000e+00 1.6552840e-27], sum to 1.0000
[2019-03-26 19:27:57,876] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7726
[2019-03-26 19:27:57,879] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.3, 84.0, 1.0, 2.0, 0.734205014842494, 1.0, 2.0, 0.734205014842494, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2053164.307682266, 2053164.307682265, 389112.9682641824], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6166800.0000, 
sim time next is 6167400.0000, 
raw observation next is [28.38333333333334, 83.33333333333333, 1.0, 2.0, 0.6919938550228144, 1.0, 2.0, 0.6919938550228144, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1935016.497028113, 1935016.497028113, 370718.3132631826], 
processed observation next is [1.0, 0.391304347826087, 0.544233807266983, 0.8333333333333333, 1.0, 1.0, 0.6289082590636318, 1.0, 1.0, 0.6289082590636318, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5375045825078092, 0.5375045825078092, 0.553310915318183], 
reward next is 0.4467, 
noisyNet noise sample is [array([1.9987097], dtype=float32), 0.24940541]. 
=============================================
[2019-03-26 19:28:04,201] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 6.1249437e-24 0.0000000e+00 1.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 19:28:04,210] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8552
[2019-03-26 19:28:04,215] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.85, 63.0, 1.0, 2.0, 0.2594426341176778, 1.0, 2.0, 0.2594426341176778, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 725068.1647541542, 725068.1647541547, 242520.5756910857], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6269400.0000, 
sim time next is 6270000.0000, 
raw observation next is [30.86666666666667, 62.66666666666666, 1.0, 2.0, 0.2585057092810084, 1.0, 2.0, 0.2585057092810084, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 722448.8452324833, 722448.8452324833, 242361.8476108506], 
processed observation next is [0.0, 0.5652173913043478, 0.6619273301737759, 0.6266666666666666, 1.0, 1.0, 0.10663338467591374, 1.0, 1.0, 0.10663338467591374, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20068023478680094, 0.20068023478680094, 0.3617341009117173], 
reward next is 0.6383, 
noisyNet noise sample is [array([-0.58092797], dtype=float32), -1.5728447]. 
=============================================
[2019-03-26 19:28:04,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[80.1183 ]
 [80.0981 ]
 [80.0709 ]
 [80.03689]
 [79.97874]], R is [[79.9863205 ]
 [79.82448578]
 [79.66400146]
 [79.50500488]
 [79.34748077]].
[2019-03-26 19:28:05,708] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 167854: loss 0.0750
[2019-03-26 19:28:05,718] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 167857: learning rate 0.0010
[2019-03-26 19:28:05,745] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 167871: loss 0.0051
[2019-03-26 19:28:05,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 167871: learning rate 0.0010
[2019-03-26 19:28:05,809] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167899: loss 0.0402
[2019-03-26 19:28:05,813] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167899: learning rate 0.0010
[2019-03-26 19:28:05,867] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167926: loss 0.0034
[2019-03-26 19:28:05,869] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167926: learning rate 0.0010
[2019-03-26 19:28:05,891] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167937: loss 0.0112
[2019-03-26 19:28:05,892] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167937: learning rate 0.0010
[2019-03-26 19:28:05,930] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167951: loss 0.0322
[2019-03-26 19:28:05,933] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167951: learning rate 0.0010
[2019-03-26 19:28:06,026] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167995: loss 0.0016
[2019-03-26 19:28:06,026] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 167995: loss 0.0216
[2019-03-26 19:28:06,029] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167995: learning rate 0.0010
[2019-03-26 19:28:06,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 167995: learning rate 0.0010
[2019-03-26 19:28:06,044] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 168000: loss 0.0047
[2019-03-26 19:28:06,046] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 168000: learning rate 0.0010
[2019-03-26 19:28:06,094] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 168025: loss 0.0001
[2019-03-26 19:28:06,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 168026: learning rate 0.0010
[2019-03-26 19:28:06,106] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 168029: loss 0.0015
[2019-03-26 19:28:06,109] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 168030: learning rate 0.0010
[2019-03-26 19:28:06,137] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168043: loss 0.0004
[2019-03-26 19:28:06,138] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168043: learning rate 0.0010
[2019-03-26 19:28:06,153] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168048: loss 0.0005
[2019-03-26 19:28:06,161] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168049: learning rate 0.0010
[2019-03-26 19:28:06,174] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168057: loss 0.0270
[2019-03-26 19:28:06,176] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168058: learning rate 0.0010
[2019-03-26 19:28:06,223] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168080: loss 0.0591
[2019-03-26 19:28:06,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168080: learning rate 0.0010
[2019-03-26 19:28:06,259] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168097: loss 0.1267
[2019-03-26 19:28:06,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168097: learning rate 0.0010
[2019-03-26 19:28:19,204] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.9867152e-25 1.4486272e-16 1.0544724e-28 1.0000000e+00 1.5437328e-24], sum to 1.0000
[2019-03-26 19:28:19,210] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5033
[2019-03-26 19:28:19,214] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.45, 61.5, 1.0, 2.0, 0.6841009981678501, 1.0, 2.0, 0.6841009981678501, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1912926.065718204, 1912926.065718204, 367383.3198422985], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6517800.0000, 
sim time next is 6518400.0000, 
raw observation next is [30.66666666666667, 60.0, 1.0, 2.0, 0.6957288522885704, 1.0, 2.0, 0.6957288522885704, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1945470.120498581, 1945470.12049858, 372291.6457996175], 
processed observation next is [1.0, 0.43478260869565216, 0.6524486571879939, 0.6, 1.0, 1.0, 0.6334082557693619, 1.0, 1.0, 0.6334082557693619, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5404083668051615, 0.5404083668051611, 0.55565917283525], 
reward next is 0.4443, 
noisyNet noise sample is [array([-1.8614235], dtype=float32), 1.3093638]. 
=============================================
[2019-03-26 19:28:20,415] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5945550e-35 1.9390558e-22 8.9365414e-35 1.0000000e+00 8.5377121e-30], sum to 1.0000
[2019-03-26 19:28:20,426] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9374
[2019-03-26 19:28:20,431] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 66.33333333333333, 1.0, 2.0, 0.2331297283397606, 1.0, 2.0, 0.2331297283397606, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 651508.7649585941, 651508.7649585934, 238263.6396805202], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6543600.0000, 
sim time next is 6544200.0000, 
raw observation next is [29.4, 66.66666666666667, 1.0, 2.0, 0.236540161707954, 1.0, 2.0, 0.236540161707954, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 661042.563570131, 661042.563570131, 238791.6470788582], 
processed observation next is [1.0, 0.7391304347826086, 0.5924170616113744, 0.6666666666666667, 1.0, 1.0, 0.08016886952765541, 1.0, 1.0, 0.08016886952765541, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18362293432503637, 0.18362293432503637, 0.3564054434012809], 
reward next is 0.6436, 
noisyNet noise sample is [array([-1.0108353], dtype=float32), -0.972025]. 
=============================================
[2019-03-26 19:28:20,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7605034e-33 3.7487676e-21 1.0267795e-32 1.0000000e+00 5.0318516e-29], sum to 1.0000
[2019-03-26 19:28:20,472] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6790
[2019-03-26 19:28:20,478] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.25, 63.0, 1.0, 2.0, 0.6867850091605372, 1.0, 2.0, 0.6867850091605372, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1920437.991776498, 1920437.991776498, 368509.9292383704], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6539400.0000, 
sim time next is 6540000.0000, 
raw observation next is [30.13333333333333, 63.66666666666666, 1.0, 2.0, 0.6787257984288904, 1.0, 2.0, 0.6787257984288904, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1897882.283084588, 1897882.283084588, 365141.6401631382], 
processed observation next is [1.0, 0.6956521739130435, 0.6271721958925749, 0.6366666666666666, 1.0, 1.0, 0.6129226487095064, 1.0, 1.0, 0.6129226487095064, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5271895230790522, 0.5271895230790522, 0.5449875226315496], 
reward next is 0.4550, 
noisyNet noise sample is [array([-0.9741903], dtype=float32), 0.8192436]. 
=============================================
[2019-03-26 19:28:20,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[47.623917]
 [47.570305]
 [47.477444]
 [47.326305]
 [47.25532 ]], R is [[47.66181183]
 [47.63518143]
 [47.61101151]
 [47.58419418]
 [47.5223732 ]].
[2019-03-26 19:28:21,642] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 19:28:21,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:28:21,646] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:28:21,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:28:21,647] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:28:21,648] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:28:21,647] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:28:21,649] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:28:21,650] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:28:21,651] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:28:21,654] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:28:21,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-03-26 19:28:21,688] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-03-26 19:28:21,691] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-03-26 19:28:21,710] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-03-26 19:28:21,744] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-03-26 19:28:59,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6310411]
[2019-03-26 19:28:59,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.15747238333334, 85.85190626333333, 1.0, 2.0, 0.2919673593916823, 1.0, 2.0, 0.2919673593916823, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 815995.9336286129, 815995.9336286129, 248832.6370882285]
[2019-03-26 19:28:59,235] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:28:59,237] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3408451e-31 8.1647756e-14 2.0179653e-33 1.0000000e+00 8.5366702e-30], sampled 0.08872311883490414
[2019-03-26 19:29:14,471] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6310411]
[2019-03-26 19:29:14,473] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.120674955, 69.36021142999999, 1.0, 2.0, 0.2597423111842038, 1.0, 2.0, 0.2597423111842038, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 725902.684601646, 725902.684601646, 243047.3502502787]
[2019-03-26 19:29:14,474] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:29:14,477] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8780763e-32 2.2488878e-16 9.4921334e-34 1.0000000e+00 8.3663841e-30], sampled 0.5155963868304527
[2019-03-26 19:29:20,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6310411]
[2019-03-26 19:29:20,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.27116354333333, 70.57738182333333, 1.0, 2.0, 0.283391166069218, 1.0, 2.0, 0.283391166069218, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 792018.2932107545, 792018.2932107545, 247235.2772943627]
[2019-03-26 19:29:20,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:29:20,908] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1353231e-33 3.2549570e-16 3.2351938e-35 1.0000000e+00 3.3857303e-31], sampled 0.12771718732138615
[2019-03-26 19:29:52,960] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6310411]
[2019-03-26 19:29:52,962] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.2683031263691343, 1.0, 2.0, 0.2683031263691343, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 749839.3691837883, 749839.3691837883, 244048.8613747112]
[2019-03-26 19:29:52,963] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:29:52,965] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.9132053e-30 1.9010423e-12 5.9701995e-32 1.0000000e+00 1.4116699e-28], sampled 0.8130347915022187
[2019-03-26 19:30:15,525] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-03-26 19:30:15,867] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-03-26 19:30:16,236] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-03-26 19:30:16,271] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-03-26 19:30:16,303] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-03-26 19:30:17,320] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 175000, evaluation results [175000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-03-26 19:30:18,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.25518625e-29 2.93677341e-17 1.16589662e-33 1.00000000e+00
 1.84056496e-31], sum to 1.0000
[2019-03-26 19:30:18,635] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8674
[2019-03-26 19:30:18,642] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.83333333333334, 92.33333333333334, 1.0, 2.0, 0.2845040190096817, 1.0, 2.0, 0.2845040190096817, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 795133.5492829222, 795133.5492829222, 246947.1438090159], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6582000.0000, 
sim time next is 6582600.0000, 
raw observation next is [25.8, 92.5, 1.0, 2.0, 0.2804705460691396, 1.0, 2.0, 0.2804705460691396, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 783856.6554054033, 783856.6554054033, 246209.4353303], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.925, 1.0, 1.0, 0.1330970434567947, 1.0, 1.0, 0.1330970434567947, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21773795983483427, 0.21773795983483427, 0.3674767691497015], 
reward next is 0.6325, 
noisyNet noise sample is [array([-1.0386052], dtype=float32), 1.3272171]. 
=============================================
[2019-03-26 19:30:19,156] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 175831: loss 0.0464
[2019-03-26 19:30:19,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 175832: learning rate 0.0010
[2019-03-26 19:30:19,191] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 175847: loss 0.0825
[2019-03-26 19:30:19,194] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 175847: learning rate 0.0010
[2019-03-26 19:30:19,233] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175862: loss 0.0942
[2019-03-26 19:30:19,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175862: learning rate 0.0010
[2019-03-26 19:30:19,301] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175895: loss 0.0933
[2019-03-26 19:30:19,302] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175895: learning rate 0.0010
[2019-03-26 19:30:19,314] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175900: loss 0.1042
[2019-03-26 19:30:19,316] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175901: learning rate 0.0010
[2019-03-26 19:30:19,439] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4669643e-30 1.5753273e-13 4.1247678e-33 1.0000000e+00 7.5201975e-32], sum to 1.0000
[2019-03-26 19:30:19,445] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9061
[2019-03-26 19:30:19,451] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.86666666666667, 86.0, 1.0, 2.0, 0.3169000010854329, 1.0, 2.0, 0.3169000010854329, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 885711.4222818569, 885711.4222818569, 253219.1102847212], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6594000.0000, 
sim time next is 6594600.0000, 
raw observation next is [26.93333333333333, 85.5, 1.0, 2.0, 0.313487516244504, 1.0, 2.0, 0.313487516244504, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 876169.8903749448, 876169.8903749448, 252529.4525564268], 
processed observation next is [1.0, 0.30434782608695654, 0.4755134281200631, 0.855, 1.0, 1.0, 0.17287652559578795, 1.0, 1.0, 0.17287652559578795, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.24338052510415134, 0.24338052510415134, 0.37690963068123406], 
reward next is 0.6231, 
noisyNet noise sample is [array([-1.0937139], dtype=float32), -0.7166369]. 
=============================================
[2019-03-26 19:30:19,454] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175961: loss 0.1072
[2019-03-26 19:30:19,459] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175963: learning rate 0.0010
[2019-03-26 19:30:19,538] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 176004: loss 0.0471
[2019-03-26 19:30:19,539] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176004: loss 0.0425
[2019-03-26 19:30:19,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 176004: learning rate 0.0010
[2019-03-26 19:30:19,539] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 176004: loss 0.0382
[2019-03-26 19:30:19,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176006: learning rate 0.0010
[2019-03-26 19:30:19,552] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 176008: learning rate 0.0010
[2019-03-26 19:30:19,580] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176021: loss 0.0446
[2019-03-26 19:30:19,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176022: learning rate 0.0010
[2019-03-26 19:30:19,611] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176032: loss 0.0148
[2019-03-26 19:30:19,613] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176033: learning rate 0.0010
[2019-03-26 19:30:19,682] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 176067: loss 0.0154
[2019-03-26 19:30:19,684] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 176068: loss 0.0214
[2019-03-26 19:30:19,684] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 176067: learning rate 0.0010
[2019-03-26 19:30:19,686] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 176068: learning rate 0.0010
[2019-03-26 19:30:19,723] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 176085: loss 0.0105
[2019-03-26 19:30:19,726] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 176087: learning rate 0.0010
[2019-03-26 19:30:19,742] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176093: loss 0.0057
[2019-03-26 19:30:19,746] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176093: learning rate 0.0010
[2019-03-26 19:30:19,785] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176110: loss 0.0056
[2019-03-26 19:30:19,789] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176112: learning rate 0.0010
[2019-03-26 19:30:22,045] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.4160392e-19 5.6761146e-05 2.0442742e-22 9.9994326e-01 3.9047429e-20], sum to 1.0000
[2019-03-26 19:30:22,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9087
[2019-03-26 19:30:22,061] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.56666666666667, 87.0, 1.0, 2.0, 0.255115246257564, 1.0, 2.0, 0.255115246257564, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 712970.3304632597, 712970.3304632597, 241791.4717243263], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6646200.0000, 
sim time next is 6646800.0000, 
raw observation next is [26.53333333333334, 87.0, 1.0, 2.0, 0.2544136828447343, 1.0, 2.0, 0.2544136828447343, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 711009.022033546, 711009.0220335466, 241674.3570498307], 
processed observation next is [1.0, 0.9565217391304348, 0.4565560821484995, 0.87, 1.0, 1.0, 0.10170323234305338, 1.0, 1.0, 0.10170323234305338, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19750250612042944, 0.1975025061204296, 0.36070799559676225], 
reward next is 0.6393, 
noisyNet noise sample is [array([-0.9300779], dtype=float32), -0.36714384]. 
=============================================
[2019-03-26 19:30:27,934] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.7117984e-22 1.5874740e-09 2.8100030e-27 1.0000000e+00 2.9536589e-25], sum to 1.0000
[2019-03-26 19:30:27,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9744
[2019-03-26 19:30:27,950] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.85, 81.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 512359.2288780213, 512359.2288780213, 236014.4606189807], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6744600.0000, 
sim time next is 6745200.0000, 
raw observation next is [22.7, 81.66666666666666, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 511210.2036776796, 511210.2036776796, 235766.6277257703], 
processed observation next is [1.0, 0.043478260869565216, 0.27488151658767773, 0.8166666666666665, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14200283435491098, 0.14200283435491098, 0.35189048914294074], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8076447], dtype=float32), -1.6314409]. 
=============================================
[2019-03-26 19:30:28,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.0843136e-18 9.9998987e-01 4.2091453e-25 1.0168495e-05 3.0612685e-22], sum to 1.0000
[2019-03-26 19:30:28,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0567
[2019-03-26 19:30:28,850] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.53333333333333, 81.66666666666667, 1.0, 2.0, 0.3487074423974252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551949.6496468761, 551949.6496468768, 171103.700153481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6762000.0000, 
sim time next is 6762600.0000, 
raw observation next is [22.65, 81.0, 1.0, 2.0, 0.3434887855873461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543394.8532013342, 543394.8532013348, 170404.7218079845], 
processed observation next is [1.0, 0.2608695652173913, 0.2725118483412322, 0.81, 1.0, 1.0, 0.20902263323776635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1509430147781484, 0.15094301477814856, 0.25433540568355895], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.9916789], dtype=float32), 0.6161302]. 
=============================================
[2019-03-26 19:30:36,815] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 183770: loss 0.3063
[2019-03-26 19:30:36,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 183770: learning rate 0.0010
[2019-03-26 19:30:36,968] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183840: loss 0.0685
[2019-03-26 19:30:36,969] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183840: learning rate 0.0010
[2019-03-26 19:30:37,048] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183874: loss 0.0021
[2019-03-26 19:30:37,048] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183874: learning rate 0.0010
[2019-03-26 19:30:37,069] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183883: loss 0.0012
[2019-03-26 19:30:37,072] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183884: learning rate 0.0010
[2019-03-26 19:30:37,139] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 183917: loss 0.0168
[2019-03-26 19:30:37,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 183917: learning rate 0.0010
[2019-03-26 19:30:37,179] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183933: loss 0.0478
[2019-03-26 19:30:37,180] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183934: learning rate 0.0010
[2019-03-26 19:30:37,306] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 183992: loss 0.0187
[2019-03-26 19:30:37,311] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 183994: learning rate 0.0010
[2019-03-26 19:30:37,322] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183999: loss 0.0026
[2019-03-26 19:30:37,324] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 184000: learning rate 0.0010
[2019-03-26 19:30:37,397] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184033: loss 0.0094
[2019-03-26 19:30:37,400] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184033: learning rate 0.0010
[2019-03-26 19:30:37,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 184040: loss 0.0535
[2019-03-26 19:30:37,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 184040: learning rate 0.0010
[2019-03-26 19:30:37,441] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 184051: loss 0.0529
[2019-03-26 19:30:37,441] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184051: loss 0.0721
[2019-03-26 19:30:37,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 184051: learning rate 0.0010
[2019-03-26 19:30:37,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184051: learning rate 0.0010
[2019-03-26 19:30:37,458] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 184058: loss 0.0698
[2019-03-26 19:30:37,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 184058: learning rate 0.0010
[2019-03-26 19:30:37,525] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184088: loss 0.0792
[2019-03-26 19:30:37,527] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184088: learning rate 0.0010
[2019-03-26 19:30:37,556] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2233663e-38 1.0000000e+00 0.0000000e+00 7.6171867e-11 0.0000000e+00], sum to 1.0000
[2019-03-26 19:30:37,566] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7582
[2019-03-26 19:30:37,567] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184105: loss 0.0729
[2019-03-26 19:30:37,569] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184105: learning rate 0.0010
[2019-03-26 19:30:37,572] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 79.0, 1.0, 2.0, 0.4148880629131098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 609709.3112719102, 609709.3112719102, 175176.3442842608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6908400.0000, 
sim time next is 6909000.0000, 
raw observation next is [25.45, 79.33333333333334, 1.0, 2.0, 0.415919574448055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610821.5112986965, 610821.5112986965, 175269.8816157605], 
processed observation next is [0.0, 1.0, 0.4052132701421801, 0.7933333333333334, 1.0, 1.0, 0.29628864391331927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1696726420274157, 0.1696726420274157, 0.26159683823247837], 
reward next is 0.7384, 
noisyNet noise sample is [array([0.46656385], dtype=float32), 0.5874675]. 
=============================================
[2019-03-26 19:30:37,588] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[88.469536]
 [88.383354]
 [88.32556 ]
 [88.26151 ]
 [88.20736 ]], R is [[88.38754272]
 [88.24221039]
 [88.09861755]
 [87.9567337 ]
 [87.81652069]].
[2019-03-26 19:30:37,674] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184151: loss 0.0005
[2019-03-26 19:30:37,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184152: learning rate 0.0010
[2019-03-26 19:30:43,667] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0622861e-24 1.0000000e+00 4.5538087e-31 3.2077583e-09 9.1487700e-22], sum to 1.0000
[2019-03-26 19:30:43,676] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1517
[2019-03-26 19:30:43,685] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 83.0, 1.0, 2.0, 0.4602755579210497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653117.9531069922, 653117.9531069922, 178890.5725801206], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7005600.0000, 
sim time next is 7006200.0000, 
raw observation next is [25.75, 83.16666666666667, 1.0, 2.0, 0.9417134402916928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336893.568229379, 1336893.568229379, 284793.4860074467], 
processed observation next is [1.0, 0.08695652173913043, 0.41943127962085314, 0.8316666666666667, 1.0, 1.0, 0.9297752292670998, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3713593245081608, 0.3713593245081608, 0.4250649044887264], 
reward next is 0.5749, 
noisyNet noise sample is [array([-0.02497675], dtype=float32), 0.24212247]. 
=============================================
[2019-03-26 19:30:47,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5995758e-29 1.5483195e-11 3.0297506e-34 1.0000000e+00 1.3283257e-32], sum to 1.0000
[2019-03-26 19:30:47,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8039
[2019-03-26 19:30:47,357] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.9, 86.0, 1.0, 2.0, 0.2422277546278819, 1.0, 2.0, 0.2422277546278819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 676942.3048893916, 676942.3048893916, 239685.5808449154], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7074000.0000, 
sim time next is 7074600.0000, 
raw observation next is [25.85, 86.16666666666667, 1.0, 2.0, 0.242796393825376, 1.0, 2.0, 0.242796393825376, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 678531.9562551177, 678531.956255117, 239776.0544627855], 
processed observation next is [1.0, 0.9130434782608695, 0.4241706161137442, 0.8616666666666667, 1.0, 1.0, 0.08770649858479034, 1.0, 1.0, 0.08770649858479034, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1884810989597549, 0.1884810989597547, 0.3578747081534112], 
reward next is 0.6421, 
noisyNet noise sample is [array([1.2393641], dtype=float32), -0.050113715]. 
=============================================
[2019-03-26 19:30:54,695] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 191809: loss 0.0791
[2019-03-26 19:30:54,697] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 191809: learning rate 0.0010
[2019-03-26 19:30:54,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 191815: loss 0.0247
[2019-03-26 19:30:54,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 191816: learning rate 0.0010
[2019-03-26 19:30:54,889] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191893: loss 0.0738
[2019-03-26 19:30:54,892] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191893: learning rate 0.0010
[2019-03-26 19:30:55,001] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191943: loss 0.0458
[2019-03-26 19:30:55,002] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191943: loss 0.0339
[2019-03-26 19:30:55,002] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191943: learning rate 0.0010
[2019-03-26 19:30:55,003] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191943: learning rate 0.0010
[2019-03-26 19:30:55,053] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191965: loss 0.0430
[2019-03-26 19:30:55,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191967: learning rate 0.0010
[2019-03-26 19:30:55,078] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 191975: loss 0.0115
[2019-03-26 19:30:55,082] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 191976: learning rate 0.0010
[2019-03-26 19:30:55,085] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 191979: loss 0.0038
[2019-03-26 19:30:55,086] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 191979: learning rate 0.0010
[2019-03-26 19:30:55,112] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191989: loss 0.0787
[2019-03-26 19:30:55,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191989: learning rate 0.0010
[2019-03-26 19:30:55,172] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 192014: loss 0.0022
[2019-03-26 19:30:55,174] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 192014: learning rate 0.0010
[2019-03-26 19:30:55,226] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 192041: loss 0.0062
[2019-03-26 19:30:55,228] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 192041: learning rate 0.0010
[2019-03-26 19:30:55,246] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192049: loss 0.0033
[2019-03-26 19:30:55,250] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192050: learning rate 0.0010
[2019-03-26 19:30:55,295] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192065: loss 0.0197
[2019-03-26 19:30:55,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192066: learning rate 0.0010
[2019-03-26 19:30:55,307] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 192072: loss 0.0435
[2019-03-26 19:30:55,310] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 192072: learning rate 0.0010
[2019-03-26 19:30:55,370] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192099: loss 0.0507
[2019-03-26 19:30:55,374] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192099: learning rate 0.0010
[2019-03-26 19:30:55,486] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192153: loss 0.0570
[2019-03-26 19:30:55,491] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192154: learning rate 0.0010
[2019-03-26 19:30:56,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4531622e-26 4.8076716e-12 6.8111244e-28 1.0000000e+00 2.0000816e-22], sum to 1.0000
[2019-03-26 19:30:56,012] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2001
[2019-03-26 19:30:56,016] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.16666666666667, 77.0, 1.0, 2.0, 0.6676427784094223, 1.0, 2.0, 0.6676427784094223, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1866864.466395944, 1866864.466395944, 360581.0374375751], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7208400.0000, 
sim time next is 7209000.0000, 
raw observation next is [29.25, 76.0, 1.0, 2.0, 0.6519488778183834, 1.0, 2.0, 0.6519488778183834, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1822943.810494138, 1822943.810494138, 354232.7952051161], 
processed observation next is [1.0, 0.43478260869565216, 0.5853080568720379, 0.76, 1.0, 1.0, 0.5806612985763655, 1.0, 1.0, 0.5806612985763655, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5063732806928161, 0.5063732806928161, 0.5287056644852479], 
reward next is 0.4713, 
noisyNet noise sample is [array([-0.882269], dtype=float32), -1.2321088]. 
=============================================
[2019-03-26 19:30:56,036] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[44.85707 ]
 [44.88277 ]
 [44.86564 ]
 [44.975517]
 [44.91833 ]], R is [[44.89300919]
 [44.90589905]
 [44.92618561]
 [44.92724228]
 [44.95965195]].
[2019-03-26 19:30:59,615] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9364956e-24 9.9999905e-01 2.1560125e-29 9.0660069e-07 6.5276386e-28], sum to 1.0000
[2019-03-26 19:30:59,621] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1240
[2019-03-26 19:30:59,626] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.68333333333333, 89.16666666666667, 1.0, 2.0, 0.3632090556018588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573143.7943653135, 573143.7943653135, 172857.3420937126], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7269000.0000, 
sim time next is 7269600.0000, 
raw observation next is [21.66666666666667, 89.33333333333334, 1.0, 2.0, 0.3346299029941324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527978.7966770342, 527978.7966770342, 169154.4137763013], 
processed observation next is [1.0, 0.13043478260869565, 0.22590837282780438, 0.8933333333333334, 1.0, 1.0, 0.1983492807158222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14666077685473172, 0.14666077685473172, 0.25246927429298704], 
reward next is 0.7475, 
noisyNet noise sample is [array([0.4842193], dtype=float32), -0.47613963]. 
=============================================
[2019-03-26 19:31:09,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.70211631e-31 1.00000000e+00 1.19129680e-35 1.25217625e-17
 2.96778341e-28], sum to 1.0000
[2019-03-26 19:31:09,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6419
[2019-03-26 19:31:09,526] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.21666666666667, 95.0, 1.0, 2.0, 0.3230113846010633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506686.1879506215, 506686.1879506215, 167439.300956639], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7449000.0000, 
sim time next is 7449600.0000, 
raw observation next is [21.23333333333333, 95.0, 1.0, 2.0, 0.3239423236257969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507934.9275484583, 507934.9275484583, 167529.4040636521], 
processed observation next is [0.0, 0.21739130434782608, 0.2053712480252764, 0.95, 1.0, 1.0, 0.18547267906722517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1410930354301273, 0.1410930354301273, 0.2500438866621673], 
reward next is 0.7500, 
noisyNet noise sample is [array([-0.48867962], dtype=float32), 0.82820123]. 
=============================================
[2019-03-26 19:31:09,731] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.2127525e-27 1.0000000e+00 1.4684489e-34 1.2868316e-13 3.8943778e-25], sum to 1.0000
[2019-03-26 19:31:09,740] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5852
[2019-03-26 19:31:09,745] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.26666666666667, 94.33333333333334, 1.0, 2.0, 0.3224694210718491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506162.5699249672, 506162.5699249672, 167407.5349259211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7446000.0000, 
sim time next is 7446600.0000, 
raw observation next is [21.25, 94.5, 1.0, 2.0, 0.3226904336105356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506466.2105589872, 506466.2105589872, 167429.6081203992], 
processed observation next is [0.0, 0.17391304347826086, 0.20616113744075834, 0.945, 1.0, 1.0, 0.1839643778440188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14068505848860757, 0.14068505848860757, 0.24989493749313316], 
reward next is 0.7501, 
noisyNet noise sample is [array([-0.3754264], dtype=float32), -0.8151154]. 
=============================================
[2019-03-26 19:31:10,629] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [6.0398905e-37 1.0000000e+00 0.0000000e+00 8.9410515e-18 3.2929135e-37], sum to 1.0000
[2019-03-26 19:31:10,643] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6735
[2019-03-26 19:31:10,654] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.33333333333334, 1.0, 2.0, 0.3411922987100115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528292.002205022, 528292.002205022, 168945.619375232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7460400.0000, 
sim time next is 7461000.0000, 
raw observation next is [21.9, 94.0, 1.0, 2.0, 0.343530330741215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531024.3913743892, 531024.3913743892, 169139.5427582343], 
processed observation next is [0.0, 0.34782608695652173, 0.23696682464454974, 0.94, 1.0, 1.0, 0.20907268764001805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14750677538177476, 0.14750677538177476, 0.2524470787436333], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.961258], dtype=float32), -0.052774042]. 
=============================================
[2019-03-26 19:31:10,685] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.82266 ]
 [83.79933 ]
 [83.769585]
 [83.71109 ]
 [83.675354]], R is [[83.75525665]
 [83.6655426 ]
 [83.57714844]
 [83.48993683]
 [83.40362549]].
[2019-03-26 19:31:12,615] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 199819: loss 0.0994
[2019-03-26 19:31:12,616] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 199819: learning rate 0.0010
[2019-03-26 19:31:12,623] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 199822: loss 0.0565
[2019-03-26 19:31:12,625] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 199822: learning rate 0.0010
[2019-03-26 19:31:12,718] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199863: loss 0.0090
[2019-03-26 19:31:12,721] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199863: learning rate 0.0010
[2019-03-26 19:31:12,774] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199891: loss 0.0011
[2019-03-26 19:31:12,779] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199891: learning rate 0.0010
[2019-03-26 19:31:12,844] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199923: loss 0.0201
[2019-03-26 19:31:12,847] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199923: learning rate 0.0010
[2019-03-26 19:31:12,879] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199937: loss 0.0522
[2019-03-26 19:31:12,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199937: learning rate 0.0010
[2019-03-26 19:31:12,966] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 199980: loss 0.0841
[2019-03-26 19:31:12,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 199981: learning rate 0.0010
[2019-03-26 19:31:12,982] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199985: loss 0.0986
[2019-03-26 19:31:12,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199986: learning rate 0.0010
[2019-03-26 19:31:13,003] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199997: loss 0.0839
[2019-03-26 19:31:13,012] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 19:31:13,013] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199999: learning rate 0.0010
[2019-03-26 19:31:13,015] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:31:13,016] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:31:13,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:31:13,017] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:31:13,018] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:31:13,020] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:31:13,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:31:13,021] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:31:13,024] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:31:13,025] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:31:13,039] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-03-26 19:31:13,039] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-03-26 19:31:13,074] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-03-26 19:31:13,076] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-03-26 19:31:13,092] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-03-26 19:31:21,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6070264]
[2019-03-26 19:31:21,496] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.16666666666666, 69.33333333333333, 1.0, 2.0, 0.2333689015814524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 387123.1741125777, 387123.1741125777, 159109.8274958585]
[2019-03-26 19:31:21,499] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:31:21,501] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.5784850e-32 1.0000000e+00 1.0886047e-36 1.4688776e-15 1.5132257e-34], sampled 0.9976152573368635
[2019-03-26 19:32:44,941] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6070264]
[2019-03-26 19:32:44,942] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.985106985, 78.99269397500001, 1.0, 2.0, 0.5118028200446193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715170.4750763668, 715170.4750763674, 185466.3905432429]
[2019-03-26 19:32:44,944] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:32:44,946] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6922775e-34 1.0000000e+00 0.0000000e+00 1.0305589e-16 1.9656883e-37], sampled 0.8201178360610929
[2019-03-26 19:32:45,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6070264]
[2019-03-26 19:32:45,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.90744029, 74.21294503499999, 1.0, 2.0, 0.9556587161395896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129125802556, 1335785.024948088, 1335785.024948088, 285708.3225785705]
[2019-03-26 19:32:45,783] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:32:45,786] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7639795e-34 1.0000000e+00 0.0000000e+00 1.5056117e-14 1.6893990e-36], sampled 0.04252585576543089
[2019-03-26 19:33:07,150] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1213 3007729742.9886 1766.0000
[2019-03-26 19:33:07,274] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0653 2927317329.7462 1338.0000
[2019-03-26 19:33:07,353] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.8889 2779256332.3854 933.0000
[2019-03-26 19:33:07,367] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9993 3164535723.4518 1776.0000
[2019-03-26 19:33:07,464] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-03-26 19:33:08,482] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 200000, evaluation results [200000.0, 7881.99929362893, 3164535723.451782, 1776.0, 8255.065342017213, 2927317329.746172, 1338.0, 8659.888853383243, 2779256332.3854055, 933.0, 7998.121261362489, 3007729742.9886193, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-03-26 19:33:08,509] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200013: loss 0.0670
[2019-03-26 19:33:08,513] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200013: learning rate 0.0010
[2019-03-26 19:33:08,599] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 200061: loss 0.0047
[2019-03-26 19:33:08,602] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 200062: learning rate 0.0010
[2019-03-26 19:33:08,624] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200073: loss 0.0042
[2019-03-26 19:33:08,628] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200073: learning rate 0.0010
[2019-03-26 19:33:08,633] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200075: loss 0.0025
[2019-03-26 19:33:08,637] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200075: learning rate 0.0010
[2019-03-26 19:33:08,685] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200089: loss 0.0285
[2019-03-26 19:33:08,688] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200091: learning rate 0.0010
[2019-03-26 19:33:08,709] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200100: loss 0.0494
[2019-03-26 19:33:08,712] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200103: learning rate 0.0010
[2019-03-26 19:33:08,830] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200159: loss 0.0160
[2019-03-26 19:33:08,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200159: learning rate 0.0010
[2019-03-26 19:33:09,012] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9040939e-31 1.0000000e+00 6.6129910e-34 2.6993279e-17 2.8875981e-37], sum to 1.0000
[2019-03-26 19:33:09,021] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0901
[2019-03-26 19:33:09,026] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.66666666666667, 1.0, 2.0, 0.4049397626363604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596156.6687155137, 596156.6687155131, 173942.5359921775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7510800.0000, 
sim time next is 7511400.0000, 
raw observation next is [23.75, 91.0, 1.0, 2.0, 0.4047130699651731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595962.373732491, 595962.373732491, 173928.9426337174], 
processed observation next is [0.0, 0.9565217391304348, 0.3246445497630332, 0.91, 1.0, 1.0, 0.2827868312833411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16554510381458085, 0.16554510381458085, 0.2595954367667424], 
reward next is 0.7404, 
noisyNet noise sample is [array([1.297284], dtype=float32), -1.0079561]. 
=============================================
[2019-03-26 19:33:14,735] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.2243197e-29 1.0000000e+00 4.3403006e-33 7.6819341e-12 9.8850925e-36], sum to 1.0000
[2019-03-26 19:33:14,744] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7646
[2019-03-26 19:33:14,749] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.86666666666667, 95.0, 1.0, 2.0, 0.6949796039538882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 998002.8077480583, 998002.807748059, 223325.3828361238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7611000.0000, 
sim time next is 7611600.0000, 
raw observation next is [23.83333333333333, 95.0, 1.0, 2.0, 0.6089114984933186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875500.9863100351, 875500.9863100344, 205584.2024865552], 
processed observation next is [1.0, 0.08695652173913043, 0.32859399684044216, 0.95, 1.0, 1.0, 0.5288090343292995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2431947184194542, 0.243194718419454, 0.30684209326351525], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.18061566], dtype=float32), -0.0017080868]. 
=============================================
[2019-03-26 19:33:14,994] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0726451e-26 1.0000000e+00 5.5204476e-28 4.2921149e-11 4.6086737e-35], sum to 1.0000
[2019-03-26 19:33:14,995] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8326
[2019-03-26 19:33:15,002] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 93.5, 1.0, 2.0, 0.4621301086147338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652642.8195107767, 652642.8195107767, 178766.3776916693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7605000.0000, 
sim time next is 7605600.0000, 
raw observation next is [24.4, 93.66666666666667, 1.0, 2.0, 0.460480244997354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651126.4823016808, 651126.4823016814, 178628.5229309812], 
processed observation next is [1.0, 0.0, 0.3554502369668246, 0.9366666666666668, 1.0, 1.0, 0.3499761987919928, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18086846730602246, 0.18086846730602263, 0.26660973571788243], 
reward next is 0.7334, 
noisyNet noise sample is [array([-1.0087589], dtype=float32), 1.3123673]. 
=============================================
[2019-03-26 19:33:18,197] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3247799e-19 6.6169418e-02 1.9720036e-20 9.3383062e-01 6.2449469e-14], sum to 1.0000
[2019-03-26 19:33:18,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9719
[2019-03-26 19:33:18,208] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.53333333333333, 80.66666666666667, 1.0, 2.0, 0.2551537666702446, 1.0, 2.0, 0.2551537666702446, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 713078.0192075577, 713078.0192075577, 241798.080558972], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7669200.0000, 
sim time next is 7669800.0000, 
raw observation next is [27.4, 81.5, 1.0, 2.0, 0.2547440593994971, 1.0, 2.0, 0.2547440593994971, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 711932.6306433236, 711932.630643323, 241729.7493410959], 
processed observation next is [1.0, 0.782608695652174, 0.4976303317535545, 0.815, 1.0, 1.0, 0.10210127638493628, 1.0, 1.0, 0.10210127638493628, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1977590640675899, 0.1977590640675897, 0.36079067065835213], 
reward next is 0.6392, 
noisyNet noise sample is [array([-0.07201457], dtype=float32), -1.1450998]. 
=============================================
[2019-03-26 19:33:22,956] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1587277e-27 1.3383657e-10 1.3286078e-27 1.0000000e+00 3.2704235e-26], sum to 1.0000
[2019-03-26 19:33:22,964] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0313
[2019-03-26 19:33:22,970] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.48333333333333, 60.16666666666666, 1.0, 2.0, 0.8243079078867558, 1.0, 2.0, 0.8243079078867558, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2305389.03070673, 2305389.03070673, 431871.6592440599], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7744200.0000, 
sim time next is 7744800.0000, 
raw observation next is [31.36666666666667, 60.33333333333334, 1.0, 2.0, 0.8571395145569499, 1.0, 2.0, 0.8571395145569499, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2397299.134791138, 2397299.134791139, 448639.2770827227], 
processed observation next is [1.0, 0.6521739130434783, 0.6856240126382308, 0.6033333333333334, 1.0, 1.0, 0.8278789332011445, 1.0, 1.0, 0.8278789332011445, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6659164263308717, 0.665916426330872, 0.6696108613174966], 
reward next is 0.3304, 
noisyNet noise sample is [array([0.08703613], dtype=float32), -3.2870307]. 
=============================================
[2019-03-26 19:33:25,847] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 207830: loss 0.1663
[2019-03-26 19:33:25,849] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 207830: learning rate 0.0010
[2019-03-26 19:33:25,850] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207830: loss 0.0026
[2019-03-26 19:33:25,854] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207831: learning rate 0.0010
[2019-03-26 19:33:25,964] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207879: loss 0.0899
[2019-03-26 19:33:25,965] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207879: learning rate 0.0010
[2019-03-26 19:33:26,086] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207938: loss 0.0153
[2019-03-26 19:33:26,088] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207938: loss 0.0149
[2019-03-26 19:33:26,090] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207938: learning rate 0.0010
[2019-03-26 19:33:26,090] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207938: learning rate 0.0010
[2019-03-26 19:33:26,115] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207947: loss 0.0330
[2019-03-26 19:33:26,117] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207947: learning rate 0.0010
[2019-03-26 19:33:26,197] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207983: loss 0.0035
[2019-03-26 19:33:26,202] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207984: learning rate 0.0010
[2019-03-26 19:33:26,237] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208000: loss 0.0097
[2019-03-26 19:33:26,239] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208000: loss 0.0076
[2019-03-26 19:33:26,241] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208000: learning rate 0.0010
[2019-03-26 19:33:26,241] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208000: learning rate 0.0010
[2019-03-26 19:33:26,251] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 208008: loss 0.0333
[2019-03-26 19:33:26,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 208008: learning rate 0.0010
[2019-03-26 19:33:26,260] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208011: loss 0.0883
[2019-03-26 19:33:26,261] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208011: learning rate 0.0010
[2019-03-26 19:33:26,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 208030: loss 0.0344
[2019-03-26 19:33:26,309] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 208030: learning rate 0.0010
[2019-03-26 19:33:26,374] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208060: loss 0.0624
[2019-03-26 19:33:26,379] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208061: learning rate 0.0010
[2019-03-26 19:33:26,488] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 208109: loss 0.0085
[2019-03-26 19:33:26,492] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 208110: learning rate 0.0010
[2019-03-26 19:33:26,505] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208116: loss 0.0035
[2019-03-26 19:33:26,510] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208119: learning rate 0.0010
[2019-03-26 19:33:26,530] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208130: loss 0.1631
[2019-03-26 19:33:26,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208130: learning rate 0.0010
[2019-03-26 19:33:26,646] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.9849975e-21 5.5106683e-04 3.7876663e-24 9.9944896e-01 2.1785044e-23], sum to 1.0000
[2019-03-26 19:33:26,653] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5108
[2019-03-26 19:33:26,657] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.9, 81.0, 1.0, 2.0, 0.5334225033405502, 1.0, 2.0, 0.5334225033405502, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1491296.24551342, 1491296.24551342, 310960.3327758114], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7806000.0000, 
sim time next is 7806600.0000, 
raw observation next is [28.0, 80.5, 1.0, 2.0, 0.6069257916430862, 1.0, 2.0, 0.6069257916430862, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1696953.035472877, 1696953.035472877, 336821.5385770213], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.805, 1.0, 1.0, 0.5264166164374533, 1.0, 1.0, 0.5264166164374533, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4713758431869103, 0.4713758431869103, 0.5027187142940616], 
reward next is 0.4973, 
noisyNet noise sample is [array([-0.19543442], dtype=float32), -0.8777913]. 
=============================================
[2019-03-26 19:33:28,903] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.740729e-33 2.807062e-16 7.341943e-36 1.000000e+00 2.778704e-29], sum to 1.0000
[2019-03-26 19:33:28,910] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3593
[2019-03-26 19:33:28,914] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.45, 73.0, 1.0, 2.0, 0.2411730988927984, 1.0, 2.0, 0.2411730988927984, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 673993.9830174411, 673993.9830174411, 239527.9214545118], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7839000.0000, 
sim time next is 7839600.0000, 
raw observation next is [29.3, 74.0, 1.0, 2.0, 0.2465408348413212, 1.0, 2.0, 0.2465408348413212, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 688999.7348681274, 688999.7348681267, 240387.8900386591], 
processed observation next is [1.0, 0.7391304347826086, 0.5876777251184835, 0.74, 1.0, 1.0, 0.09221787330279661, 1.0, 1.0, 0.09221787330279661, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1913888152411465, 0.19138881524114632, 0.3587878955800882], 
reward next is 0.6412, 
noisyNet noise sample is [array([-0.6482472], dtype=float32), -1.3122107]. 
=============================================
[2019-03-26 19:33:35,780] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:35,780] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:35,781] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-03-26 19:33:35,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:35,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:35,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-03-26 19:33:35,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:35,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:35,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-03-26 19:33:35,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:35,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:35,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-03-26 19:33:35,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:35,982] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:35,983] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-03-26 19:33:36,029] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,029] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-03-26 19:33:36,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,051] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,052] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-03-26 19:33:36,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,069] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-03-26 19:33:36,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-03-26 19:33:36,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-03-26 19:33:36,137] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,152] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-03-26 19:33:36,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-03-26 19:33:36,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-03-26 19:33:36,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:33:36,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:33:36,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-03-26 19:33:36,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-03-26 19:33:36,346] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-03-26 19:33:42,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.279358e-32 4.380605e-10 9.868869e-33 1.000000e+00 3.604755e-29], sum to 1.0000
[2019-03-26 19:33:42,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1059
[2019-03-26 19:33:42,381] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.76666666666667, 82.33333333333334, 1.0, 2.0, 0.1854982270350368, 1.0, 2.0, 0.1854982270350368, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 562281.2726796047, 562281.2726796041, 239130.6161180436], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 71400.0000, 
sim time next is 72000.0000, 
raw observation next is [23.6, 83.0, 1.0, 2.0, 0.1844401989284413, 1.0, 2.0, 0.1844401989284413, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 560319.4096436611, 560319.4096436611, 239176.7165592999], 
processed observation next is [1.0, 0.8695652173913043, 0.3175355450236968, 0.83, 1.0, 1.0, 0.017397830034266606, 1.0, 1.0, 0.017397830034266606, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15564428045657253, 0.15564428045657253, 0.35698017396910436], 
reward next is 0.6430, 
noisyNet noise sample is [array([0.69261676], dtype=float32), -1.5223521]. 
=============================================
[2019-03-26 19:33:42,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[61.197083]
 [61.264755]
 [61.317352]
 [61.334343]
 [61.377705]], R is [[61.17448425]
 [61.20582962]
 [61.23691177]
 [61.26767349]
 [61.29813766]].
[2019-03-26 19:33:46,152] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.5635518e-29 1.0000000e+00 7.6110159e-33 5.8137761e-09 7.0807539e-26], sum to 1.0000
[2019-03-26 19:33:46,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4827
[2019-03-26 19:33:46,164] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 96.0, 1.0, 2.0, 0.7684594695932436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1153431.100435433, 1153431.100435433, 246902.4024887146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 145200.0000, 
sim time next is 145800.0000, 
raw observation next is [22.55, 96.0, 1.0, 2.0, 0.7553360354431568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1134393.075448903, 1134393.075448903, 243666.2669851979], 
processed observation next is [1.0, 0.6956521739130435, 0.26777251184834133, 0.96, 1.0, 1.0, 0.7052241390881406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3151091876246953, 0.3151091876246953, 0.36368099550029537], 
reward next is 0.6363, 
noisyNet noise sample is [array([-1.7194943], dtype=float32), 0.2968115]. 
=============================================
[2019-03-26 19:33:48,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9006118e-26 1.0000000e+00 2.4482533e-29 6.9612510e-11 3.8676247e-33], sum to 1.0000
[2019-03-26 19:33:48,098] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7536
[2019-03-26 19:33:48,107] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2872358174508378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462614.425810521, 462614.4258105204, 164451.5098731454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 183600.0000, 
sim time next is 184200.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2869226459956143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462157.2970068382, 462157.2970068382, 164420.2911017553], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.1408706578260413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12837702694634395, 0.12837702694634395, 0.24540341955485867], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.5717278], dtype=float32), -0.8593364]. 
=============================================
[2019-03-26 19:33:53,384] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.3952954e-33 1.0000000e+00 0.0000000e+00 2.0259391e-12 0.0000000e+00], sum to 1.0000
[2019-03-26 19:33:53,394] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6623
[2019-03-26 19:33:53,399] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2855877156481978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458898.6671830605, 458898.6671830605, 164194.8710287105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 259200.0000, 
sim time next is 259800.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2860133779352211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459525.246932158, 459525.246932158, 164237.2160093909], 
processed observation next is [0.0, 0.0, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13977515413882058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12764590192559944, 0.12764590192559944, 0.24513017314834462], 
reward next is 0.7549, 
noisyNet noise sample is [array([-0.25824484], dtype=float32), -0.35549313]. 
=============================================
[2019-03-26 19:33:57,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7437765e-30 1.0000000e+00 7.9579119e-35 1.0750538e-11 3.8212533e-33], sum to 1.0000
[2019-03-26 19:33:57,239] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5891
[2019-03-26 19:33:57,244] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 87.16666666666667, 1.0, 2.0, 0.2821006884393824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459653.4282804437, 459653.4282804437, 164179.2948031635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596], 
processed observation next is [1.0, 0.08695652173913043, 0.15955766192733034, 0.8733333333333334, 1.0, 1.0, 0.11214203675045889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11906322705412137, 0.11906322705412137, 0.24203490633725375], 
reward next is 0.7580, 
noisyNet noise sample is [array([-0.10405818], dtype=float32), -0.118289195]. 
=============================================
[2019-03-26 19:33:57,264] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.79076]
 [73.41694]
 [73.32149]
 [73.37953]
 [73.39729]], R is [[74.26456451]
 [74.27687836]
 [74.29251099]
 [74.30794525]
 [74.32320404]].
[2019-03-26 19:33:58,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.9902444e-38 1.0000000e+00 7.8820711e-37 7.0067075e-17 6.2152963e-33], sum to 1.0000
[2019-03-26 19:33:58,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9382
[2019-03-26 19:33:58,472] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.23333333333333, 87.66666666666667, 1.0, 2.0, 0.2570072202831113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418694.6661228547, 418694.6661228547, 161545.5676615469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 355200.0000, 
sim time next is 355800.0000, 
raw observation next is [20.21666666666667, 87.83333333333334, 1.0, 2.0, 0.2554891253045659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 416197.0379709683, 416197.0379709689, 161392.5708607158], 
processed observation next is [1.0, 0.08695652173913043, 0.15718799368088482, 0.8783333333333334, 1.0, 1.0, 0.1029989461500794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11561028832526897, 0.11561028832526914, 0.24088443412047134], 
reward next is 0.7591, 
noisyNet noise sample is [array([-0.70654666], dtype=float32), -0.109363645]. 
=============================================
[2019-03-26 19:33:58,725] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6702360e-29 1.0000000e+00 2.1309642e-34 3.0509408e-11 4.8123453e-33], sum to 1.0000
[2019-03-26 19:33:58,734] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7707
[2019-03-26 19:33:58,739] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.15, 88.5, 1.0, 2.0, 0.2560397980330008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 416996.7051916098, 416996.7051916098, 161443.9544366249], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 358200.0000, 
sim time next is 358800.0000, 
raw observation next is [20.13333333333333, 88.66666666666666, 1.0, 2.0, 0.256341999938463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417466.0831842221, 417466.0831842227, 161473.3418926864], 
processed observation next is [1.0, 0.13043478260869565, 0.15323854660347538, 0.8866666666666666, 1.0, 1.0, 0.10402650594995543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11596280088450614, 0.1159628008845063, 0.24100498789953195], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.89348316], dtype=float32), -1.2379932]. 
=============================================
[2019-03-26 19:33:59,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3830655e-30 9.9999976e-01 1.6003688e-29 1.8615304e-07 2.2997282e-36], sum to 1.0000
[2019-03-26 19:33:59,013] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4467
[2019-03-26 19:33:59,018] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.18982379], dtype=float32), -0.37258127]. 
=============================================
[2019-03-26 19:33:59,157] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.33528468e-32 1.00000000e+00 1.04377676e-32 3.33206401e-12
 7.49554161e-31], sum to 1.0000
[2019-03-26 19:33:59,163] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9991
[2019-03-26 19:33:59,169] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 87.16666666666667, 1.0, 2.0, 0.2821006884393824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459653.4282804437, 459653.4282804437, 164179.2948031635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596], 
processed observation next is [1.0, 0.08695652173913043, 0.15955766192733034, 0.8733333333333334, 1.0, 1.0, 0.11214203675045889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11906322705412137, 0.11906322705412137, 0.24203490633725375], 
reward next is 0.7580, 
noisyNet noise sample is [array([-0.44869485], dtype=float32), -1.3694012]. 
=============================================
[2019-03-26 19:33:59,181] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.15755]
 [73.79345]
 [73.69024]
 [73.72632]
 [73.70961]], R is [[74.60416412]
 [74.61308289]
 [74.62535095]
 [74.6374588 ]
 [74.64942169]].
[2019-03-26 19:33:59,196] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4088387e-31 1.0000000e+00 1.0044591e-35 4.1439838e-15 2.6559091e-31], sum to 1.0000
[2019-03-26 19:33:59,206] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9219
[2019-03-26 19:33:59,211] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 77.0, 1.0, 2.0, 0.4164444057377109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678175.7822122558, 678175.7822122563, 181800.0735574057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 379200.0000, 
sim time next is 379800.0000, 
raw observation next is [21.7, 76.5, 1.0, 2.0, 0.416809459428919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678802.9976968332, 678802.9976968338, 181856.7601443976], 
processed observation next is [1.0, 0.391304347826087, 0.2274881516587678, 0.765, 1.0, 1.0, 0.2973607944926735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18855638824912033, 0.1885563882491205, 0.27142800021551877], 
reward next is 0.7286, 
noisyNet noise sample is [array([-2.261164], dtype=float32), 1.5647647]. 
=============================================
[2019-03-26 19:34:06,517] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 19:34:06,519] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:34:06,520] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:34:06,520] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:34:06,521] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:34:06,523] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:34:06,524] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:34:06,526] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:34:06,526] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:34:06,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:34:06,528] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:34:06,547] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-03-26 19:34:06,566] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-03-26 19:34:06,584] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-03-26 19:34:06,584] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-03-26 19:34:06,625] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-03-26 19:34:25,209] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:34:25,210] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.66666666666666, 35.33333333333334, 1.0, 2.0, 0.379837167541547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 630163.3183006065, 630163.318300607, 176371.4023408132]
[2019-03-26 19:34:25,211] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:34:25,214] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00160244e-32 1.00000000e+00 3.11058753e-37 2.80273123e-14
 6.53733706e-36], sampled 0.3061231125539279
[2019-03-26 19:34:28,224] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:34:28,225] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.8, 76.0, 1.0, 2.0, 0.2733311672238354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445147.5125457734, 445147.5125457734, 163226.5794914156]
[2019-03-26 19:34:28,226] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:34:28,231] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6429181e-34 1.0000000e+00 0.0000000e+00 2.3942860e-15 1.2338937e-37], sampled 0.8798718618618037
[2019-03-26 19:35:13,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:13,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.877344875, 74.91632408333334, 1.0, 2.0, 0.4390548936840148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 620022.3447102415, 620022.3447102415, 175446.9363253427]
[2019-03-26 19:35:13,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:35:13,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.7547476e-33 1.0000000e+00 6.4563392e-38 5.9717210e-11 1.1082948e-35], sampled 0.5556845832682028
[2019-03-26 19:35:14,737] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:14,737] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.75093975, 69.14109419, 1.0, 2.0, 0.5284047512450264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738377.3375093259, 738377.3375093259, 188165.6723840021]
[2019-03-26 19:35:14,742] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:35:14,744] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.9972390e-35 1.0000000e+00 0.0000000e+00 4.7545636e-15 2.4650449e-38], sampled 0.31634180211625984
[2019-03-26 19:35:22,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:22,953] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.66666666666667, 61.0, 1.0, 2.0, 0.5816665999604069, 0.0, 2.0, 0.0, 1.0, 2.0, 1.010162524047185, 6.911199999999999, 6.9112, 168.9129401709992, 1626287.238157168, 1626287.238157168, 356031.0439259103]
[2019-03-26 19:35:22,954] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:35:22,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9356879e-32 1.0000000e+00 7.6379914e-37 4.9795172e-12 5.3714622e-35], sampled 0.15834209774418695
[2019-03-26 19:35:23,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:23,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 68.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.967400653358294, 6.9112, 168.9123977855795, 1493652.778550328, 1453782.232663693, 311354.1324075502]
[2019-03-26 19:35:23,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:35:23,135] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0898980e-31 9.9999833e-01 2.0191829e-36 1.7271516e-06 1.7744030e-33], sampled 0.5250356423883387
[2019-03-26 19:35:26,210] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:26,211] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 81.5, 1.0, 2.0, 0.5864736439488645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819552.4130455853, 819552.4130455846, 198252.4230382738]
[2019-03-26 19:35:26,212] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:35:26,214] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.0622899e-35 1.0000000e+00 0.0000000e+00 6.1541064e-15 3.4716320e-38], sampled 0.31523806781633423
[2019-03-26 19:35:37,461] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:37,462] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.2, 87.0, 1.0, 2.0, 0.577553376354313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807082.2800495934, 807082.2800495934, 196638.0128594472]
[2019-03-26 19:35:37,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:35:37,469] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2009684e-35 1.0000000e+00 0.0000000e+00 3.0618009e-15 1.2084268e-38], sampled 0.9148706034619536
[2019-03-26 19:35:57,111] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:57,112] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.326995775, 83.09618053, 1.0, 2.0, 0.5427437857664706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758421.437934458, 758421.4379344586, 190564.8522492502]
[2019-03-26 19:35:57,113] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:35:57,116] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3358574e-34 1.0000000e+00 0.0000000e+00 4.1568257e-14 1.3473116e-37], sampled 0.8522643770705564
[2019-03-26 19:35:59,419] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.44513226]
[2019-03-26 19:35:59,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.1, 65.33333333333334, 1.0, 2.0, 0.3528701720008025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557452.4597651741, 557452.4597651741, 171546.1249492915]
[2019-03-26 19:35:59,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:35:59,424] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1626727e-33 1.0000000e+00 1.1543350e-37 1.7934981e-14 2.4647652e-36], sampled 0.774426645359387
[2019-03-26 19:36:00,435] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8021.4236 3152804036.9439 1354.0000
[2019-03-26 19:36:00,494] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8157.2129 2991130225.4043 1303.0000
[2019-03-26 19:36:00,517] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8679.7703 2778098621.6261 875.0000
[2019-03-26 19:36:00,718] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8561.0997 2836249249.6923 942.0000
[2019-03-26 19:36:00,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8338.8784 2920956947.7594 1127.0000
[2019-03-26 19:36:01,845] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 225000, evaluation results [225000.0, 8021.423594827728, 3152804036.9438815, 1354.0, 8338.878372456653, 2920956947.7594094, 1127.0, 8679.770309961114, 2778098621.6261, 875.0, 8157.212896820122, 2991130225.4042697, 1303.0, 8561.099679786716, 2836249249.692307, 942.0]
[2019-03-26 19:36:12,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.0360477e-33 1.0000000e+00 2.3210570e-36 1.1685750e-14 4.5239001e-34], sum to 1.0000
[2019-03-26 19:36:12,378] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3640
[2019-03-26 19:36:12,389] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 61.33333333333333, 1.0, 2.0, 0.2504187298447559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412273.4695163005, 412273.4695163011, 160939.509386387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 670200.0000, 
sim time next is 670800.0000, 
raw observation next is [22.96666666666667, 62.66666666666667, 1.0, 2.0, 0.2497542569032178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411159.580403137, 411159.5804031377, 160874.469992949], 
processed observation next is [1.0, 0.782608695652174, 0.2875197472353872, 0.6266666666666667, 1.0, 1.0, 0.09608946614845515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11421099455642696, 0.11421099455642715, 0.24011114924320745], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.32160935], dtype=float32), -0.039344378]. 
=============================================
[2019-03-26 19:36:13,163] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8031703e-29 1.0000000e+00 3.2074679e-37 1.5065748e-13 4.5088079e-34], sum to 1.0000
[2019-03-26 19:36:13,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6963
[2019-03-26 19:36:13,177] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.66666666666667, 1.0, 2.0, 0.2393789772093673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 159798.8369622887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 682800.0000, 
sim time next is 683400.0000, 
raw observation next is [19.6, 83.33333333333333, 1.0, 2.0, 0.2397787823615639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396477.27191647, 396477.2719164694, 159832.2482227832], 
processed observation next is [1.0, 0.9130434782608695, 0.127962085308057, 0.8333333333333333, 1.0, 1.0, 0.08407082212236615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11013257553235278, 0.1101325755323526, 0.238555594362363], 
reward next is 0.7614, 
noisyNet noise sample is [array([0.2628675], dtype=float32), 0.717613]. 
=============================================
[2019-03-26 19:36:15,050] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2644544e-36 1.0000000e+00 3.4867888e-38 1.1478579e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 19:36:15,061] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3959
[2019-03-26 19:36:15,066] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.55, 92.0, 1.0, 2.0, 0.2250363663102666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 375212.4245053321, 375212.4245053321, 157944.9050268299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 707400.0000, 
sim time next is 708000.0000, 
raw observation next is [17.53333333333333, 92.0, 1.0, 2.0, 0.2172313405602214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 362233.1496192599, 362233.1496192593, 157244.0798844782], 
processed observation next is [1.0, 0.17391304347826086, 0.030015797788309612, 0.92, 1.0, 1.0, 0.05690522959062817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1006203193386833, 0.10062031933868314, 0.23469265654399732], 
reward next is 0.7653, 
noisyNet noise sample is [array([-0.24530809], dtype=float32), 0.029432073]. 
=============================================
[2019-03-26 19:36:15,078] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.2732 ]
 [79.28745]
 [79.31722]
 [79.35752]
 [79.35288]], R is [[79.16057587]
 [79.13323212]
 [79.10655975]
 [79.07912445]
 [79.05387115]].
[2019-03-26 19:36:21,373] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2819799e-37 1.0000000e+00 9.8077167e-36 1.5040474e-14 1.2941823e-34], sum to 1.0000
[2019-03-26 19:36:21,383] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7200
[2019-03-26 19:36:21,388] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 62.0, 1.0, 2.0, 0.2895444917947173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463480.6529531262, 463480.6529531256, 164493.9486541766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 820800.0000, 
sim time next is 821400.0000, 
raw observation next is [24.96666666666667, 62.16666666666667, 1.0, 2.0, 0.28930423503913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463132.4665881591, 463132.4665881597, 164470.3522888233], 
processed observation next is [0.0, 0.5217391304347826, 0.3823064770932071, 0.6216666666666667, 1.0, 1.0, 0.14374004221581926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12864790738559975, 0.12864790738559992, 0.2454781377445124], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.87637687], dtype=float32), 0.40081584]. 
=============================================
[2019-03-26 19:36:24,541] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7235183e-33 1.0000000e+00 5.9789844e-36 1.5859174e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 19:36:24,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8162
[2019-03-26 19:36:24,556] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.95, 88.5, 1.0, 2.0, 0.2873202962730949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461280.7366350644, 461280.7366350638, 164354.9904660491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 876600.0000, 
sim time next is 877200.0000, 
raw observation next is [20.93333333333333, 88.33333333333334, 1.0, 2.0, 0.2867807129147474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460748.844830786, 460748.844830786, 164320.6354003196], 
processed observation next is [0.0, 0.13043478260869565, 0.19115323854660338, 0.8833333333333334, 1.0, 1.0, 0.14069965411415347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1279857902307739, 0.1279857902307739, 0.24525467970196954], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.17517975], dtype=float32), 0.24843884]. 
=============================================
[2019-03-26 19:36:32,256] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3372751e-26 1.0000000e+00 4.3232649e-25 9.2622993e-11 1.2204514e-26], sum to 1.0000
[2019-03-26 19:36:32,268] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4655
[2019-03-26 19:36:32,277] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 95.0, 1.0, 2.0, 0.4035655307059746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622761.6777443201, 622761.6777443201, 177079.374243855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 995400.0000, 
sim time next is 996000.0000, 
raw observation next is [21.76666666666667, 95.0, 1.0, 2.0, 0.4490403868515986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693652.4545397687, 693652.4545397687, 183952.0066599745], 
processed observation next is [1.0, 0.5217391304347826, 0.23064770932069528, 0.95, 1.0, 1.0, 0.33619323717060073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19268123737215798, 0.19268123737215798, 0.27455523382085745], 
reward next is 0.7254, 
noisyNet noise sample is [array([-0.08107157], dtype=float32), 0.50458574]. 
=============================================
[2019-03-26 19:36:32,287] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[64.594055]
 [64.46364 ]
 [64.2385  ]
 [64.38285 ]
 [64.39155 ]], R is [[64.652771  ]
 [64.74194336]
 [64.81443787]
 [64.82216644]
 [64.86049652]].
[2019-03-26 19:36:33,288] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.3936977e-30 1.0000000e+00 6.1716759e-35 5.3059515e-11 4.9090476e-37], sum to 1.0000
[2019-03-26 19:36:33,297] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3936
[2019-03-26 19:36:33,303] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 97.5, 1.0, 2.0, 0.3724396340667769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564686.0933807458, 564686.0933807465, 171638.2772160558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [22.13333333333333, 97.33333333333333, 1.0, 2.0, 0.372787209659153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564968.6092827584, 564968.6092827584, 171655.3101044312], 
processed observation next is [1.0, 1.0, 0.24802527646129527, 0.9733333333333333, 1.0, 1.0, 0.24432193934837712, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15693572480076623, 0.15693572480076623, 0.25620195537974805], 
reward next is 0.7438, 
noisyNet noise sample is [array([-0.65981334], dtype=float32), -0.14349985]. 
=============================================
[2019-03-26 19:36:33,640] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2980601e-32 1.0000000e+00 2.3554843e-38 7.9653246e-12 0.0000000e+00], sum to 1.0000
[2019-03-26 19:36:33,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7203
[2019-03-26 19:36:33,656] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 97.16666666666667, 1.0, 2.0, 0.3622591237435236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553264.9287388698, 553264.9287388698, 170782.7768147032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1030200.0000, 
sim time next is 1030800.0000, 
raw observation next is [21.93333333333333, 97.33333333333334, 1.0, 2.0, 0.3636611244506445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554782.474965627, 554782.4749656275, 170892.6030231996], 
processed observation next is [1.0, 0.9565217391304348, 0.23854660347551332, 0.9733333333333334, 1.0, 1.0, 0.23332665596463195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15410624304600748, 0.15410624304600765, 0.25506358660179046], 
reward next is 0.7449, 
noisyNet noise sample is [array([-0.7458949], dtype=float32), -1.453248]. 
=============================================
[2019-03-26 19:36:38,201] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2900703e-29 1.0000000e+00 8.7028982e-34 3.2277551e-12 1.0247158e-35], sum to 1.0000
[2019-03-26 19:36:38,210] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1840
[2019-03-26 19:36:38,216] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 67.16666666666667, 1.0, 2.0, 0.797217510878386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1231854.706438085, 1231854.706438085, 258606.5675795001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1093800.0000, 
sim time next is 1094400.0000, 
raw observation next is [25.7, 67.0, 1.0, 2.0, 0.7950108555506639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1229328.373533733, 1229328.373533734, 258107.6580123094], 
processed observation next is [1.0, 0.6956521739130435, 0.4170616113744076, 0.67, 1.0, 1.0, 0.7530251271694746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34148010375937027, 0.34148010375937055, 0.38523531046613346], 
reward next is 0.6148, 
noisyNet noise sample is [array([-1.8678237], dtype=float32), -1.0208259]. 
=============================================
[2019-03-26 19:36:38,837] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3054369e-29 1.0000000e+00 9.9262378e-37 1.9605717e-09 2.1728422e-36], sum to 1.0000
[2019-03-26 19:36:38,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8452
[2019-03-26 19:36:38,853] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 72.5, 1.0, 2.0, 0.3301714180145942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516698.9302897274, 516698.9302897268, 168180.979037402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1104600.0000, 
sim time next is 1105200.0000, 
raw observation next is [24.2, 73.0, 1.0, 2.0, 0.3295946731302897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516485.2663641399, 516485.2663641393, 168181.3163408892], 
processed observation next is [1.0, 0.8260869565217391, 0.3459715639810427, 0.73, 1.0, 1.0, 0.19228273871119236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14346812954559443, 0.14346812954559424, 0.2510168900610287], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.0075576], dtype=float32), 0.67706996]. 
=============================================
[2019-03-26 19:36:45,615] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9527164e-30 1.0000000e+00 2.9236140e-34 1.3102919e-12 2.0598215e-35], sum to 1.0000
[2019-03-26 19:36:45,623] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8010
[2019-03-26 19:36:45,628] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 92.33333333333334, 1.0, 2.0, 0.3665101887854174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569940.3009527042, 569940.3009527042, 172461.4239260066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1221600.0000, 
sim time next is 1222200.0000, 
raw observation next is [21.85, 92.5, 1.0, 2.0, 0.3702202436027319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575654.3116356224, 575654.311635623, 172952.9782742802], 
processed observation next is [1.0, 0.13043478260869565, 0.23459715639810438, 0.925, 1.0, 1.0, 0.24122920915991797, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15990397545433957, 0.1599039754543397, 0.2581387735437018], 
reward next is 0.7419, 
noisyNet noise sample is [array([0.8675472], dtype=float32), -1.8119738]. 
=============================================
[2019-03-26 19:36:46,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8430038e-30 1.0000000e+00 1.0240794e-34 2.4538225e-13 3.4514833e-22], sum to 1.0000
[2019-03-26 19:36:46,823] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7035
[2019-03-26 19:36:46,828] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 81.33333333333333, 1.0, 2.0, 0.7862657705120224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1155656.439618069, 1155656.43961807, 248331.3772217134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1240800.0000, 
sim time next is 1241400.0000, 
raw observation next is [25.38333333333333, 80.66666666666667, 1.0, 2.0, 0.790140451536843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1156353.602854636, 1156353.602854637, 248658.5106076122], 
processed observation next is [1.0, 0.34782608695652173, 0.4020537124802526, 0.8066666666666668, 1.0, 1.0, 0.7471571705263168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32120933412628777, 0.32120933412628805, 0.3711321053844958], 
reward next is 0.6289, 
noisyNet noise sample is [array([-0.38679972], dtype=float32), -0.32367596]. 
=============================================
[2019-03-26 19:36:56,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.517774e-28 0.000000e+00], sum to 1.0000
[2019-03-26 19:36:56,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6850
[2019-03-26 19:36:56,860] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 70.66666666666667, 1.0, 2.0, 0.4270259918218965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617269.5286702916, 617269.5286702916, 175605.094778121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [27.33333333333334, 70.33333333333334, 1.0, 2.0, 0.4272811100127285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617414.2181239652, 617414.2181239659, 175612.5894926342], 
processed observation next is [0.0, 0.6086956521739131, 0.4944707740916275, 0.7033333333333335, 1.0, 1.0, 0.30997724097919094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17150394947887923, 0.17150394947887942, 0.2621083425263197], 
reward next is 0.7379, 
noisyNet noise sample is [array([-0.37195668], dtype=float32), -2.1923945]. 
=============================================
[2019-03-26 19:36:56,890] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[76.99966]
 [77.01521]
 [77.01755]
 [77.03636]
 [77.04506]], R is [[76.95127869]
 [76.9196701 ]
 [76.8883667 ]
 [76.85729218]
 [76.82642365]].
[2019-03-26 19:36:57,400] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 19:36:57,402] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:36:57,403] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:36:57,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:36:57,405] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:36:57,406] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:36:57,405] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:36:57,409] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:36:57,406] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:36:57,409] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:36:57,413] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:36:57,427] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-03-26 19:36:57,427] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-03-26 19:36:57,427] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-03-26 19:36:57,464] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-03-26 19:36:57,503] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-03-26 19:37:05,154] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:37:05,155] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.43333333333333, 67.66666666666667, 1.0, 2.0, 0.4671205029577186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766549.5944158662, 766549.5944158655, 190072.9993363132]
[2019-03-26 19:37:05,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:37:05,161] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2552738e-27 0.0000000e+00], sampled 0.03855521102761206
[2019-03-26 19:37:15,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:37:15,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.60119179333334, 82.93034093000001, 1.0, 2.0, 0.345863040543175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565905.2125560676, 565905.2125560676, 171935.3946906426]
[2019-03-26 19:37:15,237] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:37:15,242] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7459948e-28 0.0000000e+00], sampled 0.02120272179649807
[2019-03-26 19:37:32,401] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:37:32,402] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.36205593666667, 97.38638660666666, 1.0, 2.0, 0.6170174908128062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862252.4465108439, 862252.4465108432, 203954.7958005008]
[2019-03-26 19:37:32,404] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:37:32,408] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.135349e-29 0.000000e+00], sampled 0.22973213533975967
[2019-03-26 19:37:43,771] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:37:43,771] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 94.0, 1.0, 2.0, 0.5216300405108971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 824621.5375973925, 824621.5375973919, 198019.8620681615]
[2019-03-26 19:37:43,772] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:37:43,775] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9123624e-28 0.0000000e+00], sampled 0.9402676855297691
[2019-03-26 19:38:10,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:38:10,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.00000000000001, 1.0, 2.0, 0.4864589666265494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679744.7718240013, 679744.7718240013, 181501.3452180053]
[2019-03-26 19:38:10,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:38:10,201] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.401028e-29 0.000000e+00], sampled 0.525652876708595
[2019-03-26 19:38:21,967] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:38:21,968] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 79.66666666666667, 1.0, 2.0, 0.5134237457774172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717436.249416491, 717436.2494164916, 185726.2133214409]
[2019-03-26 19:38:21,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:38:21,971] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 5.38042e-29 0.00000e+00], sampled 0.33748217595865004
[2019-03-26 19:38:29,258] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:38:29,259] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.7, 56.0, 1.0, 2.0, 1.035789288977966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9128407187875, 1447865.008114324, 1447865.008114324, 310024.5710436031]
[2019-03-26 19:38:29,262] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:38:29,264] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.339018e-29 0.000000e+00], sampled 0.7767335697394758
[2019-03-26 19:38:32,717] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:38:32,718] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.4, 82.0, 1.0, 2.0, 0.5193917096283556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725778.4742334208, 725778.4742334208, 186689.7429614227]
[2019-03-26 19:38:32,721] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:38:32,723] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.298743e-30 0.000000e+00], sampled 0.006510192969657047
[2019-03-26 19:38:52,829] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.46746913]
[2019-03-26 19:38:52,830] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.26666666666667, 78.66666666666667, 1.0, 2.0, 0.6975143622545229, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975128257893, 6.9112, 168.9123160543568, 1871650.515170465, 1804414.03221323, 384109.7986651143]
[2019-03-26 19:38:52,832] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:38:52,836] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5421030e-38 1.0000000e+00 0.0000000e+00 1.5538637e-26 0.0000000e+00], sampled 0.7698542923090625
[2019-03-26 19:38:52,836] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1871650.515170465 W.
[2019-03-26 19:38:55,049] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4752 2927712708.6915 1338.0000
[2019-03-26 19:38:55,303] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7732 2779589745.4232 933.0000
[2019-03-26 19:38:55,307] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9210 2842890288.6408 1131.0000
[2019-03-26 19:38:55,413] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.4024 3164387323.3700 1778.0000
[2019-03-26 19:38:55,498] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3731 3008038374.3173 1766.0000
[2019-03-26 19:38:56,513] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 250000, evaluation results [250000.0, 7880.402365377816, 3164387323.3700466, 1778.0, 8254.475224188334, 2927712708.691521, 1338.0, 8660.773226357216, 2779589745.4231777, 933.0, 7998.373066976429, 3008038374.3172884, 1766.0, 8496.920990814182, 2842890288.6407876, 1131.0]
[2019-03-26 19:38:57,712] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.969424e-29 0.000000e+00], sum to 1.0000
[2019-03-26 19:38:57,721] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3644
[2019-03-26 19:38:57,727] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 89.33333333333333, 1.0, 2.0, 0.3907755687023597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584580.0612230237, 584580.0612230243, 173168.24205628], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1453200.0000, 
sim time next is 1453800.0000, 
raw observation next is [23.25, 90.66666666666667, 1.0, 2.0, 0.3877274178202514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581494.2746672592, 581494.2746672592, 172934.328901482], 
processed observation next is [0.0, 0.8260869565217391, 0.30094786729857825, 0.9066666666666667, 1.0, 1.0, 0.2623221901448812, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.161526187407572, 0.161526187407572, 0.25811093865892837], 
reward next is 0.7419, 
noisyNet noise sample is [array([1.1471168], dtype=float32), 0.027828598]. 
=============================================
[2019-03-26 19:39:00,641] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.250123e-26 0.000000e+00], sum to 1.0000
[2019-03-26 19:39:00,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5746
[2019-03-26 19:39:00,656] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 98.16666666666667, 1.0, 2.0, 0.3148335086808646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497406.8588087675, 497406.8588087681, 166822.593909154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1483800.0000, 
sim time next is 1484400.0000, 
raw observation next is [20.46666666666667, 98.33333333333334, 1.0, 2.0, 0.3131027740343431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 495233.7151540142, 495233.7151540136, 166672.5821964107], 
processed observation next is [0.0, 0.17391304347826086, 0.16903633491311232, 0.9833333333333334, 1.0, 1.0, 0.1724129807642688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13756492087611505, 0.13756492087611488, 0.2487650480543443], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.6367755], dtype=float32), -0.76312006]. 
=============================================
[2019-03-26 19:39:06,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0868787e-36 1.0000000e+00 0.0000000e+00 2.4333489e-27 6.5790878e-37], sum to 1.0000
[2019-03-26 19:39:06,853] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6615
[2019-03-26 19:39:06,861] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.68333333333333, 85.0, 1.0, 2.0, 0.8268311027735775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253689.908224907, 1253689.908224907, 263925.0723206759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1594200.0000, 
sim time next is 1594800.0000, 
raw observation next is [23.7, 85.0, 1.0, 2.0, 0.8333633007763968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1262923.366575425, 1262923.366575424, 265653.1000151281], 
processed observation next is [1.0, 0.4782608695652174, 0.3222748815165877, 0.85, 1.0, 1.0, 0.7992328925016828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3508120462709514, 0.35081204627095114, 0.39649716420168374], 
reward next is 0.6035, 
noisyNet noise sample is [array([-1.2265798], dtype=float32), -0.33415455]. 
=============================================
[2019-03-26 19:39:09,592] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5733132e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 19:39:09,607] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1200
[2019-03-26 19:39:09,614] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.00000000000001, 1.0, 2.0, 0.4387657657414944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632192.7737544909, 632192.7737544916, 177013.8418019125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1653600.0000, 
sim time next is 1654200.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4301957118243803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619853.3233610058, 619853.3233610058, 175798.5548519476], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.3134888094269642, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17218147871139047, 0.17218147871139047, 0.2623859027641009], 
reward next is 0.7376, 
noisyNet noise sample is [array([-0.12908639], dtype=float32), 0.63241935]. 
=============================================
[2019-03-26 19:39:13,401] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.305302e-32 1.000000e+00 0.000000e+00 5.609507e-18 4.624014e-34], sum to 1.0000
[2019-03-26 19:39:13,406] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1387
[2019-03-26 19:39:13,411] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 0.5082964139928445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710269.1412918553, 710269.141291856, 184906.389975793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [26.93333333333333, 84.66666666666666, 1.0, 2.0, 0.5090827636249359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711368.3166479467, 711368.3166479474, 185031.53148485], 
processed observation next is [1.0, 0.8260869565217391, 0.4755134281200631, 0.8466666666666666, 1.0, 1.0, 0.4085334501505251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976023101799852, 0.1976023101799854, 0.2761664649027612], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.08135161], dtype=float32), -0.7022805]. 
=============================================
[2019-03-26 19:39:18,713] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.9480355e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 19:39:18,724] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2138
[2019-03-26 19:39:18,731] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 95.16666666666667, 1.0, 2.0, 0.3628187414379867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558332.0404360032, 558332.0404360032, 171333.4046020497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1825800.0000, 
sim time next is 1826400.0000, 
raw observation next is [21.9, 95.33333333333334, 1.0, 2.0, 0.3455716688716803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531477.8385304972, 531477.8385304979, 169093.2739835533], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.9533333333333335, 1.0, 1.0, 0.21153213117069913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1476327329251381, 0.1476327329251383, 0.2523780208709751], 
reward next is 0.7476, 
noisyNet noise sample is [array([-2.5861042], dtype=float32), 0.6526745]. 
=============================================
[2019-03-26 19:39:21,351] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.0008134e-37 1.0000000e+00 0.0000000e+00 1.5916869e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 19:39:21,358] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3265
[2019-03-26 19:39:21,368] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.48333333333333, 90.33333333333333, 1.0, 2.0, 0.8876414164202816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1275697.956631741, 1275697.956631741, 271685.1948351348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1849800.0000, 
sim time next is 1850400.0000, 
raw observation next is [24.6, 90.0, 1.0, 2.0, 0.8642812292572144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238442.543499014, 1238442.543499014, 264706.07610416], 
processed observation next is [1.0, 0.43478260869565216, 0.36492890995260674, 0.9, 1.0, 1.0, 0.8364834087436318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34401181763861505, 0.34401181763861505, 0.39508369567785073], 
reward next is 0.6049, 
noisyNet noise sample is [array([1.379238], dtype=float32), 0.48996627]. 
=============================================
[2019-03-26 19:39:26,134] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7983417e-29 1.0000000e+00 9.1628551e-35 7.7950440e-18 3.1477891e-33], sum to 1.0000
[2019-03-26 19:39:26,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5988
[2019-03-26 19:39:26,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1795029.086004036 W.
[2019-03-26 19:39:26,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.46666666666667, 74.33333333333333, 1.0, 2.0, 0.4279826299264294, 1.0, 2.0, 0.4279826299264294, 1.0, 2.0, 0.7204167782940138, 6.911200000000001, 6.9112, 170.5573041426782, 1795029.086004036, 1795029.086004035, 365375.9525064499], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1950000.0000, 
sim time next is 1950600.0000, 
raw observation next is [27.53333333333333, 74.16666666666667, 1.0, 2.0, 0.4253455118683497, 1.0, 2.0, 0.4253455118683497, 1.0, 2.0, 0.7161315218124976, 6.9112, 6.9112, 170.5573041426782, 1783959.374743096, 1783959.374743096, 363890.3470896698], 
processed observation next is [1.0, 0.5652173913043478, 0.5039494470774091, 0.7416666666666667, 1.0, 1.0, 0.307645195022108, 1.0, 1.0, 0.307645195022108, 1.0, 1.0, 0.6538189290396311, 0.0, 0.0, 0.8375144448122397, 0.4955442707619711, 0.4955442707619711, 0.5431199210293579], 
reward next is 0.4569, 
noisyNet noise sample is [array([-1.8603504], dtype=float32), 0.19003059]. 
=============================================
[2019-03-26 19:39:30,481] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8265146e-37 1.0000000e+00 0.0000000e+00 4.3996967e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 19:39:30,494] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1673
[2019-03-26 19:39:30,498] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 96.66666666666666, 1.0, 2.0, 0.4660310343147669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656733.6792914147, 656733.6792914141, 179160.3661694448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2002800.0000, 
sim time next is 2003400.0000, 
raw observation next is [24.05, 97.0, 1.0, 2.0, 0.4661282528298522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657108.7704664328, 657108.7704664321, 179205.3507496878], 
processed observation next is [0.0, 0.17391304347826086, 0.3388625592417062, 0.97, 1.0, 1.0, 0.35678102750584606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18253021401845354, 0.18253021401845335, 0.26747067276072806], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.1946915], dtype=float32), -0.86783767]. 
=============================================
[2019-03-26 19:39:42,717] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.41468370e-20 1.00000000e+00 4.06970817e-24 8.53961433e-12
 1.12029095e-26], sum to 1.0000
[2019-03-26 19:39:42,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1938
[2019-03-26 19:39:42,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2002618.204014954 W.
[2019-03-26 19:39:42,738] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.48333333333333, 76.5, 1.0, 2.0, 0.477431178583599, 1.0, 2.0, 0.477431178583599, 1.0, 2.0, 0.82914006829627, 6.9112, 6.9112, 170.5573041426782, 2002618.204014954, 2002618.204014954, 399889.6081769278], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [29.6, 76.0, 1.0, 2.0, 0.7145402727406491, 1.0, 2.0, 0.7145402727406491, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1998121.644423944, 1998121.644423944, 380413.4272646828], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.76, 1.0, 1.0, 0.6560726177598182, 1.0, 1.0, 0.6560726177598182, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5550337901177622, 0.5550337901177622, 0.5677812347234071], 
reward next is 0.4322, 
noisyNet noise sample is [array([0.19361964], dtype=float32), -0.41950598]. 
=============================================
[2019-03-26 19:39:42,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[38.532085]
 [39.23608 ]
 [40.66183 ]
 [40.828194]
 [41.030685]], R is [[38.64796066]
 [38.66463089]
 [38.6803093 ]
 [38.71601486]
 [38.76848221]].
[2019-03-26 19:39:43,264] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.8171494e-14 9.9999940e-01 8.1183079e-18 5.5497628e-07 5.5798121e-20], sum to 1.0000
[2019-03-26 19:39:43,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6040
[2019-03-26 19:39:43,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2554134.042993798 W.
[2019-03-26 19:39:43,284] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.98333333333333, 66.16666666666667, 1.0, 2.0, 0.9131576161879676, 1.0, 1.0, 0.9131576161879676, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2554134.042993798, 2554134.042993798, 478722.2459018106], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2214600.0000, 
sim time next is 2215200.0000, 
raw observation next is [31.96666666666667, 66.33333333333334, 1.0, 2.0, 0.5751210127117099, 1.0, 2.0, 0.5751210127117099, 1.0, 1.0, 0.9987950036549798, 6.911199999999999, 6.9112, 170.5573041426782, 2412814.309264848, 2412814.309264849, 470980.1073749951], 
processed observation next is [1.0, 0.6521739130434783, 0.7140600315955767, 0.6633333333333334, 1.0, 1.0, 0.4880976056767589, 1.0, 1.0, 0.4880976056767589, 1.0, 0.5, 0.9985304922621704, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6702261970180133, 0.6702261970180136, 0.7029553841417837], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5580331], dtype=float32), 0.030614043]. 
=============================================
[2019-03-26 19:39:46,363] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.0909001e-30 1.0000000e+00 2.8636434e-35 4.1155848e-20 4.9637911e-36], sum to 1.0000
[2019-03-26 19:39:46,372] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5968
[2019-03-26 19:39:46,377] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 87.16666666666667, 1.0, 2.0, 0.6251125631082837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 873569.57527318, 873569.5752731793, 205508.0825212715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [26.03333333333333, 86.33333333333334, 1.0, 2.0, 0.5980096843371455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835679.4958897944, 835679.4958897944, 200367.9193378125], 
processed observation next is [1.0, 0.21739130434782608, 0.4328593996840442, 0.8633333333333334, 1.0, 1.0, 0.5156743184784885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23213319330272067, 0.23213319330272067, 0.2990565960265858], 
reward next is 0.7009, 
noisyNet noise sample is [array([-0.43677256], dtype=float32), 0.87952083]. 
=============================================
[2019-03-26 19:39:51,330] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5028668e-32 1.0000000e+00 1.6031053e-38 1.9437170e-19 5.6844244e-34], sum to 1.0000
[2019-03-26 19:39:51,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1697
[2019-03-26 19:39:51,343] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 81.0, 1.0, 2.0, 0.5278805504544675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737644.5803271668, 737644.5803271674, 188079.6796327491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [27.9, 81.0, 1.0, 2.0, 0.5258523360577524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734809.4335724213, 734809.4335724219, 187745.7522814385], 
processed observation next is [1.0, 0.043478260869565216, 0.5213270142180094, 0.81, 1.0, 1.0, 0.4287377542864486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2041137315478948, 0.20411373154789497, 0.2802175407185649], 
reward next is 0.7198, 
noisyNet noise sample is [array([-0.24976288], dtype=float32), -1.0711559]. 
=============================================
[2019-03-26 19:39:52,011] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 19:39:52,015] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:39:52,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:39:52,018] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:39:52,019] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:39:52,019] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:39:52,022] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:39:52,024] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:39:52,025] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:39:52,025] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:39:52,027] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:39:52,038] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-03-26 19:39:52,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-03-26 19:39:52,078] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-03-26 19:39:52,079] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-03-26 19:39:52,097] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-03-26 19:40:36,595] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5501025]
[2019-03-26 19:40:36,596] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.98814969666667, 98.83301161666665, 1.0, 2.0, 0.4271043988056772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624367.795857928, 624367.7958579273, 176489.6271454376]
[2019-03-26 19:40:36,597] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:40:36,600] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6868342e-29 1.0000000e+00 6.7433468e-33 9.1226863e-18 2.0745066e-33], sampled 0.7351778954102052
[2019-03-26 19:40:45,702] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5501025]
[2019-03-26 19:40:45,703] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.01666666666667, 79.5, 1.0, 2.0, 0.5699665331176168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796476.3295593641, 796476.3295593641, 195283.1322256087]
[2019-03-26 19:40:45,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:40:45,707] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9526772e-29 1.0000000e+00 3.2846148e-33 6.2183555e-18 9.9806624e-34], sampled 0.6410999412376878
[2019-03-26 19:41:16,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5501025]
[2019-03-26 19:41:16,926] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.87367448, 77.06643718, 1.0, 2.0, 0.7970113051879113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1115178.854408976, 1115178.854408977, 243352.589681289]
[2019-03-26 19:41:16,926] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:41:16,929] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7187941e-27 1.0000000e+00 8.7637110e-31 1.2201131e-16 2.9310893e-31], sampled 0.3980072304769231
[2019-03-26 19:41:26,516] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5501025]
[2019-03-26 19:41:26,517] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.74510152666667, 61.73774275000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.868140496262816, 6.9112, 168.9078847982536, 2133086.116900139, 1454219.977473657, 311348.6274760471]
[2019-03-26 19:41:26,517] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:41:26,519] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.4205488e-25 1.0000000e+00 5.7790272e-28 3.8770845e-15 2.1607913e-28], sampled 0.2591396902264995
[2019-03-26 19:41:26,520] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2133086.116900139 W.
[2019-03-26 19:41:46,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2741 3008104656.7243 1766.0000
[2019-03-26 19:41:46,096] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8875 3164502873.7783 1778.0000
[2019-03-26 19:41:46,224] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2748 2927846980.7461 1338.0000
[2019-03-26 19:41:46,384] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6708 2779658361.4638 933.0000
[2019-03-26 19:41:46,454] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8190 2842958600.4139 1131.0000
[2019-03-26 19:41:47,468] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 275000, evaluation results [275000.0, 7878.887539409773, 3164502873.778303, 1778.0, 8254.274818136657, 2927846980.7461457, 1338.0, 8660.670814356308, 2779658361.463785, 933.0, 7998.274138010797, 3008104656.724262, 1766.0, 8496.819032943851, 2842958600.4139094, 1131.0]
[2019-03-26 19:41:50,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4223817e-27 1.0000000e+00 2.6178184e-30 4.1581360e-17 2.3555479e-30], sum to 1.0000
[2019-03-26 19:41:50,407] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5903
[2019-03-26 19:41:50,414] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.85, 80.0, 1.0, 2.0, 0.5530945427769307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 772890.6815433354, 772890.681543336, 192332.9599338659], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2422200.0000, 
sim time next is 2422800.0000, 
raw observation next is [28.8, 80.0, 1.0, 2.0, 0.5516432354518738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770861.8983163009, 770861.8983163016, 192083.0174811172], 
processed observation next is [1.0, 0.043478260869565216, 0.5639810426540285, 0.8, 1.0, 1.0, 0.4598111270504503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21412830508786138, 0.21412830508786157, 0.2866910708673391], 
reward next is 0.7133, 
noisyNet noise sample is [array([-2.3842208], dtype=float32), -0.36829123]. 
=============================================
[2019-03-26 19:41:51,006] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0443102e-27 1.0000000e+00 2.6610697e-32 4.7280439e-17 3.3237056e-30], sum to 1.0000
[2019-03-26 19:41:51,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0473
[2019-03-26 19:41:51,019] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.3, 76.33333333333334, 1.0, 2.0, 0.5818054634330592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813026.4883291817, 813026.4883291817, 197405.3537050742], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2409000.0000, 
sim time next is 2409600.0000, 
raw observation next is [30.2, 76.66666666666667, 1.0, 2.0, 0.5805956275692469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811335.1940017126, 811335.1940017126, 197186.6646657431], 
processed observation next is [1.0, 0.9130434782608695, 0.6303317535545023, 0.7666666666666667, 1.0, 1.0, 0.49469352719186366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22537088722269794, 0.22537088722269794, 0.2943084547249897], 
reward next is 0.7057, 
noisyNet noise sample is [array([-0.82495344], dtype=float32), 3.0079355]. 
=============================================
[2019-03-26 19:41:53,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4739405e-14 9.9999940e-01 1.3319318e-18 5.9476537e-07 3.6255432e-17], sum to 1.0000
[2019-03-26 19:41:53,141] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6768
[2019-03-26 19:41:53,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1892564.5855308 W.
[2019-03-26 19:41:53,154] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.2, 85.66666666666667, 1.0, 2.0, 0.676825747171379, 1.0, 2.0, 0.676825747171379, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1892564.5855308, 1892564.5855308, 364358.1095302784], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2473800.0000, 
sim time next is 2474400.0000, 
raw observation next is [27.3, 85.33333333333334, 1.0, 2.0, 0.8093195828956886, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.983694348723276, 6.9112, 168.9125253293782, 2028116.612810653, 1976686.758047834, 410960.6091517977], 
processed observation next is [1.0, 0.6521739130434783, 0.4928909952606636, 0.8533333333333334, 1.0, 1.0, 0.770264557705649, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007249434872327587, 0.0, 0.829437827855938, 0.563365725780737, 0.5490796550132873, 0.6133740435101458], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.418655], dtype=float32), 0.8658317]. 
=============================================
[2019-03-26 19:41:53,589] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0360694e-16 9.9999905e-01 2.2746473e-17 9.7238558e-07 3.1912111e-13], sum to 1.0000
[2019-03-26 19:41:53,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8030
[2019-03-26 19:41:53,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2699822.083478765 W.
[2019-03-26 19:41:53,610] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.26666666666667, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.666592849268682, 6.9112, 168.9031309717789, 2699822.083478765, 1454558.420505565, 310093.9101274606], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [26.3, 89.0, 1.0, 2.0, 0.7797534484110238, 1.0, 1.0, 0.7797534484110238, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2180667.668756892, 2180667.668756892, 410126.8824782255], 
processed observation next is [1.0, 0.5652173913043478, 0.4454976303317536, 0.89, 1.0, 1.0, 0.7346427089289443, 1.0, 0.5, 0.7346427089289443, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6057410190991367, 0.6057410190991367, 0.6121296753406351], 
reward next is 0.3879, 
noisyNet noise sample is [array([-0.5884629], dtype=float32), 0.6322978]. 
=============================================
[2019-03-26 19:42:01,532] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4977617e-33 1.0000000e+00 2.7711888e-36 1.4983280e-16 1.9049901e-32], sum to 1.0000
[2019-03-26 19:42:01,544] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9322
[2019-03-26 19:42:01,550] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 90.66666666666667, 1.0, 2.0, 0.5097320774808656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712275.9414929863, 712275.9414929857, 185134.6251956974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2587800.0000, 
sim time next is 2588400.0000, 
raw observation next is [25.8, 91.0, 1.0, 2.0, 0.5082032329436343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710138.8910195903, 710138.8910195897, 184890.9914090777], 
processed observation next is [1.0, 1.0, 0.42180094786729866, 0.91, 1.0, 1.0, 0.40747377463088474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1972608030609973, 0.19726080306099714, 0.27595670359563834], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.783618], dtype=float32), 0.46894926]. 
=============================================
[2019-03-26 19:42:13,658] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3453255e-27 1.0000000e+00 2.4937099e-32 6.4866840e-14 2.3366572e-36], sum to 1.0000
[2019-03-26 19:42:13,668] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7125
[2019-03-26 19:42:13,675] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3733463671546196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575119.4601687302, 575119.4601687302, 172793.2798823809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2792400.0000, 
sim time next is 2793000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3799617438694608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585306.2526241723, 585306.2526241723, 173689.40024381], 
processed observation next is [1.0, 0.30434782608695654, 0.2417061611374408, 0.94, 1.0, 1.0, 0.25296595646922987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16258507017338117, 0.16258507017338117, 0.2592379108116567], 
reward next is 0.7408, 
noisyNet noise sample is [array([0.6886928], dtype=float32), -0.63968897]. 
=============================================
[2019-03-26 19:42:13,692] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[65.64075 ]
 [65.67388 ]
 [65.684135]
 [65.70428 ]
 [65.793076]], R is [[65.67507172]
 [65.76041412]
 [65.84661865]
 [65.93160248]
 [66.01382446]].
[2019-03-26 19:42:14,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.6039264e-25 1.0000000e+00 5.1307045e-29 2.0336833e-11 3.3154795e-27], sum to 1.0000
[2019-03-26 19:42:14,812] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0123
[2019-03-26 19:42:14,817] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.7713855513912249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1136277.973138236, 1136277.973138236, 244897.0288183452], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2824200.0000, 
sim time next is 2824800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.7195063087567697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1059854.113421362, 1059854.113421363, 232299.514397812], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.662055793682855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29440392039482277, 0.29440392039482305, 0.34671569313106265], 
reward next is 0.6533, 
noisyNet noise sample is [array([-0.6319139], dtype=float32), -0.23593672]. 
=============================================
[2019-03-26 19:42:16,250] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7007165e-34 1.0000000e+00 7.5789575e-37 5.3711811e-18 9.2245800e-35], sum to 1.0000
[2019-03-26 19:42:16,262] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0498
[2019-03-26 19:42:16,269] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 89.83333333333334, 1.0, 2.0, 0.40969293241241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604490.5301258386, 604490.5301258386, 174756.7883453504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2837400.0000, 
sim time next is 2838000.0000, 
raw observation next is [23.66666666666666, 90.66666666666667, 1.0, 2.0, 0.4064164472869222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600874.1257394182, 600874.1257394182, 174456.3596540048], 
processed observation next is [1.0, 0.8695652173913043, 0.3206951026856238, 0.9066666666666667, 1.0, 1.0, 0.28483909311677374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669094793720606, 0.1669094793720606, 0.2603826263492609], 
reward next is 0.7396, 
noisyNet noise sample is [array([-1.1139628], dtype=float32), 1.2764039]. 
=============================================
[2019-03-26 19:42:16,283] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[78.05537]
 [78.08289]
 [78.07933]
 [78.10026]
 [78.14607]], R is [[78.0320816 ]
 [77.99092865]
 [77.95000458]
 [77.90979767]
 [77.87019348]].
[2019-03-26 19:42:16,310] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0037104e-31 1.0000000e+00 1.2093358e-37 8.1260667e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 19:42:16,317] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2438
[2019-03-26 19:42:16,322] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.83333333333333, 89.83333333333334, 1.0, 2.0, 0.40969293241241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604490.5301258386, 604490.5301258386, 174756.7883453504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2837400.0000, 
sim time next is 2838000.0000, 
raw observation next is [23.66666666666666, 90.66666666666667, 1.0, 2.0, 0.4064164472869222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600874.1257394182, 600874.1257394182, 174456.3596540048], 
processed observation next is [1.0, 0.8695652173913043, 0.3206951026856238, 0.9066666666666667, 1.0, 1.0, 0.28483909311677374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669094793720606, 0.1669094793720606, 0.2603826263492609], 
reward next is 0.7396, 
noisyNet noise sample is [array([0.520211], dtype=float32), 0.11297345]. 
=============================================
[2019-03-26 19:42:16,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[77.99145 ]
 [78.01669 ]
 [78.01097 ]
 [78.029785]
 [78.073425]], R is [[77.9705658 ]
 [77.93003082]
 [77.8897171 ]
 [77.85011292]
 [77.81110382]].
[2019-03-26 19:42:18,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4672481e-29 1.0000000e+00 8.0439807e-35 1.5861587e-15 2.9205033e-27], sum to 1.0000
[2019-03-26 19:42:18,203] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2746
[2019-03-26 19:42:18,210] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3648769842953998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 562068.2695467542, 562068.2695467548, 171666.4570487527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2870400.0000, 
sim time next is 2871000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3708309786404282, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.1525737698, 571243.1525737704, 172456.0830141283], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.24196503450653997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15867865349271384, 0.15867865349271398, 0.25739713882705717], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.3411901], dtype=float32), -0.2737472]. 
=============================================
[2019-03-26 19:42:18,221] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[70.67777 ]
 [70.662186]
 [70.7569  ]
 [70.714905]
 [70.70116 ]], R is [[70.70983124]
 [70.74651337]
 [70.77906036]
 [70.8188858 ]
 [70.85820007]].
[2019-03-26 19:42:21,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.3902386e-30 1.0000000e+00 1.4375266e-35 9.6809884e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 19:42:21,568] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2216
[2019-03-26 19:42:21,574] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.31207332160021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495274.6924825286, 495274.6924825286, 166706.4719911926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2926800.0000, 
sim time next is 2927400.0000, 
raw observation next is [20.58333333333334, 96.50000000000001, 1.0, 2.0, 0.312656314054623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495910.2715004249, 495910.2715004249, 166748.6065074436], 
processed observation next is [1.0, 0.9130434782608695, 0.17456556082148533, 0.9650000000000002, 1.0, 1.0, 0.1718750771742446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13775285319456246, 0.13775285319456246, 0.24887851717528894], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.17568177], dtype=float32), -0.030081995]. 
=============================================
[2019-03-26 19:42:24,206] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.7220970e-31 1.0000000e+00 6.8098481e-29 6.0703527e-16 1.0213760e-26], sum to 1.0000
[2019-03-26 19:42:24,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6058
[2019-03-26 19:42:24,225] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.5808879828875657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912708.653975482, 912708.653975482, 209038.4272906284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [21.66666666666667, 90.0, 1.0, 2.0, 0.6884180546139048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1083074.385866981, 1083074.385866981, 232968.1364439665], 
processed observation next is [1.0, 0.5217391304347826, 0.22590837282780438, 0.9, 1.0, 1.0, 0.6246000657998853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008539960741614, 0.3008539960741614, 0.3477136364835321], 
reward next is 0.6523, 
noisyNet noise sample is [array([0.46639776], dtype=float32), 1.0256399]. 
=============================================
[2019-03-26 19:42:26,155] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2846470e-31 1.0000000e+00 0.0000000e+00 4.9183222e-16 1.4649784e-36], sum to 1.0000
[2019-03-26 19:42:26,163] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4002
[2019-03-26 19:42:26,168] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5037235837189078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796358.1181283569, 796358.1181283569, 194773.7812918411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2997000.0000, 
sim time next is 2997600.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5604322365157103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 886033.3068963632, 886033.3068963637, 205436.9159682793], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4704002849586871, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24612036302676754, 0.2461203630267677, 0.3066222626392228], 
reward next is 0.6934, 
noisyNet noise sample is [array([-0.11660741], dtype=float32), 1.4287704]. 
=============================================
[2019-03-26 19:42:30,707] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.0350336e-33 1.0000000e+00 6.6873414e-35 9.0568490e-16 4.4751984e-37], sum to 1.0000
[2019-03-26 19:42:30,719] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0610
[2019-03-26 19:42:30,725] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 97.0, 1.0, 2.0, 0.4176422193116753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613899.0158457832, 613899.0158457839, 175577.3353345396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3094200.0000, 
sim time next is 3094800.0000, 
raw observation next is [23.0, 96.0, 1.0, 2.0, 0.4113425000650328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607420.662355208, 607420.662355208, 175045.2776978148], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.96, 1.0, 1.0, 0.29077409646389496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16872796176533555, 0.16872796176533555, 0.2612616085042012], 
reward next is 0.7387, 
noisyNet noise sample is [array([0.5821513], dtype=float32), 0.080227174]. 
=============================================
[2019-03-26 19:42:33,178] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.22499246e-31 1.00000000e+00 1.02839446e-32 4.00250918e-14
 1.90168981e-36], sum to 1.0000
[2019-03-26 19:42:33,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6162
[2019-03-26 19:42:33,196] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 99.0, 1.0, 2.0, 0.3832877370347389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576040.0824591557, 576040.0824591557, 172482.5858854541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3120600.0000, 
sim time next is 3121200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3798861857858509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571866.868337525, 571866.868337525, 172141.3025700911], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2528749226335553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15885190787153475, 0.15885190787153475, 0.2569273172687927], 
reward next is 0.7431, 
noisyNet noise sample is [array([-0.8677574], dtype=float32), -0.6993675]. 
=============================================
[2019-03-26 19:42:40,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1204451e-33 1.0000000e+00 1.0087588e-37 2.7633710e-18 6.1972635e-23], sum to 1.0000
[2019-03-26 19:42:40,763] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2010
[2019-03-26 19:42:40,768] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.5970997092513171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834407.3656158712, 834407.3656158712, 200207.3675610389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3260400.0000, 
sim time next is 3261000.0000, 
raw observation next is [31.16666666666667, 74.33333333333333, 1.0, 2.0, 0.5986154564799745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 836526.3562738885, 836526.3562738892, 200488.5665501464], 
processed observation next is [0.0, 0.7391304347826086, 0.6761453396524489, 0.7433333333333333, 1.0, 1.0, 0.5164041644337042, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23236843229830237, 0.23236843229830256, 0.2992366664927558], 
reward next is 0.7008, 
noisyNet noise sample is [array([-0.55436933], dtype=float32), 0.5791867]. 
=============================================
[2019-03-26 19:42:40,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.83919 ]
 [73.75872 ]
 [73.678116]
 [73.602646]
 [73.53036 ]], R is [[73.86384583]
 [73.8263855 ]
 [73.78914642]
 [73.75196075]
 [73.71491241]].
[2019-03-26 19:42:40,982] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1632782e-36 1.0000000e+00 0.0000000e+00 4.5986918e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 19:42:40,991] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7261
[2019-03-26 19:42:40,997] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4875924157084424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681329.0846189507, 681329.0846189507, 181674.5114991432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3280800.0000, 
sim time next is 3281400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4870881078076754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680624.172679582, 680624.172679582, 181597.4172454706], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3820338648285246, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18906227018877278, 0.18906227018877278, 0.27104092126189644], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.35009933], dtype=float32), 1.2153106]. 
=============================================
[2019-03-26 19:42:43,024] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 19:42:43,026] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:42:43,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:42:43,027] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:42:43,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:42:43,029] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:42:43,030] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:42:43,031] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:42:43,031] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:42:43,032] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:42:43,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:42:43,050] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-03-26 19:42:43,068] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-03-26 19:42:43,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-03-26 19:42:43,090] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-03-26 19:42:43,131] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-03-26 19:42:50,714] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:42:50,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.83333333333334, 75.66666666666667, 1.0, 2.0, 0.7442067099726382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1060081.550339031, 1060081.550339031, 233407.1373075156]
[2019-03-26 19:42:50,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:42:50,719] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.0492504e-30 1.0000000e+00 2.0455011e-34 4.8568096e-18 1.3269323e-33], sampled 0.07112318957405939
[2019-03-26 19:42:52,094] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:42:52,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.68382343, 72.73897732, 1.0, 2.0, 0.3083129118766029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506326.2109428213, 506326.2109428207, 167248.3394750218]
[2019-03-26 19:42:52,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:42:52,099] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1034146e-31 1.0000000e+00 8.2610685e-36 9.2809179e-19 5.7659484e-35], sampled 0.2447616610626966
[2019-03-26 19:43:10,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:43:10,219] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.47269644, 83.27025453, 1.0, 2.0, 0.3514755733548119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572416.5011928972, 572416.5011928972, 172595.4562828523]
[2019-03-26 19:43:10,221] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:43:10,226] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.6850189e-31 1.0000000e+00 1.6577039e-35 1.3291563e-18 1.1387882e-34], sampled 0.9626585381552712
[2019-03-26 19:43:21,634] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:43:21,634] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.491495855, 85.559088255, 1.0, 2.0, 0.5236334838446604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731707.8117310618, 731707.8117310624, 187380.256771968]
[2019-03-26 19:43:21,635] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:43:21,640] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0243896e-31 1.0000000e+00 2.3077609e-36 4.8081041e-19 1.6582858e-35], sampled 0.06442424777772093
[2019-03-26 19:43:23,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:43:23,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 89.0, 1.0, 2.0, 0.763174694854204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1066602.718238556, 1066602.718238555, 235072.206888042]
[2019-03-26 19:43:23,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:43:23,251] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5616093e-30 1.0000000e+00 9.3694172e-35 3.2470293e-18 6.1872609e-34], sampled 0.8580870777802683
[2019-03-26 19:43:23,324] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:43:23,325] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 83.0, 1.0, 2.0, 0.8153726605854218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1193582.024290414, 1193582.024290413, 255233.7849349417]
[2019-03-26 19:43:23,326] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:43:23,329] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7725153e-29 1.0000000e+00 8.6749233e-34 1.0231394e-17 5.4451194e-33], sampled 0.21651215105791488
[2019-03-26 19:43:33,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:43:33,374] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.99225129, 75.56929797, 1.0, 2.0, 0.6042691654356707, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 844430.1892874138, 844430.1892874131, 201544.005419832]
[2019-03-26 19:43:33,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:43:33,378] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1885689e-32 1.0000000e+00 3.9081349e-37 1.9242405e-19 2.9243249e-36], sampled 0.2587762342092116
[2019-03-26 19:44:31,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.51371366]
[2019-03-26 19:44:31,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.48333333333333, 69.16666666666667, 1.0, 2.0, 0.5819903225922749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813284.9130822163, 813284.9130822163, 197438.5707288102]
[2019-03-26 19:44:31,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:44:31,877] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6686887e-32 1.0000000e+00 2.8605398e-37 1.6382167e-19 2.1557520e-36], sampled 0.7766475795040921
[2019-03-26 19:44:37,043] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8875 3164502873.7783 1778.0000
[2019-03-26 19:44:37,261] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1744 2927914232.3319 1338.0000
[2019-03-26 19:44:37,297] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4620 2779798268.0133 933.0000
[2019-03-26 19:44:37,409] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 19:44:37,444] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.4169 3008223289.7646 1765.0000
[2019-03-26 19:44:38,459] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 300000, evaluation results [300000.0, 7878.887539409773, 3164502873.778303, 1778.0, 8254.174442635573, 2927914232.3318715, 1338.0, 8660.46199861073, 2779798268.0133233, 933.0, 7998.416857356147, 3008223289.7646136, 1765.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 19:44:46,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2481934e-35 1.0000000e+00 0.0000000e+00 3.1223142e-19 1.0686597e-32], sum to 1.0000
[2019-03-26 19:44:46,386] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9897
[2019-03-26 19:44:46,390] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5157695029439897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 186104.0597786985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3440400.0000, 
sim time next is 3441000.0000, 
raw observation next is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.516028016286732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721076.5801110404, 721076.5801110397, 186145.6822240083], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7816666666666667, 1.0, 1.0, 0.41690122444184574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20029905003084456, 0.20029905003084436, 0.2778293764537437], 
reward next is 0.7222, 
noisyNet noise sample is [array([-0.5907329], dtype=float32), -2.2389314]. 
=============================================
[2019-03-26 19:44:46,413] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.43208 ]
 [69.16756 ]
 [69.883705]
 [70.524956]
 [70.8454  ]], R is [[67.57077026]
 [67.61729431]
 [67.66313934]
 [67.70825195]
 [67.75255585]].
[2019-03-26 19:44:46,699] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1548909e-28 1.0000000e+00 1.3266691e-33 4.8311272e-17 6.6071981e-32], sum to 1.0000
[2019-03-26 19:44:46,708] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8215
[2019-03-26 19:44:46,714] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5078598009336115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709658.8352535367, 709658.8352535367, 184836.8505617697], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3459600.0000, 
sim time next is 3460200.0000, 
raw observation next is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5074331952403807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709062.5180404293, 709062.5180404286, 184768.9709951507], 
processed observation next is [1.0, 0.043478260869565216, 0.470774091627172, 0.8483333333333333, 1.0, 1.0, 0.4065460183619044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19696181056678594, 0.19696181056678574, 0.2757745835748518], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.7196079], dtype=float32), 0.836275]. 
=============================================
[2019-03-26 19:44:56,967] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9862702e-27 1.0000000e+00 1.7346059e-32 4.1312735e-17 2.6211959e-30], sum to 1.0000
[2019-03-26 19:44:56,977] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3435
[2019-03-26 19:44:56,980] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.5234008158471993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731382.5773714219, 731382.5773714213, 187343.8294250153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3615600.0000, 
sim time next is 3616200.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5245965319221539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733054.0070592929, 733054.0070592923, 187539.6007289533], 
processed observation next is [1.0, 0.8695652173913043, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4272247372556071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2036261130720258, 0.20362611307202563, 0.27990985183425865], 
reward next is 0.7201, 
noisyNet noise sample is [array([1.5712075], dtype=float32), 0.25470337]. 
=============================================
[2019-03-26 19:44:58,639] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0693717e-35 1.0000000e+00 1.1333352e-36 1.8795317e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 19:44:58,650] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5235
[2019-03-26 19:44:58,654] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6748815702130709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 943150.6355299543, 943150.6355299538, 215513.4039092174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3643800.0000, 
sim time next is 3644400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6450945540044076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901505.4521294803, 901505.4521294803, 209436.7986959239], 
processed observation next is [1.0, 0.17391304347826086, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.5724030771137442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25041818114707787, 0.25041818114707787, 0.3125922368595879], 
reward next is 0.6874, 
noisyNet noise sample is [array([1.2213316], dtype=float32), 0.58232194]. 
=============================================
[2019-03-26 19:45:02,321] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.0210183e-33 1.0000000e+00 9.9412111e-36 2.9639517e-18 2.5983218e-35], sum to 1.0000
[2019-03-26 19:45:02,329] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3945
[2019-03-26 19:45:02,338] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 76.5, 1.0, 2.0, 0.5105657294396888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713441.2392319824, 713441.2392319817, 185267.6695008594], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3706200.0000, 
sim time next is 3706800.0000, 
raw observation next is [28.0, 75.66666666666666, 1.0, 2.0, 0.5054199822890488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706248.4168026005, 706248.4168026012, 184449.2617156579], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.7566666666666666, 1.0, 1.0, 0.4041204605892154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19618011577850014, 0.19618011577850034, 0.27529740554575804], 
reward next is 0.7247, 
noisyNet noise sample is [array([1.4876658], dtype=float32), 1.4533738]. 
=============================================
[2019-03-26 19:45:13,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2916018e-36 1.0000000e+00 0.0000000e+00 1.3977656e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 19:45:13,040] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4142
[2019-03-26 19:45:13,044] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 92.33333333333334, 1.0, 2.0, 0.5751620359736044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 803739.3186089606, 803739.3186089612, 196209.1342637642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [27.16666666666666, 93.16666666666667, 1.0, 2.0, 0.5728608688741702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800522.4243562263, 800522.4243562256, 195798.149770918], 
processed observation next is [0.0, 0.17391304347826086, 0.4865718799368086, 0.9316666666666668, 1.0, 1.0, 0.48537454081225323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22236734009895176, 0.22236734009895157, 0.292236044434206], 
reward next is 0.7078, 
noisyNet noise sample is [array([-1.5463502], dtype=float32), -0.4189116]. 
=============================================
[2019-03-26 19:45:13,415] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 5.70834e-37 0.00000e+00], sum to 1.0000
[2019-03-26 19:45:13,424] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6398
[2019-03-26 19:45:13,431] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.33333333333333, 1.0, 2.0, 0.5572060011874463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778638.1123257191, 778638.1123257191, 193045.2620676647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
processed observation next is [0.0, 0.782608695652174, 0.7235387045813582, 0.6516666666666667, 1.0, 1.0, 0.4747250180793821, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21893499314501716, 0.218934993145017, 0.28990387343795404], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.2908206], dtype=float32), 2.6171114]. 
=============================================
[2019-03-26 19:45:18,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.6074606e-23 1.0000000e+00 4.5270216e-28 3.2151431e-18 5.4052948e-28], sum to 1.0000
[2019-03-26 19:45:19,004] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0713
[2019-03-26 19:45:19,012] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2138510.552105604 W.
[2019-03-26 19:45:19,018] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5097960844704535, 1.0, 2.0, 0.5097960844704535, 1.0, 1.0, 0.8853472065838045, 6.9112, 6.9112, 170.5573041426782, 2138510.552105604, 2138510.552105604, 421986.882642235], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3984000.0000, 
sim time next is 3984600.0000, 
raw observation next is [29.83333333333333, 79.83333333333334, 1.0, 2.0, 0.716816297697881, 1.0, 2.0, 0.716816297697881, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2004492.215103357, 2004492.215103357, 381414.6146209703], 
processed observation next is [1.0, 0.08695652173913043, 0.6129541864139019, 0.7983333333333335, 1.0, 1.0, 0.6588148165034711, 1.0, 1.0, 0.6588148165034711, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5568033930842659, 0.5568033930842659, 0.5692755442104034], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3939469], dtype=float32), 0.5625742]. 
=============================================
[2019-03-26 19:45:25,166] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6308588e-33 1.0000000e+00 1.9747977e-37 8.9287996e-29 3.0781463e-32], sum to 1.0000
[2019-03-26 19:45:25,175] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8475
[2019-03-26 19:45:25,181] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.735177323856685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027455.038609861, 1027455.038609861, 228622.2823998151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4089600.0000, 
sim time next is 4090200.0000, 
raw observation next is [28.33333333333334, 83.16666666666667, 1.0, 2.0, 0.7867123272419863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1099515.62842214, 1099515.62842214, 240676.6542889805], 
processed observation next is [1.0, 0.34782608695652173, 0.5418641390205374, 0.8316666666666667, 1.0, 1.0, 0.7430269002915497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30542100789503884, 0.30542100789503884, 0.35921888699847837], 
reward next is 0.6408, 
noisyNet noise sample is [array([-0.03473308], dtype=float32), 1.2951]. 
=============================================
[2019-03-26 19:45:26,126] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0878073e-10 9.9999166e-01 1.9098988e-11 8.3716441e-06 1.4221298e-10], sum to 1.0000
[2019-03-26 19:45:26,135] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1287
[2019-03-26 19:45:26,142] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3852165.997336903 W.
[2019-03-26 19:45:26,146] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 69.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.225852549125783, 6.9112, 170.5573041426782, 3852165.997336903, 2910426.89975149, 545756.9849536415], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [34.0, 68.33333333333333, 1.0, 2.0, 0.9742068934888737, 1.0, 2.0, 0.8076934862586996, 1.0, 1.0, 1.03, 7.005119363082537, 6.9112, 170.5573041426782, 3389853.247157119, 3322574.986985669, 621944.0089672077], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6833333333333332, 1.0, 1.0, 0.9689239680588839, 1.0, 1.0, 0.7683054051309633, 1.0, 0.5, 1.0365853658536586, 0.0093919363082537, 0.0, 0.8375144448122397, 0.9416259019880887, 0.9229374963849081, 0.9282746402495637], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02953551], dtype=float32), 1.3760713]. 
=============================================
[2019-03-26 19:45:26,161] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[25.091906]
 [26.44021 ]
 [29.45098 ]
 [30.03932 ]
 [29.54587 ]], R is [[23.18455887]
 [22.95271301]
 [22.72318649]
 [22.49595451]
 [22.27099609]].
[2019-03-26 19:45:27,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7091091e-06 9.8271126e-01 9.0482260e-08 1.7271068e-02 1.4912298e-05], sum to 1.0000
[2019-03-26 19:45:27,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1357
[2019-03-26 19:45:27,273] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3333206.180504852 W.
[2019-03-26 19:45:27,278] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 67.66666666666667, 1.0, 2.0, 0.9472485110458276, 1.0, 2.0, 0.7942142950371762, 1.0, 1.0, 1.03, 7.005117235575027, 6.9112, 170.5573041426782, 3333206.180504852, 3265929.444353572, 610808.1087722722], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4110600.0000, 
sim time next is 4111200.0000, 
raw observation next is [34.0, 67.0, 1.0, 2.0, 0.8821419030000531, 1.0, 2.0, 0.7616609910142892, 1.0, 2.0, 1.03, 7.0051120982729, 6.9112, 170.5573041426782, 3196409.667867403, 3129136.611774609, 585034.6356048723], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.67, 1.0, 1.0, 0.8580022927711484, 1.0, 1.0, 0.7128445674870953, 1.0, 1.0, 1.0365853658536586, 0.00939120982728996, 0.0, 0.8375144448122397, 0.8878915744076119, 0.8692046143818358, 0.8731860232908542], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2528922], dtype=float32), 1.088693]. 
=============================================
[2019-03-26 19:45:34,142] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 19:45:34,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:45:34,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:45:34,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:45:34,144] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:45:34,145] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:45:34,146] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:45:34,145] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:45:34,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:45:34,146] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:45:34,149] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:45:34,167] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-03-26 19:45:34,186] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-03-26 19:45:34,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-03-26 19:45:34,209] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-03-26 19:45:34,248] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-03-26 19:45:47,498] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:45:47,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.9, 95.0, 1.0, 2.0, 0.3601322940614678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 554301.9946029171, 554301.9946029171, 170995.0183208259]
[2019-03-26 19:45:47,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:45:47,501] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5216041e-32 1.0000000e+00 3.9912578e-36 9.3555790e-31 2.0450481e-27], sampled 0.4383402653231283
[2019-03-26 19:45:58,694] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:45:58,696] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.86666666666667, 81.50000000000001, 1.0, 2.0, 0.6295651609637904, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.959298778426604, 6.9112, 168.9125992260001, 1760318.353819709, 1726195.49714845, 371015.8318274947]
[2019-03-26 19:45:58,699] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:45:58,700] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8605958e-19 1.0000000e+00 2.2621667e-21 1.5672309e-15 3.2679493e-14], sampled 0.37039714437907534
[2019-03-26 19:45:58,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1760318.353819709 W.
[2019-03-26 19:46:10,804] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:46:10,808] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.40457560666667, 98.5662635, 1.0, 2.0, 0.8659336775693491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104075, 1210299.142106853, 1210299.142106852, 260735.4812088263]
[2019-03-26 19:46:10,809] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:46:10,812] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2192305e-20 1.0000000e+00 6.0571921e-23 2.3978430e-10 2.6789493e-10], sampled 0.5240895383972363
[2019-03-26 19:46:14,687] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:46:14,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.98001037333333, 93.21646902333333, 1.0, 2.0, 0.4069499413355669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 605985.2979856096, 605985.2979856103, 175053.9793712816]
[2019-03-26 19:46:14,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:46:14,693] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.8775030e-30 1.0000000e+00 8.0612179e-34 8.5018685e-29 1.1462462e-25], sampled 0.04245320374227601
[2019-03-26 19:46:33,049] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:46:33,050] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.10686914, 71.435059215, 1.0, 2.0, 0.7215329372138084, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976628009609, 6.9112, 168.9123160332099, 1905261.424999761, 1838023.878079493, 389395.3592577161]
[2019-03-26 19:46:33,052] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:46:33,054] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6048074e-14 9.9429792e-01 2.2766207e-16 5.2363891e-03 4.6568571e-04], sampled 0.10610690707395731
[2019-03-26 19:46:33,055] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1905261.424999761 W.
[2019-03-26 19:46:54,594] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:46:54,595] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.492323065, 66.514592055, 1.0, 2.0, 0.581137047127627, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004250358055401, 6.9112, 6.9112, 168.9129383911252, 1624805.523004652, 1624805.523004652, 354615.3008448206]
[2019-03-26 19:46:54,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:46:54,601] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3572859e-18 1.0000000e+00 4.4710712e-21 3.6529015e-13 1.9204232e-12], sampled 0.23993211212275944
[2019-03-26 19:47:13,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:47:13,956] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.93333333333333, 73.66666666666667, 1.0, 2.0, 0.503212935920093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703163.3806820402, 703163.3806820402, 184099.908005244]
[2019-03-26 19:47:13,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:47:13,960] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6154895e-32 1.0000000e+00 7.8576675e-36 1.6633774e-30 3.4185588e-27], sampled 0.18889431173031113
[2019-03-26 19:47:17,815] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:47:17,816] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.45, 91.5, 1.0, 2.0, 0.3612032419229793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 553953.6647652133, 553953.6647652133, 170909.5404931365]
[2019-03-26 19:47:17,817] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:47:17,820] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2150861e-32 1.0000000e+00 1.0967207e-35 2.2080429e-30 4.4022606e-27], sampled 0.31956748568737814
[2019-03-26 19:47:19,511] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.43806976]
[2019-03-26 19:47:19,512] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.9, 55.0, 1.0, 2.0, 0.8972157904570675, 1.0, 2.0, 0.8972157904570675, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2509522.032018709, 2509522.032018709, 469698.6235986202]
[2019-03-26 19:47:19,512] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:47:19,516] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2496717e-14 9.9999905e-01 1.2696066e-16 5.5050185e-07 4.0523886e-07], sampled 0.8856667711983233
[2019-03-26 19:47:19,517] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2509522.032018709 W.
[2019-03-26 19:47:28,042] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.3246 3164148779.3123 1762.0000
[2019-03-26 19:47:28,166] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8269.3626 2926459124.5068 1299.0000
[2019-03-26 19:47:28,465] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.1387 3007612908.3438 1753.0000
[2019-03-26 19:47:28,537] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.8413 2779382418.2493 923.0000
[2019-03-26 19:47:28,623] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.8946 2842777660.4116 1125.0000
[2019-03-26 19:47:29,638] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 325000, evaluation results [325000.0, 7881.324609850836, 3164148779.3123465, 1762.0, 8269.362620689772, 2926459124.5068283, 1299.0, 8663.841263997903, 2779382418.249261, 923.0, 8002.138742436081, 3007612908.3437524, 1753.0, 8499.894645795499, 2842777660.411625, 1125.0]
[2019-03-26 19:47:31,985] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0577947e-11 7.1141609e-10 9.9226113e-11 1.1636207e-02 9.8836374e-01], sum to 1.0000
[2019-03-26 19:47:31,994] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6454
[2019-03-26 19:47:32,000] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.33333333333334, 73.66666666666667, 1.0, 2.0, 0.7467456834714521, 1.0, 2.0, 0.6939628812499886, 1.0, 2.0, 1.03, 7.005101418299161, 6.9112, 170.5573041426782, 2911974.52614532, 2844709.120552149, 536502.3453087397], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [33.5, 73.0, 1.0, 2.0, 0.7490925587988072, 1.0, 2.0, 0.6951363189136662, 1.0, 2.0, 1.03, 7.005101603377903, 6.9112, 170.5573041426782, 2916904.199464536, 2849638.661291933, 537285.4663738246], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.73, 1.0, 1.0, 0.697701878070852, 1.0, 1.0, 0.6326943601369472, 1.0, 1.0, 1.0365853658536586, 0.009390160337790299, 0.0, 0.8375144448122397, 0.8102511665179266, 0.7915662948033148, 0.8019186065280964], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0743437], dtype=float32), 0.5862135]. 
=============================================
[2019-03-26 19:47:37,236] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2506384e-04 7.1869581e-04 1.2229119e-03 1.0030880e-01 8.9702451e-01], sum to 1.0000
[2019-03-26 19:47:37,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8054
[2019-03-26 19:47:37,249] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666667, 68.66666666666667, 1.0, 2.0, 0.6709408803613333, 1.0, 2.0, 0.6560604796949293, 1.0, 2.0, 1.03, 7.005095440983322, 6.9112, 170.5573041426782, 2752755.006293586, 2685493.882494893, 512308.195355847], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4371600.0000, 
sim time next is 4372200.0000, 
raw observation next is [33.0, 65.5, 1.0, 2.0, 0.6443549842063873, 1.0, 2.0, 0.6427675316174563, 1.0, 2.0, 1.03, 7.005093345008496, 6.9112, 170.5573041426782, 2696919.107918584, 2629659.485551917, 504328.5953787944], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.655, 1.0, 1.0, 0.571512029164322, 1.0, 1.0, 0.5695994356836822, 1.0, 1.0, 1.0365853658536586, 0.0093893345008496, 0.0, 0.8375144448122397, 0.7491441966440511, 0.7304609682088659, 0.7527292468340215], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.78302234], dtype=float32), -1.1834501]. 
=============================================
[2019-03-26 19:47:39,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.13753286e-35 1.00000000e+00 0.00000000e+00 3.57529644e-37
 2.30727498e-30], sum to 1.0000
[2019-03-26 19:47:39,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0596
[2019-03-26 19:47:39,519] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5853686592339303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 818007.6861553146, 818007.6861553151, 198051.4949600791], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4422000.0000, 
sim time next is 4422600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5831738709746941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814939.4617405524, 814939.4617405524, 197652.9735506554], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49779984454782417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22637207270570903, 0.22637207270570903, 0.2950044381353066], 
reward next is 0.7050, 
noisyNet noise sample is [array([-1.0447993], dtype=float32), 0.9400379]. 
=============================================
[2019-03-26 19:47:39,757] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.6997981e-37 1.0000000e+00 0.0000000e+00 1.5941077e-34 6.9353113e-31], sum to 1.0000
[2019-03-26 19:47:39,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4729
[2019-03-26 19:47:39,772] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6162549675906934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861186.4247549446, 861186.4247549446, 203813.6635075772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4413600.0000, 
sim time next is 4414200.0000, 
raw observation next is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.617766594828498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863299.7090065364, 863299.709006537, 204102.6997208977], 
processed observation next is [0.0, 0.08695652173913043, 0.6129541864139019, 0.8483333333333333, 1.0, 1.0, 0.539477825094576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23980547472403788, 0.23980547472403804, 0.3046308951058175], 
reward next is 0.6954, 
noisyNet noise sample is [array([0.422734], dtype=float32), 0.6291323]. 
=============================================
[2019-03-26 19:47:40,877] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.2903444e-34 1.0000000e+00 5.6146554e-37 2.7487361e-33 6.0591905e-27], sum to 1.0000
[2019-03-26 19:47:40,888] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2446
[2019-03-26 19:47:40,892] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5821232008402634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813470.6709085847, 813470.6709085847, 197462.7016949702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4429200.0000, 
sim time next is 4429800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5814528363471861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812533.5317684455, 812533.5317684455, 197341.4723990394], 
processed observation next is [0.0, 0.2608695652173913, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49572630885203145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2257037588245682, 0.2257037588245682, 0.29453951104334236], 
reward next is 0.7055, 
noisyNet noise sample is [array([-1.174348], dtype=float32), 0.07772045]. 
=============================================
[2019-03-26 19:47:48,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 1.5159724e-31], sum to 1.0000
[2019-03-26 19:47:48,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3233
[2019-03-26 19:47:48,577] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5498011070409204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768286.7916807865, 768286.791680786, 191766.3772246501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4575600.0000, 
sim time next is 4576200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507766209849794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769650.4593724336, 769650.4593724343, 191933.6903364013], 
processed observation next is [0.0, 1.0, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45876701323491487, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21379179427012043, 0.21379179427012063, 0.2864681945319423], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.7951032], dtype=float32), 0.4419109]. 
=============================================
[2019-03-26 19:47:49,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4226297e-18 1.0000000e+00 1.5283897e-20 2.1762632e-18 1.1964194e-14], sum to 1.0000
[2019-03-26 19:47:49,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1028
[2019-03-26 19:47:49,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 94.0, 1.0, 2.0, 0.9628427586335631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564741287, 1345832.983168381, 1345832.98316838, 287810.8730659269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4595400.0000, 
sim time next is 4596000.0000, 
raw observation next is [27.0, 94.0, 1.0, 2.0, 0.9548348081645575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104219, 1334632.672216664, 1334632.672216664, 285468.5487487307], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.94, 1.0, 1.0, 0.9455841062223583, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829439945152258, 0.3707312978379622, 0.3707312978379622, 0.426072460819001], 
reward next is 0.5739, 
noisyNet noise sample is [array([-1.3424602], dtype=float32), 0.16611782]. 
=============================================
[2019-03-26 19:47:49,839] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[43.090946]
 [42.20814 ]
 [41.13074 ]
 [39.02421 ]
 [37.759895]], R is [[43.94240189]
 [44.07341003]
 [44.18955994]
 [43.74766541]
 [43.31018829]].
[2019-03-26 19:47:49,953] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.5600287e-21 1.0000000e+00 1.7534098e-21 1.4861310e-21 7.2943123e-15], sum to 1.0000
[2019-03-26 19:47:49,963] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4659
[2019-03-26 19:47:49,968] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 94.0, 1.0, 2.0, 0.9976301526291429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9127890931008, 1394489.705403513, 1394489.705403513, 298209.3153583678], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4600800.0000, 
sim time next is 4601400.0000, 
raw observation next is [28.16666666666667, 93.16666666666667, 1.0, 2.0, 0.9919741131948946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564686264, 1386578.516435549, 1386578.516435549, 296494.8538506472], 
processed observation next is [1.0, 0.2608695652173913, 0.5339652448657191, 0.9316666666666668, 1.0, 1.0, 0.9903302568613188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399449470229, 0.3851606990098747, 0.3851606990098747, 0.4425296326129063], 
reward next is 0.5575, 
noisyNet noise sample is [array([0.34263092], dtype=float32), 1.5581036]. 
=============================================
[2019-03-26 19:47:50,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9172914e-13 1.0000000e+00 7.2090317e-14 7.8758408e-13 2.8934888e-09], sum to 1.0000
[2019-03-26 19:47:50,414] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2656
[2019-03-26 19:47:50,418] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.917175427952253, 6.9112, 168.9126476262729, 1457996.99829592, 1453757.831800237, 311356.4027389151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4608000.0000, 
sim time next is 4608600.0000, 
raw observation next is [30.33333333333333, 82.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.876816102995563, 6.9112, 168.9075742834826, 2139243.665847994, 1454224.196534584, 311356.9084134893], 
processed observation next is [1.0, 0.34782608695652173, 0.6366508688783569, 0.825, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0965616102995563, 0.0, 0.8294135159539919, 0.5942343516244427, 0.4039511657040511, 0.4647118036022228], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4110755], dtype=float32), -0.5229207]. 
=============================================
[2019-03-26 19:47:51,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5007288e-04 9.9906439e-01 1.5920226e-04 1.3799885e-04 4.8824260e-04], sum to 1.0000
[2019-03-26 19:47:51,697] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0030
[2019-03-26 19:47:51,705] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2811337.727411381 W.
[2019-03-26 19:47:51,710] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6988333807667875, 1.0, 2.0, 0.6700067298976563, 1.0, 1.0, 1.03, 7.005097640169315, 6.9112, 170.5573041426782, 2811337.727411381, 2744075.028246303, 520962.1331197109], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4611600.0000, 
sim time next is 4612200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.950085284387321, 1.0, 2.0, 0.950085284387321, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2657531.88098107, 2657531.88098107, 499564.4930825355], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.939861788418459, 1.0, 1.0, 0.939861788418459, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7382033002725195, 0.7382033002725195, 0.745618646391844], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1803639], dtype=float32), -0.7056616]. 
=============================================
[2019-03-26 19:47:53,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.6903483e-14 9.9999905e-01 3.2297274e-13 1.6125538e-10 9.0923379e-07], sum to 1.0000
[2019-03-26 19:47:53,600] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7105
[2019-03-26 19:47:53,606] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.3967171986199033, 0.0, 1.0, 0.0, 1.0, 2.0, 0.6888477908373848, 6.9112, 6.9112, 168.9129440116671, 1108915.354420224, 1108915.354420224, 258131.7454598161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4641000.0000, 
sim time next is 4641600.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5250169095498837, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.91295650731, 733641.6318382611, 733641.6318382606, 187611.8095142981], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.42773121632516103, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451369772, 0.20378934217729477, 0.2037893421772946, 0.28001762614074344], 
reward next is 0.7200, 
noisyNet noise sample is [array([0.85167927], dtype=float32), 0.54127043]. 
=============================================
[2019-03-26 19:47:55,921] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4462679e-32 1.0000000e+00 3.0253827e-28 4.8975194e-32 5.0390409e-27], sum to 1.0000
[2019-03-26 19:47:55,931] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8910
[2019-03-26 19:47:55,938] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6436963845000729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 899550.7135327054, 899550.7135327049, 209161.7714681792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4686000.0000, 
sim time next is 4686600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6461895460635244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903036.3300000359, 903036.3300000359, 209659.2655917867], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5737223446548486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25084342500001, 0.25084342500001, 0.31292427700266673], 
reward next is 0.6871, 
noisyNet noise sample is [array([-0.22927539], dtype=float32), -0.024042068]. 
=============================================
[2019-03-26 19:47:57,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6800098e-21 1.0000000e+00 2.2730464e-23 2.3950927e-20 2.2005286e-20], sum to 1.0000
[2019-03-26 19:47:57,164] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4480
[2019-03-26 19:47:57,169] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.7939864652292251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1109687.335726188, 1109687.335726188, 242440.7364978558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4690200.0000, 
sim time next is 4690800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.8087786723191052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1130372.149146386, 1130372.149146385, 246077.820539602], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.84, 1.0, 1.0, 0.7696128582157894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31399226365177385, 0.31399226365177363, 0.36728032916358505], 
reward next is 0.6327, 
noisyNet noise sample is [array([1.4633604], dtype=float32), 0.35097796]. 
=============================================
[2019-03-26 19:48:08,485] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.0621204e-17 4.3904921e-10 5.4360787e-15 9.7778119e-10 1.0000000e+00], sum to 1.0000
[2019-03-26 19:48:08,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9181
[2019-03-26 19:48:08,499] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5184535352259216, 1.0, 2.0, 0.5184535352259216, 1.0, 2.0, 0.8979624925126707, 6.9112, 6.9112, 170.5573041426782, 2174863.999472658, 2174863.999472658, 427687.2658119448], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4879200.0000, 
sim time next is 4879800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5221109210202572, 1.0, 2.0, 0.5221109210202572, 1.0, 2.0, 0.9044955144469603, 6.9112, 6.9112, 170.5573041426782, 2190222.076022087, 2190222.076022087, 430349.4051160432], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4242300253256111, 1.0, 1.0, 0.4242300253256111, 1.0, 1.0, 0.8835311151792198, 0.0, 0.0, 0.8375144448122397, 0.6083950211172464, 0.6083950211172464, 0.6423125449493182], 
reward next is 0.3577, 
noisyNet noise sample is [array([1.0838884], dtype=float32), -0.17199343]. 
=============================================
[2019-03-26 19:48:08,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2093091e-28 1.0000000e+00 3.0156824e-27 1.1036910e-29 1.6447729e-19], sum to 1.0000
[2019-03-26 19:48:08,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7966
[2019-03-26 19:48:08,776] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4994365494971811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697884.722825183, 697884.722825183, 183507.6516266482], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.815, 1.0, 1.0, 0.39691150541829046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1938568674514397, 0.1938568674514397, 0.2738920173532063], 
reward next is 0.7261, 
noisyNet noise sample is [array([1.750205], dtype=float32), -0.18638344]. 
=============================================
[2019-03-26 19:48:25,288] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 19:48:25,291] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:48:25,291] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:48:25,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:48:25,294] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:48:25,296] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:48:25,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:48:25,299] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:48:25,299] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:48:25,297] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:48:25,302] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:48:25,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-03-26 19:48:25,314] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-03-26 19:48:25,333] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-03-26 19:48:25,368] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-03-26 19:48:25,386] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-03-26 19:48:38,331] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:48:38,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.0, 92.0, 1.0, 2.0, 0.3515970389853695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 545285.1096504586, 545285.1096504593, 170352.5879151331]
[2019-03-26 19:48:38,333] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:48:38,336] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2426864e-34 1.0000000e+00 2.0310632e-33 1.7869827e-37 4.7633915e-28], sampled 0.2153306392471368
[2019-03-26 19:48:53,436] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:48:53,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.96666666666667, 87.66666666666667, 1.0, 2.0, 0.5133279620350568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733619.3829110729, 733619.3829110729, 187791.8007409923]
[2019-03-26 19:48:53,438] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:48:53,441] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6856761e-31 1.0000000e+00 3.9547448e-30 2.3284300e-33 6.6762655e-24], sampled 0.5719029002885039
[2019-03-26 19:49:09,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:49:09,477] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.86666666666667, 54.5, 1.0, 2.0, 0.4237375895234432, 1.0, 2.0, 0.4237375895234432, 1.0, 2.0, 0.7358920608400205, 6.911200000000001, 6.9112, 169.0403247858759, 1777223.15414692, 1777223.154146919, 366122.804971087]
[2019-03-26 19:49:09,478] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:49:09,480] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6185685e-15 7.9280423e-04 5.4451972e-13 1.0775772e-11 9.9920720e-01], sampled 0.5365449211121591
[2019-03-26 19:49:11,329] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:49:11,330] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.620258505, 76.49340077, 1.0, 2.0, 0.4009456559817233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594686.5184675426, 594686.5184675419, 173942.6267914057]
[2019-03-26 19:49:11,331] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:49:11,333] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7380943e-32 1.0000000e+00 7.6667397e-31 4.3309868e-34 2.9634114e-24], sampled 0.6160646857361238
[2019-03-26 19:49:27,529] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:49:27,530] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.4, 48.0, 1.0, 2.0, 0.4247725490621791, 1.0, 2.0, 0.4247725490621791, 1.0, 2.0, 0.7376894432924552, 6.911200000000001, 6.9112, 178.6582176852504, 1781487.140613832, 1781487.140613832, 369064.1861190923]
[2019-03-26 19:49:27,531] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:49:27,533] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5494056e-20 4.1325769e-08 2.2067524e-17 2.1041584e-15 1.0000000e+00], sampled 0.32613673921794195
[2019-03-26 19:49:39,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:49:39,814] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.26666666666667, 70.33333333333334, 1.0, 2.0, 0.6941260589164745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970057.1951526062, 970057.1951526057, 219588.4016556207]
[2019-03-26 19:49:39,815] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:49:39,817] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.27377645e-36 1.00000000e+00 1.83985931e-35 0.00000000e+00
 1.97315567e-30], sampled 0.2823981125785344
[2019-03-26 19:49:42,144] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5331946]
[2019-03-26 19:49:42,147] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.25, 78.5, 1.0, 2.0, 0.6276846513960663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 877165.4491132319, 877165.4491132324, 206017.5566451002]
[2019-03-26 19:49:42,147] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:49:42,152] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 2.4853643e-38 0.0000000e+00 6.6966407e-33], sampled 0.084737106369152
[2019-03-26 19:50:18,659] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8314.4742 3002109351.1241 1010.0000
[2019-03-26 19:50:18,753] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8635.2287 2846639764.7938 720.0000
[2019-03-26 19:50:19,103] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8549.4742 2931979268.9347 649.0000
[2019-03-26 19:50:19,209] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8827.7757 2789616614.0465 469.0000
[2019-03-26 19:50:19,231] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8097.6262 3159618491.9790 1220.0000
[2019-03-26 19:50:20,249] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 350000, evaluation results [350000.0, 8097.626229065918, 3159618491.978955, 1220.0, 8549.474236086282, 2931979268.934674, 649.0, 8827.775693957052, 2789616614.0464554, 469.0, 8314.474239480785, 3002109351.1240964, 1010.0, 8635.228655599527, 2846639764.7937803, 720.0]
[2019-03-26 19:50:20,725] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 0.000000e+00 4.712852e-37], sum to 1.0000
[2019-03-26 19:50:20,737] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6310
[2019-03-26 19:50:20,745] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5171155593673222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722596.7855046365, 722596.7855046365, 186320.7193050761], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5186400.0000, 
sim time next is 5187000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5168586989980881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722237.7369859146, 722237.7369859153, 186279.1976108113], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4179020469856483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2006215936071985, 0.2006215936071987, 0.27802865315046466], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.32172173], dtype=float32), 0.8178427]. 
=============================================
[2019-03-26 19:50:20,762] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.68568]
 [67.17599]
 [67.87715]
 [68.65354]
 [69.99391]], R is [[66.42899323]
 [66.48661041]
 [66.54382324]
 [66.60071564]
 [66.65740204]].
[2019-03-26 19:50:21,528] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0977482e-24 1.0000000e+00 6.0347149e-20 2.1680361e-17 1.3388929e-12], sum to 1.0000
[2019-03-26 19:50:21,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7490
[2019-03-26 19:50:21,539] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5163661980249745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721549.3020272445, 721549.3020272438, 186199.6390236945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5190000.0000, 
sim time next is 5190600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5161590341472515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721259.7213000394, 721259.7213000394, 186166.1952553471], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4170590772858452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20034992258334428, 0.20034992258334428, 0.2778599929184285], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.09755705], dtype=float32), -0.04457093]. 
=============================================
[2019-03-26 19:50:22,872] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.2763299e-11 1.9447060e-09 5.7367977e-10 4.1977097e-09 1.0000000e+00], sum to 1.0000
[2019-03-26 19:50:22,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6993
[2019-03-26 19:50:22,886] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 67.5, 1.0, 2.0, 0.6005488769896321, 1.0, 2.0, 0.6005488769896321, 1.0, 2.0, 1.03, 6.925763164234365, 6.9112, 170.5573041426782, 2519599.784683873, 2509167.597751152, 488177.4632647521], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5226600.0000, 
sim time next is 5227200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6241021871604564, 1.0, 2.0, 0.6241021871604564, 1.0, 2.0, 1.03, 6.971748239342964, 6.9112, 170.5573041426782, 2618521.127108811, 2575147.96032254, 496883.3071814512], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5471110688680197, 1.0, 1.0, 0.5471110688680197, 1.0, 1.0, 1.0365853658536586, 0.006054823934296394, 0.0, 0.8375144448122397, 0.7273669797524475, 0.7153188778673721, 0.7416168763902257], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.98919386], dtype=float32), 0.63148546]. 
=============================================
[2019-03-26 19:50:40,672] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:50:40,682] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3797
[2019-03-26 19:50:40,690] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 89.0, 1.0, 2.0, 0.5497869995469871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768267.0708748704, 768267.0708748698, 191764.1559140442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5528400.0000, 
sim time next is 5529000.0000, 
raw observation next is [27.26666666666667, 89.5, 1.0, 2.0, 0.5498509575134044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768356.4774477064, 768356.4774477064, 191775.1292037778], 
processed observation next is [1.0, 1.0, 0.4913112164297, 0.895, 1.0, 1.0, 0.4576517560402462, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2134323548465851, 0.2134323548465851, 0.2862315361250415], 
reward next is 0.7138, 
noisyNet noise sample is [array([-1.06094], dtype=float32), 1.2620409]. 
=============================================
[2019-03-26 19:50:40,708] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[46.585056]
 [46.719654]
 [46.87093 ]
 [47.03326 ]
 [47.19024 ]], R is [[46.69595718]
 [46.94277954]
 [47.1872673 ]
 [47.42934799]
 [47.66890717]].
[2019-03-26 19:50:41,486] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9835112e-28 1.0000000e+00 2.7257519e-27 3.8749996e-30 1.4007729e-31], sum to 1.0000
[2019-03-26 19:50:41,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2953
[2019-03-26 19:50:41,506] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2011510.098922495 W.
[2019-03-26 19:50:41,511] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.26666666666667, 95.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.693277045166149, 6.9112, 169.6884640040826, 2011510.098922495, 1454130.398308694, 311494.5481175783], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5537400.0000, 
sim time next is 5538000.0000, 
raw observation next is [26.23333333333333, 95.0, 1.0, 2.0, 0.4039465503127977, 1.0, 1.0, 0.4039465503127977, 1.0, 1.0, 0.6961135284389518, 6.911199999999999, 6.9112, 170.5573041426782, 1694138.149786136, 1694138.149786137, 354318.8463266812], 
processed observation next is [1.0, 0.08695652173913043, 0.44233807266982617, 0.95, 1.0, 1.0, 0.2818633136298767, 1.0, 0.5, 0.2818633136298767, 1.0, 0.5, 0.6294067419987216, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4705939304961489, 0.47059393049614917, 0.5288340989950465], 
reward next is 0.4712, 
noisyNet noise sample is [array([2.468246], dtype=float32), -0.7155758]. 
=============================================
[2019-03-26 19:50:41,523] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[35.166508]
 [38.806152]
 [38.8659  ]
 [39.041653]
 [39.294064]], R is [[30.29720116]
 [29.99423027]
 [30.40960503]
 [30.82081223]
 [31.22801208]].
[2019-03-26 19:50:51,865] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:50:51,874] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1901
[2019-03-26 19:50:51,880] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 87.66666666666667, 1.0, 2.0, 0.5097269183795204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712268.7299873319, 712268.7299873313, 185133.8424928221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5710800.0000, 
sim time next is 5711400.0000, 
raw observation next is [26.3, 88.0, 1.0, 2.0, 0.5099865169177973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712631.6026108376, 712631.6026108376, 185175.2902713618], 
processed observation next is [0.0, 0.08695652173913043, 0.4454976303317536, 0.88, 1.0, 1.0, 0.4096223095395148, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19795322294745488, 0.19795322294745488, 0.2763810302557639], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.09033508], dtype=float32), 2.2108226]. 
=============================================
[2019-03-26 19:50:55,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:50:55,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7544
[2019-03-26 19:50:55,499] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.0, 1.0, 2.0, 0.5512923579204319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770371.4070233355, 770371.4070233355, 192022.4183767498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5772000.0000, 
sim time next is 5772600.0000, 
raw observation next is [28.8, 79.0, 1.0, 2.0, 0.5495153097932858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767887.2769922784, 767887.276992279, 191717.473560603], 
processed observation next is [0.0, 0.8260869565217391, 0.5639810426540285, 0.79, 1.0, 1.0, 0.4572473611967298, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.213302021386744, 0.21330202138674417, 0.2861454829262731], 
reward next is 0.7139, 
noisyNet noise sample is [array([0.47145665], dtype=float32), -0.49266353]. 
=============================================
[2019-03-26 19:50:57,099] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 5.780969e-35 0.000000e+00 0.000000e+00], sum to 1.0000
[2019-03-26 19:50:57,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9050
[2019-03-26 19:50:57,113] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 88.66666666666666, 1.0, 2.0, 0.5385480237658156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752556.2706470994, 752556.2706471, 189855.7897335218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5791200.0000, 
sim time next is 5791800.0000, 
raw observation next is [26.93333333333333, 88.83333333333334, 1.0, 2.0, 0.5379146463188768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751670.888473976, 751670.8884739766, 189749.3929585985], 
processed observation next is [1.0, 0.0, 0.4755134281200631, 0.8883333333333334, 1.0, 1.0, 0.44327065821551415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2087974690205489, 0.20879746902054908, 0.28320804919193804], 
reward next is 0.7168, 
noisyNet noise sample is [array([1.3278806], dtype=float32), 2.0493834]. 
=============================================
[2019-03-26 19:51:00,461] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:51:00,467] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3057
[2019-03-26 19:51:00,471] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.8, 76.83333333333333, 1.0, 2.0, 0.5583851429277124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780286.446919334, 780286.446919334, 193250.050459017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5856600.0000, 
sim time next is 5857200.0000, 
raw observation next is [29.6, 78.0, 1.0, 2.0, 0.5590737159651281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781249.0115661294, 781249.0115661294, 193369.8562955442], 
processed observation next is [1.0, 0.8260869565217391, 0.6018957345971565, 0.78, 1.0, 1.0, 0.4687635132109977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21701361432392482, 0.21701361432392482, 0.28861172581424505], 
reward next is 0.7114, 
noisyNet noise sample is [array([0.09995145], dtype=float32), -0.1022838]. 
=============================================
[2019-03-26 19:51:01,978] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 7.1201518e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 19:51:01,987] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0939
[2019-03-26 19:51:01,997] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 93.66666666666667, 1.0, 2.0, 0.525196342282297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733892.4519447049, 733892.4519447049, 187638.0130278393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5881800.0000, 
sim time next is 5882400.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.5250008546552274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733619.1895011489, 733619.1895011495, 187605.9281109089], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.94, 1.0, 1.0, 0.4277118730785872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20378310819476358, 0.20378310819476375, 0.2800088479267297], 
reward next is 0.7200, 
noisyNet noise sample is [array([-0.6630592], dtype=float32), 2.0791833]. 
=============================================
[2019-03-26 19:51:04,936] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3821137e-19 1.0000000e+00 1.1494774e-18 1.6092087e-22 2.4405715e-20], sum to 1.0000
[2019-03-26 19:51:04,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9541
[2019-03-26 19:51:04,953] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2296988.223480402 W.
[2019-03-26 19:51:04,958] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.26666666666667, 78.33333333333334, 1.0, 2.0, 0.8213068986825606, 1.0, 2.0, 0.8213068986825606, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2296988.223480402, 2296988.223480402, 430384.5555183012], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5934000.0000, 
sim time next is 5934600.0000, 
raw observation next is [30.3, 78.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.439459119609833, 6.9112, 168.9091129459914, 2658780.100572481, 2284023.427516987, 474938.1047303075], 
processed observation next is [1.0, 0.6956521739130435, 0.6350710900473934, 0.785, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.05282591196098334, 0.0, 0.8294210714913128, 0.7385500279368002, 0.634450952088052, 0.7088628428810559], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.25058687], dtype=float32), -1.6729039]. 
=============================================
[2019-03-26 19:51:04,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0205560e-15 1.0000000e+00 1.7477298e-14 1.0885167e-15 1.0304101e-15], sum to 1.0000
[2019-03-26 19:51:04,988] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0499
[2019-03-26 19:51:04,995] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2454846.514757056 W.
[2019-03-26 19:51:05,000] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.43333333333333, 71.66666666666666, 1.0, 2.0, 0.8776950343238015, 1.0, 2.0, 0.8776950343238015, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2454846.514757056, 2454846.514757056, 459474.3607298597], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5917800.0000, 
sim time next is 5918400.0000, 
raw observation next is [31.3, 72.0, 1.0, 2.0, 0.6046992427879906, 1.0, 2.0, 0.6046992427879906, 1.0, 1.0, 1.03, 6.93386600260629, 6.9112, 170.5573041426782, 2537030.301461533, 2520793.72180061, 489688.9983560219], 
processed observation next is [1.0, 0.5217391304347826, 0.6824644549763034, 0.72, 1.0, 1.0, 0.5237340274554103, 1.0, 1.0, 0.5237340274554103, 1.0, 0.5, 1.0365853658536586, 0.002266600260629037, 0.0, 0.8375144448122397, 0.7047306392948703, 0.7002204782779472, 0.7308791020239133], 
reward next is 0.1558, 
noisyNet noise sample is [array([0.73454684], dtype=float32), 1.2323074]. 
=============================================
[2019-03-26 19:51:05,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.09393186e-26 1.00000000e+00 1.14725174e-26 8.47903071e-30
 6.56881115e-28], sum to 1.0000
[2019-03-26 19:51:05,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2049
[2019-03-26 19:51:05,672] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2145994.967887642 W.
[2019-03-26 19:51:05,679] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.03333333333333, 78.5, 1.0, 2.0, 0.8935404553190295, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005987327761655, 6.9112, 168.9123931466989, 2145994.967887642, 2078749.799524394, 432260.7000772187], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5907000.0000, 
sim time next is 5907600.0000, 
raw observation next is [30.2, 78.0, 1.0, 2.0, 0.5191500283660374, 1.0, 1.0, 0.5191500283660374, 1.0, 2.0, 0.9015919137339176, 6.9112, 6.9112, 170.5573041426782, 2177788.693087529, 2177788.693087529, 428642.0285954332], 
processed observation next is [1.0, 0.391304347826087, 0.6303317535545023, 0.78, 1.0, 1.0, 0.4206626847783583, 1.0, 0.5, 0.4206626847783583, 1.0, 1.0, 0.8799901386998993, 0.0, 0.0, 0.8375144448122397, 0.6049413036354248, 0.6049413036354248, 0.6397642217842286], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.92115813], dtype=float32), -0.5790351]. 
=============================================
[2019-03-26 19:51:13,222] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:51:13,227] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1770
[2019-03-26 19:51:13,230] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6074400.0000, 
sim time next is 6075000.0000, 
raw observation next is [28.2, 83.0, 1.0, 2.0, 0.7095382681428593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 991606.1691697689, 991606.1691697689, 222918.5250526618], 
processed observation next is [1.0, 0.30434782608695654, 0.5355450236966824, 0.83, 1.0, 1.0, 0.650046106196216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2754461581027136, 0.2754461581027136, 0.3327142164965102], 
reward next is 0.6673, 
noisyNet noise sample is [array([-0.28190213], dtype=float32), 0.08344445]. 
=============================================
[2019-03-26 19:51:13,252] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[47.98052 ]
 [47.840927]
 [47.34146 ]
 [46.778168]
 [47.298748]], R is [[48.31365204]
 [48.49685669]
 [48.68399048]
 [48.86246872]
 [49.01915741]].
[2019-03-26 19:51:15,861] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 19:51:15,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:51:15,863] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:51:15,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:51:15,864] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:51:15,866] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:51:15,867] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:51:15,868] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:51:15,869] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:51:15,870] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:51:15,870] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:51:15,889] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-03-26 19:51:15,909] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-03-26 19:51:15,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-03-26 19:51:15,948] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-03-26 19:51:15,974] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-03-26 19:51:46,777] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.59995705]
[2019-03-26 19:51:46,778] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.51666666666667, 87.33333333333333, 1.0, 2.0, 0.5458882343340453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762817.0175572584, 762817.0175572584, 191098.1354574556]
[2019-03-26 19:51:46,778] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:51:46,781] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6297571230243038
[2019-03-26 19:52:04,429] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.59995705]
[2019-03-26 19:52:04,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.1, 81.66666666666666, 1.0, 2.0, 0.4930263275784424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688924.5323875515, 688924.5323875515, 182511.1182568988]
[2019-03-26 19:52:04,430] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:52:04,431] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7470326127239986
[2019-03-26 19:52:09,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.59995705]
[2019-03-26 19:52:09,950] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.87923825166667, 72.71833781833334, 1.0, 2.0, 0.5285554968949208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738588.058303121, 738588.0583031217, 188189.9834573503]
[2019-03-26 19:52:09,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:52:09,955] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.23112803707166574
[2019-03-26 19:52:16,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.59995705]
[2019-03-26 19:52:16,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.43333333333333, 47.0, 1.0, 2.0, 0.5257171106690818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734620.408547886, 734620.408547886, 187722.8889884887]
[2019-03-26 19:52:16,961] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:52:16,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15040176205948808
[2019-03-26 19:52:21,560] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.59995705]
[2019-03-26 19:52:21,562] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.83333333333334, 46.66666666666667, 1.0, 2.0, 0.5286260510557294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738686.6829131611, 738686.6829131617, 188202.5717359383]
[2019-03-26 19:52:21,563] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:52:21,566] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1845550865423624
[2019-03-26 19:53:09,086] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.1698 2842847906.5028 1131.0000
[2019-03-26 19:53:09,324] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7993.8735 3008074935.8820 1766.0000
[2019-03-26 19:53:09,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.0254 2779571564.5107 933.0000
[2019-03-26 19:53:09,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.9842 2927761875.2740 1338.0000
[2019-03-26 19:53:09,614] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.4023 3164369872.1585 1778.0000
[2019-03-26 19:53:10,630] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 375000, evaluation results [375000.0, 7880.402349447878, 3164369872.158471, 1778.0, 8249.984224632335, 2927761875.274041, 1338.0, 8657.025437658545, 2779571564.5106506, 933.0, 7993.873477747932, 3008074935.8820424, 1766.0, 8493.169770734836, 2842847906.5028043, 1131.0]
[2019-03-26 19:53:11,170] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:53:11,181] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7914
[2019-03-26 19:53:11,190] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 87.0, 1.0, 2.0, 0.5373321750653592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750856.6674212178, 750856.6674212184, 189651.9503366337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6129600.0000, 
sim time next is 6130200.0000, 
raw observation next is [27.3, 87.0, 1.0, 2.0, 0.5368334427715126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750159.5033260094, 750159.5033260094, 189568.362043944], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.87, 1.0, 1.0, 0.44196800333917174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20837763981278037, 0.20837763981278037, 0.28293785379693137], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.28175008], dtype=float32), 0.56316054]. 
=============================================
[2019-03-26 19:53:38,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:53:38,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1984
[2019-03-26 19:53:38,252] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 92.5, 1.0, 2.0, 0.5607225865789942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783553.9914521124, 783553.9914521124, 193653.0624439428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6582600.0000, 
sim time next is 6583200.0000, 
raw observation next is [25.76666666666667, 92.66666666666667, 1.0, 2.0, 0.5516146446212815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770821.9312190055, 770821.9312190055, 192075.0497460303], 
processed observation next is [1.0, 0.17391304347826086, 0.42022116903633505, 0.9266666666666667, 1.0, 1.0, 0.45977668026660423, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2141172031163904, 0.2141172031163904, 0.28667917872541837], 
reward next is 0.7133, 
noisyNet noise sample is [array([-1.1445405], dtype=float32), 1.6894865]. 
=============================================
[2019-03-26 19:53:40,811] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3776542e-35 1.0000000e+00 1.2183532e-34 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 19:53:40,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6391
[2019-03-26 19:53:40,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2359879.031092306 W.
[2019-03-26 19:53:40,835] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.41666666666666, 60.5, 1.0, 2.0, 0.5625152056933829, 1.0, 2.0, 0.5625152056933829, 1.0, 2.0, 0.9661140610305675, 6.911199999999999, 6.9112, 170.5573041426782, 2359879.031092306, 2359879.031092307, 458837.4294683043], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6617400.0000, 
sim time next is 6618000.0000, 
raw observation next is [31.33333333333334, 61.00000000000001, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.342779117863287, 6.9112, 168.9104945049974, 2606669.280832512, 2300496.663523975, 475943.4213693975], 
processed observation next is [1.0, 0.6086956521739131, 0.6840442338072673, 0.6100000000000001, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.04315791178632873, 0.0, 0.8294278555785127, 0.7240748002312533, 0.6390268509788819, 0.7103633154767127], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.44187987], dtype=float32), -1.8391324]. 
=============================================
[2019-03-26 19:53:40,849] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[37.198242]
 [38.424496]
 [38.88248 ]
 [40.4802  ]
 [41.568596]], R is [[37.27662659]
 [37.21902847]
 [37.21769714]
 [37.21258926]
 [37.20025253]].
[2019-03-26 19:53:46,556] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:53:46,568] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8621
[2019-03-26 19:53:46,577] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1841680.291055385 W.
[2019-03-26 19:53:46,585] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.63333333333333, 66.66666666666667, 1.0, 2.0, 0.6586439490330911, 1.0, 1.0, 0.6586439490330911, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1841680.291055385, 1841680.291055385, 356914.893405098], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6713400.0000, 
sim time next is 6714000.0000, 
raw observation next is [29.6, 67.0, 1.0, 2.0, 0.4265640660109442, 1.0, 2.0, 0.4265640660109442, 1.0, 1.0, 0.7253563244100578, 6.911199999999999, 6.9112, 170.5573041426782, 1789074.430169842, 1789074.430169842, 365720.7299009895], 
processed observation next is [1.0, 0.7391304347826086, 0.6018957345971565, 0.67, 1.0, 1.0, 0.30911333254330625, 1.0, 1.0, 0.30911333254330625, 1.0, 0.5, 0.6650686883049486, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49696511949162275, 0.49696511949162275, 0.5458518356731187], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22977817], dtype=float32), 0.49954233]. 
=============================================
[2019-03-26 19:53:46,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[53.82344 ]
 [53.69212 ]
 [53.321987]
 [51.837704]
 [50.83062 ]], R is [[53.2795372 ]
 [52.74674225]
 [52.35224915]
 [52.31393051]
 [51.79079056]].
[2019-03-26 19:53:51,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:53:51,888] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5709
[2019-03-26 19:53:51,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 49.0, 1.0, 2.0, 0.9046513252383885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399623.417966495, 1399623.417966495, 289886.4719257845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6796800.0000, 
sim time next is 6797400.0000, 
raw observation next is [29.18333333333333, 49.16666666666667, 1.0, 2.0, 0.8643341268109255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1336284.007303215, 1336284.007303216, 277656.5160724401], 
processed observation next is [1.0, 0.6956521739130435, 0.5821484992101105, 0.4916666666666667, 1.0, 1.0, 0.8365471407360547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37119000202867086, 0.37119000202867114, 0.4144127105558807], 
reward next is 0.5856, 
noisyNet noise sample is [array([-1.66866], dtype=float32), -2.5992398]. 
=============================================
[2019-03-26 19:54:01,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:54:01,881] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0122
[2019-03-26 19:54:01,891] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 56.33333333333334, 1.0, 2.0, 0.3965311551739365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591149.2877517611, 591149.2877517617, 173707.0568044419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6981600.0000, 
sim time next is 6982200.0000, 
raw observation next is [28.9, 56.0, 1.0, 2.0, 0.3939389622078006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589670.6571792588, 589670.6571792588, 173642.7863888592], 
processed observation next is [0.0, 0.8260869565217391, 0.5687203791469194, 0.56, 1.0, 1.0, 0.2698059785636152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16379740477201632, 0.16379740477201632, 0.2591683378938197], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.5070419], dtype=float32), 1.4956383]. 
=============================================
[2019-03-26 19:54:06,232] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 19:54:06,235] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:54:06,235] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:54:06,237] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:54:06,236] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:54:06,239] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:54:06,239] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:54:06,238] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:54:06,244] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:54:06,241] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:54:06,244] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:54:06,261] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-03-26 19:54:06,280] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-03-26 19:54:06,303] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-03-26 19:54:06,322] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-03-26 19:54:06,323] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-03-26 19:54:35,008] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.529142]
[2019-03-26 19:54:35,009] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.13333333333333, 94.83333333333333, 1.0, 2.0, 0.51760948734368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723287.2158594686, 723287.2158594686, 186399.7944994998]
[2019-03-26 19:54:35,009] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:54:35,010] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39707750850732426
[2019-03-26 19:54:44,292] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.529142]
[2019-03-26 19:54:44,294] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666667, 82.33333333333334, 1.0, 2.0, 0.4869201134928406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680389.353542873, 680389.3535428736, 181572.1421513857]
[2019-03-26 19:54:44,296] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:54:44,301] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11144906006591326
[2019-03-26 19:55:17,688] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.529142]
[2019-03-26 19:55:17,689] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.76666666666667, 79.16666666666667, 1.0, 2.0, 0.5900554764587899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824559.6950334675, 824559.6950334675, 198907.2818208466]
[2019-03-26 19:55:17,690] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:55:17,693] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4233544579696472
[2019-03-26 19:55:31,815] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.529142]
[2019-03-26 19:55:31,816] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.33333333333334, 81.33333333333334, 1.0, 2.0, 0.5666369186222883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791821.7622108033, 791821.7622108033, 194695.9060357982]
[2019-03-26 19:55:31,818] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:55:31,820] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47280715444508314
[2019-03-26 19:55:40,890] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.529142]
[2019-03-26 19:55:40,892] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.45, 81.66666666666667, 1.0, 2.0, 0.5095254918336827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711987.1715569719, 711987.1715569724, 185102.2386850603]
[2019-03-26 19:55:40,893] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:55:40,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7219073845040761
[2019-03-26 19:56:00,187] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0966 3007867509.5417 1766.0000
[2019-03-26 19:56:00,480] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2482 2779349048.6670 933.0000
[2019-03-26 19:56:00,703] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2074 2927546585.9425 1338.0000
[2019-03-26 19:56:00,743] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.6521 2842708650.5645 1131.0000
[2019-03-26 19:56:00,750] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9141 3164254244.9626 1778.0000
[2019-03-26 19:56:01,765] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 400000, evaluation results [400000.0, 7881.914110821881, 3164254244.9625816, 1778.0, 8252.207390139796, 2927546585.9425, 1338.0, 8659.248225290772, 2779349048.6670237, 933.0, 7996.096616164504, 3007867509.5416894, 1766.0, 8494.652070649916, 2842708650.564488, 1131.0]
[2019-03-26 19:56:02,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:02,410] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3351
[2019-03-26 19:56:02,416] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 88.5, 1.0, 2.0, 0.4739172688450439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664048.1388739002, 664048.1388738996, 179847.0565907421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7083000.0000, 
sim time next is 7083600.0000, 
raw observation next is [25.26666666666667, 88.66666666666666, 1.0, 2.0, 0.4730573396743231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663154.2918153381, 663154.2918153375, 179758.9218738756], 
processed observation next is [1.0, 1.0, 0.3965244865718801, 0.8866666666666666, 1.0, 1.0, 0.36512932490882305, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18420952550426056, 0.18420952550426042, 0.2682968983192173], 
reward next is 0.7317, 
noisyNet noise sample is [array([-0.7152525], dtype=float32), 0.4456878]. 
=============================================
[2019-03-26 19:56:07,276] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:07,283] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4107
[2019-03-26 19:56:07,287] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 85.66666666666667, 1.0, 2.0, 0.4558445268702861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636953.4048898576, 636953.4048898583, 176968.2596178896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7147200.0000, 
sim time next is 7147800.0000, 
raw observation next is [26.1, 85.5, 1.0, 2.0, 0.4571307578720981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638751.2010035329, 638751.2010035329, 177153.0790117436], 
processed observation next is [1.0, 0.7391304347826086, 0.4360189573459717, 0.855, 1.0, 1.0, 0.3459406721350579, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17743088916764801, 0.17743088916764801, 0.26440758061454267], 
reward next is 0.7356, 
noisyNet noise sample is [array([-0.93864006], dtype=float32), 2.1127706]. 
=============================================
[2019-03-26 19:56:07,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:07,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8808
[2019-03-26 19:56:07,312] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 82.16666666666666, 1.0, 2.0, 0.5359684677292955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763047.9393231751, 763047.9393231751, 191225.3700392197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7109400.0000, 
sim time next is 7110000.0000, 
raw observation next is [26.0, 81.0, 1.0, 2.0, 0.537309990994946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765116.9800715566, 765116.9800715559, 191474.8870578634], 
processed observation next is [1.0, 0.30434782608695654, 0.4312796208530806, 0.81, 1.0, 1.0, 0.44254215782523615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2125324944643213, 0.2125324944643211, 0.2857834135191991], 
reward next is 0.7142, 
noisyNet noise sample is [array([1.8284855], dtype=float32), -1.3407737]. 
=============================================
[2019-03-26 19:56:07,328] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[67.82946 ]
 [67.96693 ]
 [68.109146]
 [68.11447 ]
 [68.07691 ]], R is [[67.94980621]
 [67.98490143]
 [68.02200317]
 [68.06401062]
 [68.10526276]].
[2019-03-26 19:56:10,777] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:10,785] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2244
[2019-03-26 19:56:10,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1789277.703940513 W.
[2019-03-26 19:56:10,801] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 79.83333333333334, 1.0, 2.0, 0.4266124915821179, 1.0, 1.0, 0.4266124915821179, 1.0, 2.0, 0.7408848149712994, 6.911200000000001, 6.9112, 170.5573041426782, 1789277.703940513, 1789277.703940512, 368109.0534054863], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7206600.0000, 
sim time next is 7207200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.7642198915809658, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.998022651819127, 6.9112, 168.9124401228415, 1964998.959738948, 1903404.170636788, 399494.9002213525], 
processed observation next is [1.0, 0.43478260869565216, 0.5734597156398105, 0.79, 1.0, 1.0, 0.715927580218031, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00868226518191273, 0.0, 0.829437409452835, 0.54583304437193, 0.5287233807324411, 0.5962610451064964], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8513389], dtype=float32), 0.42853174]. 
=============================================
[2019-03-26 19:56:16,530] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:16,540] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6342
[2019-03-26 19:56:16,544] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.58333333333334, 59.5, 1.0, 2.0, 0.5314612537988196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 810713.4058482694, 810713.4058482688, 196925.4549290281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7305000.0000, 
sim time next is 7305600.0000, 
raw observation next is [27.66666666666667, 59.00000000000001, 1.0, 2.0, 0.8192553128984903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1250146.897308022, 1250146.897308022, 262814.2048696524], 
processed observation next is [1.0, 0.5652173913043478, 0.5102685624012641, 0.5900000000000001, 1.0, 1.0, 0.782235316745169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3472630270300061, 0.3472630270300061, 0.3922600072681379], 
reward next is 0.6077, 
noisyNet noise sample is [array([-0.96710837], dtype=float32), -0.037788413]. 
=============================================
[2019-03-26 19:56:20,090] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:20,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2834
[2019-03-26 19:56:20,107] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.98333333333333, 66.5, 1.0, 2.0, 0.3762490660728819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562824.6120057555, 562824.6120057555, 171227.0679055988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7321800.0000, 
sim time next is 7322400.0000, 
raw observation next is [26.9, 67.0, 1.0, 2.0, 0.3800291821125275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 568403.7290773244, 568403.729077325, 171715.604130279], 
processed observation next is [1.0, 0.782608695652174, 0.4739336492890995, 0.67, 1.0, 1.0, 0.253047207364491, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15788992474370123, 0.1578899247437014, 0.256291946463103], 
reward next is 0.7437, 
noisyNet noise sample is [array([1.4958174], dtype=float32), 0.33618096]. 
=============================================
[2019-03-26 19:56:25,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:25,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7622
[2019-03-26 19:56:25,664] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 83.0, 1.0, 2.0, 0.2872973960283477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462798.8579883144, 462798.8579883144, 164464.1637426504], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7414200.0000, 
sim time next is 7414800.0000, 
raw observation next is [21.56666666666667, 82.66666666666667, 1.0, 2.0, 0.2874157617010465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462797.0075401866, 462797.0075401873, 164463.9532688753], 
processed observation next is [1.0, 0.8260869565217391, 0.22116903633491333, 0.8266666666666667, 1.0, 1.0, 0.14146477313379094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1285547243167185, 0.1285547243167187, 0.2454685869684706], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.8158264], dtype=float32), -0.7421567]. 
=============================================
[2019-03-26 19:56:31,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:31,532] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1666
[2019-03-26 19:56:31,536] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 74.83333333333334, 1.0, 2.0, 0.4738316320516899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662094.6921360975, 662094.6921360975, 179598.1152550197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7555800.0000, 
sim time next is 7556400.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4764315527761743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665728.7532254775, 665728.7532254775, 179986.3776654281], 
processed observation next is [0.0, 0.4782608695652174, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3691946418990052, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18492465367374375, 0.18492465367374375, 0.26863638457526584], 
reward next is 0.7314, 
noisyNet noise sample is [array([-2.1909027], dtype=float32), -0.50504994]. 
=============================================
[2019-03-26 19:56:34,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:34,806] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0929
[2019-03-26 19:56:34,809] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 91.0, 1.0, 2.0, 0.5626204026534609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 794781.8612301777, 794781.861230177, 195089.2512603661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7632000.0000, 
sim time next is 7632600.0000, 
raw observation next is [25.06666666666667, 89.33333333333334, 1.0, 2.0, 0.5538310938006488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780730.8434152335, 780730.843415233, 193333.0884085688], 
processed observation next is [1.0, 0.34782608695652173, 0.38704581358609813, 0.8933333333333334, 1.0, 1.0, 0.46244710096463704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21686967872645377, 0.2168696787264536, 0.2885568483709982], 
reward next is 0.7114, 
noisyNet noise sample is [array([0.32463408], dtype=float32), -1.303226]. 
=============================================
[2019-03-26 19:56:39,458] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:56:39,466] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5932
[2019-03-26 19:56:39,470] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 93.0, 1.0, 2.0, 0.4756899223566954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664692.13179326, 664692.1317932606, 179874.5957393567], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7694400.0000, 
sim time next is 7695000.0000, 
raw observation next is [24.75, 93.5, 1.0, 2.0, 0.4765910510135424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 665951.6936347912, 665951.6936347906, 180009.3996625681], 
processed observation next is [1.0, 0.043478260869565216, 0.3720379146919432, 0.935, 1.0, 1.0, 0.369386808450051, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18498658156521977, 0.1849865815652196, 0.268670745765027], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.2207633], dtype=float32), 1.3356506]. 
=============================================
[2019-03-26 19:56:39,485] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.1691  ]
 [70.1668  ]
 [70.186745]
 [70.14044 ]
 [70.13299 ]], R is [[70.14096069]
 [70.17108154]
 [70.20085907]
 [70.23021698]
 [70.25909424]].
[2019-03-26 19:56:42,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.3432516e-38 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 19:56:42,212] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9264
[2019-03-26 19:56:42,220] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2001196.821680424 W.
[2019-03-26 19:56:42,224] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.25, 74.5, 1.0, 2.0, 0.7156389484010772, 1.0, 2.0, 0.7156389484010772, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2001196.821680424, 2001196.821680424, 380889.5281965043], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7723800.0000, 
sim time next is 7724400.0000, 
raw observation next is [29.46666666666667, 73.33333333333333, 1.0, 2.0, 0.4842152748559776, 1.0, 2.0, 0.4842152748559776, 1.0, 1.0, 0.8354638024247912, 6.911200000000001, 6.9112, 170.5573041426782, 2031101.54210564, 2031101.542105639, 403450.6820801883], 
processed observation next is [1.0, 0.391304347826087, 0.5955766192733019, 0.7333333333333333, 1.0, 1.0, 0.3785726203084067, 1.0, 1.0, 0.3785726203084067, 1.0, 0.5, 0.799346100518038, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5641948728071222, 0.5641948728071219, 0.6021651971346094], 
reward next is 0.3978, 
noisyNet noise sample is [array([1.5420032], dtype=float32), 0.8010588]. 
=============================================
[2019-03-26 19:56:52,253] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.3620991e-37 0.0000000e+00 1.4417427e-37], sum to 1.0000
[2019-03-26 19:56:52,262] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8406
[2019-03-26 19:56:52,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2007601.33147838 W.
[2019-03-26 19:56:52,281] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.73333333333333, 72.0, 1.0, 2.0, 0.4786180614949215, 1.0, 2.0, 0.4786180614949215, 1.0, 2.0, 0.8268634617379501, 6.9112, 6.9112, 170.5573041426782, 2007601.33147838, 2007601.33147838, 399929.2390319413], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7904400.0000, 
sim time next is 7905000.0000, 
raw observation next is [29.76666666666667, 72.0, 1.0, 2.0, 0.7930624410510496, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990745339973302, 6.9112, 168.9124828395285, 2005364.081201497, 1948932.037534036, 406700.7225159716], 
processed observation next is [1.0, 0.4782608695652174, 0.6097946287519749, 0.72, 1.0, 1.0, 0.7506776398205417, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007954533997330237, 0.0, 0.829437619211324, 0.557045578111527, 0.5413700104261211, 0.6070160037551815], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0418881], dtype=float32), -1.0082307]. 
=============================================
[2019-03-26 19:56:52,293] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[50.983437]
 [50.419174]
 [51.489113]
 [53.290268]
 [54.374706]], R is [[51.08012772]
 [50.97241974]
 [50.82716751]
 [50.31889725]
 [50.26747894]].
[2019-03-26 19:56:54,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:54,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:54,812] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-03-26 19:56:54,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:54,935] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:54,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-03-26 19:56:55,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,047] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-03-26 19:56:55,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-03-26 19:56:55,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-03-26 19:56:55,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-03-26 19:56:55,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-03-26 19:56:55,699] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,702] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-03-26 19:56:55,725] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,725] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,726] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-03-26 19:56:55,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-03-26 19:56:55,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,965] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,966] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-03-26 19:56:55,985] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:55,986] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:55,987] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-03-26 19:56:56,005] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:56,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:56,007] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-03-26 19:56:56,036] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:56,036] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:56,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-03-26 19:56:56,067] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:56,067] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:56,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-03-26 19:56:56,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 19:56:56,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:56,182] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-03-26 19:56:56,716] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2321882e-24 2.6614359e-01 1.4914621e-21 6.8598958e-15 7.3385644e-01], sum to 1.0000
[2019-03-26 19:56:56,717] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9966
[2019-03-26 19:56:56,718] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [20.55, 84.83333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 459108.8362580346, 459108.8362580339, 222951.6136858409], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3000.0000, 
sim time next is 3600.0000, 
raw observation next is [20.2, 84.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 1.0, 2.0, 0.18, 6.911200000000001, 6.9112, 170.5573041426782, 443192.2454956224, 443192.2454956218, 219795.4616213473], 
processed observation next is [1.0, 0.043478260869565216, 0.15639810426540288, 0.84, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.12310895708211733, 0.12310895708211717, 0.32805292779305567], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.72925], dtype=float32), -1.7282264]. 
=============================================
[2019-03-26 19:56:58,642] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 19:56:58,649] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:56:58,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:58,651] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:56:58,653] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:56:58,653] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:58,655] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:56:58,655] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:58,658] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:58,655] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:56:58,662] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:56:58,674] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-03-26 19:56:58,675] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-03-26 19:56:58,710] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-03-26 19:56:58,729] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-03-26 19:56:58,730] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-03-26 19:57:07,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:07,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.1, 76.0, 1.0, 2.0, 0.2290363796647551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380212.3874429845, 380212.3874429839, 158673.4106363688]
[2019-03-26 19:57:07,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:57:07,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.528956814646129
[2019-03-26 19:57:08,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:08,708] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.75, 77.5, 1.0, 2.0, 0.3299626021126095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514859.5742660682, 514859.5742660688, 167998.1765352577]
[2019-03-26 19:57:08,709] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:57:08,711] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7239360707556268
[2019-03-26 19:57:18,256] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:18,257] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.86666666666667, 48.66666666666667, 1.0, 2.0, 0.289633149338981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 474859.5319223051, 474859.5319223044, 165083.4154468953]
[2019-03-26 19:57:18,259] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:57:18,262] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9279388640810158
[2019-03-26 19:57:20,069] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:20,070] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.75, 47.33333333333334, 1.0, 2.0, 0.2962381671518284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475208.9047335431, 475208.9047335431, 165319.4594384755]
[2019-03-26 19:57:20,071] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 19:57:20,073] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4302009738337186
[2019-03-26 19:57:30,335] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:30,337] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.34786726833333, 95.42939456333333, 1.0, 2.0, 0.5515646043063948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771407.938191182, 771407.9381911826, 192148.9186429137]
[2019-03-26 19:57:30,339] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:57:30,341] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5045916393128694
[2019-03-26 19:57:39,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:39,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.12148921833333, 90.81203385333335, 1.0, 2.0, 0.3177543296023934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506384.9222501, 506384.9222501, 167561.9841630562]
[2019-03-26 19:57:39,975] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:57:39,978] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6790482948198383
[2019-03-26 19:57:40,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:40,080] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.2, 58.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.121948948202007, 6.9112, 168.9116932885467, 1603368.760585525, 1453857.321447284, 311347.7789426416]
[2019-03-26 19:57:40,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:57:40,086] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.13466473039811944
[2019-03-26 19:57:40,165] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:40,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.96897916333334, 95.30675409, 1.0, 2.0, 0.4009987312014427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594117.4535476015, 594117.4535476008, 173869.8392544144]
[2019-03-26 19:57:40,167] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:57:40,169] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05792247825075947
[2019-03-26 19:57:52,271] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:57:52,271] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.33333333333334, 78.16666666666667, 1.0, 2.0, 0.7734726480349765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1108856.419180989, 1108856.419180989, 241347.843446347]
[2019-03-26 19:57:52,273] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 19:57:52,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9271443246113709
[2019-03-26 19:58:18,838] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:58:18,839] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.45, 94.5, 1.0, 2.0, 0.629641901732847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879901.7650858628, 879901.7650858628, 206397.7727407626]
[2019-03-26 19:58:18,842] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 19:58:18,845] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0623321371211677
[2019-03-26 19:58:28,207] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:58:28,208] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.48043525, 64.33524794666667, 1.0, 2.0, 0.5867910734859069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 819996.1679627359, 819996.1679627366, 198310.5691253005]
[2019-03-26 19:58:28,208] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 19:58:28,212] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2908911913227977
[2019-03-26 19:58:49,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.40700266]
[2019-03-26 19:58:49,947] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.23253116666667, 86.00896068, 1.0, 2.0, 0.5121188218142325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715612.1906169378, 715612.1906169384, 185515.4329525237]
[2019-03-26 19:58:49,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 19:58:49,951] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9068456912022076
[2019-03-26 19:58:52,538] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2074 2927546585.9425 1338.0000
[2019-03-26 19:58:52,590] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.7669 2779497663.8967 933.0000
[2019-03-26 19:58:52,985] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.9112 2842778418.7734 1131.0000
[2019-03-26 19:58:53,019] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.3563 3007937057.5632 1766.0000
[2019-03-26 19:58:53,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9141 3164254244.9626 1778.0000
[2019-03-26 19:58:54,143] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 425000, evaluation results [425000.0, 7881.914110821881, 3164254244.9625816, 1778.0, 8252.207390139796, 2927546585.9425, 1338.0, 8657.766851449904, 2779497663.8966665, 933.0, 7995.356252368326, 3007937057.5631742, 1766.0, 8493.911166823438, 2842778418.7733655, 1131.0]
[2019-03-26 19:58:54,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:58:54,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0916
[2019-03-26 19:58:54,271] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 65.66666666666666, 1.0, 2.0, 1.022795235846997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128939175237, 1554033.728951713, 1554033.728951714, 324712.2155476182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 42000.0000, 
sim time next is 42600.0000, 
raw observation next is [26.7, 65.33333333333334, 1.0, 2.0, 1.021612509034909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1550388.656170097, 1550388.656170097, 324055.4728581236], 
processed observation next is [1.0, 0.4782608695652174, 0.46445497630331756, 0.6533333333333334, 1.0, 1.0, 1.0260391675119387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4306635156028047, 0.4306635156028047, 0.483664884862871], 
reward next is 0.5163, 
noisyNet noise sample is [array([-2.1473677], dtype=float32), 0.3588232]. 
=============================================
[2019-03-26 19:58:57,228] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:58:57,239] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6673
[2019-03-26 19:58:57,245] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 78.33333333333334, 1.0, 2.0, 0.3816620748357473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575991.1523182008, 575991.1523182008, 172551.5399395633], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 67800.0000, 
sim time next is 68400.0000, 
raw observation next is [24.6, 79.0, 1.0, 2.0, 0.3801840191386233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 574968.5705695054, 574968.570569506, 172496.9382077948], 
processed observation next is [1.0, 0.8260869565217391, 0.36492890995260674, 0.79, 1.0, 1.0, 0.25323375799834136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15971349182486264, 0.15971349182486277, 0.2574581167280519], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.29478145], dtype=float32), 0.6058046]. 
=============================================
[2019-03-26 19:58:58,532] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:58:58,541] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2696
[2019-03-26 19:58:58,547] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.91666666666666, 91.00000000000001, 1.0, 2.0, 0.4676925561097179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708839.6244483565, 708839.6244483565, 185466.9295031413], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 112200.0000, 
sim time next is 112800.0000, 
raw observation next is [22.93333333333333, 91.0, 1.0, 2.0, 0.4666124415444929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706773.0203536035, 706773.020353603, 185245.6791247776], 
processed observation next is [1.0, 0.30434782608695654, 0.28593996840442326, 0.91, 1.0, 1.0, 0.35736438740300347, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1963258389871121, 0.19632583898711192, 0.27648608824593673], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.817683], dtype=float32), -1.8433111]. 
=============================================
[2019-03-26 19:58:58,583] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:58:58,592] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3298
[2019-03-26 19:58:58,599] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.3892994580637094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599667.7422333704, 599667.7422333698, 174970.4936730671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 99000.0000, 
sim time next is 99600.0000, 
raw observation next is [22.5, 90.0, 1.0, 2.0, 0.3837474741719324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591120.1740616008, 591120.1740616008, 174207.1257026495], 
processed observation next is [1.0, 0.13043478260869565, 0.2654028436018958, 0.9, 1.0, 1.0, 0.25752707731558117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16420004835044466, 0.16420004835044466, 0.2600106353770888], 
reward next is 0.7400, 
noisyNet noise sample is [array([1.4556757], dtype=float32), -1.9047017]. 
=============================================
[2019-03-26 19:59:22,377] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:22,386] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1532
[2019-03-26 19:59:22,394] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.66666666666667, 82.0, 1.0, 2.0, 0.2388942770793331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395431.8616058314, 395431.8616058314, 159716.1237722852], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 508800.0000, 
sim time next is 509400.0000, 
raw observation next is [19.6, 82.5, 1.0, 2.0, 0.2383751792109612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394586.0703036775, 394586.0703036775, 159665.8583637842], 
processed observation next is [1.0, 0.9130434782608695, 0.127962085308057, 0.825, 1.0, 1.0, 0.08237973398910989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10960724175102153, 0.10960724175102153, 0.23830725128923017], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.21398365], dtype=float32), 0.43321025]. 
=============================================
[2019-03-26 19:59:30,458] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:30,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5979
[2019-03-26 19:59:30,480] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 76.0, 1.0, 2.0, 0.2415907449897015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399704.3218930089, 399704.3218930089, 159988.1921951164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 632400.0000, 
sim time next is 633000.0000, 
raw observation next is [20.76666666666667, 75.0, 1.0, 2.0, 0.2451580971734862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 405215.5084417796, 405215.5084417796, 160358.747180089], 
processed observation next is [1.0, 0.30434782608695654, 0.18325434439178534, 0.75, 1.0, 1.0, 0.09055192430540504, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11255986345604989, 0.11255986345604989, 0.23934141370162537], 
reward next is 0.7607, 
noisyNet noise sample is [array([0.7205723], dtype=float32), 1.1972903]. 
=============================================
[2019-03-26 19:59:30,493] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.400276]
 [70.374916]
 [70.395935]
 [70.428734]
 [70.41297 ]], R is [[70.48428345]
 [70.54065704]
 [70.59683228]
 [70.6536026 ]
 [70.71118164]].
[2019-03-26 19:59:32,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:32,391] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8279
[2019-03-26 19:59:32,396] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 73.83333333333333, 1.0, 2.0, 0.2422879635159711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399902.8632683962, 399902.8632683962, 160113.5336271545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 676200.0000, 
sim time next is 676800.0000, 
raw observation next is [20.8, 75.0, 1.0, 2.0, 0.2413351189298515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398553.193688629, 398553.1936886283, 160011.0346077084], 
processed observation next is [1.0, 0.8695652173913043, 0.1848341232227489, 0.75, 1.0, 1.0, 0.0859459264215078, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11070922046906362, 0.11070922046906342, 0.2388224397129976], 
reward next is 0.7612, 
noisyNet noise sample is [array([-1.1014664], dtype=float32), 0.17776285]. 
=============================================
[2019-03-26 19:59:32,803] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:32,814] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9434
[2019-03-26 19:59:32,817] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 75.0, 1.0, 2.0, 0.2413351189298515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 398553.193688629, 398553.1936886283, 160011.0346077084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 676800.0000, 
sim time next is 677400.0000, 
raw observation next is [20.68333333333334, 75.83333333333333, 1.0, 2.0, 0.2418179191474085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399429.7051594391, 399429.7051594391, 160052.8851681269], 
processed observation next is [1.0, 0.8695652173913043, 0.17930489731437638, 0.7583333333333333, 1.0, 1.0, 0.08652761343061263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11095269587762198, 0.11095269587762198, 0.2388849032360103], 
reward next is 0.7611, 
noisyNet noise sample is [array([-0.7280629], dtype=float32), 0.27760532]. 
=============================================
[2019-03-26 19:59:34,527] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:34,538] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2473
[2019-03-26 19:59:34,541] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.03333333333333, 84.83333333333334, 1.0, 2.0, 0.2348593271863968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 389869.3055047795, 389869.3055047789, 159212.0337395614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 713400.0000, 
sim time next is 714000.0000, 
raw observation next is [19.26666666666667, 83.66666666666667, 1.0, 2.0, 0.2273105243081364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 377038.4691451734, 377038.4691451727, 158558.7234487081], 
processed observation next is [1.0, 0.2608695652173913, 0.1121642969984204, 0.8366666666666667, 1.0, 1.0, 0.06904882446763419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1047329080958815, 0.1047329080958813, 0.23665481111747477], 
reward next is 0.7633, 
noisyNet noise sample is [array([0.08641246], dtype=float32), -1.1552343]. 
=============================================
[2019-03-26 19:59:34,557] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.274734]
 [77.37679 ]
 [77.39926 ]
 [77.43958 ]
 [77.48894 ]], R is [[77.25950623]
 [77.24928284]
 [77.24076843]
 [77.23265076]
 [77.22497559]].
[2019-03-26 19:59:39,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:40,004] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8096
[2019-03-26 19:59:40,009] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 70.0, 1.0, 2.0, 0.2890485833937786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463072.8546722764, 463072.8546722764, 164469.9306947154], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 811800.0000, 
sim time next is 812400.0000, 
raw observation next is [23.83333333333334, 69.0, 1.0, 2.0, 0.2896023743382179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463720.3568616569, 463720.3568616569, 164512.0927808028], 
processed observation next is [0.0, 0.391304347826087, 0.32859399684044266, 0.69, 1.0, 1.0, 0.14409924619062395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12881121023934913, 0.12881121023934913, 0.24554043698627284], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.27570048], dtype=float32), 0.038125966]. 
=============================================
[2019-03-26 19:59:41,291] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:41,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4657
[2019-03-26 19:59:41,308] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 62.66666666666667, 1.0, 2.0, 0.288554224655352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462045.0355241917, 462045.0355241924, 164396.7581199094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 823200.0000, 
sim time next is 823800.0000, 
raw observation next is [24.83333333333334, 62.83333333333333, 1.0, 2.0, 0.2893466080141956, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463352.7591582451, 463352.7591582457, 164487.1777494199], 
processed observation next is [0.0, 0.5217391304347826, 0.3759873617693526, 0.6283333333333333, 1.0, 1.0, 0.14379109399300674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1287090997661792, 0.12870909976617936, 0.24550325037226853], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.8393085], dtype=float32), -0.3339452]. 
=============================================
[2019-03-26 19:59:49,560] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 19:59:49,569] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2841
[2019-03-26 19:59:49,572] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.88333333333333, 93.16666666666666, 1.0, 2.0, 0.3316361779264139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514014.6031773683, 514014.6031773683, 167830.4349445109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 964200.0000, 
sim time next is 964800.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3315436675871201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513924.8772747177, 513924.8772747171, 167825.1107573162], 
processed observation next is [1.0, 0.17391304347826086, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19463092480375918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14275691035408825, 0.14275691035408808, 0.25048523993629285], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.71853495], dtype=float32), 1.0115112]. 
=============================================
[2019-03-26 19:59:49,708] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 19:59:49,708] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 19:59:49,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 19:59:49,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:59:49,710] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:59:49,711] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 19:59:49,713] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 19:59:49,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:59:49,714] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:59:49,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 19:59:49,716] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 19:59:49,738] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-03-26 19:59:49,739] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-03-26 19:59:49,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-03-26 19:59:49,794] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-03-26 19:59:49,812] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-03-26 20:00:02,523] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:00:02,524] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.19665766, 79.89821685666668, 1.0, 2.0, 0.3438434348478732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535564.90614204, 535564.9061420406, 169616.2159124248]
[2019-03-26 20:00:02,525] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:00:02,529] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7733009432537693
[2019-03-26 20:00:15,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:00:15,255] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.11866719333334, 95.54050666, 1.0, 2.0, 0.7526222278093179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1063775.406624269, 1063775.406624269, 234253.0079003109]
[2019-03-26 20:00:15,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:00:15,258] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8975335014722059
[2019-03-26 20:00:36,966] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:00:36,967] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.18272237333333, 87.53724194, 1.0, 2.0, 0.5096070995360632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712101.2445788793, 712101.2445788793, 185114.3389293423]
[2019-03-26 20:00:36,969] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:00:36,971] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5291083358347525
[2019-03-26 20:00:41,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:00:41,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.88414707333333, 69.94988542666667, 1.0, 2.0, 0.7714294222766186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1078145.271200176, 1078145.271200176, 237018.3350331293]
[2019-03-26 20:00:41,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:00:41,922] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8442878423878125
[2019-03-26 20:00:44,145] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:00:44,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.94106138333333, 58.64807492999999, 1.0, 2.0, 0.6935577278786914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 969262.5769539227, 969262.5769539227, 219469.8226563269]
[2019-03-26 20:00:44,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:00:44,154] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06126225255363438
[2019-03-26 20:01:01,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:01:01,680] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.33333333333334, 65.0, 1.0, 2.0, 0.508066914699337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709948.342770735, 709948.3427707345, 184870.064162707]
[2019-03-26 20:01:01,681] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:01:01,685] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6465234278427068
[2019-03-26 20:01:02,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:01:02,124] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.140545565, 75.96264486, 1.0, 2.0, 0.54297271266073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758741.4509476878, 758741.4509476884, 190602.7462035587]
[2019-03-26 20:01:02,125] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:01:02,127] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8734509309104372
[2019-03-26 20:01:02,887] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.39649525]
[2019-03-26 20:01:02,888] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.03333333333333, 73.33333333333334, 1.0, 2.0, 0.6071775903924442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848496.1634830114, 848496.1634830114, 202089.6746038916]
[2019-03-26 20:01:02,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:01:02,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8360236959920797
[2019-03-26 20:01:43,794] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.5078 2779423486.9439 933.0000
[2019-03-26 20:01:43,827] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.6674 3164193356.1757 1778.0000
[2019-03-26 20:01:43,861] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0966 3007867509.5417 1766.0000
[2019-03-26 20:01:44,005] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2074 2927546585.9425 1338.0000
[2019-03-26 20:01:44,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.6521 2842708650.5645 1131.0000
[2019-03-26 20:01:45,058] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 450000, evaluation results [450000.0, 7882.6673903550445, 3164193356.175722, 1778.0, 8252.207390139796, 2927546585.9425, 1338.0, 8658.507764904378, 2779423486.9438543, 933.0, 7996.096616164504, 3007867509.5416894, 1766.0, 8494.652070649916, 2842708650.564488, 1131.0]
[2019-03-26 20:01:54,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:01:54,119] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0741
[2019-03-26 20:01:54,124] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 73.0, 1.0, 2.0, 0.3295946731302897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516485.2663641399, 516485.2663641393, 168181.3163408892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1105200.0000, 
sim time next is 1105800.0000, 
raw observation next is [24.06666666666667, 73.66666666666667, 1.0, 2.0, 0.3276070559093963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513846.8382420852, 513846.8382420846, 167988.2240346978], 
processed observation next is [1.0, 0.8260869565217391, 0.3396524486571882, 0.7366666666666667, 1.0, 1.0, 0.18988801916794737, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14273523284502365, 0.1427352328450235, 0.2507286925891012], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.64605623], dtype=float32), 0.98829764]. 
=============================================
[2019-03-26 20:02:00,575] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:00,586] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7653
[2019-03-26 20:02:00,590] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 91.66666666666667, 1.0, 2.0, 0.345449281210454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537458.4168164312, 537458.4168164312, 169755.1575519492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1219800.0000, 
sim time next is 1220400.0000, 
raw observation next is [21.9, 92.0, 1.0, 2.0, 0.3420071343676661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531989.4497294846, 531989.4497294846, 169310.1688530309], 
processed observation next is [1.0, 0.13043478260869565, 0.23696682464454974, 0.92, 1.0, 1.0, 0.2072375112863447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14777484714707906, 0.14777484714707906, 0.25270174455676253], 
reward next is 0.7473, 
noisyNet noise sample is [array([1.1977432], dtype=float32), 0.49046087]. 
=============================================
[2019-03-26 20:02:01,488] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:01,495] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3417
[2019-03-26 20:02:01,502] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 94.0, 1.0, 2.0, 0.4536550556866039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646032.0440954482, 646032.0440954489, 178216.6684105062], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1303200.0000, 
sim time next is 1303800.0000, 
raw observation next is [24.21666666666667, 93.83333333333334, 1.0, 2.0, 0.7559299908264423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1076912.142342209, 1076912.142342209, 236186.4472046813], 
processed observation next is [1.0, 0.08695652173913043, 0.34676145339652464, 0.9383333333333335, 1.0, 1.0, 0.7059397479836654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2991422617617247, 0.2991422617617247, 0.35251708538012133], 
reward next is 0.6475, 
noisyNet noise sample is [array([0.13505651], dtype=float32), 0.70110387]. 
=============================================
[2019-03-26 20:02:07,370] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:07,384] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6577
[2019-03-26 20:02:07,390] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.06666666666667, 95.0, 1.0, 2.0, 0.650022440921547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 963176.7206946, 963176.7206946007, 217447.8527135873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1326000.0000, 
sim time next is 1326600.0000, 
raw observation next is [23.05, 95.0, 1.0, 2.0, 0.732035705460625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1085236.327081783, 1085236.327081784, 236145.6492130463], 
processed observation next is [1.0, 0.34782608695652173, 0.2914691943127963, 0.95, 1.0, 1.0, 0.6771514523621988, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30145453530049526, 0.30145453530049554, 0.352456192855293], 
reward next is 0.6475, 
noisyNet noise sample is [array([1.2335335], dtype=float32), 2.5175176]. 
=============================================
[2019-03-26 20:02:07,469] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:07,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6513
[2019-03-26 20:02:07,482] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 90.0, 1.0, 2.0, 0.6095514060263363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954581.580616492, 954581.5806164913, 214705.5538799792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1344000.0000, 
sim time next is 1344600.0000, 
raw observation next is [21.75, 89.5, 1.0, 2.0, 0.6325786714045516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 994203.1120139954, 994203.1120139954, 220021.807208456], 
processed observation next is [1.0, 0.5652173913043478, 0.2298578199052133, 0.895, 1.0, 1.0, 0.5573237004874115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2761675311149987, 0.2761675311149987, 0.3283907570275463], 
reward next is 0.6716, 
noisyNet noise sample is [array([-0.36238384], dtype=float32), -0.37067178]. 
=============================================
[2019-03-26 20:02:10,293] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:10,301] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5273
[2019-03-26 20:02:10,304] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 98.0, 1.0, 2.0, 0.313372102674091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496429.3250085902, 496429.3250085902, 166776.1571103009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1396200.0000, 
sim time next is 1396800.0000, 
raw observation next is [20.5, 98.0, 1.0, 2.0, 0.3132743445435494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495886.8030634278, 495886.8030634284, 166728.6082702915], 
processed observation next is [0.0, 0.17391304347826086, 0.1706161137440759, 0.98, 1.0, 1.0, 0.17261969222114387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1377463341842855, 0.13774633418428567, 0.24884866906013656], 
reward next is 0.7512, 
noisyNet noise sample is [array([1.0225847], dtype=float32), -1.2849041]. 
=============================================
[2019-03-26 20:02:19,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:19,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3566
[2019-03-26 20:02:19,033] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1529400.0000, 
sim time next is 1530000.0000, 
raw observation next is [27.1, 59.0, 1.0, 2.0, 0.3437258197622903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531896.1454583206, 531896.14545832, 169226.5476227], 
processed observation next is [0.0, 0.7391304347826086, 0.4834123222748816, 0.59, 1.0, 1.0, 0.20930821658107263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14774892929397795, 0.1477489292939778, 0.25257693675029846], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.43459305], dtype=float32), 0.24761714]. 
=============================================
[2019-03-26 20:02:19,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.095955]
 [69.12684 ]
 [69.1537  ]
 [69.18606 ]
 [69.22646 ]], R is [[69.1223526 ]
 [69.1782608 ]
 [69.23326874]
 [69.28733063]
 [69.34053802]].
[2019-03-26 20:02:19,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 0.0000000e+00 8.3188044e-38], sum to 1.0000
[2019-03-26 20:02:19,502] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1447
[2019-03-26 20:02:19,506] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 83.0, 1.0, 2.0, 0.3586410818911617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549815.2703252132, 549815.2703252139, 170554.578030716], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1540800.0000, 
sim time next is 1541400.0000, 
raw observation next is [23.51666666666667, 83.5, 1.0, 2.0, 0.3583791891732259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549517.5405913909, 549517.5405913909, 170532.6455839205], 
processed observation next is [0.0, 0.8695652173913043, 0.31358609794628767, 0.835, 1.0, 1.0, 0.22696287852195893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15264376127538637, 0.15264376127538637, 0.25452633669241864], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.7047209], dtype=float32), 0.4240083]. 
=============================================
[2019-03-26 20:02:25,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:02:25,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4433
[2019-03-26 20:02:25,894] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 87.5, 1.0, 2.0, 0.5092486435614985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711600.1871100579, 711600.1871100579, 185058.0255248951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1715400.0000, 
sim time next is 1716000.0000, 
raw observation next is [26.5, 88.0, 1.0, 2.0, 0.508513914151567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710573.1672812877, 710573.167281287, 184941.0680809185], 
processed observation next is [1.0, 0.8695652173913043, 0.4549763033175356, 0.88, 1.0, 1.0, 0.40784808933923733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19738143535591327, 0.19738143535591307, 0.2760314448968933], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.54747427], dtype=float32), -1.7347125]. 
=============================================
[2019-03-26 20:02:25,911] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.210846]
 [74.04094 ]
 [74.10898 ]
 [74.25136 ]
 [73.94854 ]], R is [[74.19554901]
 [74.17739105]
 [74.15929413]
 [74.14172363]
 [74.12470245]].
[2019-03-26 20:02:32,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0608208e-37 1.0000000e+00 2.1941006e-34 0.0000000e+00 1.3768080e-36], sum to 1.0000
[2019-03-26 20:02:32,726] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7559
[2019-03-26 20:02:32,730] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.7594696038246377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1162191.58322926, 1162191.58322926, 247302.023874254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1771800.0000, 
sim time next is 1772400.0000, 
raw observation next is [23.16666666666667, 85.66666666666667, 1.0, 2.0, 0.6935849271315306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064085.705241788, 1064085.705241787, 231391.2392456986], 
processed observation next is [1.0, 0.5217391304347826, 0.2969984202211693, 0.8566666666666667, 1.0, 1.0, 0.6308252134114827, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29557936256716333, 0.29557936256716305, 0.3453600585756696], 
reward next is 0.6546, 
noisyNet noise sample is [array([-1.1673819], dtype=float32), -0.5433093]. 
=============================================
[2019-03-26 20:02:40,681] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 20:02:40,686] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:02:40,687] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:02:40,688] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:02:40,688] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:02:40,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:02:40,690] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:02:40,691] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:02:40,692] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:02:40,694] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:02:40,689] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:02:40,706] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-03-26 20:02:40,707] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-03-26 20:02:40,725] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-03-26 20:02:40,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-03-26 20:02:40,775] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-03-26 20:03:24,391] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:03:24,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 100.0, 1.0, 2.0, 0.3300879593457952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513241.08261196, 513241.0826119606, 167820.2310250227]
[2019-03-26 20:03:24,393] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:03:24,396] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5669658463801638
[2019-03-26 20:03:43,353] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:03:43,354] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.0, 59.0, 1.0, 2.0, 0.6302051105377229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 880689.1553221784, 880689.1553221778, 206508.3735358223]
[2019-03-26 20:03:43,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:03:43,356] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:03:43,357] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.74988257333333, 85.21653490333334, 1.0, 2.0, 0.6931490882293797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 968691.2333678192, 968691.2333678186, 219382.125401291]
[2019-03-26 20:03:43,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:03:43,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3838471072559031
[2019-03-26 20:03:43,361] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0692662617908727
[2019-03-26 20:04:05,848] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:04:05,848] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.55, 80.5, 1.0, 2.0, 0.7393261843622403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1033256.144428274, 1033256.144428274, 229566.2089398333]
[2019-03-26 20:04:05,849] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:04:05,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01067325218293691
[2019-03-26 20:04:18,141] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:04:18,144] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98506899666667, 57.58447064333333, 1.0, 2.0, 0.5288650063204039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807168.6243793704, 807168.6243793704, 196499.417728309]
[2019-03-26 20:04:18,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:04:18,152] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3513333311579022
[2019-03-26 20:04:24,400] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33689347]
[2019-03-26 20:04:24,402] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.6, 92.66666666666666, 1.0, 2.0, 0.5816025895820179, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9680344864713892, 6.911200000000001, 6.9112, 168.9129564458975, 1626765.849168869, 1626765.849168868, 346699.9389117768]
[2019-03-26 20:04:24,404] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:04:24,405] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.39428468423782337
[2019-03-26 20:04:34,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.0966 3007867509.5417 1766.0000
[2019-03-26 20:04:34,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.5078 2779423486.9439 933.0000
[2019-03-26 20:04:34,941] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.9141 3164254244.9626 1778.0000
[2019-03-26 20:04:34,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.6521 2842708650.5645 1131.0000
[2019-03-26 20:04:35,043] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.2074 2927546585.9425 1338.0000
[2019-03-26 20:04:36,058] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 475000, evaluation results [475000.0, 7881.914110821881, 3164254244.9625816, 1778.0, 8252.207390139796, 2927546585.9425, 1338.0, 8658.507764904378, 2779423486.9438543, 933.0, 7996.096616164504, 3007867509.5416894, 1766.0, 8494.652070649916, 2842708650.564488, 1131.0]
[2019-03-26 20:04:37,045] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.4684702e-37 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 20:04:37,054] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1382
[2019-03-26 20:04:37,064] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 82.0, 1.0, 2.0, 0.9743470879759385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390709.438635488, 1390709.438635488, 295639.1465482148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1933200.0000, 
sim time next is 1933800.0000, 
raw observation next is [25.85, 81.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.009109463974895, 6.9112, 168.9124048539953, 1553421.696664891, 1483961.579989437, 316169.3551255277], 
processed observation next is [1.0, 0.391304347826087, 0.4241706161137442, 0.8166666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.009790946397489541, 0.0, 0.829437236266654, 0.4315060268513586, 0.41221154999706583, 0.4718945598888473], 
reward next is 0.0386, 
noisyNet noise sample is [array([-0.6721469], dtype=float32), -0.49256068]. 
=============================================
[2019-03-26 20:04:43,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:43,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4020
[2019-03-26 20:04:43,208] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5037652164897599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703935.3643046204, 703935.3643046204, 184188.2834954883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5033578552556454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703365.9504080118, 703365.9504080118, 184124.0527852743], 
processed observation next is [0.0, 0.43478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4016359701875245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19537943066889216, 0.19537943066889216, 0.27481201908249897], 
reward next is 0.7252, 
noisyNet noise sample is [array([1.7103077], dtype=float32), -0.33156994]. 
=============================================
[2019-03-26 20:04:44,973] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:44,983] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8345
[2019-03-26 20:04:44,990] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 74.5, 1.0, 2.0, 0.5522999413513621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771779.907337789, 771779.9073377895, 192196.7328692878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2115000.0000, 
sim time next is 2115600.0000, 
raw observation next is [30.0, 74.66666666666667, 1.0, 2.0, 0.5532418400802809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773096.5887330913, 773096.5887330906, 192359.046273165], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7466666666666667, 1.0, 1.0, 0.46173715672322996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21474905242585868, 0.21474905242585848, 0.28710305413905224], 
reward next is 0.7129, 
noisyNet noise sample is [array([-0.43184173], dtype=float32), 0.45243663]. 
=============================================
[2019-03-26 20:04:45,794] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:45,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4950
[2019-03-26 20:04:45,807] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.4, 75.0, 1.0, 2.0, 0.572840619621864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800494.1171533844, 800494.1171533844, 195795.676688901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2129400.0000, 
sim time next is 2130000.0000, 
raw observation next is [30.43333333333333, 74.66666666666666, 1.0, 2.0, 0.5726233780259352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800190.4268804025, 800190.4268804025, 195756.90941491], 
processed observation next is [0.0, 0.6521739130434783, 0.6413902053712479, 0.7466666666666666, 1.0, 1.0, 0.4850884072601629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2222751185778896, 0.2222751185778896, 0.2921744916640448], 
reward next is 0.7078, 
noisyNet noise sample is [array([-0.09302478], dtype=float32), 0.10253308]. 
=============================================
[2019-03-26 20:04:45,822] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.19962 ]
 [68.17225 ]
 [68.152306]
 [68.16992 ]
 [68.14437 ]], R is [[68.25693512]
 [68.28213501]
 [68.30685425]
 [68.33029175]
 [68.35423279]].
[2019-03-26 20:04:47,043] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:47,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1443
[2019-03-26 20:04:47,060] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.4583683932920895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648304.8621784303, 648304.8621784303, 178340.272852537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2089200.0000, 
sim time next is 2089800.0000, 
raw observation next is [23.9, 97.5, 1.0, 2.0, 0.4569873957359317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646711.1113267598, 646711.1113267604, 178184.7499755667], 
processed observation next is [0.0, 0.17391304347826086, 0.33175355450236965, 0.975, 1.0, 1.0, 0.3457679466697972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1796419753685444, 0.17964197536854454, 0.26594738802323387], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.24999876], dtype=float32), 0.6451837]. 
=============================================
[2019-03-26 20:04:47,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:47,217] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2965
[2019-03-26 20:04:47,221] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.25, 75.0, 1.0, 2.0, 0.5683418105705752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794205.0789347552, 794205.0789347552, 194996.6378245307], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2136600.0000, 
sim time next is 2137200.0000, 
raw observation next is [30.06666666666666, 75.66666666666666, 1.0, 2.0, 0.5661459106881557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791135.3693099535, 791135.3693099535, 194608.8102520434], 
processed observation next is [0.0, 0.7391304347826086, 0.6240126382306473, 0.7566666666666666, 1.0, 1.0, 0.47728422974476586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21975982480832043, 0.21975982480832043, 0.2904609108239454], 
reward next is 0.7095, 
noisyNet noise sample is [array([1.0664337], dtype=float32), -0.016247958]. 
=============================================
[2019-03-26 20:04:51,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:04:51,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2048
[2019-03-26 20:04:51,889] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 73.33333333333334, 1.0, 2.0, 0.5552965038335744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775968.8113924285, 775968.8113924279, 192714.5463767894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2226000.0000, 
sim time next is 2226600.0000, 
raw observation next is [30.35, 74.0, 1.0, 2.0, 0.5545794842767985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774966.4860106757, 774966.4860106757, 192590.4977421925], 
processed observation next is [1.0, 0.782608695652174, 0.637440758293839, 0.74, 1.0, 1.0, 0.4633487762371066, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21526846833629878, 0.21526846833629878, 0.28744850409282463], 
reward next is 0.7126, 
noisyNet noise sample is [array([-0.37757194], dtype=float32), 0.75402653]. 
=============================================
[2019-03-26 20:04:53,207] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.7938605e-32 1.0000000e+00 2.2333766e-26 4.0087965e-33 4.0467110e-28], sum to 1.0000
[2019-03-26 20:04:53,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0109
[2019-03-26 20:04:53,220] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 96.66666666666666, 1.0, 2.0, 0.5738524123772334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801908.541795593, 801908.541795593, 195969.5670910848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2176800.0000, 
sim time next is 2177400.0000, 
raw observation next is [24.55, 96.83333333333334, 1.0, 2.0, 0.5637536282232145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787791.14109876, 787791.14109876, 194182.0596976558], 
processed observation next is [1.0, 0.17391304347826086, 0.3625592417061612, 0.9683333333333334, 1.0, 1.0, 0.47440196171471627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21883087252743333, 0.21883087252743333, 0.28982396969799373], 
reward next is 0.7102, 
noisyNet noise sample is [array([-0.13773032], dtype=float32), -0.9138041]. 
=============================================
[2019-03-26 20:04:58,048] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9606892e-32 1.0000000e+00 9.9375578e-31 8.9841511e-36 1.4974632e-26], sum to 1.0000
[2019-03-26 20:04:58,054] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4559
[2019-03-26 20:04:58,058] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.7262191595118556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014929.476273502, 1014929.476273502, 226603.468039346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2258400.0000, 
sim time next is 2259000.0000, 
raw observation next is [26.1, 86.5, 1.0, 2.0, 0.7114756872993393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 994315.052190677, 994315.0521906777, 223338.8088254442], 
processed observation next is [1.0, 0.13043478260869565, 0.4360189573459717, 0.865, 1.0, 1.0, 0.6523803461437823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2761986256085214, 0.2761986256085216, 0.3333415057096182], 
reward next is 0.6667, 
noisyNet noise sample is [array([-0.73682207], dtype=float32), -2.1418262]. 
=============================================
[2019-03-26 20:04:58,071] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.97778 ]
 [59.135044]
 [59.57444 ]
 [59.515335]
 [59.40994 ]], R is [[58.98796844]
 [59.05987549]
 [59.12075424]
 [59.21009827]
 [59.29276276]].
[2019-03-26 20:04:58,826] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8993864e-34 1.0000000e+00 4.7446710e-30 6.8558213e-38 9.3579443e-29], sum to 1.0000
[2019-03-26 20:04:58,838] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9673
[2019-03-26 20:04:58,843] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 77.33333333333333, 1.0, 2.0, 0.6467653924060522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 903841.4056028577, 903841.4056028584, 209771.9709028536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2272200.0000, 
sim time next is 2272800.0000, 
raw observation next is [27.93333333333334, 76.66666666666667, 1.0, 2.0, 0.661238315044903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 924075.8179274382, 924075.8179274389, 212698.9042671591], 
processed observation next is [1.0, 0.30434782608695654, 0.5229067930489735, 0.7666666666666667, 1.0, 1.0, 0.5918533916203651, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2566877272020662, 0.25668772720206634, 0.31746105114501355], 
reward next is 0.6825, 
noisyNet noise sample is [array([1.4774855], dtype=float32), -0.962308]. 
=============================================
[2019-03-26 20:05:06,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 1.190003e-34 0.000000e+00 6.289336e-38], sum to 1.0000
[2019-03-26 20:05:06,762] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8822
[2019-03-26 20:05:06,768] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.43333333333334, 70.66666666666667, 1.0, 2.0, 0.5799419195826917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810421.3417291143, 810421.3417291143, 197069.1712000144], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2402400.0000, 
sim time next is 2403000.0000, 
raw observation next is [31.3, 71.5, 1.0, 2.0, 0.5793604633452023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809608.4945816617, 809608.4945816617, 196964.4400551743], 
processed observation next is [1.0, 0.8260869565217391, 0.6824644549763034, 0.715, 1.0, 1.0, 0.4932053775243401, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22489124849490602, 0.22489124849490602, 0.29397677620175267], 
reward next is 0.7060, 
noisyNet noise sample is [array([0.5364439], dtype=float32), -3.7421198]. 
=============================================
[2019-03-26 20:05:06,792] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.522762]
 [58.838524]
 [58.864285]
 [59.012157]
 [59.58466 ]], R is [[58.34870529]
 [58.47108841]
 [58.59238434]
 [58.71340179]
 [58.83403015]].
[2019-03-26 20:05:07,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 7.188034e-34 0.000000e+00 5.635911e-33], sum to 1.0000
[2019-03-26 20:05:07,221] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8214
[2019-03-26 20:05:07,225] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 82.66666666666667, 1.0, 2.0, 0.736007205789709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1028615.410809548, 1028615.410809548, 228809.0875869458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2434800.0000, 
sim time next is 2435400.0000, 
raw observation next is [27.75, 83.0, 1.0, 2.0, 0.7142453982520539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 998187.6505371014, 998187.650537102, 223949.9227639498], 
processed observation next is [1.0, 0.17391304347826086, 0.514218009478673, 0.83, 1.0, 1.0, 0.6557173472916312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27727434737141704, 0.2772743473714172, 0.3342536160655967], 
reward next is 0.6657, 
noisyNet noise sample is [array([-0.4434104], dtype=float32), -0.13481835]. 
=============================================
[2019-03-26 20:05:09,123] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.4011501e-36 1.0000000e+00 1.2977996e-29 0.0000000e+00 2.6019209e-36], sum to 1.0000
[2019-03-26 20:05:09,132] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3696
[2019-03-26 20:05:09,137] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 94.16666666666667, 1.0, 2.0, 0.5484156411384205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766350.0565213942, 766350.0565213948, 191529.5675732923], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2506200.0000, 
sim time next is 2506800.0000, 
raw observation next is [26.63333333333333, 94.33333333333334, 1.0, 2.0, 0.5478081280687839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765500.8184588548, 765500.8184588548, 191425.7336431171], 
processed observation next is [1.0, 0.0, 0.46129541864139006, 0.9433333333333335, 1.0, 1.0, 0.4551905157455227, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2126391162385708, 0.2126391162385708, 0.2857100502136076], 
reward next is 0.7143, 
noisyNet noise sample is [array([1.7890986], dtype=float32), -1.3536166]. 
=============================================
[2019-03-26 20:05:19,186] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:05:19,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6158
[2019-03-26 20:05:19,201] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4741184900357455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662626.665547584, 662626.665547584, 179657.0114156023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2622000.0000, 
sim time next is 2622600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4745087563260036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663170.6458928146, 663170.6458928146, 179714.9691434588], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36687801966988387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1842140683035596, 0.1842140683035596, 0.268231297229043], 
reward next is 0.7318, 
noisyNet noise sample is [array([-1.7183589], dtype=float32), -1.0379531]. 
=============================================
[2019-03-26 20:05:23,499] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:05:23,508] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1065
[2019-03-26 20:05:23,512] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 97.33333333333333, 1.0, 2.0, 0.4605251161129595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649945.3692947906, 649945.3692947912, 178475.3750857814], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2695200.0000, 
sim time next is 2695800.0000, 
raw observation next is [24.0, 97.66666666666667, 1.0, 2.0, 0.462076859757189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650915.6321728296, 650915.632172829, 178546.2408703369], 
processed observation next is [0.0, 0.17391304347826086, 0.3364928909952607, 0.9766666666666667, 1.0, 1.0, 0.35189983103275785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.180809897825786, 0.18080989782578583, 0.26648692667214463], 
reward next is 0.7335, 
noisyNet noise sample is [array([-0.0932379], dtype=float32), 0.48027444]. 
=============================================
[2019-03-26 20:05:27,782] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4922420e-38 1.0000000e+00 2.1556934e-37 0.0000000e+00 3.4488530e-36], sum to 1.0000
[2019-03-26 20:05:27,790] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3579
[2019-03-26 20:05:27,799] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3359076187736589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 517463.466596218, 517463.4665962186, 167998.4663704457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2785200.0000, 
sim time next is 2785800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3387719845445776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521876.3759363471, 521876.3759363471, 168348.8799461964], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2033397404151537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14496565998231864, 0.14496565998231864, 0.251266984994323], 
reward next is 0.7487, 
noisyNet noise sample is [array([-0.6509885], dtype=float32), 1.0839958]. 
=============================================
[2019-03-26 20:05:29,015] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.0266388e-38 0.0000000e+00 1.5837998e-31], sum to 1.0000
[2019-03-26 20:05:29,024] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5534
[2019-03-26 20:05:29,028] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3307810144630378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513519.0467604169, 513519.0467604169, 167817.558638361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2779200.0000, 
sim time next is 2779800.0000, 
raw observation next is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.358306491563518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555958.1000476147, 555958.1000476147, 171247.8310646035], 
processed observation next is [1.0, 0.17391304347826086, 0.2022116903633494, 0.9900000000000001, 1.0, 1.0, 0.22687529104038315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15443280556878186, 0.15443280556878186, 0.25559377770836345], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.48726097], dtype=float32), 0.23340087]. 
=============================================
[2019-03-26 20:05:29,717] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:05:29,726] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2195
[2019-03-26 20:05:29,732] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 83.0, 1.0, 2.0, 0.6701957018074916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1026873.463135121, 1026873.463135121, 225814.6117439364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2806800.0000, 
sim time next is 2807400.0000, 
raw observation next is [23.83333333333333, 83.0, 1.0, 2.0, 0.699917861205047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067141.025263553, 1067141.025263553, 232137.4131958989], 
processed observation next is [1.0, 0.4782608695652174, 0.32859399684044216, 0.83, 1.0, 1.0, 0.6384552544639119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2964280625732092, 0.2964280625732092, 0.3464737510386551], 
reward next is 0.6535, 
noisyNet noise sample is [array([-1.2246941], dtype=float32), 0.7876281]. 
=============================================
[2019-03-26 20:05:31,730] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 20:05:31,734] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:05:31,735] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:05:31,735] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:05:31,736] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:05:31,736] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:05:31,737] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:05:31,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:05:31,737] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:05:31,738] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:05:31,739] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:05:31,747] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-03-26 20:05:31,748] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-03-26 20:05:31,765] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-03-26 20:05:31,782] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-03-26 20:05:31,816] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-03-26 20:05:40,513] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:05:40,514] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.64540216, 81.32142433000001, 1.0, 2.0, 0.2290211361367389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 379162.4538327176, 379162.4538327169, 158791.3602147718]
[2019-03-26 20:05:40,516] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:05:40,519] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14322148808240864
[2019-03-26 20:05:48,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:05:48,660] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.93460414, 94.739570205, 1.0, 2.0, 0.3107979332917399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 493117.2650736825, 493117.2650736825, 166545.3992115085]
[2019-03-26 20:05:48,662] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:05:48,664] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28484858157710147
[2019-03-26 20:06:14,681] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:06:14,682] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.64567831666667, 95.50068749833333, 1.0, 2.0, 0.3810537608759284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572558.4262883748, 572558.4262883748, 172169.0797015345]
[2019-03-26 20:06:14,686] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:06:14,689] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40806767755516526
[2019-03-26 20:06:36,243] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:06:36,245] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.5, 59.5, 1.0, 2.0, 0.8646531660992057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1208508.376857906, 1208508.376857906, 260405.5026196333]
[2019-03-26 20:06:36,246] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:06:36,248] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.0115368816224235
[2019-03-26 20:06:49,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:06:49,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.471567655, 80.393748405, 1.0, 2.0, 0.4220683058947143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 620941.8913883708, 620941.8913883708, 176265.46343554]
[2019-03-26 20:06:49,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:06:49,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7127804609768746
[2019-03-26 20:07:19,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:07:19,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.29946966666666, 89.87519880666667, 1.0, 2.0, 0.4075522532725361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669383.5971597641, 669383.5971597636, 180588.6424893038]
[2019-03-26 20:07:19,851] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:07:19,855] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8016097727199873
[2019-03-26 20:07:26,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3217067]
[2019-03-26 20:07:26,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.97211070166667, 73.06263382833333, 1.0, 2.0, 0.8382349316882268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1199589.311347537, 1199589.311347537, 257512.6973804204]
[2019-03-26 20:07:26,269] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:07:26,274] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06857598985045255
[2019-03-26 20:07:27,794] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7994.6151 3008006107.9315 1766.0000
[2019-03-26 20:07:28,106] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.1698 2842847906.5028 1131.0000
[2019-03-26 20:07:28,128] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.7669 2779497663.8967 933.0000
[2019-03-26 20:07:28,218] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7881.1591 3164313068.1626 1778.0000
[2019-03-26 20:07:28,267] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8250.7258 2927690427.6328 1338.0000
[2019-03-26 20:07:29,282] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 500000, evaluation results [500000.0, 7881.159078603526, 3164313068.162566, 1778.0, 8250.725820219795, 2927690427.6327953, 1338.0, 8657.766851449904, 2779497663.8966665, 933.0, 7994.615069066108, 3008006107.9315424, 1766.0, 8493.169770734836, 2842847906.5028043, 1131.0]
[2019-03-26 20:07:29,811] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 3.42554e-36 0.00000e+00 0.00000e+00], sum to 1.0000
[2019-03-26 20:07:29,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6008
[2019-03-26 20:07:29,827] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3502578939371979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539562.0788390585, 539562.0788390585, 169780.5686765022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2853600.0000, 
sim time next is 2854200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3499797968893803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539133.7468120259, 539133.7468120259, 169745.3673435445], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2168431287823859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14975937411445164, 0.14975937411445164, 0.25335129454260374], 
reward next is 0.7466, 
noisyNet noise sample is [array([-0.09435907], dtype=float32), -0.3591685]. 
=============================================
[2019-03-26 20:07:31,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:31,014] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6557
[2019-03-26 20:07:31,021] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3159174402727212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499659.2191652672, 499659.2191652672, 167001.6353500614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2930400.0000, 
sim time next is 2931000.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.3162247988953518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 500056.7546586314, 500056.754658632, 167029.6317560708], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.17617445650042388, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13890465407184205, 0.13890465407184222, 0.2492979578448818], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.06199252], dtype=float32), 1.7754813]. 
=============================================
[2019-03-26 20:07:31,036] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[77.46165 ]
 [77.441986]
 [77.43319 ]
 [77.394615]
 [77.370155]], R is [[77.4163208 ]
 [77.39289856]
 [77.36963654]
 [77.34660339]
 [77.32397461]].
[2019-03-26 20:07:35,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:35,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3640
[2019-03-26 20:07:35,240] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.3201091971202503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506196.7057556303, 506196.7057556303, 167491.6476772758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2952600.0000, 
sim time next is 2953200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3103552240950783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490769.21008797, 490769.21008797, 166340.4255789926], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.1691026796326245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13632478057999167, 0.13632478057999167, 0.2482692919089442], 
reward next is 0.7517, 
noisyNet noise sample is [array([-1.1103095], dtype=float32), -0.88399655]. 
=============================================
[2019-03-26 20:07:37,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:37,820] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8811
[2019-03-26 20:07:37,827] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3055869598939788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486628.50575581, 486628.5057558094, 166099.293527577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3018600.0000, 
sim time next is 3019200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3050170820939111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485721.227767029, 485721.2277670284, 166033.5916795073], 
processed observation next is [1.0, 0.9565217391304348, 0.1469194312796209, 1.0, 1.0, 1.0, 0.162671183245676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13492256326861918, 0.13492256326861898, 0.24781133086493629], 
reward next is 0.7522, 
noisyNet noise sample is [array([-1.7277793], dtype=float32), -0.051139254]. 
=============================================
[2019-03-26 20:07:41,034] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:41,044] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9829
[2019-03-26 20:07:41,050] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3539231406670067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563599.6850036548, 563599.6850036555, 172083.1644948465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3031800.0000, 
sim time next is 3032400.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3275383449927264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521563.783305758, 521563.783305758, 168714.6045830214], 
processed observation next is [1.0, 0.08695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.18980523493099563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1448788286960439, 0.1448788286960439, 0.25181284266122594], 
reward next is 0.7482, 
noisyNet noise sample is [array([1.926786], dtype=float32), -0.6898692]. 
=============================================
[2019-03-26 20:07:43,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0856662e-33 1.0000000e+00 2.2942953e-29 1.0856474e-37 1.0146024e-28], sum to 1.0000
[2019-03-26 20:07:43,684] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1067
[2019-03-26 20:07:43,690] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.7874252853470698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174616.704686965, 1174616.704686964, 250870.7695471696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3065400.0000, 
sim time next is 3066000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.7852241844331339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1171331.216517576, 1171331.216517575, 250300.4449124924], 
processed observation next is [1.0, 0.4782608695652174, 0.28909952606635075, 0.94, 1.0, 1.0, 0.741233957148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32536978236599334, 0.32536978236599307, 0.37358275360073495], 
reward next is 0.6264, 
noisyNet noise sample is [array([-2.0880587], dtype=float32), -0.95240164]. 
=============================================
[2019-03-26 20:07:43,715] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[60.93431 ]
 [61.069187]
 [61.13716 ]
 [61.93798 ]
 [62.071575]], R is [[60.88972855]
 [60.90639496]
 [60.92657471]
 [60.93266678]
 [60.97953415]].
[2019-03-26 20:07:48,990] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:48,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6243
[2019-03-26 20:07:49,003] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 82.33333333333334, 1.0, 2.0, 0.5879840693374689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831913.991455402, 831913.991455402, 199849.2379311827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3156000.0000, 
sim time next is 3156600.0000, 
raw observation next is [26.0, 83.16666666666666, 1.0, 2.0, 0.5879904127852683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 827587.1979108624, 827587.197910863, 199290.3774250898], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.8316666666666666, 1.0, 1.0, 0.5036029069702027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22988533275301734, 0.22988533275301748, 0.2974483245150594], 
reward next is 0.7026, 
noisyNet noise sample is [array([0.18318215], dtype=float32), -0.80430764]. 
=============================================
[2019-03-26 20:07:57,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 20:07:57,638] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3393
[2019-03-26 20:07:57,647] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5880381520989124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821739.5401817085, 821739.5401817085, 198538.2724028351], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3327000.0000, 
sim time next is 3327600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5758696029024122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804728.4569005421, 804728.4569005414, 196337.1040093072], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.48899952156917126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22353568247237282, 0.22353568247237263, 0.29304045374523463], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.73954576], dtype=float32), -0.022232767]. 
=============================================
[2019-03-26 20:08:01,133] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.6009223e-28 1.0000000e+00 2.9714183e-29 1.6656052e-31 2.8844767e-26], sum to 1.0000
[2019-03-26 20:08:01,144] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3671
[2019-03-26 20:08:01,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2846785.037490111 W.
[2019-03-26 20:08:01,159] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 1.017667541635666, 1.0, 2.0, 1.017667541635666, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2846785.037490111, 2846785.037490111, 539757.8209319953], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3422400.0000, 
sim time next is 3423000.0000, 
raw observation next is [34.0, 60.5, 1.0, 2.0, 0.7232951234917071, 1.0, 2.0, 0.6822376012601161, 1.0, 1.0, 1.03, 7.005099569026683, 6.9112, 170.5573041426782, 2862717.076860587, 2795452.995976515, 528788.1714741496], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.605, 1.0, 1.0, 0.6666206307129001, 1.0, 1.0, 0.6171537364579712, 1.0, 0.5, 1.0365853658536586, 0.009389956902668306, 0.0, 0.8375144448122397, 0.7951991880168298, 0.7765147211045875, 0.789236076827089], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.319468], dtype=float32), -1.4081175]. 
=============================================
[2019-03-26 20:08:01,180] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[51.473434]
 [51.19404 ]
 [52.52235 ]
 [52.46817 ]
 [53.546124]], R is [[50.70763397]
 [50.20055771]
 [49.69855118]
 [49.20156479]
 [48.70954895]].
[2019-03-26 20:08:06,444] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 8.9384715e-38 0.0000000e+00 1.9305847e-35], sum to 1.0000
[2019-03-26 20:08:06,451] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2444
[2019-03-26 20:08:06,455] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5113292983090572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714508.5743503075, 714508.574350308, 185390.1024351827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3451200.0000, 
sim time next is 3451800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5110902771966108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714174.4647164326, 714174.4647164332, 185351.8733841149], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4109521412007359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983817957545646, 0.19838179575456477, 0.27664458714047], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.28046346], dtype=float32), -0.6391102]. 
=============================================
[2019-03-26 20:08:09,513] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9056371e-33 1.0000000e+00 1.9831662e-33 5.0669041e-38 4.7739896e-31], sum to 1.0000
[2019-03-26 20:08:09,521] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1537
[2019-03-26 20:08:09,531] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.0, 1.0, 2.0, 0.7631696739617206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1066595.697582479, 1066595.697582479, 235069.5729072058], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3567000.0000, 
sim time next is 3567600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.7939722997936991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1109667.527557437, 1109667.527557437, 242435.3599111704], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.79, 1.0, 1.0, 0.7517738551731314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30824097987706583, 0.30824097987706583, 0.3618438207629409], 
reward next is 0.6382, 
noisyNet noise sample is [array([0.47915384], dtype=float32), -0.318621]. 
=============================================
[2019-03-26 20:08:15,101] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.68213656e-30 1.00000000e+00 4.50933325e-30 3.56576954e-33
 1.06130966e-26], sum to 1.0000
[2019-03-26 20:08:15,109] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4459
[2019-03-26 20:08:15,113] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 72.0, 1.0, 2.0, 0.8874003259451771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240320.195969404, 1240320.195969404, 266492.5916803419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3655800.0000, 
sim time next is 3656400.0000, 
raw observation next is [28.66666666666666, 71.33333333333333, 1.0, 2.0, 0.7719786979650235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078913.32569186, 1078913.32569186, 237146.3100996096], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.7133333333333333, 1.0, 1.0, 0.7252755397168957, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29969814602551664, 0.29969814602551664, 0.3539497165665815], 
reward next is 0.6461, 
noisyNet noise sample is [array([-1.3204961], dtype=float32), -1.1507971]. 
=============================================
[2019-03-26 20:08:18,559] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1969028e-37 1.0000000e+00 1.0072809e-32 0.0000000e+00 3.1959619e-30], sum to 1.0000
[2019-03-26 20:08:18,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2039
[2019-03-26 20:08:18,567] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5250013921539649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733619.94084384, 733619.9408438393, 187605.9176718834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3701400.0000, 
sim time next is 3702000.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5240767536253047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732327.4355252552, 732327.4355252557, 187454.3107003784], 
processed observation next is [1.0, 0.8695652173913043, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4265984983437406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2034242876459042, 0.20342428764590437, 0.27978255328414686], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.91275513], dtype=float32), 0.9340139]. 
=============================================
[2019-03-26 20:08:18,580] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[67.88398]
 [68.05043]
 [68.01315]
 [68.05253]
 [68.80117]], R is [[67.67906189]
 [67.72226715]
 [67.76480103]
 [67.80571747]
 [67.84494019]].
[2019-03-26 20:08:21,015] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.3481973e-33 0.0000000e+00 0.0000000e+00], sum to 1.0000
[2019-03-26 20:08:21,022] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6044
[2019-03-26 20:08:21,028] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4904130667426979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685271.7453175467, 685271.7453175472, 182107.3996066342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3712800.0000, 
sim time next is 3713400.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4905703973505663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685491.6599443932, 685491.6599443926, 182131.5274935262], 
processed observation next is [1.0, 1.0, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3862293943982726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19041434998455367, 0.1904143499845535, 0.27183810073660625], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.36405393], dtype=float32), 0.6179369]. 
=============================================
[2019-03-26 20:08:23,034] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7538745e-25 1.0000000e+00 1.7893732e-26 1.2444317e-27 3.6658373e-23], sum to 1.0000
[2019-03-26 20:08:23,041] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6525
[2019-03-26 20:08:23,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1997699.901792459 W.
[2019-03-26 20:08:23,052] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.7143895955335277, 1.0, 1.0, 0.7143895955335277, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1997699.901792459, 1997699.90179246, 380337.0579605121], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3747600.0000, 
sim time next is 3748200.0000, 
raw observation next is [29.33333333333334, 68.83333333333334, 1.0, 2.0, 0.4170002574604792, 1.0, 2.0, 0.4170002574604792, 1.0, 1.0, 0.7078813455336952, 6.9112, 6.9112, 170.5573041426782, 1748929.631093716, 1748929.631093716, 360067.1115085815], 
processed observation next is [1.0, 0.391304347826087, 0.5892575039494474, 0.6883333333333335, 1.0, 1.0, 0.29759067163913155, 1.0, 1.0, 0.29759067163913155, 1.0, 0.5, 0.6437577384557259, 0.0, 0.0, 0.8375144448122397, 0.4858137864149211, 0.4858137864149211, 0.5374135992665395], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3283672], dtype=float32), -0.11267612]. 
=============================================
[2019-03-26 20:08:25,004] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 20:08:25,006] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:08:25,006] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:08:25,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:08:25,007] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:08:25,008] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:08:25,008] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:08:25,010] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:08:25,007] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:08:25,011] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:08:25,013] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:08:25,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-03-26 20:08:25,028] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-03-26 20:08:25,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-03-26 20:08:25,047] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-03-26 20:08:25,086] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-03-26 20:08:50,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:08:50,946] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 87.5, 1.0, 2.0, 0.4773347085769531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673279.8978287958, 673279.8978287952, 180930.5758266034]
[2019-03-26 20:08:50,947] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:08:50,950] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1243383e-35 1.0000000e+00 1.2102296e-32 1.5645791e-37 4.7793671e-30], sampled 0.07154255617372285
[2019-03-26 20:09:01,025] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:09:01,025] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.28333333333333, 84.5, 1.0, 2.0, 0.4608227608288043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661698.0000770678, 661698.0000770672, 179961.7693505241]
[2019-03-26 20:09:01,026] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:09:01,029] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0707269e-35 1.0000000e+00 4.4875500e-33 4.9769066e-38 1.9111206e-30], sampled 0.4908752670611407
[2019-03-26 20:09:06,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:09:06,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.56666666666667, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.042663164313307, 6.9112, 168.9119633301264, 1547082.744248375, 1453818.799251418, 311349.9900708033]
[2019-03-26 20:09:06,539] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:09:06,545] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1778193e-29 1.0000000e+00 8.2355569e-27 8.4828111e-31 1.1708849e-24], sampled 0.9268979138914031
[2019-03-26 20:09:17,895] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:09:17,897] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.33333333333334, 62.0, 1.0, 2.0, 0.5634343299824001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787344.787336178, 787344.787336178, 194131.3432552084]
[2019-03-26 20:09:17,899] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:09:17,901] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1008188e-37 1.0000000e+00 6.4612522e-35 0.0000000e+00 3.7991478e-32], sampled 0.08795878922476719
[2019-03-26 20:09:32,005] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:09:32,006] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 0.925532506029299, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000958605181095, 6.9112, 168.9123504361537, 2190775.547024564, 2127097.931737924, 441423.2919498155]
[2019-03-26 20:09:32,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:09:32,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1838972e-26 1.0000000e+00 2.6807856e-24 6.7509825e-28 2.4544898e-22], sampled 0.41828131135318813
[2019-03-26 20:09:32,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2190775.547024564 W.
[2019-03-26 20:09:50,940] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.36609626]
[2019-03-26 20:09:50,942] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.85571235, 95.70338887, 1.0, 2.0, 0.6329811125382967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884570.134554273, 884570.134554273, 207052.1934059689]
[2019-03-26 20:09:50,943] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:09:50,945] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1359009e-36 1.0000000e+00 1.8590651e-33 1.7993147e-38 8.4661304e-31], sampled 0.990863119752848
[2019-03-26 20:10:19,482] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.6450 3164426017.2998 1778.0000
[2019-03-26 20:10:19,538] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7993.1316 3008143638.3828 1766.0000
[2019-03-26 20:10:19,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8249.2424 2927833197.1978 1338.0000
[2019-03-26 20:10:19,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8655.5390 2779716974.6015 933.0000
[2019-03-26 20:10:19,924] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8491.6834 2842984501.9777 1131.0000
[2019-03-26 20:10:20,938] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 525000, evaluation results [525000.0, 7879.645043455771, 3164426017.2998238, 1778.0, 8249.242360207787, 2927833197.197803, 1338.0, 8655.539002596335, 2779716974.601524, 933.0, 7993.131616413509, 3008143638.3827953, 1766.0, 8491.68339128166, 2842984501.977716, 1131.0]
[2019-03-26 20:10:28,865] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.3349845e-37 0.0000000e+00 1.6501580e-35], sum to 1.0000
[2019-03-26 20:10:28,878] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6066
[2019-03-26 20:10:28,884] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 79.83333333333334, 1.0, 2.0, 0.6345517343025823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886765.9427293071, 886765.9427293071, 207359.2051816386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3971400.0000, 
sim time next is 3972000.0000, 
raw observation next is [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6242372947493807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 872345.9207552606, 872345.9207552613, 205348.4448074379], 
processed observation next is [0.0, 1.0, 0.6524486571879939, 0.8066666666666668, 1.0, 1.0, 0.5472738490956394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24231831132090573, 0.24231831132090592, 0.3064902161305043], 
reward next is 0.6935, 
noisyNet noise sample is [array([-1.2259164], dtype=float32), 1.4956585]. 
=============================================
[2019-03-26 20:10:28,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.444534]
 [74.549995]
 [74.56772 ]
 [74.58438 ]
 [74.57733 ]], R is [[74.41160583]
 [74.35799408]
 [74.30873108]
 [74.26070404]
 [74.21385193]].
[2019-03-26 20:10:37,855] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.3617847e-18 1.0000000e+00 3.1544741e-17 3.6715062e-15 2.7848241e-16], sum to 1.0000
[2019-03-26 20:10:37,864] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5165
[2019-03-26 20:10:37,872] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2586181.401809486 W.
[2019-03-26 20:10:37,878] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 75.0, 1.0, 2.0, 0.9246033876608276, 1.0, 2.0, 0.9246033876608276, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2586181.401809486, 2586181.401809486, 485100.8140621254], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4098600.0000, 
sim time next is 4099200.0000, 
raw observation next is [31.66666666666666, 73.66666666666666, 1.0, 2.0, 0.964535629045208, 1.0, 2.0, 0.964535629045208, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2697995.319194643, 2697995.319194642, 507930.3636028414], 
processed observation next is [1.0, 0.43478260869565216, 0.6998420221169034, 0.7366666666666666, 1.0, 1.0, 0.9572718422231421, 1.0, 1.0, 0.9572718422231421, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7494431442207341, 0.7494431442207339, 0.7581050203027483], 
reward next is 0.2419, 
noisyNet noise sample is [array([0.7569764], dtype=float32), 0.18995911]. 
=============================================
[2019-03-26 20:10:42,499] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.3237927e-37 1.0000000e+00 6.0211385e-38 5.0341234e-34 1.7667679e-32], sum to 1.0000
[2019-03-26 20:10:42,508] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6254
[2019-03-26 20:10:42,512] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5855470420180096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818257.0583969193, 818257.0583969193, 198083.9477058538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4144800.0000, 
sim time next is 4145400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5839874637341148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 816076.8301808379, 816076.8301808379, 197800.5362762454], 
processed observation next is [1.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4987800767880901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2266880083835661, 0.2266880083835661, 0.2952246810093215], 
reward next is 0.7048, 
noisyNet noise sample is [array([0.43882138], dtype=float32), -2.1216447]. 
=============================================
[2019-03-26 20:10:44,048] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.10236694e-32 1.00000000e+00 1.14029076e-29 2.09392843e-27
 4.27953102e-20], sum to 1.0000
[2019-03-26 20:10:44,054] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2045
[2019-03-26 20:10:44,059] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.8569236478978629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1197698.891135359, 1197698.891135359, 258369.2484678563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4172400.0000, 
sim time next is 4173000.0000, 
raw observation next is [30.33333333333333, 83.16666666666667, 1.0, 2.0, 0.957621953381459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1338530.896345231, 1338530.896345231, 286286.2908198257], 
processed observation next is [1.0, 0.30434782608695654, 0.6366508688783569, 0.8316666666666667, 1.0, 1.0, 0.9489421125077818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3718141378736753, 0.3718141378736753, 0.4272929713728742], 
reward next is 0.5727, 
noisyNet noise sample is [array([-0.7803873], dtype=float32), -1.5124351]. 
=============================================
[2019-03-26 20:10:44,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[54.592552]
 [54.236984]
 [54.1552  ]
 [54.03357 ]
 [54.02234 ]], R is [[54.23883438]
 [54.31082153]
 [54.38599014]
 [54.47044754]
 [54.56448364]].
[2019-03-26 20:10:45,465] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1724668e-23 1.0000000e+00 1.1508961e-23 2.6483060e-20 9.0283558e-16], sum to 1.0000
[2019-03-26 20:10:45,473] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8308
[2019-03-26 20:10:45,478] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.66666666666666, 1.0, 2.0, 0.5685495654897653, 0.0, 1.0, 0.0, 1.0, 2.0, 0.98706902836242, 6.911200000000001, 6.9112, 168.9129565104293, 1589585.732118708, 1589585.732118707, 347770.8981698806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4243800.0000, 
sim time next is 4244400.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.440011452030399, 6.9112, 168.9099495112963, 1829162.24713929, 1454011.8813528, 311352.6768955276], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0528811452030399, 0.0, 0.8294251794099412, 0.5081006242053583, 0.40389218926466663, 0.46470548790377253], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.19410983], dtype=float32), -0.11126491]. 
=============================================
[2019-03-26 20:10:50,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.9666236e-13 1.0000000e+00 1.4290713e-12 2.3361221e-10 4.0469508e-09], sum to 1.0000
[2019-03-26 20:10:50,457] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6182
[2019-03-26 20:10:50,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2894563.976575692 W.
[2019-03-26 20:10:50,470] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.83333333333334, 57.5, 1.0, 2.0, 0.7384569487076226, 1.0, 2.0, 0.6898185138680738, 1.0, 2.0, 1.03, 7.005100764646556, 6.9112, 170.5573041426782, 2894563.976575692, 2827299.039220471, 533753.1604784], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4276200.0000, 
sim time next is 4276800.0000, 
raw observation next is [37.0, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.247884763812026, 6.9112, 170.5573041426782, 3150791.659889818, 2909610.668110705, 551882.7092115207], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.57, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.03366847638120261, 0.0, 0.8375144448122397, 0.8752199055249494, 0.808225185586307, 0.8237055361365981], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5845622], dtype=float32), -1.1659943]. 
=============================================
[2019-03-26 20:10:51,247] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1241136e-11 9.9999940e-01 3.3047321e-12 4.3633078e-10 5.5393201e-07], sum to 1.0000
[2019-03-26 20:10:51,254] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3326
[2019-03-26 20:10:51,260] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2734952.709483574 W.
[2019-03-26 20:10:51,273] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [37.5, 55.5, 1.0, 2.0, 0.6624645627300093, 1.0, 2.0, 0.6518223208792673, 1.0, 2.0, 1.03, 7.005094772708351, 6.9112, 170.5573041426782, 2734952.709483574, 2667692.064397425, 509737.6381493793], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4278600.0000, 
sim time next is 4279200.0000, 
raw observation next is [37.66666666666666, 55.0, 1.0, 2.0, 0.7548039072733624, 1.0, 2.0, 0.6979919931509437, 1.0, 2.0, 1.03, 7.005102053791051, 6.9112, 170.5573041426782, 2928901.118359112, 2861635.257537249, 539199.9964760432], 
processed observation next is [1.0, 0.5217391304347826, 0.9842022116903629, 0.55, 1.0, 1.0, 0.70458302081128, 1.0, 1.0, 0.6361349315071612, 1.0, 1.0, 1.0365853658536586, 0.009390205379105065, 0.0, 0.8375144448122397, 0.8135836439886422, 0.7948986826492358, 0.8047761141433482], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03787515], dtype=float32), -1.2566175]. 
=============================================
[2019-03-26 20:10:53,587] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3237834e-15 9.9999976e-01 3.8569480e-15 4.6266867e-15 2.5596046e-07], sum to 1.0000
[2019-03-26 20:10:53,596] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2612
[2019-03-26 20:10:53,603] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3606631.571067888 W.
[2019-03-26 20:10:53,608] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666666, 68.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.8834894268214, 6.9112, 170.5573041426782, 3606631.571067888, 2910141.104430519, 548067.756321329], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4369200.0000, 
sim time next is 4369800.0000, 
raw observation next is [32.83333333333334, 71.5, 1.0, 2.0, 0.888556849194499, 1.0, 2.0, 0.764868464111512, 1.0, 1.0, 1.03, 7.005112604400705, 6.9112, 170.5573041426782, 3209887.546172363, 3142614.127519635, 587503.4509262341], 
processed observation next is [1.0, 0.5652173913043478, 0.7551342812006324, 0.715, 1.0, 1.0, 0.8657311436078301, 1.0, 1.0, 0.7167089929054361, 1.0, 0.5, 1.0365853658536586, 0.009391260440070503, 0.0, 0.8375144448122397, 0.8916354294923231, 0.8729483687554541, 0.8768708222779614], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4532057], dtype=float32), 0.09206383]. 
=============================================
[2019-03-26 20:11:03,864] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2877088e-34 1.0000000e+00 7.8099477e-35 4.7756649e-31 2.5901982e-19], sum to 1.0000
[2019-03-26 20:11:03,870] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4203
[2019-03-26 20:11:03,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507745637686434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709499.2557531429, 709499.2557531429, 184818.0670325271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5079039030453599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 184843.2213397491], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4071131361992288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19714457833561494, 0.19714457833561513, 0.27588540498470016], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.0874024], dtype=float32), -2.1496625]. 
=============================================
[2019-03-26 20:11:05,642] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7639675e-36 1.0000000e+00 1.8776202e-37 8.6666809e-28 3.3648195e-24], sum to 1.0000
[2019-03-26 20:11:05,653] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0956
[2019-03-26 20:11:05,659] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5085835229625861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710670.4678615045, 710670.4678615045, 184951.6564859185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4516200.0000, 
sim time next is 4516800.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5096431343431829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712151.6148201709, 712151.6148201709, 185120.5779413317], 
processed observation next is [0.0, 0.2608695652173913, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4092085955941962, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19781989300560304, 0.19781989300560304, 0.2762993700616891], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.21725522], dtype=float32), -1.3373567]. 
=============================================
[2019-03-26 20:11:16,742] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 20:11:16,745] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:11:16,745] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:11:16,746] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:11:16,746] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:11:16,747] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:11:16,748] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:11:16,750] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:11:16,747] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:11:16,752] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:11:16,754] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:11:16,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-03-26 20:11:16,794] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-03-26 20:11:16,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-03-26 20:11:16,842] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-03-26 20:11:16,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-03-26 20:11:27,866] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:11:27,867] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [15.8396115, 93.28156783, 1.0, 2.0, 0.1985846862061041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 332178.9508946795, 332178.9508946795, 133830.1076957567]
[2019-03-26 20:11:27,868] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:11:27,871] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.3544501e-27 1.0000000e+00 1.2929879e-27 2.0728169e-15 4.8058193e-16], sampled 0.36926870018467395
[2019-03-26 20:11:52,013] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:11:52,014] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.35, 86.0, 1.0, 2.0, 0.4027804142624438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594767.5411885501, 594767.5411885501, 173869.3017970506]
[2019-03-26 20:11:52,016] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:11:52,018] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5356318e-26 1.0000000e+00 2.7670502e-27 3.1855672e-15 7.5682907e-16], sampled 0.6220178106007449
[2019-03-26 20:12:22,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:12:22,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.35, 65.16666666666666, 1.0, 2.0, 0.5156419716330037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720536.9540435895, 720536.9540435902, 186083.1725914174]
[2019-03-26 20:12:22,557] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:12:22,562] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2967855e-28 1.0000000e+00 1.5253081e-28 6.1984209e-16 1.3418225e-16], sampled 0.1335039435363482
[2019-03-26 20:12:28,323] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:12:28,323] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.50638669333333, 65.70352964666667, 1.0, 2.0, 0.536900336605653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750253.0123421231, 750253.0123421238, 189581.1659595895]
[2019-03-26 20:12:28,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:12:28,328] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1374137e-25 1.0000000e+00 4.2056874e-26 1.4814723e-14 3.8409010e-15], sampled 0.7753805861208939
[2019-03-26 20:12:28,582] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:12:28,583] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.74738024333333, 66.46928465, 1.0, 2.0, 0.6094804160161824, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.942752479903065, 6.9112, 168.9126534573414, 1704114.589075477, 1681730.214040415, 366355.9074954162]
[2019-03-26 20:12:28,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:12:28,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7559254e-20 1.0000000e+00 8.0354199e-21 1.4241659e-11 5.4550427e-12], sampled 0.6647919224120126
[2019-03-26 20:12:28,587] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1704114.589075477 W.
[2019-03-26 20:12:42,827] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.45226803]
[2019-03-26 20:12:42,829] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.24761802333333, 82.77409081, 1.0, 2.0, 0.5600577080434939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 782624.5476071058, 782624.5476071051, 193539.7813292411]
[2019-03-26 20:12:42,829] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:12:42,832] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3569716e-27 1.0000000e+00 3.9891582e-28 1.0668564e-15 2.3818770e-16], sampled 0.14948227430548466
[2019-03-26 20:13:10,699] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164650260.9350 1778.0000
[2019-03-26 20:13:10,976] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7990.1625 3008417738.7924 1766.0000
[2019-03-26 20:13:11,198] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8653.2987 2779927580.8313 933.0000
[2019-03-26 20:13:11,338] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8488.6935 2843245586.4942 1131.0000
[2019-03-26 20:13:11,376] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8246.2734 2928117667.3375 1338.0000
[2019-03-26 20:13:12,393] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 550000, evaluation results [550000.0, 7876.615395271476, 3164650260.93499, 1778.0, 8246.273358047021, 2928117667.3375263, 1338.0, 8653.298705978405, 2779927580.8313117, 933.0, 7990.162463052581, 3008417738.7924104, 1766.0, 8488.693495291653, 2843245586.494196, 1131.0]
[2019-03-26 20:13:13,465] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1651096e-15 1.0000000e+00 2.1265275e-14 8.4042826e-09 7.4135857e-09], sum to 1.0000
[2019-03-26 20:13:13,473] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8714
[2019-03-26 20:13:13,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2045547.028491113 W.
[2019-03-26 20:13:13,485] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7314836982065661, 1.0, 2.0, 0.7314836982065661, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2045547.028491113, 2045547.028491113, 387892.9526034787], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.00881517549566, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.991797532671896, 6.9112, 168.9124775410152, 2307340.530649931, 2250162.029155259, 466561.9904245081], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 1.010620693368265, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008059753267189596, 0.0, 0.8294375931931979, 0.6409279251805364, 0.625045008098683, 0.6963611797380718], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34296978], dtype=float32), 0.41203356]. 
=============================================
[2019-03-26 20:13:17,031] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8079694e-16 1.0000000e+00 2.6484023e-16 4.7960746e-09 8.9504599e-09], sum to 1.0000
[2019-03-26 20:13:17,040] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8053
[2019-03-26 20:13:17,052] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1928059.136616374 W.
[2019-03-26 20:13:17,057] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666666, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.579323724291761, 6.9112, 168.9092440956237, 1928059.136616374, 1454079.589039982, 311351.7861432175], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4797600.0000, 
sim time next is 4798200.0000, 
raw observation next is [31.83333333333333, 63.5, 1.0, 2.0, 0.4055320973657978, 1.0, 1.0, 0.4055320973657978, 1.0, 1.0, 0.7034512184697054, 6.9112, 6.9112, 170.5573041426782, 1700793.157406323, 1700793.157406323, 355855.4515686854], 
processed observation next is [1.0, 0.5217391304347826, 0.7077409162717218, 0.635, 1.0, 1.0, 0.28377361128409373, 1.0, 0.5, 0.28377361128409373, 1.0, 0.5, 0.6383551444752503, 0.0, 0.0, 0.8375144448122397, 0.4724425437239786, 0.4724425437239786, 0.5311275396547543], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1344526], dtype=float32), -0.7815623]. 
=============================================
[2019-03-26 20:13:26,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2202670e-16 9.9999952e-01 2.2574067e-14 4.6342780e-07 2.2054548e-10], sum to 1.0000
[2019-03-26 20:13:26,036] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6208
[2019-03-26 20:13:26,043] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2034200.159170058 W.
[2019-03-26 20:13:26,052] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 68.66666666666667, 1.0, 2.0, 0.8136663400486942, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98410579061679, 6.9112, 168.9125229509847, 2034200.159170058, 1982478.414840621, 412035.1050560275], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4959600.0000, 
sim time next is 4960200.0000, 
raw observation next is [30.0, 69.33333333333333, 1.0, 2.0, 0.4924774870750362, 1.0, 1.0, 0.4924774870750362, 1.0, 2.0, 0.8470362740616076, 6.911199999999999, 6.9112, 170.5573041426782, 2065791.840347832, 2065791.840347832, 408516.23562079], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.6933333333333332, 1.0, 1.0, 0.3885270928614894, 1.0, 0.5, 0.3885270928614894, 1.0, 1.0, 0.8134588708068384, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5738310667632867, 0.5738310667632867, 0.6097257248071493], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.1999357], dtype=float32), -1.7314236]. 
=============================================
[2019-03-26 20:13:37,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4026840e-28 1.0000000e+00 5.7635397e-24 1.1329616e-13 1.4603173e-09], sum to 1.0000
[2019-03-26 20:13:37,536] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1870
[2019-03-26 20:13:37,542] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 84.0, 1.0, 2.0, 0.4923487007868947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 687977.3519527548, 687977.3519527555, 182405.8863755389], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5121000.0000, 
sim time next is 5121600.0000, 
raw observation next is [26.66666666666667, 84.0, 1.0, 2.0, 0.4974532312340174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695112.4379774907, 695112.4379774913, 183198.0284346595], 
processed observation next is [0.0, 0.2608695652173913, 0.4628751974723541, 0.84, 1.0, 1.0, 0.39452196534218964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19308678832708076, 0.19308678832708093, 0.273429893186059], 
reward next is 0.7266, 
noisyNet noise sample is [array([-0.39256835], dtype=float32), 0.04356929]. 
=============================================
[2019-03-26 20:13:39,035] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.1933496e-27 1.0000000e+00 6.8123112e-23 6.4354498e-12 1.2246848e-13], sum to 1.0000
[2019-03-26 20:13:39,043] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0751
[2019-03-26 20:13:39,052] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5516185973084917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770827.45667694, 770827.4566769394, 192079.1430813605], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5155200.0000, 
sim time next is 5155800.0000, 
raw observation next is [31.83333333333334, 63.5, 1.0, 2.0, 0.5556764701865975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 776499.9687244205, 776499.9687244212, 192778.9960582308], 
processed observation next is [0.0, 0.6956521739130435, 0.7077409162717223, 0.635, 1.0, 1.0, 0.46467044600794877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2156944357567835, 0.21569443575678368, 0.28772984486303105], 
reward next is 0.7123, 
noisyNet noise sample is [array([-0.71796024], dtype=float32), -0.41464224]. 
=============================================
[2019-03-26 20:13:39,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8339859e-28 1.0000000e+00 4.6356664e-21 6.2661417e-12 1.5697827e-19], sum to 1.0000
[2019-03-26 20:13:39,806] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7279
[2019-03-26 20:13:39,813] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 79.0, 1.0, 2.0, 0.5152970779166053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720054.8503580085, 720054.8503580092, 186026.9613251802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5178000.0000, 
sim time next is 5178600.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5110061883386375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714056.9232520408, 714056.9232520408, 185337.8384153096], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4108508293236596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1983491453477891, 0.1983491453477891, 0.27662363942583523], 
reward next is 0.7234, 
noisyNet noise sample is [array([1.627588], dtype=float32), 0.7909084]. 
=============================================
[2019-03-26 20:13:39,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3619604e-21 1.0000000e+00 2.5404991e-17 2.1230757e-13 8.9862985e-16], sum to 1.0000
[2019-03-26 20:13:39,920] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6282
[2019-03-26 20:13:39,927] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5161590341472515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721259.7213000394, 721259.7213000394, 186166.1952553471], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5190600.0000, 
sim time next is 5191200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.51552657840625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720375.6536032964, 720375.6536032964, 186064.1725502636], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41629708241716873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20010434822313788, 0.20010434822313788, 0.27770772022427404], 
reward next is 0.7223, 
noisyNet noise sample is [array([0.63206136], dtype=float32), -0.09146166]. 
=============================================
[2019-03-26 20:13:43,453] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8430885e-13 1.0000000e+00 5.8222563e-12 1.3965982e-08 2.0168662e-09], sum to 1.0000
[2019-03-26 20:13:43,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3774
[2019-03-26 20:13:43,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2516034.334230164 W.
[2019-03-26 20:13:43,473] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.8995498551386762, 1.0, 2.0, 0.8995498551386762, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2516034.334230164, 2516034.334230165, 471246.5991164661], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5224200.0000, 
sim time next is 5224800.0000, 
raw observation next is [31.33333333333334, 69.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.30672964004614, 6.9112, 168.9050813118145, 3274582.025682795, 2284591.426917626, 472653.932174427], 
processed observation next is [1.0, 0.4782608695652174, 0.6840442338072673, 0.69, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.13955296400461395, 0.0, 0.8294012743218142, 0.9096061182452208, 0.6346087296993406, 0.7054536301110851], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41006145], dtype=float32), -1.5050817]. 
=============================================
[2019-03-26 20:13:48,967] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4388752e-13 9.9998629e-01 3.4106915e-11 1.3685701e-05 6.2500827e-09], sum to 1.0000
[2019-03-26 20:13:48,976] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8013
[2019-03-26 20:13:48,986] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2276038.160631194 W.
[2019-03-26 20:13:48,990] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.2, 52.0, 1.0, 2.0, 0.9864511959997594, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00599259176818, 6.9112, 168.9123930989401, 2276038.160631194, 2208789.257832014, 459168.7210722269], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5317200.0000, 
sim time next is 5317800.0000, 
raw observation next is [36.18333333333334, 52.16666666666667, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 0.0, 1.0, 0.0, 7.004483001901961, 6.9112, 170.5573041426782, 2976230.000093373, 2909407.591321087, 553161.5974764419], 
processed observation next is [1.0, 0.5652173913043478, 0.9139020537124805, 0.5216666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 0.5, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.009328300190196082, 0.0, 0.8375144448122397, 0.8267305555814926, 0.8081687753669686, 0.8256143245917043], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16934381], dtype=float32), -0.57269955]. 
=============================================
[2019-03-26 20:13:53,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.9620017e-04 8.4260088e-01 9.3578297e-04 1.3121432e-01 2.4652813e-02], sum to 1.0000
[2019-03-26 20:13:53,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6867
[2019-03-26 20:13:53,691] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3375370.704687957 W.
[2019-03-26 20:13:53,695] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 54.0, 1.0, 2.0, 0.967314756794768, 1.0, 2.0, 0.8042474179116464, 1.0, 1.0, 1.03, 7.005118819149033, 6.9112, 170.5573041426782, 3375370.704687957, 3308092.8341582, 619071.9944963923], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5410800.0000, 
sim time next is 5411400.0000, 
raw observation next is [37.5, 53.83333333333333, 1.0, 2.0, 0.8576740629912881, 1.0, 2.0, 0.7494270710099066, 1.0, 2.0, 1.03, 7.005110167904228, 6.9112, 170.5573041426782, 3145003.780224822, 3077732.10693363, 575758.5735702727], 
processed observation next is [1.0, 0.6521739130434783, 0.976303317535545, 0.5383333333333333, 1.0, 1.0, 0.8285229674593833, 1.0, 1.0, 0.6981049048312127, 1.0, 1.0, 1.0365853658536586, 0.009391016790422757, 0.0, 0.8375144448122397, 0.8736121611735617, 0.8549255852593417, 0.8593411545824965], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1834805], dtype=float32), -0.33451942]. 
=============================================
[2019-03-26 20:14:02,030] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6012304e-17 9.9999952e-01 5.7365620e-15 4.3385805e-07 1.6068178e-12], sum to 1.0000
[2019-03-26 20:14:02,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8823
[2019-03-26 20:14:02,043] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 89.33333333333333, 1.0, 2.0, 0.8164064319881933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1141038.651709217, 1141038.651709217, 247977.6576206631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5551800.0000, 
sim time next is 5552400.0000, 
raw observation next is [27.03333333333333, 88.66666666666667, 1.0, 2.0, 0.7850647749198626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1097211.806089695, 1097211.806089695, 240277.8492756816], 
processed observation next is [1.0, 0.2608695652173913, 0.48025276461295413, 0.8866666666666667, 1.0, 1.0, 0.7410418974938103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3047810572471375, 0.3047810572471375, 0.35862365563534565], 
reward next is 0.6414, 
noisyNet noise sample is [array([-0.30637726], dtype=float32), -0.5481684]. 
=============================================
[2019-03-26 20:14:08,256] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 20:14:08,259] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:14:08,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:14:08,260] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:14:08,261] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:14:08,261] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:14:08,263] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:14:08,263] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:14:08,266] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:14:08,267] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:14:08,268] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:14:08,290] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-03-26 20:14:08,310] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-03-26 20:14:08,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-03-26 20:14:08,352] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-03-26 20:14:08,369] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-03-26 20:14:12,657] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:14:12,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.607778755, 84.10258349, 1.0, 2.0, 0.3102917887195782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 496290.4715574562, 496290.4715574569, 166826.7453203378]
[2019-03-26 20:14:12,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:14:12,662] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8923983e-19 1.8574772e-33], sampled 0.833853913469066
[2019-03-26 20:14:21,982] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:14:21,983] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.05065218, 70.69023478, 1.0, 2.0, 0.3151210187466064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499016.502123319, 499016.5021233196, 166965.1375124937]
[2019-03-26 20:14:21,985] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:14:21,988] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0959810e-19 3.3989116e-34], sampled 0.20995492167791885
[2019-03-26 20:14:25,402] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:14:25,403] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.22740802, 60.92323455, 1.0, 2.0, 0.5601388520756075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853938.5264638053, 853938.5264638053, 202222.2129482466]
[2019-03-26 20:14:25,405] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:14:25,406] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 2.5074492e-37 7.2962531e-17 2.9675442e-29], sampled 0.4310702893460727
[2019-03-26 20:14:33,696] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:14:33,697] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.26666666666667, 82.83333333333334, 1.0, 2.0, 0.4450397809819127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644504.3094031591, 644504.3094031591, 178329.0756518003]
[2019-03-26 20:14:33,698] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:14:33,700] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.8919713e-37 6.4245676e-17 2.3751743e-29], sampled 0.26482318816723704
[2019-03-26 20:15:11,829] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:15:11,830] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.00251740833333, 84.7064955, 1.0, 2.0, 0.630036058174074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880452.8127544809, 880452.8127544809, 206474.9348511965]
[2019-03-26 20:15:11,832] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:15:11,835] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5965245e-18 3.6928387e-32], sampled 0.21147038251257022
[2019-03-26 20:15:22,031] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:15:22,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.57155237, 81.96318642333333, 1.0, 2.0, 0.4436480460355017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643079.1545725921, 643079.1545725928, 178200.4607981545]
[2019-03-26 20:15:22,036] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:15:22,038] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2229989e-18 6.5911373e-32], sampled 0.8992591693914794
[2019-03-26 20:15:38,420] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:15:38,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.7, 83.5, 1.0, 2.0, 0.5766090974436819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805762.2291614377, 805762.2291614383, 196468.4865428588]
[2019-03-26 20:15:38,422] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:15:38,425] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5959408e-17 2.0757842e-30], sampled 0.9420377548557203
[2019-03-26 20:15:43,471] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.33207914]
[2019-03-26 20:15:43,473] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.94501338166667, 91.06093814166667, 1.0, 2.0, 0.5262968708140424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735430.8274252691, 735430.8274252691, 187817.6051763331]
[2019-03-26 20:15:43,474] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:15:43,478] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 6.2803215e-38 3.9038683e-17 9.9325225e-30], sampled 0.6650542195235141
[2019-03-26 20:16:02,116] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8487.8419 2843388159.1415 1131.0000
[2019-03-26 20:16:02,814] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8650.9417 2780192181.4574 933.0000
[2019-03-26 20:16:03,060] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7991.1793 3008594365.1484 1766.0000
[2019-03-26 20:16:03,125] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.2838 2928293311.1787 1338.0000
[2019-03-26 20:16:03,141] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164907195.1427 1778.0000
[2019-03-26 20:16:04,160] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 575000, evaluation results [575000.0, 7874.25493406606, 3164907195.142727, 1778.0, 8247.283841780014, 2928293311.178663, 1338.0, 8650.941674342437, 2780192181.4574094, 933.0, 7991.179294820579, 3008594365.1484313, 1766.0, 8487.8418525275, 2843388159.141459, 1131.0]
[2019-03-26 20:16:21,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.7819595e-27 9.9980110e-01 3.0810603e-23 1.9886451e-04 6.2462629e-18], sum to 1.0000
[2019-03-26 20:16:21,574] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4224
[2019-03-26 20:16:21,579] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.93333333333333, 93.83333333333334, 1.0, 2.0, 0.6643650645838307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928447.3380341934, 928447.3380341934, 213340.6206680305], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5979000.0000, 
sim time next is 5979600.0000, 
raw observation next is [25.9, 94.0, 1.0, 2.0, 0.6603204725723009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 922792.5823845468, 922792.5823845468, 212512.677932431], 
processed observation next is [1.0, 0.21739130434782608, 0.42654028436018954, 0.94, 1.0, 1.0, 0.5907475573160251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25633127288459634, 0.25633127288459634, 0.31718310139168804], 
reward next is 0.6828, 
noisyNet noise sample is [array([0.08209663], dtype=float32), -0.34646007]. 
=============================================
[2019-03-26 20:16:22,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2675013e-38 9.9993408e-01 2.3043137e-33 6.5918495e-05 2.8819643e-27], sum to 1.0000
[2019-03-26 20:16:22,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3308
[2019-03-26 20:16:22,097] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.68333333333334, 80.33333333333334, 1.0, 2.0, 0.5710748388473635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798025.667698816, 798025.667698816, 195481.9636586174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5940600.0000, 
sim time next is 5941200.0000, 
raw observation next is [29.56666666666667, 80.66666666666667, 1.0, 2.0, 0.5706608332825299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797446.9150249014, 797446.9150249014, 195408.2689506001], 
processed observation next is [1.0, 0.782608695652174, 0.6003159557661929, 0.8066666666666668, 1.0, 1.0, 0.4827238955211204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2215130319513615, 0.2215130319513615, 0.2916541327620897], 
reward next is 0.7083, 
noisyNet noise sample is [array([-0.9000964], dtype=float32), -1.972229]. 
=============================================
[2019-03-26 20:16:33,822] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.0631206e-26 9.9990392e-01 7.3384747e-26 9.6038741e-05 2.9184923e-15], sum to 1.0000
[2019-03-26 20:16:33,836] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1098
[2019-03-26 20:16:33,840] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 90.66666666666667, 1.0, 2.0, 0.5369747140129834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750356.9824451448, 750356.9824451454, 189592.1813109072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6139200.0000, 
sim time next is 6139800.0000, 
raw observation next is [26.8, 91.0, 1.0, 2.0, 0.538071823698464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751890.6026538422, 751890.6026538422, 189776.2377056621], 
processed observation next is [1.0, 0.043478260869565216, 0.4691943127962086, 0.91, 1.0, 1.0, 0.4434600285523662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20885850073717838, 0.20885850073717838, 0.2832481159786001], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.8212292], dtype=float32), 0.9915486]. 
=============================================
[2019-03-26 20:16:33,909] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0127690e-21 2.7033007e-01 3.8554409e-18 7.2966993e-01 4.3028901e-12], sum to 1.0000
[2019-03-26 20:16:33,919] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5225
[2019-03-26 20:16:33,923] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.66666666666667, 92.0, 1.0, 2.0, 0.4292659083253552, 1.0, 1.0, 0.4292659083253552, 1.0, 1.0, 0.7405334519567189, 6.9112, 6.9112, 170.5573041426782, 1800415.887093655, 1800415.887093655, 368927.6024771989], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6142800.0000, 
sim time next is 6143400.0000, 
raw observation next is [26.65, 92.0, 1.0, 2.0, 0.5018834552426503, 1.0, 2.0, 0.5018834552426503, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1403064.408106404, 1403064.408106404, 300839.4337263106], 
processed observation next is [1.0, 0.08695652173913043, 0.462085308056872, 0.92, 1.0, 1.0, 0.39985958462969917, 1.0, 1.0, 0.39985958462969917, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.38974011336289, 0.38974011336289, 0.4490140801885233], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7069963], dtype=float32), -0.010567255]. 
=============================================
[2019-03-26 20:16:44,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9792841e-38 1.0000000e+00 5.2694808e-34 1.5952291e-09 1.0390866e-29], sum to 1.0000
[2019-03-26 20:16:44,167] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2448
[2019-03-26 20:16:44,172] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 86.66666666666667, 1.0, 2.0, 0.531298477050887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742422.358375934, 742422.3583759333, 188645.1469677149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6316800.0000, 
sim time next is 6317400.0000, 
raw observation next is [27.13333333333333, 86.83333333333333, 1.0, 2.0, 0.5313407538515516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742481.4554994003, 742481.4554994003, 188652.1362509135], 
processed observation next is [0.0, 0.08695652173913043, 0.484992101105845, 0.8683333333333333, 1.0, 1.0, 0.4353503058452428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2062448487498334, 0.2062448487498334, 0.2815703526133037], 
reward next is 0.7184, 
noisyNet noise sample is [array([0.76756555], dtype=float32), 0.008142761]. 
=============================================
[2019-03-26 20:16:53,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5256348e-30 9.9970406e-01 5.3032915e-25 2.9590999e-04 1.0543668e-18], sum to 1.0000
[2019-03-26 20:16:53,224] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3819
[2019-03-26 20:16:53,234] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 76.66666666666667, 1.0, 2.0, 0.5053332822338025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706127.2262472076, 706127.2262472083, 184435.7964293429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6466200.0000, 
sim time next is 6466800.0000, 
raw observation next is [27.9, 77.33333333333334, 1.0, 2.0, 0.5068638940061424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708266.7389208385, 708266.7389208392, 184678.4127319815], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.7733333333333334, 1.0, 1.0, 0.4058601132604125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.196740760811344, 0.1967407608113442, 0.2756394219880321], 
reward next is 0.7244, 
noisyNet noise sample is [array([-0.3880978], dtype=float32), -1.295816]. 
=============================================
[2019-03-26 20:16:56,873] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.39852040e-32 9.99897957e-01 6.66044322e-26 1.02070575e-04
 2.65165460e-20], sum to 1.0000
[2019-03-26 20:16:56,882] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8721
[2019-03-26 20:16:56,889] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 86.0, 1.0, 2.0, 0.5197586279417052, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726291.3673899743, 726291.3673899743, 186749.9526150364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6564600.0000, 
sim time next is 6565200.0000, 
raw observation next is [27.0, 86.33333333333334, 1.0, 2.0, 0.520269961060075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727006.1297747858, 727006.1297747851, 186833.0678158203], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.8633333333333334, 1.0, 1.0, 0.4220120012771987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20194614715966272, 0.20194614715966253, 0.2788553250982393], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.22260253], dtype=float32), 0.7363667]. 
=============================================
[2019-03-26 20:16:57,248] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3608288e-28 8.7757474e-01 2.9254771e-25 1.2242523e-01 2.6760216e-17], sum to 1.0000
[2019-03-26 20:16:57,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7767
[2019-03-26 20:16:57,258] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 65.66666666666667, 1.0, 2.0, 0.2314586418802454, 1.0, 2.0, 0.2314586418802454, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 646837.3069485797, 646837.3069485797, 238007.8546430743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6542400.0000, 
sim time next is 6543000.0000, 
raw observation next is [29.6, 66.0, 1.0, 2.0, 0.4578194734837097, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639713.8366863262, 639713.8366863262, 177252.9447489907], 
processed observation next is [1.0, 0.7391304347826086, 0.6018957345971565, 0.66, 1.0, 1.0, 0.3467704499803732, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17769828796842393, 0.17769828796842393, 0.2645566339537175], 
reward next is 0.7354, 
noisyNet noise sample is [array([0.1329033], dtype=float32), -1.0703454]. 
=============================================
[2019-03-26 20:16:57,270] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[59.77214 ]
 [56.14505 ]
 [49.442715]
 [48.732544]
 [49.307434]], R is [[63.09022522]
 [63.10408783]
 [63.09414673]
 [62.46320724]
 [61.954216  ]].
[2019-03-26 20:17:00,007] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 20:17:00,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:17:00,010] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:17:00,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:17:00,012] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:17:00,013] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:17:00,012] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:17:00,014] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:17:00,017] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:17:00,021] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:17:00,018] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:17:00,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-03-26 20:17:00,038] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-03-26 20:17:00,057] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-03-26 20:17:00,098] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-03-26 20:17:00,099] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-03-26 20:17:20,760] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:17:20,760] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.4, 82.0, 1.0, 2.0, 0.2849641668154601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460932.100251351, 460932.1002513504, 164328.9578880786]
[2019-03-26 20:17:20,760] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:17:20,762] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 9.9999082e-01 1.9414019e-34 9.2046657e-06 5.3021448e-24], sampled 0.39107852326206616
[2019-03-26 20:17:22,963] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:17:22,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.08333333333334, 96.0, 1.0, 2.0, 0.4126703675221082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 608315.5858651221, 608315.5858651227, 175098.835103869]
[2019-03-26 20:17:22,967] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:17:22,971] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5973601e-36 9.9997163e-01 2.8767600e-32 2.8381850e-05 2.0952885e-22], sampled 0.4156048375771676
[2019-03-26 20:17:26,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:17:26,632] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.04564258666667, 95.39381155000001, 1.0, 2.0, 0.457460235669568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 649118.7650768878, 649118.7650768884, 178476.2828719348]
[2019-03-26 20:17:26,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:17:26,636] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4733417e-37 9.9998248e-01 3.3943579e-33 1.7536826e-05 4.3502912e-23], sampled 0.9491392070497859
[2019-03-26 20:17:28,946] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:17:28,947] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.11074051333333, 89.97066229, 1.0, 2.0, 0.5007066883409659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699660.1283532759, 699660.1283532759, 183705.0835985761]
[2019-03-26 20:17:28,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:17:28,952] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.3358497e-37 9.9997509e-01 1.6056041e-32 2.4887760e-05 1.3644132e-22], sampled 0.4933018789128668
[2019-03-26 20:18:06,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:18:06,309] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.53333333333334, 85.0, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 1.0, 1.03, 12.24786211123054, 6.9112, 178.6582176852504, 7749255.486784304, 3744813.613824722, 652777.1782125215]
[2019-03-26 20:18:06,311] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:18:06,314] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9699329e-14 1.2443793e-02 7.4874516e-12 9.8755580e-01 4.7258115e-07], sampled 0.04924587125615698
[2019-03-26 20:18:06,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 7749255.486784304 W.
[2019-03-26 20:18:17,959] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.32427415]
[2019-03-26 20:18:17,960] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.3, 74.0, 1.0, 2.0, 0.8245608289578767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1152441.71156714, 1152441.71156714, 250031.3870685071]
[2019-03-26 20:18:17,963] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:18:17,966] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5709541e-32 9.9978548e-01 2.2849401e-28 2.1457080e-04 1.5487253e-19], sampled 0.5439552772987816
[2019-03-26 20:18:52,830] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.6940 2928895463.3087 1338.0000
[2019-03-26 20:18:52,905] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7867.9829 3165519866.7258 1776.0000
[2019-03-26 20:18:53,147] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8486.9829 2843703950.0615 1126.0000
[2019-03-26 20:18:53,256] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8652.0363 2780467312.9457 930.0000
[2019-03-26 20:18:53,432] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.5509 3008944026.9589 1765.0000
[2019-03-26 20:18:54,447] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 600000, evaluation results [600000.0, 7867.982862109405, 3165519866.725827, 1776.0, 8252.694036693098, 2928895463.3087363, 1338.0, 8652.036338704107, 2780467312.9456897, 930.0, 7998.5508866252285, 3008944026.9589005, 1765.0, 8486.982896370237, 2843703950.061492, 1126.0]
[2019-03-26 20:18:54,650] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5750616e-22 9.9845159e-01 1.8015706e-18 1.5484494e-03 3.0438251e-14], sum to 1.0000
[2019-03-26 20:18:54,660] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2095
[2019-03-26 20:18:54,666] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2076190.349115588 W.
[2019-03-26 20:18:54,675] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.26666666666667, 73.16666666666667, 1.0, 2.0, 0.8436680878369414, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.985733105390921, 6.9112, 168.9125129092245, 2076190.349115588, 2023314.137784252, 419652.6172134328], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6603000.0000, 
sim time next is 6603600.0000, 
raw observation next is [29.43333333333334, 72.33333333333334, 1.0, 2.0, 0.9644793841238664, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.983490521092355, 6.9112, 168.9124660911467, 2245285.468026148, 2194000.233216709, 453381.9821370981], 
processed observation next is [1.0, 0.43478260869565216, 0.5939968404423385, 0.7233333333333334, 1.0, 1.0, 0.9572040772576703, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007229052109235479, 0.0, 0.8294375369691019, 0.6236904077850411, 0.6094445092268637, 0.6766895255777583], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2863785], dtype=float32), -1.0137737]. 
=============================================
[2019-03-26 20:19:00,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2882592e-26 1.0000000e+00 9.9171491e-23 1.7507045e-08 1.9889727e-15], sum to 1.0000
[2019-03-26 20:19:00,353] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5880
[2019-03-26 20:19:00,362] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1925974.094409144 W.
[2019-03-26 20:19:00,371] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.7363339680683881, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.978636387597992, 6.9112, 168.9124991895495, 1925974.094409144, 1878132.529594078, 393690.4319716908], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6688800.0000, 
sim time next is 6689400.0000, 
raw observation next is [28.36666666666667, 77.0, 1.0, 2.0, 0.6126822929603407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.922463770696725, 6.9112, 168.912801422262, 1713074.299918658, 1705083.400654219, 367905.9071460977], 
processed observation next is [1.0, 0.43478260869565216, 0.543443917851501, 0.77, 1.0, 1.0, 0.5333521601931815, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0011263770696724683, 0.0, 0.8294391835983932, 0.4758539721996273, 0.4736342779595053, 0.549113294247907], 
reward next is 0.3946, 
noisyNet noise sample is [array([2.1038225], dtype=float32), -0.21720552]. 
=============================================
[2019-03-26 20:19:07,182] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2289918e-27 9.9680907e-01 2.0170995e-23 3.1909645e-03 6.4642163e-12], sum to 1.0000
[2019-03-26 20:19:07,190] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4042
[2019-03-26 20:19:07,196] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.11666666666667, 49.83333333333334, 1.0, 2.0, 0.9959389549247336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1536659.084714192, 1536659.084714191, 318770.0041731353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6799800.0000, 
sim time next is 6800400.0000, 
raw observation next is [29.1, 50.0, 1.0, 2.0, 0.9962828163074118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1536412.822916233, 1536412.822916233, 318786.9920947404], 
processed observation next is [1.0, 0.7391304347826086, 0.5781990521327015, 0.5, 1.0, 1.0, 0.9955214654306166, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4267813396989536, 0.4267813396989536, 0.4758014807384185], 
reward next is 0.5242, 
noisyNet noise sample is [array([0.3775956], dtype=float32), 0.14629692]. 
=============================================
[2019-03-26 20:19:07,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 9.9998844e-01 0.0000000e+00 1.1540960e-05 2.0620861e-27], sum to 1.0000
[2019-03-26 20:19:07,366] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4256
[2019-03-26 20:19:07,373] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 64.5, 1.0, 2.0, 0.3244562366740797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510224.5839530903, 510224.5839530903, 167740.2925411642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6816600.0000, 
sim time next is 6817200.0000, 
raw observation next is [25.33333333333334, 65.33333333333333, 1.0, 2.0, 0.3265185474731737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513167.4999976011, 513167.4999976018, 167959.746766201], 
processed observation next is [1.0, 0.9130434782608695, 0.3996840442338076, 0.6533333333333333, 1.0, 1.0, 0.18857656322069122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1425465277771114, 0.1425465277771116, 0.25068618920328506], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.0622963], dtype=float32), -1.4864564]. 
=============================================
[2019-03-26 20:19:23,421] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 8.8171644e-38 1.0873920e-15 1.0493188e-23], sum to 1.0000
[2019-03-26 20:19:23,430] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6738
[2019-03-26 20:19:23,436] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 81.0, 1.0, 2.0, 0.4831327692617746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675095.4923229417, 675095.4923229422, 180995.5421858316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7067400.0000, 
sim time next is 7068000.0000, 
raw observation next is [26.63333333333333, 82.0, 1.0, 2.0, 0.4842626667678272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676674.833879201, 676674.8338792004, 181167.0911568115], 
processed observation next is [1.0, 0.8260869565217391, 0.46129541864139006, 0.82, 1.0, 1.0, 0.3786297189973822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18796523163311138, 0.1879652316331112, 0.2703986435176291], 
reward next is 0.7296, 
noisyNet noise sample is [array([-1.6058861], dtype=float32), 1.6282376]. 
=============================================
[2019-03-26 20:19:23,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[71.53942 ]
 [71.802284]
 [71.81223 ]
 [71.50057 ]
 [71.69307 ]], R is [[71.2394104 ]
 [71.25687408]
 [71.27438354]
 [71.29180908]
 [71.30921936]].
[2019-03-26 20:19:24,067] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1281594e-17 7.2391118e-25], sum to 1.0000
[2019-03-26 20:19:24,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0882
[2019-03-26 20:19:24,079] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 87.66666666666667, 1.0, 2.0, 0.4784223821799258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668809.4186767162, 668809.4186767162, 180322.4065576535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7080000.0000, 
sim time next is 7080600.0000, 
raw observation next is [25.43333333333333, 87.83333333333334, 1.0, 2.0, 0.4763658806335168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666240.161627436, 666240.1616274355, 180053.5501212139], 
processed observation next is [1.0, 0.9565217391304348, 0.40442338072669815, 0.8783333333333334, 1.0, 1.0, 0.3691155188355624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1850667115631767, 0.18506671156317653, 0.268736641971961], 
reward next is 0.7313, 
noisyNet noise sample is [array([1.701706], dtype=float32), 0.4565915]. 
=============================================
[2019-03-26 20:19:30,730] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4284810e-25 9.9999976e-01 1.3234261e-22 2.4929088e-07 5.5529301e-13], sum to 1.0000
[2019-03-26 20:19:30,740] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3684
[2019-03-26 20:19:30,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1979820.160724317 W.
[2019-03-26 20:19:30,758] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.7748103793313128, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979955035142, 6.9112, 168.912316035653, 1979820.160724317, 1912580.253505605, 401701.4887373851], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7203600.0000, 
sim time next is 7204200.0000, 
raw observation next is [29.0, 83.16666666666667, 1.0, 2.0, 0.7327956772591324, 1.0, 1.0, 0.7327956772591324, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2049219.402445563, 2049219.402445564, 388485.8862273065], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.8316666666666667, 1.0, 1.0, 0.6780670810350993, 1.0, 0.5, 0.6780670810350993, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5692276117904342, 0.5692276117904345, 0.5798296809362784], 
reward next is 0.4202, 
noisyNet noise sample is [array([0.6193076], dtype=float32), -0.13610868]. 
=============================================
[2019-03-26 20:19:35,482] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.0737516e-36 1.0896132e-08 8.2809820e-20], sum to 1.0000
[2019-03-26 20:19:35,491] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7086
[2019-03-26 20:19:35,500] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 85.66666666666667, 1.0, 2.0, 0.3202498099343761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504565.0852633117, 504565.0852633124, 167330.6141950369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7285200.0000, 
sim time next is 7285800.0000, 
raw observation next is [22.25, 85.33333333333334, 1.0, 2.0, 0.3194155092829265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503177.2750253567, 503177.2750253567, 167224.0212844154], 
processed observation next is [1.0, 0.30434782608695654, 0.2535545023696683, 0.8533333333333334, 1.0, 1.0, 0.18001868588304398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1397714652848213, 0.1397714652848213, 0.24958809146927668], 
reward next is 0.7504, 
noisyNet noise sample is [array([-0.33105156], dtype=float32), 1.5106071]. 
=============================================
[2019-03-26 20:19:36,831] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.3224095e-27 1.0000000e+00 2.3352534e-25 2.4563901e-08 3.6926208e-13], sum to 1.0000
[2019-03-26 20:19:36,842] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8982
[2019-03-26 20:19:36,848] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 58.00000000000001, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.964758930878284, 6.9112, 168.9124963630703, 1622594.904888347, 1584598.459476157, 331221.0055080327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7309200.0000, 
sim time next is 7309800.0000, 
raw observation next is [27.9, 58.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.16910805149819, 6.9112, 168.91136831862, 1764837.706487911, 1581870.578596328, 330829.8164752468], 
processed observation next is [1.0, 0.6086956521739131, 0.5213270142180094, 0.585, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.02579080514981902, 0.0, 0.8294321464034333, 0.49023269624664195, 0.4394084940545355, 0.493775845485443], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7114111], dtype=float32), -1.857911]. 
=============================================
[2019-03-26 20:19:37,400] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0126014e-27 9.9954361e-01 5.3046072e-24 4.5633459e-04 7.2345753e-15], sum to 1.0000
[2019-03-26 20:19:37,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1456
[2019-03-26 20:19:37,417] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 60.0, 1.0, 2.0, 0.5122263150738964, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8961078934279184, 6.9112, 6.9112, 168.9127596988931, 1540987.927778391, 1540987.927778391, 323877.1739966316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7311600.0000, 
sim time next is 7312200.0000, 
raw observation next is [27.76666666666667, 60.33333333333333, 1.0, 2.0, 0.8070539441775163, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564612865, 1216820.616689628, 1216820.616689628, 257667.7315468164], 
processed observation next is [1.0, 0.6521739130434783, 0.515007898894155, 0.6033333333333333, 1.0, 1.0, 0.7675348725030317, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399449109807, 0.33800572685823, 0.33800572685823, 0.3845787038012185], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.3348783], dtype=float32), -0.6830217]. 
=============================================
[2019-03-26 20:19:38,039] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 2.26993564e-38 1.20467565e-08
 1.17145807e-22], sum to 1.0000
[2019-03-26 20:19:38,052] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0105
[2019-03-26 20:19:38,060] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666666, 90.66666666666667, 1.0, 2.0, 0.3343134563354721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524635.574303105, 524635.5743031057, 168836.3574700976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7368000.0000, 
sim time next is 7368600.0000, 
raw observation next is [21.48333333333333, 90.83333333333334, 1.0, 2.0, 0.3277640517223998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516408.1460469738, 516408.1460469738, 168237.0159392175], 
processed observation next is [1.0, 0.2608695652173913, 0.21721958925750387, 0.9083333333333334, 1.0, 1.0, 0.1900771707498793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1434467072352705, 0.1434467072352705, 0.25110002378987684], 
reward next is 0.7489, 
noisyNet noise sample is [array([-0.9823159], dtype=float32), 0.7696797]. 
=============================================
[2019-03-26 20:19:40,113] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 9.9999785e-01 1.5332290e-33 2.0960761e-06 7.0710720e-12], sum to 1.0000
[2019-03-26 20:19:40,119] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0723
[2019-03-26 20:19:40,123] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 87.0, 1.0, 2.0, 0.4187632833010237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 628245.6697787711, 628245.6697787705, 177263.7214131462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7360800.0000, 
sim time next is 7361400.0000, 
raw observation next is [23.68333333333333, 88.0, 1.0, 2.0, 0.4133061746971389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619006.4355631499, 619006.4355631499, 176363.4572957827], 
processed observation next is [1.0, 0.17391304347826086, 0.32148499210110576, 0.88, 1.0, 1.0, 0.2931399695146251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17194623210087495, 0.17194623210087495, 0.26322904073997416], 
reward next is 0.7368, 
noisyNet noise sample is [array([0.15355624], dtype=float32), -1.7360004]. 
=============================================
[2019-03-26 20:19:49,520] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.5014803e-12 1.2737302e-25], sum to 1.0000
[2019-03-26 20:19:49,530] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6078
[2019-03-26 20:19:49,537] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.5, 1.0, 2.0, 0.4058138906486476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 596810.241652311, 596810.2416523115, 173983.2552016551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7515000.0000, 
sim time next is 7515600.0000, 
raw observation next is [23.6, 92.66666666666667, 1.0, 2.0, 0.4066744334745181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597597.7098401077, 597597.7098401077, 174041.3197998646], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9266666666666667, 1.0, 1.0, 0.285149919848817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16599936384447436, 0.16599936384447436, 0.25976316388039494], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.15913564], dtype=float32), 0.8843523]. 
=============================================
[2019-03-26 20:19:50,165] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 20:19:50,167] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:19:50,168] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:19:50,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:19:50,169] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:19:50,170] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:19:50,170] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:19:50,172] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:19:50,173] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:19:50,174] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:19:50,174] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:19:50,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-03-26 20:19:50,207] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-03-26 20:19:50,209] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-03-26 20:19:50,257] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-03-26 20:19:50,257] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-03-26 20:21:29,425] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.3107133]
[2019-03-26 20:21:29,426] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.081574, 50.60030628, 1.0, 2.0, 0.473051288425793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662693.2075661977, 662693.2075661983, 179699.5957641838]
[2019-03-26 20:21:29,427] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:21:29,432] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.8757364e-38 9.7563606e-12 1.9770128e-23], sampled 0.45343756171405514
[2019-03-26 20:21:43,841] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8653.5743 2780429570.5853 933.0000
[2019-03-26 20:21:44,277] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.1216 3008876889.3043 1766.0000
[2019-03-26 20:21:44,365] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7871.1405 3165253654.0675 1778.0000
[2019-03-26 20:21:44,390] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8489.7239 2843725838.7279 1131.0000
[2019-03-26 20:21:44,564] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.2080 2928561770.3254 1338.0000
[2019-03-26 20:21:45,579] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 625000, evaluation results [625000.0, 7871.140521287784, 3165253654.067518, 1778.0, 8253.207968018407, 2928561770.3253736, 1338.0, 8653.574286246634, 2780429570.5853157, 933.0, 7997.121552070433, 3008876889.3043056, 1766.0, 8489.723891416836, 2843725838.7279377, 1131.0]
[2019-03-26 20:21:48,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 6.0981388e-37 6.1121179e-09 2.6257375e-24], sum to 1.0000
[2019-03-26 20:21:48,444] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4087
[2019-03-26 20:21:48,450] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 74.5, 1.0, 2.0, 0.4594131902218633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645908.7461473537, 645908.7461473542, 177994.9319047655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7583400.0000, 
sim time next is 7584000.0000, 
raw observation next is [27.23333333333333, 75.66666666666667, 1.0, 2.0, 0.4632503953924129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649275.9558403647, 649275.9558403647, 178293.890136891], 
processed observation next is [0.0, 0.782608695652174, 0.4897314375987361, 0.7566666666666667, 1.0, 1.0, 0.35331372938844935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18035443217787908, 0.18035443217787908, 0.26611028378640444], 
reward next is 0.7339, 
noisyNet noise sample is [array([0.81139296], dtype=float32), 0.74133295]. 
=============================================
[2019-03-26 20:21:48,471] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.445496]
 [72.523   ]
 [72.606606]
 [72.702965]
 [72.72511 ]], R is [[72.38072205]
 [72.39125061]
 [72.40205383]
 [72.41299438]
 [72.42395782]].
[2019-03-26 20:21:50,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6916451e-31 9.9999475e-01 2.5479851e-27 5.2333644e-06 1.1812283e-17], sum to 1.0000
[2019-03-26 20:21:50,351] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7931
[2019-03-26 20:21:50,356] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 94.66666666666666, 1.0, 2.0, 0.451311847112216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644604.2572524298, 644604.2572524291, 178118.7040082693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7609200.0000, 
sim time next is 7609800.0000, 
raw observation next is [23.96666666666667, 94.83333333333333, 1.0, 2.0, 0.448102153616107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 641251.1882009836, 641251.1882009836, 177809.1674965124], 
processed observation next is [1.0, 0.043478260869565216, 0.33491311216429714, 0.9483333333333333, 1.0, 1.0, 0.33506283568205664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17812533005582876, 0.17812533005582876, 0.2653868171589738], 
reward next is 0.7346, 
noisyNet noise sample is [array([-1.3429726], dtype=float32), 0.78148174]. 
=============================================
[2019-03-26 20:21:52,597] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3049817e-27 9.9999964e-01 2.9333597e-23 4.0245035e-07 4.9874922e-15], sum to 1.0000
[2019-03-26 20:21:52,605] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1725
[2019-03-26 20:21:52,615] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1673875.131079417 W.
[2019-03-26 20:21:52,621] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.01666666666667, 62.33333333333334, 1.0, 2.0, 0.3991188513917397, 1.0, 2.0, 0.3991188513917397, 1.0, 1.0, 0.6742837772686829, 6.9112, 6.9112, 170.5573041426782, 1673875.131079417, 1673875.131079417, 349716.073997352], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7645800.0000, 
sim time next is 7646400.0000, 
raw observation next is [30.1, 62.0, 1.0, 2.0, 0.6334611335264201, 1.0, 2.0, 0.6334611335264201, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1771206.665474075, 1771206.665474075, 346929.8277045569], 
processed observation next is [1.0, 0.5217391304347826, 0.6255924170616115, 0.62, 1.0, 1.0, 0.5583869078631567, 1.0, 1.0, 0.5583869078631567, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.49200185152057635, 0.49200185152057635, 0.517805712991876], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0451033], dtype=float32), -0.5822121]. 
=============================================
[2019-03-26 20:21:54,540] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7602467e-34 9.9972004e-01 2.4083695e-31 2.7995754e-04 5.0216952e-16], sum to 1.0000
[2019-03-26 20:21:54,550] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5715
[2019-03-26 20:21:54,558] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 88.83333333333334, 1.0, 2.0, 0.5033813306917474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703398.7646157708, 703398.7646157702, 184127.4413848462], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7675800.0000, 
sim time next is 7676400.0000, 
raw observation next is [26.03333333333333, 88.66666666666667, 1.0, 2.0, 0.5023513730567796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701959.0801179268, 701959.0801179275, 183965.1251545989], 
processed observation next is [1.0, 0.8695652173913043, 0.4328593996840442, 0.8866666666666667, 1.0, 1.0, 0.4004233410322645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1949886333660908, 0.19498863336609099, 0.27457481366358044], 
reward next is 0.7254, 
noisyNet noise sample is [array([0.59474826], dtype=float32), 0.47452772]. 
=============================================
[2019-03-26 20:21:55,699] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7755653e-34 9.9347669e-01 7.3906201e-29 6.5233745e-03 9.8076634e-12], sum to 1.0000
[2019-03-26 20:21:55,709] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3222
[2019-03-26 20:21:55,713] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.00000000000001, 1.0, 2.0, 0.5280437127119593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742144.040853196, 742144.0408531955, 188653.3408578768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7704600.0000, 
sim time next is 7705200.0000, 
raw observation next is [24.5, 94.0, 1.0, 2.0, 0.5017017369180252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705140.2329091638, 705140.2329091632, 184387.8920264565], 
processed observation next is [1.0, 0.17391304347826086, 0.3601895734597157, 0.94, 1.0, 1.0, 0.399640646889187, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19587228691921216, 0.19587228691921202, 0.2752058089947112], 
reward next is 0.7248, 
noisyNet noise sample is [array([-0.89524037], dtype=float32), 0.16973807]. 
=============================================
[2019-03-26 20:22:02,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2685041e-23 9.9999988e-01 7.4226228e-21 1.2815532e-07 2.5762317e-10], sum to 1.0000
[2019-03-26 20:22:02,697] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4064
[2019-03-26 20:22:02,704] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1795803.099376648 W.
[2019-03-26 20:22:02,711] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.6433105096462534, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.003228753253159, 6.9112, 168.912330825992, 1795803.099376648, 1730514.976273643, 372945.3417437675], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7820400.0000, 
sim time next is 7821000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.581000943209599, 1.0, 1.0, 0.581000943209599, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1624412.712880888, 1624412.712880888, 327346.5780693935], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.49518185928867353, 1.0, 0.5, 0.49518185928867353, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.45122575357802447, 0.45122575357802447, 0.4885769821931247], 
reward next is 0.5114, 
noisyNet noise sample is [array([1.0508243], dtype=float32), 0.17974094]. 
=============================================
[2019-03-26 20:22:02,722] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[45.855747]
 [42.178886]
 [41.847828]
 [43.64012 ]
 [43.43431 ]], R is [[47.03798676]
 [46.56760788]
 [46.10193253]
 [45.9452095 ]
 [45.87262344]].
[2019-03-26 20:22:04,961] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3424532e-22 3.7746748e-01 1.6085053e-19 6.2253159e-01 9.4282791e-07], sum to 1.0000
[2019-03-26 20:22:04,971] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0736
[2019-03-26 20:22:04,981] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1918041.204685747 W.
[2019-03-26 20:22:04,988] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.06666666666667, 74.66666666666667, 1.0, 2.0, 0.7306652527876907, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.985416153953917, 6.9112, 168.9125147340557, 1918041.204685747, 1865389.848453114, 392191.1695331716], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7899600.0000, 
sim time next is 7900200.0000, 
raw observation next is [29.2, 74.0, 1.0, 2.0, 0.6880381936121137, 1.0, 1.0, 0.6880381936121137, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1923945.383279033, 1923945.383279033, 369043.5870087774], 
processed observation next is [1.0, 0.43478260869565216, 0.5829383886255924, 0.74, 1.0, 1.0, 0.6241424019423056, 1.0, 0.5, 0.6241424019423056, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5344292731330648, 0.5344292731330648, 0.5508113238936977], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.92153937], dtype=float32), -0.6652085]. 
=============================================
[2019-03-26 20:22:05,350] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7967398e-28 9.8306757e-01 2.0858635e-26 1.6932383e-02 7.7047974e-10], sum to 1.0000
[2019-03-26 20:22:05,356] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9921
[2019-03-26 20:22:05,365] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.3, 89.0, 1.0, 2.0, 0.5071408397273727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708653.8583693281, 708653.8583693281, 184722.7078784022], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7862400.0000, 
sim time next is 7863000.0000, 
raw observation next is [26.26666666666667, 89.33333333333334, 1.0, 2.0, 0.2539170916394295, 1.0, 1.0, 0.2539170916394295, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 709620.7412616409, 709620.7412616409, 241592.4476300147], 
processed observation next is [1.0, 0.0, 0.44391785150079005, 0.8933333333333334, 1.0, 1.0, 0.10110492968605962, 1.0, 0.5, 0.10110492968605962, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19711687257267804, 0.19711687257267804, 0.36058574273136523], 
reward next is 0.6394, 
noisyNet noise sample is [array([1.2425927], dtype=float32), -0.7328492]. 
=============================================
[2019-03-26 20:22:05,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[61.67173 ]
 [62.066395]
 [61.94158 ]
 [61.837406]
 [61.759808]], R is [[60.63285065]
 [60.75081635]
 [60.86759567]
 [60.98330307]
 [61.09815598]].
[2019-03-26 20:22:06,220] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.7235278e-28 9.9999905e-01 4.1181125e-26 9.6561280e-07 9.5710549e-13], sum to 1.0000
[2019-03-26 20:22:06,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4490
[2019-03-26 20:22:06,235] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 89.5, 1.0, 2.0, 0.5937695231609816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 829751.8329669524, 829751.8329669529, 199584.9481805256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7878600.0000, 
sim time next is 7879200.0000, 
raw observation next is [26.26666666666667, 89.33333333333333, 1.0, 2.0, 0.693426089341136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 969078.5251910627, 969078.5251910621, 219431.0477504276], 
processed observation next is [1.0, 0.17391304347826086, 0.44391785150079005, 0.8933333333333333, 1.0, 1.0, 0.6306338425796819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26918847921973965, 0.2691884792197395, 0.32750902649317554], 
reward next is 0.6725, 
noisyNet noise sample is [array([0.342941], dtype=float32), 1.7489133]. 
=============================================
[2019-03-26 20:22:08,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:08,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:08,299] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-03-26 20:22:08,559] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:08,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:08,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-03-26 20:22:08,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:08,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:08,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-03-26 20:22:09,456] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:09,457] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:09,472] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-03-26 20:22:10,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,098] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-03-26 20:22:10,149] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,150] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,159] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-03-26 20:22:10,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.5818498e-30 9.9659592e-01 9.3022977e-26 2.5588647e-03 8.4516173e-04], sum to 1.0000
[2019-03-26 20:22:10,231] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1698
[2019-03-26 20:22:10,233] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 84.0, 1.0, 2.0, 0.3407849854143521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539199.0849194026, 539199.0849194026, 170068.0473896108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [22.3, 84.0, 1.0, 2.0, 0.3294673708643176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 520693.2050942397, 520693.2050942397, 168598.3714292786], 
processed observation next is [1.0, 0.34782608695652173, 0.25592417061611383, 0.84, 1.0, 1.0, 0.19212936248712964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14463700141506658, 0.14463700141506658, 0.25163936034220685], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.17418112], dtype=float32), -0.17483023]. 
=============================================
[2019-03-26 20:22:10,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,408] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-03-26 20:22:10,434] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-03-26 20:22:10,475] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-03-26 20:22:10,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-03-26 20:22:10,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,527] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-03-26 20:22:10,536] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-03-26 20:22:10,556] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,557] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,578] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,582] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-03-26 20:22:10,603] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:22:10,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:10,605] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-03-26 20:22:10,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-03-26 20:22:10,672] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-03-26 20:22:20,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3432256e-32 3.2248765e-01 1.8660674e-30 6.7751235e-01 6.6036183e-11], sum to 1.0000
[2019-03-26 20:22:20,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3214
[2019-03-26 20:22:20,794] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.58333333333334, 96.0, 1.0, 2.0, 0.8188058515441471, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1222329.738009549, 1222329.73800955, 259281.9894097141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [22.56666666666667, 96.0, 1.0, 2.0, 0.7686168487608611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1153430.746249281, 1153430.746249281, 246913.0105025868], 
processed observation next is [1.0, 0.6956521739130435, 0.26856240126382325, 0.96, 1.0, 1.0, 0.7212251189889893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3203974295136892, 0.3203974295136892, 0.3685268813471445], 
reward next is 0.6315, 
noisyNet noise sample is [array([-0.84702533], dtype=float32), 1.5135305]. 
=============================================
[2019-03-26 20:22:22,532] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.2415479e-36 7.1749445e-10 2.4128068e-15], sum to 1.0000
[2019-03-26 20:22:22,541] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6430
[2019-03-26 20:22:22,549] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2866967877716635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461796.446713019, 461796.446713019, 164395.6499334722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 184800.0000, 
sim time next is 185400.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.2863924508741391, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461306.5985648271, 461306.5985648271, 164362.226335434], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.14023186852305916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1281407218235631, 0.1281407218235631, 0.24531675572452838], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.45949012], dtype=float32), 0.1370198]. 
=============================================
[2019-03-26 20:22:29,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5972279e-37 1.0000000e+00 3.0756884e-35 6.2565495e-09 2.5476789e-11], sum to 1.0000
[2019-03-26 20:22:29,115] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2659
[2019-03-26 20:22:29,122] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.13333333333333, 77.66666666666667, 1.0, 2.0, 0.3094962625492548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489258.1009426742, 489258.1009426742, 166226.0873306207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 315600.0000, 
sim time next is 316200.0000, 
raw observation next is [23.06666666666667, 77.83333333333333, 1.0, 2.0, 0.3080996017122549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487513.7142572048, 487513.7142572048, 166108.3679744664], 
processed observation next is [0.0, 0.6521739130434783, 0.29225908372827825, 0.7783333333333333, 1.0, 1.0, 0.16638506230392158, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1354204761825569, 0.1354204761825569, 0.247922937275323], 
reward next is 0.7521, 
noisyNet noise sample is [array([0.42474133], dtype=float32), -0.5029715]. 
=============================================
[2019-03-26 20:22:32,659] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.9634102e-37 1.0000000e+00 1.0586781e-32 2.4215174e-10 1.9897221e-13], sum to 1.0000
[2019-03-26 20:22:32,667] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4524
[2019-03-26 20:22:32,674] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.53333333333333, 86.33333333333334, 1.0, 2.0, 0.2654771746370886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 431528.6926698316, 431528.692669831, 162369.0579879229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 346800.0000, 
sim time next is 347400.0000, 
raw observation next is [20.5, 86.5, 1.0, 2.0, 0.2644502284675434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 429946.2735186359, 429946.2735186366, 162267.0934253527], 
processed observation next is [1.0, 0.0, 0.1706161137440759, 0.865, 1.0, 1.0, 0.11379545598499204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11942952042184331, 0.1194295204218435, 0.24218969167963092], 
reward next is 0.7578, 
noisyNet noise sample is [array([0.73912215], dtype=float32), -0.0417178]. 
=============================================
[2019-03-26 20:22:32,703] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7523466e-36 1.0000000e+00 3.6602487e-30 1.4215484e-09 2.9155061e-19], sum to 1.0000
[2019-03-26 20:22:32,713] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1020
[2019-03-26 20:22:32,721] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 87.16666666666667, 1.0, 2.0, 0.2821006884393824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459653.4282804437, 459653.4282804437, 164179.2948031635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596], 
processed observation next is [1.0, 0.08695652173913043, 0.15955766192733034, 0.8733333333333334, 1.0, 1.0, 0.11214203675045889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11906322705412137, 0.11906322705412137, 0.24203490633725375], 
reward next is 0.7580, 
noisyNet noise sample is [array([0.02409138], dtype=float32), -1.5507716]. 
=============================================
[2019-03-26 20:22:32,732] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.63205]
 [66.9978 ]
 [66.59083]
 [66.61782]
 [66.72834]], R is [[70.19766998]
 [70.25065613]
 [70.30654907]
 [70.36184692]
 [70.41656494]].
[2019-03-26 20:22:33,639] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 9.9999857e-01 0.0000000e+00 4.7787769e-15 1.4709065e-06], sum to 1.0000
[2019-03-26 20:22:33,648] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6725
[2019-03-26 20:22:33,652] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.95, 80.66666666666667, 1.0, 2.0, 0.2629360286976634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 429372.7078993038, 429372.7078993038, 162180.3633512583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 373800.0000, 
sim time next is 374400.0000, 
raw observation next is [21.0, 80.0, 1.0, 2.0, 0.25827091080741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 421985.7443882529, 421985.7443882535, 161711.2830845121], 
processed observation next is [1.0, 0.34782608695652173, 0.19431279620853087, 0.8, 1.0, 1.0, 0.10635049494868673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11721826233007025, 0.11721826233007042, 0.24136012400673448], 
reward next is 0.7586, 
noisyNet noise sample is [array([-0.942893], dtype=float32), -0.17967159]. 
=============================================
[2019-03-26 20:22:37,333] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9930157e-37 1.0000000e+00 7.1348409e-35 1.2712660e-11 1.2031023e-20], sum to 1.0000
[2019-03-26 20:22:37,350] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1310
[2019-03-26 20:22:37,355] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.2, 88.5, 1.0, 2.0, 0.2141789626914568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 356631.4660712971, 356631.4660712964, 157136.8971167101], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 538200.0000, 
sim time next is 538800.0000, 
raw observation next is [18.4, 87.66666666666666, 1.0, 2.0, 0.2158356069159676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 359130.1103477972, 359130.1103477978, 157343.2921589192], 
processed observation next is [1.0, 0.21739130434782608, 0.07109004739336493, 0.8766666666666666, 1.0, 1.0, 0.05522362279032239, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09975836398549923, 0.0997583639854994, 0.23484073456555107], 
reward next is 0.7652, 
noisyNet noise sample is [array([-1.3049452], dtype=float32), 0.5465162]. 
=============================================
[2019-03-26 20:22:40,280] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.8482224e-36 1.0000000e+00 3.2720450e-34 3.6823861e-10 3.1778132e-14], sum to 1.0000
[2019-03-26 20:22:40,286] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3584
[2019-03-26 20:22:40,294] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 55.66666666666667, 1.0, 2.0, 0.5498549963960074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898547.6938167696, 898547.6938167696, 205195.2833264635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 480000.0000, 
sim time next is 480600.0000, 
raw observation next is [24.95, 55.0, 1.0, 2.0, 0.5787618109575396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 945408.8146812594, 945408.8146812594, 211038.9238447963], 
processed observation next is [1.0, 0.5652173913043478, 0.3815165876777251, 0.55, 1.0, 1.0, 0.49248410958739713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26261355963368316, 0.26261355963368316, 0.31498346842506914], 
reward next is 0.6850, 
noisyNet noise sample is [array([1.2594286], dtype=float32), -0.45135805]. 
=============================================
[2019-03-26 20:22:40,550] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 5.9200279e-34 2.8612965e-12 1.5219797e-18], sum to 1.0000
[2019-03-26 20:22:40,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7506
[2019-03-26 20:22:40,564] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.23333333333333, 72.0, 1.0, 2.0, 0.2397650428501931, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395975.6637060771, 395975.6637060766, 159860.6397402199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 501600.0000, 
sim time next is 502200.0000, 
raw observation next is [21.0, 73.5, 1.0, 2.0, 0.2395569635656393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395710.2236278414, 395710.2236278414, 159836.5058957534], 
processed observation next is [1.0, 0.8260869565217391, 0.19431279620853087, 0.735, 1.0, 1.0, 0.08380357056101119, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10991950656328928, 0.10991950656328928, 0.23856194909813938], 
reward next is 0.7614, 
noisyNet noise sample is [array([0.06308849], dtype=float32), -0.87066275]. 
=============================================
[2019-03-26 20:22:40,967] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 20:22:40,969] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:22:40,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:40,971] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:22:40,972] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:22:40,972] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:40,974] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:22:40,974] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:40,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:22:40,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:40,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:22:40,997] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-03-26 20:22:41,018] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-03-26 20:22:41,020] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-03-26 20:22:41,020] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-03-26 20:22:41,088] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-03-26 20:22:42,376] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:22:42,377] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.29149270333334, 79.83395094, 1.0, 2.0, 0.293184175275549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 471483.4413050678, 471483.4413050684, 165063.458012478]
[2019-03-26 20:22:42,379] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:22:42,381] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 4.6831894e-38 6.7463648e-12 4.7058216e-17], sampled 0.7007935202330013
[2019-03-26 20:23:06,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:23:06,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.15000532, 92.795798215, 1.0, 2.0, 0.5961906851606508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 833136.566338875, 833136.5663388744, 200030.7442504216]
[2019-03-26 20:23:06,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:23:06,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 8.705537e-38 8.442991e-12 6.363128e-17], sampled 0.03204003114857035
[2019-03-26 20:23:32,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:23:32,136] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.63569538666667, 66.64765211, 1.0, 2.0, 0.5651433474902618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789733.8612333903, 789733.8612333903, 194427.2950185833]
[2019-03-26 20:23:32,137] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:23:32,139] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3016073e-13 5.0062403e-19], sampled 0.314761502136656
[2019-03-26 20:23:36,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:23:36,508] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.27709867666667, 57.91157114833333, 1.0, 2.0, 0.6224625778039776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 869864.8112776207, 869864.8112776213, 205003.7038691425]
[2019-03-26 20:23:36,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:23:36,514] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2709597e-13 1.1498195e-18], sampled 0.015500981333854136
[2019-03-26 20:23:43,320] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:23:43,322] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.18231081, 82.73417187666668, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 1.0, 1.03, 16.31661607523065, 6.9112, 184.5923449428631, 11039109.3632595, 3747204.567600967, 632870.9083749107]
[2019-03-26 20:23:43,324] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:23:43,328] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.2614703e-13 2.9910326e-01 5.5035192e-11 6.8894333e-01 1.1953347e-02], sampled 0.40415765842902684
[2019-03-26 20:23:43,329] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 11039109.3632595 W.
[2019-03-26 20:23:58,487] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:23:58,488] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.5, 52.0, 1.0, 2.0, 0.6581686081886676, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005972671665523, 6.9112, 168.9123160826414, 1816593.48776713, 1749358.747583205, 375840.3338136103]
[2019-03-26 20:23:58,489] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:23:58,490] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 4.8365595e-35 8.3106563e-11 1.3785149e-15], sampled 0.7448416361768804
[2019-03-26 20:23:58,491] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1816593.48776713 W.
[2019-03-26 20:24:12,822] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:24:12,824] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.3135325, 67.48150722, 1.0, 2.0, 0.5082452020218767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710197.5562066642, 710197.5562066636, 184897.2562919626]
[2019-03-26 20:24:12,825] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:24:12,826] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9086792e-12 8.6126740e-18], sampled 0.002186324954408514
[2019-03-26 20:24:14,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:24:14,676] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.87871423, 85.96236060999999, 1.0, 2.0, 0.5764077105633096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805480.701397891, 805480.701397891, 196433.8561203916]
[2019-03-26 20:24:14,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:24:14,683] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3479868e-12 5.3948984e-18], sampled 0.5742687057949267
[2019-03-26 20:24:20,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.23727557]
[2019-03-26 20:24:20,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.20913224, 73.80349185, 1.0, 2.0, 0.269378786942592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 442929.639826911, 442929.6398269117, 162880.8530077676]
[2019-03-26 20:24:20,164] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:24:20,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3661286e-12 5.4927899e-18], sampled 0.9337585163445556
[2019-03-26 20:24:35,244] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.8732 2928193357.7728 1338.0000
[2019-03-26 20:24:35,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164921908.5803 1778.0000
[2019-03-26 20:24:35,926] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8655.5460 2780105654.4285 933.0000
[2019-03-26 20:24:35,972] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.0860 2843317058.9716 1131.0000
[2019-03-26 20:24:36,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.0366 3008577742.1100 1766.0000
[2019-03-26 20:24:37,076] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 650000, evaluation results [650000.0, 7874.254934002473, 3164921908.580304, 1778.0, 8251.873184964594, 2928193357.7728205, 1338.0, 8655.546018810906, 2780105654.428513, 933.0, 7995.036567555068, 3008577742.1100116, 1766.0, 8493.086021879199, 2843317058.9715824, 1131.0]
[2019-03-26 20:24:40,651] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.3982701e-37 2.8246086e-12 2.1190420e-17], sum to 1.0000
[2019-03-26 20:24:40,662] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7446
[2019-03-26 20:24:40,667] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.6, 84.0, 1.0, 2.0, 0.22190195719751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 369653.2720077017, 369653.2720077011, 157767.6579558848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 601200.0000, 
sim time next is 601800.0000, 
raw observation next is [18.51666666666667, 84.5, 1.0, 2.0, 0.2206124718573001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 367596.1322680204, 367596.132268021, 157627.6287426749], 
processed observation next is [1.0, 1.0, 0.07661927330173794, 0.845, 1.0, 1.0, 0.06097888175578324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10211003674111678, 0.10211003674111695, 0.23526511752638044], 
reward next is 0.7647, 
noisyNet noise sample is [array([-0.0825384], dtype=float32), 1.7915527]. 
=============================================
[2019-03-26 20:24:43,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.3192789e-35 9.9999988e-01 1.5995200e-30 1.7815238e-07 2.7270231e-10], sum to 1.0000
[2019-03-26 20:24:43,038] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1769
[2019-03-26 20:24:43,049] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.66666666666667, 93.0, 1.0, 2.0, 0.2334899960315701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388733.2498148087, 388733.2498148087, 158874.2273807432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 699600.0000, 
sim time next is 700200.0000, 
raw observation next is [17.65, 93.0, 1.0, 2.0, 0.2227443206335127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370888.6993214728, 370888.6993214734, 157886.9921243822], 
processed observation next is [1.0, 0.08695652173913043, 0.035545023696682464, 0.93, 1.0, 1.0, 0.06354737425724422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10302463870040911, 0.10302463870040929, 0.23565222705131672], 
reward next is 0.7643, 
noisyNet noise sample is [array([-0.33154905], dtype=float32), -0.20931642]. 
=============================================
[2019-03-26 20:24:52,663] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6804872e-14 3.6963463e-18], sum to 1.0000
[2019-03-26 20:24:52,678] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3134
[2019-03-26 20:24:52,684] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 80.5, 1.0, 2.0, 0.2554862561026551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419403.3804272165, 419403.3804272165, 161456.5559875444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 765000.0000, 
sim time next is 765600.0000, 
raw observation next is [20.53333333333333, 81.0, 1.0, 2.0, 0.2552438100066881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419011.686924542, 419011.6869245426, 161432.2397282651], 
processed observation next is [1.0, 0.8695652173913043, 0.17219589257503945, 0.81, 1.0, 1.0, 0.10270338555022662, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11639213525681723, 0.11639213525681738, 0.24094364138547028], 
reward next is 0.7591, 
noisyNet noise sample is [array([-1.7193232], dtype=float32), -1.1709101]. 
=============================================
[2019-03-26 20:25:05,489] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0079212e-16 2.0117604e-20], sum to 1.0000
[2019-03-26 20:25:05,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3045
[2019-03-26 20:25:05,503] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 93.33333333333334, 1.0, 2.0, 0.3315436398908417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513817.9847701254, 513817.9847701254, 167813.384624132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 963600.0000, 
sim time next is 964200.0000, 
raw observation next is [21.88333333333333, 93.16666666666666, 1.0, 2.0, 0.3316361779264139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514014.6031773683, 514014.6031773683, 167830.4349445109], 
processed observation next is [1.0, 0.13043478260869565, 0.2361769352290678, 0.9316666666666665, 1.0, 1.0, 0.1947423830438722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14278183421593563, 0.14278183421593563, 0.2504931864843446], 
reward next is 0.7495, 
noisyNet noise sample is [array([-0.37596422], dtype=float32), -0.7957356]. 
=============================================
[2019-03-26 20:25:07,332] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 4.5936141e-35 5.1432823e-15 1.4224690e-15], sum to 1.0000
[2019-03-26 20:25:07,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5062
[2019-03-26 20:25:07,351] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 94.66666666666666, 1.0, 2.0, 0.5663689881175034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 871975.5004917851, 871975.5004917858, 204341.0745419479], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 992400.0000, 
sim time next is 993000.0000, 
raw observation next is [21.91666666666666, 94.83333333333333, 1.0, 2.0, 0.5968879034950557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918821.9658997034, 918821.9658997034, 210431.7754291751], 
processed observation next is [1.0, 0.4782608695652174, 0.23775671406003138, 0.9483333333333333, 1.0, 1.0, 0.5143227752952478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2552283238610287, 0.2552283238610287, 0.31407727675996283], 
reward next is 0.6859, 
noisyNet noise sample is [array([-0.09211665], dtype=float32), -1.3223093]. 
=============================================
[2019-03-26 20:25:07,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.348976]
 [63.75887 ]
 [63.29695 ]
 [62.45841 ]
 [61.821613]], R is [[63.09465408]
 [63.15872192]
 [63.24363708]
 [63.32418442]
 [63.3825264 ]].
[2019-03-26 20:25:12,210] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.5652426e-36 7.8030806e-17 8.1789686e-23], sum to 1.0000
[2019-03-26 20:25:12,222] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7448
[2019-03-26 20:25:12,228] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 70.5, 1.0, 2.0, 0.6693726472986706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1036894.003653821, 1036894.003653821, 226848.2525376893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1085400.0000, 
sim time next is 1086000.0000, 
raw observation next is [25.26666666666667, 70.0, 1.0, 2.0, 0.6543213605870494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1011252.448447034, 1011252.448447034, 223177.816186254], 
processed observation next is [1.0, 0.5652173913043478, 0.3965244865718801, 0.7, 1.0, 1.0, 0.5835197115506618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28090345790195387, 0.28090345790195387, 0.3331012181884388], 
reward next is 0.6669, 
noisyNet noise sample is [array([1.6289994], dtype=float32), -0.4938239]. 
=============================================
[2019-03-26 20:25:12,244] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[63.80626 ]
 [64.73977 ]
 [65.99775 ]
 [66.02679 ]
 [66.172386]], R is [[63.65424347]
 [63.67911911]
 [63.72673035]
 [63.81335068]
 [63.90037537]].
[2019-03-26 20:25:18,545] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3720649e-17 6.3557953e-19], sum to 1.0000
[2019-03-26 20:25:18,556] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9855
[2019-03-26 20:25:18,563] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 74.66666666666667, 1.0, 2.0, 0.3513624789805923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541945.5373656523, 541945.5373656517, 169996.6903790539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1196400.0000, 
sim time next is 1197000.0000, 
raw observation next is [24.5, 75.5, 1.0, 2.0, 0.3516749710018386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542032.8963588405, 542032.8963588398, 169992.5359783983], 
processed observation next is [1.0, 0.8695652173913043, 0.3601895734597157, 0.755, 1.0, 1.0, 0.21888550723113082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15056469343301127, 0.15056469343301107, 0.25372020295283326], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.18430856], dtype=float32), -0.3994942]. 
=============================================
[2019-03-26 20:25:18,582] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.7078  ]
 [81.742096]
 [81.956795]
 [81.97099 ]
 [82.11649 ]], R is [[81.59750366]
 [81.52780914]
 [81.45865631]
 [81.39022064]
 [81.32271576]].
[2019-03-26 20:25:20,383] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6533252e-20 5.2782216e-23], sum to 1.0000
[2019-03-26 20:25:20,391] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5411
[2019-03-26 20:25:20,397] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 98.0, 1.0, 2.0, 0.3146155162227376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 497157.3719752533, 497157.3719752527, 166806.0286991008], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1399200.0000, 
sim time next is 1399800.0000, 
raw observation next is [20.58333333333334, 98.0, 1.0, 2.0, 0.3146256245799867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496975.2427550306, 496975.2427550299, 166788.3153678298], 
processed observation next is [0.0, 0.17391304347826086, 0.17456556082148533, 0.98, 1.0, 1.0, 0.17424774045781533, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13804867854306405, 0.13804867854306385, 0.24893778413108927], 
reward next is 0.7511, 
noisyNet noise sample is [array([0.9489298], dtype=float32), -1.1902432]. 
=============================================
[2019-03-26 20:25:26,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 1.0077357e-37 6.0734764e-17 1.4524619e-16], sum to 1.0000
[2019-03-26 20:25:26,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6176
[2019-03-26 20:25:26,424] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.85, 93.5, 1.0, 2.0, 0.5543588314973338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802578.0055774802, 802578.0055774802, 196096.477506292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1319400.0000, 
sim time next is 1320000.0000, 
raw observation next is [23.76666666666667, 93.66666666666667, 1.0, 2.0, 0.536796362988044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779058.4267126544, 779058.4267126544, 193226.5869690716], 
processed observation next is [1.0, 0.2608695652173913, 0.32543443917851517, 0.9366666666666668, 1.0, 1.0, 0.4419233289012578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2164051185312929, 0.2164051185312929, 0.2883978909986143], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.01829829], dtype=float32), 0.014883037]. 
=============================================
[2019-03-26 20:25:26,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.07499 ]
 [70.342285]
 [70.89957 ]
 [71.53772 ]
 [71.6087  ]], R is [[70.15279388]
 [70.15858459]
 [70.16835022]
 [70.18634033]
 [70.22115326]].
[2019-03-26 20:25:27,029] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.3108748e-37 9.9772241e-14 2.3114631e-12], sum to 1.0000
[2019-03-26 20:25:27,037] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0811
[2019-03-26 20:25:27,041] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 88.0, 1.0, 2.0, 0.6708684931586075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1065116.051333728, 1065116.051333728, 229682.4849613019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1346400.0000, 
sim time next is 1347000.0000, 
raw observation next is [21.46666666666667, 88.16666666666667, 1.0, 2.0, 0.5704487401589996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906774.0291150539, 906774.0291150539, 207844.0228276239], 
processed observation next is [1.0, 0.6086956521739131, 0.21642969984202226, 0.8816666666666667, 1.0, 1.0, 0.4824683616373489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25188167475418166, 0.25188167475418166, 0.3102149594442148], 
reward next is 0.6898, 
noisyNet noise sample is [array([-0.73703367], dtype=float32), 1.850965]. 
=============================================
[2019-03-26 20:25:27,055] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[72.98736 ]
 [73.10482 ]
 [73.484726]
 [74.08048 ]
 [74.48203 ]], R is [[73.70250702]
 [73.62267303]
 [73.54047394]
 [73.46211243]
 [73.39910126]].
[2019-03-26 20:25:28,017] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.11940020e-15
 1.03253784e-13], sum to 1.0000
[2019-03-26 20:25:28,029] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4381
[2019-03-26 20:25:28,034] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.7187401081279726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1092218.434412109, 1092218.434412109, 236227.0199749477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1338600.0000, 
sim time next is 1339200.0000, 
raw observation next is [22.4, 93.0, 1.0, 2.0, 0.77006259614506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1174780.441420588, 1174780.441420588, 249615.5148186972], 
processed observation next is [1.0, 0.5217391304347826, 0.2606635071090047, 0.93, 1.0, 1.0, 0.7229669833073012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32632790039460774, 0.32632790039460774, 0.37256046987865254], 
reward next is 0.6274, 
noisyNet noise sample is [array([2.3741956], dtype=float32), -1.1180481]. 
=============================================
[2019-03-26 20:25:28,111] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.6665516e-36 2.3427066e-15 6.8470049e-18], sum to 1.0000
[2019-03-26 20:25:28,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9158
[2019-03-26 20:25:28,127] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.5, 1.0, 2.0, 0.3064054166652293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483867.5143219733, 483867.5143219727, 165820.8009257269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1571400.0000, 
sim time next is 1572000.0000, 
raw observation next is [21.6, 89.33333333333333, 1.0, 2.0, 0.307159065139851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485294.8178840481, 485294.8178840481, 165930.2536523599], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.8933333333333333, 1.0, 1.0, 0.16525188571066382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13480411607890225, 0.13480411607890225, 0.24765709500352226], 
reward next is 0.7523, 
noisyNet noise sample is [array([-0.45469463], dtype=float32), -1.0881928]. 
=============================================
[2019-03-26 20:25:28,146] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.57951]
 [76.74686]
 [76.95445]
 [76.86581]
 [77.07326]], R is [[76.33399963]
 [76.32316589]
 [76.31248474]
 [76.30116272]
 [76.28843689]].
[2019-03-26 20:25:29,905] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.7807979e-33 2.4659745e-13 1.5564922e-18], sum to 1.0000
[2019-03-26 20:25:29,914] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3731
[2019-03-26 20:25:29,918] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 95.66666666666667, 1.0, 2.0, 0.3150424282966958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498548.6015814939, 498548.6015814944, 166923.9400284876], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1377600.0000, 
sim time next is 1378200.0000, 
raw observation next is [20.73333333333333, 95.83333333333333, 1.0, 2.0, 0.3149105760068865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498500.6409171044, 498500.6409171037, 166923.4013324424], 
processed observation next is [1.0, 0.9565217391304348, 0.18167456556082143, 0.9583333333333333, 1.0, 1.0, 0.1745910554299837, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13847240025475122, 0.13847240025475102, 0.2491394049737946], 
reward next is 0.7509, 
noisyNet noise sample is [array([1.2852495], dtype=float32), -0.12408327]. 
=============================================
[2019-03-26 20:25:31,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.4363365e-37 1.3207342e-13 2.1448631e-17], sum to 1.0000
[2019-03-26 20:25:31,364] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0754
[2019-03-26 20:25:31,376] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 95.0, 1.0, 2.0, 0.4116493389435469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607956.8027249908, 607956.8027249908, 175098.0413635816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1626000.0000, 
sim time next is 1626600.0000, 
raw observation next is [23.18333333333333, 95.0, 1.0, 2.0, 0.4111974519779803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606916.7836883408, 606916.7836883408, 174989.8254714457], 
processed observation next is [1.0, 0.8260869565217391, 0.29778830963665076, 0.95, 1.0, 1.0, 0.2905993397325064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16858799546898356, 0.16858799546898356, 0.2611788439872324], 
reward next is 0.7388, 
noisyNet noise sample is [array([-0.73139334], dtype=float32), -1.5509639]. 
=============================================
[2019-03-26 20:25:32,791] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 20:25:32,793] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:25:32,794] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:25:32,795] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:25:32,795] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:25:32,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:25:32,797] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:25:32,796] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:25:32,798] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:25:32,798] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:25:32,802] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:25:32,824] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-03-26 20:25:32,824] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-03-26 20:25:32,825] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-03-26 20:25:32,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-03-26 20:25:32,909] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-03-26 20:26:23,737] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:26:23,739] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.46461314, 79.03385822, 1.0, 2.0, 0.4886208692544546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688616.2576464604, 688616.2576464604, 182583.9782510498]
[2019-03-26 20:26:23,739] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:26:23,742] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4067862e-17 3.5483448e-19], sampled 0.2616303938929265
[2019-03-26 20:26:24,069] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:26:24,071] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5959326189443598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832775.7947019509, 832775.7947019509, 199982.5151668325]
[2019-03-26 20:26:24,072] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:26:24,076] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.7657787e-38 1.6917702e-16 1.5936647e-18], sampled 0.28191467937726855
[2019-03-26 20:26:29,382] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:26:29,384] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.0, 49.0, 1.0, 2.0, 0.5847262870682925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817109.6751662266, 817109.6751662266, 197934.6919893911]
[2019-03-26 20:26:29,386] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:26:29,389] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.2865171e-18 3.3238366e-20], sampled 0.9228729167627152
[2019-03-26 20:26:34,332] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:26:34,333] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.95, 48.0, 1.0, 2.0, 0.8460581253263009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1182503.988971924, 1182503.988971924, 255535.3865754806]
[2019-03-26 20:26:34,336] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:26:34,340] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4418742e-17 3.5799149e-19], sampled 0.4422431359527109
[2019-03-26 20:26:49,759] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:26:49,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.79360109, 83.33741476, 1.0, 2.0, 0.6661203068678487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930901.3566557951, 930901.3566557951, 213710.3524947216]
[2019-03-26 20:26:49,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:26:49,763] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5429813e-17 1.9204053e-19], sampled 0.8549796907365167
[2019-03-26 20:27:02,444] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:27:02,445] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.71666666666667, 74.5, 1.0, 2.0, 0.8412762729374065, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994774252086307, 6.9112, 168.9123913162312, 2072842.725911815, 2013552.473467888, 418717.7312775371]
[2019-03-26 20:27:02,446] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:27:02,450] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 7.8004461e-36 2.8096021e-15 3.6732815e-17], sampled 0.19401028880836602
[2019-03-26 20:27:02,452] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2072842.725911815 W.
[2019-03-26 20:27:15,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.19307639]
[2019-03-26 20:27:15,340] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.55, 48.0, 1.0, 2.0, 0.7039841398821349, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.941701255622585, 6.9112, 168.9127511773033, 1916256.358148526, 1894617.743941919, 391729.0506990736]
[2019-03-26 20:27:15,344] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:27:15,346] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.3727966e-35 3.6466602e-15 4.9149064e-17], sampled 0.4649359744360956
[2019-03-26 20:27:15,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1916256.358148526 W.
[2019-03-26 20:27:26,286] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8248.8402 2928097619.8215 1338.0000
[2019-03-26 20:27:27,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.6531 2843189204.4954 1131.0000
[2019-03-26 20:27:27,637] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7994.1196 3008333249.7797 1766.0000
[2019-03-26 20:27:27,709] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.1478 2779902905.6182 933.0000
[2019-03-26 20:27:27,748] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164660858.3405 1778.0000
[2019-03-26 20:27:28,764] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 675000, evaluation results [675000.0, 7876.615402862868, 3164660858.3404922, 1778.0, 8248.84019698634, 2928097619.821522, 1338.0, 8657.147806274199, 2779902905.6182404, 933.0, 7994.1196179763165, 3008333249.779738, 1766.0, 8492.65313717663, 2843189204.495385, 1131.0]
[2019-03-26 20:27:32,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.777731e-19 4.088808e-21], sum to 1.0000
[2019-03-26 20:27:32,769] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5798
[2019-03-26 20:27:32,776] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.23333333333333, 51.00000000000001, 1.0, 2.0, 0.3512963936820009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537565.9046053564, 537565.9046053557, 169504.0934773705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1513200.0000, 
sim time next is 1513800.0000, 
raw observation next is [29.3, 51.0, 1.0, 2.0, 0.3526428651476576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538711.9606284414, 538711.9606284414, 169568.5944270689], 
processed observation next is [0.0, 0.5217391304347826, 0.5876777251184835, 0.51, 1.0, 1.0, 0.22005164475621394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14964221128567817, 0.14964221128567817, 0.25308745436875957], 
reward next is 0.7469, 
noisyNet noise sample is [array([-1.1933242], dtype=float32), -0.6300395]. 
=============================================
[2019-03-26 20:27:33,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5356672e-18 1.3416430e-24], sum to 1.0000
[2019-03-26 20:27:33,900] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0553
[2019-03-26 20:27:33,905] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 51.0, 1.0, 2.0, 0.3545495643181173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540696.9328101261, 540696.9328101267, 169702.3073243424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1514400.0000, 
sim time next is 1515000.0000, 
raw observation next is [29.43333333333333, 51.0, 1.0, 2.0, 0.356860421157582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543278.6023775876, 543278.6023775876, 169886.0060121602], 
processed observation next is [0.0, 0.5217391304347826, 0.5939968404423379, 0.51, 1.0, 1.0, 0.2251330375392554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15091072288266322, 0.15091072288266322, 0.25356120300322416], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.9627889], dtype=float32), -0.018100936]. 
=============================================
[2019-03-26 20:27:33,915] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[79.75477]
 [79.73439]
 [79.7121 ]
 [79.70884]
 [79.7331 ]], R is [[79.70986938]
 [79.65948486]
 [79.60980225]
 [79.56071472]
 [79.51218414]].
[2019-03-26 20:27:38,776] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 4.0654340e-35 7.7614338e-16 1.0216919e-18], sum to 1.0000
[2019-03-26 20:27:38,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5589
[2019-03-26 20:27:38,795] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.91666666666667, 85.00000000000001, 1.0, 2.0, 0.7486606651701452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1126843.687314722, 1126843.687314722, 242303.078706332], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1599000.0000, 
sim time next is 1599600.0000, 
raw observation next is [23.93333333333333, 85.0, 1.0, 2.0, 0.6382923479185131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 960205.2470163631, 960205.2470163625, 216681.1652413676], 
processed observation next is [1.0, 0.5217391304347826, 0.3333333333333332, 0.85, 1.0, 1.0, 0.564207648094594, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2667236797267675, 0.26672367972676736, 0.32340472424084715], 
reward next is 0.6766, 
noisyNet noise sample is [array([-0.08391315], dtype=float32), -0.4760683]. 
=============================================
[2019-03-26 20:27:42,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3200160e-18 1.0837586e-18], sum to 1.0000
[2019-03-26 20:27:42,687] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5820
[2019-03-26 20:27:42,692] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.31666666666667, 99.0, 1.0, 2.0, 0.4465584681719386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.0587700157, 643133.0587700164, 178103.1880650497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1656600.0000, 
sim time next is 1657200.0000, 
raw observation next is [23.33333333333334, 99.00000000000001, 1.0, 2.0, 0.4301777949515875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619112.6441190926, 619112.6441190919, 175705.510988684], 
processed observation next is [1.0, 0.17391304347826086, 0.3048973143759877, 0.9900000000000001, 1.0, 1.0, 0.3134672228332379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17197573447752573, 0.17197573447752554, 0.26224703132639404], 
reward next is 0.7378, 
noisyNet noise sample is [array([-0.955089], dtype=float32), 2.215879]. 
=============================================
[2019-03-26 20:27:48,452] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2319316e-31 1.0000000e+00 1.2398315e-26 4.3857631e-10 6.3783453e-12], sum to 1.0000
[2019-03-26 20:27:48,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8742
[2019-03-26 20:27:48,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1756405.504503039 W.
[2019-03-26 20:27:48,474] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.337521059127193, 6.9112, 168.9110588598789, 1756405.504503039, 1453962.070154995, 311348.8603486515], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [27.03333333333333, 84.33333333333334, 1.0, 2.0, 0.4067278992483989, 1.0, 1.0, 0.4067278992483989, 1.0, 1.0, 0.6955508872474537, 6.911200000000001, 6.9112, 170.5573041426782, 1705812.31718062, 1705812.31718062, 355091.8060222259], 
processed observation next is [1.0, 0.6086956521739131, 0.48025276461295413, 0.8433333333333334, 1.0, 1.0, 0.28521433644385413, 1.0, 0.5, 0.28521433644385413, 1.0, 0.5, 0.6287205942042118, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.47383675477239445, 0.47383675477239445, 0.5299877701824267], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.33198708], dtype=float32), -1.0064231]. 
=============================================
[2019-03-26 20:27:58,172] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 9.9999976e-01 2.5787246e-32 2.0325758e-07 7.5268421e-20], sum to 1.0000
[2019-03-26 20:27:58,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2603
[2019-03-26 20:27:58,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 94.0, 1.0, 2.0, 0.4170525105465058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 612808.8123020224, 612808.8123020229, 175467.4892080617], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1918800.0000, 
sim time next is 1919400.0000, 
raw observation next is [23.51666666666667, 93.33333333333334, 1.0, 2.0, 0.4608808574322754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677225.1522928596, 677225.1522928596, 181884.2629197574], 
processed observation next is [1.0, 0.21739130434782608, 0.31358609794628767, 0.9333333333333335, 1.0, 1.0, 0.35045886437623547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18811809785912767, 0.18811809785912767, 0.27146904913396624], 
reward next is 0.7285, 
noisyNet noise sample is [array([-1.0857239], dtype=float32), -2.9191298]. 
=============================================
[2019-03-26 20:27:58,888] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3400709e-34 1.0000000e+00 1.7740094e-30 5.6253182e-08 1.1910908e-15], sum to 1.0000
[2019-03-26 20:27:58,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9999
[2019-03-26 20:27:58,906] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 82.0, 1.0, 2.0, 0.9743470879759385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1390709.438635488, 1390709.438635488, 295639.1465482148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1933200.0000, 
sim time next is 1933800.0000, 
raw observation next is [25.85, 81.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.009109463974895, 6.9112, 168.9124048539953, 1553421.696664891, 1483961.579989437, 316169.3551255277], 
processed observation next is [1.0, 0.391304347826087, 0.4241706161137442, 0.8166666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.009790946397489541, 0.0, 0.829437236266654, 0.4315060268513586, 0.41221154999706583, 0.4718945598888473], 
reward next is 0.0386, 
noisyNet noise sample is [array([-0.89237505], dtype=float32), -1.0211716]. 
=============================================
[2019-03-26 20:28:00,684] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8078505e-31 9.9998748e-01 1.0307446e-27 1.2481765e-05 1.6030569e-13], sum to 1.0000
[2019-03-26 20:28:00,692] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7499
[2019-03-26 20:28:00,703] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2117456.269744956 W.
[2019-03-26 20:28:00,707] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.76666666666667, 77.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.842718100487202, 6.9112, 168.9078628704263, 2117456.269744956, 1456625.197115624, 311736.0671148876], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1954200.0000, 
sim time next is 1954800.0000, 
raw observation next is [26.6, 78.0, 1.0, 2.0, 0.6253220354516484, 1.0, 1.0, 0.6253220354516484, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1748430.546880371, 1748430.546880371, 343779.4307152414], 
processed observation next is [1.0, 0.6521739130434783, 0.4597156398104266, 0.78, 1.0, 1.0, 0.5485807656043957, 1.0, 0.5, 0.5485807656043957, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48567515191121413, 0.48567515191121413, 0.5131036279331961], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00421356], dtype=float32), -1.4965066]. 
=============================================
[2019-03-26 20:28:04,556] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3651279e-38 9.9978977e-01 7.8890585e-34 2.1028251e-04 3.0837011e-19], sum to 1.0000
[2019-03-26 20:28:04,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4141
[2019-03-26 20:28:04,570] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 88.5, 1.0, 2.0, 0.4819775380918156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673480.742453085, 673480.7424530843, 180820.3905261952], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2057400.0000, 
sim time next is 2058000.0000, 
raw observation next is [25.53333333333333, 88.66666666666666, 1.0, 2.0, 0.480554741605886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671492.0006293404, 671492.0006293404, 180605.3452978999], 
processed observation next is [0.0, 0.8260869565217391, 0.4091627172195892, 0.8866666666666666, 1.0, 1.0, 0.37416233928420006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18652555573037233, 0.18652555573037233, 0.26956021686253717], 
reward next is 0.7304, 
noisyNet noise sample is [array([0.9727279], dtype=float32), -0.87246364]. 
=============================================
[2019-03-26 20:28:04,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.32559 ]
 [79.281784]
 [79.26029 ]
 [79.26799 ]
 [79.219955]], R is [[79.31558228]
 [79.25254822]
 [79.18982697]
 [79.12741852]
 [79.06535339]].
[2019-03-26 20:28:08,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.9304824e-33 6.8021889e-14 4.2746386e-19], sum to 1.0000
[2019-03-26 20:28:08,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1483
[2019-03-26 20:28:08,362] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 91.5, 1.0, 2.0, 0.4922107507916471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687784.5268342736, 687784.5268342736, 182385.0354891311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2099400.0000, 
sim time next is 2100000.0000, 
raw observation next is [25.76666666666667, 91.0, 1.0, 2.0, 0.4946767327523364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 691231.4569425497, 691231.4569425491, 182766.6217646147], 
processed observation next is [0.0, 0.30434782608695654, 0.42022116903633505, 0.91, 1.0, 1.0, 0.39117678644859816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19200873803959714, 0.19200873803959698, 0.2727860026337533], 
reward next is 0.7272, 
noisyNet noise sample is [array([0.298508], dtype=float32), 0.28284398]. 
=============================================
[2019-03-26 20:28:08,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.19015 ]
 [71.203026]
 [71.16346 ]
 [71.13967 ]
 [71.10569 ]], R is [[71.21993256]
 [71.23551941]
 [71.25151062]
 [71.26792145]
 [71.28476715]].
[2019-03-26 20:28:11,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6333780e-08 2.8690233e-20], sum to 1.0000
[2019-03-26 20:28:11,105] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1059
[2019-03-26 20:28:11,110] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 89.33333333333334, 1.0, 2.0, 0.5373715115108973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750911.6547711656, 750911.654771165, 189658.679064698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
processed observation next is [0.0, 0.9130434782608695, 0.4755134281200636, 0.8966666666666667, 1.0, 1.0, 0.4447071547769099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2092604344049143, 0.2092604344049143, 0.2835075185041212], 
reward next is 0.7165, 
noisyNet noise sample is [array([-2.7070637], dtype=float32), -1.1604234]. 
=============================================
[2019-03-26 20:28:16,894] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3275427e-32 9.6211153e-01 1.7251511e-28 3.7888426e-02 5.1760391e-11], sum to 1.0000
[2019-03-26 20:28:16,905] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3715
[2019-03-26 20:28:16,911] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 82.0, 1.0, 2.0, 0.5457633867199854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762642.4945186689, 762642.4945186696, 191077.0945698183], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2235600.0000, 
sim time next is 2236200.0000, 
raw observation next is [28.3, 82.33333333333334, 1.0, 2.0, 0.5445081688488133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 760887.8415388272, 760887.8415388279, 190863.5972684061], 
processed observation next is [1.0, 0.9130434782608695, 0.5402843601895735, 0.8233333333333335, 1.0, 1.0, 0.4512146612636304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21135773376078532, 0.2113577337607855, 0.28487104069911356], 
reward next is 0.7151, 
noisyNet noise sample is [array([-1.5405047], dtype=float32), -0.38767755]. 
=============================================
[2019-03-26 20:28:21,784] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.9631806e-36 1.1883284e-01 3.7716065e-32 8.8116711e-01 2.6828886e-10], sum to 1.0000
[2019-03-26 20:28:21,796] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9951
[2019-03-26 20:28:21,801] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 93.0, 1.0, 2.0, 0.2772721625542368, 1.0, 2.0, 0.2772721625542368, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 774914.6116525705, 774914.6116525705, 245640.6268341332], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2491200.0000, 
sim time next is 2491800.0000, 
raw observation next is [26.98333333333333, 93.0, 1.0, 2.0, 0.2777568531921441, 1.0, 2.0, 0.2777568531921441, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 776269.705418265, 776269.705418265, 245727.7308313216], 
processed observation next is [1.0, 0.8695652173913043, 0.4778830963665086, 0.93, 1.0, 1.0, 0.12982753396643867, 1.0, 1.0, 0.12982753396643867, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21563047372729582, 0.21563047372729582, 0.36675780721092777], 
reward next is 0.6332, 
noisyNet noise sample is [array([0.05686946], dtype=float32), 0.116090916]. 
=============================================
[2019-03-26 20:28:24,518] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 20:28:24,521] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:28:24,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:28:24,523] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:28:24,524] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:28:24,525] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:28:24,526] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:28:24,528] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:28:24,528] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:28:24,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:28:24,531] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:28:24,544] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-03-26 20:28:24,564] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-03-26 20:28:24,583] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-03-26 20:28:24,584] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-03-26 20:28:24,602] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-03-26 20:28:34,645] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:28:34,646] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.395441125, 70.03964468000001, 1.0, 2.0, 0.2928231494241302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473474.1948005774, 473474.1948005774, 165192.3063664452]
[2019-03-26 20:28:34,647] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:28:34,651] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1369941e-37 1.0000000e+00 2.4985555e-33 1.0861044e-12 7.0062650e-21], sampled 0.28683014326888323
[2019-03-26 20:28:44,537] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:28:44,538] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.975459575, 94.93876928, 1.0, 2.0, 0.4185590118050752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655625.6958036791, 655625.6958036784, 180226.4825858689]
[2019-03-26 20:28:44,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:28:44,543] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.69703369e-36 1.00000000e+00 1.14531544e-32 2.15041093e-12
 1.95437042e-20], sampled 0.8672835425413176
[2019-03-26 20:28:54,194] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:28:54,195] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.53333333333333, 81.50000000000001, 1.0, 2.0, 0.490343729179658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685174.8262104837, 685174.826210483, 182096.2418256078]
[2019-03-26 20:28:54,197] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:28:54,200] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1482742e-37 1.0000000e+00 1.0093690e-33 7.2322969e-13 3.8042594e-21], sampled 0.9284353080686061
[2019-03-26 20:29:09,524] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:29:09,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.751480225, 89.98422936166666, 1.0, 2.0, 0.3754466032318077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 574799.6277643741, 574799.6277643747, 172678.2063729673]
[2019-03-26 20:29:09,527] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:29:09,529] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8418938e-37 1.0000000e+00 1.5457174e-33 8.7560384e-13 5.0695388e-21], sampled 0.05989319802607207
[2019-03-26 20:29:24,336] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:29:24,337] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.54994173, 62.56750566, 1.0, 2.0, 0.8154608203620296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1139716.322001414, 1139716.322001414, 247746.4168197952]
[2019-03-26 20:29:24,338] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:29:24,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0586379e-34 1.0000000e+00 2.6362375e-30 2.4671671e-11 7.6290056e-19], sampled 0.0425416569793341
[2019-03-26 20:29:31,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:29:31,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.16666666666666, 74.16666666666667, 1.0, 2.0, 0.9127263192643151, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.001447476360211, 6.9112, 168.9123469045359, 2172849.978089194, 2108825.543345491, 437777.9380527213]
[2019-03-26 20:29:31,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:29:31,524] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.7856092e-24 9.9999952e-01 2.6184085e-21 4.2780312e-07 1.3328154e-12], sampled 0.35949051833899803
[2019-03-26 20:29:31,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2172849.978089194 W.
[2019-03-26 20:29:32,773] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:29:32,774] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.15, 80.83333333333334, 1.0, 2.0, 0.6520209366583688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 911189.0717845376, 911189.0717845376, 210837.7767278042]
[2019-03-26 20:29:32,775] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:29:32,778] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6200122e-37 1.0000000e+00 4.2273547e-33 1.3750821e-12 9.9852607e-21], sampled 0.0006203614826291171
[2019-03-26 20:29:47,675] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:29:47,675] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.22032389, 79.89353974, 1.0, 2.0, 0.3883143071902441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589051.0496123554, 589051.0496123554, 173806.4579704464]
[2019-03-26 20:29:47,678] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:29:47,680] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 9.7699698e-35 2.5368824e-13 7.8880567e-22], sampled 0.4728776748774772
[2019-03-26 20:30:02,978] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:30:02,979] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.48333333333333, 35.83333333333334, 1.0, 2.0, 0.2694974462198418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 438744.8061118831, 438744.8061118825, 162815.8778717775]
[2019-03-26 20:30:02,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:30:02,982] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 3.1188844e-35 1.5199558e-13 3.6547386e-22], sampled 0.567438714535393
[2019-03-26 20:30:13,765] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17746852]
[2019-03-26 20:30:13,766] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.13333333333333, 76.66666666666666, 1.0, 2.0, 0.4838039509713575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676033.6525013294, 676033.6525013294, 181096.7129716808]
[2019-03-26 20:30:13,766] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:30:13,767] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.4097849e-37 1.0000000e+00 3.3969043e-33 1.2465698e-12 8.6171629e-21], sampled 0.27756222167978895
[2019-03-26 20:30:18,642] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.0081 3164859047.8827 1778.0000
[2019-03-26 20:30:19,016] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.9776 2843378969.8112 1131.0000
[2019-03-26 20:30:19,058] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.4170 2928250869.2091 1338.0000
[2019-03-26 20:30:19,253] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.8290 2780079899.7209 933.0000
[2019-03-26 20:30:19,262] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.4188 3008499702.8754 1766.0000
[2019-03-26 20:30:20,279] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 700000, evaluation results [700000.0, 7875.008135704462, 3164859047.882678, 1778.0, 8252.416984994034, 2928250869.20911, 1338.0, 8656.829044373291, 2780079899.7208705, 933.0, 7996.418845919515, 3008499702.8753877, 1766.0, 8492.97764518503, 2843378969.811234, 1131.0]
[2019-03-26 20:30:22,517] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.2166161e-13 1.0620837e-25], sum to 1.0000
[2019-03-26 20:30:22,531] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8491
[2019-03-26 20:30:22,537] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.71666666666667, 78.33333333333333, 1.0, 2.0, 0.5741466427085284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802319.8582302403, 802319.8582302403, 196028.3315380805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2412600.0000, 
sim time next is 2413200.0000, 
raw observation next is [29.63333333333334, 78.66666666666667, 1.0, 2.0, 0.5723458124047743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799802.4074891193, 799802.4074891187, 195707.1499520162], 
processed observation next is [1.0, 0.9565217391304348, 0.6034755134281204, 0.7866666666666667, 1.0, 1.0, 0.4847539908491257, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22216733541364425, 0.22216733541364408, 0.2921002238089794], 
reward next is 0.7079, 
noisyNet noise sample is [array([-0.59118456], dtype=float32), 0.6435408]. 
=============================================
[2019-03-26 20:30:26,852] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6271618e-34 1.0000000e+00 2.5334633e-32 2.9682012e-10 1.9083766e-20], sum to 1.0000
[2019-03-26 20:30:26,865] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7449
[2019-03-26 20:30:26,877] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.25, 96.5, 1.0, 2.0, 0.6676652381900295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 933061.3424901541, 933061.3424901547, 214021.5692119592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2525400.0000, 
sim time next is 2526000.0000, 
raw observation next is [26.26666666666667, 96.33333333333334, 1.0, 2.0, 0.7307054591673569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1021202.330666356, 1021202.330666356, 227613.4451615379], 
processed observation next is [1.0, 0.21739130434782608, 0.44391785150079005, 0.9633333333333334, 1.0, 1.0, 0.6755487459847673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28366731407398776, 0.28366731407398776, 0.3397215599425939], 
reward next is 0.6603, 
noisyNet noise sample is [array([0.90567285], dtype=float32), -0.15667702]. 
=============================================
[2019-03-26 20:30:26,892] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[57.838196]
 [57.84492 ]
 [57.249416]
 [58.355106]
 [58.4242  ]], R is [[57.35048676]
 [57.45754623]
 [57.57165527]
 [57.64567566]
 [57.75822449]].
[2019-03-26 20:30:32,580] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.15480506e-38 9.99988556e-01 5.35304982e-36 1.14570485e-05
 3.38026799e-18], sum to 1.0000
[2019-03-26 20:30:32,585] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8505
[2019-03-26 20:30:32,591] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 89.33333333333334, 1.0, 2.0, 0.5197959559566341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726343.5460024896, 726343.546002489, 186755.3305043713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2585400.0000, 
sim time next is 2586000.0000, 
raw observation next is [26.2, 89.66666666666667, 1.0, 2.0, 0.5173097320172007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722868.206990218, 722868.206990218, 186352.1543742471], 
processed observation next is [1.0, 0.9565217391304348, 0.44075829383886256, 0.8966666666666667, 1.0, 1.0, 0.4184454602616875, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20079672416394945, 0.20079672416394945, 0.27813754384215983], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.09716033], dtype=float32), 0.21020551]. 
=============================================
[2019-03-26 20:30:32,608] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.07055 ]
 [77.1222  ]
 [77.138824]
 [77.285576]
 [77.06857 ]], R is [[77.04355621]
 [76.99438477]
 [76.94525909]
 [76.89638519]
 [76.84757996]].
[2019-03-26 20:30:35,396] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0446095e-09 1.1176408e-25], sum to 1.0000
[2019-03-26 20:30:35,406] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0830
[2019-03-26 20:30:35,417] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.4763915593832231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665672.8520295065, 665672.8520295065, 179979.603333494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2632800.0000, 
sim time next is 2633400.0000, 
raw observation next is [26.5, 81.5, 1.0, 2.0, 0.4773453548638306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667006.0307422256, 667006.0307422249, 180122.5480859272], 
processed observation next is [0.0, 0.4782608695652174, 0.4549763033175356, 0.815, 1.0, 1.0, 0.3702956082696755, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18527945298395154, 0.18527945298395135, 0.2688396240088466], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.59128314], dtype=float32), 1.868034]. 
=============================================
[2019-03-26 20:30:35,759] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.4565417e-38 9.9945241e-01 1.3120956e-33 5.4755574e-04 7.7216337e-17], sum to 1.0000
[2019-03-26 20:30:35,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6228
[2019-03-26 20:30:35,776] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 92.33333333333334, 1.0, 2.0, 0.357656080822618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548574.3854320905, 548574.3854320912, 170458.4324271643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2850000.0000, 
sim time next is 2850600.0000, 
raw observation next is [22.16666666666667, 93.16666666666667, 1.0, 2.0, 0.354584679752377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544869.9315685949, 544869.9315685949, 170179.3055353848], 
processed observation next is [1.0, 1.0, 0.24960505529225935, 0.9316666666666668, 1.0, 1.0, 0.2223911804245506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15135275876905413, 0.15135275876905413, 0.253998963485649], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.6255667], dtype=float32), -0.0877799]. 
=============================================
[2019-03-26 20:30:43,313] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1386233e-10 1.2234079e-24], sum to 1.0000
[2019-03-26 20:30:43,320] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1300
[2019-03-26 20:30:43,325] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.361777469942939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552609.0492098521, 552609.0492098528, 170729.5282432186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3568873499027899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547143.1780567582, 547143.1780567582, 170330.9879352086], 
processed observation next is [0.0, 0.9565217391304348, 0.2417061611374408, 0.95, 1.0, 1.0, 0.22516548181059026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1519842161268773, 0.1519842161268773, 0.25422535512717703], 
reward next is 0.7458, 
noisyNet noise sample is [array([-1.4663645], dtype=float32), -1.4882722]. 
=============================================
[2019-03-26 20:30:44,654] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 4.8850454e-36 1.1656746e-10 3.7884394e-24], sum to 1.0000
[2019-03-26 20:30:44,664] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3743
[2019-03-26 20:30:44,669] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3476887940397863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535605.8759647226, 535605.8759647219, 169456.450300268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2766600.0000, 
sim time next is 2767200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3479686845064236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536037.1310052006, 536037.1310052006, 169491.6814487811], 
processed observation next is [1.0, 0.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21442010181496818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14889920305700016, 0.14889920305700016, 0.2529726588787778], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.46321926], dtype=float32), 0.5519392]. 
=============================================
[2019-03-26 20:30:46,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1510606e-38 1.0000000e+00 3.1573659e-38 5.6415149e-14 5.2900821e-20], sum to 1.0000
[2019-03-26 20:30:46,940] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1095
[2019-03-26 20:30:46,945] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4083411635024524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601657.3465114563, 601657.3465114563, 174467.6989200493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2830200.0000, 
sim time next is 2830800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4102168239075166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 604419.0189070036, 604419.018907003, 174725.3111859522], 
processed observation next is [1.0, 0.782608695652174, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28941786012953813, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1678941719186121, 0.16789417191861195, 0.2607840465461973], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.73988616], dtype=float32), -0.2345333]. 
=============================================
[2019-03-26 20:30:51,611] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4178212e-37 1.0000000e+00 2.7479993e-35 8.0575456e-11 2.6489209e-21], sum to 1.0000
[2019-03-26 20:30:51,617] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4028
[2019-03-26 20:30:51,624] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5967660794159431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 919100.3964346413, 919100.3964346407, 210456.2377780009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2883000.0000, 
sim time next is 2883600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.5847745970982471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900618.5821089275, 900618.5821089275, 208019.128533037], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.94, 1.0, 1.0, 0.4997284302388519, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.250171828363591, 0.250171828363591, 0.3104763112433388], 
reward next is 0.6895, 
noisyNet noise sample is [array([1.1606317], dtype=float32), -0.94011253]. 
=============================================
[2019-03-26 20:30:55,481] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 9.8434325e-37 3.1198402e-10 7.3571706e-20], sum to 1.0000
[2019-03-26 20:30:55,490] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9653
[2019-03-26 20:30:55,496] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3097695162867541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493276.0703199785, 493276.0703199792, 166583.9649269864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3003000.0000, 
sim time next is 3003600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3110172995467023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 495273.0358376002, 495273.0358375995, 166730.9243207221], 
processed observation next is [1.0, 0.782608695652174, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16990036089964133, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13757584328822228, 0.1375758432882221, 0.248852125851824], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.654622], dtype=float32), -1.3526754]. 
=============================================
[2019-03-26 20:31:06,024] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.4321573e-13 3.7128956e-25], sum to 1.0000
[2019-03-26 20:31:06,031] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0029
[2019-03-26 20:31:06,037] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5231463325092446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731026.848608408, 731026.848608408, 187301.7586755215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.5234511477541129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731452.9336891326, 731452.9336891333, 187351.8494333081], 
processed observation next is [0.0, 0.9130434782608695, 0.5181674565560824, 0.8066666666666668, 1.0, 1.0, 0.42584475633025654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20318137046920348, 0.20318137046920368, 0.2796296260198628], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.1726385], dtype=float32), -1.2292923]. 
=============================================
[2019-03-26 20:31:08,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0146527e-16 4.9073923e-25], sum to 1.0000
[2019-03-26 20:31:08,790] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4209
[2019-03-26 20:31:08,798] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4862664657376061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679475.6981089027, 679475.6981089032, 181472.2854978997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3198600.0000, 
sim time next is 3199200.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4864275370605765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679700.8402295926, 679700.840229592, 181496.8613062119], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3812379964585259, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18880578895266462, 0.18880578895266445, 0.2708908377704655], 
reward next is 0.7291, 
noisyNet noise sample is [array([1.0886527], dtype=float32), 0.9028508]. 
=============================================
[2019-03-26 20:31:11,530] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9354549e-09 7.3771555e-19], sum to 1.0000
[2019-03-26 20:31:11,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4076
[2019-03-26 20:31:11,547] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5067806721666409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708150.4100429899, 708150.4100429906, 184666.289243801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [28.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5156798263094226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720589.8685582536, 720589.8685582536, 186090.4566880261], 
processed observation next is [0.0, 0.391304347826087, 0.5576619273301735, 0.7733333333333333, 1.0, 1.0, 0.41648171844508736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20016385237729267, 0.20016385237729267, 0.277746950280636], 
reward next is 0.7223, 
noisyNet noise sample is [array([-1.0713356], dtype=float32), -0.56750596]. 
=============================================
[2019-03-26 20:31:15,207] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 2.7130199e-37 7.7113816e-16 2.0489071e-18], sum to 1.0000
[2019-03-26 20:31:15,218] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6384
[2019-03-26 20:31:15,222] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 83.66666666666667, 1.0, 2.0, 0.4643756041318868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 658046.0924876723, 658046.092487673, 179383.2837849105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3295200.0000, 
sim time next is 3295800.0000, 
raw observation next is [25.5, 83.5, 1.0, 2.0, 0.4553574347421961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650414.1350752919, 650414.1350752924, 178714.4579361897], 
processed observation next is [0.0, 0.13043478260869565, 0.40758293838862564, 0.835, 1.0, 1.0, 0.3438041382436098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18067059307646996, 0.18067059307647013, 0.2667379969196861], 
reward next is 0.7333, 
noisyNet noise sample is [array([-1.4614412], dtype=float32), 1.3849807]. 
=============================================
[2019-03-26 20:31:15,995] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 20:31:15,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:31:15,998] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:31:15,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:31:15,999] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:31:16,001] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:31:16,002] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:31:16,003] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:31:16,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:31:16,005] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:31:16,007] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:31:16,025] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-03-26 20:31:16,045] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-03-26 20:31:16,069] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-03-26 20:31:16,070] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-03-26 20:31:16,107] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-03-26 20:31:49,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:31:49,853] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.9, 70.0, 1.0, 2.0, 0.5196521528473552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763741.928069072, 763741.9280690713, 191448.2179910581]
[2019-03-26 20:31:49,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:31:49,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.04396813e-38 1.00000000e+00 1.16210344e-35 3.41521538e-12
 1.26732124e-20], sampled 0.03470019900645516
[2019-03-26 20:31:56,251] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:31:56,252] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.54335099333334, 92.84954787499998, 1.0, 2.0, 0.3016661707617463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484231.0911991388, 484231.0911991394, 165961.2801494255]
[2019-03-26 20:31:56,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:31:56,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.58233880e-38 1.00000000e+00 9.12451661e-36 3.05500959e-12
 1.07824415e-20], sampled 0.852669745839086
[2019-03-26 20:32:15,537] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:15,539] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.937401545, 85.27178549499999, 1.0, 2.0, 0.6581572844371008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 919768.2352285556, 919768.2352285563, 212082.0334995577]
[2019-03-26 20:32:15,540] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:32:15,543] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 1.7976563e-36 1.4450896e-12 3.6426869e-21], sampled 0.0741453504166093
[2019-03-26 20:32:25,141] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:25,142] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.80289152, 79.19795097, 1.0, 2.0, 0.5564084897820633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777523.2642259982, 777523.2642259988, 192905.2753874115]
[2019-03-26 20:32:25,144] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:32:25,147] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 5.8504014e-37 8.6144378e-13 1.7208410e-21], sampled 0.276865604045765
[2019-03-26 20:32:35,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:35,805] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.23333333333333, 60.66666666666667, 1.0, 2.0, 0.7561873666481005, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005978792033775, 6.9112, 168.9123160237241, 1953757.723058299, 1886518.640914405, 397330.3072003644]
[2019-03-26 20:32:35,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:32:35,809] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3730356e-26 9.9999917e-01 5.1872728e-24 7.9796013e-07 7.6777490e-13], sampled 0.05895840101516414
[2019-03-26 20:32:35,811] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1953757.723058299 W.
[2019-03-26 20:32:42,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:42,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.31666666666667, 87.66666666666667, 1.0, 2.0, 0.5438795897082672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760009.1599010023, 760009.1599010017, 190756.434037076]
[2019-03-26 20:32:42,902] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:32:42,905] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 5.3114297e-37 8.2391505e-13 1.6132354e-21], sampled 0.7297481654265456
[2019-03-26 20:32:42,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:42,923] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.36666666666667, 63.0, 1.0, 2.0, 0.897028045164024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1253784.812861649, 1253784.81286165, 269123.9397086793]
[2019-03-26 20:32:42,925] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:32:42,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7699019e-35 1.0000000e+00 6.9275306e-33 6.4920659e-11 9.0550728e-19], sampled 0.05347520577721265
[2019-03-26 20:32:59,625] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.16302074]
[2019-03-26 20:32:59,627] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.56666666666667, 85.0, 1.0, 2.0, 0.7670448436310895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1072014.316463863, 1072014.316463863, 235985.0782339207]
[2019-03-26 20:32:59,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:32:59,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.00147484e-35 1.00000000e+00 4.04498773e-33 5.06646149e-11
 6.32121860e-19], sampled 0.8924944532027144
[2019-03-26 20:33:09,917] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8252.3198 2928314360.4977 1338.0000
[2019-03-26 20:33:10,364] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8655.4327 2780159343.4963 933.0000
[2019-03-26 20:33:10,475] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7995.6793 3008572576.9851 1766.0000
[2019-03-26 20:33:10,573] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164930392.4171 1778.0000
[2019-03-26 20:33:10,574] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8491.5809 2843444954.4583 1131.0000
[2019-03-26 20:33:11,589] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 725000, evaluation results [725000.0, 7874.254934002473, 3164930392.4170628, 1778.0, 8252.319848496612, 2928314360.4977336, 1338.0, 8655.432663684038, 2780159343.4962626, 933.0, 7995.679306472705, 3008572576.985058, 1766.0, 8491.580877992481, 2843444954.4583273, 1131.0]
[2019-03-26 20:33:13,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.917146e-15 9.991842e-23], sum to 1.0000
[2019-03-26 20:33:13,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8724
[2019-03-26 20:33:13,145] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5741437883975836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 802315.8680732055, 802315.8680732049, 196028.4719071925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3330000.0000, 
sim time next is 3330600.0000, 
raw observation next is [32.0, 67.66666666666667, 1.0, 2.0, 0.5811754934602593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812145.819240161, 812145.819240161, 197291.8184000549], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.6766666666666667, 1.0, 1.0, 0.49539216079549314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22559606090004472, 0.22559606090004472, 0.29446540059709686], 
reward next is 0.7055, 
noisyNet noise sample is [array([-0.73587644], dtype=float32), -2.3915865]. 
=============================================
[2019-03-26 20:33:25,126] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.0145724e-17 4.9095237e-01 1.3695193e-15 5.0904757e-01 3.3915915e-11], sum to 1.0000
[2019-03-26 20:33:25,134] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5395
[2019-03-26 20:33:25,141] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7748621066668048, 1.0, 2.0, 0.7748621066668048, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2166974.648026465, 2166974.648026465, 407816.0483993409], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3576000.0000, 
sim time next is 3576600.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.7752677914356922, 1.0, 2.0, 0.7752677914356922, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2168110.331787534, 2168110.331787534, 408007.896501869], 
processed observation next is [1.0, 0.391304347826087, 0.6445497630331753, 0.68, 1.0, 1.0, 0.7292383029345689, 1.0, 1.0, 0.7292383029345689, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6022528699409817, 0.6022528699409817, 0.6089670097042821], 
reward next is 0.3910, 
noisyNet noise sample is [array([-0.34414837], dtype=float32), -0.41457218]. 
=============================================
[2019-03-26 20:33:30,042] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 3.9712451e-34 2.4845633e-08 7.3470308e-24], sum to 1.0000
[2019-03-26 20:33:30,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7896
[2019-03-26 20:33:30,054] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.535962459009392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748941.9802847204, 748941.9802847204, 189422.5201043372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3822000.0000, 
sim time next is 3822600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5358341797314213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748762.6625055039, 748762.6625055044, 189401.0652399772], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4407640719655678, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20798962847375108, 0.20798962847375124, 0.2826881570745928], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.83930093], dtype=float32), -0.864343]. 
=============================================
[2019-03-26 20:33:44,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9287469e-10 7.7293293e-28], sum to 1.0000
[2019-03-26 20:33:44,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7509
[2019-03-26 20:33:44,410] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5433314895713639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759242.9792850231, 759242.9792850225, 190663.0707257695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3874200.0000, 
sim time next is 3874800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.538117117563818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751953.9177811923, 751953.9177811918, 189783.508443075], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.44351459947447947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20887608827255344, 0.20887608827255327, 0.2832589678254851], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.8956141], dtype=float32), 0.42868727]. 
=============================================
[2019-03-26 20:33:48,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1933633e-13 1.2254325e-28], sum to 1.0000
[2019-03-26 20:33:48,158] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0303
[2019-03-26 20:33:48,164] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5945514491222607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830844.9477774323, 830844.9477774329, 199736.0541405589], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3937800.0000, 
sim time next is 3938400.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5942153171896634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830375.0427994833, 830375.0427994839, 199674.0139918176], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5111027917947751, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23065973411096757, 0.23065973411096774, 0.2980209164056979], 
reward next is 0.7020, 
noisyNet noise sample is [array([-0.5910578], dtype=float32), 0.5016465]. 
=============================================
[2019-03-26 20:33:56,372] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5576127e-10 5.6030627e-28], sum to 1.0000
[2019-03-26 20:33:56,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1784
[2019-03-26 20:33:56,383] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 86.0, 1.0, 2.0, 0.5427630285483152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758448.3370883181, 758448.3370883187, 190567.3412119676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4068000.0000, 
sim time next is 4068600.0000, 
raw observation next is [27.56666666666667, 86.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.578196710368428, 6.9112, 169.2900105806816, 1928323.497414847, 1454076.800761533, 311420.3639341666], 
processed observation next is [1.0, 0.08695652173913043, 0.505529225908373, 0.8616666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.06669967103684282, 0.0, 0.8312914532533334, 0.5356454159485686, 0.4039102224337592, 0.464806513334577], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.83268464], dtype=float32), -0.89564]. 
=============================================
[2019-03-26 20:33:56,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.6882412e-18 1.7248140e-32], sum to 1.0000
[2019-03-26 20:33:56,664] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0233
[2019-03-26 20:33:56,669] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5419617560366935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757328.2525986155, 757328.2525986155, 190432.0600581613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4060800.0000, 
sim time next is 4061400.0000, 
raw observation next is [27.96666666666667, 84.16666666666667, 1.0, 2.0, 0.5423837146603072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757918.1008522207, 757918.1008522207, 190503.3959678632], 
processed observation next is [1.0, 0.0, 0.524486571879937, 0.8416666666666667, 1.0, 1.0, 0.44865507790398457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2105328057922835, 0.2105328057922835, 0.2843334268177063], 
reward next is 0.7157, 
noisyNet noise sample is [array([-2.825419], dtype=float32), 0.69780374]. 
=============================================
[2019-03-26 20:33:57,212] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.7672759e-36 1.0000000e+00 7.5251749e-36 4.4676819e-12 3.7126407e-25], sum to 1.0000
[2019-03-26 20:33:57,223] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9808
[2019-03-26 20:33:57,228] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.46666666666667, 86.66666666666667, 1.0, 2.0, 0.8849170889881192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103971, 1236847.351900825, 1236847.351900826, 265822.8258896743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4070400.0000, 
sim time next is 4071000.0000, 
raw observation next is [27.43333333333333, 86.83333333333333, 1.0, 2.0, 0.8560973863711036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1196543.397142065, 1196543.397142065, 258146.9177549946], 
processed observation next is [1.0, 0.08695652173913043, 0.49921011058451803, 0.8683333333333333, 1.0, 1.0, 0.8266233570736188, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3323731658727958, 0.3323731658727958, 0.38529390709700684], 
reward next is 0.6147, 
noisyNet noise sample is [array([-0.02168047], dtype=float32), 0.50335956]. 
=============================================
[2019-03-26 20:33:57,240] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[62.917248]
 [61.615158]
 [61.69639 ]
 [64.213036]
 [70.2505  ]], R is [[63.87092972]
 [63.83546829]
 [63.19711304]
 [63.06245804]
 [62.43183517]].
[2019-03-26 20:34:07,374] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 20:34:07,375] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:34:07,376] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:34:07,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:34:07,379] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:34:07,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:34:07,379] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:34:07,383] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:34:07,383] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:34:07,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:34:07,388] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:34:07,401] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-03-26 20:34:07,402] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-03-26 20:34:07,441] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-03-26 20:34:07,443] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-03-26 20:34:07,443] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-03-26 20:34:40,138] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:34:40,139] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.684780705, 89.60645611999999, 1.0, 2.0, 0.7063544972634889, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.981176340495336, 6.9112, 168.9124876509422, 1884021.022849922, 1834377.53630086, 386923.2441751388]
[2019-03-26 20:34:40,140] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:34:40,144] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.9443243e-26 1.0000000e+00 9.3945846e-26 3.5757175e-08 8.2448445e-16], sampled 0.6574259195823239
[2019-03-26 20:34:40,145] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1884021.022849922 W.
[2019-03-26 20:35:23,222] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:35:23,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.15, 57.5, 1.0, 2.0, 0.5751589935797301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104099, 803735.065517487, 803735.0655174864, 196212.7232852759]
[2019-03-26 20:35:23,226] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:35:23,228] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5001521e-37 1.0000000e+00 1.5281490e-37 2.9818535e-14 1.0183580e-24], sampled 0.7541159879538917
[2019-03-26 20:35:29,537] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:35:29,538] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.059676305, 70.00078073666667, 1.0, 2.0, 0.6338325888724314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 885760.5407060952, 885760.5407060952, 207219.216113188]
[2019-03-26 20:35:29,538] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:35:29,540] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8983130e-15 2.3459756e-26], sampled 0.5846090508433922
[2019-03-26 20:35:31,232] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:35:31,236] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.57064417, 81.20572438333333, 1.0, 2.0, 0.591078782767124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825990.2473444166, 825990.2473444166, 199092.7277489866]
[2019-03-26 20:35:31,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:35:31,239] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7389153e-35 1.0000000e+00 1.7456955e-35 2.3408955e-13 2.7970897e-23], sampled 0.6808764171514142
[2019-03-26 20:35:39,054] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:35:39,054] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 90.33333333333334, 1.0, 2.0, 0.7192550265602135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1005192.127894601, 1005192.127894601, 225056.6899283132]
[2019-03-26 20:35:39,055] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:35:39,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.0708952e-34 1.0000000e+00 2.0325891e-34 7.5888568e-13 1.6899267e-22], sampled 0.5862533840590158
[2019-03-26 20:35:42,475] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.006304911]
[2019-03-26 20:35:42,476] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.8, 68.0, 1.0, 2.0, 0.7691874092931011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1075010.260832704, 1075010.260832704, 236489.1273032969]
[2019-03-26 20:35:42,479] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:35:42,484] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8748735e-35 1.0000000e+00 1.8336093e-35 2.3958277e-13 2.8989868e-23], sampled 0.23958200103418936
[2019-03-26 20:36:01,223] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164933419.7733 1778.0000
[2019-03-26 20:36:01,362] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.3222 3008558204.4413 1766.0000
[2019-03-26 20:36:01,717] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.0586 2928241882.8146 1338.0000
[2019-03-26 20:36:01,848] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.0742 2780141322.3502 933.0000
[2019-03-26 20:36:01,995] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.9778 2843378227.1976 1131.0000
[2019-03-26 20:36:03,010] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 750000, evaluation results [750000.0, 7874.254934002473, 3164933419.7732573, 1778.0, 8253.05861172702, 2928241882.8145804, 1338.0, 8656.074218006363, 2780141322.350202, 933.0, 7996.322233343432, 3008558204.4412923, 1766.0, 8492.97779728114, 2843378227.1976113, 1131.0]
[2019-03-26 20:36:04,020] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7821369e-24 1.0000000e+00 2.8149197e-25 1.5551944e-08 1.9855157e-15], sum to 1.0000
[2019-03-26 20:36:04,027] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8600
[2019-03-26 20:36:04,032] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.941671190300175, 6.9112, 168.912485234512, 1475386.983507744, 1453769.732684788, 311352.5252626254], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4248000.0000, 
sim time next is 4248600.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.9893153650480473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129316994846, 1382859.705618216, 1382859.705618216, 295689.3685540853], 
processed observation next is [1.0, 0.17391304347826086, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.9871269458410208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294398233191974, 0.38412769600505997, 0.38412769600505997, 0.44132741575236617], 
reward next is 0.5587, 
noisyNet noise sample is [array([0.39667025], dtype=float32), 1.596591]. 
=============================================
[2019-03-26 20:36:13,637] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4698369e-22 1.0171801e-27], sum to 1.0000
[2019-03-26 20:36:13,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7253
[2019-03-26 20:36:13,650] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.581932232265512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 813203.7054230262, 813203.7054230255, 197428.1532226166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4423200.0000, 
sim time next is 4423800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5809484113354244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811828.3690380023, 811828.3690380023, 197250.3394906312], 
processed observation next is [0.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49511856787400527, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.225507880288334, 0.225507880288334, 0.2944034917770615], 
reward next is 0.7056, 
noisyNet noise sample is [array([0.31561837], dtype=float32), 0.083558746]. 
=============================================
[2019-03-26 20:36:16,399] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.037321e-21 4.774384e-26], sum to 1.0000
[2019-03-26 20:36:16,407] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1933
[2019-03-26 20:36:16,412] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6465546392236101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903546.7570592399, 903546.7570592399, 209737.4679518104], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4468200.0000, 
sim time next is 4468800.0000, 
raw observation next is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6069456993342348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848171.9795230635, 848171.9795230641, 202046.9249382585], 
processed observation next is [0.0, 0.7391304347826086, 0.6840442338072673, 0.7366666666666667, 1.0, 1.0, 0.5264406016075118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23560332764529543, 0.2356033276452956, 0.3015625745347142], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.61884063], dtype=float32), -0.85573703]. 
=============================================
[2019-03-26 20:36:16,633] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9233292e-23 1.5172351e-29], sum to 1.0000
[2019-03-26 20:36:16,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6710
[2019-03-26 20:36:16,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 79.0, 1.0, 2.0, 0.5847947394891898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 817205.3689186447, 817205.368918644, 197946.8030978707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4472400.0000, 
sim time next is 4473000.0000, 
raw observation next is [29.5, 79.0, 1.0, 2.0, 0.5792908206771125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809511.1376075802, 809511.1376075802, 196950.6193577511], 
processed observation next is [0.0, 0.782608695652174, 0.5971563981042655, 0.79, 1.0, 1.0, 0.4931214706953162, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22486420489099448, 0.22486420489099448, 0.2939561482951509], 
reward next is 0.7060, 
noisyNet noise sample is [array([-2.1428394], dtype=float32), 0.34559634]. 
=============================================
[2019-03-26 20:36:16,679] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[79.5889  ]
 [79.52941 ]
 [79.559044]
 [79.461555]
 [79.38064 ]], R is [[79.57472229]
 [79.48353577]
 [79.39080811]
 [79.29989624]
 [79.20927429]].
[2019-03-26 20:36:23,467] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5344701e-18 9.9992967e-01 1.7237653e-17 7.0293536e-05 8.7889873e-09], sum to 1.0000
[2019-03-26 20:36:23,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7052
[2019-03-26 20:36:23,488] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 89.0, 1.0, 2.0, 0.3798637457501445, 1.0, 1.0, 0.3798637457501445, 1.0, 2.0, 0.6584836232869392, 6.9112, 6.9112, 170.5573041426782, 1593060.612078284, 1593060.612078284, 341863.0557029924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4591800.0000, 
sim time next is 4592400.0000, 
raw observation next is [27.33333333333334, 90.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.345719164666725, 6.9112, 168.9104513219598, 1762224.335926823, 1453966.057436853, 311352.0513997599], 
processed observation next is [1.0, 0.13043478260869565, 0.4944707740916275, 0.9066666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.04345191646667246, 0.0, 0.8294276435300288, 0.4895067599796731, 0.4038794603991258, 0.46470455432799984], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8848889], dtype=float32), -0.15587574]. 
=============================================
[2019-03-26 20:36:25,494] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0663469e-15 9.9693310e-01 9.9571747e-15 3.0664958e-03 3.6136660e-07], sum to 1.0000
[2019-03-26 20:36:25,506] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0823
[2019-03-26 20:36:25,514] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3081681.975412983 W.
[2019-03-26 20:36:25,522] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 61.5, 1.0, 2.0, 0.827533177827652, 1.0, 2.0, 0.7343566284280885, 1.0, 1.0, 1.03, 7.005107790183965, 6.9112, 170.5573041426782, 3081681.975412983, 3014412.005379535, 564637.0521225533], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4627800.0000, 
sim time next is 4628400.0000, 
raw observation next is [34.66666666666666, 61.0, 1.0, 2.0, 0.8089117751147976, 1.0, 2.0, 0.7250459270716615, 1.0, 2.0, 1.03, 7.005106321320092, 6.9112, 170.5573041426782, 3042562.60436552, 2975293.686539013, 557936.1524309991], 
processed observation next is [1.0, 0.5652173913043478, 0.842022116903633, 0.61, 1.0, 1.0, 0.7697732230298766, 1.0, 1.0, 0.6687300326164596, 1.0, 1.0, 1.0365853658536586, 0.009390632132009191, 0.0, 0.8375144448122397, 0.8451562789904222, 0.8264704684830592, 0.8327405260164166], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5683492], dtype=float32), -0.018730747]. 
=============================================
[2019-03-26 20:36:29,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5896230e-21 9.9993205e-01 5.1452276e-23 6.7951056e-05 5.6316046e-11], sum to 1.0000
[2019-03-26 20:36:29,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1965
[2019-03-26 20:36:29,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2126197.823889785 W.
[2019-03-26 20:36:29,635] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 66.0, 1.0, 2.0, 0.8793965900368647, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.988208072342857, 6.9112, 168.9124981871528, 2126197.823889785, 2071565.795193747, 429060.203672104], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4897200.0000, 
sim time next is 4897800.0000, 
raw observation next is [30.5, 66.0, 1.0, 2.0, 0.5111445254344309, 1.0, 1.0, 0.5111445254344309, 1.0, 2.0, 0.8787583528977609, 6.9112, 6.9112, 170.5573041426782, 2144172.701013858, 2144172.701013858, 421279.0677533136], 
processed observation next is [1.0, 0.6956521739130435, 0.6445497630331753, 0.66, 1.0, 1.0, 0.4110175005234107, 1.0, 0.5, 0.4110175005234107, 1.0, 1.0, 0.8521443328021475, 0.0, 0.0, 0.8375144448122397, 0.595603528059405, 0.595603528059405, 0.6287747279900202], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28517815], dtype=float32), 0.6983194]. 
=============================================
[2019-03-26 20:36:46,191] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.1227617e-23 9.1653234e-01 1.3455745e-24 8.3467655e-02 4.8594761e-12], sum to 1.0000
[2019-03-26 20:36:46,203] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8867
[2019-03-26 20:36:46,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3026862.124979978 W.
[2019-03-26 20:36:46,219] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.46666666666667, 64.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.127373732079436, 6.9112, 168.9002453770475, 3026862.124979978, 1454750.518965166, 308939.7945660483], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4972800.0000, 
sim time next is 4973400.0000, 
raw observation next is [30.5, 64.5, 1.0, 2.0, 0.544728268978881, 1.0, 1.0, 0.544728268978881, 1.0, 1.0, 0.932928121128385, 6.9112, 6.9112, 170.5573041426782, 2285190.566983133, 2285190.566983133, 444837.3018803612], 
processed observation next is [1.0, 0.5652173913043478, 0.6445497630331753, 0.645, 1.0, 1.0, 0.45147984214323017, 1.0, 0.5, 0.45147984214323017, 1.0, 0.5, 0.918205025766323, 0.0, 0.0, 0.8375144448122397, 0.6347751574953148, 0.6347751574953148, 0.6639362714632258], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7250722], dtype=float32), -1.2740518]. 
=============================================
[2019-03-26 20:36:47,669] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4266465e-26 9.9997020e-01 8.2762571e-26 2.9810761e-05 1.1767930e-15], sum to 1.0000
[2019-03-26 20:36:47,681] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8359
[2019-03-26 20:36:47,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1999307.263592263 W.
[2019-03-26 20:36:47,696] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.9, 63.0, 1.0, 2.0, 0.7887346647977563, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98068017119267, 6.9112, 168.9125478554993, 1999307.263592263, 1950015.758077584, 405975.9913264529], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4980600.0000, 
sim time next is 4981200.0000, 
raw observation next is [30.93333333333333, 63.0, 1.0, 2.0, 0.7521756401855689, 1.0, 1.0, 0.7521756401855689, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2103467.520964051, 2103467.520964051, 397255.0737021636], 
processed observation next is [1.0, 0.6521739130434783, 0.6650868878357029, 0.63, 1.0, 1.0, 0.7014164339585167, 1.0, 0.5, 0.7014164339585167, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5842965336011253, 0.5842965336011253, 0.5929180204509904], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.40005228], dtype=float32), 0.12835182]. 
=============================================
[2019-03-26 20:36:48,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0873482e-08 1.3394777e-27], sum to 1.0000
[2019-03-26 20:36:48,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5329
[2019-03-26 20:36:48,642] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4808737448191366, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672062.9188923375, 672062.9188923368, 180669.213913349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5110800.0000, 
sim time next is 5111400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.482364311720772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674146.5682088706, 674146.5682088699, 180894.5739038551], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37634254424189395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18726293561357515, 0.18726293561357496, 0.2699919013490375], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.9035144], dtype=float32), -1.1227944]. 
=============================================
[2019-03-26 20:36:50,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4465649e-12 1.9543168e-29], sum to 1.0000
[2019-03-26 20:36:50,418] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7327
[2019-03-26 20:36:50,423] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.508664754023036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710784.0142643537, 710784.0142643544, 184964.7932893815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5035800.0000, 
sim time next is 5036400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5095876984073302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712074.1252579205, 712074.1252579205, 185111.9381594009], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40914180531003635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19779836812720014, 0.19779836812720014, 0.2762864748647774], 
reward next is 0.7237, 
noisyNet noise sample is [array([1.7464997], dtype=float32), -1.9547712]. 
=============================================
[2019-03-26 20:36:58,707] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 20:36:58,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:36:58,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:36:58,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:36:58,711] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:36:58,712] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:36:58,714] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:36:58,712] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:36:58,715] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:36:58,716] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:36:58,719] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:36:58,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-03-26 20:36:58,730] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-03-26 20:36:58,771] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-03-26 20:36:58,803] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-03-26 20:36:58,830] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-03-26 20:37:16,177] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:37:16,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.6, 61.0, 1.0, 2.0, 0.9160060260170056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1385697.653478942, 1385697.653478942, 289429.0482113513]
[2019-03-26 20:37:16,180] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:37:16,184] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0604208e-33 1.0000000e+00 4.0815189e-34 6.2254063e-10 8.2742198e-21], sampled 0.6472919258879286
[2019-03-26 20:37:16,188] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:37:16,189] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.73333333333333, 77.5, 1.0, 2.0, 0.2575727465057806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424167.4905855851, 424167.4905855851, 161651.7146285443]
[2019-03-26 20:37:16,190] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:37:16,192] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0598890e-12 4.0534977e-26], sampled 0.48437161123381656
[2019-03-26 20:37:28,923] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:37:28,926] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.99555461, 94.09787357, 1.0, 2.0, 0.648792314510923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 906675.1968600744, 906675.1968600738, 210190.7301429741]
[2019-03-26 20:37:28,927] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:37:28,930] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4472902e-12 3.8913781e-25], sampled 0.11127486800248498
[2019-03-26 20:37:58,434] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:37:58,437] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 56.66666666666667, 1.0, 2.0, 0.9341456396989677, 1.0, 2.0, 0.7876628593637466, 1.0, 1.0, 1.03, 7.005116201589427, 6.9112, 170.5573041426782, 3305674.328041233, 3238398.332575899, 605492.4773824506]
[2019-03-26 20:37:58,437] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:37:58,441] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1031305e-20 9.9989820e-01 2.5239032e-20 1.0183239e-04 1.2256455e-11], sampled 0.41459965997979553
[2019-03-26 20:37:58,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3305674.328041233 W.
[2019-03-26 20:38:42,591] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:38:42,593] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.67908108166667, 90.35661915666667, 1.0, 2.0, 0.6158719519537789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 882451.9984719885, 882451.9984719879, 206558.2300444112]
[2019-03-26 20:38:42,594] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:38:42,595] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.7890908e-38 1.0000000e+00 1.4825057e-38 2.0657878e-11 1.2058987e-23], sampled 0.8341365282740781
[2019-03-26 20:38:49,139] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.046917412]
[2019-03-26 20:38:49,142] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.76906323, 92.34299742666666, 1.0, 2.0, 0.4285675626270312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627385.3517349544, 627385.3517349544, 176806.2468638065]
[2019-03-26 20:38:49,143] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:38:49,146] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3788198e-12 1.2665550e-24], sampled 0.3060623891923048
[2019-03-26 20:38:52,883] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7003 2928231904.9171 1338.0000
[2019-03-26 20:38:53,117] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.1120 2780070553.5627 933.0000
[2019-03-26 20:38:53,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.5014 3164995964.7091 1778.0000
[2019-03-26 20:38:53,483] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 20:38:53,491] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5068 2843430346.4143 1131.0000
[2019-03-26 20:38:54,507] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 775000, evaluation results [775000.0, 7873.501410909826, 3164995964.709079, 1778.0, 8253.700304448676, 2928231904.9170933, 1338.0, 8658.112016599982, 2780070553.5627275, 933.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8493.50677167813, 2843430346.4142575, 1131.0]
[2019-03-26 20:38:56,200] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7293809e-26 1.0000000e+00 1.7051221e-25 5.9014702e-08 1.3835204e-14], sum to 1.0000
[2019-03-26 20:38:56,210] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9035
[2019-03-26 20:38:56,221] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2962863.311400098 W.
[2019-03-26 20:38:56,227] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.037198697067522, 6.9112, 168.9010567260096, 2962863.311400098, 1454712.919962238, 309179.71382418], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5214000.0000, 
sim time next is 5214600.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.5735630686525022, 1.0, 1.0, 0.5735630686525022, 1.0, 1.0, 0.9874404293735684, 6.9112, 6.9112, 170.5573041426782, 2406271.948833141, 2406271.948833141, 467919.6483129376], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.72, 1.0, 1.0, 0.4862205646415689, 1.0, 0.5, 0.4862205646415689, 1.0, 0.5, 0.9846834504555712, 0.0, 0.0, 0.8375144448122397, 0.6684088746758725, 0.6684088746758725, 0.6983875347954293], 
reward next is 0.3016, 
noisyNet noise sample is [array([-1.4588182], dtype=float32), -1.3366807]. 
=============================================
[2019-03-26 20:39:00,020] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8521822e-28 1.0000000e+00 1.3230531e-29 1.2551768e-10 4.9616382e-19], sum to 1.0000
[2019-03-26 20:39:00,027] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3734
[2019-03-26 20:39:00,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1924132.619249038 W.
[2019-03-26 20:39:00,046] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.05, 88.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.573792740115749, 6.9112, 168.9092278978081, 1924132.619249038, 1454076.901045732, 311355.8992541993], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5367000.0000, 
sim time next is 5367600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 0.6739526526938603, 1.0, 1.0, 0.6739526526938603, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1884523.669703183, 1884523.669703183, 363185.9905056847], 
processed observation next is [1.0, 0.13043478260869565, 0.5734597156398105, 0.89, 1.0, 1.0, 0.6071718707154943, 1.0, 0.5, 0.6071718707154943, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5234787971397731, 0.5234787971397731, 0.542068642545798], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2656909], dtype=float32), -0.23561749]. 
=============================================
[2019-03-26 20:39:04,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4758604e-33 1.0000000e+00 4.7034047e-37 2.7114108e-11 2.1026873e-25], sum to 1.0000
[2019-03-26 20:39:04,227] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9465
[2019-03-26 20:39:04,233] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 78.33333333333334, 1.0, 2.0, 0.6295074414243633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879713.7838530749, 879713.7838530749, 206372.6021989454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5343600.0000, 
sim time next is 5344200.0000, 
raw observation next is [31.25, 78.5, 1.0, 2.0, 0.6276846513960663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 877165.4491132319, 877165.4491132324, 206017.5566451002], 
processed observation next is [1.0, 0.8695652173913043, 0.6800947867298578, 0.785, 1.0, 1.0, 0.5514272908386341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24365706919811997, 0.2436570691981201, 0.30748889051507494], 
reward next is 0.6925, 
noisyNet noise sample is [array([0.5762104], dtype=float32), 0.31620234]. 
=============================================
[2019-03-26 20:39:06,090] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.2984279e-13 9.9713755e-01 6.0559999e-13 2.8623610e-03 1.4619381e-07], sum to 1.0000
[2019-03-26 20:39:06,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5315
[2019-03-26 20:39:06,107] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3201373.673354237 W.
[2019-03-26 20:39:06,113] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.46666666666667, 62.66666666666667, 1.0, 2.0, 0.8845045847171044, 1.0, 2.0, 0.7628423318728148, 1.0, 2.0, 1.03, 7.005112284682999, 6.9112, 170.5573041426782, 3201373.673354237, 3134100.483728306, 585941.820093168], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5394000.0000, 
sim time next is 5394600.0000, 
raw observation next is [34.65000000000001, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.958188075379613, 6.9112, 170.5573041426782, 3660203.603161911, 2910203.456015544, 547576.9240166047], 
processed observation next is [1.0, 0.43478260869565216, 0.8412322274881523, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.10469880753796126, 0.0, 0.8375144448122397, 1.0167232231005308, 0.8083898488932066, 0.8172789910695593], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.75622094], dtype=float32), 2.6418412]. 
=============================================
[2019-03-26 20:39:06,817] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6156769e-14 9.9009621e-01 2.6565992e-14 9.9037569e-03 8.4982005e-10], sum to 1.0000
[2019-03-26 20:39:06,826] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8664
[2019-03-26 20:39:06,833] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2293373.086271169 W.
[2019-03-26 20:39:06,840] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.3, 66.66666666666667, 1.0, 2.0, 0.8200154618118384, 1.0, 2.0, 0.8200154618118384, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2293373.086271169, 2293373.086271169, 429746.2227219695], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5390400.0000, 
sim time next is 5391000.0000, 
raw observation next is [33.5, 66.0, 1.0, 2.0, 0.5515430469027414, 1.0, 2.0, 0.5515430469027414, 1.0, 1.0, 0.9578478743972452, 6.9112, 6.9112, 170.5573041426782, 2313805.719899366, 2313805.719899366, 452620.8014772006], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.66, 1.0, 1.0, 0.45969041795511006, 1.0, 1.0, 0.45969041795511006, 1.0, 0.5, 0.9485949687771282, 0.0, 0.0, 0.8375144448122397, 0.6427238110831572, 0.6427238110831572, 0.6755534350405978], 
reward next is 0.3244, 
noisyNet noise sample is [array([-0.03987825], dtype=float32), -1.7228857]. 
=============================================
[2019-03-26 20:39:06,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[24.487492]
 [22.756348]
 [20.724258]
 [18.922405]
 [18.696785]], R is [[23.79811859]
 [23.91872406]
 [24.01781845]
 [24.02646637]
 [23.97432899]].
[2019-03-26 20:39:09,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.151180e-21 4.044534e-36], sum to 1.0000
[2019-03-26 20:39:09,929] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3960
[2019-03-26 20:39:09,933] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333334, 72.66666666666667, 1.0, 2.0, 0.541256075815571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756341.7958374814, 756341.795837482, 190312.8282231646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5649600.0000, 
sim time next is 5650200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.5419428748475508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757301.8589368007, 757301.8589368014, 190428.9813475051], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.72, 1.0, 1.0, 0.44812394559945873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21036162748244466, 0.21036162748244483, 0.28422236022015684], 
reward next is 0.7158, 
noisyNet noise sample is [array([-0.5633726], dtype=float32), 1.684738]. 
=============================================
[2019-03-26 20:39:13,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0015041e-09 4.6489509e-03 1.8647355e-09 9.9534988e-01 1.1838966e-06], sum to 1.0000
[2019-03-26 20:39:13,205] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3957
[2019-03-26 20:39:13,209] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.7, 58.0, 1.0, 2.0, 0.8277875674345669, 1.0, 2.0, 0.7344838232315462, 1.0, 1.0, 1.03, 7.005107810250957, 6.9112, 170.5573041426782, 3082216.39913812, 3014946.41472987, 564729.9197506675], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5486400.0000, 
sim time next is 5487000.0000, 
raw observation next is [35.8, 57.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.367246097955717, 6.9112, 170.5573041426782, 3236394.635664412, 2909710.26492953, 551208.4113800331], 
processed observation next is [1.0, 0.5217391304347826, 0.895734597156398, 0.57, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.0456046097955717, 0.0, 0.8375144448122397, 0.8989985099067811, 0.8082528513693139, 0.822699121462736], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.66669554], dtype=float32), 0.21379383]. 
=============================================
[2019-03-26 20:39:13,223] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[16.799051]
 [17.118488]
 [17.470581]
 [18.555967]
 [17.409782]], R is [[16.98807716]
 [16.81819725]
 [16.65001488]
 [16.48351479]
 [16.31867981]].
[2019-03-26 20:39:18,821] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.109252e-19 0.000000e+00], sum to 1.0000
[2019-03-26 20:39:18,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5026
[2019-03-26 20:39:18,840] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.0, 1.0, 2.0, 0.5512923579204319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770371.4070233355, 770371.4070233355, 192022.4183767498], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5772000.0000, 
sim time next is 5772600.0000, 
raw observation next is [28.8, 79.0, 1.0, 2.0, 0.5495153097932858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767887.2769922784, 767887.276992279, 191717.473560603], 
processed observation next is [0.0, 0.8260869565217391, 0.5639810426540285, 0.79, 1.0, 1.0, 0.4572473611967298, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.213302021386744, 0.21330202138674417, 0.2861454829262731], 
reward next is 0.7139, 
noisyNet noise sample is [array([0.01535737], dtype=float32), -0.53578347]. 
=============================================
[2019-03-26 20:39:21,304] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3549806e-09 3.7404311e-33], sum to 1.0000
[2019-03-26 20:39:21,316] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0223
[2019-03-26 20:39:21,323] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 91.66666666666666, 1.0, 2.0, 0.522220160378864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 187150.3899612127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5618400.0000, 
sim time next is 5619000.0000, 
raw observation next is [26.05, 91.83333333333333, 1.0, 2.0, 0.5207622642880092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727694.291853428, 727694.291853428, 186912.7021412179], 
processed observation next is [0.0, 0.0, 0.43364928909952616, 0.9183333333333333, 1.0, 1.0, 0.4226051376963966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2021373032926189, 0.2021373032926189, 0.2789741823003252], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.36824328], dtype=float32), 0.109466426]. 
=============================================
[2019-03-26 20:39:21,345] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.6599  ]
 [68.67521 ]
 [68.57017 ]
 [68.373375]
 [67.819275]], R is [[68.66692352]
 [68.70092773]
 [68.73456573]
 [68.76774597]
 [68.80023193]].
[2019-03-26 20:39:27,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0592222e-11 0.0000000e+00], sum to 1.0000
[2019-03-26 20:39:27,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5258
[2019-03-26 20:39:27,580] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.63333333333333, 75.83333333333333, 1.0, 2.0, 0.5236745060107719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731765.1544703929, 731765.1544703934, 187388.4692998539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5730600.0000, 
sim time next is 5731200.0000, 
raw observation next is [28.8, 75.0, 1.0, 2.0, 0.5239357236267489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732130.2969789476, 732130.2969789482, 187431.2950984177], 
processed observation next is [0.0, 0.34782608695652173, 0.5639810426540285, 0.75, 1.0, 1.0, 0.42642858268282996, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20336952693859656, 0.20336952693859672, 0.27974820163942943], 
reward next is 0.7203, 
noisyNet noise sample is [array([1.5418705], dtype=float32), 0.030183814]. 
=============================================
[2019-03-26 20:39:29,717] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.683174e-13 4.771431e-35], sum to 1.0000
[2019-03-26 20:39:29,726] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8144
[2019-03-26 20:39:29,730] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 82.66666666666667, 1.0, 2.0, 0.5468786600017321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764201.5231817621, 764201.5231817621, 191266.5657343108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5775600.0000, 
sim time next is 5776200.0000, 
raw observation next is [28.05, 83.0, 1.0, 2.0, 0.546436695655818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763583.7056478023, 763583.7056478023, 191191.241292817], 
processed observation next is [0.0, 0.8695652173913043, 0.528436018957346, 0.83, 1.0, 1.0, 0.4535381875371301, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2121065849021673, 0.2121065849021673, 0.28536006163107014], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.5981705], dtype=float32), -1.0391191]. 
=============================================
[2019-03-26 20:39:34,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2336488e-19 9.9108511e-01 3.1758104e-20 8.9148888e-03 1.0826789e-13], sum to 1.0000
[2019-03-26 20:39:34,241] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1161
[2019-03-26 20:39:34,250] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2413101.09298336 W.
[2019-03-26 20:39:34,256] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.5751893048479085, 1.0, 2.0, 0.5751893048479085, 1.0, 1.0, 0.9989136045110023, 6.9112, 6.9112, 170.5573041426782, 2413101.09298336, 2413101.09298336, 471035.3250915896], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6019200.0000, 
sim time next is 6019800.0000, 
raw observation next is [31.83333333333334, 70.16666666666667, 1.0, 2.0, 0.887446255238561, 1.0, 2.0, 0.887446255238561, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2482147.029342678, 2482147.029342677, 464695.011125361], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.7016666666666667, 1.0, 1.0, 0.8643930786006759, 1.0, 1.0, 0.8643930786006759, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6894852859285217, 0.6894852859285214, 0.6935746434706881], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2560972], dtype=float32), -1.0096539]. 
=============================================
[2019-03-26 20:39:41,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.6588802e-28 1.0000000e+00 1.7063606e-30 1.3559498e-08 1.7331520e-23], sum to 1.0000
[2019-03-26 20:39:41,647] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8299
[2019-03-26 20:39:41,651] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.96666666666667, 93.66666666666667, 1.0, 2.0, 0.6710105856193039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937738.5253616713, 937738.525361672, 214711.4824841856], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5978400.0000, 
sim time next is 5979000.0000, 
raw observation next is [25.93333333333333, 93.83333333333334, 1.0, 2.0, 0.6643650645838307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928447.3380341934, 928447.3380341934, 213340.6206680305], 
processed observation next is [1.0, 0.17391304347826086, 0.42812006319115314, 0.9383333333333335, 1.0, 1.0, 0.5956205597395551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2579020383428315, 0.2579020383428315, 0.31841883681795596], 
reward next is 0.6816, 
noisyNet noise sample is [array([1.3424473], dtype=float32), 1.1305021]. 
=============================================
[2019-03-26 20:39:41,672] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[50.364918]
 [50.38628 ]
 [50.27303 ]
 [50.131664]
 [50.422897]], R is [[50.52825546]
 [50.70250702]
 [50.87302017]
 [51.03664398]
 [51.18102646]].
[2019-03-26 20:39:49,856] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.7722167e-19 6.8988961e-01 2.1918309e-21 3.1011033e-01 1.3299905e-14], sum to 1.0000
[2019-03-26 20:39:49,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2179
[2019-03-26 20:39:49,872] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2316606.245900053 W.
[2019-03-26 20:39:49,876] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.45, 68.0, 1.0, 2.0, 0.5522099915435533, 1.0, 2.0, 0.5522099915435533, 1.0, 2.0, 0.955279677090243, 6.911200000000001, 6.9112, 170.5573041426782, 2316606.245900053, 2316606.245900053, 452376.8785482061], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6107400.0000, 
sim time next is 6108000.0000, 
raw observation next is [30.4, 68.33333333333334, 1.0, 2.0, 0.8293444183850559, 1.0, 2.0, 0.8293444183850559, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2319487.999351577, 2319487.999351577, 434406.1122696155], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6833333333333335, 1.0, 1.0, 0.7943908655241637, 1.0, 1.0, 0.7943908655241637, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6443022220421047, 0.6443022220421047, 0.6483673317456948], 
reward next is 0.3516, 
noisyNet noise sample is [array([1.814669], dtype=float32), -0.7036528]. 
=============================================
[2019-03-26 20:39:49,902] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[39.796825]
 [39.34372 ]
 [40.27384 ]
 [40.965908]
 [41.97734 ]], R is [[40.79367447]
 [40.7105484 ]
 [40.30344391]
 [39.9004097 ]
 [39.85993958]].
[2019-03-26 20:39:50,399] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 20:39:50,400] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:39:50,401] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:39:50,401] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:39:50,402] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:39:50,402] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:39:50,404] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:39:50,404] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:39:50,403] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:39:50,406] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:39:50,405] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:39:50,428] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-03-26 20:39:50,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-03-26 20:39:50,449] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-03-26 20:39:50,450] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-03-26 20:39:50,487] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-03-26 20:39:55,034] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:39:55,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.35489453333333, 93.70036553666667, 1.0, 2.0, 0.3466861557496351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541639.524024408, 541639.524024408, 170147.4519388966]
[2019-03-26 20:39:55,040] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:39:55,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.109751e-15 0.000000e+00], sampled 0.23308820192470825
[2019-03-26 20:39:58,835] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:39:58,837] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.39805354666667, 79.75570011166667, 1.0, 2.0, 0.2518046877710969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 414620.1805650072, 414620.1805650072, 161075.0425179896]
[2019-03-26 20:39:58,838] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:39:58,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1005574e-15 0.0000000e+00], sampled 0.18748797821576468
[2019-03-26 20:40:03,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:40:03,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.74475125, 79.45681274, 1.0, 2.0, 0.3918708215239612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588476.8125130566, 588476.8125130573, 173589.7075831385]
[2019-03-26 20:40:03,370] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:40:03,376] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.02303166e-14
 0.00000000e+00], sampled 0.3202527913259605
[2019-03-26 20:40:23,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:40:23,440] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.77402521833334, 63.38117100500001, 1.0, 2.0, 0.5858544193948654, 0.0, 2.0, 0.0, 1.0, 1.0, 1.008820746121992, 6.9112, 6.9112, 168.9126983768342, 1638005.039907722, 1638005.039907722, 356801.1092609669]
[2019-03-26 20:40:23,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:40:23,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.5598744e-36 1.0000000e+00 9.4267287e-38 7.4513298e-11 1.9236132e-30], sampled 0.3052256484486603
[2019-03-26 20:40:48,627] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:40:48,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 53.66666666666667, 1.0, 2.0, 0.9259681657672011, 1.0, 1.0, 0.9259681657672011, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2590002.739811298, 2590002.739811298, 485865.9905117985]
[2019-03-26 20:40:48,629] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:40:48,631] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.5457700e-29 1.0000000e+00 1.8995496e-30 3.4234269e-08 2.8129352e-24], sampled 0.06181693013438483
[2019-03-26 20:40:48,633] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2590002.739811298 W.
[2019-03-26 20:40:57,134] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:40:57,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.55, 84.16666666666667, 1.0, 2.0, 0.6453740872167867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 901896.2595108424, 901896.2595108418, 209503.8987273905]
[2019-03-26 20:40:57,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:40:57,139] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2236715e-13 5.8999764e-37], sampled 0.6241488131552402
[2019-03-26 20:41:27,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:41:27,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.1, 68.0, 1.0, 2.0, 0.3715438503338529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565620.250718045, 565620.250718045, 171788.4464033191]
[2019-03-26 20:41:27,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:41:27,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2500891e-14 4.9727694e-38], sampled 0.7333880908468468
[2019-03-26 20:41:34,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), 0.036322355]
[2019-03-26 20:41:34,582] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.86687778, 57.87084898666667, 1.0, 2.0, 0.7263734010952156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1108136.057028729, 1108136.057028729, 238587.3332902514]
[2019-03-26 20:41:34,584] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:41:34,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8237954e-38 1.0000000e+00 0.0000000e+00 1.0032402e-11 1.7668622e-32], sampled 0.30884650095408506
[2019-03-26 20:41:42,705] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.3211 3164922268.7314 1778.0000
[2019-03-26 20:41:43,591] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7003 2928231904.9171 1338.0000
[2019-03-26 20:41:43,608] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5068 2843430346.4143 1131.0000
[2019-03-26 20:41:43,694] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 20:41:43,706] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.3572 2780132369.7409 933.0000
[2019-03-26 20:41:44,723] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 800000, evaluation results [800000.0, 7875.321099382461, 3164922268.731367, 1778.0, 8253.700304448676, 2928231904.9170933, 1338.0, 8657.357172976928, 2780132369.740925, 933.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8493.50677167813, 2843430346.4142575, 1131.0]
[2019-03-26 20:41:54,296] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6014631e-32 9.9999964e-01 9.1986439e-35 3.0964449e-07 4.5616079e-26], sum to 1.0000
[2019-03-26 20:41:54,306] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0504
[2019-03-26 20:41:54,312] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.0, 1.0, 2.0, 0.6646483518587758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928843.403911114, 928843.403911114, 213399.1069552598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6499800.0000, 
sim time next is 6500400.0000, 
raw observation next is [26.36666666666667, 91.66666666666667, 1.0, 2.0, 0.643526251608902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899312.8561165351, 899312.8561165351, 209127.188783195], 
processed observation next is [1.0, 0.21739130434782608, 0.4486571879936811, 0.9166666666666667, 1.0, 1.0, 0.5705135561553035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24980912669903754, 0.24980912669903754, 0.3121301325122313], 
reward next is 0.6879, 
noisyNet noise sample is [array([-0.93677926], dtype=float32), -0.61608714]. 
=============================================
[2019-03-26 20:42:08,138] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1864058e-31 9.9998856e-01 6.0559635e-32 1.1487003e-05 6.2904539e-26], sum to 1.0000
[2019-03-26 20:42:08,152] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2956
[2019-03-26 20:42:08,156] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 92.66666666666667, 1.0, 2.0, 0.655766349758343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 916425.4863003239, 916425.4863003232, 211586.4064488214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6496800.0000, 
sim time next is 6497400.0000, 
raw observation next is [26.11666666666667, 92.83333333333333, 1.0, 2.0, 0.6530760839191702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 912664.2571650674, 912664.257165068, 211042.0464341715], 
processed observation next is [1.0, 0.17391304347826086, 0.43680884676145365, 0.9283333333333332, 1.0, 1.0, 0.5820193782158677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2535178492125187, 0.25351784921251885, 0.31498812900622614], 
reward next is 0.6850, 
noisyNet noise sample is [array([0.81848824], dtype=float32), -0.012817654]. 
=============================================
[2019-03-26 20:42:08,608] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6385380e-22 9.4272465e-01 6.6573818e-24 5.7275329e-02 3.2181812e-17], sum to 1.0000
[2019-03-26 20:42:08,619] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6438
[2019-03-26 20:42:08,626] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1851038.587240945 W.
[2019-03-26 20:42:08,631] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.06666666666667, 79.66666666666667, 1.0, 2.0, 0.6619878874953385, 1.0, 2.0, 0.6619878874953385, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1851038.587240945, 1851038.587240945, 358272.6400172826], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6511200.0000, 
sim time next is 6511800.0000, 
raw observation next is [28.28333333333333, 77.83333333333333, 1.0, 2.0, 0.4460836314757233, 1.0, 2.0, 0.4460836314757233, 1.0, 1.0, 0.7625427178391065, 6.9112, 6.9112, 170.5573041426782, 1871013.928588384, 1871013.928588384, 377919.4063728312], 
processed observation next is [1.0, 0.34782608695652173, 0.5394944707740915, 0.7783333333333333, 1.0, 1.0, 0.3326308812960521, 1.0, 1.0, 0.3326308812960521, 1.0, 0.5, 0.7104179485842762, 0.0, 0.0, 0.8375144448122397, 0.5197260912745512, 0.5197260912745512, 0.5640588154818376], 
reward next is 0.4359, 
noisyNet noise sample is [array([0.8320931], dtype=float32), -1.6855333]. 
=============================================
[2019-03-26 20:42:10,373] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.4686906e-38 1.0000000e+00 0.0000000e+00 3.3461443e-09 2.8273242e-35], sum to 1.0000
[2019-03-26 20:42:10,381] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1403
[2019-03-26 20:42:10,385] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 73.0, 1.0, 2.0, 0.4830780110132218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675018.9527168088, 675018.9527168088, 180987.1319754755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6552000.0000, 
sim time next is 6552600.0000, 
raw observation next is [27.95, 73.66666666666667, 1.0, 2.0, 0.4850318140300224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677749.9294035789, 677749.9294035796, 181284.0457140137], 
processed observation next is [1.0, 0.8695652173913043, 0.523696682464455, 0.7366666666666667, 1.0, 1.0, 0.37955640244581007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1882638692787719, 0.1882638692787721, 0.27057320255822936], 
reward next is 0.7294, 
noisyNet noise sample is [array([0.38228542], dtype=float32), 0.8279379]. 
=============================================
[2019-03-26 20:42:19,077] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.9327569e-12 2.7758837e-38], sum to 1.0000
[2019-03-26 20:42:19,085] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6043
[2019-03-26 20:42:19,092] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.11666666666667, 71.66666666666666, 1.0, 2.0, 0.3588082607352506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552194.5937708472, 552194.5937708472, 170815.5840962378], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6735000.0000, 
sim time next is 6735600.0000, 
raw observation next is [25.0, 72.0, 1.0, 2.0, 0.3550359818122086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547326.3541794426, 547326.3541794426, 170434.4798124389], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.72, 1.0, 1.0, 0.22293491784603442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1520350983831785, 0.1520350983831785, 0.25437982061558045], 
reward next is 0.7456, 
noisyNet noise sample is [array([0.40434223], dtype=float32), -0.197143]. 
=============================================
[2019-03-26 20:42:21,503] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 9.999999e-01 0.000000e+00 6.791514e-08 8.134632e-37], sum to 1.0000
[2019-03-26 20:42:21,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6541
[2019-03-26 20:42:21,518] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 67.0, 1.0, 2.0, 0.4229472398859399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616878.4965556355, 616878.4965556355, 175726.7397801908], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6723000.0000, 
sim time next is 6723600.0000, 
raw observation next is [27.46666666666667, 67.0, 1.0, 2.0, 0.4170665138822869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611146.1835130262, 611146.1835130256, 175261.3542101542], 
processed observation next is [1.0, 0.8260869565217391, 0.500789889415482, 0.67, 1.0, 1.0, 0.2976704986533577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1697628287536184, 0.16976282875361823, 0.2615841107614242], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.9303112], dtype=float32), -0.92930657]. 
=============================================
[2019-03-26 20:42:31,903] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.608502e-10 1.211787e-31], sum to 1.0000
[2019-03-26 20:42:31,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6139
[2019-03-26 20:42:31,921] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4165771048668537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612831.3522172116, 612831.3522172116, 175490.061568499], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6926400.0000, 
sim time next is 6927000.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.416518356706541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 613002.4067127772, 613002.4067127772, 175513.5447305596], 
processed observation next is [0.0, 0.17391304347826086, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.29701006832113375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17027844630910477, 0.17027844630910477, 0.2619605145232233], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.42338678], dtype=float32), -1.0414902]. 
=============================================
[2019-03-26 20:42:31,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[66.064354]
 [65.94938 ]
 [65.899315]
 [65.869   ]
 [65.82216 ]], R is [[66.21230316]
 [66.28825378]
 [66.36330414]
 [66.43746185]
 [66.51070404]].
[2019-03-26 20:42:36,010] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6913909e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 20:42:36,020] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9458
[2019-03-26 20:42:36,028] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 53.66666666666667, 1.0, 2.0, 0.4729709186584572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660891.6254807549, 660891.6254807543, 179469.7601195881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6966600.0000, 
sim time next is 6967200.0000, 
raw observation next is [31.33333333333334, 55.33333333333334, 1.0, 2.0, 0.4751808813675474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663980.6151790711, 663980.6151790704, 179798.9960230589], 
processed observation next is [0.0, 0.6521739130434783, 0.6840442338072673, 0.5533333333333335, 1.0, 1.0, 0.36768780887656316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1844390597719642, 0.184439059771964, 0.26835671048217746], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.33419284], dtype=float32), -0.6792623]. 
=============================================
[2019-03-26 20:42:36,039] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.4736913e-11 1.7681760e-33], sum to 1.0000
[2019-03-26 20:42:36,047] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2324
[2019-03-26 20:42:36,055] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.45, 52.0, 1.0, 2.0, 0.4579014458159271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647568.229588663, 647568.2295886637, 178262.1412250422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6960600.0000, 
sim time next is 6961200.0000, 
raw observation next is [31.26666666666667, 52.0, 1.0, 2.0, 0.4516574988904564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 643087.5587748671, 643087.5587748665, 177912.960581691], 
processed observation next is [0.0, 0.5652173913043478, 0.6808846761453398, 0.52, 1.0, 1.0, 0.3393463842053692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17863543299301865, 0.17863543299301848, 0.2655417322114791], 
reward next is 0.7345, 
noisyNet noise sample is [array([0.5257453], dtype=float32), 0.97793704]. 
=============================================
[2019-03-26 20:42:40,501] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 20:42:40,503] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:42:40,503] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:42:40,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:42:40,503] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:42:40,504] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:42:40,506] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:42:40,506] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:42:40,507] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:42:40,508] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:42:40,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:42:40,534] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-03-26 20:42:40,553] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-03-26 20:42:40,584] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-03-26 20:42:40,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-03-26 20:42:40,607] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-03-26 20:42:43,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:42:43,787] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 96.0, 1.0, 2.0, 0.3711299105779521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567008.1271868538, 567008.1271868545, 171966.7106981881]
[2019-03-26 20:42:43,789] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:42:43,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3028835e-16 3.5618740e-37], sampled 0.4484572611451022
[2019-03-26 20:42:45,230] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:42:45,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.34419990333333, 91.17240058666667, 1.0, 2.0, 0.2971554161203396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479223.1867496327, 479223.1867496327, 165603.4581633066]
[2019-03-26 20:42:45,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:42:45,237] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6911120e-16 1.3641133e-38], sampled 0.08445563503212661
[2019-03-26 20:42:53,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:42:53,544] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.6, 96.0, 1.0, 2.0, 0.3925001534882189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588982.6796473247, 588982.6796473254, 173623.1385807835]
[2019-03-26 20:42:53,545] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:42:53,552] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1076321e-16 5.2986576e-38], sampled 0.7737348299607395
[2019-03-26 20:42:57,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:42:57,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.5, 88.5, 1.0, 2.0, 0.2838623207071004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459649.234667621, 459649.234667621, 164237.067426398]
[2019-03-26 20:42:57,347] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:42:57,348] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6339191e-16 1.2633612e-38], sampled 0.023117199757590523
[2019-03-26 20:43:01,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:01,784] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.11552543666667, 88.42638113833333, 1.0, 2.0, 0.5931227267126302, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 828847.6270097368, 828847.6270097368, 199472.9828337947]
[2019-03-26 20:43:01,787] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:43:01,790] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1685255e-15 1.0161011e-36], sampled 0.35505709877724856
[2019-03-26 20:43:07,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:07,343] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.41599941, 93.17812686666667, 1.0, 2.0, 0.375866210163673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572576.3393942625, 572576.3393942631, 172405.9830516815]
[2019-03-26 20:43:07,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:43:07,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2592956e-16 1.0702603e-37], sampled 0.1079953366451003
[2019-03-26 20:43:08,666] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:08,667] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.65835812, 92.32396494333334, 1.0, 2.0, 0.345152749104966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 540344.1845452079, 540344.1845452072, 170065.8467613844]
[2019-03-26 20:43:08,670] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:43:08,673] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8812911e-16 4.4763742e-38], sampled 0.16048026752145728
[2019-03-26 20:43:22,336] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:22,337] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.6864237728453633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1055366.531807516, 1055366.531807515, 229960.76234144]
[2019-03-26 20:43:22,338] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:43:22,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.0784154e-38 1.0000000e+00 0.0000000e+00 7.8467032e-14 1.2062474e-32], sampled 0.3981255465434723
[2019-03-26 20:43:22,930] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:22,932] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.80278608, 98.71597801, 1.0, 2.0, 0.3232107814144812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507698.7310041908, 507698.7310041908, 167533.9866911893]
[2019-03-26 20:43:22,933] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:43:22,936] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4139102e-16 3.0165284e-38], sampled 0.21097560641399982
[2019-03-26 20:43:47,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:47,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.0, 1.0, 2.0, 0.9106114270254874, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997159136116382, 6.9112, 168.9123723012221, 2169889.668476625, 2108907.509746622, 437350.0266311595]
[2019-03-26 20:43:47,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:43:47,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0453569e-28 1.0000000e+00 3.2200699e-30 1.2553115e-09 2.7343477e-24], sampled 0.38464663301023716
[2019-03-26 20:43:47,535] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2169889.668476625 W.
[2019-03-26 20:43:58,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:43:58,290] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.08333333333334, 92.33333333333333, 1.0, 2.0, 0.5916375406498293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826771.375346926, 826771.375346926, 199198.4885205289]
[2019-03-26 20:43:58,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:43:58,295] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3758664e-15 4.4699478e-35], sampled 0.9291515810588399
[2019-03-26 20:44:04,386] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:44:04,388] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.25, 62.0, 1.0, 2.0, 0.8374158315135615, 1.0, 1.0, 0.8374158315135615, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2342053.797176626, 2342053.797176625, 439001.729916719]
[2019-03-26 20:44:04,389] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:44:04,392] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3761551e-31 1.0000000e+00 2.8242874e-33 3.0755735e-11 4.5252451e-27], sampled 0.440793751516653
[2019-03-26 20:44:04,395] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2342053.797176626 W.
[2019-03-26 20:44:16,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:44:16,648] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.3, 79.0, 1.0, 2.0, 0.7231211896200421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1010597.840290804, 1010597.840290803, 225917.2807647505]
[2019-03-26 20:44:16,649] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:44:16,651] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9912247e-15 2.5892387e-35], sampled 0.2797645966779365
[2019-03-26 20:44:31,864] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.007971859]
[2019-03-26 20:44:31,865] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.21666666666667, 85.5, 1.0, 2.0, 0.6570326166987376, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.022631466098272, 6.9112, 143.6817159167873, 1815276.572667684, 1748031.782791968, 372574.7749933337]
[2019-03-26 20:44:31,869] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:44:31,871] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.7812581e-28 1.0000000e+00 2.3449073e-29 3.6653529e-09 1.6882892e-23], sampled 0.6641472888501904
[2019-03-26 20:44:31,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1815276.572667684 W.
[2019-03-26 20:44:34,273] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164933419.7733 1778.0000
[2019-03-26 20:44:35,004] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7003 2928231904.9171 1338.0000
[2019-03-26 20:44:35,032] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 20:44:35,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.3572 2780132369.7409 933.0000
[2019-03-26 20:44:35,233] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5068 2843430346.4143 1131.0000
[2019-03-26 20:44:36,248] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 825000, evaluation results [825000.0, 7874.254934002473, 3164933419.7732573, 1778.0, 8253.700304448676, 2928231904.9170933, 1338.0, 8657.357172976928, 2780132369.740925, 933.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8493.50677167813, 2843430346.4142575, 1131.0]
[2019-03-26 20:44:37,143] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1222468e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 20:44:37,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6424
[2019-03-26 20:44:37,158] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 87.66666666666667, 1.0, 2.0, 0.4784223821799258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668809.4186767162, 668809.4186767162, 180322.4065576535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7080000.0000, 
sim time next is 7080600.0000, 
raw observation next is [25.43333333333333, 87.83333333333334, 1.0, 2.0, 0.4763658806335168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666240.161627436, 666240.1616274355, 180053.5501212139], 
processed observation next is [1.0, 0.9565217391304348, 0.40442338072669815, 0.8783333333333334, 1.0, 1.0, 0.3691155188355624, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1850667115631767, 0.18506671156317653, 0.268736641971961], 
reward next is 0.7313, 
noisyNet noise sample is [array([-0.9606331], dtype=float32), -1.607768]. 
=============================================
[2019-03-26 20:44:40,209] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5692287e-24 9.9999917e-01 4.5882548e-26 8.5942702e-07 2.7544634e-19], sum to 1.0000
[2019-03-26 20:44:40,220] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6234
[2019-03-26 20:44:40,227] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1676907.750686751 W.
[2019-03-26 20:44:40,233] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.9, 71.0, 1.0, 2.0, 0.3998413840172431, 1.0, 2.0, 0.3998413840172431, 1.0, 1.0, 0.6711739937625928, 6.911200000000001, 6.9112, 170.5573041426782, 1676907.750686751, 1676907.75068675, 349461.5108036282], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7128000.0000, 
sim time next is 7128600.0000, 
raw observation next is [27.78333333333333, 71.83333333333334, 1.0, 2.0, 0.6280787264126705, 1.0, 2.0, 0.6280787264126705, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1756144.703708345, 1756144.703708345, 344841.6482327541], 
processed observation next is [1.0, 0.5217391304347826, 0.5157977883096366, 0.7183333333333334, 1.0, 1.0, 0.5519020800152656, 1.0, 1.0, 0.5519020800152656, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48781797325231807, 0.48781797325231807, 0.5146890272130658], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1615038], dtype=float32), 1.6368781]. 
=============================================
[2019-03-26 20:44:44,074] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2880974e-12 8.8857941e-34], sum to 1.0000
[2019-03-26 20:44:44,083] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3324
[2019-03-26 20:44:44,091] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.16666666666667, 1.0, 2.0, 0.6429331572530604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898483.67006563, 898483.67006563, 209006.9241447223], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7182600.0000, 
sim time next is 7183200.0000, 
raw observation next is [25.8, 89.33333333333334, 1.0, 2.0, 0.5550310196432704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775597.6895465234, 775597.6895465241, 192663.0975303033], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.8933333333333334, 1.0, 1.0, 0.46389279475092815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21544380265181204, 0.21544380265181223, 0.2875568619855273], 
reward next is 0.7124, 
noisyNet noise sample is [array([0.88177216], dtype=float32), -0.3692742]. 
=============================================
[2019-03-26 20:44:54,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2331838e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 20:44:54,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0209
[2019-03-26 20:44:54,270] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 77.0, 1.0, 2.0, 0.3941799265309842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603844.4343416754, 603844.4343416754, 175278.5936769881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7354800.0000, 
sim time next is 7355400.0000, 
raw observation next is [24.43333333333333, 78.0, 1.0, 2.0, 0.4173523204465318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638088.4572700417, 638088.4572700424, 178426.5306008872], 
processed observation next is [1.0, 0.13043478260869565, 0.3570300157977882, 0.78, 1.0, 1.0, 0.2980148439114841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1772467936861227, 0.1772467936861229, 0.26630825462818986], 
reward next is 0.7337, 
noisyNet noise sample is [array([-2.068017], dtype=float32), -0.25701663]. 
=============================================
[2019-03-26 20:45:03,289] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.349914e-24 3.955306e-36], sum to 1.0000
[2019-03-26 20:45:03,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9792
[2019-03-26 20:45:03,305] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.15, 90.0, 1.0, 2.0, 0.3728689970565986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563456.4493623809, 563456.4493623809, 171472.3764390005], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7536600.0000, 
sim time next is 7537200.0000, 
raw observation next is [23.2, 90.0, 1.0, 2.0, 0.374328916147929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564747.1899638186, 564747.1899638181, 171555.5652551689], 
processed observation next is [0.0, 0.21739130434782608, 0.29857819905213273, 0.9, 1.0, 1.0, 0.2461794170456976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15687421943439406, 0.15687421943439392, 0.25605308247040137], 
reward next is 0.7439, 
noisyNet noise sample is [array([0.15215601], dtype=float32), -0.37863743]. 
=============================================
[2019-03-26 20:45:04,902] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3629926e-34 1.0000000e+00 7.2103815e-36 2.8307510e-16 1.2304659e-24], sum to 1.0000
[2019-03-26 20:45:04,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4139
[2019-03-26 20:45:04,921] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2587603.203969482 W.
[2019-03-26 20:45:04,928] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.13333333333333, 60.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.312600073994239, 6.9112, 168.9106321297878, 2587603.203969482, 2302840.094981178, 476104.5216707885], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7746000.0000, 
sim time next is 7746600.0000, 
raw observation next is [31.01666666666667, 60.83333333333334, 1.0, 2.0, 0.5747012635799811, 1.0, 1.0, 0.5747012635799811, 1.0, 2.0, 0.9829163166141266, 6.911200000000001, 6.9112, 170.5573041426782, 2411051.63020947, 2411051.630209469, 467428.3840518942], 
processed observation next is [1.0, 0.6521739130434783, 0.6690363349131123, 0.6083333333333334, 1.0, 1.0, 0.4875918838313025, 1.0, 0.5, 0.4875918838313025, 1.0, 1.0, 0.9791662397733251, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.669736563947075, 0.6697365639470747, 0.6976543045550659], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1904713], dtype=float32), 0.34838265]. 
=============================================
[2019-03-26 20:45:05,085] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1938464e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 20:45:05,098] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4785
[2019-03-26 20:45:05,108] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 76.5, 1.0, 2.0, 0.4702476916781942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657085.2296747924, 657085.2296747918, 179066.041318213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7554600.0000, 
sim time next is 7555200.0000, 
raw observation next is [27.53333333333333, 75.66666666666666, 1.0, 2.0, 0.4717161429883107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659137.7586540052, 659137.7586540047, 179283.6296646017], 
processed observation next is [0.0, 0.43478260869565216, 0.5039494470774091, 0.7566666666666666, 1.0, 1.0, 0.3635134252871214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18309382184833478, 0.18309382184833461, 0.2675875069620921], 
reward next is 0.7324, 
noisyNet noise sample is [array([-1.3731905], dtype=float32), -1.4317418]. 
=============================================
[2019-03-26 20:45:07,721] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5080927e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 20:45:07,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4867
[2019-03-26 20:45:07,736] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 61.5, 1.0, 2.0, 0.458272404193082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 641912.5485249424, 641912.548524943, 177519.9782074696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7570200.0000, 
sim time next is 7570800.0000, 
raw observation next is [30.0, 62.0, 1.0, 2.0, 0.4670701675409974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652643.8563759451, 652643.8563759451, 178598.1383882726], 
processed observation next is [0.0, 0.6521739130434783, 0.6208530805687204, 0.62, 1.0, 1.0, 0.35791586450722573, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18128996010442922, 0.18128996010442922, 0.2665643856541382], 
reward next is 0.7334, 
noisyNet noise sample is [array([2.4550846], dtype=float32), -0.5584859]. 
=============================================
[2019-03-26 20:45:09,472] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8003240e-16 1.1206272e-36], sum to 1.0000
[2019-03-26 20:45:09,485] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2098
[2019-03-26 20:45:09,494] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.65, 89.33333333333334, 1.0, 2.0, 0.6426093189449883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898030.9224663615, 898030.9224663615, 208942.2316539853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7791000.0000, 
sim time next is 7791600.0000, 
raw observation next is [25.6, 89.66666666666667, 1.0, 2.0, 0.5998932225081471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 838312.6562606614, 838312.656260662, 200718.202091726], 
processed observation next is [1.0, 0.17391304347826086, 0.4123222748815167, 0.8966666666666667, 1.0, 1.0, 0.5179436415760809, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23286462673907263, 0.2328646267390728, 0.29957940610705375], 
reward next is 0.7004, 
noisyNet noise sample is [array([-1.2858682], dtype=float32), 0.631219]. 
=============================================
[2019-03-26 20:45:14,820] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.893135e-13 9.317602e-33], sum to 1.0000
[2019-03-26 20:45:14,829] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8187
[2019-03-26 20:45:14,835] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4839619989981175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680227.970374999, 680227.9703749996, 181632.8029373225], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7707600.0000, 
sim time next is 7708200.0000, 
raw observation next is [24.66666666666667, 93.50000000000001, 1.0, 2.0, 0.5144899568696594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720863.7082075112, 720863.7082075119, 186144.7597249588], 
processed observation next is [1.0, 0.21739130434782608, 0.36808846761453423, 0.9350000000000002, 1.0, 1.0, 0.4150481408068185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2002399189465309, 0.2002399189465311, 0.2778279995894907], 
reward next is 0.7222, 
noisyNet noise sample is [array([1.8139713], dtype=float32), -1.0832078]. 
=============================================
[2019-03-26 20:45:16,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.1845053e-25 1.0000000e+00 4.3745366e-28 4.5804438e-10 1.9363988e-18], sum to 1.0000
[2019-03-26 20:45:16,828] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8820
[2019-03-26 20:45:16,837] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2247627.278228564 W.
[2019-03-26 20:45:16,844] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.3, 63.0, 1.0, 2.0, 0.803673346289486, 1.0, 1.0, 0.803673346289486, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2247627.278228564, 2247627.278228564, 421653.3250387987], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7750800.0000, 
sim time next is 7751400.0000, 
raw observation next is [30.13333333333333, 64.16666666666667, 1.0, 2.0, 0.3488019946352044, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5885742150534586, 6.911200000000001, 6.9112, 168.9129564988923, 974919.9141144776, 974919.914114477, 236810.2124342005], 
processed observation next is [1.0, 0.7391304347826086, 0.6271721958925749, 0.6416666666666667, 1.0, 1.0, 0.215424089921933, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.4982612378700715, 8.881784197001253e-17, 0.0, 0.8294399450956423, 0.27081108725402153, 0.27081108725402137, 0.35344807826000074], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.112531], dtype=float32), 0.58535725]. 
=============================================
[2019-03-26 20:45:17,320] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2973654e-23 9.9989235e-01 7.0804078e-23 1.0766439e-04 1.1416362e-19], sum to 1.0000
[2019-03-26 20:45:17,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1007
[2019-03-26 20:45:17,348] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2289324.2440199 W.
[2019-03-26 20:45:17,353] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 62.33333333333334, 1.0, 2.0, 0.9959435092442908, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.974323148437224, 6.9112, 168.9125807242082, 2289324.2440199, 2244542.609614831, 463143.5699457467], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7749600.0000, 
sim time next is 7750200.0000, 
raw observation next is [30.4, 62.66666666666666, 1.0, 2.0, 1.010906375406937, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.970989644226503, 6.9112, 168.9125495756534, 2310267.576807024, 2267850.847580041, 467855.2940568507], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6266666666666666, 1.0, 1.0, 1.013140211333659, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005978964422650268, 0.0, 0.8294379469162492, 0.6417409935575067, 0.6299585687722336, 0.6982914836669414], 
reward next is 0.0028, 
noisyNet noise sample is [array([0.74092245], dtype=float32), -0.7602627]. 
=============================================
[2019-03-26 20:45:18,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:18,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:18,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-03-26 20:45:19,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:19,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:19,887] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-03-26 20:45:20,405] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0449633e-23 9.9135756e-01 2.3509002e-25 8.6423885e-03 5.6163587e-15], sum to 1.0000
[2019-03-26 20:45:20,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3779
[2019-03-26 20:45:20,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1812977.540113598 W.
[2019-03-26 20:45:20,422] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.2, 79.5, 1.0, 2.0, 0.6555844503505441, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98534860541117, 6.9112, 168.9125144024416, 1812977.540113598, 1760374.10511997, 376072.6005285073], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7807800.0000, 
sim time next is 7808400.0000, 
raw observation next is [28.3, 79.0, 1.0, 2.0, 0.4074708823383705, 1.0, 1.0, 0.4074708823383705, 1.0, 2.0, 0.6995428892316515, 6.9112, 6.9112, 170.5573041426782, 1708930.864702475, 1708930.864702475, 355900.8251301261], 
processed observation next is [1.0, 0.391304347826087, 0.5402843601895735, 0.79, 1.0, 1.0, 0.28610949679321745, 1.0, 0.5, 0.28610949679321745, 1.0, 1.0, 0.6335888893068922, 0.0, 0.0, 0.8375144448122397, 0.4747030179729097, 0.4747030179729097, 0.5311952613882479], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.18515077], dtype=float32), 1.5609099]. 
=============================================
[2019-03-26 20:45:23,489] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:23,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:23,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-03-26 20:45:23,661] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4954037e-23 9.9823844e-01 7.8021468e-26 1.7616040e-03 6.4413236e-17], sum to 1.0000
[2019-03-26 20:45:23,666] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5183
[2019-03-26 20:45:23,676] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2201448.026074077 W.
[2019-03-26 20:45:23,680] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 68.0, 1.0, 2.0, 0.5247842454185982, 1.0, 2.0, 0.5247842454185982, 1.0, 1.0, 0.9054045074084736, 6.911199999999999, 6.9112, 170.5573041426782, 2201448.026074077, 2201448.026074078, 431568.588262291], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7915200.0000, 
sim time next is 7915800.0000, 
raw observation next is [30.36666666666666, 67.5, 1.0, 2.0, 0.524476395985893, 1.0, 2.0, 0.524476395985893, 1.0, 2.0, 0.9046869812797739, 6.911200000000001, 6.9112, 170.5573041426782, 2200155.284204906, 2200155.284204905, 431310.8943386111], 
processed observation next is [1.0, 0.6086956521739131, 0.6382306477093204, 0.675, 1.0, 1.0, 0.4270799951637265, 1.0, 1.0, 0.4270799951637265, 1.0, 1.0, 0.8837646113167973, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6111542456124739, 0.6111542456124736, 0.6437476034904643], 
reward next is 0.3563, 
noisyNet noise sample is [array([-0.391173], dtype=float32), -1.1709936]. 
=============================================
[2019-03-26 20:45:26,258] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:26,259] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:26,303] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-03-26 20:45:27,059] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.1021167e-38 1.4161450e-01 0.0000000e+00 8.5838544e-01 8.6053710e-29], sum to 1.0000
[2019-03-26 20:45:27,063] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6851
[2019-03-26 20:45:27,069] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.25, 79.0, 1.0, 2.0, 0.2615902664146748, 1.0, 1.0, 0.2615902664146748, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 731072.2299619713, 731072.2299619713, 242887.3804992491], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7932600.0000, 
sim time next is 7933200.0000, 
raw observation next is [28.16666666666666, 79.66666666666667, 1.0, 2.0, 0.2618844118003648, 1.0, 2.0, 0.2618844118003648, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 731894.5651907506, 731894.5651907506, 242937.5856227361], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657185, 0.7966666666666667, 1.0, 1.0, 0.11070411060284917, 1.0, 1.0, 0.11070411060284917, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20330404588631962, 0.20330404588631962, 0.36259341137721807], 
reward next is 0.6374, 
noisyNet noise sample is [array([-0.5569004], dtype=float32), 0.40035805]. 
=============================================
[2019-03-26 20:45:27,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:27,293] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:27,322] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-03-26 20:45:27,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:27,827] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:27,852] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-03-26 20:45:28,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,063] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-03-26 20:45:28,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,109] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-03-26 20:45:28,533] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-03-26 20:45:28,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-03-26 20:45:28,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,623] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,634] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-03-26 20:45:28,657] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-03-26 20:45:28,702] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-03-26 20:45:28,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-03-26 20:45:28,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:28,942] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:28,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-03-26 20:45:29,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 20:45:29,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:29,002] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-03-26 20:45:30,552] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 20:45:30,560] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:45:30,560] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:45:30,561] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:45:30,561] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:45:30,560] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:30,563] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:30,562] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:30,562] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:45:30,564] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:30,567] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:45:30,585] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-03-26 20:45:30,585] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-03-26 20:45:30,622] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-03-26 20:45:30,624] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-03-26 20:45:30,657] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-03-26 20:45:50,479] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.045460675]
[2019-03-26 20:45:50,480] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.30286631, 93.25169237333333, 1.0, 2.0, 0.4332491128760608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692469.8269800462, 692469.8269800462, 183637.4855703442]
[2019-03-26 20:45:50,481] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:45:50,484] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7018505e-10 3.9948742e-33], sampled 0.9842014328580845
[2019-03-26 20:45:56,495] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.045460675]
[2019-03-26 20:45:56,497] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.60676944, 87.18844033, 1.0, 2.0, 0.9200747095848364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1286016.868882885, 1286016.868882885, 275520.3842509381]
[2019-03-26 20:45:56,498] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:45:56,501] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7740140e-37 1.0000000e+00 0.0000000e+00 4.4666685e-08 2.5831496e-29], sampled 0.973625200097344
[2019-03-26 20:46:08,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.045460675]
[2019-03-26 20:46:08,154] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.0860296, 89.18349738, 1.0, 2.0, 0.4902391758273488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687914.1509398469, 687914.1509398469, 182451.5890358495]
[2019-03-26 20:46:08,156] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:46:08,160] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5855395e-11 5.2282516e-35], sampled 0.1272483572085944
[2019-03-26 20:46:46,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.045460675]
[2019-03-26 20:46:46,338] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 75.0, 1.0, 2.0, 0.5538151683282866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773898.0457974691, 773898.0457974698, 192456.7882176087]
[2019-03-26 20:46:46,340] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:46:46,344] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.9709434e-11 2.9825056e-34], sampled 0.33629280692078944
[2019-03-26 20:47:24,819] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.3391 3164548751.0494 1768.0000
[2019-03-26 20:47:25,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.1098 2843414596.5766 1130.0000
[2019-03-26 20:47:25,326] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.3910 2927530140.5753 1319.0000
[2019-03-26 20:47:25,346] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.7979 2779920135.0716 924.0000
[2019-03-26 20:47:25,361] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9367 3008482456.9084 1764.0000
[2019-03-26 20:47:26,377] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 850000, evaluation results [850000.0, 7876.339125278887, 3164548751.0493984, 1768.0, 8261.39100827087, 2927530140.575255, 1319.0, 8658.797889512321, 2779920135.071585, 924.0, 7997.936664755855, 3008482456.908417, 1764.0, 8494.109836709371, 2843414596.5766215, 1130.0]
[2019-03-26 20:47:30,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 9.9707067e-01 0.0000000e+00 2.9293532e-03 7.3705766e-33], sum to 1.0000
[2019-03-26 20:47:30,675] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8644
[2019-03-26 20:47:30,681] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 87.33333333333334, 1.0, 2.0, 0.3184106743305473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503183.2770729339, 503183.2770729333, 167257.9517252924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 220800.0000, 
sim time next is 221400.0000, 
raw observation next is [21.95, 87.0, 1.0, 2.0, 0.3201268575072396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 505418.0504865164, 505418.0504865164, 167417.2512629884], 
processed observation next is [0.0, 0.5652173913043478, 0.2393364928909953, 0.87, 1.0, 1.0, 0.18087573193643328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14039390291292123, 0.14039390291292123, 0.24987649442237075], 
reward next is 0.7501, 
noisyNet noise sample is [array([-2.0891225], dtype=float32), 0.31847623]. 
=============================================
[2019-03-26 20:47:31,146] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.1250117e-38 9.9976069e-01 0.0000000e+00 2.3927129e-04 4.0176645e-33], sum to 1.0000
[2019-03-26 20:47:31,155] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6542
[2019-03-26 20:47:31,164] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.3837474741719324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591120.1740616008, 591120.1740616008, 174207.1257026495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 99600.0000, 
sim time next is 100200.0000, 
raw observation next is [22.5, 90.0, 1.0, 2.0, 0.3810260201994432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586932.2282260108, 586932.2282260101, 173833.5239642372], 
processed observation next is [1.0, 0.13043478260869565, 0.2654028436018958, 0.9, 1.0, 1.0, 0.2542482171077629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16303673006278077, 0.16303673006278058, 0.25945302084214505], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.30456644], dtype=float32), -0.8735237]. 
=============================================
[2019-03-26 20:47:31,587] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.5048212e-37 1.0000000e+00 0.0000000e+00 1.8275713e-10 1.3266286e-33], sum to 1.0000
[2019-03-26 20:47:31,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4185
[2019-03-26 20:47:31,603] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 89.0, 1.0, 2.0, 0.3437129169526595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534126.1433099344, 534126.1433099338, 169468.9904006323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 90600.0000, 
sim time next is 91200.0000, 
raw observation next is [22.33333333333334, 89.0, 1.0, 2.0, 0.3438353877293951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534082.3449685606, 534082.3449685606, 169459.2737652238], 
processed observation next is [1.0, 0.043478260869565216, 0.2575039494470777, 0.89, 1.0, 1.0, 0.20944022617999408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14835620693571128, 0.14835620693571128, 0.2529242892018266], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.30584812], dtype=float32), -0.23106363]. 
=============================================
[2019-03-26 20:47:32,198] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8233775e-08 8.7003465e-33], sum to 1.0000
[2019-03-26 20:47:32,208] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1855
[2019-03-26 20:47:32,213] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 75.0, 1.0, 2.0, 0.407138167310761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663157.342125536, 663157.3421255352, 180406.5290903884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 381600.0000, 
sim time next is 382200.0000, 
raw observation next is [21.98333333333333, 74.5, 1.0, 2.0, 0.478043141022105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778574.8402033462, 778574.8402033462, 191805.6140737044], 
processed observation next is [1.0, 0.43478260869565216, 0.24091627172195884, 0.745, 1.0, 1.0, 0.3711363144844638, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21627078894537394, 0.21627078894537394, 0.2862770359309021], 
reward next is 0.7137, 
noisyNet noise sample is [array([1.6797479], dtype=float32), 0.32737282]. 
=============================================
[2019-03-26 20:47:38,267] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.818586e-20 8.057862e-35], sum to 1.0000
[2019-03-26 20:47:38,277] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8077
[2019-03-26 20:47:38,281] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 89.33333333333334, 1.0, 2.0, 0.2961501490464502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473780.3036984643, 473780.3036984637, 165208.6884903521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [21.0, 89.5, 1.0, 2.0, 0.2948494713123677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471822.59639589, 471822.59639589, 165072.4082548655], 
processed observation next is [0.0, 0.8695652173913043, 0.19431279620853087, 0.895, 1.0, 1.0, 0.150421049773937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13106183233219165, 0.13106183233219165, 0.24637672873860522], 
reward next is 0.7536, 
noisyNet noise sample is [array([-0.65121573], dtype=float32), -1.1919681]. 
=============================================
[2019-03-26 20:47:40,683] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.939701e-15 3.032053e-33], sum to 1.0000
[2019-03-26 20:47:40,694] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2641
[2019-03-26 20:47:40,706] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.95, 81.83333333333334, 1.0, 2.0, 0.2303358724793361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 380295.462992765, 380295.4629927656, 158986.5359185053], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 454200.0000, 
sim time next is 454800.0000, 
raw observation next is [20.0, 81.66666666666667, 1.0, 2.0, 0.2308156578175492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380968.9817660752, 380968.9817660752, 159036.7521455919], 
processed observation next is [1.0, 0.2608695652173913, 0.1469194312796209, 0.8166666666666668, 1.0, 1.0, 0.07327187688861349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1058247171572431, 0.1058247171572431, 0.2373682867844655], 
reward next is 0.7626, 
noisyNet noise sample is [array([-1.1952977], dtype=float32), 3.560042]. 
=============================================
[2019-03-26 20:47:42,934] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.490306e-10 0.000000e+00], sum to 1.0000
[2019-03-26 20:47:42,945] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2042
[2019-03-26 20:47:42,952] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 91.33333333333334, 1.0, 2.0, 0.2725676136235184, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 440822.8523189685, 440822.8523189685, 162992.3844759037], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 281400.0000, 
sim time next is 282000.0000, 
raw observation next is [20.4, 90.66666666666667, 1.0, 2.0, 0.2773877877685151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448052.7956005751, 448052.7956005757, 163469.7129210312], 
processed observation next is [0.0, 0.2608695652173913, 0.16587677725118483, 0.9066666666666667, 1.0, 1.0, 0.12938287682953623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12445910988904864, 0.1244591098890488, 0.24398464615079282], 
reward next is 0.7560, 
noisyNet noise sample is [array([-2.898722], dtype=float32), 2.0178783]. 
=============================================
[2019-03-26 20:47:42,976] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[83.47264 ]
 [83.499596]
 [83.44272 ]
 [83.456505]
 [83.458   ]], R is [[83.35400391]
 [83.27719116]
 [83.20140839]
 [83.12666321]
 [83.05290985]].
[2019-03-26 20:47:45,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3629469e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 20:47:45,455] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8445
[2019-03-26 20:47:45,462] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 85.0, 1.0, 2.0, 0.2886586707403454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462513.7222599693, 462513.72225997, 164432.0790236724], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 329400.0000, 
sim time next is 330000.0000, 
raw observation next is [21.43333333333333, 85.33333333333333, 1.0, 2.0, 0.2878063677158854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461361.9366500977, 461361.9366500982, 164355.0262740652], 
processed observation next is [0.0, 0.8260869565217391, 0.21484992101105835, 0.8533333333333333, 1.0, 1.0, 0.14193538279022339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12815609351391602, 0.12815609351391616, 0.2453060093642764], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.3815678], dtype=float32), 1.2921236]. 
=============================================
[2019-03-26 20:47:45,484] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.777596]
 [74.7523  ]
 [74.73274 ]
 [74.72212 ]
 [74.63142 ]], R is [[74.81278229]
 [74.81922913]
 [74.82549286]
 [74.83153534]
 [74.83726501]].
[2019-03-26 20:47:51,582] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1684958e-12 1.0125383e-27], sum to 1.0000
[2019-03-26 20:47:51,589] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5909
[2019-03-26 20:47:51,595] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.0, 1.0, 2.0, 0.2376285661083507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 392091.3341572003, 392091.3341572003, 159676.3655643652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 438600.0000, 
sim time next is 439200.0000, 
raw observation next is [19.6, 85.0, 1.0, 2.0, 0.2375883467431448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392025.2315532363, 392025.2315532357, 159672.5643642883], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.85, 1.0, 1.0, 0.08143174306402987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10889589765367676, 0.10889589765367658, 0.2383172602452064], 
reward next is 0.7617, 
noisyNet noise sample is [array([-0.8045402], dtype=float32), -0.29829678]. 
=============================================
[2019-03-26 20:47:51,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9094421e-10 3.7084727e-36], sum to 1.0000
[2019-03-26 20:47:51,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2067
[2019-03-26 20:47:51,984] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.63333333333333, 85.33333333333334, 1.0, 2.0, 0.2408108809142203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396950.562634402, 396950.5626344027, 159993.4607243839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 434400.0000, 
sim time next is 435000.0000, 
raw observation next is [19.61666666666667, 85.16666666666667, 1.0, 2.0, 0.2397959677748113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395445.2857745519, 395445.2857745519, 159890.7016942344], 
processed observation next is [1.0, 0.0, 0.12875197472353894, 0.8516666666666667, 1.0, 1.0, 0.08409152743953167, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10984591271515332, 0.10984591271515332, 0.23864283834960356], 
reward next is 0.7614, 
noisyNet noise sample is [array([-1.8307664], dtype=float32), 0.6342343]. 
=============================================
[2019-03-26 20:47:51,995] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[69.173645]
 [69.24636 ]
 [69.40314 ]
 [69.81305 ]
 [70.96672 ]], R is [[69.15343475]
 [69.22309875]
 [69.29194641]
 [69.35997009]
 [69.42716217]].
[2019-03-26 20:47:55,909] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2470961e-15 3.1415239e-35], sum to 1.0000
[2019-03-26 20:47:55,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6123
[2019-03-26 20:47:55,925] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.46666666666667, 83.5, 1.0, 2.0, 0.2371244943561485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392546.6935223611, 392546.6935223617, 159545.2269239772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 510600.0000, 
sim time next is 511200.0000, 
raw observation next is [19.4, 84.0, 1.0, 2.0, 0.2367450751573401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 391936.0690190203, 391936.0690190203, 159508.0170305053], 
processed observation next is [1.0, 0.9565217391304348, 0.11848341232227487, 0.84, 1.0, 1.0, 0.08041575320161458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1088711302830612, 0.1088711302830612, 0.23807166720970938], 
reward next is 0.7619, 
noisyNet noise sample is [array([-0.13864575], dtype=float32), -0.11171521]. 
=============================================
[2019-03-26 20:47:55,968] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3559527e-13 0.0000000e+00], sum to 1.0000
[2019-03-26 20:47:55,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2468
[2019-03-26 20:47:55,986] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 70.5, 1.0, 2.0, 0.2499026127298319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411276.6103205053, 411276.6103205059, 160891.6280815469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [21.6, 71.0, 1.0, 2.0, 0.2496570161819618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 411226.8252311549, 411226.8252311542, 160859.4074592386], 
processed observation next is [1.0, 0.8260869565217391, 0.22274881516587688, 0.71, 1.0, 1.0, 0.095972308652966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1142296736753208, 0.11422967367532061, 0.24008866784960986], 
reward next is 0.7599, 
noisyNet noise sample is [array([0.92269516], dtype=float32), -1.0748925]. 
=============================================
[2019-03-26 20:47:55,999] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.81187]
 [79.9025 ]
 [79.76542]
 [79.77866]
 [79.8084 ]], R is [[79.79351044]
 [79.75543976]
 [79.71760559]
 [79.6798172 ]
 [79.6421051 ]].
[2019-03-26 20:47:58,999] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0820417e-11 3.2390604e-34], sum to 1.0000
[2019-03-26 20:47:59,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7615
[2019-03-26 20:47:59,013] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 54.33333333333333, 1.0, 2.0, 0.5921154285601717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 968977.7556978606, 968977.75569786, 213881.6616905544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 562200.0000, 
sim time next is 562800.0000, 
raw observation next is [24.8, 54.66666666666667, 1.0, 2.0, 0.4082328409843701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668352.9711337485, 668352.9711337485, 180665.8312832849], 
processed observation next is [1.0, 0.5217391304347826, 0.3744075829383887, 0.5466666666666667, 1.0, 1.0, 0.28702751925827724, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18565360309270793, 0.18565360309270793, 0.26965049445266404], 
reward next is 0.7303, 
noisyNet noise sample is [array([-1.3047371], dtype=float32), -0.23703232]. 
=============================================
[2019-03-26 20:48:01,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9721733e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 20:48:01,387] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0764
[2019-03-26 20:48:01,396] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 72.33333333333334, 1.0, 2.0, 0.3499500078984439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577136.3519913014, 577136.351991302, 172492.5667516778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 634800.0000, 
sim time next is 635400.0000, 
raw observation next is [21.5, 71.5, 1.0, 2.0, 0.3807037680022441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627504.771609925, 627504.771609925, 176693.1715568655], 
processed observation next is [1.0, 0.34782608695652173, 0.21800947867298584, 0.715, 1.0, 1.0, 0.2538599614484869, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17430688100275693, 0.17430688100275693, 0.2637211515774112], 
reward next is 0.7363, 
noisyNet noise sample is [array([-0.75072175], dtype=float32), 0.005183677]. 
=============================================
[2019-03-26 20:48:01,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2663166e-14 8.9776031e-37], sum to 1.0000
[2019-03-26 20:48:01,467] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4554
[2019-03-26 20:48:01,474] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.1, 87.0, 1.0, 2.0, 0.21592877989939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 360056.391797975, 360056.3917979744, 157132.8464596541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 604800.0000, 
sim time next is 605400.0000, 
raw observation next is [18.03333333333333, 87.33333333333333, 1.0, 2.0, 0.2152333624153343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358951.9704273189, 358951.9704273189, 157053.0264604676], 
processed observation next is [1.0, 0.0, 0.05371248025276459, 0.8733333333333333, 1.0, 1.0, 0.054498027006426863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09970888067425525, 0.09970888067425525, 0.23440750217980239], 
reward next is 0.7656, 
noisyNet noise sample is [array([-0.3529089], dtype=float32), -1.2691981]. 
=============================================
[2019-03-26 20:48:01,646] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4739317e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 20:48:01,660] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2192
[2019-03-26 20:48:01,664] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.73333333333333, 83.5, 1.0, 2.0, 0.2227071482667095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370819.1397679032, 370819.1397679038, 157885.6006832933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 600600.0000, 
sim time next is 601200.0000, 
raw observation next is [18.6, 84.0, 1.0, 2.0, 0.22190195719751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 369653.2720077017, 369653.2720077011, 157767.6579558848], 
processed observation next is [1.0, 1.0, 0.08056872037914704, 0.84, 1.0, 1.0, 0.06253247855121684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1026814644465838, 0.10268146444658365, 0.23547411635206686], 
reward next is 0.7645, 
noisyNet noise sample is [array([1.5414674], dtype=float32), 0.30853465]. 
=============================================
[2019-03-26 20:48:01,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6270765e-12 4.9902242e-34], sum to 1.0000
[2019-03-26 20:48:01,815] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7533
[2019-03-26 20:48:01,820] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666667, 48.16666666666666, 1.0, 2.0, 0.6114461933550479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003030.436615798, 1003030.436615798, 218083.1726137713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 742200.0000, 
sim time next is 742800.0000, 
raw observation next is [25.83333333333334, 48.33333333333333, 1.0, 2.0, 0.5711676738858369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937474.5379057351, 937474.5379057344, 209554.2178707939], 
processed observation next is [1.0, 0.6086956521739131, 0.42338072669826254, 0.4833333333333333, 1.0, 1.0, 0.4833345468504059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2604095938627042, 0.260409593862704, 0.31276748935939386], 
reward next is 0.6872, 
noisyNet noise sample is [array([0.5213993], dtype=float32), -0.592077]. 
=============================================
[2019-03-26 20:48:09,883] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3995434e-10 0.0000000e+00], sum to 1.0000
[2019-03-26 20:48:09,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9689
[2019-03-26 20:48:09,899] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 96.16666666666666, 1.0, 2.0, 0.3827724543523893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 574503.0928234248, 574503.0928234248, 172321.7144258202], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1043400.0000, 
sim time next is 1044000.0000, 
raw observation next is [22.6, 96.0, 1.0, 2.0, 0.3844018335886054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576693.127541197, 576693.1275411964, 172509.1392122945], 
processed observation next is [1.0, 0.08695652173913043, 0.27014218009478685, 0.96, 1.0, 1.0, 0.2583154621549463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16019253542811027, 0.16019253542811013, 0.2574763271825291], 
reward next is 0.7425, 
noisyNet noise sample is [array([-1.4878114], dtype=float32), 0.3429248]. 
=============================================
[2019-03-26 20:48:09,920] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.184845]
 [76.22395 ]
 [76.246025]
 [76.237114]
 [76.19368 ]], R is [[76.34363556]
 [76.32300568]
 [76.30276489]
 [76.28274536]
 [76.26292419]].
[2019-03-26 20:48:11,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.410397e-18 0.000000e+00], sum to 1.0000
[2019-03-26 20:48:11,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2940
[2019-03-26 20:48:11,510] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 80.0, 1.0, 2.0, 0.2984134135397092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475912.278578721, 475912.278578721, 165341.1375158966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 847800.0000, 
sim time next is 848400.0000, 
raw observation next is [22.33333333333334, 80.33333333333334, 1.0, 2.0, 0.297552861713451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474741.94150616, 474741.94150616, 165261.1024417164], 
processed observation next is [0.0, 0.8260869565217391, 0.2575039494470777, 0.8033333333333335, 1.0, 1.0, 0.15367814664271207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1318727615294889, 0.1318727615294889, 0.24665836185330803], 
reward next is 0.7533, 
noisyNet noise sample is [array([1.5443462], dtype=float32), 0.503593]. 
=============================================
[2019-03-26 20:48:17,219] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3137478e-12 0.0000000e+00], sum to 1.0000
[2019-03-26 20:48:17,234] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5033
[2019-03-26 20:48:17,238] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 95.5, 1.0, 2.0, 0.5313514610631803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 822365.051914735, 822365.0519147357, 198156.9302293351], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 999000.0000, 
sim time next is 999600.0000, 
raw observation next is [21.63333333333333, 95.66666666666667, 1.0, 2.0, 0.5612675163992599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868538.8901138138, 868538.8901138138, 203809.6542918139], 
processed observation next is [1.0, 0.5652173913043478, 0.2243285939968403, 0.9566666666666667, 1.0, 1.0, 0.4714066462641685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24126080280939272, 0.24126080280939272, 0.30419351386837895], 
reward next is 0.6958, 
noisyNet noise sample is [array([0.4315348], dtype=float32), 0.85519195]. 
=============================================
[2019-03-26 20:48:20,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.4609730e-17 7.1355104e-38], sum to 1.0000
[2019-03-26 20:48:20,584] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3672
[2019-03-26 20:48:20,589] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 95.0, 1.0, 2.0, 0.3432621314945186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529967.3369254038, 529967.3369254038, 169034.8521152187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 954600.0000, 
sim time next is 955200.0000, 
raw observation next is [21.8, 95.0, 1.0, 2.0, 0.3426359610186044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 528995.1205086668, 528995.1205086668, 168956.2888355418], 
processed observation next is [1.0, 0.043478260869565216, 0.23222748815165886, 0.95, 1.0, 1.0, 0.20799513375735468, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14694308903018524, 0.14694308903018524, 0.2521735654261818], 
reward next is 0.7478, 
noisyNet noise sample is [array([0.25605014], dtype=float32), 2.1746786]. 
=============================================
[2019-03-26 20:48:21,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.5313474e-14 4.7002124e-37], sum to 1.0000
[2019-03-26 20:48:21,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5710
[2019-03-26 20:48:21,852] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 76.66666666666667, 1.0, 2.0, 0.9627624593586369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1386511.577371322, 1386511.577371322, 293996.219844892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [26.6, 75.83333333333333, 1.0, 2.0, 0.9844087125408639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1413843.379648927, 1413843.379648927, 300052.910774734], 
processed observation next is [1.0, 0.391304347826087, 0.4597156398104266, 0.7583333333333333, 1.0, 1.0, 0.9812153163142939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3927342721247019, 0.3927342721247019, 0.44784016533542387], 
reward next is 0.5522, 
noisyNet noise sample is [array([-0.6117316], dtype=float32), -1.8173692]. 
=============================================
[2019-03-26 20:48:21,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.26132 ]
 [68.42023 ]
 [68.409195]
 [68.7366  ]
 [68.63417 ]], R is [[68.06638336]
 [67.9469223 ]
 [67.83990479]
 [67.73355103]
 [67.6468811 ]].
[2019-03-26 20:48:22,202] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 20:48:22,204] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:48:22,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:48:22,205] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:48:22,206] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:48:22,206] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:48:22,206] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:48:22,207] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:48:22,207] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:48:22,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:48:22,209] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:48:22,229] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-03-26 20:48:22,249] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-03-26 20:48:22,250] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-03-26 20:48:22,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-03-26 20:48:22,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-03-26 20:48:29,618] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:29,620] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.50298993, 76.58616857166666, 1.0, 2.0, 0.2545803065270565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420117.6473921646, 420117.6473921652, 161321.4690810231]
[2019-03-26 20:48:29,620] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:48:29,624] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.189031e-16 0.000000e+00], sampled 0.7281626307356931
[2019-03-26 20:48:34,000] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:34,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.2, 60.66666666666667, 1.0, 2.0, 0.4601105387390205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 642916.1243251144, 642916.124325115, 177583.0442719002]
[2019-03-26 20:48:34,004] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:48:34,007] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3570574e-17 0.0000000e+00], sampled 0.16971112990162251
[2019-03-26 20:48:52,506] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:52,507] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.01666666666667, 92.83333333333333, 1.0, 2.0, 0.4940152579731356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690306.851622809, 690306.8516228095, 182662.9616250041]
[2019-03-26 20:48:52,508] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:48:52,511] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6725261e-16 0.0000000e+00], sampled 0.10301151355553562
[2019-03-26 20:48:52,840] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:52,842] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.21828182, 100.0, 1.0, 2.0, 0.4980262206956388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695913.3626489796, 695913.3626489802, 183286.706516056]
[2019-03-26 20:48:52,843] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:48:52,846] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7743536e-16 0.0000000e+00], sampled 0.8560746811137337
[2019-03-26 20:48:53,986] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:53,987] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.51249777, 94.93504324, 1.0, 2.0, 0.5536077780800014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773608.134354638, 773608.1343546386, 192421.0612007734]
[2019-03-26 20:48:53,990] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:48:53,992] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.10441006e-16
 0.00000000e+00], sampled 0.7533200882559584
[2019-03-26 20:48:57,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:48:57,446] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.5, 94.0, 1.0, 2.0, 0.3827968489730174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580759.6817606973, 580759.6817606973, 173064.4348713016]
[2019-03-26 20:48:57,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:48:57,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9901025e-16 0.0000000e+00], sampled 0.6627639496131965
[2019-03-26 20:49:03,076] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:03,077] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.95, 52.5, 1.0, 2.0, 0.9321104843129363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1302849.965528699, 1302849.965528698, 278921.5508193385]
[2019-03-26 20:49:03,079] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:49:03,081] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.1512765e-16 6.1598827e-38], sampled 0.4460868222672676
[2019-03-26 20:49:08,620] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:08,622] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.73333333333333, 69.83333333333333, 1.0, 2.0, 0.5636859488495742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787696.5306555631, 787696.5306555631, 194175.3103084056]
[2019-03-26 20:49:08,623] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:49:08,625] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.0447924e-17 0.0000000e+00], sampled 0.34448299401706706
[2019-03-26 20:49:08,880] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:08,881] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.00084598833333, 71.49784611833333, 1.0, 2.0, 0.6036793558927813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 843605.6382344458, 843605.6382344452, 201434.1778937926]
[2019-03-26 20:49:08,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:49:08,886] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9880538e-16 0.0000000e+00], sampled 0.5973392389695911
[2019-03-26 20:49:10,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:10,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.981605445, 90.67264947000001, 1.0, 2.0, 0.6113568857929168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854338.8401109033, 854338.8401109033, 202878.7201904071]
[2019-03-26 20:49:10,876] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:49:10,882] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6180222e-16 0.0000000e+00], sampled 0.38711450814075143
[2019-03-26 20:49:18,583] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:18,585] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.66666666666666, 72.33333333333333, 1.0, 2.0, 0.605403271208467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846015.6657879984, 846015.665787999, 201757.0875099954]
[2019-03-26 20:49:18,586] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:49:18,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.998388e-17 0.000000e+00], sampled 0.8861521916165268
[2019-03-26 20:49:34,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:34,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.66666666666667, 79.66666666666666, 1.0, 2.0, 0.5895464429912006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823848.081840867, 823848.081840867, 198813.9713125009]
[2019-03-26 20:49:34,504] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:49:34,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.05059224e-16
 0.00000000e+00], sampled 0.5444993778490167
[2019-03-26 20:49:38,513] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:38,516] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.74060128333333, 54.20715804333334, 1.0, 2.0, 0.5994614429285191, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.923756716247718, 6.9112, 168.9127863330956, 1676079.297823491, 1667171.140123388, 364469.1601440799]
[2019-03-26 20:49:38,516] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:49:38,521] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5747751e-16 0.0000000e+00], sampled 0.37945334440290635
[2019-03-26 20:49:38,522] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1676079.297823491 W.
[2019-03-26 20:49:44,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:44,492] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.36666666666667, 94.0, 1.0, 2.0, 0.6231605543561598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870840.6035108891, 870840.6035108891, 205139.630513438]
[2019-03-26 20:49:44,494] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:49:44,497] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5043773e-17 0.0000000e+00], sampled 0.6853700292454169
[2019-03-26 20:49:53,197] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:53,199] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.8, 76.0, 1.0, 2.0, 0.5391995524211833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753467.0267013926, 753467.0267013926, 189965.0309933735]
[2019-03-26 20:49:53,200] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:49:53,203] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6334304e-17 0.0000000e+00], sampled 0.1351582658649817
[2019-03-26 20:49:55,330] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.040818956]
[2019-03-26 20:49:55,330] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.35, 73.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.396318442155311, 6.9112, 168.9101492845756, 1798144.647011047, 1453990.647373399, 311352.968396439]
[2019-03-26 20:49:55,332] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:49:55,334] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2852287e-14 1.5442491e-34], sampled 0.31489827481407684
[2019-03-26 20:49:55,336] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1798144.647011047 W.
[2019-03-26 20:50:16,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164676497.9081 1778.0000
[2019-03-26 20:50:16,922] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9641 2779935595.8501 933.0000
[2019-03-26 20:50:17,099] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 20:50:17,140] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 20:50:17,169] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 20:50:18,185] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 875000, evaluation results [875000.0, 7876.615402862808, 3164676497.9080544, 1778.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8658.964140262613, 2779935595.8500915, 933.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 20:50:32,762] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.374815e-12 1.298336e-34], sum to 1.0000
[2019-03-26 20:50:32,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2940
[2019-03-26 20:50:32,775] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 94.33333333333334, 1.0, 2.0, 0.3727137809671536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577912.448543163, 577912.448543163, 173118.2213656409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1228200.0000, 
sim time next is 1228800.0000, 
raw observation next is [21.9, 93.66666666666667, 1.0, 2.0, 0.3507683142374582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542852.3509460131, 542852.3509460137, 170122.107738015], 
processed observation next is [1.0, 0.21739130434782608, 0.23696682464454974, 0.9366666666666668, 1.0, 1.0, 0.21779314968368457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15079231970722587, 0.15079231970722604, 0.25391359363882837], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.2679654], dtype=float32), -0.49525914]. 
=============================================
[2019-03-26 20:50:33,640] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5085538e-16 3.4523656e-37], sum to 1.0000
[2019-03-26 20:50:33,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3286
[2019-03-26 20:50:33,652] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 90.0, 1.0, 2.0, 0.338512519037039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 527088.408291667, 527088.4082916677, 168931.7885704193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1216800.0000, 
sim time next is 1217400.0000, 
raw observation next is [22.06666666666667, 90.33333333333333, 1.0, 2.0, 0.4521071560793938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 704014.5907130953, 704014.5907130947, 185005.6770500243], 
processed observation next is [1.0, 0.08695652173913043, 0.2448657187993683, 0.9033333333333333, 1.0, 1.0, 0.3398881398546913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19555960853141535, 0.19555960853141519, 0.27612787619406615], 
reward next is 0.7239, 
noisyNet noise sample is [array([-0.64272594], dtype=float32), -0.9665392]. 
=============================================
[2019-03-26 20:50:40,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6844282e-17 2.2663040e-36], sum to 1.0000
[2019-03-26 20:50:40,263] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7023
[2019-03-26 20:50:40,269] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 85.0, 1.0, 2.0, 0.808003420546072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1225805.69447403, 1225805.694474031, 258861.4250405442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1593600.0000, 
sim time next is 1594200.0000, 
raw observation next is [23.68333333333333, 85.0, 1.0, 2.0, 0.8268311027735775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1253689.908224907, 1253689.908224907, 263925.0723206759], 
processed observation next is [1.0, 0.43478260869565216, 0.32148499210110576, 0.85, 1.0, 1.0, 0.791362774425997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34824719672914084, 0.34824719672914084, 0.3939180183890685], 
reward next is 0.6061, 
noisyNet noise sample is [array([1.4766549], dtype=float32), 1.9703285]. 
=============================================
[2019-03-26 20:50:44,661] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4450328e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 20:50:44,668] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6871
[2019-03-26 20:50:44,674] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 98.0, 1.0, 2.0, 0.4341939420624599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619786.8178648998, 619786.817864899, 175622.3809237987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1668600.0000, 
sim time next is 1669200.0000, 
raw observation next is [23.66666666666667, 98.0, 1.0, 2.0, 0.4493481692295083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 177726.5250660207], 
processed observation next is [1.0, 0.30434782608695654, 0.3206951026856243, 0.98, 1.0, 1.0, 0.33656405931266065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1780447300067447, 0.1780447300067447, 0.26526347024779207], 
reward next is 0.7347, 
noisyNet noise sample is [array([-0.5884155], dtype=float32), -0.4222748]. 
=============================================
[2019-03-26 20:50:51,073] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.371815e-18 0.000000e+00], sum to 1.0000
[2019-03-26 20:50:51,083] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9799
[2019-03-26 20:50:51,091] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 86.0, 1.0, 2.0, 0.7947320367797845, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1214071.774762889, 1214071.77476289, 256314.6138661358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1771200.0000, 
sim time next is 1771800.0000, 
raw observation next is [23.23333333333333, 85.83333333333334, 1.0, 2.0, 0.7594696038246377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1162191.58322926, 1162191.58322926, 247302.023874254], 
processed observation next is [1.0, 0.5217391304347826, 0.3001579778830963, 0.8583333333333334, 1.0, 1.0, 0.7102043419573948, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32283099534146115, 0.32283099534146115, 0.36910749831978207], 
reward next is 0.6309, 
noisyNet noise sample is [array([-0.4209721], dtype=float32), -0.22312061]. 
=============================================
[2019-03-26 20:50:52,555] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8467239e-14 1.5035115e-36], sum to 1.0000
[2019-03-26 20:50:52,564] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1397
[2019-03-26 20:50:52,573] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 85.0, 1.0, 2.0, 0.7575287462398848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1135957.378612038, 1135957.378612038, 244002.6636194027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1603200.0000, 
sim time next is 1603800.0000, 
raw observation next is [24.05, 85.0, 1.0, 2.0, 0.7897740726728175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1183587.757241845, 1183587.757241845, 252176.5898389725], 
processed observation next is [1.0, 0.5652173913043478, 0.3388625592417062, 0.85, 1.0, 1.0, 0.7467157502082139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3287743770116236, 0.3287743770116236, 0.3763829699089142], 
reward next is 0.6236, 
noisyNet noise sample is [array([0.69933957], dtype=float32), 1.0350108]. 
=============================================
[2019-03-26 20:50:53,764] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.020006e-16 0.000000e+00], sum to 1.0000
[2019-03-26 20:50:53,774] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8634
[2019-03-26 20:50:53,779] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 84.5, 1.0, 2.0, 0.3581996044590471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549699.2639673849, 549699.2639673844, 170561.4200263185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1542600.0000, 
sim time next is 1543200.0000, 
raw observation next is [23.26666666666667, 85.0, 1.0, 2.0, 0.3581687164955957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549893.545817771, 549893.5458177704, 170584.802836403], 
processed observation next is [0.0, 0.8695652173913043, 0.3017377567140602, 0.85, 1.0, 1.0, 0.2267092969826454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15274820717160306, 0.1527482071716029, 0.2546041833379149], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.5868691], dtype=float32), 1.0929424]. 
=============================================
[2019-03-26 20:50:54,077] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.801082e-20 0.000000e+00], sum to 1.0000
[2019-03-26 20:50:54,087] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6563
[2019-03-26 20:50:54,091] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 91.0, 1.0, 2.0, 0.3421657492197361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531002.4731346571, 531002.4731346571, 169197.6043721182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1551600.0000, 
sim time next is 1552200.0000, 
raw observation next is [22.06666666666667, 91.00000000000001, 1.0, 2.0, 0.3411531859773741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529851.4239202309, 529851.4239202302, 169116.6463757893], 
processed observation next is [0.0, 1.0, 0.2448657187993683, 0.9100000000000001, 1.0, 1.0, 0.20620865780406517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14718095108895304, 0.14718095108895285, 0.2524129050384915], 
reward next is 0.7476, 
noisyNet noise sample is [array([1.6738191], dtype=float32), -0.080965795]. 
=============================================
[2019-03-26 20:50:57,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1223022e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 20:50:57,284] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6681
[2019-03-26 20:50:57,290] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.41666666666666, 94.33333333333334, 1.0, 2.0, 0.3335875844193171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521827.7258664079, 521827.7258664072, 168577.0882926433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1807800.0000, 
sim time next is 1808400.0000, 
raw observation next is [21.43333333333333, 94.66666666666667, 1.0, 2.0, 0.3361926734803173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525136.4967936483, 525136.4967936483, 168819.4305102034], 
processed observation next is [1.0, 0.9565217391304348, 0.21484992101105835, 0.9466666666666668, 1.0, 1.0, 0.20023213672327383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14587124910934676, 0.14587124910934676, 0.25196929926896033], 
reward next is 0.7480, 
noisyNet noise sample is [array([0.09911839], dtype=float32), 0.2131776]. 
=============================================
[2019-03-26 20:50:59,752] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.549204e-16 0.000000e+00], sum to 1.0000
[2019-03-26 20:50:59,761] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7658
[2019-03-26 20:50:59,767] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 98.0, 1.0, 2.0, 0.4211168896421672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614692.7642501778, 614692.7642501778, 175530.9628274303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1641000.0000, 
sim time next is 1641600.0000, 
raw observation next is [23.1, 98.0, 1.0, 2.0, 0.4209252236900972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614413.0298833767, 614413.0298833773, 175504.1799133219], 
processed observation next is [1.0, 0.0, 0.2938388625592418, 0.98, 1.0, 1.0, 0.30231954661457494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17067028607871573, 0.1706702860787159, 0.2619465371840625], 
reward next is 0.7381, 
noisyNet noise sample is [array([0.1889677], dtype=float32), 0.36179385]. 
=============================================
[2019-03-26 20:51:00,942] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0652615e-17 3.0203543e-36], sum to 1.0000
[2019-03-26 20:51:00,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0180
[2019-03-26 20:51:00,956] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.28333333333333, 94.66666666666667, 1.0, 2.0, 0.9093030040862334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1285945.528602892, 1285945.528602891, 274744.5653379023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1673400.0000, 
sim time next is 1674000.0000, 
raw observation next is [24.4, 94.0, 1.0, 2.0, 0.9075780463729445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1281737.819993569, 1281737.819993569, 273997.0819213995], 
processed observation next is [1.0, 0.391304347826087, 0.3554502369668246, 0.94, 1.0, 1.0, 0.8886482486421018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35603828333154697, 0.35603828333154697, 0.40895086853940227], 
reward next is 0.5910, 
noisyNet noise sample is [array([-0.5101355], dtype=float32), 0.35615623]. 
=============================================
[2019-03-26 20:51:00,969] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[63.93846 ]
 [64.254684]
 [64.75771 ]
 [65.876656]
 [67.99531 ]], R is [[63.89787674]
 [63.84883499]
 [63.80766296]
 [63.77772522]
 [63.78089142]].
[2019-03-26 20:51:06,251] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8247850e-13 4.5021543e-35], sum to 1.0000
[2019-03-26 20:51:06,260] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1698
[2019-03-26 20:51:06,268] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 93.0, 1.0, 2.0, 0.4871256155342776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688954.6414912388, 688954.6414912394, 182666.2265049748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1749600.0000, 
sim time next is 1750200.0000, 
raw observation next is [24.56666666666667, 92.66666666666667, 1.0, 2.0, 0.5490223477861003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775670.7534867938, 775670.7534867938, 192719.2377094287], 
processed observation next is [1.0, 0.2608695652173913, 0.3633491311216432, 0.9266666666666667, 1.0, 1.0, 0.45665343106759065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21546409819077605, 0.21546409819077605, 0.2876406532976548], 
reward next is 0.7124, 
noisyNet noise sample is [array([-0.9668242], dtype=float32), 0.45623383]. 
=============================================
[2019-03-26 20:51:06,574] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7421443e-16 1.8462553e-37], sum to 1.0000
[2019-03-26 20:51:06,587] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6230
[2019-03-26 20:51:06,593] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.21666666666667, 87.00000000000001, 1.0, 2.0, 0.8346412022571453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1228578.621944959, 1228578.621944959, 261279.0360663521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1764600.0000, 
sim time next is 1765200.0000, 
raw observation next is [24.13333333333334, 86.0, 1.0, 2.0, 0.8258029945387197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1225044.500419657, 1225044.500419656, 260161.9662070169], 
processed observation next is [1.0, 0.43478260869565216, 0.3428120063191157, 0.86, 1.0, 1.0, 0.7901240898056864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3402901390054603, 0.34029013900546, 0.3883014421000252], 
reward next is 0.6117, 
noisyNet noise sample is [array([-0.20183305], dtype=float32), -1.4340891]. 
=============================================
[2019-03-26 20:51:13,858] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 20:51:13,863] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:51:13,863] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:51:13,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:51:13,865] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:51:13,866] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:51:13,865] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:51:13,868] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:51:13,868] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:51:13,874] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:51:13,869] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:51:13,886] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-03-26 20:51:13,908] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-03-26 20:51:13,930] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-03-26 20:51:13,930] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-03-26 20:51:13,931] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-03-26 20:51:40,898] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:51:40,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.91429402833333, 94.62019625166667, 1.0, 2.0, 0.3630646532544795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 559862.2268624716, 559862.226862471, 171493.5377069724]
[2019-03-26 20:51:40,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:51:40,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.526329e-19 0.000000e+00], sampled 0.17483022835509943
[2019-03-26 20:51:43,087] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:51:43,090] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.48410472333333, 97.59010459666666, 1.0, 2.0, 0.4463071619200775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 645444.4072811874, 645444.4072811867, 178402.6437763749]
[2019-03-26 20:51:43,092] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:51:43,094] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.955291e-19 0.000000e+00], sampled 0.07821133653797796
[2019-03-26 20:51:46,247] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:51:46,248] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.60134944333333, 67.53188418833334, 1.0, 2.0, 0.7646276138558379, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979319124881, 6.9112, 168.9123160160255, 1965569.557331443, 1898330.101255459, 399310.7611594098]
[2019-03-26 20:51:46,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:51:46,253] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1416932e-15 3.8884835e-33], sampled 0.7234306205466308
[2019-03-26 20:51:46,254] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1965569.557331443 W.
[2019-03-26 20:52:05,883] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:05,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.82823961166667, 74.14351685333334, 1.0, 2.0, 0.463142553176667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662997.2262725452, 662997.2262725446, 180052.5771558436]
[2019-03-26 20:52:05,887] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:52:05,891] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.274818e-19 0.000000e+00], sampled 0.38324611740393644
[2019-03-26 20:52:15,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:15,747] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [40.57918976333333, 50.63723403333334, 1.0, 2.0, 0.8311606767071958, 1.0, 1.0, 0.8311606767071958, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 2324543.557115409, 2324543.557115409, 435847.6488183968]
[2019-03-26 20:52:15,748] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:52:15,752] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8418162e-15 2.8307158e-33], sampled 0.8734289893474876
[2019-03-26 20:52:15,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2324543.557115409 W.
[2019-03-26 20:52:20,677] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:20,679] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.0, 1.0, 2.0, 0.9106114270254874, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997159136116382, 6.9112, 168.9123723012221, 2169889.668476625, 2108907.509746622, 437350.0266311595]
[2019-03-26 20:52:20,681] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:52:20,684] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7700077e-15 2.6034043e-33], sampled 0.7418370311164622
[2019-03-26 20:52:20,686] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2169889.668476625 W.
[2019-03-26 20:52:24,366] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:24,367] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.63765020666666, 97.69927730333333, 1.0, 2.0, 0.7954857214408353, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981246293237, 6.9112, 168.9123160088559, 2008755.531730204, 1941514.708462699, 406746.4757577945]
[2019-03-26 20:52:24,369] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:52:24,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3250216e-16 1.1118773e-35], sampled 0.32203470746894014
[2019-03-26 20:52:24,373] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2008755.531730204 W.
[2019-03-26 20:52:37,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:37,151] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.72029103, 68.65557794333333, 1.0, 2.0, 0.5616991093511317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784919.0890029285, 784919.0890029285, 193826.6767380138]
[2019-03-26 20:52:37,154] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:52:37,155] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 8.71652e-19 0.00000e+00], sampled 0.25906161399952443
[2019-03-26 20:52:38,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:38,117] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.0, 94.0, 1.0, 2.0, 0.50570475250594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706646.4727101914, 706646.4727101908, 184493.6082041218]
[2019-03-26 20:52:38,118] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:52:38,122] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.873033e-19 0.000000e+00], sampled 0.2678143001614014
[2019-03-26 20:52:48,250] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.10304152]
[2019-03-26 20:52:48,251] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 86.0, 1.0, 2.0, 0.9533838786844598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332603.343969324, 1332603.343969324, 285043.1509984019]
[2019-03-26 20:52:48,254] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:52:48,260] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.509137e-18 0.000000e+00], sampled 0.8961723213681584
[2019-03-26 20:53:07,575] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.1300 3164560515.3511 1778.0000
[2019-03-26 20:53:07,944] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2748 2927846980.7461 1338.0000
[2019-03-26 20:53:08,131] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1751 3008171027.5419 1766.0000
[2019-03-26 20:53:08,240] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7157 2843027813.7524 1131.0000
[2019-03-26 20:53:08,472] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4620 2779798268.0133 933.0000
[2019-03-26 20:53:09,492] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 900000, evaluation results [900000.0, 7878.129995644267, 3164560515.3511353, 1778.0, 8254.274818136657, 2927846980.7461457, 1338.0, 8660.46199861073, 2779798268.0133233, 933.0, 7998.175077088882, 3008171027.5419445, 1766.0, 8496.715729453535, 2843027813.7524204, 1131.0]
[2019-03-26 20:53:13,219] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.903838e-15 0.000000e+00], sum to 1.0000
[2019-03-26 20:53:13,226] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3585
[2019-03-26 20:53:13,237] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333334, 92.66666666666666, 1.0, 2.0, 0.5262457040990456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735359.3039055692, 735359.3039055692, 187810.3494373734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2155800.0000, 
sim time next is 2156400.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.5246303416972946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733101.2680343774, 733101.2680343774, 187545.0784434577], 
processed observation next is [0.0, 1.0, 0.4360189573459717, 0.93, 1.0, 1.0, 0.4272654719244513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20363924112066037, 0.20363924112066037, 0.2799180275275488], 
reward next is 0.7201, 
noisyNet noise sample is [array([-0.3341132], dtype=float32), 0.34332603]. 
=============================================
[2019-03-26 20:53:13,299] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3844735e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 20:53:13,309] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4198
[2019-03-26 20:53:13,314] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 96.66666666666666, 1.0, 2.0, 0.4587731826357281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648564.1280802464, 648564.128080247, 178359.3039740096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2086800.0000, 
sim time next is 2087400.0000, 
raw observation next is [24.01666666666667, 96.83333333333334, 1.0, 2.0, 0.4598223004457416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649923.4768743078, 649923.4768743071, 178497.0319980301], 
processed observation next is [0.0, 0.13043478260869565, 0.33728278041074267, 0.9683333333333334, 1.0, 1.0, 0.34918349451294173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18053429913175217, 0.18053429913175198, 0.26641348059407477], 
reward next is 0.7336, 
noisyNet noise sample is [array([-0.88588285], dtype=float32), -0.44649592]. 
=============================================
[2019-03-26 20:53:13,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0424626e-34 1.0000000e+00 4.9073087e-36 2.4140060e-13 1.5978195e-29], sum to 1.0000
[2019-03-26 20:53:13,574] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0260
[2019-03-26 20:53:13,581] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 79.66666666666667, 1.0, 2.0, 0.5835217977296676, 1.0, 2.0, 0.5835217977296676, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1631466.100810756, 1631466.100810756, 328231.2279278962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1956000.0000, 
sim time next is 1956600.0000, 
raw observation next is [26.1, 80.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.893439996034282, 6.9112, 168.907467887197, 2165089.991872842, 1468277.775404582, 313603.690857724], 
processed observation next is [1.0, 0.6521739130434783, 0.4360189573459717, 0.805, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.09822399960342816, 0.0, 0.8294129934995211, 0.601413886631345, 0.4078549376123839, 0.4680652102354089], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3585356], dtype=float32), -1.6747371]. 
=============================================
[2019-03-26 20:53:18,355] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0847493e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 20:53:18,370] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2549
[2019-03-26 20:53:18,379] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2349110.924547573 W.
[2019-03-26 20:53:18,383] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.86666666666667, 65.0, 1.0, 2.0, 0.559950864149851, 1.0, 2.0, 0.559950864149851, 1.0, 1.0, 0.9724494724478221, 6.9112, 6.9112, 170.5573041426782, 2349110.924547573, 2349110.924547573, 459078.5804513382], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2298000.0000, 
sim time next is 2298600.0000, 
raw observation next is [31.9, 65.0, 1.0, 2.0, 0.8478019341463049, 1.0, 2.0, 0.8478019341463049, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2371158.453328004, 2371158.453328003, 443814.997313189], 
processed observation next is [1.0, 0.6086956521739131, 0.7109004739336492, 0.65, 1.0, 1.0, 0.8166288363208493, 1.0, 1.0, 0.8166288363208493, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6586551259244456, 0.6586551259244453, 0.6624104437510283], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7969377], dtype=float32), -0.7053087]. 
=============================================
[2019-03-26 20:53:23,369] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1308871e-15 3.5203725e-32], sum to 1.0000
[2019-03-26 20:53:23,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9335
[2019-03-26 20:53:23,382] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.1, 77.0, 1.0, 2.0, 0.5793113965352257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809539.9016293401, 809539.9016293401, 196955.0082168444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2410200.0000, 
sim time next is 2410800.0000, 
raw observation next is [30.0, 77.33333333333334, 1.0, 2.0, 0.5780193806038082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807733.7294120664, 807733.729412067, 196722.4401446359], 
processed observation next is [1.0, 0.9130434782608695, 0.6208530805687204, 0.7733333333333334, 1.0, 1.0, 0.4915896151853111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22437048039224067, 0.22437048039224083, 0.2936155823054267], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.40966868], dtype=float32), 1.680325]. 
=============================================
[2019-03-26 20:53:25,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7462326e-31 1.0000000e+00 1.1399182e-34 2.0759616e-12 3.8371703e-28], sum to 1.0000
[2019-03-26 20:53:25,340] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2055
[2019-03-26 20:53:25,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2058183.014306735 W.
[2019-03-26 20:53:25,357] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.7, 80.0, 1.0, 2.0, 0.8308020888640424, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.992143855976874, 6.9112, 168.9115868268401, 2058183.014306735, 2000759.1226476, 416112.479292169], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2191200.0000, 
sim time next is 2191800.0000, 
raw observation next is [28.8, 79.5, 1.0, 2.0, 0.7732730385611871, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993006845986092, 6.9112, 168.9124036376243, 1977668.665799124, 1919632.263643125, 401835.6416177846], 
processed observation next is [1.0, 0.34782608695652173, 0.5639810426540285, 0.795, 1.0, 1.0, 0.7268349862182977, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008180684598609211, 0.0, 0.8294372302937155, 0.5493524071664233, 0.5332311843453125, 0.5997546889817681], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.3559738], dtype=float32), 0.44002816]. 
=============================================
[2019-03-26 20:53:26,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2178004e-16 2.3344298e-36], sum to 1.0000
[2019-03-26 20:53:26,277] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6341
[2019-03-26 20:53:26,282] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.45, 94.0, 1.0, 2.0, 0.5063128050034075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707496.4177415373, 707496.4177415373, 184590.9725924404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2165400.0000, 
sim time next is 2166000.0000, 
raw observation next is [25.4, 94.0, 1.0, 2.0, 0.5054614807527384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706306.4239431076, 706306.4239431082, 184456.0371933277], 
processed observation next is [1.0, 0.043478260869565216, 0.4028436018957346, 0.94, 1.0, 1.0, 0.40417045873823904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19619622887308544, 0.1961962288730856, 0.27530751819899657], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.01395489], dtype=float32), 0.5224322]. 
=============================================
[2019-03-26 20:53:26,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[59.01853 ]
 [59.30355 ]
 [59.50912 ]
 [59.820545]
 [59.996284]], R is [[58.77880859]
 [58.91551208]
 [59.050457  ]
 [59.18371582]
 [59.31536102]].
[2019-03-26 20:53:31,107] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7687586e-36 1.0000000e+00 5.0429161e-38 3.5263974e-15 5.3116313e-30], sum to 1.0000
[2019-03-26 20:53:31,119] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0167
[2019-03-26 20:53:31,126] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 85.33333333333334, 1.0, 2.0, 0.5162732218746887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721419.3367865313, 721419.3367865313, 186184.9187354792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2247600.0000, 
sim time next is 2248200.0000, 
raw observation next is [26.9, 85.5, 1.0, 2.0, 0.5158807715841106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720870.7565319655, 720870.7565319655, 186121.5643318131], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.855, 1.0, 1.0, 0.4167238211856754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20024187681443484, 0.20024187681443484, 0.27779337959972106], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.05619371], dtype=float32), 0.77989596]. 
=============================================
[2019-03-26 20:53:42,106] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8270017e-34 1.0000000e+00 3.4255063e-33 3.7818134e-12 3.6151021e-26], sum to 1.0000
[2019-03-26 20:53:42,115] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3982
[2019-03-26 20:53:42,121] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 83.33333333333334, 1.0, 2.0, 0.72440626249048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1012394.649632712, 1012394.649632711, 226201.2772083527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2436000.0000, 
sim time next is 2436600.0000, 
raw observation next is [27.65, 83.66666666666666, 1.0, 2.0, 0.7232866737916072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010829.222722952, 1010829.222722953, 225951.7114248287], 
processed observation next is [1.0, 0.17391304347826086, 0.509478672985782, 0.8366666666666666, 1.0, 1.0, 0.6666104503513339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28078589520082, 0.28078589520082026, 0.33724136033556523], 
reward next is 0.6628, 
noisyNet noise sample is [array([0.11625277], dtype=float32), -0.30011794]. 
=============================================
[2019-03-26 20:53:44,232] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1895551e-24 1.0000000e+00 3.2260798e-24 2.6495549e-08 8.1626878e-18], sum to 1.0000
[2019-03-26 20:53:44,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1365
[2019-03-26 20:53:44,253] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2699822.083478765 W.
[2019-03-26 20:53:44,256] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.26666666666667, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.666592849268682, 6.9112, 168.9031309717789, 2699822.083478765, 1454558.420505565, 310093.9101273886], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2467200.0000, 
sim time next is 2467800.0000, 
raw observation next is [26.3, 89.0, 1.0, 2.0, 0.519835632274016, 1.0, 1.0, 0.519835632274016, 1.0, 1.0, 0.8874504807507809, 6.911199999999999, 6.9112, 170.5573041426782, 2180667.668756892, 2180667.668756892, 426213.747204425], 
processed observation next is [1.0, 0.5652173913043478, 0.4454976303317536, 0.89, 1.0, 1.0, 0.42148871358315176, 1.0, 0.5, 0.42148871358315176, 1.0, 0.5, 0.8627444887204644, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6057410190991367, 0.6057410190991367, 0.6361399212006343], 
reward next is 0.3639, 
noisyNet noise sample is [array([0.6397154], dtype=float32), -0.5576001]. 
=============================================
[2019-03-26 20:53:52,153] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.8669115e-13 4.9687925e-36], sum to 1.0000
[2019-03-26 20:53:52,161] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3926
[2019-03-26 20:53:52,164] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5835624193741584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898725.3745763114, 898725.3745763114, 207772.7548374229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2911800.0000, 
sim time next is 2912400.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6698743651496316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031683.842355097, 1031683.842355097, 226323.6684308749], 
processed observation next is [1.0, 0.7391304347826086, 0.2417061611374408, 0.94, 1.0, 1.0, 0.6022582712646164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.286578845098638, 0.286578845098638, 0.33779652004608196], 
reward next is 0.6622, 
noisyNet noise sample is [array([0.08444927], dtype=float32), -0.99565816]. 
=============================================
[2019-03-26 20:54:03,254] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9401607e-13 3.0731280e-30], sum to 1.0000
[2019-03-26 20:54:03,265] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6691
[2019-03-26 20:54:03,274] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.7586531783133827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1117487.748113957, 1117487.748113958, 241722.7986918934], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2823600.0000, 
sim time next is 2824200.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.7713855513912249, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1136277.973138236, 1136277.973138236, 244897.0288183452], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.7245609052906323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31563277031617665, 0.31563277031617665, 0.3655179534602167], 
reward next is 0.6345, 
noisyNet noise sample is [array([-0.25262094], dtype=float32), 1.0744914]. 
=============================================
[2019-03-26 20:54:05,290] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 20:54:05,291] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:54:05,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:54:05,292] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:54:05,293] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:54:05,293] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:54:05,294] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:54:05,295] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:54:05,298] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:54:05,295] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:54:05,301] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:54:05,318] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-03-26 20:54:05,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-03-26 20:54:05,341] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-03-26 20:54:05,363] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-03-26 20:54:05,382] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-03-26 20:54:44,363] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.058863834]
[2019-03-26 20:54:44,365] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.76541263, 88.84586873, 1.0, 2.0, 0.3376283390598541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530865.7923827062, 530865.7923827068, 169350.104611596]
[2019-03-26 20:54:44,367] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:54:44,370] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.6471326e-18 0.0000000e+00], sampled 0.4246819626993533
[2019-03-26 20:54:44,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.058863834]
[2019-03-26 20:54:44,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 96.0, 1.0, 2.0, 0.3579552349395392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548122.8818235274, 548122.8818235274, 170393.262155754]
[2019-03-26 20:54:44,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:54:44,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2584541e-17 2.0386729e-38], sampled 0.3451459925228293
[2019-03-26 20:55:05,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.058863834]
[2019-03-26 20:55:05,145] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.6, 69.0, 1.0, 2.0, 0.9808912634278423, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993329508321703, 6.9112, 168.91240143774, 2268256.167853042, 2209990.859439059, 457959.6308338996]
[2019-03-26 20:55:05,147] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:55:05,150] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2026896e-27 1.0000000e+00 7.7452934e-29 4.5079451e-09 1.6461333e-21], sampled 0.14150490330309584
[2019-03-26 20:55:05,152] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2268256.167853042 W.
[2019-03-26 20:55:26,404] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.058863834]
[2019-03-26 20:55:26,405] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.93333333333334, 64.0, 1.0, 2.0, 0.5579368814429562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779659.8171915929, 779659.8171915929, 193171.5707137766]
[2019-03-26 20:55:26,406] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:55:26,410] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8182475e-16 3.9966819e-36], sampled 0.7000390520135414
[2019-03-26 20:55:38,883] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.058863834]
[2019-03-26 20:55:38,886] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.8, 66.0, 1.0, 2.0, 0.7674160855249207, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.971263102317703, 6.9112, 168.9125490791335, 1969471.95343955, 1926861.224223597, 401119.8413126364]
[2019-03-26 20:55:38,888] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:55:38,890] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3739370e-30 1.0000000e+00 6.3954204e-32 2.5708669e-10 5.7269176e-24], sampled 0.9985308618125518
[2019-03-26 20:55:38,892] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1969471.95343955 W.
[2019-03-26 20:55:59,126] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.9734 2928048931.2162 1338.0000
[2019-03-26 20:55:59,338] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9641 2779935595.8501 933.0000
[2019-03-26 20:55:59,606] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8585 3164735026.8284 1778.0000
[2019-03-26 20:55:59,790] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 20:55:59,946] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 20:56:00,962] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 925000, evaluation results [925000.0, 7875.858472249107, 3164735026.8283834, 1778.0, 8253.973399524697, 2928048931.2161584, 1338.0, 8658.964140262613, 2779935595.8500915, 933.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 20:56:05,494] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0761644e-15 6.7156442e-34], sum to 1.0000
[2019-03-26 20:56:05,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9736
[2019-03-26 20:56:05,511] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 96.0, 1.0, 2.0, 0.3078368523663025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488082.7785863078, 488082.7785863078, 166169.8120022653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2947200.0000, 
sim time next is 2947800.0000, 
raw observation next is [20.83333333333333, 95.0, 1.0, 2.0, 0.3073100318811158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486690.1820198278, 486690.1820198271, 166057.2253637769], 
processed observation next is [1.0, 0.08695652173913043, 0.1864139020537123, 0.95, 1.0, 1.0, 0.1654337733507419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13519171722772994, 0.13519171722772974, 0.24784660502056255], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.1873993], dtype=float32), 1.2474152]. 
=============================================
[2019-03-26 20:56:13,609] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1627582e-15 4.5145058e-26], sum to 1.0000
[2019-03-26 20:56:13,615] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1161
[2019-03-26 20:56:13,619] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 96.0, 1.0, 2.0, 0.7583974758994021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1123210.192958733, 1123210.192958733, 242446.031017916], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072000.0000, 
sim time next is 3072600.0000, 
raw observation next is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.97, 1.0, 1.0, 0.7998977235020032, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34147668853811947, 0.3414766885381192, 0.39003888943722104], 
reward next is 0.6100, 
noisyNet noise sample is [array([0.19414538], dtype=float32), 2.3655577]. 
=============================================
[2019-03-26 20:56:14,414] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4067360e-18 1.5328614e-36], sum to 1.0000
[2019-03-26 20:56:14,418] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4100
[2019-03-26 20:56:14,427] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4234433790790483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614557.5426876508, 614557.5426876508, 175415.638473793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3086400.0000, 
sim time next is 3087000.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4221929928478546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612799.3362681925, 612799.3362681925, 175248.3666270096], 
processed observation next is [1.0, 0.7391304347826086, 0.28909952606635075, 1.0, 1.0, 1.0, 0.30384697933476457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1702220378522757, 0.1702220378522757, 0.26156472630896954], 
reward next is 0.7384, 
noisyNet noise sample is [array([0.12564324], dtype=float32), -0.073311225]. 
=============================================
[2019-03-26 20:56:14,452] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[71.50043 ]
 [70.111176]
 [67.19161 ]
 [66.90626 ]
 [66.97671 ]], R is [[71.99541473]
 [72.01364136]
 [72.01538086]
 [71.92034912]
 [71.81757355]].
[2019-03-26 20:56:15,627] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2068236e-38 1.0000000e+00 0.0000000e+00 4.5074608e-14 2.8804405e-34], sum to 1.0000
[2019-03-26 20:56:15,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2732
[2019-03-26 20:56:15,643] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.7874252853470698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174616.704686965, 1174616.704686964, 250870.7695471696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3065400.0000, 
sim time next is 3066000.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.7852241844331339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1171331.216517576, 1171331.216517575, 250300.4449124924], 
processed observation next is [1.0, 0.4782608695652174, 0.28909952606635075, 0.94, 1.0, 1.0, 0.741233957148354, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32536978236599334, 0.32536978236599307, 0.37358275360073495], 
reward next is 0.6264, 
noisyNet noise sample is [array([-2.0072234], dtype=float32), 0.29446942]. 
=============================================
[2019-03-26 20:56:15,659] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.36302 ]
 [64.47957 ]
 [64.400536]
 [65.52795 ]
 [65.62319 ]], R is [[64.33638763]
 [64.31858826]
 [64.30464172]
 [64.27695465]
 [64.29038239]].
[2019-03-26 20:56:25,720] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7469082e-14 1.2572347e-33], sum to 1.0000
[2019-03-26 20:56:25,727] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0359
[2019-03-26 20:56:25,730] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5231976593354678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731098.595647361, 731098.595647361, 187310.149921335], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3535200.0000, 
sim time next is 3535800.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5205249772431118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727362.6020245068, 727362.6020245062, 186874.2498524237], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.42231924969049606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20204516722902965, 0.2020451672290295, 0.278916790824513], 
reward next is 0.7211, 
noisyNet noise sample is [array([-1.096182], dtype=float32), 2.0902054]. 
=============================================
[2019-03-26 20:56:30,150] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6544220e-23 9.9999905e-01 2.6851431e-24 9.2520412e-07 6.0969734e-21], sum to 1.0000
[2019-03-26 20:56:30,159] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7996
[2019-03-26 20:56:30,168] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2168884.517977794 W.
[2019-03-26 20:56:30,180] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.7755443431276663, 1.0, 2.0, 0.7755443431276663, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2168884.517977794, 2168884.517977794, 408138.5048265292], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3576600.0000, 
sim time next is 3577200.0000, 
raw observation next is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.8088594730178869, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.990286575900647, 6.9112, 168.9124852916676, 2027472.664048351, 1971366.081676289, 410624.5147499153], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.6733333333333335, 1.0, 1.0, 0.7697102084552854, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007908657590064738, 0.0, 0.8294376312524495, 0.563186851124542, 0.5476016893545247, 0.6128724100745004], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1152463], dtype=float32), 1.2560956]. 
=============================================
[2019-03-26 20:56:32,132] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3197466e-34 9.9999881e-01 0.0000000e+00 1.1491367e-06 1.2173294e-27], sum to 1.0000
[2019-03-26 20:56:32,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8650
[2019-03-26 20:56:32,148] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7310351953082055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1021663.377300856, 1021663.377300855, 227686.0590640607], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3387600.0000, 
sim time next is 3388200.0000, 
raw observation next is [26.16666666666667, 93.16666666666667, 1.0, 2.0, 0.8525567548986385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1191591.978612995, 1191591.978612995, 257219.8526596784], 
processed observation next is [1.0, 0.21739130434782608, 0.4391785150078992, 0.9316666666666668, 1.0, 1.0, 0.822357536022456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33099777183694307, 0.33099777183694307, 0.3839102278502663], 
reward next is 0.6161, 
noisyNet noise sample is [array([-1.1880823], dtype=float32), 0.26230115]. 
=============================================
[2019-03-26 20:56:33,299] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3078068e-11 3.9443503e-37], sum to 1.0000
[2019-03-26 20:56:33,310] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5380
[2019-03-26 20:56:33,314] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5377674587055347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751465.138770296, 751465.138770296, 189724.9251875314], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3364800.0000, 
sim time next is 3365400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5373605047599435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750896.2687323533, 750896.2687323539, 189656.6609110289], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4426030177830644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20858229687009816, 0.20858229687009833, 0.2830696431507894], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.33912015], dtype=float32), -0.07794951]. 
=============================================
[2019-03-26 20:56:37,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.83768834e-33 9.99993324e-01 1.42338418e-35 6.70864119e-06
 1.10967584e-26], sum to 1.0000
[2019-03-26 20:56:37,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6554
[2019-03-26 20:56:37,535] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4946573118244739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691204.310479273, 691204.3104792737, 182762.8828469088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3628800.0000, 
sim time next is 3629400.0000, 
raw observation next is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.4954583617351405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692324.0140958324, 692324.0140958324, 182887.4061905441], 
processed observation next is [1.0, 0.0, 0.5181674565560824, 0.7566666666666667, 1.0, 1.0, 0.3921185081146272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19231222613773122, 0.19231222613773122, 0.2729662778963345], 
reward next is 0.7270, 
noisyNet noise sample is [array([-0.9736486], dtype=float32), -0.53978074]. 
=============================================
[2019-03-26 20:56:41,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.8723543e-34 9.9999928e-01 1.4331342e-35 7.4514531e-07 2.0311298e-27], sum to 1.0000
[2019-03-26 20:56:41,351] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5494
[2019-03-26 20:56:41,358] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 79.0, 1.0, 2.0, 0.5347848399878199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747295.8228908767, 747295.8228908767, 189225.2850913939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [28.16666666666667, 79.0, 1.0, 2.0, 0.5292705421878953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739587.5896174719, 739587.5896174725, 188308.8996478552], 
processed observation next is [1.0, 0.9130434782608695, 0.5339652448657191, 0.79, 1.0, 1.0, 0.43285607492517497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20544099711596442, 0.20544099711596459, 0.28105805917590326], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.6389199], dtype=float32), -1.2013445]. 
=============================================
[2019-03-26 20:56:43,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0177569e-35 9.9934155e-01 5.9622517e-36 6.5848936e-04 5.0709634e-26], sum to 1.0000
[2019-03-26 20:56:43,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2361
[2019-03-26 20:56:43,142] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5512261777908358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770278.8939225607, 770278.8939225607, 192011.4572317818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5516599039562212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770885.1992047711, 770885.1992047705, 192085.9950600215], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4598312095858087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21413477755688085, 0.2141347775568807, 0.28669551501495744], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.2132891], dtype=float32), 0.21395431]. 
=============================================
[2019-03-26 20:56:52,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1013807e-17 1.7288402e-38], sum to 1.0000
[2019-03-26 20:56:52,752] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3978
[2019-03-26 20:56:52,764] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.638353093640639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892080.4529525923, 892080.4529525923, 208107.0619841013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3960600.0000, 
sim time next is 3961200.0000, 
raw observation next is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.6120671115098942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855331.7425388523, 855331.7425388523, 203015.3604700686], 
processed observation next is [0.0, 0.8695652173913043, 0.6998420221169038, 0.7233333333333334, 1.0, 1.0, 0.5326109777227641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23759215070523673, 0.23759215070523673, 0.30300800070159495], 
reward next is 0.6970, 
noisyNet noise sample is [array([-0.28675327], dtype=float32), -0.09324496]. 
=============================================
[2019-03-26 20:56:53,262] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3965066e-13 1.0783898e-32], sum to 1.0000
[2019-03-26 20:56:53,271] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4924
[2019-03-26 20:56:53,279] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 78.16666666666667, 1.0, 2.0, 0.5485338291444588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766515.270746748, 766515.2707467487, 191549.6587717301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3697800.0000, 
sim time next is 3698400.0000, 
raw observation next is [29.0, 77.33333333333334, 1.0, 2.0, 0.5458690742630228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762790.2339132758, 762790.2339132765, 191094.7013062788], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7733333333333334, 1.0, 1.0, 0.4528543063409913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21188617608702104, 0.21188617608702123, 0.2852159720989236], 
reward next is 0.7148, 
noisyNet noise sample is [array([0.4774869], dtype=float32), -0.9700335]. 
=============================================
[2019-03-26 20:56:56,880] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 20:56:56,885] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:56:56,886] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:56:56,888] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:56:56,889] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:56:56,889] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:56:56,891] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:56:56,891] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:56:56,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:56:56,895] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:56:56,895] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:56:56,913] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-03-26 20:56:56,935] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-03-26 20:56:56,958] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-03-26 20:56:56,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-03-26 20:56:57,002] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-03-26 20:57:00,069] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:00,070] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.840409835, 93.94808416000001, 1.0, 2.0, 0.3000175735058938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486221.2797052862, 486221.2797052862, 166072.9488457477]
[2019-03-26 20:57:00,071] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 20:57:00,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7266276e-21 0.0000000e+00], sampled 0.12416296116257752
[2019-03-26 20:57:08,137] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:08,140] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 56.5, 1.0, 2.0, 0.5711455446654399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928565.8693541674, 928565.8693541674, 209314.956994964]
[2019-03-26 20:57:08,141] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 20:57:08,145] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1613025e-18 7.4180989e-35], sampled 0.5718923245494687
[2019-03-26 20:57:20,169] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:20,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.05, 87.0, 1.0, 2.0, 0.4377536394675692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650053.2685138287, 650053.2685138282, 179248.7142121781]
[2019-03-26 20:57:20,173] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 20:57:20,176] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.923323e-20 3.652216e-37], sampled 0.5314611257960449
[2019-03-26 20:57:21,745] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:21,746] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.88045620666666, 77.65358253333335, 1.0, 2.0, 0.9135541574760684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1276897.410869284, 1276897.410869283, 273691.2598596346]
[2019-03-26 20:57:21,747] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:57:21,750] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0414262e-17 1.0376313e-32], sampled 0.22562224321421953
[2019-03-26 20:57:24,894] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:24,895] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.13750320666667, 93.38776799666667, 1.0, 2.0, 0.3699654810612316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 566337.2451267077, 566337.2451267083, 171939.0413159899]
[2019-03-26 20:57:24,896] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:57:24,899] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0690337e-19 1.0812215e-36], sampled 0.09826661653968805
[2019-03-26 20:57:43,558] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:57:43,560] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.87453848, 65.81312572, 1.0, 2.0, 0.6003357689427444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838931.3310163091, 838931.3310163091, 200808.9364321431]
[2019-03-26 20:57:43,561] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:57:43,565] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.1551337e-20 2.8292418e-37], sampled 0.015671983928234656
[2019-03-26 20:58:07,783] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.067322075]
[2019-03-26 20:58:07,785] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.82523647666667, 67.38149264, 1.0, 2.0, 0.7127990109451332, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597608264154, 6.9112, 168.9123160393431, 1893039.294107633, 1825802.134086335, 387438.2615936032]
[2019-03-26 20:58:07,786] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 20:58:07,787] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.3742022e-30 1.0000000e+00 2.7387121e-31 2.1005005e-12 4.5180596e-24], sampled 0.6285416744514588
[2019-03-26 20:58:07,788] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1893039.294107633 W.
[2019-03-26 20:58:50,358] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4620 2779798268.0133 933.0000
[2019-03-26 20:58:50,624] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0759 3008237470.0170 1766.0000
[2019-03-26 20:58:50,861] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 20:58:50,910] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1744 2927914232.3319 1338.0000
[2019-03-26 20:58:50,988] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.3726 3164618349.2034 1778.0000
[2019-03-26 20:58:52,002] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 950000, evaluation results [950000.0, 7877.372585301126, 3164618349.203418, 1778.0, 8254.174442635573, 2927914232.3318715, 1338.0, 8660.46199861073, 2779798268.0133233, 933.0, 7998.075909215718, 3008237470.0169654, 1766.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 20:58:54,766] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2782499e-17 9.9698228e-01 1.2748464e-18 3.0177243e-03 7.1215040e-14], sum to 1.0000
[2019-03-26 20:58:54,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4183
[2019-03-26 20:58:54,784] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2521299.44711397 W.
[2019-03-26 20:58:54,788] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 71.0, 1.0, 2.0, 0.6009535847395152, 1.0, 2.0, 0.6009535847395152, 1.0, 2.0, 1.03, 6.926553277930925, 6.9112, 170.5573041426782, 2521299.44711397, 2510301.269600633, 488325.9193986871], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4102800.0000, 
sim time next is 4103400.0000, 
raw observation next is [32.83333333333333, 71.0, 1.0, 2.0, 0.8950034023469962, 1.0, 2.0, 0.8950034023469962, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2503305.200947068, 2503305.200947068, 468786.3005976528], 
processed observation next is [1.0, 0.4782608695652174, 0.7551342812006318, 0.71, 1.0, 1.0, 0.8734980751168628, 1.0, 1.0, 0.8734980751168628, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.69536255581863, 0.69536255581863, 0.6996810456681385], 
reward next is 0.3003, 
noisyNet noise sample is [array([-0.42160872], dtype=float32), -0.5090832]. 
=============================================
[2019-03-26 20:59:02,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7696034e-19 5.9011467e-37], sum to 1.0000
[2019-03-26 20:59:02,239] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9331
[2019-03-26 20:59:02,245] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 84.0, 1.0, 2.0, 0.6003891332512326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839005.9337217159, 839005.9337217159, 200818.0299813241], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3976800.0000, 
sim time next is 3977400.0000, 
raw observation next is [29.16666666666667, 84.0, 1.0, 2.0, 0.5962188130494563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833175.8885971106, 833175.8885971106, 200043.0258720054], 
processed observation next is [1.0, 0.0, 0.581358609794629, 0.84, 1.0, 1.0, 0.5135166422282607, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23143774683253074, 0.23143774683253074, 0.2985716804059782], 
reward next is 0.7014, 
noisyNet noise sample is [array([-0.15038458], dtype=float32), -0.13123481]. 
=============================================
[2019-03-26 20:59:02,937] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0145376e-16 2.1776212e-29], sum to 1.0000
[2019-03-26 20:59:02,947] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5728
[2019-03-26 20:59:02,953] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5984022742334975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836228.3306087685, 836228.3306087685, 200449.1914506043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3945000.0000, 
sim time next is 3945600.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5985692231426372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836461.7227368458, 836461.7227368458, 200480.2081664106], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5163484616176351, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23235047853801274, 0.23235047853801274, 0.29922419129315014], 
reward next is 0.7008, 
noisyNet noise sample is [array([0.5628747], dtype=float32), -1.0718627]. 
=============================================
[2019-03-26 20:59:03,522] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5242208e-14 2.2481257e-01 3.3544271e-15 7.7518743e-01 1.1965196e-10], sum to 1.0000
[2019-03-26 20:59:03,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7363
[2019-03-26 20:59:03,534] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 77.66666666666667, 1.0, 2.0, 0.9261380621912924, 1.0, 2.0, 0.9261380621912924, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2590478.445550673, 2590478.445550673, 485961.8194006178], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4097400.0000, 
sim time next is 4098000.0000, 
raw observation next is [31.33333333333334, 76.33333333333334, 1.0, 2.0, 0.9248698547411895, 1.0, 2.0, 0.9248698547411895, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2586927.50077058, 2586927.500770579, 485250.5349826436], 
processed observation next is [1.0, 0.43478260869565216, 0.6840442338072673, 0.7633333333333334, 1.0, 1.0, 0.9094817527002284, 1.0, 1.0, 0.9094817527002284, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7185909724362722, 0.718590972436272, 0.7242545298248412], 
reward next is 0.2757, 
noisyNet noise sample is [array([-1.669502], dtype=float32), -1.0268579]. 
=============================================
[2019-03-26 20:59:03,545] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[35.08118 ]
 [34.528324]
 [33.774532]
 [33.818905]
 [33.482346]], R is [[35.47953033]
 [35.39942169]
 [35.31190491]
 [35.19473648]
 [35.08903122]].
[2019-03-26 20:59:05,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4781507e-17 9.9971861e-01 7.9521000e-18 2.8141629e-04 1.1295137e-11], sum to 1.0000
[2019-03-26 20:59:05,642] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4166
[2019-03-26 20:59:05,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1836420.864966193 W.
[2019-03-26 20:59:05,660] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.6723382087368741, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.0059735132411, 6.9112, 168.9123928016504, 1836420.864966193, 1769185.497203946, 378759.2238208535], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3990600.0000, 
sim time next is 3991200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.6229051109962863, 1.0, 1.0, 0.6229051109962863, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1741667.217727852, 1741667.217727853, 342874.2975874734], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5456688084292606, 1.0, 0.5, 0.5456688084292606, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4837964493688478, 0.483796449368848, 0.5117526829663782], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6294715], dtype=float32), -0.2203644]. 
=============================================
[2019-03-26 20:59:13,192] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.4025048e-38 1.0000000e+00 1.9118567e-38 3.1180596e-15 8.0555326e-26], sum to 1.0000
[2019-03-26 20:59:13,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9988
[2019-03-26 20:59:13,209] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.5, 1.0, 2.0, 0.6199593662311755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 866365.2535100605, 866365.2535100605, 204523.6742513632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4401000.0000, 
sim time next is 4401600.0000, 
raw observation next is [30.33333333333334, 82.33333333333334, 1.0, 2.0, 0.6188710837720963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864843.8086701048, 864843.8086701054, 204314.6587111072], 
processed observation next is [1.0, 0.9565217391304348, 0.6366508688783573, 0.8233333333333335, 1.0, 1.0, 0.5408085346651763, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24023439129725135, 0.2402343912972515, 0.30494725180762267], 
reward next is 0.6951, 
noisyNet noise sample is [array([1.0911064], dtype=float32), 1.592936]. 
=============================================
[2019-03-26 20:59:13,437] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.36901423e-27 1.00000000e+00 1.07816515e-29 5.15103693e-08
 3.38035500e-20], sum to 1.0000
[2019-03-26 20:59:13,442] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0391
[2019-03-26 20:59:13,455] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2686948.683060544 W.
[2019-03-26 20:59:13,460] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.640617743256142, 6.9112, 169.6696770914696, 2686948.683060544, 1454543.732831527, 310321.7702259737], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4155000.0000, 
sim time next is 4155600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.4585407525016811, 1.0, 1.0, 0.4585407525016811, 1.0, 1.0, 0.796333645351346, 6.9112, 6.9112, 170.5573041426782, 1923309.877128724, 1923309.877128724, 387660.8308080292], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.34763946084539893, 1.0, 0.5, 0.34763946084539893, 1.0, 0.5, 0.751626396769934, 0.0, 0.0, 0.8375144448122397, 0.5342527436468678, 0.5342527436468678, 0.5785982549373571], 
reward next is 0.4214, 
noisyNet noise sample is [array([1.6370091], dtype=float32), -0.5055656]. 
=============================================
[2019-03-26 20:59:14,744] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 3.20912e-22 9.00202e-37], sum to 1.0000
[2019-03-26 20:59:14,752] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0162
[2019-03-26 20:59:14,758] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5825627364649959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814085.1222146007, 814085.1222146007, 197542.2599234823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4141800.0000, 
sim time next is 4142400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5832246196768941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815010.4062550204, 815010.4062550204, 197662.1721715487], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.497860987562523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22639177951528344, 0.22639177951528344, 0.29501816742022197], 
reward next is 0.7050, 
noisyNet noise sample is [array([-0.6844094], dtype=float32), 0.12986974]. 
=============================================
[2019-03-26 20:59:27,782] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2448533e-10 2.2821923e-33], sum to 1.0000
[2019-03-26 20:59:27,791] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0298
[2019-03-26 20:59:27,797] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 63.0, 1.0, 2.0, 0.5382246803433589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752104.2770344032, 752104.2770344032, 189801.9792848361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4390200.0000, 
sim time next is 4390800.0000, 
raw observation next is [31.33333333333333, 63.0, 1.0, 2.0, 0.5346285395900005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747077.3355660398, 747077.3355660398, 189199.5498623884], 
processed observation next is [1.0, 0.8260869565217391, 0.6840442338072668, 0.63, 1.0, 1.0, 0.4393114934819282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20752148210167773, 0.20752148210167773, 0.28238738785431106], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.24145506], dtype=float32), -2.1847744]. 
=============================================
[2019-03-26 20:59:36,316] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.508519e-21 5.217425e-38], sum to 1.0000
[2019-03-26 20:59:36,331] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6523
[2019-03-26 20:59:36,338] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507506380853706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709164.8182007936, 709164.8182007929, 184780.0542154979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4502400.0000, 
sim time next is 4503000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5074416623186471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709074.3534734903, 709074.3534734909, 184769.774726508], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4065562196610205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19696509818708066, 0.1969650981870808, 0.27577578317389256], 
reward next is 0.7242, 
noisyNet noise sample is [array([-0.21146616], dtype=float32), -0.69140106]. 
=============================================
[2019-03-26 20:59:36,349] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[79.0237  ]
 [79.119026]
 [79.23087 ]
 [78.84491 ]
 [77.53804 ]], R is [[78.82434845]
 [78.76031494]
 [78.69684601]
 [78.63399506]
 [78.57180786]].
[2019-03-26 20:59:39,585] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1670033e-16 5.0369230e-36], sum to 1.0000
[2019-03-26 20:59:39,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4585
[2019-03-26 20:59:39,606] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 68.16666666666667, 1.0, 2.0, 0.5316097653041234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742857.4964020469, 742857.4964020469, 188697.1334090805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4557000.0000, 
sim time next is 4557600.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5325549707344602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744178.764265495, 744178.764265495, 188854.1586855058], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4368132177523617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20671632340708196, 0.20671632340708196, 0.2818718786350833], 
reward next is 0.7181, 
noisyNet noise sample is [array([-1.9174976], dtype=float32), -0.008019034]. 
=============================================
[2019-03-26 20:59:46,713] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2664955e-19 1.6591482e-02 7.8444722e-19 9.8340845e-01 8.7193409e-14], sum to 1.0000
[2019-03-26 20:59:46,727] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8425
[2019-03-26 20:59:46,731] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666666, 73.5, 1.0, 2.0, 0.791269185436653, 1.0, 1.0, 0.791269185436653, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2212905.941609609, 2212905.941609609, 415644.7708420611], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4719000.0000, 
sim time next is 4719600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.801951812887187, 1.0, 2.0, 0.801951812887187, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2242808.358467985, 2242808.358467985, 420824.4225912035], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.75, 1.0, 1.0, 0.7613877263701048, 1.0, 1.0, 0.7613877263701048, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6230023217966625, 0.6230023217966625, 0.6280961531211993], 
reward next is 0.3719, 
noisyNet noise sample is [array([-1.2079619], dtype=float32), -0.24266145]. 
=============================================
[2019-03-26 20:59:47,894] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 20:59:47,895] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 20:59:47,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:59:47,897] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 20:59:47,898] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:59:47,900] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 20:59:47,901] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 20:59:47,902] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 20:59:47,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:59:47,903] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:59:47,902] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 20:59:47,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-03-26 20:59:47,928] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-03-26 20:59:47,947] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-03-26 20:59:47,966] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-03-26 20:59:48,006] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-03-26 20:59:56,929] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 20:59:56,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.71666666666667, 57.16666666666666, 1.0, 2.0, 0.3115956537643351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508002.9362410499, 508002.9362410499, 167565.3074400358]
[2019-03-26 20:59:56,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 20:59:56,932] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3094155e-15 1.8808642e-33], sampled 0.29297441659633383
[2019-03-26 21:00:04,701] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 21:00:04,702] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.45, 91.33333333333334, 1.0, 2.0, 0.3483192195694967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560963.5639090809, 560963.5639090802, 171829.06402252]
[2019-03-26 21:00:04,705] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:00:04,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6972201e-15 3.2445158e-33], sampled 0.3632384707018498
[2019-03-26 21:00:06,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 21:00:06,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.23333333333334, 73.66666666666667, 1.0, 2.0, 0.5858543537126845, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9817998996662797, 6.911200000000001, 6.9112, 168.9126160012299, 1654672.817811926, 1654672.817811925, 351970.31968934]
[2019-03-26 21:00:06,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:00:06,415] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4669003e-31 1.0000000e+00 2.0551616e-33 2.2055767e-11 1.4379239e-24], sampled 0.3585006800153424
[2019-03-26 21:00:35,910] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 21:00:35,912] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.9, 57.0, 1.0, 2.0, 0.5184288382109958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724432.535067838, 724432.5350678375, 186533.4585425569]
[2019-03-26 21:00:35,914] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:00:35,917] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.4931640e-16 4.3060783e-34], sampled 0.9370884348590958
[2019-03-26 21:01:26,307] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 21:01:26,309] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.22249329666666, 68.61723293666667, 1.0, 2.0, 0.4743230147283076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 669332.9733435145, 669332.9733435152, 180514.4193994592]
[2019-03-26 21:01:26,310] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:01:26,315] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9818987e-15 4.4946771e-33], sampled 0.22108139868248455
[2019-03-26 21:01:41,385] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.08309895]
[2019-03-26 21:01:41,388] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.63333333333333, 68.66666666666667, 1.0, 2.0, 0.3316482792530147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522833.7477794597, 522833.7477794591, 168742.7077054101]
[2019-03-26 21:01:41,388] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:01:41,393] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1965715e-15 1.2275823e-32], sampled 0.01956185509066577
[2019-03-26 21:01:42,075] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4734 2779819857.1693 931.0000
[2019-03-26 21:01:42,459] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.5184 2927916749.3950 1338.0000
[2019-03-26 21:01:42,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 21:01:42,630] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 21:01:42,759] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.1551 3164638718.8196 1776.0000
[2019-03-26 21:01:43,774] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 975000, evaluation results [975000.0, 7877.155125466313, 3164638718.81958, 1776.0, 8254.518362806384, 2927916749.3949623, 1338.0, 8660.473395341205, 2779819857.1692877, 931.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 21:01:44,741] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6165086e-17 3.5035148e-01 5.9073087e-18 6.4964855e-01 1.8987923e-12], sum to 1.0000
[2019-03-26 21:01:44,750] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1413
[2019-03-26 21:01:44,759] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2062218.320672034 W.
[2019-03-26 21:01:44,763] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.7374395876912637, 1.0, 2.0, 0.7374395876912637, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2062218.320672034, 2062218.320672034, 390556.7156582015], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4899600.0000, 
sim time next is 4900200.0000, 
raw observation next is [29.83333333333333, 66.66666666666667, 1.0, 2.0, 0.3385061891092346, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5724356130925675, 6.9112, 6.9112, 168.912956510431, 946129.7776454339, 946129.7776454339, 233296.2228641873], 
processed observation next is [1.0, 0.7391304347826086, 0.6129541864139019, 0.6666666666666667, 1.0, 1.0, 0.20301950495088503, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.4785800159665457, 0.0, 0.0, 0.8294399451523027, 0.26281382712373164, 0.26281382712373164, 0.3482033177077422], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.22517954], dtype=float32), 0.10249162]. 
=============================================
[2019-03-26 21:01:48,970] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3055198e-23 1.0000000e+00 1.2433091e-24 2.2094400e-09 1.2521141e-17], sum to 1.0000
[2019-03-26 21:01:48,977] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2717
[2019-03-26 21:01:48,987] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1722035.484883404 W.
[2019-03-26 21:01:48,994] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 65.0, 1.0, 2.0, 0.6158846692010641, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.925641299822568, 6.9112, 168.9127898535023, 1722035.484883404, 1711790.339866412, 368600.351743291], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4969800.0000, 
sim time next is 4970400.0000, 
raw observation next is [30.33333333333334, 65.0, 1.0, 2.0, 0.4361044622896948, 1.0, 1.0, 0.4361044622896948, 1.0, 2.0, 0.744559289499002, 6.9112, 6.9112, 170.5573041426782, 1829122.453056666, 1829122.453056666, 371774.7963159626], 
processed observation next is [1.0, 0.5217391304347826, 0.6366508688783573, 0.65, 1.0, 1.0, 0.32060778589119854, 1.0, 0.5, 0.32060778589119854, 1.0, 1.0, 0.6884869384134169, 0.0, 0.0, 0.8375144448122397, 0.5080895702935183, 0.5080895702935183, 0.5548877556954666], 
reward next is 0.4451, 
noisyNet noise sample is [array([-0.31021187], dtype=float32), 0.62470156]. 
=============================================
[2019-03-26 21:01:51,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5606218e-38 1.0000000e+00 0.0000000e+00 2.5887863e-11 1.1893168e-28], sum to 1.0000
[2019-03-26 21:01:51,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5693
[2019-03-26 21:01:51,497] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.5, 1.0, 2.0, 0.6465221921074618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903501.3935972198, 903501.3935972198, 209724.8204450519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4861800.0000, 
sim time next is 4862400.0000, 
raw observation next is [27.0, 87.33333333333333, 1.0, 2.0, 0.6417696448468935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 896857.0028543914, 896857.0028543914, 208777.9244044003], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.8733333333333333, 1.0, 1.0, 0.5683971624661367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24912694523733092, 0.24912694523733092, 0.31160884239462733], 
reward next is 0.6884, 
noisyNet noise sample is [array([-0.9212756], dtype=float32), 1.1961797]. 
=============================================
[2019-03-26 21:01:54,642] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6807640e-14 9.4892454e-01 5.5151796e-14 5.1075380e-02 5.9475763e-10], sum to 1.0000
[2019-03-26 21:01:54,651] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3722
[2019-03-26 21:01:54,660] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2040305.540038189 W.
[2019-03-26 21:01:54,666] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.7296111351860481, 1.0, 2.0, 0.7296111351860481, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2040305.540038189, 2040305.540038189, 387056.1318058739], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4874400.0000, 
sim time next is 4875000.0000, 
raw observation next is [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.7458961413623508, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98799573869101, 6.9112, 168.912499053986, 1939355.68492743, 1884874.29234365, 395582.6681447471], 
processed observation next is [1.0, 0.43478260869565216, 0.6287519747235385, 0.6933333333333335, 1.0, 1.0, 0.6938507727257239, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007679573869101031, 0.0, 0.8294376988317333, 0.5387099124798417, 0.5235761923176806, 0.5904218927533539], 
reward next is 0.0256, 
noisyNet noise sample is [array([-0.8590383], dtype=float32), 0.5073582]. 
=============================================
[2019-03-26 21:01:54,678] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[29.802128]
 [28.837606]
 [28.996626]
 [29.119255]
 [29.157444]], R is [[30.08586121]
 [30.20730782]
 [29.90523529]
 [29.60618401]
 [29.31012154]].
[2019-03-26 21:01:55,726] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2675694e-33 1.0000000e+00 3.5887613e-34 5.1249881e-12 2.6000476e-27], sum to 1.0000
[2019-03-26 21:01:55,727] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6384
[2019-03-26 21:01:55,732] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5940333328374745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830120.6328028388, 830120.6328028388, 199632.858922759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4942200.0000, 
sim time next is 4942800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5994737717057516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837726.269262195, 837726.2692621943, 200640.8238202819], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.5174382791635561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23270174146172082, 0.23270174146172062, 0.29946391614967444], 
reward next is 0.7005, 
noisyNet noise sample is [array([-1.4762839], dtype=float32), 0.7254527]. 
=============================================
[2019-03-26 21:02:15,339] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.9859803e-20 3.3659176e-35], sum to 1.0000
[2019-03-26 21:02:15,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7542
[2019-03-26 21:02:15,351] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.85, 82.5, 1.0, 2.0, 0.567361603932105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792834.8196694682, 792834.8196694682, 194822.9990674461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5520600.0000, 
sim time next is 5521200.0000, 
raw observation next is [28.7, 83.0, 1.0, 2.0, 0.56496112017837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789479.121331899, 789479.121331899, 194399.8114578796], 
processed observation next is [1.0, 0.9130434782608695, 0.5592417061611374, 0.83, 1.0, 1.0, 0.4758567712992409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2192997559255275, 0.2192997559255275, 0.29014897232519343], 
reward next is 0.7099, 
noisyNet noise sample is [array([0.8401669], dtype=float32), 0.9058212]. 
=============================================
[2019-03-26 21:02:18,052] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.541461e-23 1.333785e-38], sum to 1.0000
[2019-03-26 21:02:18,061] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9576
[2019-03-26 21:02:18,064] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 83.16666666666667, 1.0, 2.0, 0.555245578425055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775897.6224667907, 775897.6224667907, 192704.7612388712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5267400.0000, 
sim time next is 5268000.0000, 
raw observation next is [28.5, 83.33333333333334, 1.0, 2.0, 0.5562804724793932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777344.3078390054, 777344.307839006, 192884.0044816049], 
processed observation next is [1.0, 1.0, 0.5497630331753555, 0.8333333333333335, 1.0, 1.0, 0.46539815961372666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21592897439972372, 0.2159289743997239, 0.28788657385314165], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.5697431], dtype=float32), 1.2309031]. 
=============================================
[2019-03-26 21:02:18,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[70.50412 ]
 [70.508965]
 [70.41079 ]
 [70.41828 ]
 [70.41923 ]], R is [[70.51047516]
 [70.5177536 ]
 [70.52523041]
 [70.53294373]
 [70.54083252]].
[2019-03-26 21:02:19,704] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.9595429e-36 1.0000000e+00 2.2959746e-38 4.8146981e-17 5.6061300e-32], sum to 1.0000
[2019-03-26 21:02:19,713] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2185
[2019-03-26 21:02:19,719] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 81.16666666666666, 1.0, 2.0, 0.5891208125267886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 823253.063857509, 823253.063857509, 198736.6960920513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5439000.0000, 
sim time next is 5439600.0000, 
raw observation next is [29.6, 81.0, 1.0, 2.0, 0.585524400314107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818225.4061662189, 818225.4061662189, 198080.1773798122], 
processed observation next is [1.0, 1.0, 0.6018957345971565, 0.81, 1.0, 1.0, 0.5006318076073578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22728483504617192, 0.22728483504617192, 0.29564205579076447], 
reward next is 0.7044, 
noisyNet noise sample is [array([-1.7865176], dtype=float32), 0.0031923277]. 
=============================================
[2019-03-26 21:02:30,273] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6878492e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:02:30,282] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9335
[2019-03-26 21:02:30,287] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.8, 53.0, 1.0, 2.0, 0.5123013271457002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715867.3014275094, 715867.3014275087, 185545.9111822582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5752800.0000, 
sim time next is 5753400.0000, 
raw observation next is [33.0, 53.0, 1.0, 2.0, 0.5296397848215211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740103.7385129881, 740103.7385129888, 188369.5678431455], 
processed observation next is [0.0, 0.6086956521739131, 0.7630331753554502, 0.53, 1.0, 1.0, 0.4333009455680977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20558437180916336, 0.20558437180916356, 0.2811486087211127], 
reward next is 0.7189, 
noisyNet noise sample is [array([-1.8663576], dtype=float32), -1.6423646]. 
=============================================
[2019-03-26 21:02:35,513] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6164734e-13 5.4181076e-08 9.2039400e-14 1.0000000e+00 4.5241971e-10], sum to 1.0000
[2019-03-26 21:02:35,514] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8582
[2019-03-26 21:02:35,518] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.5, 67.0, 1.0, 2.0, 0.7615581159909399, 1.0, 2.0, 0.7615581159909399, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2129731.80171736, 2129731.80171736, 401593.5945672185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5566200.0000, 
sim time next is 5566800.0000, 
raw observation next is [31.7, 66.0, 1.0, 2.0, 0.8295522569508444, 1.0, 2.0, 0.8295522569508444, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2320069.816193835, 2320069.816193835, 434516.424106167], 
processed observation next is [1.0, 0.43478260869565216, 0.7014218009478673, 0.66, 1.0, 1.0, 0.7946412734347523, 1.0, 1.0, 0.7946412734347523, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6444638378316208, 0.6444638378316208, 0.6485319762778612], 
reward next is 0.3515, 
noisyNet noise sample is [array([-1.6141189], dtype=float32), 1.5424511]. 
=============================================
[2019-03-26 21:02:39,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6432409e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 21:02:39,559] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2116
[2019-03-26 21:02:39,563] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.61666666666667, 91.16666666666667, 1.0, 2.0, 0.5019505234115789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701398.7692300805, 701398.7692300805, 183901.9238461858], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5629800.0000, 
sim time next is 5630400.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4998949253814117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698525.4422616044, 698525.4422616044, 183579.3229989482], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 1.0, 1.0, 0.3974637655197732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19403484507266788, 0.19403484507266788, 0.2739989895506689], 
reward next is 0.7260, 
noisyNet noise sample is [array([-0.24357156], dtype=float32), -0.2581409]. 
=============================================
[2019-03-26 21:02:39,736] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 21:02:39,737] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:02:39,738] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:02:39,739] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:02:39,740] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:02:39,740] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:02:39,741] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:02:39,742] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:02:39,742] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:02:39,745] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:02:39,746] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:02:40,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-03-26 21:02:40,303] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-03-26 21:02:40,469] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-03-26 21:02:40,486] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-03-26 21:02:40,544] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-03-26 21:02:55,906] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:02:55,908] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.1, 97.5, 1.0, 2.0, 0.3724396340667769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564686.0933807458, 564686.0933807465, 171638.2772160558]
[2019-03-26 21:02:55,911] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:02:55,914] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1776444e-23 0.0000000e+00], sampled 0.6712936247089718
[2019-03-26 21:02:59,265] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:02:59,266] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.65, 76.66666666666667, 1.0, 2.0, 0.4317444592857451, 1.0, 1.0, 0.4317444592857451, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1206873.593748378, 1206873.593748378, 280411.428857114]
[2019-03-26 21:02:59,267] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:02:59,270] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8649604e-31 1.0000000e+00 3.7274691e-33 3.4695115e-13 1.6441365e-30], sampled 0.7768183278901959
[2019-03-26 21:03:00,022] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:03:00,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.8, 51.0, 1.0, 2.0, 0.2056515171260029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343724.4516716925, 343724.4516716925, 155849.2810393436]
[2019-03-26 21:03:00,024] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:03:00,028] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.09775655e-24
 0.00000000e+00], sampled 0.508855378976424
[2019-03-26 21:03:41,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:03:41,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.75, 54.0, 1.0, 2.0, 0.708078664089272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989565.3677340576, 989565.3677340576, 222602.1905018881]
[2019-03-26 21:03:41,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:03:41,115] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.347499e-18 0.000000e+00], sampled 0.02185054583472179
[2019-03-26 21:03:52,304] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:03:52,306] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 84.0, 1.0, 2.0, 0.5913937776984254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826430.6014398711, 826430.6014398711, 199153.3841534985]
[2019-03-26 21:03:52,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:03:52,311] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.4108746e-20 0.0000000e+00], sampled 0.9309605740453875
[2019-03-26 21:04:31,302] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.20474927]
[2019-03-26 21:04:31,302] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.52597212, 72.15225610666667, 1.0, 2.0, 0.469123561841996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669044.2032443894, 669044.2032443894, 180635.2181661369]
[2019-03-26 21:04:31,303] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:04:31,305] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4041746e-19 0.0000000e+00], sampled 0.9570915399485075
[2019-03-26 21:04:34,628] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.5158 2779390412.4753 921.0000
[2019-03-26 21:04:35,054] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8261.0115 2927414458.7913 1327.0000
[2019-03-26 21:04:35,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7903.1528 3162062320.6445 1723.0000
[2019-03-26 21:04:35,129] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6247 3007950249.2698 1761.0000
[2019-03-26 21:04:35,250] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.0789 2843018712.7339 1127.0000
[2019-03-26 21:04:36,265] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1000000, evaluation results [1000000.0, 7903.152845851217, 3162062320.6445174, 1723.0, 8261.01146308221, 2927414458.791339, 1327.0, 8664.51583888715, 2779390412.4752865, 921.0, 8000.624746848568, 3007950249.269781, 1761.0, 8498.078900375036, 2843018712.7338977, 1127.0]
[2019-03-26 21:04:37,181] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1567463e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 21:04:37,192] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9885
[2019-03-26 21:04:37,195] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.4, 60.0, 1.0, 2.0, 0.5446811968666186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761129.7150157875, 761129.7150157881, 190893.1658854079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5668200.0000, 
sim time next is 5668800.0000, 
raw observation next is [32.36666666666667, 60.0, 1.0, 2.0, 0.5433957148214752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759332.7587298921, 759332.7587298927, 190674.9880485581], 
processed observation next is [0.0, 0.6086956521739131, 0.7330173775671407, 0.6, 1.0, 1.0, 0.4498743552065966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21092576631385893, 0.21092576631385906, 0.28458953440083296], 
reward next is 0.7154, 
noisyNet noise sample is [array([-1.5092756], dtype=float32), -1.3727298]. 
=============================================
[2019-03-26 21:04:46,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1613968e-20 1.0056836e-05 5.4575518e-22 9.9998999e-01 9.9248331e-18], sum to 1.0000
[2019-03-26 21:04:46,901] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3005
[2019-03-26 21:04:46,908] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.6, 62.0, 1.0, 2.0, 1.007866393103678, 1.0, 2.0, 1.007866393103678, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2819336.732213841, 2819336.732213841, 533754.4604043363], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5842800.0000, 
sim time next is 5843400.0000, 
raw observation next is [32.55, 62.16666666666667, 1.0, 2.0, 0.8877548980970035, 1.0, 2.0, 0.8877548980970035, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2483011.147169054, 2483011.147169055, 464855.7422388209], 
processed observation next is [1.0, 0.6521739130434783, 0.7417061611374406, 0.6216666666666667, 1.0, 1.0, 0.8647649374662693, 1.0, 1.0, 0.8647649374662693, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6897253186580705, 0.6897253186580709, 0.6938145406549565], 
reward next is 0.3062, 
noisyNet noise sample is [array([-0.8121999], dtype=float32), 1.471035]. 
=============================================
[2019-03-26 21:04:47,964] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.976813e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:04:47,971] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1269
[2019-03-26 21:04:47,977] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 90.0, 1.0, 2.0, 0.5293183546834279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739654.4246990532, 739654.4246990532, 188317.2765606468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5875200.0000, 
sim time next is 5875800.0000, 
raw observation next is [26.63333333333333, 90.33333333333333, 1.0, 2.0, 0.5301574870471409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740827.4133099086, 740827.4133099093, 188456.0373991262], 
processed observation next is [1.0, 0.0, 0.46129541864139006, 0.9033333333333333, 1.0, 1.0, 0.4339246831893264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20578539258608572, 0.2057853925860859, 0.2812776677598898], 
reward next is 0.7187, 
noisyNet noise sample is [array([0.19574542], dtype=float32), 1.9375085]. 
=============================================
[2019-03-26 21:04:50,334] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.261599e-18 0.000000e+00], sum to 1.0000
[2019-03-26 21:04:50,343] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8164
[2019-03-26 21:04:50,352] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 87.66666666666667, 1.0, 2.0, 0.5391237589005263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753361.0767429426, 753361.0767429426, 189952.7347433777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5870400.0000, 
sim time next is 5871000.0000, 
raw observation next is [27.08333333333333, 87.83333333333334, 1.0, 2.0, 0.5359402213410656, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748910.894908808, 748910.8949088088, 189418.5853816767], 
processed observation next is [1.0, 0.9565217391304348, 0.4826224328593995, 0.8783333333333334, 1.0, 1.0, 0.44089183294104284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20803080414133557, 0.20803080414133576, 0.282714306539816], 
reward next is 0.7173, 
noisyNet noise sample is [array([0.851254], dtype=float32), -1.9053206]. 
=============================================
[2019-03-26 21:04:50,372] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.0153 ]
 [68.02426]
 [68.0502 ]
 [68.10024]
 [68.1652 ]], R is [[68.04409027]
 [68.08013916]
 [68.1150589 ]
 [68.149086  ]
 [68.18261719]].
[2019-03-26 21:04:57,032] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3173569e-37 1.0000000e+00 3.7772866e-38 2.4998788e-12 9.5210001e-34], sum to 1.0000
[2019-03-26 21:04:57,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5552
[2019-03-26 21:04:57,050] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 87.5, 1.0, 2.0, 0.7724593662494886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079585.446619537, 1079585.446619537, 237264.1381100006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5986200.0000, 
sim time next is 5986800.0000, 
raw observation next is [27.8, 87.0, 1.0, 2.0, 0.7406611294908749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1035122.725842849, 1035122.72584285, 229868.1972868438], 
processed observation next is [1.0, 0.30434782608695654, 0.5165876777251186, 0.87, 1.0, 1.0, 0.6875435295070782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2875340905119025, 0.2875340905119028, 0.3430868616221549], 
reward next is 0.6569, 
noisyNet noise sample is [array([-1.4847304], dtype=float32), 1.4084307]. 
=============================================
[2019-03-26 21:05:02,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0454201e-23 9.9609643e-01 3.0545956e-25 3.9035717e-03 1.2756874e-19], sum to 1.0000
[2019-03-26 21:05:02,843] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7027
[2019-03-26 21:05:02,850] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3542558.270254852 W.
[2019-03-26 21:05:02,856] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.86666666666667, 79.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.853983225804193, 6.9112, 168.8962106021904, 3542558.270254852, 1455053.541314377, 306783.9370471115], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6078000.0000, 
sim time next is 6078600.0000, 
raw observation next is [29.0, 78.5, 1.0, 2.0, 0.6362090056398496, 1.0, 1.0, 0.6362090056398496, 1.0, 1.0, 1.03, 6.995386797565336, 6.9112, 170.5573041426782, 2669371.496413226, 2609065.068817199, 501479.7684497423], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.785, 1.0, 1.0, 0.5616975971564453, 1.0, 0.5, 0.5616975971564453, 1.0, 0.5, 1.0365853658536586, 0.008418679756533631, 0.0, 0.8375144448122397, 0.7414920823370071, 0.7247402968936665, 0.7484772663428989], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02760877], dtype=float32), -0.122780725]. 
=============================================
[2019-03-26 21:05:03,338] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8274453e-22 9.6869546e-01 5.8900422e-24 3.1304512e-02 3.7510818e-19], sum to 1.0000
[2019-03-26 21:05:03,346] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3777
[2019-03-26 21:05:03,357] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2015872.107969355 W.
[2019-03-26 21:05:03,364] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.73333333333333, 66.33333333333334, 1.0, 2.0, 0.4805879855294945, 1.0, 2.0, 0.4805879855294945, 1.0, 2.0, 0.8295584782410337, 6.9112, 6.9112, 170.5573041426782, 2015872.107969355, 2015872.107969355, 401106.7096744934], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6103200.0000, 
sim time next is 6103800.0000, 
raw observation next is [30.7, 66.5, 1.0, 2.0, 0.5004554482890019, 1.0, 2.0, 0.5004554482890019, 1.0, 2.0, 0.8641684924974001, 6.9112, 6.9112, 170.5573041426782, 2099289.728674488, 2099289.728674488, 414564.7071232815], 
processed observation next is [1.0, 0.6521739130434783, 0.6540284360189573, 0.665, 1.0, 1.0, 0.39813909432409866, 1.0, 1.0, 0.39813909432409866, 1.0, 1.0, 0.8343518201187805, 0.0, 0.0, 0.8375144448122397, 0.5831360357429134, 0.5831360357429134, 0.618753294213853], 
reward next is 0.3812, 
noisyNet noise sample is [array([0.6758599], dtype=float32), 1.3884115]. 
=============================================
[2019-03-26 21:05:03,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1677466e-19 1.4975637e-01 2.0790569e-21 8.5024363e-01 1.2031955e-16], sum to 1.0000
[2019-03-26 21:05:03,810] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2955
[2019-03-26 21:05:03,816] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.5784561818489872, 1.0, 2.0, 0.5784561818489872, 1.0, 1.0, 1.00019564363103, 6.9112, 6.9112, 170.5573041426782, 2426819.984159267, 2426819.984159267, 472707.1318896275], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6089400.0000, 
sim time next is 6090000.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8250101709943019, 1.0, 2.0, 0.8250101709943019, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2307354.903203788, 2307354.903203788, 432226.801122972], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.65, 1.0, 1.0, 0.7891688807160263, 1.0, 1.0, 0.7891688807160263, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6409319175566078, 0.6409319175566078, 0.6451146285417493], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1740981], dtype=float32), 0.8849901]. 
=============================================
[2019-03-26 21:05:03,837] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[46.37435 ]
 [47.15761 ]
 [46.75857 ]
 [48.097797]
 [48.95102 ]], R is [[47.16041565]
 [46.98328018]
 [46.83189011]
 [46.70672989]
 [46.66799545]].
[2019-03-26 21:05:05,003] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.268142e-37 1.000000e+00 1.512380e-38 1.898369e-15 1.368275e-34], sum to 1.0000
[2019-03-26 21:05:05,014] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0303
[2019-03-26 21:05:05,019] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 83.0, 1.0, 2.0, 0.5110430279115776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714108.4185074257, 714108.4185074263, 185344.0614224369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6399600.0000, 
sim time next is 6400200.0000, 
raw observation next is [27.01666666666667, 83.0, 1.0, 2.0, 0.510108049918798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712801.4841913255, 712801.4841913255, 185194.6590759626], 
processed observation next is [1.0, 0.043478260869565216, 0.4794628751974725, 0.83, 1.0, 1.0, 0.4097687348419253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1980004122753682, 0.1980004122753682, 0.2764099389193472], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.60728294], dtype=float32), 0.47559878]. 
=============================================
[2019-03-26 21:05:07,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3375397e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:05:07,953] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6442
[2019-03-26 21:05:07,958] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.85, 62.5, 1.0, 2.0, 0.5178005561098981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723554.2987983349, 723554.2987983349, 186431.8985461741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6355800.0000, 
sim time next is 6356400.0000, 
raw observation next is [30.8, 62.33333333333333, 1.0, 2.0, 0.5156978665998968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 720615.0858719458, 720615.0858719453, 186091.9705653025], 
processed observation next is [0.0, 0.5652173913043478, 0.6587677725118484, 0.6233333333333333, 1.0, 1.0, 0.4165034537348154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20017085718665162, 0.20017085718665145, 0.27774920979895895], 
reward next is 0.7223, 
noisyNet noise sample is [array([1.4588108], dtype=float32), -0.92997277]. 
=============================================
[2019-03-26 21:05:11,313] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.035626e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:05:11,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1906
[2019-03-26 21:05:11,329] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.61666666666667, 62.83333333333333, 1.0, 2.0, 0.5102231700791207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712962.4017829729, 712962.4017829729, 185213.3395992655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6277800.0000, 
sim time next is 6278400.0000, 
raw observation next is [30.6, 63.0, 1.0, 2.0, 0.5102283636509687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712969.6614769972, 712969.6614769972, 185214.2027767767], 
processed observation next is [0.0, 0.6956521739130435, 0.6492890995260664, 0.63, 1.0, 1.0, 0.4099136911457454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19804712818805478, 0.19804712818805478, 0.2764391086220548], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.0682207], dtype=float32), -1.0398438]. 
=============================================
[2019-03-26 21:05:13,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6045297e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:05:13,760] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7665
[2019-03-26 21:05:13,773] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 75.66666666666666, 1.0, 2.0, 0.5417871942861617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757084.2360113957, 757084.2360113951, 190402.7467325003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6255600.0000, 
sim time next is 6256200.0000, 
raw observation next is [29.55, 74.83333333333334, 1.0, 2.0, 0.5418757336280602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757208.0034848315, 757208.0034848315, 190417.7333256967], 
processed observation next is [0.0, 0.391304347826087, 0.5995260663507109, 0.7483333333333334, 1.0, 1.0, 0.4480430525639279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2103355565235643, 0.2103355565235643, 0.28420557212790554], 
reward next is 0.7158, 
noisyNet noise sample is [array([-2.0122979], dtype=float32), 0.31273273]. 
=============================================
[2019-03-26 21:05:25,299] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2309069e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:05:25,311] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1050
[2019-03-26 21:05:25,316] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 71.5, 1.0, 2.0, 0.5065285289715604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707797.9601494962, 707797.9601494956, 184625.491827253], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6460200.0000, 
sim time next is 6460800.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5039603006425784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704208.0551312147, 704208.0551312154, 184219.2377788638], 
processed observation next is [1.0, 0.782608695652174, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4023618080031064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19561334864755964, 0.19561334864755983, 0.27495408623711015], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.7618886], dtype=float32), -0.24074976]. 
=============================================
[2019-03-26 21:05:31,695] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.2857595e-30 1.0000000e+00 2.3406867e-31 6.2249583e-09 3.8854539e-28], sum to 1.0000
[2019-03-26 21:05:31,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6410
[2019-03-26 21:05:31,715] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 92.0, 1.0, 2.0, 0.7725671885780743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1079736.215193073, 1079736.215193073, 237286.6588786263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6585600.0000, 
sim time next is 6586200.0000, 
raw observation next is [25.9, 91.5, 1.0, 2.0, 0.754033857688654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053821.267887807, 1053821.267887807, 232938.8551451454], 
processed observation next is [1.0, 0.21739130434782608, 0.42654028436018954, 0.915, 1.0, 1.0, 0.703655250227294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2927281299688353, 0.2927281299688353, 0.34766993305245586], 
reward next is 0.6523, 
noisyNet noise sample is [array([-1.8233905], dtype=float32), 1.4017248]. 
=============================================
[2019-03-26 21:05:31,918] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 21:05:31,919] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:05:31,920] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:05:31,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:05:31,921] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:05:31,922] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:05:31,922] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:05:31,924] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:05:31,923] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:05:31,926] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:05:31,924] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:05:31,943] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-03-26 21:05:31,944] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-03-26 21:05:31,962] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-03-26 21:05:31,963] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-03-26 21:05:32,020] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-03-26 21:06:54,851] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.26578894]
[2019-03-26 21:06:54,854] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.3, 79.16666666666667, 1.0, 2.0, 0.4101873536573752, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7123597822721369, 6.911200000000001, 6.9112, 168.9127781073836, 1146587.849503701, 1146587.8495037, 264022.713566764]
[2019-03-26 21:06:54,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:06:54,860] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1955669e-24 9.9999928e-01 1.0837705e-25 6.7342353e-07 1.6673471e-22], sampled 0.9543695000123624
[2019-03-26 21:07:24,181] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7933.6026 3158836977.1790 1627.0000
[2019-03-26 21:07:24,676] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8507.9321 2840994257.5069 1086.0000
[2019-03-26 21:07:24,719] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.3289 2927237231.0559 1317.0000
[2019-03-26 21:07:24,774] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8671.7605 2778480848.3602 897.0000
[2019-03-26 21:07:24,810] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8029.9981 3004941348.2067 1686.0000
[2019-03-26 21:07:25,825] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1025000, evaluation results [1025000.0, 7933.602577583876, 3158836977.1789756, 1627.0, 8263.328854052339, 2927237231.055892, 1317.0, 8671.760461477288, 2778480848.3601985, 897.0, 8029.998087660251, 3004941348.2066836, 1686.0, 8507.932123911858, 2840994257.506929, 1086.0]
[2019-03-26 21:07:38,845] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0155723e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:07:38,852] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4740
[2019-03-26 21:07:38,858] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666666, 61.16666666666667, 1.0, 2.0, 0.3242004567002977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 511178.0306159031, 511178.0306159037, 167842.3541795695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6814200.0000, 
sim time next is 6814800.0000, 
raw observation next is [25.8, 62.0, 1.0, 2.0, 0.3237298596240198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 510070.4290974017, 510070.4290974011, 167750.061073115], 
processed observation next is [1.0, 0.9130434782608695, 0.42180094786729866, 0.62, 1.0, 1.0, 0.1852166983421925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1416862303048338, 0.14168623030483363, 0.2503732254822612], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.55991524], dtype=float32), 0.6807489]. 
=============================================
[2019-03-26 21:07:41,871] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4793101e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:07:41,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1281
[2019-03-26 21:07:41,883] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 82.0, 1.0, 2.0, 0.3373104225798131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526188.2841999789, 526188.2841999796, 168885.4356389139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6840000.0000, 
sim time next is 6840600.0000, 
raw observation next is [23.08333333333334, 82.33333333333334, 1.0, 2.0, 0.3384398181616116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527583.3709242905, 527583.3709242905, 168987.0938982597], 
processed observation next is [0.0, 0.17391304347826086, 0.2930489731437602, 0.8233333333333335, 1.0, 1.0, 0.20293953995374894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1465509363678585, 0.1465509363678585, 0.2522195431317309], 
reward next is 0.7478, 
noisyNet noise sample is [array([0.12289391], dtype=float32), 0.10955262]. 
=============================================
[2019-03-26 21:07:50,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.05047684e-22
 0.00000000e+00], sum to 1.0000
[2019-03-26 21:07:50,282] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7787
[2019-03-26 21:07:50,287] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.96666666666667, 56.33333333333334, 1.0, 2.0, 0.3965311551739365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591149.2877517611, 591149.2877517617, 173707.0568044419], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6981600.0000, 
sim time next is 6982200.0000, 
raw observation next is [28.9, 56.0, 1.0, 2.0, 0.3939389622078006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589670.6571792588, 589670.6571792588, 173642.7863888592], 
processed observation next is [0.0, 0.8260869565217391, 0.5687203791469194, 0.56, 1.0, 1.0, 0.2698059785636152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16379740477201632, 0.16379740477201632, 0.2591683378938197], 
reward next is 0.7408, 
noisyNet noise sample is [array([-2.2448173], dtype=float32), 0.4085613]. 
=============================================
[2019-03-26 21:07:51,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.43082385e-36 1.00000000e+00 4.56214082e-37 5.22949883e-16
 8.62019223e-35], sum to 1.0000
[2019-03-26 21:07:51,452] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3313
[2019-03-26 21:07:51,460] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666666, 79.33333333333333, 1.0, 2.0, 0.4525203593305189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646806.3773682071, 646806.3773682065, 178355.5033537819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7000800.0000, 
sim time next is 7001400.0000, 
raw observation next is [26.13333333333334, 79.66666666666667, 1.0, 2.0, 0.4528319381208636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646801.0828001656, 646801.0828001656, 178343.8153820888], 
processed observation next is [1.0, 0.0, 0.43759873617693557, 0.7966666666666667, 1.0, 1.0, 0.34076137122995614, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17966696744449046, 0.17966696744449046, 0.26618479907774445], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.2252661], dtype=float32), 0.3572909]. 
=============================================
[2019-03-26 21:07:54,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 8.37278e-20 0.00000e+00], sum to 1.0000
[2019-03-26 21:07:54,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1120
[2019-03-26 21:07:54,115] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 92.5, 1.0, 2.0, 0.5898481219932419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 830319.3067673407, 830319.30676734, 199647.9751432616], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7093800.0000, 
sim time next is 7094400.0000, 
raw observation next is [24.6, 92.66666666666667, 1.0, 2.0, 0.569235914044757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802345.5808369551, 802345.5808369551, 196039.0806612568], 
processed observation next is [1.0, 0.08695652173913043, 0.36492890995260674, 0.9266666666666667, 1.0, 1.0, 0.48100712535512896, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287377245470974, 0.22287377245470974, 0.2925956427779952], 
reward next is 0.7074, 
noisyNet noise sample is [array([-0.45608833], dtype=float32), -0.56517047]. 
=============================================
[2019-03-26 21:07:58,090] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9507324e-17 7.8761028e-03 7.3922914e-18 9.9212390e-01 9.4606763e-14], sum to 1.0000
[2019-03-26 21:07:58,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8572
[2019-03-26 21:07:58,102] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 70.33333333333333, 1.0, 2.0, 0.6102216655711586, 1.0, 2.0, 0.6102216655711586, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1706175.572664806, 1706175.572664805, 338045.0806174761], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7120200.0000, 
sim time next is 7120800.0000, 
raw observation next is [28.1, 70.0, 1.0, 2.0, 0.6206221281829034, 1.0, 2.0, 0.6206221281829034, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1735278.739416648, 1735278.739416648, 341980.9641802596], 
processed observation next is [1.0, 0.43478260869565216, 0.5308056872037916, 0.7, 1.0, 1.0, 0.5429182267263897, 1.0, 1.0, 0.5429182267263897, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48202187206018, 0.48202187206018, 0.5104193495227756], 
reward next is 0.4896, 
noisyNet noise sample is [array([-1.387371], dtype=float32), -0.0042730756]. 
=============================================
[2019-03-26 21:08:01,148] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2746915e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:01,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0897
[2019-03-26 21:08:01,166] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 72.66666666666667, 1.0, 2.0, 0.3542607385432091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545295.6405269419, 545295.6405269412, 170241.8241660794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7350000.0000, 
sim time next is 7350600.0000, 
raw observation next is [24.98333333333333, 72.33333333333333, 1.0, 2.0, 0.3534448289554285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 544564.6543764675, 544564.6543764668, 170196.2865857596], 
processed observation next is [1.0, 0.043478260869565216, 0.3830963665086887, 0.7233333333333333, 1.0, 1.0, 0.22101786621135963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15126795954901873, 0.15126795954901853, 0.2540243083369546], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.11420336], dtype=float32), 0.0866251]. 
=============================================
[2019-03-26 21:08:03,319] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.0549053e-38 1.0000000e+00 0.0000000e+00 4.0037378e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:03,326] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8238
[2019-03-26 21:08:03,331] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 93.0, 1.0, 2.0, 0.6163464047426903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 972426.3776905333, 972426.3776905339, 216825.7324020278], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7387200.0000, 
sim time next is 7387800.0000, 
raw observation next is [21.16666666666667, 92.83333333333333, 1.0, 2.0, 0.6387313323200511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1008690.98886403, 1008690.988864029, 221827.4207823641], 
processed observation next is [1.0, 0.5217391304347826, 0.2022116903633494, 0.9283333333333332, 1.0, 1.0, 0.5647365449639169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28019194135111947, 0.28019194135111913, 0.3310857026602449], 
reward next is 0.6689, 
noisyNet noise sample is [array([-0.03957478], dtype=float32), -0.2871992]. 
=============================================
[2019-03-26 21:08:11,827] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5179565e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:11,837] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1845
[2019-03-26 21:08:11,842] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.98333333333333, 72.33333333333333, 1.0, 2.0, 0.3534448289554285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 544564.6543764675, 544564.6543764668, 170196.2865857596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7350600.0000, 
sim time next is 7351200.0000, 
raw observation next is [25.0, 72.0, 1.0, 2.0, 0.3529430983346466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544314.467373413, 544314.467373413, 170190.4459524591], 
processed observation next is [1.0, 0.08695652173913043, 0.38388625592417064, 0.72, 1.0, 1.0, 0.22041337148752602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1511984631592814, 0.1511984631592814, 0.25401559097381954], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.4933356], dtype=float32), 0.5334706]. 
=============================================
[2019-03-26 21:08:14,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7880674e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:14,266] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4810
[2019-03-26 21:08:14,271] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.35, 91.5, 1.0, 2.0, 0.3145421837505583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 496828.6231944223, 496828.6231944217, 166777.0656819067], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7435800.0000, 
sim time next is 7436400.0000, 
raw observation next is [21.36666666666667, 91.33333333333334, 1.0, 2.0, 0.3141037069787677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496181.5054580989, 496181.5054580989, 166729.8703255484], 
processed observation next is [0.0, 0.043478260869565216, 0.21169036334913136, 0.9133333333333334, 1.0, 1.0, 0.17361892407080448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13782819596058302, 0.13782819596058302, 0.2488505527246991], 
reward next is 0.7511, 
noisyNet noise sample is [array([-0.37082297], dtype=float32), 0.615213]. 
=============================================
[2019-03-26 21:08:18,162] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 5.08475e-33 0.00000e+00], sum to 1.0000
[2019-03-26 21:08:18,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9619
[2019-03-26 21:08:18,176] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 82.0, 1.0, 2.0, 0.394913388322563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585719.1098138968, 585719.1098138962, 173115.0791195052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7477200.0000, 
sim time next is 7477800.0000, 
raw observation next is [24.9, 81.66666666666667, 1.0, 2.0, 0.3972185431418468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588120.0993658563, 588120.099365857, 173303.1534490435], 
processed observation next is [0.0, 0.5652173913043478, 0.3791469194312796, 0.8166666666666668, 1.0, 1.0, 0.2737572808937913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1633666942682934, 0.1633666942682936, 0.2586614230582739], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.19926351], dtype=float32), 0.047928043]. 
=============================================
[2019-03-26 21:08:19,963] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2905752e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:19,969] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8729
[2019-03-26 21:08:19,975] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 90.0, 1.0, 2.0, 0.3607101615473272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549408.6630846766, 549408.6630846772, 170409.6980436966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7466400.0000, 
sim time next is 7467000.0000, 
raw observation next is [23.0, 89.5, 1.0, 2.0, 0.362235323173999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551038.4019061476, 551038.4019061476, 170525.2175578875], 
processed observation next is [0.0, 0.43478260869565216, 0.28909952606635075, 0.895, 1.0, 1.0, 0.2316088231012036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15306622275170767, 0.15306622275170767, 0.25451525008639925], 
reward next is 0.7455, 
noisyNet noise sample is [array([1.8224455], dtype=float32), -3.392191]. 
=============================================
[2019-03-26 21:08:19,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[81.97683 ]
 [81.868   ]
 [81.858406]
 [81.84028 ]
 [81.84263 ]], R is [[81.88298035]
 [81.80980682]
 [81.7375412 ]
 [81.66613007]
 [81.59555054]].
[2019-03-26 21:08:20,668] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3415351e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 21:08:20,677] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9392
[2019-03-26 21:08:20,682] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.05, 90.0, 1.0, 2.0, 0.3692852006950881, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559792.6163644551, 559792.6163644557, 171211.1128700213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7535400.0000, 
sim time next is 7536000.0000, 
raw observation next is [23.1, 90.0, 1.0, 2.0, 0.3711948784039869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561825.6157539217, 561825.6157539217, 171359.5274785451], 
processed observation next is [0.0, 0.21739130434782608, 0.2938388625592418, 0.9, 1.0, 1.0, 0.24240346795661072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15606267104275603, 0.15606267104275603, 0.2557604887739479], 
reward next is 0.7442, 
noisyNet noise sample is [array([-1.7772362], dtype=float32), -2.1866426]. 
=============================================
[2019-03-26 21:08:20,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[77.00525 ]
 [77.010895]
 [76.861595]
 [76.80584 ]
 [76.82123 ]], R is [[76.96520996]
 [76.9400177 ]
 [76.91516876]
 [76.89030457]
 [76.86540222]].
[2019-03-26 21:08:21,719] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 21:08:21,722] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:08:21,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:08:21,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:08:21,724] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:08:21,724] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:08:21,726] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:08:21,727] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:08:21,728] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:08:21,730] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:08:21,730] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:08:21,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-03-26 21:08:21,750] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-03-26 21:08:21,791] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-03-26 21:08:21,818] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-03-26 21:08:21,818] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-03-26 21:09:02,093] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.18917927]
[2019-03-26 21:09:02,095] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.05, 73.0, 1.0, 2.0, 0.7448860534151376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1041030.229583693, 1041030.229583693, 230831.1154615911]
[2019-03-26 21:09:02,095] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:09:02,098] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6419595e-25 0.0000000e+00], sampled 0.5387384117541224
[2019-03-26 21:09:04,596] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.18917927]
[2019-03-26 21:09:04,597] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.33333333333334, 98.0, 1.0, 2.0, 0.4793927355192894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742797.372358481, 742797.3723584805, 189103.1106037358]
[2019-03-26 21:09:04,598] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:09:04,601] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7482042e-29 0.0000000e+00], sampled 0.8311846886796406
[2019-03-26 21:09:10,820] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.18917927]
[2019-03-26 21:09:10,822] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.36749526, 79.95759508, 1.0, 2.0, 0.9261086419191327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1294455.824155948, 1294455.824155948, 277225.7673835238]
[2019-03-26 21:09:10,825] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:09:10,826] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7375277e-23 0.0000000e+00], sampled 0.8651765498429749
[2019-03-26 21:10:06,298] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.18917927]
[2019-03-26 21:10:06,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.41666666666667, 74.0, 1.0, 2.0, 0.6829098421683427, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988074666966231, 6.9112, 168.9124352404834, 1851213.884996809, 1796676.518733427, 381653.8280969222]
[2019-03-26 21:10:06,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:10:06,305] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.12618575e-29 1.00000000e+00 2.80065648e-30 1.08706655e-11
 4.28536399e-27], sampled 0.45140808234885355
[2019-03-26 21:10:06,308] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1851213.884996809 W.
[2019-03-26 21:10:15,125] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.18917927]
[2019-03-26 21:10:15,127] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.4, 71.0, 1.0, 2.0, 0.331549381078738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 521097.5879003964, 521097.5879003964, 168575.919186011]
[2019-03-26 21:10:15,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:10:15,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.195044e-30 0.000000e+00], sampled 0.9651195996058457
[2019-03-26 21:10:15,835] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.3614 3008257759.8898 1768.0000
[2019-03-26 21:10:16,136] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.9431 2779421571.3217 919.0000
[2019-03-26 21:10:16,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.5548 2927636896.2275 1326.0000
[2019-03-26 21:10:16,338] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.2099 2843143093.4172 1129.0000
[2019-03-26 21:10:16,438] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.1068 3164975987.6445 1770.0000
[2019-03-26 21:10:17,456] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1050000, evaluation results [1050000.0, 7882.106785907934, 3164975987.644517, 1770.0, 8260.55482184453, 2927636896.227545, 1326.0, 8664.943103899815, 2779421571.32169, 919.0, 8000.36137264863, 3008257759.889753, 1768.0, 8496.209854576988, 2843143093.417241, 1129.0]
[2019-03-26 21:10:24,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.9247884e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:10:24,606] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5433
[2019-03-26 21:10:24,610] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 88.0, 1.0, 2.0, 0.494073798632051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690388.6793581739, 690388.6793581746, 182672.5519115474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7678800.0000, 
sim time next is 7679400.0000, 
raw observation next is [25.86666666666667, 88.00000000000001, 1.0, 2.0, 0.4925865389700069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688309.7998596545, 688309.7998596552, 182442.4780367959], 
processed observation next is [1.0, 0.9130434782608695, 0.42496050552922615, 0.8800000000000001, 1.0, 1.0, 0.3886584806867553, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1911971666276818, 0.191197166627682, 0.2723022060250685], 
reward next is 0.7277, 
noisyNet noise sample is [array([0.16303803], dtype=float32), 0.79183125]. 
=============================================
[2019-03-26 21:10:26,756] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:26,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:26,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-03-26 21:10:28,189] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:28,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:28,270] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-03-26 21:10:30,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.794582e-25 0.000000e+00], sum to 1.0000
[2019-03-26 21:10:30,814] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4011
[2019-03-26 21:10:30,821] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 83.66666666666667, 1.0, 2.0, 0.6462760089666977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 903157.2114684619, 903157.2114684612, 209674.6952352519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7888200.0000, 
sim time next is 7888800.0000, 
raw observation next is [27.1, 83.33333333333334, 1.0, 2.0, 0.661616269386269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924604.2363353607, 924604.2363353601, 212776.644352342], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.8333333333333335, 1.0, 1.0, 0.5923087582967097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2568345100931557, 0.25683451009315555, 0.3175770811228985], 
reward next is 0.6824, 
noisyNet noise sample is [array([0.31017494], dtype=float32), 0.21854167]. 
=============================================
[2019-03-26 21:10:30,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5181196e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:10:30,957] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1691
[2019-03-26 21:10:30,963] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 82.33333333333334, 1.0, 2.0, 0.6884623185816641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 962138.401794215, 962138.4017942144, 218372.8667873019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7890600.0000, 
sim time next is 7891200.0000, 
raw observation next is [27.3, 82.0, 1.0, 2.0, 0.693768660305655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 969557.4943428776, 969557.4943428782, 219504.2672522336], 
processed observation next is [1.0, 0.34782608695652173, 0.4928909952606636, 0.82, 1.0, 1.0, 0.631046578681512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2693215262063549, 0.2693215262063551, 0.32761830933169195], 
reward next is 0.6724, 
noisyNet noise sample is [array([2.0045295], dtype=float32), 1.4339004]. 
=============================================
[2019-03-26 21:10:33,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:33,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:33,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-03-26 21:10:34,496] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.477468e-17 9.999691e-01 4.043472e-19 3.082050e-05 3.154160e-17], sum to 1.0000
[2019-03-26 21:10:34,506] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1991
[2019-03-26 21:10:34,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1743303.995922912 W.
[2019-03-26 21:10:34,520] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.65, 77.0, 1.0, 2.0, 0.6234850928638141, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950818614682857, 6.9112, 168.9126851894014, 1743303.995922912, 1715197.232246737, 369732.1980601398], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7810200.0000, 
sim time next is 7810800.0000, 
raw observation next is [28.76666666666667, 76.33333333333334, 1.0, 2.0, 0.5859183042835416, 1.0, 1.0, 0.5859183042835416, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1638171.604351366, 1638171.604351366, 329107.4657101447], 
processed observation next is [1.0, 0.391304347826087, 0.5624012638230649, 0.7633333333333334, 1.0, 1.0, 0.5011063907030622, 1.0, 0.5, 0.5011063907030622, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4550476678753794, 0.4550476678753794, 0.49120517270170855], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.312015], dtype=float32), 0.68234015]. 
=============================================
[2019-03-26 21:10:34,912] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.2895632e-26 1.0000000e+00 1.4507649e-27 1.1902734e-11 1.2514309e-25], sum to 1.0000
[2019-03-26 21:10:34,921] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5548
[2019-03-26 21:10:34,927] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2010954.94626976 W.
[2019-03-26 21:10:34,931] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.696094923887607, 6.9112, 168.9090522282963, 2010954.94626976, 1454136.343960603, 311353.130305023], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7821600.0000, 
sim time next is 7822200.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.6642529739603071, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.00271586825764, 6.9112, 168.9117735026306, 1825107.238559937, 1760183.186593606, 377214.1570230675], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.7, 1.0, 1.0, 0.5954855107955508, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009151586825764025, 0.0, 0.8294341360424194, 0.5069742329333159, 0.4889397740537794, 0.5630062045120411], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.3520408], dtype=float32), 0.11344083]. 
=============================================
[2019-03-26 21:10:35,209] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:35,211] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:35,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-03-26 21:10:35,796] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1058586: loss 0.7655
[2019-03-26 21:10:35,799] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1058586: learning rate 0.0010
[2019-03-26 21:10:35,856] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.8473613e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:10:35,862] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6136
[2019-03-26 21:10:35,866] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.73333333333333, 78.0, 1.0, 2.0, 0.5261354190154246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735205.1416437647, 735205.1416437647, 187793.2281707237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7842000.0000, 
sim time next is 7842600.0000, 
raw observation next is [28.6, 79.0, 1.0, 2.0, 0.5261323101251307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735200.7958745282, 735200.7958745282, 187792.7899914839], 
processed observation next is [1.0, 0.782608695652174, 0.5545023696682465, 0.79, 1.0, 1.0, 0.4290750724399165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20422244329848008, 0.20422244329848008, 0.28028774625594616], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.7291429], dtype=float32), 0.7143398]. 
=============================================
[2019-03-26 21:10:37,279] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1059339: loss 0.9519
[2019-03-26 21:10:37,283] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1059341: learning rate 0.0010
[2019-03-26 21:10:37,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:37,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:37,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-03-26 21:10:38,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2452757e-15 4.3267542e-01 3.3636464e-16 5.6732452e-01 6.8600663e-14], sum to 1.0000
[2019-03-26 21:10:38,610] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1920
[2019-03-26 21:10:38,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2109511.729452129 W.
[2019-03-26 21:10:38,622] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 65.5, 1.0, 2.0, 0.7543348531358043, 1.0, 2.0, 0.7543348531358043, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2109511.729452129, 2109511.729452129, 398247.2930493431], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7918200.0000, 
sim time next is 7918800.0000, 
raw observation next is [30.53333333333333, 65.0, 1.0, 2.0, 0.7024101506697246, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.981443476661921, 6.9112, 168.912537321678, 1878501.453495463, 1828668.437227766, 386051.4073403339], 
processed observation next is [1.0, 0.6521739130434783, 0.646129541864139, 0.65, 1.0, 1.0, 0.6414580128550899, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00702434766619211, 0.0, 0.8294378867436201, 0.5218059593042953, 0.5079634547854905, 0.5761961303587073], 
reward next is 0.0726, 
noisyNet noise sample is [array([1.2404271], dtype=float32), 0.80328876]. 
=============================================
[2019-03-26 21:10:39,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:39,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:39,928] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-03-26 21:10:40,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:40,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:40,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-03-26 21:10:40,763] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:40,764] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:40,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-03-26 21:10:40,970] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1061244: loss 0.7425
[2019-03-26 21:10:40,972] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1061244: learning rate 0.0010
[2019-03-26 21:10:41,086] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0777747e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:10:41,089] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1461
[2019-03-26 21:10:41,096] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.98333333333333, 91.16666666666667, 1.0, 2.0, 0.480370222992408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725837.356430714, 725837.3564307146, 187275.9277884234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 115800.0000, 
sim time next is 116400.0000, 
raw observation next is [22.96666666666667, 91.33333333333334, 1.0, 2.0, 0.6535443448802459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 987346.1738046781, 987346.1738046781, 220471.0667936307], 
processed observation next is [1.0, 0.34782608695652173, 0.2875197472353872, 0.9133333333333334, 1.0, 1.0, 0.5825835480484891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27426282605685504, 0.27426282605685504, 0.3290612937218369], 
reward next is 0.6709, 
noisyNet noise sample is [array([0.04127279], dtype=float32), 1.1776724]. 
=============================================
[2019-03-26 21:10:41,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,098] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-03-26 21:10:41,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,501] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-03-26 21:10:41,777] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,811] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-03-26 21:10:41,837] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-03-26 21:10:41,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:41,900] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:41,938] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-03-26 21:10:41,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-03-26 21:10:41,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-03-26 21:10:42,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:10:42,017] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:10:42,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-03-26 21:10:42,074] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1061813: loss 0.6440
[2019-03-26 21:10:42,078] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1061815: learning rate 0.0010
[2019-03-26 21:10:43,088] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.306326e-27 0.000000e+00], sum to 1.0000
[2019-03-26 21:10:43,093] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5303
[2019-03-26 21:10:43,096] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 86.0, 1.0, 2.0, 0.2777831290618156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447934.9116638265, 447934.9116638265, 163463.0103031926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 336000.0000, 
sim time next is 336600.0000, 
raw observation next is [21.0, 86.0, 1.0, 2.0, 0.2767291524129772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446506.1808600551, 446506.1808600551, 163368.3651714692], 
processed observation next is [0.0, 0.9130434782608695, 0.19431279620853087, 0.86, 1.0, 1.0, 0.12858934025659902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12402949468334863, 0.12402949468334863, 0.2438333808529391], 
reward next is 0.7562, 
noisyNet noise sample is [array([2.0519583], dtype=float32), 1.1453949]. 
=============================================
[2019-03-26 21:10:43,380] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1062602: loss 0.7346
[2019-03-26 21:10:43,387] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1062602: learning rate 0.0010
[2019-03-26 21:10:46,965] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1064216: loss 1.0878
[2019-03-26 21:10:46,968] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1064217: learning rate 0.0010
[2019-03-26 21:10:47,293] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1064365: loss 0.4617
[2019-03-26 21:10:47,295] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1064365: learning rate 0.0010
[2019-03-26 21:10:47,986] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1064679: loss 0.0024
[2019-03-26 21:10:47,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1064679: learning rate 0.0010
[2019-03-26 21:10:49,086] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9701637e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:10:49,095] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6519
[2019-03-26 21:10:49,106] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2860759064107357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460796.9032282293, 460796.9032282293, 164327.4826521666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 186000.0000, 
sim time next is 186600.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.286127378626733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460879.9610897925, 460879.9610897932, 164333.1420590986], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.1399125043695578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12802221141383124, 0.12802221141383144, 0.2452733463568636], 
reward next is 0.7547, 
noisyNet noise sample is [array([-0.12292155], dtype=float32), 0.121034496]. 
=============================================
[2019-03-26 21:10:49,607] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1065404: loss 0.4722
[2019-03-26 21:10:49,609] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1065404: learning rate 0.0010
[2019-03-26 21:10:49,894] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1065539: loss 0.0039
[2019-03-26 21:10:49,898] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1065541: learning rate 0.0010
[2019-03-26 21:10:50,054] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1065615: loss 0.2756
[2019-03-26 21:10:50,057] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1065615: learning rate 0.0010
[2019-03-26 21:10:51,398] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1066210: loss 0.7907
[2019-03-26 21:10:51,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1066210: learning rate 0.0010
[2019-03-26 21:10:52,030] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1066494: loss 0.1018
[2019-03-26 21:10:52,032] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1066495: learning rate 0.0010
[2019-03-26 21:10:52,234] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1066588: loss 0.0975
[2019-03-26 21:10:52,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1066588: learning rate 0.0010
[2019-03-26 21:10:52,251] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1066592: loss 0.1029
[2019-03-26 21:10:52,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1066594: learning rate 0.0010
[2019-03-26 21:10:52,510] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1066709: loss 0.1445
[2019-03-26 21:10:52,514] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1066711: learning rate 0.0010
[2019-03-26 21:10:52,537] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1066721: loss 0.1010
[2019-03-26 21:10:52,538] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1066722: learning rate 0.0010
[2019-03-26 21:10:52,548] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1066725: loss 0.0954
[2019-03-26 21:10:52,554] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1066729: learning rate 0.0010
[2019-03-26 21:10:55,441] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1068015: loss 0.0022
[2019-03-26 21:10:55,444] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1068015: learning rate 0.0010
[2019-03-26 21:10:58,334] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1069319: loss 0.0026
[2019-03-26 21:10:58,339] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1069320: learning rate 0.0010
[2019-03-26 21:11:01,153] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1070582: loss 0.0067
[2019-03-26 21:11:01,158] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1070582: learning rate 0.0010
[2019-03-26 21:11:01,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.7589174e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:11:01,458] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6245
[2019-03-26 21:11:01,463] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 76.0, 1.0, 2.0, 0.5188976835265001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833468.6755814737, 833468.6755814737, 198534.5302903979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 399600.0000, 
sim time next is 400200.0000, 
raw observation next is [22.55, 76.5, 1.0, 2.0, 0.522385142679708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 838717.0915180474, 838717.0915180481, 199160.1414957579], 
processed observation next is [1.0, 0.6521739130434783, 0.26777251184834133, 0.765, 1.0, 1.0, 0.4245604128671181, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23297696986612426, 0.23297696986612446, 0.297253942530982], 
reward next is 0.7027, 
noisyNet noise sample is [array([-0.24358858], dtype=float32), 0.0533651]. 
=============================================
[2019-03-26 21:11:04,730] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1072188: loss 0.0431
[2019-03-26 21:11:04,731] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1072188: learning rate 0.0010
[2019-03-26 21:11:05,087] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1072347: loss 0.0017
[2019-03-26 21:11:05,089] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1072347: learning rate 0.0010
[2019-03-26 21:11:05,889] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1072711: loss 4.0199
[2019-03-26 21:11:05,893] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1072711: learning rate 0.0010
[2019-03-26 21:11:07,390] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1073392: loss 0.0019
[2019-03-26 21:11:07,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1073393: learning rate 0.0010
[2019-03-26 21:11:07,771] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1073544: loss 3.0479
[2019-03-26 21:11:07,775] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1073545: learning rate 0.0010
[2019-03-26 21:11:08,113] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1073668: loss 0.0021
[2019-03-26 21:11:08,117] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1073669: learning rate 0.0010
[2019-03-26 21:11:09,383] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1074229: loss 0.0236
[2019-03-26 21:11:09,385] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1074229: learning rate 0.0010
[2019-03-26 21:11:09,832] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1074435: loss 0.0230
[2019-03-26 21:11:09,835] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1074435: learning rate 0.0010
[2019-03-26 21:11:10,135] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1074574: loss 0.0125
[2019-03-26 21:11:10,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1074575: learning rate 0.0010
[2019-03-26 21:11:10,155] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1074582: loss 0.0059
[2019-03-26 21:11:10,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1074582: learning rate 0.0010
[2019-03-26 21:11:10,370] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1074685: loss 0.0277
[2019-03-26 21:11:10,372] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1074686: learning rate 0.0010
[2019-03-26 21:11:10,443] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1074718: loss 0.0046
[2019-03-26 21:11:10,446] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1074719: learning rate 0.0010
[2019-03-26 21:11:10,455] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1074721: loss 0.0034
[2019-03-26 21:11:10,458] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1074722: learning rate 0.0010
[2019-03-26 21:11:11,060] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 21:11:11,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:11:11,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:11:11,063] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:11:11,065] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:11:11,065] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:11:11,066] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:11:11,067] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:11:11,067] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:11:11,068] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:11:11,070] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:11:11,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-03-26 21:11:11,116] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-03-26 21:11:11,136] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-03-26 21:11:11,167] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-03-26 21:11:11,167] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-03-26 21:11:30,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:11:30,664] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.16666666666667, 70.16666666666667, 1.0, 2.0, 0.2545285350105946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417344.4070227034, 417344.4070227041, 161359.2172201032]
[2019-03-26 21:11:30,665] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:11:30,667] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8482876e-25 0.0000000e+00], sampled 0.27813739209086563
[2019-03-26 21:11:36,683] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:11:36,684] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.92929874, 86.69787407666668, 1.0, 2.0, 0.4555950240072448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657072.8609516602, 657072.8609516595, 179547.3604555922]
[2019-03-26 21:11:36,687] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:11:36,690] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.4942525e-27 0.0000000e+00], sampled 0.6440579231113172
[2019-03-26 21:12:05,922] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:12:05,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.83333333333333, 71.5, 1.0, 2.0, 0.5427197607385906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758387.8537620524, 758387.8537620524, 190559.7242899053]
[2019-03-26 21:12:05,924] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:12:05,927] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3803377e-27 0.0000000e+00], sampled 0.5186400506845056
[2019-03-26 21:12:08,799] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:12:08,801] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.7492116210321024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047078.492507196, 1047078.492507196, 231826.2129548819]
[2019-03-26 21:12:08,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:12:08,804] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6611734e-23 0.0000000e+00], sampled 0.7290736954752579
[2019-03-26 21:12:14,485] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:12:14,485] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.48333333333333, 75.33333333333334, 1.0, 2.0, 0.5478595057094756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765572.6388595215, 765572.6388595221, 191434.4672676616]
[2019-03-26 21:12:14,487] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:12:14,488] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.9512024e-26 0.0000000e+00], sampled 0.23048021036907318
[2019-03-26 21:12:20,755] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:12:20,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.14261106, 69.59503552, 1.0, 2.0, 0.5769622315949157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806255.8914294239, 806255.8914294239, 196532.0449464339]
[2019-03-26 21:12:20,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:12:20,762] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.180222e-27 0.000000e+00], sampled 0.014041446298955318
[2019-03-26 21:12:50,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:12:50,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.0, 44.33333333333334, 1.0, 2.0, 0.5630426436644519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786797.2410060572, 786797.2410060572, 194063.4748819417]
[2019-03-26 21:12:50,435] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:12:50,437] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.476341e-27 0.000000e+00], sampled 0.629161402540726
[2019-03-26 21:13:04,802] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17528316]
[2019-03-26 21:13:04,805] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.73333333333333, 72.0, 1.0, 2.0, 0.8333664670834602, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98763216577812, 6.9112, 168.9124396145508, 2061772.102251322, 2007548.659187577, 416927.5204114142]
[2019-03-26 21:13:04,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:13:04,810] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.2017135e-35 1.0000000e+00 4.3779721e-37 8.1400899e-16 6.7582247e-35], sampled 0.34060331617768924
[2019-03-26 21:13:04,812] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2061772.102251322 W.
[2019-03-26 21:13:05,440] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.5479 3164832779.9872 1776.0000
[2019-03-26 21:13:05,943] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 21:13:06,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3646 2928274756.9043 1339.0000
[2019-03-26 21:13:06,169] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.2616 2843368869.3792 1131.0000
[2019-03-26 21:13:06,186] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9780 2779795557.8698 926.0000
[2019-03-26 21:13:07,204] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1075000, evaluation results [1075000.0, 7875.547858307967, 3164832779.987157, 1776.0, 8254.364560983993, 2928274756.9042673, 1339.0, 8660.978046802229, 2779795557.8698077, 926.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8494.261612308823, 2843368869.379151, 1131.0]
[2019-03-26 21:13:09,467] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1076016: loss 3.0578
[2019-03-26 21:13:09,469] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1076017: learning rate 0.0010
[2019-03-26 21:13:10,945] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.024485e-30 0.000000e+00], sum to 1.0000
[2019-03-26 21:13:10,956] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3323
[2019-03-26 21:13:10,962] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.63333333333333, 78.16666666666667, 1.0, 2.0, 0.2951472756626367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471027.3742975031, 471027.3742975031, 165001.1317824985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 904200.0000, 
sim time next is 904800.0000, 
raw observation next is [22.76666666666667, 77.33333333333334, 1.0, 2.0, 0.2960186946083676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472269.7981640099, 472269.7981640099, 165086.349841958], 
processed observation next is [0.0, 0.4782608695652174, 0.2780410742496052, 0.7733333333333334, 1.0, 1.0, 0.15182975254020192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1311860550455583, 0.1311860550455583, 0.24639753707754924], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.47476852], dtype=float32), -0.9651173]. 
=============================================
[2019-03-26 21:13:12,044] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3730337e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:13:12,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7136
[2019-03-26 21:13:12,058] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.15, 88.83333333333334, 1.0, 2.0, 0.2669562477402215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434605.4772025349, 434605.4772025342, 162550.7975552343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 798600.0000, 
sim time next is 799200.0000, 
raw observation next is [20.3, 88.0, 1.0, 2.0, 0.2680936876375796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 436119.7882263062, 436119.7882263068, 162655.294652], 
processed observation next is [0.0, 0.2608695652173913, 0.16113744075829392, 0.88, 1.0, 1.0, 0.11818516582840918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1211443856184184, 0.12114438561841855, 0.24276909649552242], 
reward next is 0.7572, 
noisyNet noise sample is [array([-1.0681567], dtype=float32), -0.30777]. 
=============================================
[2019-03-26 21:13:12,423] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1077339: loss 2.5453
[2019-03-26 21:13:12,427] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1077340: learning rate 0.0010
[2019-03-26 21:13:15,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0485172e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:13:15,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0005
[2019-03-26 21:13:15,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 58.33333333333334, 1.0, 2.0, 0.5886806297422197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 964628.6644423761, 964628.6644423761, 213178.2764068074], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 645600.0000, 
sim time next is 646200.0000, 
raw observation next is [24.1, 58.0, 1.0, 2.0, 0.6083669729066368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 997175.8916316932, 997175.8916316939, 217401.3184324259], 
processed observation next is [1.0, 0.4782608695652174, 0.3412322274881518, 0.58, 1.0, 1.0, 0.5281529794055865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2769933032310259, 0.2769933032310261, 0.3244795797498894], 
reward next is 0.6755, 
noisyNet noise sample is [array([1.386894], dtype=float32), -0.2181312]. 
=============================================
[2019-03-26 21:13:15,208] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1078561: loss 1.6905
[2019-03-26 21:13:15,210] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1078562: learning rate 0.0010
[2019-03-26 21:13:18,916] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1080221: loss 1.2092
[2019-03-26 21:13:18,919] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1080222: learning rate 0.0010
[2019-03-26 21:13:19,237] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1080368: loss 1.3016
[2019-03-26 21:13:19,240] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1080369: learning rate 0.0010
[2019-03-26 21:13:20,081] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1080754: loss 0.0103
[2019-03-26 21:13:20,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1080754: learning rate 0.0010
[2019-03-26 21:13:21,442] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1081364: loss 1.4177
[2019-03-26 21:13:21,444] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1081364: learning rate 0.0010
[2019-03-26 21:13:21,855] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1081551: loss 0.0174
[2019-03-26 21:13:21,858] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1081553: learning rate 0.0010
[2019-03-26 21:13:22,261] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1081732: loss 1.1594
[2019-03-26 21:13:22,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1081733: learning rate 0.0010
[2019-03-26 21:13:23,233] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1082168: loss 0.6433
[2019-03-26 21:13:23,236] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1082169: learning rate 0.0010
[2019-03-26 21:13:23,737] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1082395: loss 0.8135
[2019-03-26 21:13:23,738] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1082395: learning rate 0.0010
[2019-03-26 21:13:24,182] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1082595: loss 0.6302
[2019-03-26 21:13:24,183] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1082596: learning rate 0.0010
[2019-03-26 21:13:24,203] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1082603: loss 0.6199
[2019-03-26 21:13:24,205] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1082604: learning rate 0.0010
[2019-03-26 21:13:24,235] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1082617: loss 0.5553
[2019-03-26 21:13:24,237] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1082618: learning rate 0.0010
[2019-03-26 21:13:24,436] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1082708: loss 0.5855
[2019-03-26 21:13:24,437] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1082708: learning rate 0.0010
[2019-03-26 21:13:24,565] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1082766: loss 0.6663
[2019-03-26 21:13:24,568] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1082768: learning rate 0.0010
[2019-03-26 21:13:27,422] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1084043: loss 0.0044
[2019-03-26 21:13:27,425] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1084044: learning rate 0.0010
[2019-03-26 21:13:30,274] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1085312: loss 0.0065
[2019-03-26 21:13:30,278] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1085312: learning rate 0.0010
[2019-03-26 21:13:30,510] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2054455e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:13:30,521] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1344
[2019-03-26 21:13:30,529] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 85.0, 1.0, 2.0, 0.3024544091692027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480719.1836881607, 480719.1836881614, 165658.2429373475], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 855000.0000, 
sim time next is 855600.0000, 
raw observation next is [21.86666666666667, 85.33333333333333, 1.0, 2.0, 0.3024213540901036, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480559.3748945583, 480559.3748945583, 165644.8633146615], 
processed observation next is [0.0, 0.9130434782608695, 0.23538704581358633, 0.8533333333333333, 1.0, 1.0, 0.15954380010855856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1334887152484884, 0.1334887152484884, 0.24723113927561416], 
reward next is 0.7528, 
noisyNet noise sample is [array([-0.70315456], dtype=float32), 1.3472902]. 
=============================================
[2019-03-26 21:13:32,979] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1086531: loss 0.0071
[2019-03-26 21:13:32,985] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1086532: learning rate 0.0010
[2019-03-26 21:13:36,830] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1088253: loss 0.0044
[2019-03-26 21:13:36,833] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1088254: learning rate 0.0010
[2019-03-26 21:13:37,036] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1088348: loss 0.0033
[2019-03-26 21:13:37,038] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1088348: learning rate 0.0010
[2019-03-26 21:13:38,016] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1088783: loss 1.6908
[2019-03-26 21:13:38,019] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1088783: learning rate 0.0010
[2019-03-26 21:13:39,299] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1089357: loss 0.0136
[2019-03-26 21:13:39,305] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1089360: learning rate 0.0010
[2019-03-26 21:13:39,963] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1089665: loss 1.3562
[2019-03-26 21:13:39,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1089665: learning rate 0.0010
[2019-03-26 21:13:40,083] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1089719: loss 0.0022
[2019-03-26 21:13:40,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1089719: learning rate 0.0010
[2019-03-26 21:13:41,026] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1090140: loss 0.0023
[2019-03-26 21:13:41,029] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1090140: learning rate 0.0010
[2019-03-26 21:13:41,553] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1090376: loss 0.0022
[2019-03-26 21:13:41,558] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1090377: learning rate 0.0010
[2019-03-26 21:13:41,781] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3637043e-35 1.0000000e+00 3.1829580e-36 6.9848772e-17 2.2210481e-34], sum to 1.0000
[2019-03-26 21:13:41,784] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4135
[2019-03-26 21:13:41,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1761021.294742933 W.
[2019-03-26 21:13:41,795] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.66666666666667, 72.33333333333333, 1.0, 2.0, 0.6298213893824568, 1.0, 1.0, 0.6298213893824568, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1761021.294742933, 1761021.294742934, 345514.8906995728], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1248000.0000, 
sim time next is 1248600.0000, 
raw observation next is [27.88333333333333, 71.66666666666667, 1.0, 2.0, 0.4209957723986577, 1.0, 2.0, 0.4209957723986577, 1.0, 1.0, 0.7076121991048478, 6.9112, 6.9112, 170.5573041426782, 1765700.919873405, 1765700.919873405, 361230.4122876253], 
processed observation next is [1.0, 0.43478260869565216, 0.5205371248025275, 0.7166666666666667, 1.0, 1.0, 0.3024045450586237, 1.0, 1.0, 0.3024045450586237, 1.0, 0.5, 0.643429511103473, 0.0, 0.0, 0.8375144448122397, 0.4904724777426125, 0.4904724777426125, 0.5391498690860079], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.67447], dtype=float32), -1.1532563]. 
=============================================
[2019-03-26 21:13:41,848] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1090508: loss 0.0033
[2019-03-26 21:13:41,851] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1090509: learning rate 0.0010
[2019-03-26 21:13:41,996] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1090577: loss 0.0052
[2019-03-26 21:13:42,000] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1090577: learning rate 0.0010
[2019-03-26 21:13:42,123] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1090629: loss 0.0063
[2019-03-26 21:13:42,124] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1090630: learning rate 0.0010
[2019-03-26 21:13:42,329] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1090724: loss 0.0022
[2019-03-26 21:13:42,332] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1090725: learning rate 0.0010
[2019-03-26 21:13:42,438] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1090771: loss 0.0029
[2019-03-26 21:13:42,443] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1090771: learning rate 0.0010
[2019-03-26 21:13:43,428] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4490326e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:13:43,433] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8010
[2019-03-26 21:13:43,442] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 76.5, 1.0, 2.0, 0.6154148951735493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970709.9612730056, 970709.9612730056, 216602.6874939174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1079400.0000, 
sim time next is 1080000.0000, 
raw observation next is [23.6, 76.0, 1.0, 2.0, 0.6042018325341043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 950976.3712487163, 950976.371248717, 214021.3898155386], 
processed observation next is [1.0, 0.5217391304347826, 0.3175355450236968, 0.76, 1.0, 1.0, 0.5231347379928968, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2641601031246434, 0.2641601031246436, 0.3194349101724457], 
reward next is 0.6806, 
noisyNet noise sample is [array([0.07916869], dtype=float32), -0.5338575]. 
=============================================
[2019-03-26 21:13:43,479] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[62.596592]
 [62.801468]
 [63.059628]
 [63.123642]
 [63.00354 ]], R is [[62.80233002]
 [62.85102081]
 [62.90369797]
 [62.96313095]
 [63.02612686]].
[2019-03-26 21:13:45,488] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1092125: loss 1.2894
[2019-03-26 21:13:45,491] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1092127: learning rate 0.0010
[2019-03-26 21:13:48,456] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1093456: loss 1.6146
[2019-03-26 21:13:48,458] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1093456: learning rate 0.0010
[2019-03-26 21:13:51,017] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1094610: loss 1.3007
[2019-03-26 21:13:51,021] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1094610: learning rate 0.0010
[2019-03-26 21:13:52,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 8.65027e-27 0.00000e+00], sum to 1.0000
[2019-03-26 21:13:52,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9367
[2019-03-26 21:13:52,087] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.66666666666667, 1.0, 2.0, 0.3507683142374582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542852.3509460131, 542852.3509460137, 170122.107738015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1228800.0000, 
sim time next is 1229400.0000, 
raw observation next is [22.05, 93.0, 1.0, 2.0, 0.3535431868440564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546104.3756138663, 546104.3756138663, 170362.8192823101], 
processed observation next is [1.0, 0.21739130434782608, 0.24407582938388633, 0.93, 1.0, 1.0, 0.2211363696916342, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15169565989274061, 0.15169565989274061, 0.25427286460046283], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.5529791], dtype=float32), -1.2079389]. 
=============================================
[2019-03-26 21:13:54,780] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1096289: loss 0.7863
[2019-03-26 21:13:54,784] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1096291: learning rate 0.0010
[2019-03-26 21:13:54,934] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1096358: loss 0.8718
[2019-03-26 21:13:54,938] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1096358: learning rate 0.0010
[2019-03-26 21:13:55,013] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.2239123e-32 1.0000000e+00 2.6415675e-34 1.3520831e-09 1.7343958e-30], sum to 1.0000
[2019-03-26 21:13:55,023] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9229
[2019-03-26 21:13:55,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2111247.15061353 W.
[2019-03-26 21:13:55,037] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.1, 74.66666666666667, 1.0, 2.0, 0.8687150609881675, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.971243138945748, 6.9112, 168.9125985275383, 2111247.15061353, 2068650.57159661, 426693.4040022239], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1266000.0000, 
sim time next is 1266600.0000, 
raw observation next is [28.05, 74.83333333333333, 1.0, 2.0, 0.4954711268883986, 1.0, 1.0, 0.4954711268883986, 1.0, 2.0, 0.8413549699199876, 6.9112, 6.9112, 170.5573041426782, 2078361.421711283, 2078361.421711283, 408560.4235439116], 
processed observation next is [1.0, 0.6521739130434783, 0.528436018957346, 0.7483333333333333, 1.0, 1.0, 0.39213388781734776, 1.0, 0.5, 0.39213388781734776, 1.0, 1.0, 0.8065304511219361, 0.0, 0.0, 0.8375144448122397, 0.577322617142023, 0.577322617142023, 0.6097916769312114], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14162138], dtype=float32), -0.80353785]. 
=============================================
[2019-03-26 21:13:55,605] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1096659: loss 0.0039
[2019-03-26 21:13:55,610] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1096659: learning rate 0.0010
[2019-03-26 21:13:57,110] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1097330: loss 0.8490
[2019-03-26 21:13:57,113] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1097330: learning rate 0.0010
[2019-03-26 21:13:57,650] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1097573: loss 0.0043
[2019-03-26 21:13:57,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1097573: learning rate 0.0010
[2019-03-26 21:13:57,923] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1097694: loss 0.6466
[2019-03-26 21:13:57,927] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1097695: learning rate 0.0010
[2019-03-26 21:13:58,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4178787e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:13:58,870] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2143
[2019-03-26 21:13:58,874] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.8019855926611088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1191201.307154278, 1191201.307154278, 254015.6109170088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1330800.0000, 
sim time next is 1331400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.8186791100821583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1215964.151809479, 1215964.151809479, 258442.3912793213], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.7815410964845281, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33776781994707755, 0.33776781994707755, 0.385734912357196], 
reward next is 0.6143, 
noisyNet noise sample is [array([-0.11564932], dtype=float32), 0.6836147]. 
=============================================
[2019-03-26 21:13:58,967] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1098164: loss 0.7569
[2019-03-26 21:13:58,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1098164: learning rate 0.0010
[2019-03-26 21:13:59,512] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1098410: loss 0.8167
[2019-03-26 21:13:59,514] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1098411: learning rate 0.0010
[2019-03-26 21:13:59,695] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1098497: loss 0.5914
[2019-03-26 21:13:59,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1098497: learning rate 0.0010
[2019-03-26 21:13:59,970] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1098603: loss 0.5368
[2019-03-26 21:13:59,974] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1098604: learning rate 0.0010
[2019-03-26 21:14:00,027] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1098627: loss 0.4640
[2019-03-26 21:14:00,029] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1098628: learning rate 0.0010
[2019-03-26 21:14:00,436] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1098782: loss 0.7251
[2019-03-26 21:14:00,439] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1098782: learning rate 0.0010
[2019-03-26 21:14:00,504] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1098804: loss 0.7521
[2019-03-26 21:14:00,509] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1098806: learning rate 0.0010
[2019-03-26 21:14:02,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.698899e-24 0.000000e+00], sum to 1.0000
[2019-03-26 21:14:02,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9241
[2019-03-26 21:14:02,248] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 98.0, 1.0, 2.0, 0.3048391428512721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485239.5302921806, 485239.53029218, 165995.854233575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1391400.0000, 
sim time next is 1392000.0000, 
raw observation next is [20.26666666666667, 98.0, 1.0, 2.0, 0.3050233257361261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 485352.8525933396, 485352.8525933389, 166001.3273036943], 
processed observation next is [0.0, 0.08695652173913043, 0.15955766192733034, 0.98, 1.0, 1.0, 0.16267870570617604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13482023683148323, 0.13482023683148303, 0.24776317508014076], 
reward next is 0.7522, 
noisyNet noise sample is [array([1.089997], dtype=float32), -0.32336882]. 
=============================================
[2019-03-26 21:14:02,263] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[79.65766]
 [80.21741]
 [80.89302]
 [80.93775]
 [81.05251]], R is [[79.33381653]
 [79.29272461]
 [79.25205994]
 [79.21183014]
 [79.17199707]].
[2019-03-26 21:14:03,153] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 21:14:03,154] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:14:03,155] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:14:03,155] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:14:03,157] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:14:03,158] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:14:03,159] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:14:03,156] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:14:03,165] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:14:03,168] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:14:03,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:14:03,178] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-03-26 21:14:03,200] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-03-26 21:14:03,217] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-03-26 21:14:03,242] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-03-26 21:14:03,260] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-03-26 21:14:41,982] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:14:41,983] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 100.0, 1.0, 2.0, 0.4300009904569671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 624140.126048276, 624140.1260482754, 176346.2269402324]
[2019-03-26 21:14:41,983] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:14:41,986] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.708621e-25 0.000000e+00], sampled 0.07148908091701578
[2019-03-26 21:14:42,090] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:14:42,091] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.36666666666667, 85.83333333333334, 1.0, 2.0, 0.5088106574395467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710987.9610119676, 710987.961011967, 184987.2946880446]
[2019-03-26 21:14:42,092] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:14:42,096] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1892015e-25 0.0000000e+00], sampled 0.7389901065750073
[2019-03-26 21:14:55,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:14:55,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.96311688666666, 74.21658608666667, 1.0, 2.0, 0.7213037080927152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1008056.613880896, 1008056.613880896, 225507.8174962612]
[2019-03-26 21:14:55,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:14:55,304] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3633586e-24 0.0000000e+00], sampled 0.5676376564945681
[2019-03-26 21:15:01,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:01,103] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 93.16666666666667, 1.0, 2.0, 0.771459003842464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078186.635213228, 1078186.635213228, 237027.3450362605]
[2019-03-26 21:15:01,104] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:15:01,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.659406e-24 0.000000e+00], sampled 0.9401321449402549
[2019-03-26 21:15:09,171] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:09,173] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.0, 60.0, 1.0, 2.0, 0.7602284354855956, 1.0, 2.0, 0.7007042572570603, 1.0, 1.0, 1.03, 7.005102481592744, 6.9112, 170.5573041426782, 2940295.666362191, 2873029.499088574, 541027.9201352113]
[2019-03-26 21:15:09,173] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:15:09,178] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6214704e-37 1.0000000e+00 0.0000000e+00 9.2839417e-16 4.3956773e-37], sampled 0.29241910200844223
[2019-03-26 21:15:09,179] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2940295.666362191 W.
[2019-03-26 21:15:13,527] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:13,528] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.08333333333334, 54.66666666666666, 1.0, 2.0, 0.5357109461493575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748590.3979827136, 748590.3979827136, 189380.264312998]
[2019-03-26 21:15:13,529] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:15:13,531] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.885769e-26 0.000000e+00], sampled 0.9574125710343493
[2019-03-26 21:15:35,501] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:35,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.260919415, 74.65009071, 1.0, 2.0, 0.8962552792785848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1252704.07327624, 1252704.07327624, 268907.399741697]
[2019-03-26 21:15:35,502] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:15:35,505] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.964117e-25 0.000000e+00], sampled 0.9888830390784505
[2019-03-26 21:15:49,675] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:49,676] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.1, 64.0, 1.0, 2.0, 0.7065831619513561, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975694517467, 6.9112, 168.9123160413208, 1884341.010112643, 1817104.125438088, 386100.7582255003]
[2019-03-26 21:15:49,678] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:15:49,680] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.063787e-17 0.000000e+00], sampled 0.17319607547403504
[2019-03-26 21:15:49,683] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1884341.010112643 W.
[2019-03-26 21:15:54,780] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.1805993]
[2019-03-26 21:15:54,782] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.13333333333333, 59.33333333333334, 1.0, 2.0, 0.9555135278994065, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98697509247299, 6.9112, 168.9124443024383, 2232736.69030135, 2178979.394736554, 450618.0945687414]
[2019-03-26 21:15:54,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:15:54,790] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4087099e-37 1.0000000e+00 0.0000000e+00 6.4808900e-16 2.5444497e-37], sampled 0.5907668585544862
[2019-03-26 21:15:54,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2232736.69030135 W.
[2019-03-26 21:15:57,512] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.2616 2843368869.3792 1131.0000
[2019-03-26 21:15:57,724] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.0081 3164870559.0756 1778.0000
[2019-03-26 21:15:57,768] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7044 3008486396.2608 1766.0000
[2019-03-26 21:15:57,841] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7981 2928166400.6515 1338.0000
[2019-03-26 21:15:57,906] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.1120 2780070553.5627 933.0000
[2019-03-26 21:15:58,921] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1100000, evaluation results [1100000.0, 7875.008135704462, 3164870559.075631, 1778.0, 8253.798072009333, 2928166400.6514535, 1338.0, 8658.112016599982, 2780070553.5627275, 933.0, 7997.704377508541, 3008486396.260774, 1766.0, 8494.261612308823, 2843368869.379151, 1131.0]
[2019-03-26 21:15:59,009] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1100044: loss 0.0174
[2019-03-26 21:15:59,012] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1100045: learning rate 0.0010
[2019-03-26 21:16:01,992] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1101384: loss 0.0128
[2019-03-26 21:16:01,996] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1101385: learning rate 0.0010
[2019-03-26 21:16:04,513] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1102517: loss 0.0155
[2019-03-26 21:16:04,515] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1102519: learning rate 0.0010
[2019-03-26 21:16:07,154] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8672282e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:07,164] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9314
[2019-03-26 21:16:07,169] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.31666666666667, 87.0, 1.0, 2.0, 0.49862492834201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696750.2368443463, 696750.2368443457, 183381.0535121461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1879800.0000, 
sim time next is 1880400.0000, 
raw observation next is [26.23333333333333, 87.0, 1.0, 2.0, 0.4979567869974339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695816.3081932284, 695816.3081932284, 183276.5572540208], 
processed observation next is [1.0, 0.782608695652174, 0.44233807266982617, 0.87, 1.0, 1.0, 0.39512865903305294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19328230783145234, 0.19328230783145234, 0.2735471003791355], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.78937984], dtype=float32), -0.23535733]. 
=============================================
[2019-03-26 21:16:08,400] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1104263: loss 0.0463
[2019-03-26 21:16:08,405] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1104263: learning rate 0.0010
[2019-03-26 21:16:08,502] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1104311: loss 0.0439
[2019-03-26 21:16:08,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1104311: learning rate 0.0010
[2019-03-26 21:16:09,574] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1104791: loss -119.3767
[2019-03-26 21:16:09,575] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1104792: learning rate 0.0010
[2019-03-26 21:16:10,753] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1105319: loss 0.0137
[2019-03-26 21:16:10,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1105319: learning rate 0.0010
[2019-03-26 21:16:11,490] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1105643: loss 0.0053
[2019-03-26 21:16:11,492] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1105643: learning rate 0.0010
[2019-03-26 21:16:11,777] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1105775: loss -37.0345
[2019-03-26 21:16:11,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1105776: learning rate 0.0010
[2019-03-26 21:16:12,548] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1106116: loss 0.0059
[2019-03-26 21:16:12,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1106116: learning rate 0.0010
[2019-03-26 21:16:13,106] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1106350: loss 0.0348
[2019-03-26 21:16:13,108] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1106351: learning rate 0.0010
[2019-03-26 21:16:13,237] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1106411: loss 0.0447
[2019-03-26 21:16:13,241] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1106411: learning rate 0.0010
[2019-03-26 21:16:13,510] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1106530: loss 0.0141
[2019-03-26 21:16:13,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1106530: learning rate 0.0010
[2019-03-26 21:16:13,566] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1106555: loss 0.0054
[2019-03-26 21:16:13,567] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1106556: learning rate 0.0010
[2019-03-26 21:16:13,883] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1106694: loss 0.0141
[2019-03-26 21:16:13,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1106695: learning rate 0.0010
[2019-03-26 21:16:14,003] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1106753: loss 0.0084
[2019-03-26 21:16:14,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1106755: learning rate 0.0010
[2019-03-26 21:16:14,092] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.0815307e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:14,099] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3124
[2019-03-26 21:16:14,104] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666666, 89.0, 1.0, 2.0, 0.9251684943578017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1293140.94533404, 1293140.94533404, 276951.4914677984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1680000.0000, 
sim time next is 1680600.0000, 
raw observation next is [25.78333333333333, 88.5, 1.0, 2.0, 0.9213328693092347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287776.504167697, 1287776.504167696, 275870.1029602659], 
processed observation next is [1.0, 0.43478260869565216, 0.4210110584518167, 0.885, 1.0, 1.0, 0.9052203244689575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35771569560213806, 0.3577156956021378, 0.411746422328755], 
reward next is 0.5883, 
noisyNet noise sample is [array([-2.2513902], dtype=float32), 0.18092792]. 
=============================================
[2019-03-26 21:16:15,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3502415e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:15,805] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7475
[2019-03-26 21:16:15,809] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.5137183490508516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721897.8675752864, 721897.8675752857, 186292.2935799577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1738800.0000, 
sim time next is 1739400.0000, 
raw observation next is [24.46666666666667, 94.00000000000001, 1.0, 2.0, 0.5072102117221868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713654.1920733452, 713654.1920733452, 185362.3277467391], 
processed observation next is [1.0, 0.13043478260869565, 0.3586097946287521, 0.9400000000000002, 1.0, 1.0, 0.406277363520707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1982372755759292, 0.1982372755759292, 0.2766601906667748], 
reward next is 0.7233, 
noisyNet noise sample is [array([-0.98016953], dtype=float32), 0.07418407]. 
=============================================
[2019-03-26 21:16:17,252] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1108203: loss -36.6791
[2019-03-26 21:16:17,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1108204: learning rate 0.0010
[2019-03-26 21:16:20,396] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1109612: loss -32.0254
[2019-03-26 21:16:20,398] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1109612: learning rate 0.0010
[2019-03-26 21:16:21,597] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.103204e-28 0.000000e+00], sum to 1.0000
[2019-03-26 21:16:21,603] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7846
[2019-03-26 21:16:21,610] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 93.0, 1.0, 2.0, 0.517420236584005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723022.6742631162, 723022.6742631169, 186370.2758857028], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2159400.0000, 
sim time next is 2160000.0000, 
raw observation next is [25.8, 93.0, 1.0, 2.0, 0.5151240273938602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719812.954775663, 719812.954775663, 185999.4616185058], 
processed observation next is [1.0, 0.0, 0.42180094786729866, 0.93, 1.0, 1.0, 0.4158120811974219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1999480429932397, 0.1999480429932397, 0.2776111367440385], 
reward next is 0.7224, 
noisyNet noise sample is [array([-0.29791045], dtype=float32), -0.24647757]. 
=============================================
[2019-03-26 21:16:21,622] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.655266]
 [75.62726 ]
 [75.59745 ]
 [75.56801 ]
 [75.548836]], R is [[71.62665558]
 [71.63222504]
 [71.6373291 ]
 [71.64219666]
 [71.64674377]].
[2019-03-26 21:16:21,726] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7257665e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:21,732] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5227
[2019-03-26 21:16:21,737] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 89.33333333333334, 1.0, 2.0, 0.3198937329153505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505013.0015271572, 505013.0015271578, 167385.8123236103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1797000.0000, 
sim time next is 1797600.0000, 
raw observation next is [21.56666666666667, 89.66666666666667, 1.0, 2.0, 0.319945060754221, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505384.0424212043, 505384.0424212049, 167419.6411399539], 
processed observation next is [1.0, 0.8260869565217391, 0.22116903633491333, 0.8966666666666667, 1.0, 1.0, 0.18065669970388074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14038445622811233, 0.14038445622811246, 0.24988006140291627], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.9701138], dtype=float32), 0.48261145]. 
=============================================
[2019-03-26 21:16:22,588] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1110597: loss -250.0738
[2019-03-26 21:16:22,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1110597: learning rate 0.0010
[2019-03-26 21:16:26,457] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1112336: loss -178.7594
[2019-03-26 21:16:26,460] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1112337: learning rate 0.0010
[2019-03-26 21:16:26,577] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1112389: loss 13.8840
[2019-03-26 21:16:26,580] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1112390: learning rate 0.0010
[2019-03-26 21:16:27,614] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1112852: loss 4.8600
[2019-03-26 21:16:27,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1112852: learning rate 0.0010
[2019-03-26 21:16:28,697] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1113330: loss -275.5043
[2019-03-26 21:16:28,699] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1113331: learning rate 0.0010
[2019-03-26 21:16:29,422] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1113652: loss 3.7499
[2019-03-26 21:16:29,426] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1113654: learning rate 0.0010
[2019-03-26 21:16:29,707] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.306722e-25 0.000000e+00], sum to 1.0000
[2019-03-26 21:16:29,717] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4272
[2019-03-26 21:16:29,729] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 95.0, 1.0, 2.0, 0.396608801474673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594971.5360482881, 594971.5360482887, 174166.8095628442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1972800.0000, 
sim time next is 1973400.0000, 
raw observation next is [22.75, 95.16666666666667, 1.0, 2.0, 0.3969729916074312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594916.8215369716, 594916.8215369722, 174144.8817710449], 
processed observation next is [1.0, 0.8695652173913043, 0.27725118483412325, 0.9516666666666667, 1.0, 1.0, 0.27346143567160386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16525467264915877, 0.16525467264915894, 0.25991773398663415], 
reward next is 0.7401, 
noisyNet noise sample is [array([1.0102803], dtype=float32), 2.242004]. 
=============================================
[2019-03-26 21:16:29,735] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1113793: loss 4.8270
[2019-03-26 21:16:29,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1113796: learning rate 0.0010
[2019-03-26 21:16:30,627] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1114192: loss -20.8427
[2019-03-26 21:16:30,633] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1114193: learning rate 0.0010
[2019-03-26 21:16:31,142] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1114418: loss 41.7726
[2019-03-26 21:16:31,145] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1114418: learning rate 0.0010
[2019-03-26 21:16:31,262] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1114470: loss -21.0602
[2019-03-26 21:16:31,266] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1114471: learning rate 0.0010
[2019-03-26 21:16:31,271] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1114472: loss -104.4294
[2019-03-26 21:16:31,273] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1114473: learning rate 0.0010
[2019-03-26 21:16:31,355] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1114509: loss -64.3892
[2019-03-26 21:16:31,357] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1114509: learning rate 0.0010
[2019-03-26 21:16:31,528] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.420297e-26 0.000000e+00], sum to 1.0000
[2019-03-26 21:16:31,535] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2572
[2019-03-26 21:16:31,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 89.33333333333334, 1.0, 2.0, 0.5373715115108973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750911.6547711656, 750911.654771165, 189658.679064698], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2149800.0000, 
sim time next is 2150400.0000, 
raw observation next is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
processed observation next is [0.0, 0.9130434782608695, 0.4755134281200636, 0.8966666666666667, 1.0, 1.0, 0.4447071547769099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2092604344049143, 0.2092604344049143, 0.2835075185041212], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.54255825], dtype=float32), 1.719632]. 
=============================================
[2019-03-26 21:16:31,716] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1114672: loss 5.0462
[2019-03-26 21:16:31,722] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1114673: learning rate 0.0010
[2019-03-26 21:16:31,909] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1114761: loss -165.4221
[2019-03-26 21:16:31,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1114762: learning rate 0.0010
[2019-03-26 21:16:35,216] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1116251: loss 4.8154
[2019-03-26 21:16:35,217] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1116251: learning rate 0.0010
[2019-03-26 21:16:38,004] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1117504: loss 3.2441
[2019-03-26 21:16:38,009] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1117504: learning rate 0.0010
[2019-03-26 21:16:39,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7863223e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:39,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2740
[2019-03-26 21:16:39,435] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 77.66666666666666, 1.0, 2.0, 0.5762160637791944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 805212.7898215837, 805212.7898215842, 196398.7064164983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2411400.0000, 
sim time next is 2412000.0000, 
raw observation next is [29.8, 78.0, 1.0, 2.0, 0.5741745914193467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802358.9288674291, 802358.9288674291, 196033.3951373182], 
processed observation next is [1.0, 0.9565217391304348, 0.6113744075829385, 0.78, 1.0, 1.0, 0.4869573390594538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22287748024095255, 0.22287748024095255, 0.29258715692137044], 
reward next is 0.7074, 
noisyNet noise sample is [array([-0.00246688], dtype=float32), -0.35116753]. 
=============================================
[2019-03-26 21:16:39,457] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[63.703854]
 [63.89965 ]
 [63.655235]
 [63.484367]
 [63.258354]], R is [[64.0138855 ]
 [64.08061981]
 [64.14620209]
 [64.21077728]
 [64.27436066]].
[2019-03-26 21:16:40,282] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4108944e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:40,290] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8916
[2019-03-26 21:16:40,298] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.33333333333334, 1.0, 2.0, 0.5635677638004177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787531.3172691067, 787531.3172691074, 194155.6260370398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2121600.0000, 
sim time next is 2122200.0000, 
raw observation next is [30.0, 76.5, 1.0, 2.0, 0.5654173247186274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790116.860644358, 790116.8606443586, 194480.7021696262], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.765, 1.0, 1.0, 0.4764064153236474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21947690573454387, 0.21947690573454404, 0.29026970473078534], 
reward next is 0.7097, 
noisyNet noise sample is [array([1.1588913], dtype=float32), -1.1660705]. 
=============================================
[2019-03-26 21:16:40,364] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1118557: loss 3.6599
[2019-03-26 21:16:40,366] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1118557: learning rate 0.0010
[2019-03-26 21:16:42,340] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 7.84359e-14 0.00000e+00], sum to 1.0000
[2019-03-26 21:16:42,348] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0553
[2019-03-26 21:16:42,353] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.46666666666667, 69.33333333333334, 1.0, 2.0, 0.5346947260411704, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104269, 747169.8554828215, 747169.8554828209, 189213.9751299256], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2222400.0000, 
sim time next is 2223000.0000, 
raw observation next is [31.3, 70.0, 1.0, 2.0, 0.5267521497373326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736067.240411562, 736067.240411562, 187897.6359255071], 
processed observation next is [1.0, 0.7391304347826086, 0.6824644549763034, 0.7, 1.0, 1.0, 0.4298218671534128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.204463122336545, 0.204463122336545, 0.2804442327246374], 
reward next is 0.7196, 
noisyNet noise sample is [array([2.1452286], dtype=float32), 1.1448841]. 
=============================================
[2019-03-26 21:16:42,367] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[60.85038 ]
 [52.453243]
 [41.8932  ]
 [43.32611 ]
 [40.366943]], R is [[64.81648254]
 [64.88591003]
 [64.83226013]
 [64.18393707]
 [63.542099  ]].
[2019-03-26 21:16:42,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.27962135e-20
 0.00000000e+00], sum to 1.0000
[2019-03-26 21:16:42,674] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5613
[2019-03-26 21:16:42,678] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.68333333333333, 80.83333333333333, 1.0, 2.0, 0.5521788851898377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771610.6828985944, 771610.682898595, 192175.1877840316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2328600.0000, 
sim time next is 2329200.0000, 
raw observation next is [28.6, 81.0, 1.0, 2.0, 0.5506626327980185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769491.1155753976, 769491.1155753976, 191914.4735542212], 
processed observation next is [1.0, 1.0, 0.5545023696682465, 0.81, 1.0, 1.0, 0.4586296780699018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2137475321042771, 0.2137475321042771, 0.2864395127674943], 
reward next is 0.7136, 
noisyNet noise sample is [array([-1.5277532], dtype=float32), 0.9923869]. 
=============================================
[2019-03-26 21:16:43,793] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6939877e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:16:43,806] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0363
[2019-03-26 21:16:43,809] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.33333333333334, 1.0, 2.0, 0.6942852340193061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970279.7476539674, 970279.7476539667, 219615.0888536948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [26.4, 89.5, 1.0, 2.0, 0.7177656982018268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003109.739681735, 1003109.739681735, 224725.4000877046], 
processed observation next is [1.0, 0.2608695652173913, 0.45023696682464454, 0.895, 1.0, 1.0, 0.6599586725323214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2786415943560375, 0.2786415943560375, 0.3354110449070218], 
reward next is 0.6646, 
noisyNet noise sample is [array([1.59963], dtype=float32), -0.7371705]. 
=============================================
[2019-03-26 21:16:44,238] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1120284: loss 2.8802
[2019-03-26 21:16:44,242] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1120284: learning rate 0.0010
[2019-03-26 21:16:44,304] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1120314: loss 2.9942
[2019-03-26 21:16:44,305] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1120314: learning rate 0.0010
[2019-03-26 21:16:46,339] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1121225: loss -44.2080
[2019-03-26 21:16:46,341] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1121226: learning rate 0.0010
[2019-03-26 21:16:46,411] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1121251: loss 3.0465
[2019-03-26 21:16:46,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1121252: learning rate 0.0010
[2019-03-26 21:16:46,816] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1121430: loss 2.1442
[2019-03-26 21:16:46,818] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1121431: learning rate 0.0010
[2019-03-26 21:16:48,148] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1122032: loss 5.4009
[2019-03-26 21:16:48,150] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1122032: learning rate 0.0010
[2019-03-26 21:16:48,216] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1122061: loss -293.4515
[2019-03-26 21:16:48,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1122062: learning rate 0.0010
[2019-03-26 21:16:48,667] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1122259: loss 3.4859
[2019-03-26 21:16:48,670] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1122259: learning rate 0.0010
[2019-03-26 21:16:48,805] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1122323: loss 3.9788
[2019-03-26 21:16:48,806] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1122323: loss 3.7128
[2019-03-26 21:16:48,807] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1122323: learning rate 0.0010
[2019-03-26 21:16:48,809] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1122326: learning rate 0.0010
[2019-03-26 21:16:48,912] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1122366: loss 3.9612
[2019-03-26 21:16:48,915] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1122367: learning rate 0.0010
[2019-03-26 21:16:49,192] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1122493: loss 3.6311
[2019-03-26 21:16:49,195] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1122493: learning rate 0.0010
[2019-03-26 21:16:49,573] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1122662: loss 4.1494
[2019-03-26 21:16:49,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1122664: learning rate 0.0010
[2019-03-26 21:16:51,816] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5895023e-22 1.0000000e+00 1.2236279e-23 2.1226347e-09 1.8612903e-21], sum to 1.0000
[2019-03-26 21:16:51,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8284
[2019-03-26 21:16:51,835] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2017686.563741975 W.
[2019-03-26 21:16:51,843] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.8018671455296374, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.000038031036274, 6.9112, 168.912428743322, 2017686.563741975, 1954662.003876106, 408523.3086317382], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [31.91666666666666, 63.83333333333334, 1.0, 2.0, 0.7372011221914514, 1.0, 1.0, 0.7372011221914514, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2061550.820422323, 2061550.820422323, 390459.0099695661], 
processed observation next is [1.0, 0.5217391304347826, 0.7116903633491308, 0.6383333333333334, 1.0, 1.0, 0.6833748460137968, 1.0, 0.5, 0.6833748460137968, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5726530056728675, 0.5726530056728675, 0.582774641745621], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.25287047], dtype=float32), -0.6066]. 
=============================================
[2019-03-26 21:16:53,616] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1124416: loss -226.0361
[2019-03-26 21:16:53,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1124417: learning rate 0.0010
[2019-03-26 21:16:54,876] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 21:16:54,878] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:16:54,879] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:16:54,880] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:16:54,881] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:16:54,881] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:16:54,883] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:16:54,882] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:16:54,885] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:16:54,886] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:16:54,888] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:16:54,907] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-03-26 21:16:54,927] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-03-26 21:16:54,948] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-03-26 21:16:54,968] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-03-26 21:16:54,969] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-03-26 21:17:00,413] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:17:00,414] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.26449362166667, 75.22848746000001, 1.0, 2.0, 0.2969783175565405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473441.4135700373, 473441.4135700373, 165163.9472349689]
[2019-03-26 21:17:00,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:17:00,417] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7585726e-23 0.0000000e+00], sampled 0.31516705681712265
[2019-03-26 21:17:14,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:17:14,420] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.83404112166667, 94.21340969666667, 1.0, 2.0, 0.5596654323732384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807519.6183291347, 807519.6183291347, 196711.4471653251]
[2019-03-26 21:17:14,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:17:14,424] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.186844e-22 0.000000e+00], sampled 0.17750113459887507
[2019-03-26 21:17:23,649] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:17:23,650] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.63333333333333, 86.66666666666667, 1.0, 2.0, 0.6206899003834251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867487.6459204779, 867487.6459204779, 204667.0481160179]
[2019-03-26 21:17:23,651] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:17:23,653] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6540582e-21 0.0000000e+00], sampled 0.38488123608266833
[2019-03-26 21:17:43,386] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:17:43,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.67799454, 90.57412400666666, 1.0, 2.0, 0.7697751312663901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1075832.072519859, 1075832.072519859, 236634.948825016]
[2019-03-26 21:17:43,391] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:17:43,395] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2414776e-20 0.0000000e+00], sampled 0.4641994903714638
[2019-03-26 21:18:05,163] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:18:05,163] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.03672105666667, 91.08338299, 1.0, 2.0, 0.6043211447080005, 0.0, 2.0, 0.0, 1.0, 1.0, 1.023083486227476, 6.9112, 6.9112, 168.9125467222781, 1689677.721614733, 1689677.721614733, 364581.2672681052]
[2019-03-26 21:18:05,164] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:18:05,166] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3661634e-18 0.0000000e+00], sampled 0.0017766705369751667
[2019-03-26 21:18:05,170] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1689677.721614733 W.
[2019-03-26 21:18:26,221] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:18:26,222] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.56666666666667, 83.0, 1.0, 2.0, 0.5671090286293949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 792481.7376208721, 792481.7376208721, 194777.861272265]
[2019-03-26 21:18:26,225] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:18:26,230] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.443328e-23 0.000000e+00], sampled 0.08820990691249186
[2019-03-26 21:18:34,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:18:34,794] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.55231149833333, 60.57125590166667, 1.0, 2.0, 0.4531381055626799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 649658.0529550506, 649658.0529550499, 178695.8097996979]
[2019-03-26 21:18:34,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:18:34,799] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5775076e-23 0.0000000e+00], sampled 0.9567496499611751
[2019-03-26 21:18:48,581] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164676497.9081 1778.0000
[2019-03-26 21:18:48,893] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7138 2779867804.7524 933.0000
[2019-03-26 21:18:49,154] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.17848648]
[2019-03-26 21:18:49,156] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.75712574666667, 64.32378248, 1.0, 2.0, 0.8395184890897291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1238544.693570351, 1238544.69357035, 262978.3897589429]
[2019-03-26 21:18:49,157] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:18:49,159] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2525982e-20 0.0000000e+00], sampled 0.5529237100473132
[2019-03-26 21:18:49,448] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 21:18:49,480] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 21:18:49,563] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 21:18:50,578] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1125000, evaluation results [1125000.0, 7876.615402862808, 3164676497.9080544, 1778.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8659.713804063274, 2779867804.752359, 933.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 21:18:52,004] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1125642: loss -281.2715
[2019-03-26 21:18:52,009] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1125642: learning rate 0.0010
[2019-03-26 21:18:52,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.8893473e-28 1.0000000e+00 5.5173921e-29 3.2469090e-12 8.4536584e-27], sum to 1.0000
[2019-03-26 21:18:52,443] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4251
[2019-03-26 21:18:52,453] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1864558.56580156 W.
[2019-03-26 21:18:52,457] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [25.9, 89.0, 1.0, 2.0, 0.6924463082902399, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.967065013234626, 6.9112, 168.9118001431194, 1864558.56580156, 1824926.274009545, 384296.56999391], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2462400.0000, 
sim time next is 2463000.0000, 
raw observation next is [25.95, 89.0, 1.0, 2.0, 0.4635521460552256, 1.0, 1.0, 0.4635521460552256, 1.0, 2.0, 0.7866802850288904, 6.911200000000001, 6.9112, 170.5573041426782, 1944348.811264633, 1944348.811264632, 387765.332605847], 
processed observation next is [1.0, 0.5217391304347826, 0.42890995260663506, 0.89, 1.0, 1.0, 0.3536772844038863, 1.0, 0.5, 0.3536772844038863, 1.0, 1.0, 0.7398540061327931, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5400968920179536, 0.5400968920179533, 0.5787542277699209], 
reward next is 0.4212, 
noisyNet noise sample is [array([0.21733741], dtype=float32), 2.0045664]. 
=============================================
[2019-03-26 21:18:52,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[37.73244 ]
 [36.94594 ]
 [34.141354]
 [35.68371 ]
 [34.110195]], R is [[33.95326614]
 [33.6137352 ]
 [33.27759933]
 [32.94482422]
 [32.61537552]].
[2019-03-26 21:18:54,349] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1126694: loss -294.8131
[2019-03-26 21:18:54,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1126694: learning rate 0.0010
[2019-03-26 21:18:55,869] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.5670965e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:18:55,878] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2960
[2019-03-26 21:18:55,885] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 85.5, 1.0, 2.0, 0.7473744893047615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044509.702720558, 1044509.702720558, 231403.9013687684], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2446200.0000, 
sim time next is 2446800.0000, 
raw observation next is [27.7, 85.66666666666666, 1.0, 2.0, 0.7379706602272049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031360.79186168, 1031360.79186168, 229255.3350940973], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.8566666666666666, 1.0, 1.0, 0.6843020002737409, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2864891088504667, 0.2864891088504667, 0.34217214193148854], 
reward next is 0.6578, 
noisyNet noise sample is [array([-2.0662632], dtype=float32), -0.6864001]. 
=============================================
[2019-03-26 21:18:58,022] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1128335: loss -70.5846
[2019-03-26 21:18:58,023] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1128335: loss -54.5895
[2019-03-26 21:18:58,023] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1128335: learning rate 0.0010
[2019-03-26 21:18:58,025] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1128335: learning rate 0.0010
[2019-03-26 21:18:59,107] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1128819: loss 0.6380
[2019-03-26 21:18:59,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1128819: learning rate 0.0010
[2019-03-26 21:19:00,484] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1129432: loss -150.9243
[2019-03-26 21:19:00,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1129432: learning rate 0.0010
[2019-03-26 21:19:00,819] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1129588: loss -410.8070
[2019-03-26 21:19:00,824] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1129589: learning rate 0.0010
[2019-03-26 21:19:01,052] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1129690: loss 0.4218
[2019-03-26 21:19:01,055] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1129691: learning rate 0.0010
[2019-03-26 21:19:01,442] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.050595e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:19:01,454] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0829
[2019-03-26 21:19:01,457] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.6481873136697233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 905829.3588800053, 905829.358880006, 210059.9434906598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2519400.0000, 
sim time next is 2520000.0000, 
raw observation next is [26.3, 96.0, 1.0, 2.0, 0.6430358996279928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898627.3108131714, 898627.310813172, 209030.992515251], 
processed observation next is [1.0, 0.17391304347826086, 0.4454976303317536, 0.96, 1.0, 1.0, 0.5699227706361358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24961869744810317, 0.24961869744810333, 0.311986555992912], 
reward next is 0.6880, 
noisyNet noise sample is [array([0.26242042], dtype=float32), -1.1068876]. 
=============================================
[2019-03-26 21:19:01,470] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[59.777428]
 [59.682304]
 [59.579975]
 [59.066216]
 [59.153793]], R is [[60.00673676]
 [60.09314728]
 [60.17551804]
 [60.24866486]
 [60.31938553]].
[2019-03-26 21:19:02,057] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1130140: loss -49.8277
[2019-03-26 21:19:02,058] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1130140: learning rate 0.0010
[2019-03-26 21:19:02,732] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1130437: loss -82.0846
[2019-03-26 21:19:02,734] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1130438: learning rate 0.0010
[2019-03-26 21:19:02,777] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1130456: loss -75.9542
[2019-03-26 21:19:02,780] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1130458: learning rate 0.0010
[2019-03-26 21:19:02,896] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1130515: loss -84.8637
[2019-03-26 21:19:02,899] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1130515: learning rate 0.0010
[2019-03-26 21:19:02,917] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1130523: loss -168.0346
[2019-03-26 21:19:02,919] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1130523: learning rate 0.0010
[2019-03-26 21:19:03,203] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1130647: loss -114.9756
[2019-03-26 21:19:03,204] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1130647: learning rate 0.0010
[2019-03-26 21:19:03,716] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1130877: loss -42.9175
[2019-03-26 21:19:03,720] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1130879: learning rate 0.0010
[2019-03-26 21:19:06,419] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1132107: loss 0.5208
[2019-03-26 21:19:06,422] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1132108: learning rate 0.0010
[2019-03-26 21:19:07,841] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.337853e-23 0.000000e+00], sum to 1.0000
[2019-03-26 21:19:07,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6141
[2019-03-26 21:19:07,860] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4749715298666664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663817.185979863, 663817.185979863, 179783.9465901326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2623200.0000, 
sim time next is 2623800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4754971946229017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664551.669029083, 664551.6690290824, 179862.3850760613], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3680689091842189, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18459768584141195, 0.18459768584141178, 0.26845132100904673], 
reward next is 0.7315, 
noisyNet noise sample is [array([-0.6345516], dtype=float32), 0.111447]. 
=============================================
[2019-03-26 21:19:09,252] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1182564e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:09,261] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0418
[2019-03-26 21:19:09,271] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3989862227200194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594915.4175127856, 594915.4175127856, 174057.6605508872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2664000.0000, 
sim time next is 2664600.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3977482839684885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593474.40523567, 593474.40523567, 173936.5160771536], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.2743955228536006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16485400145435278, 0.16485400145435278, 0.25960674041366205], 
reward next is 0.7404, 
noisyNet noise sample is [array([0.6054963], dtype=float32), 0.36802995]. 
=============================================
[2019-03-26 21:19:09,398] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2156785e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:09,408] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6508
[2019-03-26 21:19:09,413] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3977482839684885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593474.40523567, 593474.40523567, 173936.5160771536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2664600.0000, 
sim time next is 2665200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3971945146211823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592670.2684337221, 592670.2684337215, 173863.0354437851], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27372833086889437, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16463063012047835, 0.16463063012047818, 0.2594970678265449], 
reward next is 0.7405, 
noisyNet noise sample is [array([-1.9317658], dtype=float32), 0.5675136]. 
=============================================
[2019-03-26 21:19:09,492] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1133466: loss 0.6235
[2019-03-26 21:19:09,495] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1133467: learning rate 0.0010
[2019-03-26 21:19:11,838] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1134512: loss 0.6133
[2019-03-26 21:19:11,841] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1134512: learning rate 0.0010
[2019-03-26 21:19:12,237] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1238363e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:12,246] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4272
[2019-03-26 21:19:12,252] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 100.0, 1.0, 2.0, 0.4503384064312532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.9493691779, 643133.9493691786, 177966.7730345486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2709600.0000, 
sim time next is 2710200.0000, 
raw observation next is [23.16666666666667, 100.0, 1.0, 2.0, 0.4427102211633037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636735.7042807669, 636735.7042807676, 177437.2248689152], 
processed observation next is [0.0, 0.34782608695652173, 0.2969984202211693, 1.0, 1.0, 1.0, 0.3285665315220526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1768710289668797, 0.1768710289668799, 0.26483167890882864], 
reward next is 0.7352, 
noisyNet noise sample is [array([-1.3141831], dtype=float32), -1.0529463]. 
=============================================
[2019-03-26 21:19:15,715] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1136255: loss 0.4075
[2019-03-26 21:19:15,716] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1136255: learning rate 0.0010
[2019-03-26 21:19:15,761] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1136275: loss 0.3994
[2019-03-26 21:19:15,763] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1136275: learning rate 0.0010
[2019-03-26 21:19:16,803] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1136744: loss 9.0882
[2019-03-26 21:19:16,806] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1136744: learning rate 0.0010
[2019-03-26 21:19:16,908] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.722208e-26 0.000000e+00], sum to 1.0000
[2019-03-26 21:19:16,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5342
[2019-03-26 21:19:16,924] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4094262076607182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603250.7644546658, 603250.7644546658, 174616.0758322382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2833800.0000, 
sim time next is 2834400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4091672451503332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602870.9496955557, 602870.9496955563, 174580.6887437804], 
processed observation next is [1.0, 0.8260869565217391, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28815330741004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1674641526932099, 0.16746415269321008, 0.2605681921548961], 
reward next is 0.7394, 
noisyNet noise sample is [array([-0.41357538], dtype=float32), 1.7518098]. 
=============================================
[2019-03-26 21:19:17,233] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1072147e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:17,249] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0304
[2019-03-26 21:19:17,256] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3084446884230885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490771.874960068, 490771.8749600674, 166395.1436372788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3035400.0000, 
sim time next is 3036000.0000, 
raw observation next is [20.33333333333334, 98.0, 1.0, 2.0, 0.3012956036091493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478834.0511005365, 478834.0511005365, 165522.358767957], 
processed observation next is [1.0, 0.13043478260869565, 0.16271721958925783, 0.98, 1.0, 1.0, 0.15818747422789073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1330094586390379, 0.1330094586390379, 0.24704829666859252], 
reward next is 0.7530, 
noisyNet noise sample is [array([1.3092005], dtype=float32), 1.6805134]. 
=============================================
[2019-03-26 21:19:17,268] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.48585 ]
 [77.454285]
 [77.477036]
 [77.581535]
 [77.82132 ]], R is [[77.34873199]
 [77.32689667]
 [77.30473328]
 [77.28268433]
 [77.26025391]].
[2019-03-26 21:19:18,198] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1137370: loss 0.6673
[2019-03-26 21:19:18,200] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1137370: learning rate 0.0010
[2019-03-26 21:19:18,653] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1137574: loss 0.7945
[2019-03-26 21:19:18,655] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1137575: learning rate 0.0010
[2019-03-26 21:19:18,773] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1137627: loss 9.3190
[2019-03-26 21:19:18,777] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1137628: learning rate 0.0010
[2019-03-26 21:19:19,892] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1138130: loss 0.7697
[2019-03-26 21:19:19,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1138131: learning rate 0.0010
[2019-03-26 21:19:20,659] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1138471: loss 0.8522
[2019-03-26 21:19:20,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1138471: learning rate 0.0010
[2019-03-26 21:19:20,673] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1138477: loss 0.7541
[2019-03-26 21:19:20,675] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1138477: learning rate 0.0010
[2019-03-26 21:19:20,787] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1138525: loss 0.7190
[2019-03-26 21:19:20,790] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1138526: learning rate 0.0010
[2019-03-26 21:19:20,872] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1138562: loss 0.7032
[2019-03-26 21:19:20,880] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1138562: learning rate 0.0010
[2019-03-26 21:19:21,086] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1138660: loss 0.3397
[2019-03-26 21:19:21,089] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1138660: learning rate 0.0010
[2019-03-26 21:19:21,787] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1138973: loss 0.5045
[2019-03-26 21:19:21,791] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1138975: learning rate 0.0010
[2019-03-26 21:19:24,390] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1140129: loss 9.5421
[2019-03-26 21:19:24,392] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1140129: learning rate 0.0010
[2019-03-26 21:19:27,368] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1141469: loss 9.1888
[2019-03-26 21:19:27,369] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1141470: learning rate 0.0010
[2019-03-26 21:19:29,702] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1142514: loss 8.6586
[2019-03-26 21:19:29,704] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1142515: learning rate 0.0010
[2019-03-26 21:19:31,999] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1276558e-20 9.9999964e-01 2.6563055e-22 3.4834991e-07 1.5970576e-21], sum to 1.0000
[2019-03-26 21:19:32,011] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7209
[2019-03-26 21:19:32,023] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2711836.7202152 W.
[2019-03-26 21:19:32,026] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666667, 70.33333333333334, 1.0, 2.0, 0.6514580236105628, 1.0, 2.0, 0.646319051319544, 1.0, 1.0, 1.03, 7.005093904978501, 6.9112, 170.5573041426782, 2711836.7202152, 2644576.69671924, 506434.8893067393], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3409800.0000, 
sim time next is 3410400.0000, 
raw observation next is [32.33333333333334, 69.66666666666667, 1.0, 2.0, 0.9373127036161045, 1.0, 2.0, 0.9373127036161045, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2621767.559869938, 2621767.559869938, 492264.4031524216], 
processed observation next is [1.0, 0.4782608695652174, 0.7314375987361774, 0.6966666666666668, 1.0, 1.0, 0.9244731368868729, 1.0, 1.0, 0.9244731368868729, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7282687666305383, 0.7282687666305383, 0.7347229897797337], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.48573408], dtype=float32), -0.8960169]. 
=============================================
[2019-03-26 21:19:33,557] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1144239: loss 9.7576
[2019-03-26 21:19:33,561] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1144239: learning rate 0.0010
[2019-03-26 21:19:33,642] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1144272: loss 9.5441
[2019-03-26 21:19:33,646] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1144273: learning rate 0.0010
[2019-03-26 21:19:33,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.1466728e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:33,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3501
[2019-03-26 21:19:33,681] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072600.0000, 
sim time next is 3073200.0000, 
raw observation next is [23.0, 98.0, 1.0, 2.0, 0.8501543741860447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1247407.785213543, 1247407.785213543, 264967.8854743962], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.98, 1.0, 1.0, 0.8194631014289695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34650216255931754, 0.34650216255931754, 0.3954744559319346], 
reward next is 0.6045, 
noisyNet noise sample is [array([0.90758616], dtype=float32), 0.8537962]. 
=============================================
[2019-03-26 21:19:34,963] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1144867: loss 7.0150
[2019-03-26 21:19:34,968] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1144867: learning rate 0.0010
[2019-03-26 21:19:35,925] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1145301: loss 9.7574
[2019-03-26 21:19:35,928] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1145302: learning rate 0.0010
[2019-03-26 21:19:36,427] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1145527: loss 9.6333
[2019-03-26 21:19:36,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1145527: learning rate 0.0010
[2019-03-26 21:19:37,071] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1145812: loss 7.1527
[2019-03-26 21:19:37,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1145812: learning rate 0.0010
[2019-03-26 21:19:37,695] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1146076: loss 9.9384
[2019-03-26 21:19:37,698] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1146076: learning rate 0.0010
[2019-03-26 21:19:38,327] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1146356: loss 9.8547
[2019-03-26 21:19:38,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1146357: learning rate 0.0010
[2019-03-26 21:19:38,547] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1146455: loss 9.4623
[2019-03-26 21:19:38,548] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1146455: learning rate 0.0010
[2019-03-26 21:19:38,594] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1146475: loss 9.4926
[2019-03-26 21:19:38,598] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1146475: learning rate 0.0010
[2019-03-26 21:19:38,634] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1146494: loss 9.6858
[2019-03-26 21:19:38,641] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1146497: learning rate 0.0010
[2019-03-26 21:19:38,929] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1146627: loss 9.9165
[2019-03-26 21:19:38,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1146627: learning rate 0.0010
[2019-03-26 21:19:39,624] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1146938: loss 10.3007
[2019-03-26 21:19:39,625] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1146938: learning rate 0.0010
[2019-03-26 21:19:39,949] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0409973e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:39,959] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0733
[2019-03-26 21:19:39,962] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 76.33333333333334, 1.0, 2.0, 0.5863564233374999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819388.542952429, 819388.542952429, 198231.5427192179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3262800.0000, 
sim time next is 3263400.0000, 
raw observation next is [30.0, 77.0, 1.0, 2.0, 0.578124993567868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807881.3708210465, 807881.3708210465, 196741.2898296856], 
processed observation next is [0.0, 0.782608695652174, 0.6208530805687204, 0.77, 1.0, 1.0, 0.4917168597203228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22441149189473514, 0.22441149189473514, 0.2936437161637098], 
reward next is 0.7064, 
noisyNet noise sample is [array([-0.19121067], dtype=float32), -0.24761532]. 
=============================================
[2019-03-26 21:19:42,547] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1148238: loss 7.5946
[2019-03-26 21:19:42,551] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1148238: learning rate 0.0010
[2019-03-26 21:19:42,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7644402e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:19:42,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4204
[2019-03-26 21:19:42,914] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 86.5, 1.0, 2.0, 0.5054346614254485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 706268.9355070761, 706268.9355070768, 184451.9452195093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3461400.0000, 
sim time next is 3462000.0000, 
raw observation next is [26.33333333333334, 87.33333333333333, 1.0, 2.0, 0.5035865515859838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703685.6245507251, 703685.6245507258, 184159.9167561295], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.8733333333333333, 1.0, 1.0, 0.40191150793492014, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19546822904186809, 0.19546822904186828, 0.2748655473972082], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.00044209], dtype=float32), 0.3506976]. 
=============================================
[2019-03-26 21:19:42,931] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.185394]
 [62.110844]
 [62.04001 ]
 [62.15608 ]
 [62.178047]], R is [[62.34987259]
 [62.45107269]
 [62.55092621]
 [62.64964294]
 [62.74727249]].
[2019-03-26 21:19:45,646] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1149569: loss 6.8946
[2019-03-26 21:19:45,648] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1149569: learning rate 0.0010
[2019-03-26 21:19:46,583] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 21:19:46,586] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:19:46,588] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:19:46,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:19:46,589] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:19:46,589] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:19:46,591] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:19:46,592] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:19:46,593] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:19:46,593] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:19:46,594] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:19:46,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-03-26 21:19:46,618] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-03-26 21:19:46,661] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-03-26 21:19:46,662] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-03-26 21:19:46,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-03-26 21:19:54,195] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.14587168]
[2019-03-26 21:19:54,196] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.03333333333334, 53.0, 1.0, 2.0, 0.5232164997916068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 857609.5708894643, 857609.5708894636, 200104.891505163]
[2019-03-26 21:19:54,197] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:19:54,201] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1957756e-15 0.0000000e+00], sampled 0.8750576625752784
[2019-03-26 21:19:57,685] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.14587168]
[2019-03-26 21:19:57,687] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.86666666666667, 74.66666666666667, 1.0, 2.0, 0.3515438377730944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564027.5833029309, 564027.5833029303, 172104.511297253]
[2019-03-26 21:19:57,688] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:19:57,690] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3745556e-15 0.0000000e+00], sampled 0.7698810388524913
[2019-03-26 21:20:12,464] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.14587168]
[2019-03-26 21:20:12,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.43333333333333, 93.0, 1.0, 2.0, 0.4421237235730829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652729.1447539162, 652729.1447539156, 179438.1233896531]
[2019-03-26 21:20:12,466] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:20:12,469] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6329667e-15 0.0000000e+00], sampled 0.574071261578851
[2019-03-26 21:21:38,497] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.14587168]
[2019-03-26 21:21:38,499] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 86.0, 1.0, 2.0, 0.6047822414663392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845147.4677606197, 845147.4677606197, 201634.1579898413]
[2019-03-26 21:21:38,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:21:38,503] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0213196e-15 0.0000000e+00], sampled 0.570998903469019
[2019-03-26 21:21:40,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.7687 3007861007.2974 1756.0000
[2019-03-26 21:21:40,869] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.3812 3163232805.8263 1738.0000
[2019-03-26 21:21:40,967] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8665.1062 2779320443.3978 914.0000
[2019-03-26 21:21:41,084] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8267.7450 2926874354.5254 1306.0000
[2019-03-26 21:21:41,103] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.1123 2843085475.1306 1124.0000
[2019-03-26 21:21:42,123] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1150000, evaluation results [1150000.0, 7889.381230299712, 3163232805.826252, 1738.0, 8267.745034146692, 2926874354.525415, 1306.0, 8665.106220721344, 2779320443.3978086, 914.0, 8002.768729742936, 3007861007.297421, 1756.0, 8498.112336572485, 2843085475.130578, 1124.0]
[2019-03-26 21:21:42,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.090192e-15 0.000000e+00], sum to 1.0000
[2019-03-26 21:21:42,523] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7367
[2019-03-26 21:21:42,529] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.4878208114317547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681648.3319470783, 681648.331947079, 181710.0035530568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3312600.0000, 
sim time next is 3313200.0000, 
raw observation next is [28.33333333333334, 72.66666666666667, 1.0, 2.0, 0.4904327901074785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 685299.3143714768, 685299.3143714775, 182110.7996345521], 
processed observation next is [0.0, 0.34782608695652173, 0.5418641390205374, 0.7266666666666667, 1.0, 1.0, 0.38606360253913075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19036092065874355, 0.19036092065874374, 0.27180716363365987], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.03231743], dtype=float32), -0.043389875]. 
=============================================
[2019-03-26 21:21:43,415] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1150577: loss 8.1676
[2019-03-26 21:21:43,419] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1150577: learning rate 0.0010
[2019-03-26 21:21:44,037] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.131298e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:21:44,047] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3906
[2019-03-26 21:21:44,051] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.5964550567010019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833506.1528253664, 833506.1528253664, 200087.6766423961], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3338400.0000, 
sim time next is 3339000.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.5949757077889694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831438.0522696073, 831438.0522696073, 199814.0484636618], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.75, 1.0, 1.0, 0.512018925046951, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23095501451933537, 0.23095501451933537, 0.2982299230800923], 
reward next is 0.7018, 
noisyNet noise sample is [array([-1.6941762], dtype=float32), -0.6024754]. 
=============================================
[2019-03-26 21:21:44,075] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.07203]
 [70.07101]
 [70.06699]
 [69.93708]
 [69.91291]], R is [[70.11676025]
 [70.11695862]
 [70.11703491]
 [70.11743164]
 [70.11733246]].
[2019-03-26 21:21:47,056] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.881524e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:21:47,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1708
[2019-03-26 21:21:47,072] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5117892573419964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715151.5167799335, 715151.516779934, 185463.7158761521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3453600.0000, 
sim time next is 3454200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5108681047745739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713863.9066937568, 713863.9066937568, 185316.3543068274], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41068446358382393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19829552963715466, 0.19829552963715466, 0.2765915735922797], 
reward next is 0.7234, 
noisyNet noise sample is [array([-0.64308614], dtype=float32), -0.2834811]. 
=============================================
[2019-03-26 21:21:47,170] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1152258: loss 7.4845
[2019-03-26 21:21:47,173] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1152259: learning rate 0.0010
[2019-03-26 21:21:47,173] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1152261: loss 7.9098
[2019-03-26 21:21:47,178] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1152263: learning rate 0.0010
[2019-03-26 21:21:49,209] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1153179: loss 22.5835
[2019-03-26 21:21:49,215] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1153183: learning rate 0.0010
[2019-03-26 21:21:49,358] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1153244: loss 5.4400
[2019-03-26 21:21:49,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1153244: learning rate 0.0010
[2019-03-26 21:21:49,845] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1153466: loss 4.5034
[2019-03-26 21:21:49,847] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1153466: learning rate 0.0010
[2019-03-26 21:21:50,985] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1153971: loss 5.6603
[2019-03-26 21:21:50,990] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1153972: learning rate 0.0010
[2019-03-26 21:21:51,118] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1154032: loss -16.2044
[2019-03-26 21:21:51,121] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1154033: learning rate 0.0010
[2019-03-26 21:21:51,476] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1154192: loss 4.8863
[2019-03-26 21:21:51,479] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1154193: learning rate 0.0010
[2019-03-26 21:21:51,755] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1154318: loss 5.6325
[2019-03-26 21:21:51,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1154319: learning rate 0.0010
[2019-03-26 21:21:51,792] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1154334: loss 5.9859
[2019-03-26 21:21:51,796] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1154334: learning rate 0.0010
[2019-03-26 21:21:52,027] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1154437: loss 5.6594
[2019-03-26 21:21:52,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1154438: learning rate 0.0010
[2019-03-26 21:21:52,162] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1154496: loss 5.9510
[2019-03-26 21:21:52,164] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1154497: learning rate 0.0010
[2019-03-26 21:21:52,964] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1154858: loss 6.7237
[2019-03-26 21:21:52,968] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1154858: learning rate 0.0010
[2019-03-26 21:21:56,448] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1156423: loss -10.7406
[2019-03-26 21:21:56,451] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1156423: learning rate 0.0010
[2019-03-26 21:21:57,958] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3745863e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:21:57,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9834
[2019-03-26 21:21:57,975] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.7268589847660603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1015824.093397069, 1015824.093397069, 226746.1007951709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3553200.0000, 
sim time next is 3553800.0000, 
raw observation next is [26.91666666666666, 79.33333333333334, 1.0, 2.0, 0.7438852858019134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039630.902383026, 1039630.902383027, 230599.472486983], 
processed observation next is [1.0, 0.13043478260869565, 0.4747235387045811, 0.7933333333333334, 1.0, 1.0, 0.6914280551830282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28878636177306277, 0.28878636177306305, 0.3441783171447507], 
reward next is 0.6558, 
noisyNet noise sample is [array([-0.619192], dtype=float32), 0.52511376]. 
=============================================
[2019-03-26 21:21:59,204] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1157656: loss 0.0710
[2019-03-26 21:21:59,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1157656: learning rate 0.0010
[2019-03-26 21:22:01,556] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1158695: loss -3.5164
[2019-03-26 21:22:01,559] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1158695: learning rate 0.0010
[2019-03-26 21:22:05,122] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1160289: loss 0.0493
[2019-03-26 21:22:05,126] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1160291: learning rate 0.0010
[2019-03-26 21:22:05,230] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1160336: loss 0.0595
[2019-03-26 21:22:05,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1160336: learning rate 0.0010
[2019-03-26 21:22:06,737] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.2162375e-17 9.9999988e-01 5.7898874e-19 6.1762115e-08 7.5755897e-18], sum to 1.0000
[2019-03-26 21:22:06,748] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3769
[2019-03-26 21:22:06,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2691583.961605778 W.
[2019-03-26 21:22:06,771] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.962246029634333, 1.0, 2.0, 0.962246029634333, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2691583.961605778, 2691583.961605778, 506586.0249939169], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3683400.0000, 
sim time next is 3684000.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.6708097502682426, 1.0, 2.0, 0.6559949146483839, 1.0, 1.0, 1.03, 7.005095430644848, 6.9112, 170.5573041426782, 2752479.599765052, 2685218.483372227, 512266.5814031787], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.59, 1.0, 1.0, 0.6033852412870393, 1.0, 1.0, 0.5855360417450408, 1.0, 0.5, 1.0365853658536586, 0.009389543064484761, 0.0, 0.8375144448122397, 0.7645776666014034, 0.7458940231589519, 0.7645769871689234], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7729827], dtype=float32), 1.0029663]. 
=============================================
[2019-03-26 21:22:06,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[25.89614 ]
 [26.068392]
 [25.145887]
 [25.166895]
 [25.239304]], R is [[24.93927193]
 [24.93378258]
 [24.98636246]
 [25.03917694]
 [25.07365227]].
[2019-03-26 21:22:06,871] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1161074: loss 0.0606
[2019-03-26 21:22:06,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1161074: learning rate 0.0010
[2019-03-26 21:22:07,485] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1161348: loss -6.8916
[2019-03-26 21:22:07,490] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1161349: learning rate 0.0010
[2019-03-26 21:22:07,744] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1161466: loss -2.2515
[2019-03-26 21:22:07,745] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1161466: learning rate 0.0010
[2019-03-26 21:22:08,725] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3088232e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:08,735] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8394
[2019-03-26 21:22:08,740] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5441111608541516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 760332.8697530448, 760332.8697530441, 190796.0943649762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4053600.0000, 
sim time next is 4054200.0000, 
raw observation next is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.543206161525321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759067.7852456463, 759067.785245647, 190642.5440530445], 
processed observation next is [1.0, 0.9565217391304348, 0.5181674565560824, 0.8483333333333333, 1.0, 1.0, 0.44964597774135057, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21085216256823508, 0.21085216256823527, 0.2845411105269321], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.4532151], dtype=float32), 0.41926682]. 
=============================================
[2019-03-26 21:22:08,830] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1161951: loss 0.0519
[2019-03-26 21:22:08,831] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1161951: learning rate 0.0010
[2019-03-26 21:22:09,210] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1162123: loss 31.0432
[2019-03-26 21:22:09,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1162123: learning rate 0.0010
[2019-03-26 21:22:09,274] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1162147: loss 0.0223
[2019-03-26 21:22:09,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1162147: learning rate 0.0010
[2019-03-26 21:22:09,773] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1162373: loss 6.3759
[2019-03-26 21:22:09,776] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1162374: learning rate 0.0010
[2019-03-26 21:22:09,808] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1162388: loss 0.0085
[2019-03-26 21:22:09,812] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1162388: learning rate 0.0010
[2019-03-26 21:22:09,844] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1162406: loss 0.0018
[2019-03-26 21:22:09,846] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1162406: learning rate 0.0010
[2019-03-26 21:22:10,176] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1162555: loss -4.6649
[2019-03-26 21:22:10,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1162555: learning rate 0.0010
[2019-03-26 21:22:10,481] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4987425e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:10,492] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4660
[2019-03-26 21:22:10,497] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.33333333333334, 61.66666666666667, 1.0, 2.0, 0.6165615271090111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861615.000578552, 861615.000578552, 203872.7903916272], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3847200.0000, 
sim time next is 3847800.0000, 
raw observation next is [34.41666666666666, 61.33333333333334, 1.0, 2.0, 0.616739349210046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861863.599213231, 861863.599213231, 203906.8101970024], 
processed observation next is [0.0, 0.5217391304347826, 0.8301737756714056, 0.6133333333333334, 1.0, 1.0, 0.5382401797711398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2394065553370086, 0.2394065553370086, 0.30433852268209316], 
reward next is 0.6957, 
noisyNet noise sample is [array([-0.4576301], dtype=float32), -0.7969873]. 
=============================================
[2019-03-26 21:22:11,150] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1162991: loss -5.5927
[2019-03-26 21:22:11,152] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1162991: learning rate 0.0010
[2019-03-26 21:22:12,407] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.6264788e-10 9.4583201e-01 9.6310277e-12 5.4167937e-02 1.7030231e-10], sum to 1.0000
[2019-03-26 21:22:12,417] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2439
[2019-03-26 21:22:12,426] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2838724.448873446 W.
[2019-03-26 21:22:12,432] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.7118723424355026, 1.0, 2.0, 0.6765262107320139, 1.0, 1.0, 1.03, 7.0050986682978, 6.9112, 170.5573041426782, 2838724.448873446, 2771461.013218112, 525105.139983118], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3774600.0000, 
sim time next is 3775200.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9638084169544441, 1.0, 2.0, 0.9638084169544441, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2695958.969561437, 2695958.969561437, 507501.5272893501], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.63, 1.0, 1.0, 0.9563956830776434, 1.0, 1.0, 0.9563956830776434, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7488774915448436, 0.7488774915448436, 0.7574649661035077], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55929637], dtype=float32), 0.83891225]. 
=============================================
[2019-03-26 21:22:14,238] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1164367: loss 0.1210
[2019-03-26 21:22:14,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1164367: learning rate 0.0010
[2019-03-26 21:22:14,958] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2938385e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:14,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2779
[2019-03-26 21:22:14,977] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 0.6242965057945657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 872428.6997376153, 872428.6997376159, 205360.8338812141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3852000.0000, 
sim time next is 3852600.0000, 
raw observation next is [35.0, 59.33333333333333, 1.0, 2.0, 0.6260128455368718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874828.201770172, 874828.201770172, 205692.8945068055], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5933333333333333, 1.0, 1.0, 0.5494130669118936, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2430078338250478, 0.2430078338250478, 0.3070043201594112], 
reward next is 0.6930, 
noisyNet noise sample is [array([1.4900746], dtype=float32), 0.3997582]. 
=============================================
[2019-03-26 21:22:16,799] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1165511: loss 0.0158
[2019-03-26 21:22:16,800] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1165511: learning rate 0.0010
[2019-03-26 21:22:19,383] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1166669: loss 0.0620
[2019-03-26 21:22:19,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1166671: learning rate 0.0010
[2019-03-26 21:22:21,738] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1408388e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:21,746] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6113
[2019-03-26 21:22:21,751] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 73.66666666666667, 1.0, 2.0, 0.6046563939403528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844971.5333202941, 844971.5333202948, 201616.6447879491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3964800.0000, 
sim time next is 3965400.0000, 
raw observation next is [31.5, 73.0, 1.0, 2.0, 0.6041539172788504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844269.0728390906, 844269.0728390906, 201522.530803418], 
processed observation next is [0.0, 0.9130434782608695, 0.6919431279620853, 0.73, 1.0, 1.0, 0.5230770087696992, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23451918689974738, 0.23451918689974738, 0.3007798967215194], 
reward next is 0.6992, 
noisyNet noise sample is [array([0.29856405], dtype=float32), 1.9196571]. 
=============================================
[2019-03-26 21:22:22,521] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0767448e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:22,528] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0439
[2019-03-26 21:22:22,535] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5452465447021885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761920.0073329152, 761920.0073329152, 190988.9409948862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4047000.0000, 
sim time next is 4047600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5460180003994733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762998.4160726334, 762998.4160726334, 191120.1916432304], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4530337354210522, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21194400446462036, 0.21194400446462036, 0.28525401737795586], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.70423126], dtype=float32), 0.692701]. 
=============================================
[2019-03-26 21:22:23,009] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1168274: loss 0.3102
[2019-03-26 21:22:23,010] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1168274: learning rate 0.0010
[2019-03-26 21:22:23,204] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1168358: loss 0.3399
[2019-03-26 21:22:23,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1168359: learning rate 0.0010
[2019-03-26 21:22:24,952] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1169138: loss -10.1950
[2019-03-26 21:22:24,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1169138: learning rate 0.0010
[2019-03-26 21:22:25,178] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1169236: loss 0.0634
[2019-03-26 21:22:25,182] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1169237: learning rate 0.0010
[2019-03-26 21:22:25,730] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1169481: loss 0.1557
[2019-03-26 21:22:25,731] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1169481: learning rate 0.0010
[2019-03-26 21:22:26,858] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1169984: loss 0.0396
[2019-03-26 21:22:26,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1169984: learning rate 0.0010
[2019-03-26 21:22:27,193] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.286539e-27 0.000000e+00], sum to 1.0000
[2019-03-26 21:22:27,202] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2690
[2019-03-26 21:22:27,208] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1170141: loss -74.2984
[2019-03-26 21:22:27,209] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7025231281703276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981797.7178059865, 981797.7178059865, 221390.9508297546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4081800.0000, 
sim time next is 4082400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6571349986278583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918338.9833539705, 918338.9833539705, 211865.1391908308], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.89, 1.0, 1.0, 0.586909636901034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2550941620427696, 0.2550941620427696, 0.31621662565795644], 
reward next is 0.6838, 
noisyNet noise sample is [array([-1.4807416], dtype=float32), 1.1584954]. 
=============================================
[2019-03-26 21:22:27,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1170141: learning rate 0.0010
[2019-03-26 21:22:27,236] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1170150: loss 0.0044
[2019-03-26 21:22:27,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1170150: learning rate 0.0010
[2019-03-26 21:22:27,526] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1170279: loss 0.0225
[2019-03-26 21:22:27,533] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1170281: learning rate 0.0010
[2019-03-26 21:22:27,577] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1170301: loss 0.0349
[2019-03-26 21:22:27,578] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1170301: learning rate 0.0010
[2019-03-26 21:22:27,682] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1170348: loss 0.0113
[2019-03-26 21:22:27,684] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1170348: learning rate 0.0010
[2019-03-26 21:22:28,059] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1170520: loss 0.0315
[2019-03-26 21:22:28,060] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1170520: learning rate 0.0010
[2019-03-26 21:22:28,435] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8073515e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:28,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5287
[2019-03-26 21:22:28,454] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1928323.497414847 W.
[2019-03-26 21:22:28,461] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.56666666666667, 86.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.578196710368428, 6.9112, 169.2900105806816, 1928323.497414847, 1454076.800761533, 311420.3639341666], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4068600.0000, 
sim time next is 4069200.0000, 
raw observation next is [27.53333333333333, 86.33333333333334, 1.0, 2.0, 0.5552209691416199, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9500166879224022, 6.9112, 6.9112, 168.912413409552, 1552293.578945326, 1552293.578945326, 336798.199211334], 
processed observation next is [1.0, 0.08695652173913043, 0.5039494470774091, 0.8633333333333334, 1.0, 1.0, 0.4641216495682167, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 0.9390447413687831, 0.0, 0.0, 0.8294372782783541, 0.4311926608181461, 0.4311926608181461, 0.5026838794199014], 
reward next is 0.4973, 
noisyNet noise sample is [array([-1.0234945], dtype=float32), -0.06761833]. 
=============================================
[2019-03-26 21:22:29,053] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1170961: loss 0.0248
[2019-03-26 21:22:29,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1170963: learning rate 0.0010
[2019-03-26 21:22:32,369] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1172438: loss 0.9437
[2019-03-26 21:22:32,372] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1172438: learning rate 0.0010
[2019-03-26 21:22:33,041] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1443295e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 21:22:33,050] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3948
[2019-03-26 21:22:33,056] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 77.66666666666667, 1.0, 2.0, 0.6299860219547007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880382.8599504848, 880382.8599504848, 206465.6320131386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4222200.0000, 
sim time next is 4222800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6253095733433646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873845.0023829468, 873845.0023829468, 205556.1514635404], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5485657510161018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24273472288415188, 0.24273472288415188, 0.30680022606498564], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.7420731], dtype=float32), 0.7250197]. 
=============================================
[2019-03-26 21:22:35,062] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1173641: loss -122.4742
[2019-03-26 21:22:35,064] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1173641: learning rate 0.0010
[2019-03-26 21:22:37,770] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1174805: loss -84.0254
[2019-03-26 21:22:37,776] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1174806: learning rate 0.0010
[2019-03-26 21:22:38,197] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 21:22:38,199] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:22:38,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:22:38,200] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:22:38,201] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:22:38,202] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:22:38,204] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:22:38,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:22:38,205] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:22:38,203] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:22:38,209] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:22:38,224] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-03-26 21:22:38,225] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-03-26 21:22:38,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-03-26 21:22:38,286] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-03-26 21:22:38,287] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-03-26 21:22:40,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:22:40,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.61666666666667, 87.33333333333333, 1.0, 2.0, 0.3503533535017277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542928.806202112, 542928.8062021126, 170147.3838959649]
[2019-03-26 21:22:40,411] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:22:40,415] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2066853e-31 0.0000000e+00], sampled 0.513991902568154
[2019-03-26 21:23:14,669] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:23:14,670] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.56666666666666, 93.50000000000001, 1.0, 2.0, 0.7417537951225357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043921.097042158, 1043921.097042158, 231110.6408076962]
[2019-03-26 21:23:14,671] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:23:14,673] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2250936e-25 0.0000000e+00], sampled 0.7996351081171139
[2019-03-26 21:23:48,131] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:23:48,133] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.4859292428987306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679004.3352443101, 679004.3352443101, 181420.5453945243]
[2019-03-26 21:23:48,133] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:23:48,135] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.243921e-29 0.000000e+00], sampled 0.4327392160379482
[2019-03-26 21:23:48,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:23:48,918] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.90000000000001, 59.16666666666666, 1.0, 2.0, 0.9535862768294205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1332886.425911356, 1332886.425911356, 285107.3610223809]
[2019-03-26 21:23:48,919] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:23:48,921] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3935933e-24 0.0000000e+00], sampled 0.2938812697401231
[2019-03-26 21:23:56,647] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:23:56,649] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.0, 78.0, 1.0, 2.0, 0.7678260366776745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073106.655827188, 1073106.655827188, 236167.519618182]
[2019-03-26 21:23:56,649] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:23:56,652] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2969184e-25 0.0000000e+00], sampled 0.9854945148806443
[2019-03-26 21:23:59,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:23:59,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.73518173666666, 93.66766902166668, 1.0, 2.0, 0.4916024094100246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686934.1937756736, 686934.193775673, 182289.8068460805]
[2019-03-26 21:23:59,766] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:23:59,768] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7580641e-29 0.0000000e+00], sampled 0.14530260034643594
[2019-03-26 21:24:20,403] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5568119]
[2019-03-26 21:24:20,404] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.86666666666667, 76.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.941662117651852, 6.9112, 168.9125731393694, 1475380.553397789, 1453769.727759226, 311358.1322184041]
[2019-03-26 21:24:20,406] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:24:20,409] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3474102e-24 0.0000000e+00], sampled 0.4422836019357991
[2019-03-26 21:24:32,521] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3751 2927779800.6357 1338.0000
[2019-03-26 21:24:32,589] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6708 2779658361.4638 933.0000
[2019-03-26 21:24:32,720] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2741 3008104656.7243 1766.0000
[2019-03-26 21:24:33,018] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8875 3164502873.7783 1778.0000
[2019-03-26 21:24:33,025] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8190 2842958600.4139 1131.0000
[2019-03-26 21:24:34,042] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1175000, evaluation results [1175000.0, 7878.887539409773, 3164502873.778303, 1778.0, 8254.375086958147, 2927779800.6357465, 1338.0, 8660.670814356308, 2779658361.463785, 933.0, 7998.274138010797, 3008104656.724262, 1766.0, 8496.819032943851, 2842958600.4139094, 1131.0]
[2019-03-26 21:24:37,016] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1176328: loss -84.0129
[2019-03-26 21:24:37,018] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1176328: learning rate 0.0010
[2019-03-26 21:24:37,078] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1176354: loss -101.3812
[2019-03-26 21:24:37,079] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1176354: learning rate 0.0010
[2019-03-26 21:24:38,227] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1176872: loss 2.0182
[2019-03-26 21:24:38,233] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1176874: learning rate 0.0010
[2019-03-26 21:24:38,595] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2218651e-30 1.0000000e+00 2.4857946e-35 4.5761336e-16 8.3865807e-37], sum to 1.0000
[2019-03-26 21:24:38,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9553
[2019-03-26 21:24:38,612] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.5595512296758571, 0.0, 1.0, 0.0, 1.0, 1.0, 0.971755439527635, 6.9112, 6.9112, 168.9129565103946, 1564409.092601331, 1564409.092601331, 342354.5742951194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4340400.0000, 
sim time next is 4341000.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.305380937569641, 6.9112, 168.9106651588315, 1733588.185162693, 1453946.454859849, 311355.8895808959], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.039418093756964104, 0.0, 0.8294286935669639, 0.4815522736563036, 0.403874015238847, 0.464710282956561], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7862447], dtype=float32), -1.2864152]. 
=============================================
[2019-03-26 21:24:38,636] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[43.564575]
 [45.359703]
 [48.238388]
 [51.656956]
 [53.020752]], R is [[44.17610931]
 [44.2233696 ]
 [44.28061676]
 [44.33849335]
 [43.89510727]].
[2019-03-26 21:24:39,109] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1177264: loss -14.5471
[2019-03-26 21:24:39,112] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1177265: learning rate 0.0010
[2019-03-26 21:24:39,616] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1177491: loss -70.1256
[2019-03-26 21:24:39,621] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1177494: learning rate 0.0010
[2019-03-26 21:24:40,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5083650e-37 1.0000000e+00 0.0000000e+00 4.8460202e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:24:40,211] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7704
[2019-03-26 21:24:40,215] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 0.9721588927334889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1358863.132388855, 1358863.132388855, 290563.2322844061], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4338000.0000, 
sim time next is 4338600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.754425148317853, 6.9112, 168.908691235275, 2052362.533851051, 1454164.697731087, 311356.0814195776], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08432251483178525, 0.0, 0.8294190006986353, 0.5701007038475142, 0.4039346382586353, 0.4647105692829516], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.61168987], dtype=float32), 2.2105083]. 
=============================================
[2019-03-26 21:24:40,628] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1177944: loss 0.7796
[2019-03-26 21:24:40,632] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1177944: learning rate 0.0010
[2019-03-26 21:24:40,874] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1178052: loss -68.4523
[2019-03-26 21:24:40,875] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1178052: learning rate 0.0010
[2019-03-26 21:24:41,286] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1178238: loss -64.4264
[2019-03-26 21:24:41,288] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1178240: learning rate 0.0010
[2019-03-26 21:24:41,527] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1178344: loss -59.6285
[2019-03-26 21:24:41,531] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1178345: learning rate 0.0010
[2019-03-26 21:24:41,602] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1178380: loss -23.4582
[2019-03-26 21:24:41,605] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1178380: learning rate 0.0010
[2019-03-26 21:24:41,707] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1178425: loss -30.1846
[2019-03-26 21:24:41,712] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1178425: learning rate 0.0010
[2019-03-26 21:24:41,829] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7157772e-10 2.5801144e-08 4.6599977e-11 1.0000000e+00 3.0010037e-09], sum to 1.0000
[2019-03-26 21:24:41,838] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2100
[2019-03-26 21:24:41,841] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.00000000000001, 1.0, 2.0, 0.9422885544530144, 1.0, 2.0, 0.9422885544530144, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2635700.247046721, 2635700.247046721, 495103.181274304], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4364400.0000, 
sim time next is 4365000.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.8808890015560283, 1.0, 2.0, 0.8808890015560283, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2463788.606134657, 2463788.606134657, 461189.2330863352], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.8564927729590703, 1.0, 1.0, 0.8564927729590703, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6843857239262936, 0.6843857239262936, 0.6883421389348287], 
reward next is 0.3117, 
noisyNet noise sample is [array([-1.1127346], dtype=float32), 0.3814573]. 
=============================================
[2019-03-26 21:24:41,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[18.062712]
 [20.507864]
 [20.507864]
 [20.507864]
 [20.507864]], R is [[20.09926033]
 [20.15930748]
 [19.95771408]
 [19.75813675]
 [19.56055641]].
[2019-03-26 21:24:42,214] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1178651: loss -61.0405
[2019-03-26 21:24:42,215] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1178651: learning rate 0.0010
[2019-03-26 21:24:43,061] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1179022: loss -29.4888
[2019-03-26 21:24:43,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1179022: learning rate 0.0010
[2019-03-26 21:24:45,551] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 4.02151e-25 0.00000e+00], sum to 1.0000
[2019-03-26 21:24:45,563] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7105
[2019-03-26 21:24:45,567] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 70.0, 1.0, 2.0, 0.5263492198758447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735504.0036963735, 735504.0036963728, 187826.8308498702], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4649400.0000, 
sim time next is 4650000.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.5123961890151861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715999.9018980737, 715999.9018980744, 185560.360413835], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7, 1.0, 1.0, 0.41252552893395916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19888886163835381, 0.198888861638354, 0.276955761811694], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.5207663], dtype=float32), 0.7541359]. 
=============================================
[2019-03-26 21:24:45,591] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.02313 ]
 [71.36534 ]
 [71.24716 ]
 [71.320984]
 [71.64079 ]], R is [[72.4045105 ]
 [72.4001236 ]
 [72.39237976]
 [72.38095093]
 [72.36808014]].
[2019-03-26 21:24:45,922] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1180301: loss 0.4707
[2019-03-26 21:24:45,927] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1180301: learning rate 0.0010
[2019-03-26 21:24:48,706] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1181546: loss 0.7705
[2019-03-26 21:24:48,711] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1181547: learning rate 0.0010
[2019-03-26 21:24:51,263] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1182696: loss 0.0766
[2019-03-26 21:24:51,267] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1182698: learning rate 0.0010
[2019-03-26 21:24:52,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.1253177e-12 7.2063034e-07 1.7014416e-12 9.9999928e-01 1.1772230e-10], sum to 1.0000
[2019-03-26 21:24:52,234] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0077
[2019-03-26 21:24:52,239] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 73.66666666666667, 1.0, 2.0, 0.9118096957904795, 1.0, 2.0, 0.9118096957904795, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2550360.014347896, 2550360.014347896, 477986.2850577785], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4616400.0000, 
sim time next is 4617000.0000, 
raw observation next is [32.0, 73.0, 1.0, 2.0, 0.9028977200939787, 1.0, 2.0, 0.9028977200939787, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2525407.750847121, 2525407.750847122, 473085.1327594805], 
processed observation next is [1.0, 0.43478260869565216, 0.7156398104265403, 0.73, 1.0, 1.0, 0.8830093013180467, 1.0, 1.0, 0.8830093013180467, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7015021530130892, 0.7015021530130895, 0.7060972130738515], 
reward next is 0.2939, 
noisyNet noise sample is [array([0.46969464], dtype=float32), -1.319682]. 
=============================================
[2019-03-26 21:24:52,255] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[21.781347]
 [21.340874]
 [20.673727]
 [20.091171]
 [19.685349]], R is [[22.16979408]
 [22.23468399]
 [22.29438972]
 [22.33212852]
 [22.34852219]].
[2019-03-26 21:24:54,708] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1184235: loss 0.1270
[2019-03-26 21:24:54,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1184235: learning rate 0.0010
[2019-03-26 21:24:54,735] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1184246: loss 0.0250
[2019-03-26 21:24:54,737] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1184248: learning rate 0.0010
[2019-03-26 21:24:56,602] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1185083: loss 217.4499
[2019-03-26 21:24:56,608] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1185083: learning rate 0.0010
[2019-03-26 21:24:56,978] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1185247: loss 0.0683
[2019-03-26 21:24:56,980] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1185247: learning rate 0.0010
[2019-03-26 21:24:57,552] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1185513: loss 0.5254
[2019-03-26 21:24:57,553] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1185513: learning rate 0.0010
[2019-03-26 21:24:58,610] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1185987: loss 214.9147
[2019-03-26 21:24:58,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1185987: learning rate 0.0010
[2019-03-26 21:24:58,653] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1186007: loss 0.6959
[2019-03-26 21:24:58,656] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1186007: learning rate 0.0010
[2019-03-26 21:24:59,141] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2562561e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:24:59,155] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3950
[2019-03-26 21:24:59,160] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 1.012963877539185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1415937.514453931, 1415937.514453931, 302902.2274587305], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4695000.0000, 
sim time next is 4695600.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.746256089327783, 6.9112, 168.9086595192302, 2046563.186868673, 1454160.727247422, 311352.2580886176], 
processed observation next is [1.0, 0.34782608695652173, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.08350560893277832, 0.0, 0.8294188449583357, 0.568489774130187, 0.4039335353465061, 0.4647048628188322], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.70923626], dtype=float32), -0.26821476]. 
=============================================
[2019-03-26 21:24:59,160] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1186230: loss 0.0465
[2019-03-26 21:24:59,163] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1186231: learning rate 0.0010
[2019-03-26 21:24:59,384] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1186327: loss 0.1130
[2019-03-26 21:24:59,386] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1186328: learning rate 0.0010
[2019-03-26 21:24:59,397] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1186332: loss 0.0117
[2019-03-26 21:24:59,399] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1186333: learning rate 0.0010
[2019-03-26 21:24:59,476] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1186372: loss 0.0358
[2019-03-26 21:24:59,481] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1186374: learning rate 0.0010
[2019-03-26 21:25:00,015] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1186614: loss 0.0382
[2019-03-26 21:25:00,017] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1186614: learning rate 0.0010
[2019-03-26 21:25:00,844] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1186983: loss 0.1164
[2019-03-26 21:25:00,851] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1186985: learning rate 0.0010
[2019-03-26 21:25:01,723] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5915186e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:25:01,732] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0289
[2019-03-26 21:25:01,738] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6367575036421909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 889849.7268055704, 889849.726805571, 207786.8547402394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4684800.0000, 
sim time next is 4685400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.634153649034275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 886209.3990846409, 886209.3990846416, 207274.5878739175], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5592212638967168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24616927752351137, 0.24616927752351156, 0.3093650565282351], 
reward next is 0.6906, 
noisyNet noise sample is [array([0.16049714], dtype=float32), -0.4928342]. 
=============================================
[2019-03-26 21:25:01,939] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3997452e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:25:01,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3759
[2019-03-26 21:25:01,956] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 90.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.95280388078335, 6.9112, 168.9121469915918, 1483290.226422459, 1453775.142952008, 311350.9824840707], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4674000.0000, 
sim time next is 4674600.0000, 
raw observation next is [27.0, 91.5, 1.0, 2.0, 0.9047999428778082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129226059842, 1264654.139598119, 1264654.139598119, 271262.4333311477], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.915, 1.0, 1.0, 0.8853011359973593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294397786659473, 0.3512928165550331, 0.3512928165550331, 0.4048693034793249], 
reward next is 0.5951, 
noisyNet noise sample is [array([-0.8881857], dtype=float32), 0.63213116]. 
=============================================
[2019-03-26 21:25:03,863] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1188331: loss 232.2075
[2019-03-26 21:25:03,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1188331: learning rate 0.0010
[2019-03-26 21:25:06,712] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1189603: loss 251.6834
[2019-03-26 21:25:06,714] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1189603: learning rate 0.0010
[2019-03-26 21:25:07,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7929672e-16 9.9160504e-01 1.0688921e-17 8.3949203e-03 7.0078576e-17], sum to 1.0000
[2019-03-26 21:25:07,805] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4267
[2019-03-26 21:25:07,811] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2056567.401569931 W.
[2019-03-26 21:25:07,818] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.7354207833791548, 1.0, 1.0, 0.7354207833791548, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2056567.401569931, 2056567.401569931, 389654.7326917903], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4875600.0000, 
sim time next is 4876200.0000, 
raw observation next is [30.5, 68.0, 1.0, 2.0, 0.5051165193376541, 1.0, 2.0, 0.5051165193376541, 1.0, 1.0, 0.8725936884411689, 6.911199999999999, 6.9112, 170.5573041426782, 2118861.133059733, 2118861.133059734, 417857.1714056206], 
processed observation next is [1.0, 0.43478260869565216, 0.6445497630331753, 0.68, 1.0, 1.0, 0.4037548425754868, 1.0, 1.0, 0.4037548425754868, 1.0, 0.5, 0.8446264493184987, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.588572536961037, 0.5885725369610372, 0.6236674200083889], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7968804], dtype=float32), -0.5884018]. 
=============================================
[2019-03-26 21:25:09,091] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1190670: loss 288.4861
[2019-03-26 21:25:09,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1190672: learning rate 0.0010
[2019-03-26 21:25:12,564] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1192222: loss 266.4891
[2019-03-26 21:25:12,566] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1192222: learning rate 0.0010
[2019-03-26 21:25:12,738] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1192301: loss 230.2056
[2019-03-26 21:25:12,739] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1192301: learning rate 0.0010
[2019-03-26 21:25:14,131] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1192922: loss 0.0584
[2019-03-26 21:25:14,138] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1192923: learning rate 0.0010
[2019-03-26 21:25:14,714] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1193183: loss 234.0196
[2019-03-26 21:25:14,716] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1193184: learning rate 0.0010
[2019-03-26 21:25:15,475] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1193520: loss 232.2074
[2019-03-26 21:25:15,478] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1193521: learning rate 0.0010
[2019-03-26 21:25:16,344] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1193909: loss 0.4636
[2019-03-26 21:25:16,348] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1193911: learning rate 0.0010
[2019-03-26 21:25:16,552] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1194000: loss 214.3953
[2019-03-26 21:25:16,556] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1194000: learning rate 0.0010
[2019-03-26 21:25:17,198] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1194288: loss 221.3532
[2019-03-26 21:25:17,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1194289: learning rate 0.0010
[2019-03-26 21:25:17,340] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1194352: loss 229.8698
[2019-03-26 21:25:17,341] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1194352: learning rate 0.0010
[2019-03-26 21:25:17,381] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1194368: loss 231.1809
[2019-03-26 21:25:17,382] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1194368: learning rate 0.0010
[2019-03-26 21:25:17,410] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1194383: loss 293.9217
[2019-03-26 21:25:17,413] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1194383: learning rate 0.0010
[2019-03-26 21:25:17,989] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1194643: loss 223.1978
[2019-03-26 21:25:17,992] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1194643: learning rate 0.0010
[2019-03-26 21:25:18,687] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1194954: loss 213.4603
[2019-03-26 21:25:18,690] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1194954: learning rate 0.0010
[2019-03-26 21:25:20,690] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9564257e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:25:20,700] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6145
[2019-03-26 21:25:20,704] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666666, 84.0, 1.0, 2.0, 0.5113504572866356, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714538.1508985457, 714538.1508985457, 185393.2993440054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5008200.0000, 
sim time next is 5008800.0000, 
raw observation next is [26.83333333333334, 84.0, 1.0, 2.0, 0.5094181981704583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711837.194158794, 711837.1941587933, 185084.5378598327], 
processed observation next is [1.0, 1.0, 0.4707740916271725, 0.84, 1.0, 1.0, 0.4089375881571786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19773255393299832, 0.19773255393299813, 0.2762455788952727], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.7111742], dtype=float32), -1.7958832]. 
=============================================
[2019-03-26 21:25:21,604] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1196254: loss 0.2395
[2019-03-26 21:25:21,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1196254: learning rate 0.0010
[2019-03-26 21:25:22,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3552723e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:25:22,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2215
[2019-03-26 21:25:22,310] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4982420050701053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696214.9862049058, 696214.9862049058, 183320.6880899467], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5181600.0000, 
sim time next is 5182200.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.5017359629652348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701098.8550179816, 701098.8550179822, 183868.1858325907], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.815, 1.0, 1.0, 0.39968188309064434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19474968194943934, 0.1947496819494395, 0.27443012810834433], 
reward next is 0.7256, 
noisyNet noise sample is [array([-0.76179296], dtype=float32), -1.2593123]. 
=============================================
[2019-03-26 21:25:24,722] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1197644: loss 0.6057
[2019-03-26 21:25:24,724] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1197645: learning rate 0.0010
[2019-03-26 21:25:26,992] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1198652: loss 0.8576
[2019-03-26 21:25:26,994] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1198652: learning rate 0.0010
[2019-03-26 21:25:30,118] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 21:25:30,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:25:30,122] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:25:30,124] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:25:30,126] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:25:30,124] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:25:30,126] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:25:30,128] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:25:30,130] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:25:30,128] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:25:30,132] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:25:30,153] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-03-26 21:25:30,175] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-03-26 21:25:30,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-03-26 21:25:30,177] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-03-26 21:25:30,177] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-03-26 21:25:36,330] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5899416]
[2019-03-26 21:25:36,332] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.93333333333333, 69.33333333333333, 1.0, 2.0, 0.2459797860483239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 404781.7177733748, 404781.7177733748, 160508.8461100984]
[2019-03-26 21:25:36,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:25:36,336] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7496946e-30 0.0000000e+00], sampled 0.785592779082518
[2019-03-26 21:26:17,734] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5899416]
[2019-03-26 21:26:17,735] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.20556185, 77.32958426, 1.0, 2.0, 0.5052878203360025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706063.6789732313, 706063.6789732313, 184427.1953661362]
[2019-03-26 21:26:17,738] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:26:17,742] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.464757e-30 0.000000e+00], sampled 0.6882108909051294
[2019-03-26 21:26:57,397] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5899416]
[2019-03-26 21:26:57,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.59750081, 88.78422103, 1.0, 2.0, 0.4408472179236374, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 635417.3122316065, 635417.3122316072, 177341.0886183339]
[2019-03-26 21:26:57,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:26:57,402] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1855104e-29 0.0000000e+00], sampled 0.08438418354740196
[2019-03-26 21:27:06,433] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.5899416]
[2019-03-26 21:27:06,435] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.86666666666667, 93.66666666666667, 1.0, 2.0, 0.5381565603948456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752009.0539014868, 752009.0539014868, 189786.5727289044]
[2019-03-26 21:27:06,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:27:06,440] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 3.68689e-29 0.00000e+00], sampled 0.28782561653740146
[2019-03-26 21:27:24,047] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2748 2927846980.7461 1338.0000
[2019-03-26 21:27:25,047] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5671 2779727878.6853 933.0000
[2019-03-26 21:27:25,054] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2741 3008104656.7243 1766.0000
[2019-03-26 21:27:25,079] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7157 2843027813.7524 1131.0000
[2019-03-26 21:27:25,231] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8875 3164502873.7783 1778.0000
[2019-03-26 21:27:26,250] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1200000, evaluation results [1200000.0, 7878.887539409773, 3164502873.778303, 1778.0, 8254.274818136657, 2927846980.7461457, 1338.0, 8660.567057309232, 2779727878.685327, 933.0, 7998.274138010797, 3008104656.724262, 1766.0, 8496.715729453535, 2843027813.7524204, 1131.0]
[2019-03-26 21:27:26,678] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1200199: loss 0.5637
[2019-03-26 21:27:26,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1200200: learning rate 0.0010
[2019-03-26 21:27:26,925] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1200307: loss 0.3195
[2019-03-26 21:27:26,932] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1200309: learning rate 0.0010
[2019-03-26 21:27:28,671] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1201090: loss 0.5979
[2019-03-26 21:27:28,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1201090: learning rate 0.0010
[2019-03-26 21:27:29,055] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1201264: loss 1.8079
[2019-03-26 21:27:29,057] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1201264: learning rate 0.0010
[2019-03-26 21:27:29,673] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1201543: loss 0.6851
[2019-03-26 21:27:29,677] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1201545: learning rate 0.0010
[2019-03-26 21:27:30,620] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1201965: loss 1.1117
[2019-03-26 21:27:30,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1201965: learning rate 0.0010
[2019-03-26 21:27:30,792] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1202043: loss 0.6275
[2019-03-26 21:27:30,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1202043: learning rate 0.0010
[2019-03-26 21:27:31,220] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1202235: loss 0.0875
[2019-03-26 21:27:31,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1202237: learning rate 0.0010
[2019-03-26 21:27:31,337] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1202286: loss 0.5794
[2019-03-26 21:27:31,340] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1202287: learning rate 0.0010
[2019-03-26 21:27:31,358] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1202296: loss 0.6224
[2019-03-26 21:27:31,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1202296: learning rate 0.0010
[2019-03-26 21:27:31,442] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1202332: loss 2.4735
[2019-03-26 21:27:31,445] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1202332: learning rate 0.0010
[2019-03-26 21:27:32,101] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1202626: loss 0.0507
[2019-03-26 21:27:32,107] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1202628: learning rate 0.0010
[2019-03-26 21:27:32,908] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1202984: loss 0.5050
[2019-03-26 21:27:32,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1202985: learning rate 0.0010
[2019-03-26 21:27:33,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9492081e-30 1.0000000e+00 2.4770318e-33 1.9794508e-15 1.0338925e-34], sum to 1.0000
[2019-03-26 21:27:33,140] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8015
[2019-03-26 21:27:33,147] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 71.66666666666667, 1.0, 2.0, 0.4352652608926945, 1.0, 2.0, 0.4352652608926945, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1216721.024483988, 1216721.024483988, 281383.7620923197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5418600.0000, 
sim time next is 5419200.0000, 
raw observation next is [30.9, 72.33333333333334, 1.0, 2.0, 0.5467727236260793, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564857762, 764053.4357502268, 764053.4357502274, 191251.3152134893], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.7233333333333334, 1.0, 1.0, 0.45394304051334855, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399450312363, 0.21223706548617413, 0.21223706548617427, 0.28544972419923775], 
reward next is 0.7146, 
noisyNet noise sample is [array([0.02267478], dtype=float32), 0.71111006]. 
=============================================
[2019-03-26 21:27:33,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4260627e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:33,435] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2944
[2019-03-26 21:27:33,439] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.55, 70.0, 1.0, 2.0, 0.5429226798948982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758671.5109867644, 758671.5109867644, 190595.2286941819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5596200.0000, 
sim time next is 5596800.0000, 
raw observation next is [30.26666666666667, 71.66666666666667, 1.0, 2.0, 0.5476704144670246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765308.3097198021, 765308.3097198021, 191402.5517945992], 
processed observation next is [1.0, 0.782608695652174, 0.6334913112164299, 0.7166666666666667, 1.0, 1.0, 0.4550245957434031, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21258564158883392, 0.21258564158883392, 0.2856754504397003], 
reward next is 0.7143, 
noisyNet noise sample is [array([-2.0393257], dtype=float32), 0.10478619]. 
=============================================
[2019-03-26 21:27:34,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3141126e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:34,686] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1963
[2019-03-26 21:27:34,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.33333333333334, 1.0, 2.0, 0.58828882238829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822089.9688113108, 822089.9688113108, 198584.6308239584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5446200.0000, 
sim time next is 5446800.0000, 
raw observation next is [28.5, 89.0, 1.0, 2.0, 0.587981458989028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821660.2851378199, 821660.2851378199, 198528.4640222481], 
processed observation next is [1.0, 0.043478260869565216, 0.5497630331753555, 0.89, 1.0, 1.0, 0.5035921192638891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22823896809383887, 0.22823896809383887, 0.2963111403317136], 
reward next is 0.7037, 
noisyNet noise sample is [array([-0.11454123], dtype=float32), 0.19984435]. 
=============================================
[2019-03-26 21:27:35,852] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1204300: loss 1.2621
[2019-03-26 21:27:35,853] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1204300: learning rate 0.0010
[2019-03-26 21:27:36,635] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1358118e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:36,650] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7561
[2019-03-26 21:27:36,658] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 78.5, 1.0, 2.0, 0.6276846513960663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 877165.4491132319, 877165.4491132324, 206017.5566451002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5344200.0000, 
sim time next is 5344800.0000, 
raw observation next is [31.16666666666666, 78.66666666666667, 1.0, 2.0, 0.6261678895408834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 875044.9588910389, 875044.9588910383, 205722.8452377084], 
processed observation next is [1.0, 0.8695652173913043, 0.6761453396524484, 0.7866666666666667, 1.0, 1.0, 0.549599866916727, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2430680441363997, 0.24306804413639954, 0.30704902274284834], 
reward next is 0.6930, 
noisyNet noise sample is [array([-0.27441925], dtype=float32), 0.19803262]. 
=============================================
[2019-03-26 21:27:37,581] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.4251144e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:37,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7044
[2019-03-26 21:27:37,596] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333333, 60.0, 1.0, 2.0, 0.5424108588292609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757956.0452014727, 757956.0452014727, 190508.1461050543], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5669400.0000, 
sim time next is 5670000.0000, 
raw observation next is [32.3, 60.0, 1.0, 2.0, 0.5416232201597805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756855.0197158619, 756855.0197158626, 190374.9048115258], 
processed observation next is [0.0, 0.6521739130434783, 0.7298578199052131, 0.6, 1.0, 1.0, 0.447738819469615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2102375054766283, 0.2102375054766285, 0.28414164897242655], 
reward next is 0.7159, 
noisyNet noise sample is [array([-0.45276886], dtype=float32), 0.27343026]. 
=============================================
[2019-03-26 21:27:37,607] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[76.96351 ]
 [76.884796]
 [76.77782 ]
 [76.663475]
 [76.556595]], R is [[77.09306335]
 [77.03779602]
 [76.98282623]
 [76.92808533]
 [76.87332153]].
[2019-03-26 21:27:38,946] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1205679: loss 1.0862
[2019-03-26 21:27:38,953] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1205681: learning rate 0.0010
[2019-03-26 21:27:40,146] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9018671e-13 1.3717250e-18 4.3822614e-13 1.0000000e+00 3.2754754e-10], sum to 1.0000
[2019-03-26 21:27:40,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2510
[2019-03-26 21:27:40,163] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.78333333333333, 65.5, 1.0, 2.0, 0.8574325370415345, 1.0, 2.0, 0.7493063080350298, 1.0, 1.0, 1.03, 7.005110148850044, 6.9112, 170.5573041426782, 3144496.3550586, 3077224.695416695, 575668.0717538554], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5482200.0000, 
sim time next is 5482800.0000, 
raw observation next is [35.0, 65.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.748159963514141, 6.9112, 170.5573041426782, 3509576.617830936, 2910028.150611166, 548936.5599103254], 
processed observation next is [1.0, 0.4782608695652174, 0.8578199052132701, 0.65, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.08369599635141407, 0.0, 0.8375144448122397, 0.9748823938419267, 0.808341152947546, 0.8193082983736201], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.05586716], dtype=float32), -0.055752344]. 
=============================================
[2019-03-26 21:27:40,722] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8849288e-19 1.0000000e+00 1.0597912e-21 3.5142600e-10 2.6776521e-21], sum to 1.0000
[2019-03-26 21:27:40,730] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5417
[2019-03-26 21:27:40,739] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1964309.830339561 W.
[2019-03-26 21:27:40,747] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.8, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.630387528007676, 6.9112, 168.9093277897452, 1964309.830339561, 1454104.406315272, 311356.5305897943], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5385600.0000, 
sim time next is 5386200.0000, 
raw observation next is [31.98333333333333, 72.16666666666667, 1.0, 2.0, 0.4781107612879184, 1.0, 1.0, 0.4781107612879184, 1.0, 1.0, 0.8303202787122382, 6.9112, 6.9112, 170.5573041426782, 2005471.428910951, 2005471.428910951, 400340.6073540525], 
processed observation next is [1.0, 0.34782608695652173, 0.7148499210110584, 0.7216666666666667, 1.0, 1.0, 0.371217784684239, 1.0, 0.5, 0.371217784684239, 1.0, 0.5, 0.7930735106246807, 0.0, 0.0, 0.8375144448122397, 0.5570753969197085, 0.5570753969197085, 0.5975232945582872], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.80165374], dtype=float32), 1.495392]. 
=============================================
[2019-03-26 21:27:41,169] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1206676: loss 0.8211
[2019-03-26 21:27:41,174] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1206676: learning rate 0.0010
[2019-03-26 21:27:41,268] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8997847e-37 1.0000000e+00 0.0000000e+00 1.0712207e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:41,277] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2277
[2019-03-26 21:27:41,283] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.46666666666667, 58.33333333333334, 1.0, 2.0, 0.4923990788460354, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688047.7699068038, 688047.7699068038, 182416.9685964483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5592000.0000, 
sim time next is 5592600.0000, 
raw observation next is [32.2, 60.0, 1.0, 2.0, 0.487147862103459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680707.6960670656, 680707.6960670656, 181610.6420791815], 
processed observation next is [1.0, 0.7391304347826086, 0.7251184834123224, 0.6, 1.0, 1.0, 0.38210585795597474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18908547112974045, 0.18908547112974045, 0.27106065981967387], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.96445054], dtype=float32), 0.9754488]. 
=============================================
[2019-03-26 21:27:42,401] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0337358e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:42,411] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2472
[2019-03-26 21:27:42,416] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 74.66666666666667, 1.0, 2.0, 0.5730722300147552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800817.8942086841, 800817.8942086835, 195837.3381750356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5514000.0000, 
sim time next is 5514600.0000, 
raw observation next is [30.35, 76.33333333333333, 1.0, 2.0, 0.576406859247662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805479.5113056763, 805479.5113056763, 196433.5117151019], 
processed observation next is [1.0, 0.8260869565217391, 0.637440758293839, 0.7633333333333333, 1.0, 1.0, 0.4896468183706771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2237443086960212, 0.2237443086960212, 0.29318434584343567], 
reward next is 0.7068, 
noisyNet noise sample is [array([-2.1611848], dtype=float32), -0.32076362]. 
=============================================
[2019-03-26 21:27:44,033] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0068513e-12 9.8254170e-16 1.6185841e-12 1.0000000e+00 4.5683685e-10], sum to 1.0000
[2019-03-26 21:27:44,042] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6909
[2019-03-26 21:27:44,047] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.23333333333333, 69.33333333333333, 1.0, 2.0, 1.002576210934348, 1.0, 2.0, 1.002576210934348, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2804521.72575315, 2804521.725753151, 530554.8964116621], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5478000.0000, 
sim time next is 5478600.0000, 
raw observation next is [33.46666666666667, 68.66666666666667, 1.0, 2.0, 1.004847000295542, 1.0, 2.0, 1.004847000295542, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2810880.986005743, 2810880.986005743, 531932.7359093797], 
processed observation next is [1.0, 0.391304347826087, 0.7851500789889416, 0.6866666666666668, 1.0, 1.0, 1.0058397593922195, 1.0, 1.0, 1.0058397593922195, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7808002738904842, 0.7808002738904842, 0.7939294565811637], 
reward next is 0.2061, 
noisyNet noise sample is [array([-0.2695106], dtype=float32), -0.09507083]. 
=============================================
[2019-03-26 21:27:44,530] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1208177: loss 0.4218
[2019-03-26 21:27:44,532] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1208178: learning rate 0.0010
[2019-03-26 21:27:44,842] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1208311: loss 1.0593
[2019-03-26 21:27:44,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1208312: learning rate 0.0010
[2019-03-26 21:27:46,187] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1208911: loss 0.3720
[2019-03-26 21:27:46,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1208911: learning rate 0.0010
[2019-03-26 21:27:47,101] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:27:47,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6219
[2019-03-26 21:27:47,117] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.8, 55.0, 1.0, 2.0, 0.5212053075555958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728313.59667673, 728313.5966767307, 186985.6740319945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5745600.0000, 
sim time next is 5746200.0000, 
raw observation next is [33.0, 54.66666666666667, 1.0, 2.0, 0.5363100960623538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749427.9319116526, 749427.9319116526, 189480.3181243428], 
processed observation next is [0.0, 0.5217391304347826, 0.7630331753554502, 0.5466666666666667, 1.0, 1.0, 0.441337465135366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2081744255310146, 0.2081744255310146, 0.28280644496170565], 
reward next is 0.7172, 
noisyNet noise sample is [array([1.2781034], dtype=float32), -0.42009905]. 
=============================================
[2019-03-26 21:27:47,241] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1209382: loss 1.7588
[2019-03-26 21:27:47,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1209384: learning rate 0.0010
[2019-03-26 21:27:47,811] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1209637: loss 1.6131
[2019-03-26 21:27:47,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1209637: learning rate 0.0010
[2019-03-26 21:27:48,440] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1209920: loss 0.8835
[2019-03-26 21:27:48,441] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1209920: learning rate 0.0010
[2019-03-26 21:27:48,595] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9165215e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:48,603] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5864
[2019-03-26 21:27:48,613] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.1, 71.33333333333334, 1.0, 2.0, 0.5685703269645005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794524.5288780818, 794524.5288780824, 195037.5903009192], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5512800.0000, 
sim time next is 5513400.0000, 
raw observation next is [30.85, 73.0, 1.0, 2.0, 0.5699094366103854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796396.5124781496, 796396.5124781496, 195274.9081794913], 
processed observation next is [1.0, 0.8260869565217391, 0.661137440758294, 0.73, 1.0, 1.0, 0.4818185983257655, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22122125346615268, 0.22122125346615268, 0.2914550868350616], 
reward next is 0.7085, 
noisyNet noise sample is [array([-1.3228749], dtype=float32), -0.44524896]. 
=============================================
[2019-03-26 21:27:48,708] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1210034: loss 1.1313
[2019-03-26 21:27:48,710] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1210035: learning rate 0.0010
[2019-03-26 21:27:49,068] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1210195: loss 1.1274
[2019-03-26 21:27:49,070] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1210195: learning rate 0.0010
[2019-03-26 21:27:49,198] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1210255: loss 1.6976
[2019-03-26 21:27:49,200] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1210255: learning rate 0.0010
[2019-03-26 21:27:49,215] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1210262: loss 1.3033
[2019-03-26 21:27:49,217] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1210262: learning rate 0.0010
[2019-03-26 21:27:49,287] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1210293: loss 0.8383
[2019-03-26 21:27:49,290] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1210294: learning rate 0.0010
[2019-03-26 21:27:50,062] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1210645: loss 2.1190
[2019-03-26 21:27:50,068] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1210646: learning rate 0.0010
[2019-03-26 21:27:51,059] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1211091: loss 1.8305
[2019-03-26 21:27:51,062] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1211092: learning rate 0.0010
[2019-03-26 21:27:53,725] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1212285: loss 3.9809
[2019-03-26 21:27:53,728] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1212285: learning rate 0.0010
[2019-03-26 21:27:56,578] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1213561: loss 1.1937
[2019-03-26 21:27:56,583] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1213562: learning rate 0.0010
[2019-03-26 21:27:59,028] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1214656: loss 2.8568
[2019-03-26 21:27:59,031] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1214657: learning rate 0.0010
[2019-03-26 21:27:59,384] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4991006e-38 1.0000000e+00 0.0000000e+00 8.8296179e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:27:59,394] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3311
[2019-03-26 21:27:59,399] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.057030420481707, 6.9112, 168.9116496823552, 1557282.100177859, 1453825.781208625, 311349.760079369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5802000.0000, 
sim time next is 5802600.0000, 
raw observation next is [26.25, 92.66666666666666, 1.0, 2.0, 1.009234403524848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128061637083, 1410720.93145806, 1410720.93145806, 301751.5491550232], 
processed observation next is [1.0, 0.13043478260869565, 0.4431279620853081, 0.9266666666666665, 1.0, 1.0, 1.011125787379335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294392068810652, 0.39186692540501666, 0.39186692540501666, 0.4503754465000346], 
reward next is 0.5496, 
noisyNet noise sample is [array([0.32076138], dtype=float32), 0.78622514]. 
=============================================
[2019-03-26 21:28:00,174] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:28:00,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5103
[2019-03-26 21:28:00,189] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.6, 60.0, 1.0, 2.0, 0.5201989665103799, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726906.8906554563, 726906.8906554563, 186821.6285277968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5742000.0000, 
sim time next is 5742600.0000, 
raw observation next is [31.8, 59.16666666666666, 1.0, 2.0, 0.5258469851833755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734801.9538435276, 734801.9538435276, 187744.7910346609], 
processed observation next is [0.0, 0.4782608695652174, 0.7061611374407584, 0.5916666666666666, 1.0, 1.0, 0.42873130744984994, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20411165384542432, 0.20411165384542432, 0.28021610602188196], 
reward next is 0.7198, 
noisyNet noise sample is [array([1.449952], dtype=float32), -0.6754158]. 
=============================================
[2019-03-26 21:28:02,263] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1216097: loss 3.6616
[2019-03-26 21:28:02,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1216097: learning rate 0.0010
[2019-03-26 21:28:02,647] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1216272: loss 3.8627
[2019-03-26 21:28:02,651] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1216273: learning rate 0.0010
[2019-03-26 21:28:04,435] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1217067: loss 0.9314
[2019-03-26 21:28:04,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1217069: learning rate 0.0010
[2019-03-26 21:28:05,167] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1217398: loss 5.8069
[2019-03-26 21:28:05,170] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1217398: learning rate 0.0010
[2019-03-26 21:28:05,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3714109e-18 1.3204317e-03 2.0097260e-19 9.9867958e-01 1.2841043e-19], sum to 1.0000
[2019-03-26 21:28:05,300] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0508
[2019-03-26 21:28:05,305] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.03333333333333, 78.5, 1.0, 2.0, 0.7497371346868698, 1.0, 2.0, 0.7497371346868698, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2096641.539723324, 2096641.539723323, 396150.7280159807], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5907000.0000, 
sim time next is 5907600.0000, 
raw observation next is [30.2, 78.0, 1.0, 2.0, 0.77276844958043, 1.0, 2.0, 0.77276844958043, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2161113.632846738, 2161113.632846738, 406839.0646555874], 
processed observation next is [1.0, 0.391304347826087, 0.6303317535545023, 0.78, 1.0, 1.0, 0.7262270476872651, 1.0, 1.0, 0.7262270476872651, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6003093424574273, 0.6003093424574273, 0.6072224845605783], 
reward next is 0.3928, 
noisyNet noise sample is [array([-1.1573068], dtype=float32), -0.40063623]. 
=============================================
[2019-03-26 21:28:05,700] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1217631: loss 10.6419
[2019-03-26 21:28:05,702] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1217632: learning rate 0.0010
[2019-03-26 21:28:06,419] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1217950: loss 1.4552
[2019-03-26 21:28:06,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1217950: learning rate 0.0010
[2019-03-26 21:28:06,497] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1217987: loss 4.0571
[2019-03-26 21:28:06,498] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1217988: learning rate 0.0010
[2019-03-26 21:28:06,903] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1218166: loss 15.4320
[2019-03-26 21:28:06,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1218167: learning rate 0.0010
[2019-03-26 21:28:07,209] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1218298: loss 5.1455
[2019-03-26 21:28:07,211] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1218299: learning rate 0.0010
[2019-03-26 21:28:07,272] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1218330: loss 5.5764
[2019-03-26 21:28:07,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1218331: learning rate 0.0010
[2019-03-26 21:28:07,295] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1218340: loss 1.1493
[2019-03-26 21:28:07,296] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1218340: learning rate 0.0010
[2019-03-26 21:28:08,094] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1218697: loss 8.2287
[2019-03-26 21:28:08,095] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1218697: learning rate 0.0010
[2019-03-26 21:28:09,045] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1219120: loss 11.4947
[2019-03-26 21:28:09,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1219120: learning rate 0.0010
[2019-03-26 21:28:11,736] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1220328: loss 1.8711
[2019-03-26 21:28:11,738] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1220330: learning rate 0.0010
[2019-03-26 21:28:14,840] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1221714: loss 2.0151
[2019-03-26 21:28:14,847] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1221714: learning rate 0.0010
[2019-03-26 21:28:15,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.782436e-28 0.000000e+00], sum to 1.0000
[2019-03-26 21:28:15,341] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9977
[2019-03-26 21:28:15,347] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 90.33333333333333, 1.0, 2.0, 0.6734618237490946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 941165.6530311353, 941165.6530311353, 215220.992789946], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6068400.0000, 
sim time next is 6069000.0000, 
raw observation next is [26.76666666666667, 89.66666666666667, 1.0, 2.0, 0.674263405035288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 942286.3631138303, 942286.3631138309, 215387.915329292], 
processed observation next is [1.0, 0.21739130434782608, 0.46761453396524505, 0.8966666666666667, 1.0, 1.0, 0.607546271126853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26174621197606396, 0.2617462119760641, 0.32147450049148063], 
reward next is 0.6785, 
noisyNet noise sample is [array([1.4609531], dtype=float32), -0.14095822]. 
=============================================
[2019-03-26 21:28:15,364] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[56.481358]
 [56.372044]
 [56.232037]
 [56.20768 ]
 [56.345753]], R is [[56.65549469]
 [56.76771545]
 [56.87810516]
 [56.98714828]
 [57.08477783]].
[2019-03-26 21:28:16,936] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1222632: loss 3.1756
[2019-03-26 21:28:16,939] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1222632: learning rate 0.0010
[2019-03-26 21:28:17,161] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3675441e-27 1.0000000e+00 1.0268980e-30 7.1838821e-15 1.3225516e-32], sum to 1.0000
[2019-03-26 21:28:17,174] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5391
[2019-03-26 21:28:17,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2152371.332426536 W.
[2019-03-26 21:28:17,194] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.7, 66.5, 1.0, 2.0, 0.898095927733748, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987020017028772, 6.9112, 168.9124417206691, 2152371.332426536, 2098582.16675448, 434225.9549184554], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6103800.0000, 
sim time next is 6104400.0000, 
raw observation next is [30.66666666666666, 66.66666666666667, 1.0, 2.0, 0.7777608289205723, 1.0, 1.0, 0.7777608289205723, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2175089.427699334, 2175089.427699334, 409186.7922301], 
processed observation next is [1.0, 0.6521739130434783, 0.6524486571879934, 0.6666666666666667, 1.0, 1.0, 0.7322419625549064, 1.0, 0.5, 0.7322419625549064, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6041915076942594, 0.6041915076942594, 0.6107265555673135], 
reward next is 0.3893, 
noisyNet noise sample is [array([0.6415629], dtype=float32), 1.1341406]. 
=============================================
[2019-03-26 21:28:20,266] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1224120: loss 2.7589
[2019-03-26 21:28:20,267] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1224120: learning rate 0.0010
[2019-03-26 21:28:20,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.4177897e-15 7.3589887e-03 7.3314837e-17 9.9264103e-01 6.9773381e-16], sum to 1.0000
[2019-03-26 21:28:20,284] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6929
[2019-03-26 21:28:20,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.977804e-23 0.000000e+00], sum to 1.0000
[2019-03-26 21:28:20,288] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.4, 68.33333333333334, 1.0, 2.0, 0.5528960204322407, 1.0, 2.0, 0.5528960204322407, 1.0, 2.0, 0.9564548139828313, 6.9112, 6.9112, 170.5573041426782, 2319486.913933739, 2319486.913933739, 452896.6236746135], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6108000.0000, 
sim time next is 6108600.0000, 
raw observation next is [30.35, 68.66666666666666, 1.0, 2.0, 0.827435686832493, 1.0, 2.0, 0.827435686832493, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2314144.768042214, 2314144.768042214, 433445.1671245024], 
processed observation next is [1.0, 0.6956521739130435, 0.637440758293839, 0.6866666666666665, 1.0, 1.0, 0.7920911889548109, 1.0, 1.0, 0.7920911889548109, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6428179911228373, 0.6428179911228373, 0.6469330852604513], 
reward next is 0.3531, 
noisyNet noise sample is [array([-0.7789157], dtype=float32), 0.765264]. 
=============================================
[2019-03-26 21:28:20,297] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2877
[2019-03-26 21:28:20,303] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.58333333333334, 92.0, 1.0, 2.0, 0.8195761215711496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1145471.112078917, 1145471.112078916, 248772.8191490404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6149400.0000, 
sim time next is 6150000.0000, 
raw observation next is [26.56666666666667, 92.0, 1.0, 2.0, 0.7094215103613662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 991442.9196514131, 991442.9196514137, 222892.1227399357], 
processed observation next is [1.0, 0.17391304347826086, 0.45813586097946307, 0.92, 1.0, 1.0, 0.6499054341703208, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27540081101428143, 0.2754008110142816, 0.33267481005960553], 
reward next is 0.6673, 
noisyNet noise sample is [array([-0.60065097], dtype=float32), -0.22889224]. 
=============================================
[2019-03-26 21:28:20,321] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.180687]
 [58.769833]
 [58.941807]
 [59.13464 ]
 [58.99747 ]], R is [[58.39900589]
 [58.44371414]
 [58.53017044]
 [58.61323929]
 [58.69249344]].
[2019-03-26 21:28:20,684] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1224307: loss 2.5729
[2019-03-26 21:28:20,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1224309: learning rate 0.0010
[2019-03-26 21:28:22,147] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1224899: loss 1378.6973
[2019-03-26 21:28:22,151] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1224900: learning rate 0.0010
[2019-03-26 21:28:22,369] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 21:28:22,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:28:22,375] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:28:22,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:28:22,378] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:28:22,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:28:22,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:28:22,381] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:28:22,378] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:28:22,382] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:28:22,386] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:28:22,396] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-03-26 21:28:22,416] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-03-26 21:28:22,438] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-03-26 21:28:22,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-03-26 21:28:22,485] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-03-26 21:28:30,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.49429128]
[2019-03-26 21:28:30,243] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.66666666666667, 74.33333333333334, 1.0, 2.0, 0.4821566031659228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742447.6737088154, 742447.6737088148, 189089.0669748627]
[2019-03-26 21:28:30,244] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:28:30,247] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5478245e-32 0.0000000e+00], sampled 0.8363661329545659
[2019-03-26 21:29:53,854] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.49429128]
[2019-03-26 21:29:53,855] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.3, 71.5, 1.0, 2.0, 0.4871206774760796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 680669.6979250896, 680669.6979250903, 181602.5562100193]
[2019-03-26 21:29:53,856] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:29:53,858] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.669276e-36 0.000000e+00], sampled 0.9902444084221942
[2019-03-26 21:30:14,226] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.1551 3164638718.8196 1776.0000
[2019-03-26 21:30:14,522] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 21:30:14,681] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 21:30:14,710] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0319 2779830318.5638 932.0000
[2019-03-26 21:30:14,752] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 21:30:15,768] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1225000, evaluation results [1225000.0, 7877.155125466313, 3164638718.81958, 1776.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8660.03188781958, 2779830318.5637875, 932.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 21:30:16,429] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1225302: loss 2.7189
[2019-03-26 21:30:16,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1225303: learning rate 0.0010
[2019-03-26 21:30:17,020] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1225572: loss 2.4716
[2019-03-26 21:30:17,021] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1225572: learning rate 0.0010
[2019-03-26 21:30:17,481] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1225778: loss 1357.4297
[2019-03-26 21:30:17,483] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1225778: learning rate 0.0010
[2019-03-26 21:30:17,922] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1225977: loss 2.1651
[2019-03-26 21:30:17,924] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1225978: learning rate 0.0010
[2019-03-26 21:30:18,427] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.542223e-28 0.000000e+00], sum to 1.0000
[2019-03-26 21:30:18,434] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5219
[2019-03-26 21:30:18,442] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 88.0, 1.0, 2.0, 0.5367055306939107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749980.6986402537, 749980.6986402532, 189547.0545906634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6132600.0000, 
sim time next is 6133200.0000, 
raw observation next is [27.16666666666667, 88.33333333333333, 1.0, 2.0, 0.5388578802352892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752989.4113689464, 752989.4113689458, 189908.2768996689], 
processed observation next is [1.0, 1.0, 0.4865718799368091, 0.8833333333333333, 1.0, 1.0, 0.4444070846208304, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2091637253802629, 0.20916372538026273, 0.2834451894024909], 
reward next is 0.7166, 
noisyNet noise sample is [array([0.79475176], dtype=float32), 0.6921818]. 
=============================================
[2019-03-26 21:30:18,503] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1226237: loss 1.6545
[2019-03-26 21:30:18,506] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1226237: learning rate 0.0010
[2019-03-26 21:30:18,643] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1226293: loss 2.2170
[2019-03-26 21:30:18,648] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1226294: learning rate 0.0010
[2019-03-26 21:30:18,707] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1226326: loss 1.4390
[2019-03-26 21:30:18,715] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1226327: learning rate 0.0010
[2019-03-26 21:30:18,758] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5112637e-19 8.4413560e-03 1.8882284e-21 9.9155867e-01 3.7270212e-20], sum to 1.0000
[2019-03-26 21:30:18,769] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2117
[2019-03-26 21:30:18,776] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.71666666666667, 80.66666666666667, 1.0, 2.0, 0.7725472046359921, 1.0, 2.0, 0.7725472046359921, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2160494.278290675, 2160494.278290675, 406727.829580905], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6169800.0000, 
sim time next is 6170400.0000, 
raw observation next is [28.8, 80.0, 1.0, 2.0, 0.780444973129417, 1.0, 2.0, 0.780444973129417, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2182603.56526164, 2182603.56526164, 410463.3842498112], 
processed observation next is [1.0, 0.43478260869565216, 0.5639810426540285, 0.8, 1.0, 1.0, 0.7354758712402614, 1.0, 1.0, 0.7354758712402614, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6062787681282333, 0.6062787681282333, 0.6126319167907629], 
reward next is 0.3874, 
noisyNet noise sample is [array([-1.2678125], dtype=float32), 0.5564328]. 
=============================================
[2019-03-26 21:30:18,866] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1226395: loss 2.0509
[2019-03-26 21:30:18,868] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1226395: learning rate 0.0010
[2019-03-26 21:30:19,783] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1226804: loss 0.8135
[2019-03-26 21:30:19,787] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1226804: learning rate 0.0010
[2019-03-26 21:30:20,698] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1227211: loss 1.7102
[2019-03-26 21:30:20,699] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1227211: learning rate 0.0010
[2019-03-26 21:30:22,880] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1228195: loss 1208.7686
[2019-03-26 21:30:22,882] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1228196: learning rate 0.0010
[2019-03-26 21:30:23,394] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3466125e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:23,404] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1473
[2019-03-26 21:30:23,408] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.86666666666667, 62.0, 1.0, 2.0, 0.511359269741871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714550.4691691964, 714550.4691691964, 185395.0851147994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6271800.0000, 
sim time next is 6272400.0000, 
raw observation next is [30.83333333333334, 62.0, 1.0, 2.0, 0.5098208307436327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712400.0027689746, 712400.002768974, 185149.2394375378], 
processed observation next is [0.0, 0.6086956521739131, 0.6603475513428123, 0.62, 1.0, 1.0, 0.40942268764293094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19788888965804852, 0.19788888965804835, 0.2763421484142355], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.45441037], dtype=float32), 1.567782]. 
=============================================
[2019-03-26 21:30:26,429] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1229783: loss 1953.7075
[2019-03-26 21:30:26,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1229783: learning rate 0.0010
[2019-03-26 21:30:28,395] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1230667: loss 1704.6156
[2019-03-26 21:30:28,397] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1230667: learning rate 0.0010
[2019-03-26 21:30:29,452] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0982952e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:29,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3988
[2019-03-26 21:30:29,468] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 77.0, 1.0, 2.0, 0.5221943287481604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729696.0940914345, 729696.0940914345, 187146.4758004152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6336000.0000, 
sim time next is 6336600.0000, 
raw observation next is [28.55, 76.33333333333334, 1.0, 2.0, 0.5230728038745603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730924.0668513494, 730924.06685135, 187290.0561081764], 
processed observation next is [0.0, 0.34782608695652173, 0.552132701421801, 0.7633333333333334, 1.0, 1.0, 0.42538892033079556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20303446301426373, 0.2030344630142639, 0.27953739717638265], 
reward next is 0.7205, 
noisyNet noise sample is [array([0.78549695], dtype=float32), -0.5733392]. 
=============================================
[2019-03-26 21:30:30,134] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5498972e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:30,138] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0910
[2019-03-26 21:30:30,148] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.8, 81.0, 1.0, 2.0, 0.5230324939998808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730867.7198315278, 730867.7198315272, 187283.4548789769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6332400.0000, 
sim time next is 6333000.0000, 
raw observation next is [27.9, 80.33333333333333, 1.0, 2.0, 0.522397246669101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729979.7419275157, 729979.7419275151, 187179.6792625401], 
processed observation next is [0.0, 0.30434782608695654, 0.5213270142180094, 0.8033333333333332, 1.0, 1.0, 0.4245749959868687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20277215053542103, 0.20277215053542086, 0.2793726556157315], 
reward next is 0.7206, 
noisyNet noise sample is [array([-1.1210904], dtype=float32), -0.6534483]. 
=============================================
[2019-03-26 21:30:30,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.936  ]
 [72.79266]
 [72.77287]
 [72.75585]
 [72.74538]], R is [[72.93621826]
 [72.92733002]
 [72.91835022]
 [72.90938568]
 [72.90061188]].
[2019-03-26 21:30:31,495] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1232057: loss 909.9354
[2019-03-26 21:30:31,498] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1232057: learning rate 0.0010
[2019-03-26 21:30:32,111] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1232331: loss 881.3734
[2019-03-26 21:30:32,115] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1232332: learning rate 0.0010
[2019-03-26 21:30:33,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6328573e-36 1.0000000e+00 0.0000000e+00 2.0258024e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:33,342] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6061
[2019-03-26 21:30:33,348] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666667, 84.66666666666667, 1.0, 2.0, 0.9667340763539486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129514303031, 1351275.613025517, 1351275.613025517, 288952.2227001259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6403800.0000, 
sim time next is 6404400.0000, 
raw observation next is [26.9, 85.0, 1.0, 2.0, 0.958417360410368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565091625, 1339643.390294484, 1339643.390294483, 286510.4543876376], 
processed observation next is [1.0, 0.13043478260869565, 0.4739336492890995, 0.85, 1.0, 1.0, 0.949900434229359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451460738, 0.37212316397069, 0.37212316397068973, 0.4276275438621457], 
reward next is 0.5724, 
noisyNet noise sample is [array([-0.9790189], dtype=float32), -0.74110883]. 
=============================================
[2019-03-26 21:30:33,882] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1233102: loss 0.1181
[2019-03-26 21:30:33,886] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1233102: learning rate 0.0010
[2019-03-26 21:30:34,264] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1233270: loss 810.5388
[2019-03-26 21:30:34,266] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1233270: learning rate 0.0010
[2019-03-26 21:30:34,810] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1233519: loss 793.0243
[2019-03-26 21:30:34,814] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1233519: learning rate 0.0010
[2019-03-26 21:30:35,749] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1233937: loss 0.0629
[2019-03-26 21:30:35,754] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1233939: learning rate 0.0010
[2019-03-26 21:30:35,759] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1233940: loss 213.6226
[2019-03-26 21:30:35,765] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1233942: learning rate 0.0010
[2019-03-26 21:30:36,288] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1234177: loss 883.1292
[2019-03-26 21:30:36,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1234177: learning rate 0.0010
[2019-03-26 21:30:36,360] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1234207: loss 410.8628
[2019-03-26 21:30:36,362] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1234207: learning rate 0.0010
[2019-03-26 21:30:36,515] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1234278: loss 841.4462
[2019-03-26 21:30:36,516] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1234278: learning rate 0.0010
[2019-03-26 21:30:36,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1234367: loss 822.3746
[2019-03-26 21:30:36,714] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1234369: learning rate 0.0010
[2019-03-26 21:30:37,446] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5485834e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:37,454] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4929
[2019-03-26 21:30:37,461] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 75.0, 1.0, 2.0, 0.4895861670088543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684115.9166385791, 684115.9166385791, 181980.4981491886], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6553800.0000, 
sim time next is 6554400.0000, 
raw observation next is [27.8, 75.66666666666666, 1.0, 2.0, 0.4916107407467386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686945.8392241794, 686945.83922418, 182292.0809893254], 
processed observation next is [1.0, 0.8695652173913043, 0.5165876777251186, 0.7566666666666666, 1.0, 1.0, 0.3874828201767935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19081828867338316, 0.19081828867338332, 0.2720777328198886], 
reward next is 0.7279, 
noisyNet noise sample is [array([-1.5520568], dtype=float32), 0.069952436]. 
=============================================
[2019-03-26 21:30:37,479] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1234709: loss 576.7833
[2019-03-26 21:30:37,481] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1234710: learning rate 0.0010
[2019-03-26 21:30:38,644] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1235227: loss 672.6201
[2019-03-26 21:30:38,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1235227: learning rate 0.0010
[2019-03-26 21:30:41,006] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1236284: loss 0.0197
[2019-03-26 21:30:41,007] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1236284: learning rate 0.0010
[2019-03-26 21:30:42,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7164182e-22 2.0503987e-02 4.8476513e-25 9.7949606e-01 2.1384436e-24], sum to 1.0000
[2019-03-26 21:30:42,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7316
[2019-03-26 21:30:42,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.13333333333333, 59.0, 1.0, 2.0, 0.7442567214277815, 1.0, 2.0, 0.7442567214277815, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2081300.663492376, 2081300.663492376, 393636.9005196973], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6535200.0000, 
sim time next is 6535800.0000, 
raw observation next is [31.0, 59.5, 1.0, 2.0, 0.7451643877360719, 1.0, 2.0, 0.7451643877360719, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2083841.404973908, 2083841.404973908, 394049.1354006689], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.595, 1.0, 1.0, 0.6929691418506889, 1.0, 1.0, 0.6929691418506889, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5788448347149745, 0.5788448347149745, 0.5881330379114461], 
reward next is 0.4119, 
noisyNet noise sample is [array([-1.6391414], dtype=float32), 0.28134075]. 
=============================================
[2019-03-26 21:30:44,426] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1237822: loss 0.0046
[2019-03-26 21:30:44,429] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1237823: learning rate 0.0010
[2019-03-26 21:30:46,347] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1238683: loss 0.0190
[2019-03-26 21:30:46,349] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1238684: learning rate 0.0010
[2019-03-26 21:30:49,593] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1240145: loss 0.0076
[2019-03-26 21:30:49,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1240146: learning rate 0.0010
[2019-03-26 21:30:50,088] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1240365: loss 0.0108
[2019-03-26 21:30:50,090] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1240366: learning rate 0.0010
[2019-03-26 21:30:51,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1240986: loss 3.2615
[2019-03-26 21:30:51,472] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1240987: learning rate 0.0010
[2019-03-26 21:30:52,206] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1241311: loss 0.0417
[2019-03-26 21:30:52,209] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1241312: learning rate 0.0010
[2019-03-26 21:30:52,717] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1241542: loss 0.1528
[2019-03-26 21:30:52,722] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1241544: learning rate 0.0010
[2019-03-26 21:30:53,348] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1241824: loss 21.2292
[2019-03-26 21:30:53,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1241825: learning rate 0.0010
[2019-03-26 21:30:53,843] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1242045: loss 0.0267
[2019-03-26 21:30:53,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1242045: learning rate 0.0010
[2019-03-26 21:30:54,241] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1242223: loss 0.0043
[2019-03-26 21:30:54,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1242224: learning rate 0.0010
[2019-03-26 21:30:54,410] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1242295: loss 0.0118
[2019-03-26 21:30:54,411] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1242296: learning rate 0.0010
[2019-03-26 21:30:54,524] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1242349: loss 0.0063
[2019-03-26 21:30:54,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1242349: learning rate 0.0010
[2019-03-26 21:30:54,859] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1242497: loss 0.0271
[2019-03-26 21:30:54,864] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1242498: learning rate 0.0010
[2019-03-26 21:30:55,640] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1242851: loss 0.0122
[2019-03-26 21:30:55,643] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1242852: learning rate 0.0010
[2019-03-26 21:30:56,512] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6395143e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:30:56,522] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4749
[2019-03-26 21:30:56,526] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 90.66666666666667, 1.0, 2.0, 0.4149436429970368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611070.153209249, 611070.153209249, 175341.5404123613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6930600.0000, 
sim time next is 6931200.0000, 
raw observation next is [24.0, 89.33333333333334, 1.0, 2.0, 0.4156428271130365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611545.8753499115, 611545.8753499115, 175370.8192671171], 
processed observation next is [0.0, 0.21739130434782608, 0.3364928909952607, 0.8933333333333334, 1.0, 1.0, 0.2959552133892006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1698738542638643, 0.1698738542638643, 0.2617474914434584], 
reward next is 0.7383, 
noisyNet noise sample is [array([-0.45864692], dtype=float32), 0.99858004]. 
=============================================
[2019-03-26 21:30:56,608] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1243282: loss 0.0767
[2019-03-26 21:30:56,610] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1243282: learning rate 0.0010
[2019-03-26 21:30:56,995] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 7.73478e-22 0.00000e+00], sum to 1.0000
[2019-03-26 21:30:57,004] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5237
[2019-03-26 21:30:57,007] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.13333333333333, 49.66666666666666, 1.0, 2.0, 0.983175641571401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1517779.401463615, 1517779.401463614, 314605.7827455242], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6799200.0000, 
sim time next is 6799800.0000, 
raw observation next is [29.11666666666667, 49.83333333333334, 1.0, 2.0, 0.9959389549247336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1536659.084714192, 1536659.084714191, 318770.0041731353], 
processed observation next is [1.0, 0.6956521739130435, 0.5789889415481835, 0.4983333333333334, 1.0, 1.0, 0.9951071746081128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4268497457539422, 0.42684974575394197, 0.47577612563154525], 
reward next is 0.5242, 
noisyNet noise sample is [array([0.30348948], dtype=float32), 0.97575504]. 
=============================================
[2019-03-26 21:30:58,655] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1244191: loss 128.8376
[2019-03-26 21:30:58,657] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1244191: learning rate 0.0010
[2019-03-26 21:31:02,229] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1245796: loss -20.6791
[2019-03-26 21:31:02,232] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1245797: learning rate 0.0010
[2019-03-26 21:31:02,311] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.9052623e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:31:02,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5922
[2019-03-26 21:31:02,326] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.86666666666667, 58.66666666666667, 1.0, 2.0, 0.4558483970616229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 648770.5358751133, 648770.5358751127, 178488.3026248644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6974400.0000, 
sim time next is 6975000.0000, 
raw observation next is [29.8, 58.5, 1.0, 2.0, 0.4528090363367156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646873.6021973578, 646873.6021973571, 178353.7599114793], 
processed observation next is [0.0, 0.7391304347826086, 0.6113744075829385, 0.585, 1.0, 1.0, 0.3407337787189345, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17968711172148827, 0.17968711172148807, 0.26619964165892435], 
reward next is 0.7338, 
noisyNet noise sample is [array([0.08059351], dtype=float32), 0.7120269]. 
=============================================
[2019-03-26 21:31:02,341] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.22909]
 [69.22768]
 [69.20823]
 [69.11203]
 [69.13991]], R is [[69.30461884]
 [69.34516907]
 [69.38528442]
 [69.42402649]
 [69.46194458]].
[2019-03-26 21:31:04,113] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1246639: loss -107.6946
[2019-03-26 21:31:04,115] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1246639: learning rate 0.0010
[2019-03-26 21:31:07,325] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1248082: loss -28.7232
[2019-03-26 21:31:07,331] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1248083: learning rate 0.0010
[2019-03-26 21:31:07,870] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1248327: loss -193.5912
[2019-03-26 21:31:07,876] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1248327: learning rate 0.0010
[2019-03-26 21:31:09,729] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1249156: loss 0.0340
[2019-03-26 21:31:09,731] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1249156: learning rate 0.0010
[2019-03-26 21:31:09,845] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8262086e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 21:31:09,853] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9822
[2019-03-26 21:31:09,858] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 78.0, 1.0, 2.0, 0.4482107158004517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642325.5437809144, 642325.5437809144, 177941.5072398884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6998400.0000, 
sim time next is 6999000.0000, 
raw observation next is [26.26666666666667, 78.33333333333333, 1.0, 2.0, 0.4498600878039608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 644362.2943696409, 644362.2943696403, 178140.1221229633], 
processed observation next is [1.0, 0.0, 0.44391785150079005, 0.7833333333333333, 1.0, 1.0, 0.3371808286794708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17898952621378914, 0.17898952621378897, 0.26588077928800496], 
reward next is 0.7341, 
noisyNet noise sample is [array([-1.4222201], dtype=float32), 0.50750554]. 
=============================================
[2019-03-26 21:31:09,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[76.87622 ]
 [82.597824]
 [82.62132 ]
 [82.65285 ]
 [82.70356 ]], R is [[74.0236969 ]
 [74.01787567]
 [74.01210785]
 [74.00650024]
 [74.00118256]].
[2019-03-26 21:31:10,054] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1249298: loss 154.8522
[2019-03-26 21:31:10,055] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1249299: learning rate 0.0010
[2019-03-26 21:31:10,615] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1249551: loss -141.7709
[2019-03-26 21:31:10,617] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1249552: learning rate 0.0010
[2019-03-26 21:31:11,287] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1249863: loss 0.0365
[2019-03-26 21:31:11,289] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1249863: learning rate 0.0010
[2019-03-26 21:31:11,659] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-03-26 21:31:11,660] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:31:11,661] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:31:11,661] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:31:11,662] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:31:11,665] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:31:11,666] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:31:11,667] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:31:11,664] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:31:11,670] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:31:11,672] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:31:11,695] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-03-26 21:31:11,717] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-03-26 21:31:11,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-03-26 21:31:11,762] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-03-26 21:31:11,763] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-03-26 21:31:19,013] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:19,014] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.8, 77.5, 1.0, 2.0, 0.3972677704483104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653737.6474729812, 653737.6474729812, 179071.1754221473]
[2019-03-26 21:31:19,016] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:31:19,020] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3603427e-24 0.0000000e+00], sampled 0.8948746930483648
[2019-03-26 21:31:40,524] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:40,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.5, 91.0, 1.0, 2.0, 0.8253365866930183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1153526.5329145, 1153526.532914501, 250222.0657609555]
[2019-03-26 21:31:40,527] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:31:40,529] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.0793284e-21 0.0000000e+00], sampled 0.4675556176979686
[2019-03-26 21:31:43,560] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:43,560] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.7, 94.0, 1.0, 2.0, 0.3768230200393364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 567989.627089897, 567989.627089897, 171822.8452709726]
[2019-03-26 21:31:43,562] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:31:43,564] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6873226e-24 0.0000000e+00], sampled 0.10575183890848339
[2019-03-26 21:31:45,830] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:45,832] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.0499483, 56.37746376, 1.0, 2.0, 0.648818463519946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906711.755212815, 906711.755212815, 210182.0900024825]
[2019-03-26 21:31:45,835] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:31:45,837] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.507018e-23 0.000000e+00], sampled 0.7697704886207185
[2019-03-26 21:31:54,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:54,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.7, 77.0, 1.0, 2.0, 0.6448287847523375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901133.8878803306, 901133.8878803306, 209384.7961152852]
[2019-03-26 21:31:54,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:31:54,616] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1702923e-22 0.0000000e+00], sampled 0.040863760297272234
[2019-03-26 21:31:57,239] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:31:57,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.7, 68.0, 1.0, 2.0, 0.7699653272104823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076098.023661495, 1076098.023661495, 236672.8178538751]
[2019-03-26 21:31:57,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:31:57,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.615208e-21 0.000000e+00], sampled 0.0399128396194105
[2019-03-26 21:32:16,723] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:32:16,724] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.58253077, 85.23818496333334, 1.0, 2.0, 0.5351916429672248, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747864.4799696682, 747864.4799696688, 189293.7649797396]
[2019-03-26 21:32:16,725] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:32:16,726] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.2888976e-24 0.0000000e+00], sampled 0.5727072167557742
[2019-03-26 21:32:22,136] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:32:22,141] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.26715754666667, 83.64705603333333, 1.0, 2.0, 0.4949789167894408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691653.8481722331, 691653.8481722337, 182812.2544962781]
[2019-03-26 21:32:22,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:32:22,149] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.221904e-24 0.000000e+00], sampled 0.9464269485024934
[2019-03-26 21:32:44,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:32:44,224] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.58846088, 85.66045873, 1.0, 2.0, 0.6956713870350888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 972217.8154124003, 972217.8154124003, 219911.6038844326]
[2019-03-26 21:32:44,224] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:32:44,227] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6082911e-21 0.0000000e+00], sampled 0.19267581373613818
[2019-03-26 21:32:51,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.56534237]
[2019-03-26 21:32:51,757] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.5, 64.66666666666667, 1.0, 2.0, 0.4408929778499958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 633161.8353945035, 633161.8353945035, 177054.3946692837]
[2019-03-26 21:32:51,759] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:32:51,762] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2958759e-24 0.0000000e+00], sampled 0.2885039763488402
[2019-03-26 21:33:05,734] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8216 2779836413.0450 927.0000
[2019-03-26 21:33:06,271] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.1922 3164270909.1007 1771.0000
[2019-03-26 21:33:06,350] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.7216 2843266204.0812 1130.0000
[2019-03-26 21:33:06,382] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.8776 2927980590.1882 1334.0000
[2019-03-26 21:33:06,384] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6418 3008292636.6174 1767.0000
[2019-03-26 21:33:07,401] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1250000, evaluation results [1250000.0, 7878.19219004646, 3164270909.1007137, 1771.0, 8255.877618126327, 2927980590.1882486, 1334.0, 8660.821567294593, 2779836413.044969, 927.0, 7997.641807735364, 3008292636.61744, 1767.0, 8495.721599672193, 2843266204.081167, 1130.0]
[2019-03-26 21:33:07,522] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1250059: loss -28.3558
[2019-03-26 21:33:07,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1250060: learning rate 0.0010
[2019-03-26 21:33:07,703] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1250138: loss -119.0245
[2019-03-26 21:33:07,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1250138: learning rate 0.0010
[2019-03-26 21:33:07,943] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1250242: loss -128.2168
[2019-03-26 21:33:07,948] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1250243: learning rate 0.0010
[2019-03-26 21:33:08,033] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1250286: loss -190.5346
[2019-03-26 21:33:08,036] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1250287: learning rate 0.0010
[2019-03-26 21:33:08,408] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1250452: loss -50.0415
[2019-03-26 21:33:08,410] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1250454: learning rate 0.0010
[2019-03-26 21:33:09,175] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1250792: loss -136.1380
[2019-03-26 21:33:09,177] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1250792: learning rate 0.0010
[2019-03-26 21:33:10,033] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1251172: loss -175.7350
[2019-03-26 21:33:10,035] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1251172: learning rate 0.0010
[2019-03-26 21:33:12,240] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1252160: loss 0.0192
[2019-03-26 21:33:12,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1252160: learning rate 0.0010
[2019-03-26 21:33:15,912] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1253804: loss 0.2194
[2019-03-26 21:33:15,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1253806: learning rate 0.0010
[2019-03-26 21:33:17,790] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1254651: loss 0.1047
[2019-03-26 21:33:17,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1254652: learning rate 0.0010
[2019-03-26 21:33:18,906] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1510711e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:33:18,918] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8104
[2019-03-26 21:33:18,924] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 88.66666666666667, 1.0, 2.0, 0.3719014736728388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 565609.8918124081, 565609.8918124087, 171771.207091038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7240200.0000, 
sim time next is 7240800.0000, 
raw observation next is [23.0, 89.33333333333334, 1.0, 2.0, 0.3710377579367017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 564517.6232246225, 564517.6232246218, 171683.0425482135], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.8933333333333334, 1.0, 1.0, 0.2422141661887972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15681045089572845, 0.15681045089572826, 0.25624334708688584], 
reward next is 0.7438, 
noisyNet noise sample is [array([0.52714247], dtype=float32), 2.166105]. 
=============================================
[2019-03-26 21:33:20,989] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1256053: loss 0.1870
[2019-03-26 21:33:20,991] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1256054: learning rate 0.0010
[2019-03-26 21:33:21,334] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1256212: loss 0.1922
[2019-03-26 21:33:21,338] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1256213: learning rate 0.0010
[2019-03-26 21:33:23,000] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1256953: loss -169.3615
[2019-03-26 21:33:23,003] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1256955: learning rate 0.0010
[2019-03-26 21:33:23,748] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1257295: loss 0.3546
[2019-03-26 21:33:23,755] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1257296: learning rate 0.0010
[2019-03-26 21:33:24,525] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1257644: loss 0.6274
[2019-03-26 21:33:24,530] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1257648: learning rate 0.0010
[2019-03-26 21:33:24,574] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1257667: loss -68.5080
[2019-03-26 21:33:24,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1257667: learning rate 0.0010
[2019-03-26 21:33:25,487] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1258076: loss 0.3961
[2019-03-26 21:33:25,489] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1258076: learning rate 0.0010
[2019-03-26 21:33:25,712] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1258174: loss 0.2035
[2019-03-26 21:33:25,716] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1258176: learning rate 0.0010
[2019-03-26 21:33:25,930] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1258276: loss 0.0983
[2019-03-26 21:33:25,932] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1258276: learning rate 0.0010
[2019-03-26 21:33:26,175] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1258383: loss 0.0676
[2019-03-26 21:33:26,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1258383: learning rate 0.0010
[2019-03-26 21:33:26,419] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1258494: loss 0.1169
[2019-03-26 21:33:26,420] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1258494: learning rate 0.0010
[2019-03-26 21:33:27,239] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1258862: loss 0.2328
[2019-03-26 21:33:27,243] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1258863: learning rate 0.0010
[2019-03-26 21:33:28,185] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1259289: loss 0.3404
[2019-03-26 21:33:28,188] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1259290: learning rate 0.0010
[2019-03-26 21:33:29,941] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1260068: loss -256.0401
[2019-03-26 21:33:29,944] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1260068: learning rate 0.0010
[2019-03-26 21:33:33,654] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1261735: loss -34.9715
[2019-03-26 21:33:33,656] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1261735: learning rate 0.0010
[2019-03-26 21:33:35,487] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1262562: loss -9.7425
[2019-03-26 21:33:35,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1262562: learning rate 0.0010
[2019-03-26 21:33:38,085] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2234421e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 21:33:38,092] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9124
[2019-03-26 21:33:38,098] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4091832003651533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602415.1954450014, 602415.1954450014, 174523.8407494047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7524000.0000, 
sim time next is 7524600.0000, 
raw observation next is [23.46666666666667, 92.83333333333333, 1.0, 2.0, 0.407581719654442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600886.2128957993, 600886.2128957993, 174406.3971178725], 
processed observation next is [0.0, 0.08695652173913043, 0.31121642969984215, 0.9283333333333332, 1.0, 1.0, 0.2862430357282434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669128369154998, 0.1669128369154998, 0.26030805539980967], 
reward next is 0.7397, 
noisyNet noise sample is [array([-0.8802371], dtype=float32), -0.39116466]. 
=============================================
[2019-03-26 21:33:38,799] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1264048: loss -23.1748
[2019-03-26 21:33:38,801] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1264048: learning rate 0.0010
[2019-03-26 21:33:39,017] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1264142: loss -215.4851
[2019-03-26 21:33:39,019] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1264142: learning rate 0.0010
[2019-03-26 21:33:41,417] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1265218: loss 0.8824
[2019-03-26 21:33:41,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1265219: learning rate 0.0010
[2019-03-26 21:33:42,111] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1265532: loss 0.1241
[2019-03-26 21:33:42,113] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1265532: learning rate 0.0010
[2019-03-26 21:33:42,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:42,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:42,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-03-26 21:33:42,852] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1265899: loss 0.1297
[2019-03-26 21:33:42,857] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1265901: learning rate 0.0010
[2019-03-26 21:33:43,053] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1266015: loss 0.1172
[2019-03-26 21:33:43,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1266015: learning rate 0.0010
[2019-03-26 21:33:43,227] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1266110: loss 0.0721
[2019-03-26 21:33:43,228] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1266110: learning rate 0.0010
[2019-03-26 21:33:43,344] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1266188: loss 0.3375
[2019-03-26 21:33:43,345] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1266188: learning rate 0.0010
[2019-03-26 21:33:43,491] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1266268: loss 0.0708
[2019-03-26 21:33:43,493] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1266268: learning rate 0.0010
[2019-03-26 21:33:43,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:43,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:43,646] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-03-26 21:33:44,101] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3894896e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:33:44,111] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0822
[2019-03-26 21:33:44,115] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666666, 74.66666666666666, 1.0, 2.0, 0.5007804407418393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699763.2198669618, 699763.2198669618, 183719.2481085293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7756800.0000, 
sim time next is 7757400.0000, 
raw observation next is [28.38333333333333, 75.83333333333334, 1.0, 2.0, 0.5038040579441986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703989.6573216347, 703989.6573216347, 184194.8833086832], 
processed observation next is [1.0, 0.782608695652174, 0.5442338072669825, 0.7583333333333334, 1.0, 1.0, 0.402173563788191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19555268258934297, 0.19555268258934297, 0.2749177362816167], 
reward next is 0.7251, 
noisyNet noise sample is [array([-0.5252218], dtype=float32), 1.0832827]. 
=============================================
[2019-03-26 21:33:44,232] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1266650: loss 0.1350
[2019-03-26 21:33:44,239] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1266653: learning rate 0.0010
[2019-03-26 21:33:44,980] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1267069: loss 0.0569
[2019-03-26 21:33:44,983] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1267069: learning rate 0.0010
[2019-03-26 21:33:47,897] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:47,898] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:47,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-03-26 21:33:50,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:50,989] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:51,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-03-26 21:33:52,372] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2728296e-21 9.9974805e-01 1.3991868e-23 2.5196152e-04 1.0075956e-25], sum to 1.0000
[2019-03-26 21:33:52,382] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6399
[2019-03-26 21:33:52,390] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2410853.795488899 W.
[2019-03-26 21:33:52,397] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.079282848992903, 6.9112, 168.9117866320078, 2410853.795488899, 2291610.863321424, 476105.9314883973], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7831800.0000, 
sim time next is 7832400.0000, 
raw observation next is [31.0, 65.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.392840077976341, 6.9112, 168.910046032034, 2637409.072369143, 2295722.911853347, 475624.7857063041], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.6533333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.04816400779763406, 0.0, 0.8294256533709505, 0.7326136312136508, 0.637700808848152, 0.7098877398601553], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.95743924], dtype=float32), 0.09104825]. 
=============================================
[2019-03-26 21:33:52,558] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:52,558] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:52,628] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-03-26 21:33:55,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:55,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:55,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-03-26 21:33:55,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:55,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:55,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-03-26 21:33:57,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:57,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:57,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-03-26 21:33:57,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:57,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:57,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-03-26 21:33:58,295] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.675594e-13 0.000000e+00], sum to 1.0000
[2019-03-26 21:33:58,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7584
[2019-03-26 21:33:58,306] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 62.16666666666667, 1.0, 2.0, 0.9338398950633804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1391413.295148267, 1391413.295148266, 292020.8634988565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 49800.0000, 
sim time next is 50400.0000, 
raw observation next is [28.0, 62.0, 1.0, 2.0, 0.9934345129241986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1477009.605252179, 1477009.60525218, 310327.2248383393], 
processed observation next is [1.0, 0.6086956521739131, 0.5260663507109005, 0.62, 1.0, 1.0, 0.9920897746074682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4102804459033831, 0.41028044590338336, 0.46317496244528256], 
reward next is 0.5368, 
noisyNet noise sample is [array([1.1697142], dtype=float32), 0.43051487]. 
=============================================
[2019-03-26 21:33:58,572] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:58,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:58,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-03-26 21:33:58,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:58,770] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:58,824] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-03-26 21:33:58,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:58,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:58,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4179528e-12 0.0000000e+00], sum to 1.0000
[2019-03-26 21:33:58,929] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3310
[2019-03-26 21:33:58,934] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 86.0, 1.0, 2.0, 0.3213215530535128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512655.718011903, 512655.718011903, 168040.2722682601], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 21600.0000, 
sim time next is 22200.0000, 
raw observation next is [21.66666666666667, 85.66666666666667, 1.0, 2.0, 0.3690137845416782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 588413.6889443405, 588413.6889443405, 174182.9217423461], 
processed observation next is [1.0, 0.2608695652173913, 0.22590837282780438, 0.8566666666666667, 1.0, 1.0, 0.23977564402611834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16344824692898346, 0.16344824692898346, 0.25997451006320316], 
reward next is 0.7400, 
noisyNet noise sample is [array([-0.09853556], dtype=float32), 0.7793431]. 
=============================================
[2019-03-26 21:33:58,970] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-03-26 21:33:59,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:59,100] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:59,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-03-26 21:33:59,173] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 9.999995e-01 0.000000e+00 4.982102e-07 0.000000e+00], sum to 1.0000
[2019-03-26 21:33:59,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0358
[2019-03-26 21:33:59,182] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 85.0, 1.0, 2.0, 0.2931794131352752, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473644.8918155906, 473644.89181559, 165207.2081659699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 10800.0000, 
sim time next is 11400.0000, 
raw observation next is [21.03333333333333, 85.00000000000001, 1.0, 2.0, 0.3447860764646196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557446.5383482359, 557446.5383482359, 171504.5631084876], 
processed observation next is [1.0, 0.13043478260869565, 0.19589257503949445, 0.8500000000000001, 1.0, 1.0, 0.21058563429472243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15484626065228774, 0.15484626065228774, 0.25597695986341434], 
reward next is 0.7440, 
noisyNet noise sample is [array([0.23457403], dtype=float32), 0.072611295]. 
=============================================
[2019-03-26 21:33:59,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:59,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:59,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-03-26 21:33:59,664] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:33:59,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:33:59,686] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-03-26 21:34:00,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:34:00,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-03-26 21:34:00,643] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 21:34:00,644] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:34:00,645] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:34:00,646] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:34:00,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:34:00,645] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:34:00,648] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,649] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,649] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,648] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,649] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:34:00,668] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-03-26 21:34:00,684] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-03-26 21:34:00,700] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-03-26 21:34:00,700] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-03-26 21:34:00,729] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-03-26 21:35:54,397] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8001.2777 3008137870.5046 1762.0000
[2019-03-26 21:35:54,792] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7882.0313 3164530702.2237 1767.0000
[2019-03-26 21:35:54,879] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.5342 2927783479.9905 1326.0000
[2019-03-26 21:35:54,896] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7694 2843141528.0607 1123.0000
[2019-03-26 21:35:55,064] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.4458 2779631480.9208 920.0000
[2019-03-26 21:35:56,080] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1275000, evaluation results [1275000.0, 7882.031316481575, 3164530702.223711, 1767.0, 8259.534153343897, 2927783479.9905014, 1326.0, 8662.445823130061, 2779631480.920754, 920.0, 8001.277655921997, 3008137870.504635, 1762.0, 8496.769448760873, 2843141528.0607495, 1123.0]
[2019-03-26 21:36:10,934] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0445169e-11 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:10,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0250
[2019-03-26 21:36:10,952] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 68.5, 1.0, 2.0, 0.4605003215267129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755739.554287621, 755739.5542876205, 188959.1340844114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 469800.0000, 
sim time next is 470400.0000, 
raw observation next is [22.43333333333333, 67.66666666666667, 1.0, 2.0, 0.4671205029577186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766549.5944158662, 766549.5944158655, 190072.9993363132], 
processed observation next is [1.0, 0.43478260869565216, 0.2622432859399683, 0.6766666666666667, 1.0, 1.0, 0.35797650958761285, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21293044289329616, 0.21293044289329596, 0.2836910437855421], 
reward next is 0.7163, 
noisyNet noise sample is [array([-0.05449015], dtype=float32), 1.178726]. 
=============================================
[2019-03-26 21:36:17,164] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6272523e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:17,174] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3042
[2019-03-26 21:36:17,178] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 89.5, 1.0, 2.0, 0.2628025625747001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 427839.1772719586, 427839.1772719593, 162122.1209462835], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 364200.0000, 
sim time next is 364800.0000, 
raw observation next is [20.1, 89.0, 1.0, 2.0, 0.2577437023177542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 419668.4362613151, 419668.4362613158, 161610.9908548877], 
processed observation next is [1.0, 0.21739130434782608, 0.15165876777251197, 0.89, 1.0, 1.0, 0.10571530399729417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11657456562814307, 0.11657456562814326, 0.24121043411177268], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.37283462], dtype=float32), -0.45102882]. 
=============================================
[2019-03-26 21:36:18,681] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4150536e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:18,691] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6349
[2019-03-26 21:36:18,707] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 66.33333333333334, 1.0, 2.0, 0.2401018453830852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395993.0779871152, 395993.0779871159, 159918.0543452117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 499200.0000, 
sim time next is 499800.0000, 
raw observation next is [21.95, 67.66666666666666, 1.0, 2.0, 0.2407609549877426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397262.7553255308, 397262.7553255308, 159973.0869926063], 
processed observation next is [1.0, 0.782608695652174, 0.2393364928909953, 0.6766666666666665, 1.0, 1.0, 0.08525416263583446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11035076536820299, 0.11035076536820299, 0.23876580148150192], 
reward next is 0.7612, 
noisyNet noise sample is [array([1.471173], dtype=float32), -1.0542854]. 
=============================================
[2019-03-26 21:36:21,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.526346e-13 0.000000e+00], sum to 1.0000
[2019-03-26 21:36:21,706] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0903
[2019-03-26 21:36:21,710] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 75.66666666666666, 1.0, 2.0, 0.2543079736086176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 418575.0686870357, 418575.0686870363, 161327.9664394889], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 718800.0000, 
sim time next is 719400.0000, 
raw observation next is [21.28333333333333, 74.83333333333334, 1.0, 2.0, 0.2638325361547078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433725.1113663855, 433725.1113663848, 162303.5312655763], 
processed observation next is [1.0, 0.30434782608695654, 0.20774091627172192, 0.7483333333333334, 1.0, 1.0, 0.11305124837916602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12047919760177374, 0.12047919760177354, 0.24224407651578553], 
reward next is 0.7578, 
noisyNet noise sample is [array([-0.22724642], dtype=float32), 0.05751031]. 
=============================================
[2019-03-26 21:36:25,090] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.574671e-13 0.000000e+00], sum to 1.0000
[2019-03-26 21:36:25,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3448
[2019-03-26 21:36:25,106] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 53.5, 1.0, 2.0, 0.5747622264378489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944144.329170262, 944144.329170262, 210297.642093279], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 491400.0000, 
sim time next is 492000.0000, 
raw observation next is [24.7, 53.66666666666667, 1.0, 2.0, 0.5577225204564762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916337.0853982749, 916337.0853982749, 206833.5854249532], 
processed observation next is [1.0, 0.6956521739130435, 0.3696682464454976, 0.5366666666666667, 1.0, 1.0, 0.46713556681503154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2545380792772986, 0.2545380792772986, 0.3087068439178406], 
reward next is 0.6913, 
noisyNet noise sample is [array([0.66004753], dtype=float32), 1.4387766]. 
=============================================
[2019-03-26 21:36:25,126] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.15876 ]
 [72.034874]
 [71.90964 ]
 [72.01181 ]
 [71.73149 ]], R is [[72.42744446]
 [72.38929749]
 [72.35293579]
 [72.3166275 ]
 [72.28670502]].
[2019-03-26 21:36:25,958] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2094332e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:25,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5823
[2019-03-26 21:36:25,975] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 55.33333333333333, 1.0, 2.0, 0.3501702907954479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 573934.1410301038, 573934.1410301044, 172526.7757412604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 564000.0000, 
sim time next is 564600.0000, 
raw observation next is [24.5, 55.66666666666667, 1.0, 2.0, 0.3498141191536414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573622.1155521977, 573622.1155521977, 172483.1554737045], 
processed observation next is [1.0, 0.5217391304347826, 0.3601895734597157, 0.5566666666666668, 1.0, 1.0, 0.21664351705258, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15933947654227712, 0.15933947654227712, 0.2574375454831411], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.07394158], dtype=float32), -0.6209243]. 
=============================================
[2019-03-26 21:36:34,955] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.936214e-17 0.000000e+00], sum to 1.0000
[2019-03-26 21:36:34,962] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1533
[2019-03-26 21:36:34,965] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 54.5, 1.0, 2.0, 0.4705730174011961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766204.2374903259, 766204.2374903265, 190516.0469403185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 735000.0000, 
sim time next is 735600.0000, 
raw observation next is [25.33333333333334, 54.0, 1.0, 2.0, 0.3495018895137921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569324.169269985, 569324.1692699844, 172337.6181527059], 
processed observation next is [1.0, 0.5217391304347826, 0.3996840442338076, 0.54, 1.0, 1.0, 0.2162673367636049, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15814560257499585, 0.15814560257499566, 0.25722032560105357], 
reward next is 0.7428, 
noisyNet noise sample is [array([-0.37623528], dtype=float32), 1.7547215]. 
=============================================
[2019-03-26 21:36:38,524] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3446097e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:38,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2198
[2019-03-26 21:36:38,537] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 70.0, 1.0, 2.0, 0.4933154680641298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 808044.4208022965, 808044.4208022971, 194610.3667792293], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 722400.0000, 
sim time next is 723000.0000, 
raw observation next is [22.41666666666667, 69.0, 1.0, 2.0, 0.5024554194723655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 822644.3336216464, 822644.3336216458, 196248.3106412334], 
processed observation next is [1.0, 0.34782608695652173, 0.26145339652448685, 0.69, 1.0, 1.0, 0.40054869815947647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22851231489490179, 0.22851231489490162, 0.29290792633019913], 
reward next is 0.7071, 
noisyNet noise sample is [array([-1.3279731], dtype=float32), -0.12715039]. 
=============================================
[2019-03-26 21:36:38,555] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.30356 ]
 [69.664795]
 [70.64778 ]
 [72.26096 ]
 [72.4062  ]], R is [[69.06280518]
 [69.08171082]
 [69.10282135]
 [69.13974762]
 [69.20504761]].
[2019-03-26 21:36:39,285] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9346485e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:39,292] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1432
[2019-03-26 21:36:39,296] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.15, 88.5, 1.0, 2.0, 0.2980085863276811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476556.6058885139, 476556.6058885139, 165402.6710470118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1121400.0000, 
sim time next is 1122000.0000, 
raw observation next is [21.1, 88.66666666666666, 1.0, 2.0, 0.2974904764655094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476011.9620746989, 476011.9620746995, 165366.9566344492], 
processed observation next is [1.0, 1.0, 0.1990521327014219, 0.8866666666666666, 1.0, 1.0, 0.15360298369338482, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13222554502074968, 0.13222554502074987, 0.24681635318574505], 
reward next is 0.7532, 
noisyNet noise sample is [array([0.36418533], dtype=float32), 0.91731477]. 
=============================================
[2019-03-26 21:36:39,309] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[76.512955]
 [76.52378 ]
 [76.54843 ]
 [76.58448 ]
 [76.49646 ]], R is [[76.49562073]
 [76.48379517]
 [76.47202301]
 [76.46026611]
 [76.44850922]].
[2019-03-26 21:36:39,369] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1223657e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:39,381] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9410
[2019-03-26 21:36:39,385] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 85.66666666666667, 1.0, 2.0, 0.3034191572796478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482038.306052571, 482038.306052571, 165749.320943273], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 856200.0000, 
sim time next is 856800.0000, 
raw observation next is [21.8, 86.0, 1.0, 2.0, 0.3047671035984224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484073.4764388891, 484073.4764388897, 165894.3172226585], 
processed observation next is [0.0, 0.9565217391304348, 0.23222748815165886, 0.86, 1.0, 1.0, 0.16237000433544868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1344648545663581, 0.13446485456635823, 0.24760345854128132], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.01164912], dtype=float32), 0.34914762]. 
=============================================
[2019-03-26 21:36:46,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9128682e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:36:46,488] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9169
[2019-03-26 21:36:46,494] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 71.33333333333333, 1.0, 2.0, 0.3014877156297535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478693.274271819, 478693.2742718183, 165503.9728785646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 909600.0000, 
sim time next is 910200.0000, 
raw observation next is [24.05, 70.66666666666667, 1.0, 2.0, 0.3027321731922025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 480197.6300833625, 480197.6300833632, 165602.6905125778], 
processed observation next is [0.0, 0.5217391304347826, 0.3388625592417062, 0.7066666666666667, 1.0, 1.0, 0.15991828095446087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1333882305787118, 0.133388230578712, 0.24716819479489224], 
reward next is 0.7528, 
noisyNet noise sample is [array([0.04485014], dtype=float32), -1.0893244]. 
=============================================
[2019-03-26 21:36:51,853] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 21:36:51,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:36:51,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:36:51,855] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:36:51,855] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:36:51,856] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:36:51,857] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:36:51,858] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:36:51,858] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:36:51,859] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:36:51,860] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:36:51,883] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-03-26 21:36:51,907] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-03-26 21:36:51,907] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-03-26 21:36:51,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-03-26 21:36:51,908] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-03-26 21:37:21,372] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6396176]
[2019-03-26 21:37:21,373] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.497127425, 95.29532252333334, 1.0, 2.0, 0.5654850278136649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790211.5044638643, 790211.5044638643, 194493.5522085142]
[2019-03-26 21:37:21,375] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:37:21,377] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.964605e-17 0.000000e+00], sampled 0.2247153400163029
[2019-03-26 21:38:09,661] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6396176]
[2019-03-26 21:38:09,663] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.562311796246484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 785775.5751789607, 785775.5751789613, 193934.2017467929]
[2019-03-26 21:38:09,664] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:38:09,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5550576e-16 0.0000000e+00], sampled 0.7669904825196148
[2019-03-26 21:38:12,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6396176]
[2019-03-26 21:38:12,806] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.80962164, 88.54499237499999, 1.0, 2.0, 0.5447811972893444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761269.5042510491, 761269.5042510486, 190908.473977865]
[2019-03-26 21:38:12,808] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:38:12,810] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.8006314e-17 0.0000000e+00], sampled 0.6566714261169369
[2019-03-26 21:38:17,592] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6396176]
[2019-03-26 21:38:17,594] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.99455262666667, 79.23724486333333, 1.0, 2.0, 0.5543060822525215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774584.2961426751, 774584.2961426751, 192542.0570547158]
[2019-03-26 21:38:17,598] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:38:17,601] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3315854e-18 0.0000000e+00], sampled 0.038173359840051546
[2019-03-26 21:38:26,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6396176]
[2019-03-26 21:38:26,640] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.17526343666667, 77.58337177, 1.0, 2.0, 0.4801941772010698, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 672823.6151070055, 672823.6151070055, 180788.1314883225]
[2019-03-26 21:38:26,643] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:38:26,646] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.979768e-17 0.000000e+00], sampled 0.14294066989801324
[2019-03-26 21:38:45,626] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.8874 2927913832.0983 1326.0000
[2019-03-26 21:38:46,080] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.5625 2779890177.5499 921.0000
[2019-03-26 21:38:46,238] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3168 3008746041.1940 1766.0000
[2019-03-26 21:38:46,380] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.0035 2843576590.7546 1130.0000
[2019-03-26 21:38:46,522] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7873.5655 3164861001.5350 1775.0000
[2019-03-26 21:38:47,539] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1300000, evaluation results [1300000.0, 7873.565463704328, 3164861001.534994, 1775.0, 8259.88738887422, 2927913832.0983257, 1326.0, 8658.562467665353, 2779890177.5498824, 921.0, 7997.316847757386, 3008746041.194046, 1766.0, 8492.00353360562, 2843576590.754619, 1130.0]
[2019-03-26 21:38:49,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7540682e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:38:49,526] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2051
[2019-03-26 21:38:49,530] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3350384879186246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519354.9549688383, 519354.9549688377, 168252.030569168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 978000.0000, 
sim time next is 978600.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3329765268366439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 516161.5966468404, 516161.5966468398, 168000.7902355249], 
processed observation next is [1.0, 0.30434782608695654, 0.23696682464454974, 0.93, 1.0, 1.0, 0.19635726124896857, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.143378221290789, 0.14337822129078884, 0.25074744811272376], 
reward next is 0.7493, 
noisyNet noise sample is [array([2.6555288], dtype=float32), -0.7712288]. 
=============================================
[2019-03-26 21:39:02,522] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 9.9999309e-01 0.0000000e+00 6.9381567e-06 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:02,532] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8655
[2019-03-26 21:39:02,538] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 66.16666666666667, 1.0, 2.0, 0.3445898396661412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 532736.5550255472, 532736.5550255479, 169280.1545796951], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1191000.0000, 
sim time next is 1191600.0000, 
raw observation next is [25.7, 67.0, 1.0, 2.0, 0.3443976029830777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532773.1660277712, 532773.1660277712, 169292.8305185069], 
processed observation next is [1.0, 0.8260869565217391, 0.4170616113744076, 0.67, 1.0, 1.0, 0.2101175939555153, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14799254611882534, 0.14799254611882534, 0.2526758664455327], 
reward next is 0.7473, 
noisyNet noise sample is [array([0.8960269], dtype=float32), -0.84168965]. 
=============================================
[2019-03-26 21:39:07,862] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9579712e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:07,873] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8812
[2019-03-26 21:39:07,878] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.0, 1.0, 2.0, 0.8618038511760767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1204523.687623024, 1204523.687623024, 259643.2187026259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1677600.0000, 
sim time next is 1678200.0000, 
raw observation next is [25.31666666666667, 90.50000000000001, 1.0, 2.0, 0.9560773333030395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1336370.52086463, 1336370.52086463, 285825.2544364419], 
processed observation next is [1.0, 0.43478260869565216, 0.39889415481832563, 0.9050000000000001, 1.0, 1.0, 0.9470811244614933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3712140335735083, 0.3712140335735083, 0.42660485736782366], 
reward next is 0.5734, 
noisyNet noise sample is [array([-0.305452], dtype=float32), -0.41780967]. 
=============================================
[2019-03-26 21:39:08,010] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.821445e-13 0.000000e+00], sum to 1.0000
[2019-03-26 21:39:08,021] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6059
[2019-03-26 21:39:08,025] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4594574191080348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651614.1455646659, 651614.1455646659, 178726.2787576435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1297800.0000, 
sim time next is 1298400.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4596510825608306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 651891.214867351, 651891.2148673504, 178755.0599349044], 
processed observation next is [1.0, 0.0, 0.3507109004739337, 0.94, 1.0, 1.0, 0.3489772079046152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1810808930187086, 0.18108089301870844, 0.26679859691776775], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.6850911], dtype=float32), -0.23709205]. 
=============================================
[2019-03-26 21:39:08,716] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.458851e-19 0.000000e+00], sum to 1.0000
[2019-03-26 21:39:08,725] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0039
[2019-03-26 21:39:08,729] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.45971656316523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 651988.9023952944, 651988.902395295, 178765.3062099393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1299600.0000, 
sim time next is 1300200.0000, 
raw observation next is [24.28333333333333, 94.00000000000001, 1.0, 2.0, 0.4590338509617207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651345.2800327229, 651345.2800327229, 178706.4688538504], 
processed observation next is [1.0, 0.043478260869565216, 0.34992101105845175, 0.9400000000000002, 1.0, 1.0, 0.34823355537556716, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18092924445353412, 0.18092924445353412, 0.26672607291619466], 
reward next is 0.7333, 
noisyNet noise sample is [array([-1.3923157], dtype=float32), 1.1576291]. 
=============================================
[2019-03-26 21:39:10,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9079314e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:10,481] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9879
[2019-03-26 21:39:10,484] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 93.0, 1.0, 2.0, 0.3300264527944478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 515928.093286766, 515928.0932867654, 168107.1509076998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1407600.0000, 
sim time next is 1408200.0000, 
raw observation next is [21.68333333333334, 92.5, 1.0, 2.0, 0.3308154636927673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516777.1966169412, 516777.1966169412, 168163.2932813919], 
processed observation next is [0.0, 0.30434782608695654, 0.22669826224328635, 0.925, 1.0, 1.0, 0.19375357071417748, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14354922128248365, 0.14354922128248365, 0.2509899899722267], 
reward next is 0.7490, 
noisyNet noise sample is [array([-1.4068879], dtype=float32), 0.042039726]. 
=============================================
[2019-03-26 21:39:12,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.240673e-17 0.000000e+00], sum to 1.0000
[2019-03-26 21:39:12,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5460
[2019-03-26 21:39:12,479] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 91.0, 1.0, 2.0, 0.7091606161431733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1130087.578516714, 1130087.578516714, 239288.0164668727], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1353600.0000, 
sim time next is 1354200.0000, 
raw observation next is [20.96666666666667, 91.16666666666667, 1.0, 2.0, 0.6389362483018863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1018434.12291141, 1018434.122911409, 222670.5488732789], 
processed observation next is [1.0, 0.6956521739130435, 0.1927330173775673, 0.9116666666666667, 1.0, 1.0, 0.5649834316890197, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2828983674753917, 0.2828983674753914, 0.3323441027959386], 
reward next is 0.6677, 
noisyNet noise sample is [array([-0.12153134], dtype=float32), -0.4338689]. 
=============================================
[2019-03-26 21:39:18,157] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4104714e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:18,169] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7458
[2019-03-26 21:39:18,175] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.75, 88.0, 1.0, 2.0, 0.3937778143103614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587622.5538527947, 587622.5538527947, 173401.3058275043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1452600.0000, 
sim time next is 1453200.0000, 
raw observation next is [23.5, 89.33333333333333, 1.0, 2.0, 0.3907755687023597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584580.0612230237, 584580.0612230243, 173168.24205628], 
processed observation next is [0.0, 0.8260869565217391, 0.31279620853080575, 0.8933333333333333, 1.0, 1.0, 0.26599466108718034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1623833503397288, 0.16238335033972898, 0.25846006277056716], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.3536689], dtype=float32), 0.9158268]. 
=============================================
[2019-03-26 21:39:18,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0754201e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:18,992] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5007
[2019-03-26 21:39:18,998] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 85.0, 1.0, 2.0, 0.8248013986985608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1246279.625636021, 1246279.625636021, 262823.3009326591], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1596600.0000, 
sim time next is 1597200.0000, 
raw observation next is [23.83333333333333, 85.0, 1.0, 2.0, 0.7987309430605135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1205591.685212153, 1205591.685212153, 255609.3047495016], 
processed observation next is [1.0, 0.4782608695652174, 0.32859399684044216, 0.85, 1.0, 1.0, 0.7575071603138717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33488657922559806, 0.33488657922559806, 0.38150642499925613], 
reward next is 0.6185, 
noisyNet noise sample is [array([-0.9312642], dtype=float32), -0.6071147]. 
=============================================
[2019-03-26 21:39:27,393] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5988613e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:27,401] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7532
[2019-03-26 21:39:27,407] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.5, 1.0, 2.0, 0.4116340556942512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607754.4664720419, 607754.4664720419, 175073.8917299772], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1621800.0000, 
sim time next is 1622400.0000, 
raw observation next is [23.1, 95.33333333333333, 1.0, 2.0, 0.4118243485850188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608498.4812234347, 608498.4812234347, 175156.9013538017], 
processed observation next is [1.0, 0.782608695652174, 0.2938388625592418, 0.9533333333333333, 1.0, 1.0, 0.29135463684942026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16902735589539852, 0.16902735589539852, 0.2614282109758234], 
reward next is 0.7386, 
noisyNet noise sample is [array([-1.4083227], dtype=float32), 0.7046456]. 
=============================================
[2019-03-26 21:39:29,824] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6045448e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:29,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5590
[2019-03-26 21:39:29,845] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 98.0, 1.0, 2.0, 0.4341939420624599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619786.8178648998, 619786.817864899, 175622.3809237987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1668600.0000, 
sim time next is 1669200.0000, 
raw observation next is [23.66666666666667, 98.0, 1.0, 2.0, 0.4493481692295083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 177726.5250660207], 
processed observation next is [1.0, 0.30434782608695654, 0.3206951026856243, 0.98, 1.0, 1.0, 0.33656405931266065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1780447300067447, 0.1780447300067447, 0.26526347024779207], 
reward next is 0.7347, 
noisyNet noise sample is [array([-2.2879906], dtype=float32), -0.7565402]. 
=============================================
[2019-03-26 21:39:32,267] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4089815e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 21:39:32,276] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3597
[2019-03-26 21:39:32,280] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 81.33333333333334, 1.0, 2.0, 0.5090368748779744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711304.1724018042, 711304.1724018042, 185024.4410137997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1709400.0000, 
sim time next is 1710000.0000, 
raw observation next is [27.4, 82.0, 1.0, 2.0, 0.5063752165239351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707583.6575528891, 707583.6575528898, 184601.4270496964], 
processed observation next is [1.0, 0.8260869565217391, 0.4976303317535545, 0.82, 1.0, 1.0, 0.4052713452095604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19655101598691363, 0.19655101598691382, 0.27552451798462146], 
reward next is 0.7245, 
noisyNet noise sample is [array([1.7569869], dtype=float32), 0.42720354]. 
=============================================
[2019-03-26 21:39:32,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.63249 ]
 [67.70497 ]
 [67.228294]
 [67.20331 ]
 [67.18379 ]], R is [[67.38738251]
 [67.43735504]
 [67.48653412]
 [67.53553772]
 [67.58470154]].
[2019-03-26 21:39:37,154] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.638729e-22 0.000000e+00], sum to 1.0000
[2019-03-26 21:39:37,166] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0742
[2019-03-26 21:39:37,175] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.81666666666667, 84.83333333333334, 1.0, 2.0, 0.5917256669463309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 919469.1522564429, 919469.1522564436, 210267.1149630368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1775400.0000, 
sim time next is 1776000.0000, 
raw observation next is [22.73333333333333, 84.66666666666667, 1.0, 2.0, 0.681857240381397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1062414.05920471, 1062414.059204711, 230387.9684804505], 
processed observation next is [1.0, 0.5652173913043478, 0.27646129541864134, 0.8466666666666667, 1.0, 1.0, 0.6166954703390325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2951150164457528, 0.2951150164457531, 0.34386263952306045], 
reward next is 0.6561, 
noisyNet noise sample is [array([-0.92642665], dtype=float32), -0.42316097]. 
=============================================
[2019-03-26 21:39:37,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.794266]
 [74.09903 ]
 [74.109665]
 [73.75336 ]
 [72.77975 ]], R is [[72.71838379]
 [72.67736816]
 [72.64577484]
 [72.61911774]
 [72.58409119]].
[2019-03-26 21:39:43,070] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 21:39:43,072] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:39:43,072] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:39:43,073] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:39:43,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:39:43,075] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:39:43,076] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:39:43,077] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:39:43,079] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:39:43,079] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:39:43,078] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:39:43,118] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-03-26 21:39:43,138] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-03-26 21:39:43,158] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-03-26 21:39:43,183] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-03-26 21:39:43,201] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-03-26 21:39:49,534] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:39:49,535] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.7, 72.5, 1.0, 2.0, 0.2802647146316176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 464807.7473827199, 464807.7473827193, 163857.4224244074]
[2019-03-26 21:39:49,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:39:49,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.07786166e-20
 0.00000000e+00], sampled 0.9419174965300672
[2019-03-26 21:39:51,296] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:39:51,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.99102785, 65.48239886, 1.0, 2.0, 0.390969628344003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631727.0848467097, 631727.0848467092, 177801.9036843827]
[2019-03-26 21:39:51,299] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:39:51,302] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.893807e-21 0.000000e+00], sampled 0.7981346302135133
[2019-03-26 21:40:12,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:40:12,341] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 91.0, 1.0, 2.0, 0.4368810511432824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635064.0412796895, 635064.0412796895, 177445.3820241003]
[2019-03-26 21:40:12,343] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:40:12,345] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1541733e-20 0.0000000e+00], sampled 0.8206902440548313
[2019-03-26 21:40:26,842] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:40:26,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.01666666666667, 88.0, 1.0, 2.0, 0.5069349408214806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708366.0493565529, 708366.0493565535, 184689.0562831664]
[2019-03-26 21:40:26,845] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:40:26,847] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1656028e-20 0.0000000e+00], sampled 0.5824980753398313
[2019-03-26 21:40:36,704] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:40:36,704] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.12637537, 72.48831587000001, 1.0, 2.0, 0.6551297707415286, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 915535.4914356638, 915535.4914356638, 211454.9515227001]
[2019-03-26 21:40:36,704] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:40:36,707] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2735543e-19 0.0000000e+00], sampled 0.8322291175011618
[2019-03-26 21:41:14,623] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:41:14,625] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.58373387, 83.13345644, 1.0, 2.0, 0.9048573363983716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1264734.407118836, 1264734.407118836, 271279.2633398755]
[2019-03-26 21:41:14,626] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:41:14,628] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5890423e-19 0.0000000e+00], sampled 0.09016084357168608
[2019-03-26 21:41:15,464] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:41:15,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.83333333333334, 68.66666666666667, 1.0, 2.0, 0.8800193565818685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1229997.833088041, 1229997.833088041, 264504.5417203527]
[2019-03-26 21:41:15,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:41:15,469] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7700784e-19 0.0000000e+00], sampled 0.025484212119522787
[2019-03-26 21:41:17,090] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:41:17,092] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.85, 81.5, 1.0, 2.0, 0.7043330272409057, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.98196269140603, 6.9112, 168.9119525258916, 1881192.256147194, 1830991.065787399, 386456.0622412128]
[2019-03-26 21:41:17,095] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:41:17,100] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6612523e-16 0.0000000e+00], sampled 0.03939080558574293
[2019-03-26 21:41:17,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1881192.256147194 W.
[2019-03-26 21:41:32,010] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:41:32,010] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.6, 73.5, 1.0, 2.0, 0.5472840287919637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764768.1844707952, 764768.1844707945, 191335.7299789043]
[2019-03-26 21:41:32,012] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:41:32,015] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.768426e-21 0.000000e+00], sampled 0.8259417949181447
[2019-03-26 21:41:32,221] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65107745]
[2019-03-26 21:41:32,222] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 80.5, 1.0, 2.0, 0.5497549925830485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768222.3284771323, 768222.328477133, 191758.3811321502]
[2019-03-26 21:41:32,223] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:41:32,226] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8100224e-21 0.0000000e+00], sampled 0.03833758549142385
[2019-03-26 21:41:36,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.2616 2843368869.3792 1131.0000
[2019-03-26 21:41:37,156] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 21:41:37,193] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7003 2928231904.9171 1338.0000
[2019-03-26 21:41:37,381] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.1120 2780070553.5627 933.0000
[2019-03-26 21:41:37,441] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164933419.7733 1778.0000
[2019-03-26 21:41:38,460] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 1325000, evaluation results [1325000.0, 7874.254934002473, 3164933419.7732573, 1778.0, 8253.700304448676, 2928231904.9170933, 1338.0, 8658.112016599982, 2780070553.5627275, 933.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8494.261612308823, 2843368869.379151, 1131.0]
[2019-03-26 21:41:39,059] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0267063e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:41:39,068] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4634
[2019-03-26 21:41:39,077] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 97.66666666666667, 1.0, 2.0, 0.4643511869416615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655091.546806363, 655091.546806363, 179005.5196459925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [23.9, 98.0, 1.0, 2.0, 0.4633728702384586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653961.8516538395, 653961.8516538395, 178893.4476831057], 
processed observation next is [0.0, 0.21739130434782608, 0.33175355450236965, 0.98, 1.0, 1.0, 0.353461289443926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1816560699038443, 0.1816560699038443, 0.2670051457956802], 
reward next is 0.7330, 
noisyNet noise sample is [array([0.6423441], dtype=float32), -0.94139475]. 
=============================================
[2019-03-26 21:41:43,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4424263e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:41:43,398] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4194
[2019-03-26 21:41:43,402] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 97.0, 1.0, 2.0, 0.460795891596776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651176.1322495863, 651176.1322495863, 178624.0099571674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2088000.0000, 
sim time next is 2088600.0000, 
raw observation next is [23.96666666666667, 97.16666666666667, 1.0, 2.0, 0.4599709408056562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650217.9598427864, 650217.9598427871, 178529.635563173], 
processed observation next is [0.0, 0.17391304347826086, 0.33491311216429714, 0.9716666666666667, 1.0, 1.0, 0.34936257928392317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18061609995632957, 0.18061609995632977, 0.2664621426316015], 
reward next is 0.7335, 
noisyNet noise sample is [array([1.3967222], dtype=float32), -0.32963207]. 
=============================================
[2019-03-26 21:41:45,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.17858795e-17
 0.00000000e+00], sum to 1.0000
[2019-03-26 21:41:45,327] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6598
[2019-03-26 21:41:45,332] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 94.83333333333334, 1.0, 2.0, 0.4649283893598679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654700.6073903277, 654700.607390327, 178935.8505071668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1991400.0000, 
sim time next is 1992000.0000, 
raw observation next is [24.43333333333334, 94.66666666666667, 1.0, 2.0, 0.4677261088709502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657310.2762225271, 657310.2762225271, 179178.3553379314], 
processed observation next is [0.0, 0.043478260869565216, 0.3570300157977887, 0.9466666666666668, 1.0, 1.0, 0.3587061552662051, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18258618783959088, 0.18258618783959088, 0.26743038110139017], 
reward next is 0.7326, 
noisyNet noise sample is [array([1.0856591], dtype=float32), -0.4397987]. 
=============================================
[2019-03-26 21:41:45,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.89022 ]
 [74.15132 ]
 [74.30622 ]
 [74.513725]
 [74.71558 ]], R is [[73.76070404]
 [73.75602722]
 [73.75163269]
 [73.7472229 ]
 [73.74279785]].
[2019-03-26 21:41:45,902] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.003027e-15 0.000000e+00], sum to 1.0000
[2019-03-26 21:41:45,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2114
[2019-03-26 21:41:45,921] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.23333333333333, 95.83333333333333, 1.0, 2.0, 0.4650883855131099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654539.6954840053, 654539.6954840046, 178909.694666516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2001000.0000, 
sim time next is 2001600.0000, 
raw observation next is [24.2, 96.0, 1.0, 2.0, 0.4643381237217728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653842.1613753759, 653842.1613753765, 178845.3110665021], 
processed observation next is [0.0, 0.17391304347826086, 0.3459715639810427, 0.96, 1.0, 1.0, 0.3546242454479191, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1816228226042711, 0.18162282260427126, 0.26693330009925686], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.39274707], dtype=float32), 1.2834053]. 
=============================================
[2019-03-26 21:41:47,028] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0597898e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:41:47,040] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8137
[2019-03-26 21:41:47,048] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 84.66666666666667, 1.0, 2.0, 0.5167793546206902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722126.8266303921, 722126.8266303921, 186266.9261211203], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2035200.0000, 
sim time next is 2035800.0000, 
raw observation next is [27.25, 84.0, 1.0, 2.0, 0.5168110363721877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722171.1124931996, 722171.1124931996, 186272.0921017974], 
processed observation next is [0.0, 0.5652173913043478, 0.490521327014218, 0.84, 1.0, 1.0, 0.41784462213516593, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20060308680366656, 0.20060308680366656, 0.27801804791313045], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.5063585], dtype=float32), -1.0797632]. 
=============================================
[2019-03-26 21:41:48,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.903475e-17 0.000000e+00], sum to 1.0000
[2019-03-26 21:41:48,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4661
[2019-03-26 21:41:48,603] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.66666666666667, 88.33333333333334, 1.0, 2.0, 0.4833796441520721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675440.5674690019, 675440.5674690013, 181032.8963085709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2056800.0000, 
sim time next is 2057400.0000, 
raw observation next is [25.6, 88.5, 1.0, 2.0, 0.4819775380918156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673480.742453085, 673480.7424530843, 180820.3905261952], 
processed observation next is [0.0, 0.8260869565217391, 0.4123222748815167, 0.885, 1.0, 1.0, 0.37587655191785013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18707798401474585, 0.18707798401474565, 0.26988117988984356], 
reward next is 0.7301, 
noisyNet noise sample is [array([-0.03077981], dtype=float32), -0.3979471]. 
=============================================
[2019-03-26 21:41:53,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.8156769e-22 9.9979538e-01 7.2918087e-25 2.0463018e-04 6.1869671e-25], sum to 1.0000
[2019-03-26 21:41:53,515] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3766
[2019-03-26 21:41:53,521] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2618762.901756771 W.
[2019-03-26 21:41:53,525] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.85, 67.5, 1.0, 2.0, 0.6241597517583264, 1.0, 2.0, 0.6241597517583264, 1.0, 2.0, 1.03, 6.971860631923112, 6.9112, 170.5573041426782, 2618762.901756771, 2575309.223593502, 496904.9184445538], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2219400.0000, 
sim time next is 2220000.0000, 
raw observation next is [31.83333333333334, 67.66666666666666, 1.0, 2.0, 0.7688585562358726, 1.0, 2.0, 0.7688585562358726, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2150168.304397942, 2150168.304397942, 405001.2425113079], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.6766666666666665, 1.0, 1.0, 0.7215163328143044, 1.0, 1.0, 0.7215163328143044, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5972689734438728, 0.5972689734438728, 0.6044794664347879], 
reward next is 0.3955, 
noisyNet noise sample is [array([0.719394], dtype=float32), 0.9293389]. 
=============================================
[2019-03-26 21:41:53,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[35.37033 ]
 [35.380104]
 [36.605324]
 [41.528313]
 [39.00562 ]], R is [[40.49794388]
 [40.09296417]
 [39.69203568]
 [39.29511642]
 [39.21186066]].
[2019-03-26 21:41:58,374] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5158111e-30 1.0000000e+00 4.5506690e-37 1.8613522e-12 3.3324520e-38], sum to 1.0000
[2019-03-26 21:41:58,385] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8375
[2019-03-26 21:41:58,390] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.4, 71.33333333333333, 1.0, 2.0, 0.5878398519148471, 1.0, 1.0, 0.5878398519148471, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1643548.187635473, 1643548.187635473, 329800.8884522849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2277600.0000, 
sim time next is 2278200.0000, 
raw observation next is [29.6, 70.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.372214334334837, 6.9112, 168.9109048682609, 1781034.392844473, 1453978.929691581, 311349.8092421904], 
processed observation next is [1.0, 0.34782608695652173, 0.6018957345971565, 0.7066666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04610143343348368, 0.0, 0.8294298706500016, 0.4947317757901314, 0.4038830360254392, 0.46470120782416474], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9739848], dtype=float32), 0.96294177]. 
=============================================
[2019-03-26 21:41:59,344] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.715191e-20 0.000000e+00], sum to 1.0000
[2019-03-26 21:41:59,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9923
[2019-03-26 21:41:59,355] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666666, 70.66666666666667, 1.0, 2.0, 0.5678245230785808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793481.9475389589, 793481.9475389589, 194905.5421730983], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2313600.0000, 
sim time next is 2314200.0000, 
raw observation next is [31.03333333333333, 71.33333333333333, 1.0, 2.0, 0.5675287176369502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793068.4326352568, 793068.4326352568, 194853.256522012], 
processed observation next is [1.0, 0.782608695652174, 0.669826224328594, 0.7133333333333333, 1.0, 1.0, 0.47895026221319303, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2202967868431269, 0.2202967868431269, 0.290825756003003], 
reward next is 0.7092, 
noisyNet noise sample is [array([1.808135], dtype=float32), -0.6125775]. 
=============================================
[2019-03-26 21:42:01,404] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.36453610e-21 9.99999881e-01 9.73007270e-25 1.06858344e-07
 2.66434985e-25], sum to 1.0000
[2019-03-26 21:42:01,416] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3630
[2019-03-26 21:42:01,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2456966.920044129 W.
[2019-03-26 21:42:01,430] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.98333333333333, 63.16666666666666, 1.0, 2.0, 0.5856349400737132, 1.0, 2.0, 0.5856349400737132, 1.0, 2.0, 1.017054218473869, 6.911199999999999, 6.9112, 170.5573041426782, 2456966.920044129, 2456966.92004413, 479413.4521574602], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2290200.0000, 
sim time next is 2290800.0000, 
raw observation next is [31.96666666666667, 63.33333333333334, 1.0, 2.0, 0.67381664028585, 1.0, 2.0, 0.67381664028585, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1884143.013920815, 1884143.013920815, 363119.9048570822], 
processed observation next is [1.0, 0.5217391304347826, 0.7140600315955767, 0.6333333333333334, 1.0, 1.0, 0.6070080003443975, 1.0, 1.0, 0.6070080003443975, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5233730594224486, 0.5233730594224486, 0.5419700072493764], 
reward next is 0.4580, 
noisyNet noise sample is [array([0.13955383], dtype=float32), -0.046424057]. 
=============================================
[2019-03-26 21:42:13,077] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6164942e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:13,087] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5645
[2019-03-26 21:42:13,094] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3959674096291794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590841.7069712145, 590841.7069712151, 173694.8769551762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2668800.0000, 
sim time next is 2669400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3958583586890215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590679.0946986069, 590679.0946986069, 173679.9431152444], 
processed observation next is [0.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27211850444460417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1640775263051686, 0.1640775263051686, 0.25922379569439463], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.03194517], dtype=float32), -0.44413766]. 
=============================================
[2019-03-26 21:42:15,477] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9999178e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:15,478] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5415
[2019-03-26 21:42:15,484] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3377006207281725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520218.5628121088, 520218.5628121095, 168216.6743281917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2787600.0000, 
sim time next is 2788200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3381475861693375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520911.9204612995, 520911.9204612989, 168271.9779602908], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20258745321606925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1446977556836943, 0.14469775568369414, 0.2511522059108818], 
reward next is 0.7488, 
noisyNet noise sample is [array([1.4375399], dtype=float32), -1.7357674]. 
=============================================
[2019-03-26 21:42:18,035] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6873714e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:18,041] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1238
[2019-03-26 21:42:18,045] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3170575583172149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 501368.4287978841, 501368.4287978835, 167127.8055310481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2931600.0000, 
sim time next is 2932200.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3177529425547354, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502467.2599172965, 502467.2599172965, 167210.3012784686], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17801559343944023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1395742388659157, 0.1395742388659157, 0.2495676138484606], 
reward next is 0.7504, 
noisyNet noise sample is [array([0.03862556], dtype=float32), -0.076401725]. 
=============================================
[2019-03-26 21:42:21,233] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.713324e-20 0.000000e+00], sum to 1.0000
[2019-03-26 21:42:21,241] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3208
[2019-03-26 21:42:21,246] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4738631162323849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 662292.3425520502, 662292.3425520497, 179621.9109307519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2621400.0000, 
sim time next is 2622000.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4741184900357455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662626.665547584, 662626.665547584, 179657.0114156023], 
processed observation next is [0.0, 0.34782608695652173, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36640781932017535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18406296265210664, 0.18406296265210664, 0.26814479315761536], 
reward next is 0.7319, 
noisyNet noise sample is [array([0.07948502], dtype=float32), -2.1756043]. 
=============================================
[2019-03-26 21:42:21,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[74.94159]
 [74.93739]
 [74.82826]
 [74.80838]
 [74.76258]], R is [[74.95858002]
 [74.94090271]
 [74.92350769]
 [74.90654755]
 [74.89001465]].
[2019-03-26 21:42:24,611] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5698373e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:24,624] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9414
[2019-03-26 21:42:24,633] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.331634081581932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515089.818378539, 515089.818378539, 167947.7659570396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3043800.0000, 
sim time next is 3044400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3314771753901596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514847.1167386528, 514847.1167386528, 167928.8227189743], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19455081372308383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14301308798295911, 0.14301308798295911, 0.25064003390891687], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.46943265], dtype=float32), -0.838521]. 
=============================================
[2019-03-26 21:42:26,383] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.5480915e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:26,393] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4509
[2019-03-26 21:42:26,397] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 83.0, 1.0, 2.0, 0.5879318061340045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909290.8951521824, 909290.8951521824, 209054.5582509945], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2805600.0000, 
sim time next is 2806200.0000, 
raw observation next is [23.5, 83.0, 1.0, 2.0, 0.6100250177025361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939134.7442926532, 939134.7442926537, 213163.2371544995], 
processed observation next is [1.0, 0.4782608695652174, 0.31279620853080575, 0.83, 1.0, 1.0, 0.5301506237379954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26087076230351475, 0.2608707623035149, 0.31815408530522316], 
reward next is 0.6818, 
noisyNet noise sample is [array([-0.20202333], dtype=float32), 2.9799676]. 
=============================================
[2019-03-26 21:42:26,822] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8698595e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:26,831] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4810
[2019-03-26 21:42:26,834] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 86.0, 1.0, 2.0, 0.5656816255115659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846588.7830245778, 846588.7830245785, 201492.78422965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2809800.0000, 
sim time next is 2810400.0000, 
raw observation next is [24.0, 87.0, 1.0, 2.0, 0.5074931472829456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756014.4061528356, 756014.4061528356, 190594.5126949097], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.87, 1.0, 1.0, 0.40661824973848865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.210004001709121, 0.210004001709121, 0.284469421932701], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.02675342], dtype=float32), -0.06889996]. 
=============================================
[2019-03-26 21:42:28,796] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.4702526e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:28,807] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3920
[2019-03-26 21:42:28,813] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [22.66666666666667, 96.0, 1.0, 2.0, 0.3899526964879866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583306.3079049023, 583306.3079049016, 173051.4434765129], 
processed observation next is [0.0, 0.8260869565217391, 0.27330173775671435, 0.96, 1.0, 1.0, 0.26500324878070675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16202952997358397, 0.16202952997358377, 0.25828573653210884], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.77745086], dtype=float32), 0.5003902]. 
=============================================
[2019-03-26 21:42:28,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[83.86092]
 [83.8327 ]
 [83.68839]
 [83.64963]
 [83.5895 ]], R is [[83.81652069]
 [83.71999359]
 [83.6242981 ]
 [83.52934265]
 [83.43523407]].
[2019-03-26 21:42:31,982] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.6118056e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:42:31,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7607
[2019-03-26 21:42:31,994] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 93.16666666666667, 1.0, 2.0, 0.3977676784717663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 591824.6310228049, 591824.6310228043, 173734.0849522283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2839800.0000, 
sim time next is 2840400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3941880045486493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587763.2868035784, 587763.2868035778, 173399.7780090811], 
processed observation next is [1.0, 0.9130434782608695, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2701060295766859, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16326757966766067, 0.1632675796676605, 0.25880563881952406], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.34135723], dtype=float32), -1.4182544]. 
=============================================
[2019-03-26 21:42:34,184] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 21:42:34,187] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:42:34,188] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:42:34,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:42:34,192] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:42:34,194] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:42:34,191] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:42:34,196] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:42:34,198] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:42:34,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:42:34,204] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:42:34,222] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-03-26 21:42:34,244] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-03-26 21:42:34,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-03-26 21:42:34,269] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-03-26 21:42:34,270] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-03-26 21:42:38,039] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:42:38,041] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.61666666666667, 77.33333333333333, 1.0, 2.0, 0.2483357195435739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 409739.8927695358, 409739.8927695351, 160706.188876242]
[2019-03-26 21:42:38,043] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:42:38,047] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9842537e-22 0.0000000e+00], sampled 0.3121459996563384
[2019-03-26 21:42:39,870] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:42:39,871] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.8255828, 83.15499528, 1.0, 2.0, 0.2960202604308501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474229.8131199123, 474229.8131199123, 165246.4037889405]
[2019-03-26 21:42:39,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:42:39,875] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.675995e-23 0.000000e+00], sampled 0.840675347405337
[2019-03-26 21:42:57,743] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:42:57,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.66666666666666, 95.16666666666667, 1.0, 2.0, 0.3453853068329361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535022.2491416008, 535022.2491416014, 169495.6252996505]
[2019-03-26 21:42:57,746] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:42:57,749] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.672472e-24 0.000000e+00], sampled 0.6275319150952798
[2019-03-26 21:43:12,081] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:12,081] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 89.0, 1.0, 2.0, 0.6572403156099959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001733.16975557, 1001733.16975557, 222295.8885578048]
[2019-03-26 21:43:12,083] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:43:12,085] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0236513e-21 0.0000000e+00], sampled 0.836003000576545
[2019-03-26 21:43:31,031] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:31,031] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.66666666666666, 58.66666666666666, 1.0, 2.0, 0.7253674252503876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1013738.564689718, 1013738.564689719, 226421.9447891558]
[2019-03-26 21:43:31,032] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:43:31,035] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5236243e-22 0.0000000e+00], sampled 0.23479042929948724
[2019-03-26 21:43:32,014] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:32,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.44109978, 74.22024112, 1.0, 2.0, 0.5750325333966895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803558.2814774769, 803558.2814774769, 196181.7051300925]
[2019-03-26 21:43:32,018] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:43:32,020] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.967509e-22 0.000000e+00], sampled 0.728879496098305
[2019-03-26 21:43:41,188] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:41,189] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.1, 88.0, 1.0, 2.0, 1.000995511487371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129194845105, 1399196.911102816, 1399196.911102816, 299231.987813753]
[2019-03-26 21:43:41,190] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:43:41,193] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.463921e-18 0.000000e+00], sampled 0.22570327918423616
[2019-03-26 21:43:51,094] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:51,095] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.85, 91.0, 1.0, 2.0, 0.6413929851651113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 896330.408269062, 896330.4082690626, 208704.1366917441]
[2019-03-26 21:43:51,096] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:43:51,100] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.8807986e-21 0.0000000e+00], sampled 0.6863412534544844
[2019-03-26 21:43:52,495] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:43:52,496] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.76244041, 88.22671152, 1.0, 2.0, 0.5258529674169915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734810.3161190164, 734810.3161190164, 187745.701863503]
[2019-03-26 21:43:52,497] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:43:52,502] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.919537e-22 0.000000e+00], sampled 0.43605248505632277
[2019-03-26 21:44:01,292] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69269735]
[2019-03-26 21:44:01,294] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.5, 70.0, 1.0, 2.0, 0.5884518668721577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822317.8995754375, 822317.8995754375, 198614.0772156733]
[2019-03-26 21:44:01,296] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:44:01,298] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5927335e-23 0.0000000e+00], sampled 0.26084288758340846
[2019-03-26 21:44:11,226] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164676497.9081 1778.0000
[2019-03-26 21:44:11,529] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 21:44:11,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.7138 2779867804.7524 933.0000
[2019-03-26 21:44:11,609] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 21:44:11,630] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 21:44:12,646] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1350000, evaluation results [1350000.0, 7876.615402862808, 3164676497.9080544, 1778.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8659.713804063274, 2779867804.752359, 933.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 21:44:16,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.2620458e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 21:44:16,302] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0773
[2019-03-26 21:44:16,308] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3022885073314969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 481377.9521034923, 481377.9521034916, 165720.6348426957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2940600.0000, 
sim time next is 2941200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3022952447846516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481388.7555171007, 481388.7555171007, 165721.4112967301], 
processed observation next is [1.0, 0.043478260869565216, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1593918611863272, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1337190987547502, 0.1337190987547502, 0.24734538999511957], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.3648363], dtype=float32), 0.36832762]. 
=============================================
[2019-03-26 21:44:19,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.253989e-22 0.000000e+00], sum to 1.0000
[2019-03-26 21:44:19,131] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5096
[2019-03-26 21:44:19,137] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4554664323995708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644889.3542668258, 644889.3542668258, 178005.615019745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214200.0000, 
sim time next is 3214800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3435988257961173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1790261510823746, 0.1790261510823746, 0.2656194289465004], 
reward next is 0.7344, 
noisyNet noise sample is [array([-2.929406], dtype=float32), 1.6165692]. 
=============================================
[2019-03-26 21:44:26,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 7.88729e-18 0.00000e+00], sum to 1.0000
[2019-03-26 21:44:26,948] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6380
[2019-03-26 21:44:26,954] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5166572696123977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721956.1719435282, 721956.1719435282, 186247.4435737031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3439800.0000, 
sim time next is 3440400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5157695029439897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 186104.0597786985], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.4165897625831201, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20019867264582358, 0.20019867264582358, 0.2777672534010425], 
reward next is 0.7222, 
noisyNet noise sample is [array([2.4548738], dtype=float32), 0.8624974]. 
=============================================
[2019-03-26 21:44:33,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6941125e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 21:44:33,434] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1070
[2019-03-26 21:44:33,439] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 75.66666666666666, 1.0, 2.0, 0.5040244115696126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704297.6701225686, 704297.6701225693, 184228.735297169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3544800.0000, 
sim time next is 3545400.0000, 
raw observation next is [28.0, 74.83333333333334, 1.0, 2.0, 0.4995048238479894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697980.1569303558, 697980.1569303565, 183518.2135709811], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.7483333333333334, 1.0, 1.0, 0.3969937636722764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19388337692509883, 0.19388337692509902, 0.27390778144922556], 
reward next is 0.7261, 
noisyNet noise sample is [array([-2.8952568], dtype=float32), 0.37904888]. 
=============================================
[2019-03-26 21:44:39,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7024086e-11 0.0000000e+00], sum to 1.0000
[2019-03-26 21:44:39,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8546
[2019-03-26 21:44:39,033] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7715458770249043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078308.110315211, 1078308.110315211, 237045.5242425282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3384000.0000, 
sim time next is 3384600.0000, 
raw observation next is [26.0, 94.00000000000001, 1.0, 2.0, 0.8375912802156392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1170663.670720548, 1170663.670720547, 253346.4939416901], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.9400000000000002, 1.0, 1.0, 0.8043268436333002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32518435297792997, 0.3251843529779297, 0.3781290954353584], 
reward next is 0.6219, 
noisyNet noise sample is [array([-1.046157], dtype=float32), 0.27334294]. 
=============================================
[2019-03-26 21:44:39,966] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2438235e-10 0.0000000e+00], sum to 1.0000
[2019-03-26 21:44:39,974] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5785
[2019-03-26 21:44:39,978] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4776390051154442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667542.1365439373, 667542.1365439373, 180182.5460899989], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3288000.0000, 
sim time next is 3288600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4777145889650721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667649.3665650703, 667649.3665650703, 180194.0888419972], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.370740468632617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1854581573791862, 0.1854581573791862, 0.26894640125671226], 
reward next is 0.7311, 
noisyNet noise sample is [array([-0.01051958], dtype=float32), 0.63634276]. 
=============================================
[2019-03-26 21:44:42,611] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1939579e-16 4.4170916e-02 1.1426761e-18 9.5582902e-01 9.5310361e-20], sum to 1.0000
[2019-03-26 21:44:42,620] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6533
[2019-03-26 21:44:42,625] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666667, 70.33333333333334, 1.0, 2.0, 0.9472690795492072, 1.0, 2.0, 0.9472690795492072, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2649646.178029408, 2649646.178029408, 497943.058761155], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3409800.0000, 
sim time next is 3410400.0000, 
raw observation next is [32.33333333333334, 69.66666666666667, 1.0, 2.0, 0.9322558554180246, 1.0, 2.0, 0.9322558554180246, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2607608.234303355, 2607608.234303355, 489402.4039340653], 
processed observation next is [1.0, 0.4782608695652174, 0.7314375987361774, 0.6966666666666668, 1.0, 1.0, 0.9183805486964152, 1.0, 1.0, 0.9183805486964152, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7243356206398209, 0.7243356206398209, 0.7304513491553214], 
reward next is 0.2695, 
noisyNet noise sample is [array([1.5226133], dtype=float32), 0.24724793]. 
=============================================
[2019-03-26 21:44:45,859] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1886666e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 21:44:45,868] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4986
[2019-03-26 21:44:45,874] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5166572696123977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721956.1719435282, 721956.1719435282, 186247.4435737031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3439800.0000, 
sim time next is 3440400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5157695029439897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720715.2215249649, 720715.2215249649, 186104.0597786985], 
processed observation next is [1.0, 0.8260869565217391, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.4165897625831201, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20019867264582358, 0.20019867264582358, 0.2777672534010425], 
reward next is 0.7222, 
noisyNet noise sample is [array([0.90862507], dtype=float32), -1.1344062]. 
=============================================
[2019-03-26 21:44:53,000] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5770857e-13 6.2681742e-08 1.2846015e-14 9.9999988e-01 2.1361548e-13], sum to 1.0000
[2019-03-26 21:44:53,005] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5377
[2019-03-26 21:44:53,012] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 61.66666666666667, 1.0, 2.0, 0.9586359029243369, 1.0, 2.0, 0.9586359029243369, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2681474.918016909, 2681474.91801691, 504491.9106713546], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3676800.0000, 
sim time next is 3677400.0000, 
raw observation next is [33.0, 61.0, 1.0, 2.0, 0.98419593760955, 1.0, 2.0, 0.98419593760955, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2753049.652610518, 2753049.652610519, 519507.2759320477], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.61, 1.0, 1.0, 0.9809589609753615, 1.0, 1.0, 0.9809589609753615, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7647360146140327, 0.7647360146140331, 0.7753839939284294], 
reward next is 0.2246, 
noisyNet noise sample is [array([0.09266236], dtype=float32), -0.37185043]. 
=============================================
[2019-03-26 21:44:56,807] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.6208085e-25 1.0000000e+00 1.5957554e-28 6.6189360e-11 2.7432408e-30], sum to 1.0000
[2019-03-26 21:44:56,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5108
[2019-03-26 21:44:56,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2216484.236871239 W.
[2019-03-26 21:44:56,829] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 72.0, 1.0, 2.0, 0.7925475435686825, 1.0, 1.0, 0.7925475435686825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2216484.236871239, 2216484.236871239, 416254.8278061872], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3573000.0000, 
sim time next is 3573600.0000, 
raw observation next is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.4810220811758265, 1.0, 2.0, 0.4810220811758265, 1.0, 1.0, 0.8259709581907494, 6.9112, 6.9112, 170.5573041426782, 2017694.678239445, 2017694.678239445, 400636.8523954644], 
processed observation next is [1.0, 0.34782608695652173, 0.6050552922590839, 0.7133333333333333, 1.0, 1.0, 0.37472539900701995, 1.0, 1.0, 0.37472539900701995, 1.0, 0.5, 0.7877694612082309, 0.0, 0.0, 0.8375144448122397, 0.5604707439554014, 0.5604707439554014, 0.597965451336514], 
reward next is 0.4020, 
noisyNet noise sample is [array([-1.5356696], dtype=float32), 0.7987683]. 
=============================================
[2019-03-26 21:45:01,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.9987412e-18 9.9999976e-01 2.5146832e-20 2.3494815e-07 1.6982888e-22], sum to 1.0000
[2019-03-26 21:45:01,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9951
[2019-03-26 21:45:01,375] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2483216.116028639 W.
[2019-03-26 21:45:01,381] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 65.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.181438928259237, 6.9112, 168.9112068834906, 2483216.116028639, 2291501.30524813, 475889.9573229707], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4014600.0000, 
sim time next is 4015200.0000, 
raw observation next is [31.66666666666667, 65.0, 1.0, 2.0, 0.9237217546232055, 1.0, 1.0, 0.9237217546232055, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2583712.861806944, 2583712.861806944, 484596.3423209155], 
processed observation next is [1.0, 0.4782608695652174, 0.6998420221169038, 0.65, 1.0, 1.0, 0.9080984995460307, 1.0, 0.5, 0.9080984995460307, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7176980171685956, 0.7176980171685956, 0.7232781228670381], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.10513403], dtype=float32), 1.1034741]. 
=============================================
[2019-03-26 21:45:03,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.1078470e-13 8.2231713e-07 7.6175171e-14 9.9999917e-01 3.0448066e-14], sum to 1.0000
[2019-03-26 21:45:03,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6799
[2019-03-26 21:45:03,158] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 60.33333333333333, 1.0, 2.0, 0.9792201655051281, 1.0, 2.0, 0.9792201655051281, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2739115.87715809, 2739115.877158089, 516553.1412626589], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3678000.0000, 
sim time next is 3678600.0000, 
raw observation next is [33.0, 59.66666666666667, 1.0, 2.0, 0.9719763931742758, 1.0, 2.0, 0.9719763931742758, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2718831.24340231, 2718831.24340231, 512278.8367730116], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.5966666666666667, 1.0, 1.0, 0.9662366182822599, 1.0, 1.0, 0.9662366182822599, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7552309009450862, 0.7552309009450862, 0.764595278765689], 
reward next is 0.2354, 
noisyNet noise sample is [array([0.23010406], dtype=float32), 1.1019801]. 
=============================================
[2019-03-26 21:45:06,132] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5305006e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:45:06,139] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2782
[2019-03-26 21:45:06,145] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4934420051251383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689505.5630190618, 689505.5630190624, 182574.681723728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3709200.0000, 
sim time next is 3709800.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.492888272639752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 688731.5604081282, 688731.5604081289, 182489.0766849304], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38902201522861685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19131432233559117, 0.19131432233559137, 0.2723717562461648], 
reward next is 0.7276, 
noisyNet noise sample is [array([0.5848173], dtype=float32), -0.87802625]. 
=============================================
[2019-03-26 21:45:07,678] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7304553e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:45:07,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8861
[2019-03-26 21:45:07,691] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 63.33333333333333, 1.0, 2.0, 0.5572060011874463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778638.1123257191, 778638.1123257191, 193045.2620676647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3868800.0000, 
sim time next is 3869400.0000, 
raw observation next is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.5640217650058872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 788165.9753220618, 788165.9753220612, 194235.5952034292], 
processed observation next is [0.0, 0.782608695652174, 0.7235387045813582, 0.6516666666666667, 1.0, 1.0, 0.4747250180793821, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21893499314501716, 0.218934993145017, 0.28990387343795404], 
reward next is 0.7101, 
noisyNet noise sample is [array([-1.8811976], dtype=float32), -0.13332887]. 
=============================================
[2019-03-26 21:45:08,169] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 21:45:08,173] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:45:08,174] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:45:08,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:45:08,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:45:08,180] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:45:08,179] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:45:08,176] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:45:08,184] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:45:08,185] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:45:08,183] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:45:08,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-03-26 21:45:08,235] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-03-26 21:45:08,254] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-03-26 21:45:08,275] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-03-26 21:45:08,295] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-03-26 21:45:09,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.86407155]
[2019-03-26 21:45:09,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.4, 73.0, 1.0, 2.0, 0.1944871282499636, 1.0, 2.0, 0.1944871282499636, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 584709.4915126814, 584709.4915126808, 239371.0410011887]
[2019-03-26 21:45:09,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:45:09,504] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0928630e-18 4.6046161e-08 4.0682730e-20 1.0000000e+00 9.0753675e-20], sampled 0.3172798370155564
[2019-03-26 21:45:23,334] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.86407155]
[2019-03-26 21:45:23,335] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.77316803166667, 92.77849892333333, 1.0, 2.0, 0.366318675112623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 560546.0718390847, 560546.0718390854, 171435.944747785]
[2019-03-26 21:45:23,336] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:45:23,338] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1245443e-32 0.0000000e+00], sampled 0.2827230808555822
[2019-03-26 21:45:49,218] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.86407155]
[2019-03-26 21:45:49,220] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.08308541, 99.43946980666666, 1.0, 2.0, 0.4717369725514896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659166.8732209255, 659166.8732209249, 179286.4975155537]
[2019-03-26 21:45:49,222] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:45:49,228] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9624363e-30 0.0000000e+00], sampled 0.3394874824187031
[2019-03-26 21:46:17,385] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.86407155]
[2019-03-26 21:46:17,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.9188989514959002, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.985142803000475, 6.9112, 168.9124570709035, 2181490.133343697, 2129032.719086113, 440111.12218496]
[2019-03-26 21:46:17,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:46:17,390] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7270623e-27 1.0000000e+00 5.2448423e-31 3.1406664e-13 5.0968293e-33], sampled 0.41143655361339415
[2019-03-26 21:46:17,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2181490.133343697 W.
[2019-03-26 21:47:02,102] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9210 2842890288.6408 1131.0000
[2019-03-26 21:47:02,169] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3731 3008038374.3173 1766.0000
[2019-03-26 21:47:02,174] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.6451 3164445220.6628 1778.0000
[2019-03-26 21:47:02,326] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7732 2779589745.4232 933.0000
[2019-03-26 21:47:02,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3751 2927779800.6357 1338.0000
[2019-03-26 21:47:03,368] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1375000, evaluation results [1375000.0, 7879.6450561207575, 3164445220.6627817, 1778.0, 8254.375086958147, 2927779800.6357465, 1338.0, 8660.773226357216, 2779589745.4231777, 933.0, 7998.373066976429, 3008038374.3172884, 1766.0, 8496.920990814182, 2842890288.6407876, 1131.0]
[2019-03-26 21:47:11,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.7703573e-12 2.1632400e-03 4.1491493e-13 9.9783677e-01 1.4858243e-13], sum to 1.0000
[2019-03-26 21:47:11,516] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8689
[2019-03-26 21:47:11,523] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333334, 76.33333333333334, 1.0, 2.0, 0.9248698547411895, 1.0, 2.0, 0.9248698547411895, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2586927.50077058, 2586927.500770579, 485250.5349840372], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4098000.0000, 
sim time next is 4098600.0000, 
raw observation next is [31.5, 75.0, 1.0, 2.0, 0.9211697049424139, 1.0, 2.0, 0.9211697049424139, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2576567.244278233, 2576567.244278233, 483181.4729931022], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.75, 1.0, 1.0, 0.9050237408944746, 1.0, 1.0, 0.9050237408944746, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7157131234106203, 0.7157131234106203, 0.7211663776016451], 
reward next is 0.2788, 
noisyNet noise sample is [array([-0.7081382], dtype=float32), -0.7707089]. 
=============================================
[2019-03-26 21:47:12,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0599421e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:12,054] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2461
[2019-03-26 21:47:12,061] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 85.66666666666667, 1.0, 2.0, 0.565386746059028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 790074.113987782, 790074.1139877826, 194474.783560228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3912000.0000, 
sim time next is 3912600.0000, 
raw observation next is [28.66666666666667, 84.83333333333333, 1.0, 2.0, 0.5721265898930277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799495.9482186974, 799495.9482186969, 195668.1644147278], 
processed observation next is [0.0, 0.2608695652173913, 0.5576619273301741, 0.8483333333333333, 1.0, 1.0, 0.4844898673409972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22208220783852706, 0.22208220783852692, 0.29204203643989224], 
reward next is 0.7080, 
noisyNet noise sample is [array([-1.4178789], dtype=float32), -1.062158]. 
=============================================
[2019-03-26 21:47:14,198] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0056525e-18 1.0000000e+00 1.1754785e-21 2.6085669e-08 9.1940729e-23], sum to 1.0000
[2019-03-26 21:47:14,206] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8329
[2019-03-26 21:47:14,217] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2643769.565043955 W.
[2019-03-26 21:47:14,223] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.00000000000001, 1.0, 2.0, 0.9451703653097268, 1.0, 2.0, 0.9451703653097268, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2643769.565043955, 2643769.565043956, 496739.4374307183], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4022400.0000, 
sim time next is 4023000.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.9572593193715073, 1.0, 2.0, 0.9572593193715073, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2677620.242962942, 2677620.242962943, 503698.1504701097], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6, 1.0, 1.0, 0.948505204062057, 1.0, 1.0, 0.948505204062057, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7437834008230394, 0.7437834008230397, 0.7517882842837459], 
reward next is 0.2482, 
noisyNet noise sample is [array([-1.6166418], dtype=float32), 0.90258324]. 
=============================================
[2019-03-26 21:47:14,243] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[26.34787 ]
 [27.532118]
 [28.621027]
 [28.048838]
 [30.920637]], R is [[29.16184044]
 [28.87022209]
 [28.58152008]
 [28.2957058 ]
 [28.01274872]].
[2019-03-26 21:47:22,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2693449e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:22,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0348
[2019-03-26 21:47:22,068] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 85.66666666666667, 1.0, 2.0, 0.9772308967099878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1365957.233263604, 1365957.233263604, 292071.0438370645], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4344000.0000, 
sim time next is 4344600.0000, 
raw observation next is [29.83333333333333, 84.83333333333333, 1.0, 2.0, 0.985891497624193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1378070.735662323, 1378070.735662324, 294662.644026053], 
processed observation next is [1.0, 0.2608695652173913, 0.6129541864139019, 0.8483333333333333, 1.0, 1.0, 0.9830018043664976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38279742657286747, 0.38279742657286775, 0.4397949910836612], 
reward next is 0.5602, 
noisyNet noise sample is [array([0.96243054], dtype=float32), -0.6871792]. 
=============================================
[2019-03-26 21:47:28,127] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1727949e-12 1.4716044e-10 5.7838348e-13 1.0000000e+00 2.9487979e-12], sum to 1.0000
[2019-03-26 21:47:28,138] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3067
[2019-03-26 21:47:28,147] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 73.66666666666666, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 6.930424796535631, 6.9112, 170.5573041426782, 2923117.311926232, 2909345.808153036, 553542.7858152691], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4178400.0000, 
sim time next is 4179000.0000, 
raw observation next is [32.83333333333333, 72.33333333333334, 1.0, 2.0, 1.009332850704634, 1.0, 2.0, 1.009332850704634, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2823443.53660696, 2823443.53660696, 534662.4834195155], 
processed observation next is [1.0, 0.34782608695652173, 0.7551342812006318, 0.7233333333333334, 1.0, 1.0, 1.011244398439318, 1.0, 1.0, 1.011244398439318, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7842898712797111, 0.7842898712797111, 0.7980037065962918], 
reward next is 0.2020, 
noisyNet noise sample is [array([-0.9205777], dtype=float32), -0.2838782]. 
=============================================
[2019-03-26 21:47:28,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[19.057026]
 [19.162567]
 [26.671162]
 [38.4736  ]
 [43.87022 ]], R is [[19.11776543]
 [18.92658806]
 [18.73732185]
 [18.54994965]
 [18.36445045]].
[2019-03-26 21:47:31,273] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.299245e-30 0.000000e+00], sum to 1.0000
[2019-03-26 21:47:31,279] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8173
[2019-03-26 21:47:31,284] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.6017710636677364, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840937.85878658, 840937.85878658, 201076.3022146555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4232400.0000, 
sim time next is 4233000.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.6008825142831682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839695.6751250174, 839695.6751250174, 200910.4748922966], 
processed observation next is [1.0, 1.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.519135559377311, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23324879864583817, 0.23324879864583817, 0.29986638043626357], 
reward next is 0.7001, 
noisyNet noise sample is [array([-2.6711855], dtype=float32), -2.3728185]. 
=============================================
[2019-03-26 21:47:31,301] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[81.552925]
 [81.58532 ]
 [81.61248 ]
 [81.64782 ]
 [81.68777 ]], R is [[81.4009552 ]
 [81.28682709]
 [81.17393494]
 [81.06244659]
 [80.95219421]].
[2019-03-26 21:47:39,280] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4823066e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:39,288] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0513
[2019-03-26 21:47:39,292] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5527835446855871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772455.9368315182, 772455.9368315176, 192278.7896705648], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4482600.0000, 
sim time next is 4483200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4580091082558088, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2135475268641311, 0.21354752686413092, 0.2863071135554025], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.3993758], dtype=float32), -0.12888466]. 
=============================================
[2019-03-26 21:47:41,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4555514e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:41,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7435
[2019-03-26 21:47:41,640] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5130121937667768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716860.9702976157, 716860.9702976157, 185659.7436488764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4519800.0000, 
sim time next is 4520400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5134090715666738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717415.7373815287, 717415.7373815293, 185723.4554471353], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4137458693574383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19928214927264687, 0.19928214927264704, 0.2771991872345303], 
reward next is 0.7228, 
noisyNet noise sample is [array([-0.43950045], dtype=float32), -0.6109163]. 
=============================================
[2019-03-26 21:47:42,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7387028e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:42,896] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4543
[2019-03-26 21:47:42,900] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 83.16666666666666, 1.0, 2.0, 0.5551925732185099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 775823.526184428, 775823.5261844286, 192694.7126408039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4477800.0000, 
sim time next is 4478400.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5527037252724505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772344.3571794131, 772344.3571794131, 192265.0418269546], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4610888256294584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21454009921650363, 0.21454009921650363, 0.28696274899545465], 
reward next is 0.7130, 
noisyNet noise sample is [array([0.819247], dtype=float32), -0.8649365]. 
=============================================
[2019-03-26 21:47:46,662] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 7.22209e-26 0.00000e+00], sum to 1.0000
[2019-03-26 21:47:46,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4950
[2019-03-26 21:47:46,676] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.8244836235566202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1152333.747422435, 1152333.747422435, 250005.8600467185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4761000.0000, 
sim time next is 4761600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7835879753519386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1095146.756755355, 1095146.756755355, 239918.6676532144], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7392626209059501, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30420743243204307, 0.30420743243204307, 0.358087563661514], 
reward next is 0.6419, 
noisyNet noise sample is [array([-0.25724903], dtype=float32), -0.33440286]. 
=============================================
[2019-03-26 21:47:53,352] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000e+00 1.0000e+00 0.0000e+00 2.2002e-21 0.0000e+00], sum to 1.0000
[2019-03-26 21:47:53,358] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4162
[2019-03-26 21:47:53,362] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 1.001329001628105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399663.372522734, 1399663.372522734, 299336.5388062986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4604400.0000, 
sim time next is 4605000.0000, 
raw observation next is [29.16666666666667, 88.16666666666667, 1.0, 2.0, 1.000461134521655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1398449.463905272, 1398449.463905272, 299071.9936719646], 
processed observation next is [1.0, 0.30434782608695654, 0.581358609794629, 0.8816666666666667, 1.0, 1.0, 1.00055558376103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3884581844181311, 0.3884581844181311, 0.44637610995815613], 
reward next is 0.5536, 
noisyNet noise sample is [array([-0.0755484], dtype=float32), -1.2124535]. 
=============================================
[2019-03-26 21:47:53,384] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[53.19219 ]
 [52.74482 ]
 [52.14148 ]
 [52.374546]
 [52.516273]], R is [[52.85293961]
 [52.87763977]
 [52.91225433]
 [52.94724655]
 [52.97376251]].
[2019-03-26 21:47:56,489] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.1423817e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:47:56,504] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0809
[2019-03-26 21:47:56,512] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4724233350370068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664883.6020439009, 664883.6020439015, 180001.7606253187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4767004995338715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668384.50316243, 668384.50316243, 180320.1372688563], 
processed observation next is [1.0, 0.9565217391304348, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.3695186741371945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856623619895639, 0.1856623619895639, 0.26913453323709896], 
reward next is 0.7309, 
noisyNet noise sample is [array([-0.05575066], dtype=float32), -0.71674514]. 
=============================================
[2019-03-26 21:47:56,526] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.4466 ]
 [69.42568]
 [69.60262]
 [69.86413]
 [69.98485]], R is [[69.40866089]
 [69.44591522]
 [69.48323822]
 [69.52067566]
 [69.55825806]].
[2019-03-26 21:47:59,124] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-03-26 21:47:59,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:47:59,126] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:47:59,127] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:47:59,128] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:47:59,128] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:47:59,129] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:47:59,130] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:47:59,130] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:47:59,131] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:47:59,131] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:47:59,165] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-03-26 21:47:59,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-03-26 21:47:59,188] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-03-26 21:47:59,209] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-03-26 21:47:59,249] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-03-26 21:48:25,406] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:48:25,409] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 93.66666666666667, 1.0, 2.0, 0.5309232420199458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840383.3991394218, 840383.3991394218, 199840.6788662375]
[2019-03-26 21:48:25,410] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:48:25,412] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6325126e-30 0.0000000e+00], sampled 0.8788716959872592
[2019-03-26 21:48:40,981] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:48:40,983] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.1, 54.0, 1.0, 2.0, 0.8653367274783101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1209464.321033755, 1209464.321033754, 260580.1166673866]
[2019-03-26 21:48:40,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:48:40,987] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5029098e-28 0.0000000e+00], sampled 0.828798112977296
[2019-03-26 21:48:48,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:48:48,607] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 65.66666666666667, 1.0, 2.0, 0.7761271981519937, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084714.208791067, 1084714.208791068, 238132.9651938812]
[2019-03-26 21:48:48,607] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:48:48,609] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0416946e-28 0.0000000e+00], sampled 0.8913337706074279
[2019-03-26 21:49:07,325] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:49:07,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.2, 58.0, 1.0, 2.0, 0.981684507975525, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104007, 1372186.440886367, 1372186.440886367, 293401.7799025718]
[2019-03-26 21:49:07,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:49:07,329] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6022059e-24 0.0000000e+00], sampled 0.6565529412868448
[2019-03-26 21:49:11,564] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:49:11,565] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.56666666666667, 77.0, 1.0, 2.0, 0.6013318297063597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 840323.8136685065, 840323.8136685059, 200994.0772361184]
[2019-03-26 21:49:11,566] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:49:11,570] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.688874e-33 0.000000e+00], sampled 0.9967290051037709
[2019-03-26 21:49:13,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:49:13,766] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.525435315, 78.85456069, 1.0, 2.0, 0.5011515372004876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700281.9407003131, 700281.9407003131, 183774.4526396506]
[2019-03-26 21:49:13,767] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:49:13,769] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.434502e-33 0.000000e+00], sampled 0.47728599579764464
[2019-03-26 21:49:24,932] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:49:24,933] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.89148104666667, 89.55340457166668, 1.0, 2.0, 1.005893612169566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1406048.03655885, 1406048.036558851, 300729.4424640421]
[2019-03-26 21:49:24,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:49:24,940] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3893404e-27 0.0000000e+00], sampled 0.329844804438861
[2019-03-26 21:49:37,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.82225335]
[2019-03-26 21:49:37,985] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.13333333333333, 80.66666666666667, 1.0, 2.0, 0.4826511604449479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683410.5022322448, 683410.5022322442, 182076.4240598995]
[2019-03-26 21:49:37,987] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:49:37,989] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.038556e-34 0.000000e+00], sampled 0.11665467344071045
[2019-03-26 21:49:52,701] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.5774 3164125882.4970 1773.0000
[2019-03-26 21:49:52,879] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.2690 2779542891.8498 928.0000
[2019-03-26 21:49:53,015] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1751 3008171027.5419 1766.0000
[2019-03-26 21:49:53,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.9589 2927814887.1326 1337.0000
[2019-03-26 21:49:53,059] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7157 2843027813.7524 1131.0000
[2019-03-26 21:49:54,077] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1400000, evaluation results [1400000.0, 7878.5774274029145, 3164125882.496975, 1773.0, 8254.95885454361, 2927814887.132611, 1337.0, 8662.269015232618, 2779542891.8498363, 928.0, 7998.175077088882, 3008171027.5419445, 1766.0, 8496.715729453535, 2843027813.7524204, 1131.0]
[2019-03-26 21:49:58,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.464911e-29 0.000000e+00], sum to 1.0000
[2019-03-26 21:49:58,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9913
[2019-03-26 21:49:58,262] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 80.66666666666667, 1.0, 2.0, 0.516402066171056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721599.4397520293, 721599.4397520293, 186205.7922090994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4746000.0000, 
sim time next is 4746600.0000, 
raw observation next is [27.5, 81.5, 1.0, 2.0, 0.5152573297455308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719999.2890821466, 719999.2890821459, 186021.0516612937], 
processed observation next is [1.0, 0.9565217391304348, 0.5023696682464456, 0.815, 1.0, 1.0, 0.4159726864403986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1999998025228185, 0.1999998025228183, 0.27764336068849804], 
reward next is 0.7224, 
noisyNet noise sample is [array([0.91397816], dtype=float32), -1.3626909]. 
=============================================
[2019-03-26 21:50:02,587] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.737991e-27 0.000000e+00], sum to 1.0000
[2019-03-26 21:50:02,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9259
[2019-03-26 21:50:02,603] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.756665916836954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1057501.607943382, 1057501.607943382, 233548.500064555], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4849200.0000, 
sim time next is 4849800.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.6573587196675788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 918651.7662806674, 918651.7662806674, 211907.3129420844], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.587179180322384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2551810461890743, 0.2551810461890743, 0.31627957155534986], 
reward next is 0.6837, 
noisyNet noise sample is [array([0.4562988], dtype=float32), -0.6901961]. 
=============================================
[2019-03-26 21:50:11,118] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 7.29995e-26 0.00000e+00], sum to 1.0000
[2019-03-26 21:50:11,130] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0084
[2019-03-26 21:50:11,134] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5059225387264951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 706950.8975243263, 706950.8975243256, 184528.8429603633], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5104200.0000, 
sim time next is 5104800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5052368700775439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705992.4599967974, 705992.459996798, 184420.3203141394], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40389984346692037, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19610901666577704, 0.1961090166657772, 0.2752542094240886], 
reward next is 0.7247, 
noisyNet noise sample is [array([-0.584922], dtype=float32), -1.9591763]. 
=============================================
[2019-03-26 21:50:16,028] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4251713e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:50:16,038] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2815
[2019-03-26 21:50:16,045] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.88333333333333, 75.66666666666667, 1.0, 2.0, 0.5821301382215632, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813480.3690571458, 813480.3690571458, 197465.3251334432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5422200.0000, 
sim time next is 5422800.0000, 
raw observation next is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.5872359064941861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820618.0285986264, 820618.0285986264, 198393.2900724916], 
processed observation next is [1.0, 0.782608695652174, 0.6619273301737759, 0.7633333333333334, 1.0, 1.0, 0.5026938632460073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22794945238850733, 0.22794945238850733, 0.2961093881678979], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.75727224], dtype=float32), 1.6685317]. 
=============================================
[2019-03-26 21:50:22,790] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5578806e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:50:22,796] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4722
[2019-03-26 21:50:22,802] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.58333333333334, 80.16666666666667, 1.0, 2.0, 0.5460183157576335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762998.8569082768, 762998.8569082768, 191120.2139459411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5260200.0000, 
sim time next is 5260800.0000, 
raw observation next is [28.56666666666667, 80.33333333333334, 1.0, 2.0, 0.5446155168730131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761037.9018531352, 761037.9018531352, 190881.7070866035], 
processed observation next is [1.0, 0.9130434782608695, 0.552922590837283, 0.8033333333333335, 1.0, 1.0, 0.4513439962325459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21139941718142644, 0.21139941718142644, 0.2848980702785127], 
reward next is 0.7151, 
noisyNet noise sample is [array([-1.6156542], dtype=float32), 1.2140509]. 
=============================================
[2019-03-26 21:50:24,606] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3702349e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:50:24,614] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5454
[2019-03-26 21:50:24,619] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.33333333333333, 1.0, 2.0, 0.5497328995329734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768191.4446982018, 768191.4446982018, 191754.9222320282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5263800.0000, 
sim time next is 5264400.0000, 
raw observation next is [28.5, 81.66666666666667, 1.0, 2.0, 0.5508259599999857, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769719.4302636089, 769719.4302636089, 191942.5302566618], 
processed observation next is [1.0, 0.9565217391304348, 0.5497630331753555, 0.8166666666666668, 1.0, 1.0, 0.4588264578313081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2138109528510025, 0.2138109528510025, 0.2864813884427788], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.09440863], dtype=float32), 0.9141682]. 
=============================================
[2019-03-26 21:50:26,916] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.660318e-25 0.000000e+00], sum to 1.0000
[2019-03-26 21:50:26,924] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7805
[2019-03-26 21:50:26,931] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.73333333333333, 82.83333333333334, 1.0, 2.0, 0.6342889859199617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 886398.606966944, 886398.606966944, 207308.6455057407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5429400.0000, 
sim time next is 5430000.0000, 
raw observation next is [30.66666666666667, 82.66666666666667, 1.0, 2.0, 0.631326348432448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 882256.6960672997, 882256.6960672997, 206727.8689989859], 
processed observation next is [1.0, 0.8695652173913043, 0.6524486571879939, 0.8266666666666667, 1.0, 1.0, 0.5558148776294554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2450713044631388, 0.2450713044631388, 0.30854905820744166], 
reward next is 0.6915, 
noisyNet noise sample is [array([-0.93488556], dtype=float32), -1.7625014]. 
=============================================
[2019-03-26 21:50:26,943] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.99602 ]
 [61.977154]
 [61.51427 ]
 [61.543262]
 [61.47194 ]], R is [[62.09889984]
 [62.16849518]
 [62.23749542]
 [62.30669403]
 [62.37627029]].
[2019-03-26 21:50:31,327] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7937903e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:50:31,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4425
[2019-03-26 21:50:31,337] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.18333333333334, 84.33333333333334, 1.0, 2.0, 0.5873787972320692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820817.7848168855, 820817.7848168849, 198418.3753768629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5442600.0000, 
sim time next is 5443200.0000, 
raw observation next is [29.1, 85.0, 1.0, 2.0, 0.5875141679837166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821007.0283939315, 821007.0283939309, 198443.1342128485], 
processed observation next is [1.0, 0.0, 0.5781990521327015, 0.85, 1.0, 1.0, 0.5030291180526706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2280575078872032, 0.22805750788720303, 0.2961837824072366], 
reward next is 0.7038, 
noisyNet noise sample is [array([-0.43418968], dtype=float32), -0.7538409]. 
=============================================
[2019-03-26 21:50:38,215] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:50:38,226] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4050
[2019-03-26 21:50:38,231] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 91.66666666666666, 1.0, 2.0, 0.522220160378864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 187150.3899612127], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5618400.0000, 
sim time next is 5619000.0000, 
raw observation next is [26.05, 91.83333333333333, 1.0, 2.0, 0.5207622642880092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727694.291853428, 727694.291853428, 186912.7021412179], 
processed observation next is [0.0, 0.0, 0.43364928909952616, 0.9183333333333333, 1.0, 1.0, 0.4226051376963966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2021373032926189, 0.2021373032926189, 0.2789741823003252], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.13562316], dtype=float32), -1.1447587]. 
=============================================
[2019-03-26 21:50:38,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.3859  ]
 [79.257126]
 [79.134315]
 [78.7943  ]
 [78.48208 ]], R is [[79.47861481]
 [79.40450287]
 [79.33110809]
 [79.25832367]
 [79.18590546]].
[2019-03-26 21:50:38,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0156315e-15 1.6978875e-22 1.3895069e-15 1.0000000e+00 3.6184909e-14], sum to 1.0000
[2019-03-26 21:50:38,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9247
[2019-03-26 21:50:38,834] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.53333333333333, 71.33333333333333, 1.0, 2.0, 0.9776575958439334, 1.0, 2.0, 0.9776575958439334, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2734740.207456575, 2734740.207456575, 515639.7931485925], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5476200.0000, 
sim time next is 5476800.0000, 
raw observation next is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.9548745306145201, 1.0, 2.0, 0.9548745306145201, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2670942.444900501, 2670942.444900502, 502324.735967196], 
processed observation next is [1.0, 0.391304347826087, 0.7519747235387049, 0.7066666666666667, 1.0, 1.0, 0.9456319645958073, 1.0, 1.0, 0.9456319645958073, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7419284569168059, 0.7419284569168061, 0.7497384118913373], 
reward next is 0.2503, 
noisyNet noise sample is [array([-0.2061094], dtype=float32), 1.4197224]. 
=============================================
[2019-03-26 21:50:41,051] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.3926141e-11 1.0253173e-10 3.1740606e-12 1.0000000e+00 5.8120483e-12], sum to 1.0000
[2019-03-26 21:50:41,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3826
[2019-03-26 21:50:41,067] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 55.0, 1.0, 2.0, 1.008974694138994, 1.0, 2.0, 1.008974694138994, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2822440.520370055, 2822440.520370055, 534440.9593354841], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5488200.0000, 
sim time next is 5488800.0000, 
raw observation next is [36.09999999999999, 54.0, 1.0, 2.0, 0.9622065841148877, 1.0, 2.0, 0.9622065841148877, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2691473.506183254, 2691473.506183253, 506575.1436447324], 
processed observation next is [1.0, 0.5217391304347826, 0.9099526066350705, 0.54, 1.0, 1.0, 0.9544657639938405, 1.0, 1.0, 0.9544657639938405, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7476315294953484, 0.7476315294953481, 0.7560823039473619], 
reward next is 0.2439, 
noisyNet noise sample is [array([1.1952412], dtype=float32), 1.2053971]. 
=============================================
[2019-03-26 21:50:43,238] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1317998e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:50:43,246] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6001
[2019-03-26 21:50:43,250] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.56666666666667, 93.0, 1.0, 2.0, 0.5420020716338375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757384.6090214167, 757384.609021416, 190438.5092067846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5534400.0000, 
sim time next is 5535000.0000, 
raw observation next is [26.5, 93.5, 1.0, 2.0, 0.5424731547390964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758043.1275469636, 758043.1275469636, 190518.1862080742], 
processed observation next is [1.0, 0.043478260869565216, 0.4549763033175356, 0.935, 1.0, 1.0, 0.4487628370350559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21056753542971213, 0.21056753542971213, 0.2843555018030958], 
reward next is 0.7156, 
noisyNet noise sample is [array([1.0942138], dtype=float32), -0.109820314]. 
=============================================
[2019-03-26 21:50:43,276] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.14008 ]
 [66.27632 ]
 [66.513985]
 [66.5122  ]
 [66.59913 ]], R is [[66.00521088]
 [66.06092834]
 [66.11598206]
 [66.17028046]
 [66.22362518]].
[2019-03-26 21:50:49,668] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 21:50:49,670] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:50:49,671] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:50:49,671] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:50:49,672] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:50:49,672] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:50:49,673] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:50:49,674] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:50:49,674] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:50:49,674] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:50:49,676] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:50:49,713] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-03-26 21:50:49,734] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-03-26 21:50:49,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-03-26 21:50:49,775] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-03-26 21:50:49,777] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-03-26 21:51:02,195] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:02,197] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.378724245, 96.206898625, 1.0, 2.0, 0.2547048327776144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419438.0588094846, 419438.0588094846, 161363.0124461048]
[2019-03-26 21:51:02,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:51:02,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7014727168620895
[2019-03-26 21:51:12,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:12,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.112669625, 64.927959765, 1.0, 2.0, 0.3948822091247733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 593028.1531218251, 593028.1531218251, 174006.5165923187]
[2019-03-26 21:51:12,496] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:51:12,499] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3194385466338763
[2019-03-26 21:51:14,260] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:14,261] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.45, 72.33333333333334, 1.0, 2.0, 0.649292421102304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 922058.9995431901, 922058.9995431907, 212210.8753325308]
[2019-03-26 21:51:14,262] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:51:14,266] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3973484e-36 0.0000000e+00], sampled 0.7722597554042145
[2019-03-26 21:51:27,112] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:27,113] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 89.0, 1.0, 2.0, 0.4573981071325827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648801.6628119738, 648801.6628119744, 178438.1370797681]
[2019-03-26 21:51:27,114] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:51:27,115] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5638994243794313
[2019-03-26 21:51:33,198] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:33,199] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 95.0, 1.0, 2.0, 0.3072213973422602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484896.8313961297, 484896.8313961304, 165889.6080298697]
[2019-03-26 21:51:33,201] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:51:33,203] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9214578005805412
[2019-03-26 21:51:38,539] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:51:38,541] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.45, 62.5, 1.0, 2.0, 0.6232507737264232, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565032755, 870966.7329721111, 870966.7329721117, 205151.3505801214]
[2019-03-26 21:51:38,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:51:38,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3641693e-28 0.0000000e+00], sampled 0.26554507588157417
[2019-03-26 21:52:01,795] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:01,797] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.16666666666667, 73.33333333333334, 1.0, 2.0, 0.5264006788904537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735575.9358176247, 735575.9358176247, 187835.9249750646]
[2019-03-26 21:52:01,800] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:52:01,803] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2405623282327075
[2019-03-26 21:52:10,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:10,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.2, 93.0, 1.0, 2.0, 0.6105296799936789, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 853182.3995655987, 853182.3995655994, 202723.4801866525]
[2019-03-26 21:52:10,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:52:10,426] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.162672e-37 0.000000e+00], sampled 0.49477950586378083
[2019-03-26 21:52:27,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:27,135] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.56666666666667, 79.0, 1.0, 2.0, 0.6151725199120217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859673.1444501642, 859673.1444501642, 203606.3983775474]
[2019-03-26 21:52:27,135] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 21:52:27,137] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3294832e-37 0.0000000e+00], sampled 0.5799232528905165
[2019-03-26 21:52:35,467] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:35,469] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.02885601, 90.64345242666667, 1.0, 2.0, 0.275334051292796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447406.8427665723, 447406.8427665723, 163397.3283772832]
[2019-03-26 21:52:35,471] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:52:35,473] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5915386357692841
[2019-03-26 21:52:37,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:37,807] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.1, 73.83333333333334, 1.0, 2.0, 0.4615860451797932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657862.8952332186, 657862.8952332186, 179451.7015075371]
[2019-03-26 21:52:37,808] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:52:37,813] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8515863202779697
[2019-03-26 21:52:42,415] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0114465]
[2019-03-26 21:52:42,417] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [31.00079171333333, 67.68187765500001, 1.0, 2.0, 0.9331969623412797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104295, 1304369.514385681, 1304369.51438568, 279233.9331470606]
[2019-03-26 21:52:42,417] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:52:42,420] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5146819e-29 0.0000000e+00], sampled 0.6976374251348796
[2019-03-26 21:52:43,017] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2741 3008104656.7243 1766.0000
[2019-03-26 21:52:43,033] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9210 2842890288.6408 1131.0000
[2019-03-26 21:52:43,107] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7732 2779589745.4232 933.0000
[2019-03-26 21:52:43,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.6446 3164158743.8923 1776.0000
[2019-03-26 21:52:43,334] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4752 2927712708.6915 1338.0000
[2019-03-26 21:52:44,353] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1425000, evaluation results [1425000.0, 7879.644641876938, 3164158743.8923206, 1776.0, 8254.475224188334, 2927712708.691521, 1338.0, 8660.773226357216, 2779589745.4231777, 933.0, 7998.274138010797, 3008104656.724262, 1766.0, 8496.920990814182, 2842890288.6407876, 1131.0]
[2019-03-26 21:52:45,473] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:52:45,480] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7487
[2019-03-26 21:52:45,484] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 82.0, 1.0, 2.0, 0.5464315738809508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763576.5459721037, 763576.5459721037, 191190.3587485276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5774400.0000, 
sim time next is 5775000.0000, 
raw observation next is [28.15, 82.33333333333334, 1.0, 2.0, 0.5469741334697272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764334.9846339363, 764334.9846339363, 191282.8422239294], 
processed observation next is [0.0, 0.8695652173913043, 0.533175355450237, 0.8233333333333335, 1.0, 1.0, 0.4541857029755749, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21231527350942675, 0.21231527350942675, 0.28549677943870055], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.11029434], dtype=float32), -0.38232192]. 
=============================================
[2019-03-26 21:52:45,495] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[84.59425]
 [84.46892]
 [84.48134]
 [84.48425]
 [84.46023]], R is [[84.43989563]
 [84.31013489]
 [84.18132782]
 [84.05355835]
 [83.92687225]].
[2019-03-26 21:52:50,278] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:52:50,290] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3229
[2019-03-26 21:52:50,295] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 87.0, 1.0, 2.0, 0.5377579331314782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751451.8232215702, 751451.8232215696, 189723.1029902895], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5785200.0000, 
sim time next is 5785800.0000, 
raw observation next is [27.18333333333333, 87.16666666666667, 1.0, 2.0, 0.5378311644703586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751554.1913829291, 751554.1913829284, 189735.4119743365], 
processed observation next is [0.0, 1.0, 0.48736176935229053, 0.8716666666666667, 1.0, 1.0, 0.44317007767513084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20876505316192476, 0.20876505316192456, 0.2831871820512485], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.05444084], dtype=float32), -1.0042108]. 
=============================================
[2019-03-26 21:52:50,879] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4866507e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 21:52:50,888] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8334
[2019-03-26 21:52:50,894] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 54.66666666666667, 1.0, 2.0, 0.5363100960623538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749427.9319116526, 749427.9319116526, 189480.3181243428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5746200.0000, 
sim time next is 5746800.0000, 
raw observation next is [33.2, 54.33333333333334, 1.0, 2.0, 0.5292446802138429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739551.4382507083, 739551.438250709, 188305.4382985118], 
processed observation next is [0.0, 0.5217391304347826, 0.7725118483412324, 0.5433333333333334, 1.0, 1.0, 0.4328249159202926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20543095506964118, 0.20543095506964137, 0.28105289298285346], 
reward next is 0.7189, 
noisyNet noise sample is [array([-0.18250675], dtype=float32), -1.3000684]. 
=============================================
[2019-03-26 21:53:00,496] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2974386e-19 9.9996018e-01 4.3981288e-25 3.9797633e-05 4.6772449e-27], sum to 1.0000
[2019-03-26 21:53:00,502] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6508
[2019-03-26 21:53:00,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2229282.919292566 W.
[2019-03-26 21:53:00,520] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.86666666666667, 75.83333333333333, 1.0, 2.0, 0.9530458598347374, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.003939719092852, 6.9112, 168.9124047960994, 2229282.919292566, 2163490.385609795, 449259.1497992757], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5928600.0000, 
sim time next is 5929200.0000, 
raw observation next is [29.9, 76.0, 1.0, 2.0, 0.5332903872889585, 1.0, 1.0, 0.5332903872889585, 1.0, 2.0, 0.9261490408948778, 6.9112, 6.9112, 170.5573041426782, 2237164.541553979, 2237164.541553979, 438930.5378808798], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.76, 1.0, 1.0, 0.4376992617939259, 1.0, 0.5, 0.4376992617939259, 1.0, 1.0, 0.909937854749851, 0.0, 0.0, 0.8375144448122397, 0.6214345948761052, 0.6214345948761052, 0.655120205792358], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.02439708], dtype=float32), -0.3177969]. 
=============================================
[2019-03-26 21:53:00,933] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2473196e-17 9.6716970e-01 2.9931058e-21 3.2830324e-02 1.4893547e-23], sum to 1.0000
[2019-03-26 21:53:00,942] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5361
[2019-03-26 21:53:00,952] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2517531.857698145 W.
[2019-03-26 21:53:00,962] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.56666666666667, 71.33333333333334, 1.0, 2.0, 0.900084721169259, 1.0, 2.0, 0.900084721169259, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2517531.857698145, 2517531.857698145, 471543.2598650322], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5917200.0000, 
sim time next is 5917800.0000, 
raw observation next is [31.43333333333333, 71.66666666666666, 1.0, 2.0, 0.5977956290654961, 1.0, 2.0, 0.5977956290654961, 1.0, 1.0, 1.03, 6.920388008058034, 6.9112, 170.5573041426782, 2508036.953916149, 2501455.209999906, 487180.7951418557], 
processed observation next is [1.0, 0.4782608695652174, 0.6887835703001578, 0.7166666666666666, 1.0, 1.0, 0.5154164205608387, 1.0, 1.0, 0.5154164205608387, 1.0, 0.5, 1.0365853658536586, 0.0009188008058034214, 0.0, 0.8375144448122397, 0.6966769316433747, 0.6948486694444183, 0.7271355151370981], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2018735], dtype=float32), -1.0452315]. 
=============================================
[2019-03-26 21:53:00,986] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 21:53:00,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3475
[2019-03-26 21:53:00,999] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.95, 66.0, 1.0, 2.0, 0.5080061117466547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709863.3512516905, 709863.3512516911, 184860.0016052689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6283800.0000, 
sim time next is 6284400.0000, 
raw observation next is [29.8, 67.0, 1.0, 2.0, 0.510293733637623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713061.0371501195, 713061.0371501202, 185224.5668484073], 
processed observation next is [0.0, 0.7391304347826086, 0.6113744075829385, 0.67, 1.0, 1.0, 0.4099924501658108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19807251031947765, 0.19807251031947784, 0.2764545773856825], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.199411], dtype=float32), 0.05879107]. 
=============================================
[2019-03-26 21:53:01,100] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7308701e-21 9.9999034e-01 2.0006708e-24 9.6684789e-06 3.7390493e-28], sum to 1.0000
[2019-03-26 21:53:01,109] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0379
[2019-03-26 21:53:01,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2299683.58680824 W.
[2019-03-26 21:53:01,126] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 1.003344724250459, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.989072942157442, 6.9112, 168.9124935588313, 2299683.58680824, 2244437.99287782, 464963.8246559508], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6090600.0000, 
sim time next is 6091200.0000, 
raw observation next is [31.0, 65.0, 1.0, 2.0, 0.8472812076444215, 1.0, 1.0, 0.8472812076444215, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2369700.688606919, 2369700.688606919, 443541.5286508137], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.65, 1.0, 1.0, 0.8160014549932789, 1.0, 0.5, 0.8160014549932789, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6582501912796996, 0.6582501912796996, 0.6620022815683786], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5424717], dtype=float32), -0.6969783]. 
=============================================
[2019-03-26 21:53:04,576] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.98089260e-24 1.00000000e+00 1.12178174e-29 5.73736969e-09
 8.27905906e-34], sum to 1.0000
[2019-03-26 21:53:04,584] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7347
[2019-03-26 21:53:04,593] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1979421.114888762 W.
[2019-03-26 21:53:04,597] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.05, 78.5, 1.0, 2.0, 0.707859023585743, 1.0, 2.0, 0.707859023585743, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1979421.114888762, 1979421.114888761, 377508.546620377], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6172200.0000, 
sim time next is 6172800.0000, 
raw observation next is [29.13333333333333, 78.0, 1.0, 2.0, 0.7382317773400465, 1.0, 2.0, 0.7382317773400465, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2064435.779804814, 2064435.779804815, 390921.6161496717], 
processed observation next is [1.0, 0.43478260869565216, 0.5797788309636649, 0.78, 1.0, 1.0, 0.6846165992048753, 1.0, 1.0, 0.6846165992048753, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.573454383279115, 0.5734543832791152, 0.5834650987308533], 
reward next is 0.4165, 
noisyNet noise sample is [array([-1.5957897], dtype=float32), -1.0506718]. 
=============================================
[2019-03-26 21:53:04,639] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4415620e-36 1.0000000e+00 0.0000000e+00 4.9529983e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 21:53:04,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7575
[2019-03-26 21:53:04,652] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.1, 65.0, 1.0, 2.0, 0.3642414875421559, 1.0, 2.0, 0.3642414875421559, 1.0, 1.0, 0.6279993563768325, 6.9112, 6.9112, 170.5573041426782, 1527497.789858244, 1527497.789858244, 333406.9433597948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6094800.0000, 
sim time next is 6095400.0000, 
raw observation next is [31.1, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.095678717311799, 6.9112, 168.906354660354, 2294597.605658242, 1454320.479939828, 311295.9822267772], 
processed observation next is [1.0, 0.5652173913043478, 0.6729857819905214, 0.65, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.11844787173117988, 0.0, 0.8294075270460846, 0.6373882237939561, 0.4039779110943966, 0.46462086899518984], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6738105], dtype=float32), 0.6923548]. 
=============================================
[2019-03-26 21:53:12,089] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1070427e-30 1.0000000e+00 8.7025576e-36 2.4260757e-10 0.0000000e+00], sum to 1.0000
[2019-03-26 21:53:12,101] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7358
[2019-03-26 21:53:12,111] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2160441.385899763 W.
[2019-03-26 21:53:12,117] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 66.66666666666667, 1.0, 2.0, 0.7725283104861934, 1.0, 2.0, 0.7725283104861934, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2160441.385899763, 2160441.385899762, 406715.7842777108], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6104400.0000, 
sim time next is 6105000.0000, 
raw observation next is [30.63333333333334, 66.83333333333333, 1.0, 2.0, 0.9491353339651306, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988592747800116, 6.9112, 168.9124959858174, 2223809.729089909, 2168904.799841012, 448691.3708350933], 
processed observation next is [1.0, 0.6521739130434783, 0.6508688783570303, 0.6683333333333333, 1.0, 1.0, 0.9387172698375068, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0077392747800115735, 0.0, 0.8294376837656209, 0.6177249247471969, 0.6024735555113921, 0.6696886131867065], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.54916894], dtype=float32), 0.961362]. 
=============================================
[2019-03-26 21:53:12,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[47.541656]
 [47.798267]
 [48.144493]
 [48.755795]
 [46.07195 ]], R is [[47.08943558]
 [47.01150513]
 [46.94929886]
 [46.90448761]
 [46.4548912 ]].
[2019-03-26 21:53:13,526] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.8440218e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 21:53:13,535] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2820
[2019-03-26 21:53:13,547] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 88.66666666666666, 1.0, 2.0, 0.5242056698408817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732507.6408724061, 732507.6408724061, 187475.409232148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6324000.0000, 
sim time next is 6324600.0000, 
raw observation next is [26.63333333333333, 88.83333333333334, 1.0, 2.0, 0.5240078060906647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732231.057325339, 732231.0573253395, 187442.9812676529], 
processed observation next is [0.0, 0.17391304347826086, 0.46129541864139006, 0.8883333333333334, 1.0, 1.0, 0.42651542902489714, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20339751592370525, 0.20339751592370542, 0.27976564368306406], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.09174428], dtype=float32), -0.3531834]. 
=============================================
[2019-03-26 21:53:17,795] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2768751e-21 4.0943572e-01 1.6906977e-24 5.9056431e-01 3.2717411e-28], sum to 1.0000
[2019-03-26 21:53:17,805] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4035
[2019-03-26 21:53:17,815] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2201695.617445488 W.
[2019-03-26 21:53:17,821] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 76.0, 1.0, 2.0, 0.7872648089956111, 1.0, 2.0, 0.7872648089956111, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2201695.617445488, 2201695.617445489, 413715.6748540277], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6195600.0000, 
sim time next is 6196200.0000, 
raw observation next is [28.91666666666667, 76.5, 1.0, 2.0, 0.360004395126957, 1.0, 2.0, 0.360004395126957, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1006241.45045736, 1006241.45045736, 262523.0301299238], 
processed observation next is [1.0, 0.7391304347826086, 0.5695102685624015, 0.765, 1.0, 1.0, 0.22892095798428552, 1.0, 1.0, 0.22892095798428552, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2795115140159333, 0.2795115140159333, 0.39182541810436383], 
reward next is 0.6082, 
noisyNet noise sample is [array([0.89357334], dtype=float32), 0.76493376]. 
=============================================
[2019-03-26 21:53:25,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2674437e-31 1.0000000e+00 6.0106143e-35 1.0367851e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 21:53:25,793] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7885
[2019-03-26 21:53:25,798] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.85, 55.83333333333334, 1.0, 2.0, 0.555026956445983, 1.0, 2.0, 0.555026956445983, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1551739.8200463, 1551739.820046299, 318227.8629041724], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6527400.0000, 
sim time next is 6528000.0000, 
raw observation next is [31.9, 55.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 9.6224836996918, 6.9112, 168.8973049252227, 3378256.725940479, 1454956.984924825, 307513.022642881], 
processed observation next is [1.0, 0.5652173913043478, 0.7109004739336492, 0.5566666666666668, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.27112836996918005, 0.0, 0.8293630887036032, 0.9384046460945775, 0.40415471803467357, 0.45897466066101644], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.0872114], dtype=float32), -1.6827755]. 
=============================================
[2019-03-26 21:53:25,813] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[38.376522]
 [34.969967]
 [34.392246]
 [36.47465 ]
 [36.002342]], R is [[34.08411026]
 [34.26830292]
 [34.39812469]
 [34.50130844]
 [34.635952  ]].
[2019-03-26 21:53:35,492] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6295968e-16 9.9959415e-01 2.0449518e-19 4.0580312e-04 3.4802623e-21], sum to 1.0000
[2019-03-26 21:53:35,498] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0498
[2019-03-26 21:53:35,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1915637.490271704 W.
[2019-03-26 21:53:35,514] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.7289475889430351, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.981268741937822, 6.9112, 168.9125391294155, 1915637.490271704, 1865928.435991554, 391933.977680661], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6688800.0000, 
sim time next is 6689400.0000, 
raw observation next is [28.36666666666667, 77.0, 1.0, 2.0, 0.612796159949826, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.9227031627431, 6.9112, 168.9127924827357, 1713392.931817634, 1705232.200184088, 367926.879824965], 
processed observation next is [1.0, 0.43478260869565216, 0.543443917851501, 0.77, 1.0, 1.0, 0.5334893493371397, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0011503162743100148, 0.0, 0.8294391397012266, 0.4759424810604539, 0.47367561116224666, 0.5491445967536791], 
reward next is 0.3933, 
noisyNet noise sample is [array([1.2928964], dtype=float32), 0.34486094]. 
=============================================
[2019-03-26 21:53:39,246] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4286506e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 21:53:39,259] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1893
[2019-03-26 21:53:39,267] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 68.33333333333334, 1.0, 2.0, 0.3963354577669586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591054.166666376, 591054.1666663768, 173704.2671741604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6727200.0000, 
sim time next is 6727800.0000, 
raw observation next is [26.61666666666667, 68.66666666666666, 1.0, 2.0, 0.3943057301243676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589150.8722051202, 589150.8722051202, 173563.4790899167], 
processed observation next is [1.0, 0.8695652173913043, 0.4605055292259086, 0.6866666666666665, 1.0, 1.0, 0.27024786761972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16365302005697782, 0.16365302005697782, 0.25904996879092046], 
reward next is 0.7410, 
noisyNet noise sample is [array([1.2691596], dtype=float32), 0.74955523]. 
=============================================
[2019-03-26 21:53:40,126] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 21:53:40,131] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:53:40,132] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:53:40,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:53:40,133] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:53:40,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:53:40,133] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:53:40,138] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:53:40,137] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:53:40,138] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:53:40,141] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:53:40,164] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-03-26 21:53:40,185] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-03-26 21:53:40,203] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-03-26 21:53:40,222] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-03-26 21:53:40,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-03-26 21:53:49,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.8518383]
[2019-03-26 21:53:49,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.61912804333333, 80.95728557666668, 1.0, 2.0, 0.2265821522168238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378087.7266210077, 378087.7266210077, 157975.294716962]
[2019-03-26 21:53:49,259] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:53:49,262] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2672552e-36 0.0000000e+00], sampled 0.1100442269275157
[2019-03-26 21:54:16,691] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.8518383]
[2019-03-26 21:54:16,692] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.13333333333334, 53.33333333333334, 1.0, 2.0, 0.5574073542780912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778919.5857397786, 778919.5857397786, 193079.2299053365]
[2019-03-26 21:54:16,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:54:16,696] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 4.45142e-34 0.00000e+00], sampled 0.8099583384079793
[2019-03-26 21:54:26,839] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.8518383]
[2019-03-26 21:54:26,840] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.66666666666666, 64.33333333333334, 1.0, 2.0, 0.912087379372175, 1.0, 2.0, 0.912087379372175, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2551137.496858804, 2551137.496858804, 478138.8232638175]
[2019-03-26 21:54:26,841] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:54:26,844] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.19641025e-29 1.00000000e+00 1.83434493e-34 5.87284128e-14
 0.00000000e+00], sampled 0.14154496121881688
[2019-03-26 21:54:26,845] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2551137.496858804 W.
[2019-03-26 21:55:30,679] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.5389 2927831873.2869 1336.0000
[2019-03-26 21:55:31,422] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.0448 3164129586.4931 1775.0000
[2019-03-26 21:55:31,594] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 21:55:31,768] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0759 3008237470.0170 1766.0000
[2019-03-26 21:55:31,798] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1740 2779530616.0520 926.0000
[2019-03-26 21:55:32,814] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1450000, evaluation results [1450000.0, 7878.044807110834, 3164129586.4930735, 1775.0, 8255.53885001214, 2927831873.2868857, 1336.0, 8662.173952134184, 2779530616.051981, 926.0, 7998.075909215718, 3008237470.0169654, 1766.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 21:55:33,792] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0171273e-21 9.9998319e-01 9.5338167e-26 1.6821612e-05 5.6140027e-30], sum to 1.0000
[2019-03-26 21:55:33,802] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5783
[2019-03-26 21:55:33,810] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1880819.571312416 W.
[2019-03-26 21:55:33,815] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.2, 60.0, 1.0, 2.0, 0.4484194242794746, 1.0, 2.0, 0.4484194242794746, 1.0, 2.0, 0.7564666367259351, 6.911199999999999, 6.9112, 170.5573041426782, 1880819.571312416, 1880819.571312416, 377672.8068608972], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6703200.0000, 
sim time next is 6703800.0000, 
raw observation next is [30.16666666666667, 60.5, 1.0, 2.0, 0.7939850753838157, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.962874287105697, 6.9112, 168.9126486742053, 2006655.33060217, 1969995.879647996, 407724.7042209035], 
processed observation next is [1.0, 0.6086956521739131, 0.6287519747235389, 0.605, 1.0, 1.0, 0.7517892474503803, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005167428710569677, 0.0, 0.8294384335355064, 0.5574042585006028, 0.547221077679999, 0.6085443346580649], 
reward next is 0.1331, 
noisyNet noise sample is [array([0.0812191], dtype=float32), -2.4321532]. 
=============================================
[2019-03-26 21:55:35,143] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1244394e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 21:55:35,151] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1829
[2019-03-26 21:55:35,155] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 67.0, 1.0, 2.0, 0.4079333296678194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603191.0876093928, 603191.0876093934, 174673.6715649245], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6724800.0000, 
sim time next is 6725400.0000, 
raw observation next is [27.08333333333334, 67.33333333333334, 1.0, 2.0, 0.4045694827339936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599843.3394211539, 599843.3394211545, 174411.3904310869], 
processed observation next is [1.0, 0.8695652173913043, 0.4826224328594, 0.6733333333333335, 1.0, 1.0, 0.2826138346192694, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16662314983920942, 0.16662314983920956, 0.2603155081060999], 
reward next is 0.7397, 
noisyNet noise sample is [array([-0.32624137], dtype=float32), -0.302063]. 
=============================================
[2019-03-26 21:55:43,374] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 8.85606e-24 0.00000e+00], sum to 1.0000
[2019-03-26 21:55:43,384] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6228
[2019-03-26 21:55:43,392] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 71.0, 1.0, 2.0, 0.705546750514573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1011798.27218437, 1011798.27218437, 225494.4398628975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7026000.0000, 
sim time next is 7026600.0000, 
raw observation next is [27.55, 70.0, 1.0, 2.0, 0.6918252489627995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 992878.5556813459, 992878.5556813465, 222552.263091632], 
processed observation next is [1.0, 0.30434782608695654, 0.504739336492891, 0.7, 1.0, 1.0, 0.6287051192322886, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27579959880037386, 0.275799598800374, 0.33216755685318206], 
reward next is 0.6678, 
noisyNet noise sample is [array([1.0781871], dtype=float32), 1.2294538]. 
=============================================
[2019-03-26 21:55:45,535] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3806709e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 21:55:45,548] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6804
[2019-03-26 21:55:45,555] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 84.66666666666667, 1.0, 2.0, 0.4787848660057578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 669018.1273309673, 669018.1273309673, 180338.8082001082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7150800.0000, 
sim time next is 7151400.0000, 
raw observation next is [26.1, 84.5, 1.0, 2.0, 0.4783634998231688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668429.1565326115, 668429.1565326108, 180275.4326957298], 
processed observation next is [1.0, 0.782608695652174, 0.4360189573459717, 0.845, 1.0, 1.0, 0.37152228894357686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1856747657035032, 0.185674765703503, 0.26906780999362656], 
reward next is 0.7309, 
noisyNet noise sample is [array([1.080224], dtype=float32), 1.2766646]. 
=============================================
[2019-03-26 21:55:47,299] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4752568e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 21:55:47,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6921
[2019-03-26 21:55:47,313] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 64.33333333333334, 1.0, 2.0, 0.4067242851163128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 601176.8986169929, 601176.8986169929, 174480.027484207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6988800.0000, 
sim time next is 6989400.0000, 
raw observation next is [27.65, 65.5, 1.0, 2.0, 0.4109523342994111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 605500.8511653418, 605500.8511653412, 174826.5594496014], 
processed observation next is [0.0, 0.9130434782608695, 0.509478672985782, 0.655, 1.0, 1.0, 0.29030401722820615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16819468087926162, 0.16819468087926145, 0.260935163357614], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.65629953], dtype=float32), -0.5600798]. 
=============================================
[2019-03-26 21:55:47,411] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.106992e-30 0.000000e+00], sum to 1.0000
[2019-03-26 21:55:47,419] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3981
[2019-03-26 21:55:47,424] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 44.0, 1.0, 2.0, 0.3082228783828225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490716.1005350184, 490716.1005350178, 166395.1013074031], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6868800.0000, 
sim time next is 6869400.0000, 
raw observation next is [28.98333333333333, 42.83333333333334, 1.0, 2.0, 0.2995691731458202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478574.1433093389, 478574.1433093389, 165540.6157849808], 
processed observation next is [0.0, 0.5217391304347826, 0.5726698262243285, 0.42833333333333345, 1.0, 1.0, 0.15610743752508457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13293726203037193, 0.13293726203037193, 0.24707554594773254], 
reward next is 0.7529, 
noisyNet noise sample is [array([-0.932365], dtype=float32), -0.8175811]. 
=============================================
[2019-03-26 21:55:52,611] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0429704e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:55:52,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7581
[2019-03-26 21:55:52,628] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 77.0, 1.0, 2.0, 0.5928815583752316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847976.1421943277, 847976.1421943277, 201930.7281508207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7022400.0000, 
sim time next is 7023000.0000, 
raw observation next is [26.65, 76.0, 1.0, 2.0, 0.5917320944556285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846554.1892061256, 846554.1892061256, 201742.0874032477], 
processed observation next is [1.0, 0.2608695652173913, 0.462085308056872, 0.76, 1.0, 1.0, 0.508110957175456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.235153941446146, 0.235153941446146, 0.3011075931391757], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.11520623], dtype=float32), 0.29005247]. 
=============================================
[2019-03-26 21:55:52,644] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[63.468185]
 [63.418106]
 [63.55935 ]
 [63.535015]
 [63.958183]], R is [[63.5989151 ]
 [63.66153717]
 [63.72216797]
 [63.78564835]
 [63.84078598]].
[2019-03-26 21:56:09,808] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9278358e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:56:09,815] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6161
[2019-03-26 21:56:09,822] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.51666666666667, 92.0, 1.0, 2.0, 0.3657264946643853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558837.8636058999, 558837.8636058999, 171265.5716764953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7246200.0000, 
sim time next is 7246800.0000, 
raw observation next is [22.5, 92.0, 1.0, 2.0, 0.3657914902121746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559219.1450910879, 559219.1450910879, 171306.429318115], 
processed observation next is [1.0, 0.9130434782608695, 0.2654028436018958, 0.92, 1.0, 1.0, 0.2358933617014152, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1553386514141911, 0.1553386514141911, 0.2556812377882313], 
reward next is 0.7443, 
noisyNet noise sample is [array([0.01403874], dtype=float32), -2.1340048]. 
=============================================
[2019-03-26 21:56:14,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.126408e-27 0.000000e+00], sum to 1.0000
[2019-03-26 21:56:14,466] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7615
[2019-03-26 21:56:14,473] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.53333333333333, 90.33333333333334, 1.0, 2.0, 0.3292589459520169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 519536.0258597115, 519536.0258597108, 168494.1916162801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7273200.0000, 
sim time next is 7273800.0000, 
raw observation next is [21.5, 90.5, 1.0, 2.0, 0.335819451357793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530053.8533534574, 530053.8533534574, 169321.492985033], 
processed observation next is [1.0, 0.17391304347826086, 0.21800947867298584, 0.905, 1.0, 1.0, 0.19978247151541326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1472371814870715, 0.1472371814870715, 0.2527186462463179], 
reward next is 0.7473, 
noisyNet noise sample is [array([2.182171], dtype=float32), -1.0296829]. 
=============================================
[2019-03-26 21:56:15,553] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2661135e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:56:15,562] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3127
[2019-03-26 21:56:15,575] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.43333333333334, 83.33333333333334, 1.0, 2.0, 0.2868760058878072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462313.4609190621, 462313.4609190621, 164430.9176967341], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7413600.0000, 
sim time next is 7414200.0000, 
raw observation next is [21.5, 83.0, 1.0, 2.0, 0.2872973960283477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462798.8579883144, 462798.8579883144, 164464.1637426504], 
processed observation next is [1.0, 0.8260869565217391, 0.21800947867298584, 0.83, 1.0, 1.0, 0.14132216388957552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12855523833008733, 0.12855523833008733, 0.24546890110843342], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.61138], dtype=float32), -0.7452536]. 
=============================================
[2019-03-26 21:56:16,898] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1860177e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 21:56:16,908] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5744
[2019-03-26 21:56:16,915] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4051850920970963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596411.0850560442, 596411.0850560435, 173962.7340567657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7509600.0000, 
sim time next is 7510200.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.4051719434541921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596370.3290199444, 596370.3290199444, 173958.2841457388], 
processed observation next is [0.0, 0.9565217391304348, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.2833396909086652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16565842472776235, 0.16565842472776235, 0.2596392300682669], 
reward next is 0.7404, 
noisyNet noise sample is [array([2.87178], dtype=float32), 0.13618292]. 
=============================================
[2019-03-26 21:56:28,545] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 21:56:28,546] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:56:28,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:56:28,547] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:56:28,548] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:56:28,548] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:56:28,549] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:56:28,550] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:56:28,551] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:56:28,550] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:56:28,553] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:56:28,587] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-03-26 21:56:28,608] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-03-26 21:56:28,628] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-03-26 21:56:28,628] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-03-26 21:56:28,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-03-26 21:56:59,936] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.81088954]
[2019-03-26 21:56:59,939] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.03333333333333, 80.33333333333333, 1.0, 2.0, 0.5604960811354082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783237.3559466929, 783237.3559466923, 193617.3095072526]
[2019-03-26 21:56:59,941] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:56:59,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9163714e-28 0.0000000e+00], sampled 0.9658637513528376
[2019-03-26 21:57:24,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.81088954]
[2019-03-26 21:57:24,953] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.760142185, 80.973508895, 1.0, 2.0, 0.656048726376772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916820.2745130569, 916820.2745130569, 211654.0116526626]
[2019-03-26 21:57:24,955] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:57:24,958] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.918604e-27 0.000000e+00], sampled 0.5421479987974904
[2019-03-26 21:57:27,278] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.81088954]
[2019-03-26 21:57:27,279] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.32836726, 74.46771684333333, 1.0, 2.0, 0.9114762658612691, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005988491620197, 6.9112, 168.9123159540104, 2171100.221827048, 2103854.258516558, 437277.4042700849]
[2019-03-26 21:57:27,280] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:57:27,281] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2724475e-25 9.9999392e-01 1.1811211e-29 6.1304381e-06 1.3101250e-33], sampled 0.5636457673994699
[2019-03-26 21:57:27,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2171100.221827048 W.
[2019-03-26 21:57:52,417] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.81088954]
[2019-03-26 21:57:52,418] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.89586717, 93.84891989, 1.0, 2.0, 0.4918189625143546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696270.3534816583, 696270.3534816583, 183481.6915702258]
[2019-03-26 21:57:52,419] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 21:57:52,421] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8955052e-28 0.0000000e+00], sampled 0.6534538003240873
[2019-03-26 21:58:22,313] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.8866 2927190142.1495 1318.0000
[2019-03-26 21:58:22,542] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1992 3008241322.0233 1768.0000
[2019-03-26 21:58:22,618] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.81088954]
[2019-03-26 21:58:22,619] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.53333333333333, 65.0, 1.0, 2.0, 0.7115610528041323, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.979912583764992, 6.9112, 168.9124897733965, 1891306.922935534, 1842559.985805, 388102.8777806655]
[2019-03-26 21:58:22,619] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 21:58:22,622] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2195987e-35 1.0000000e+00 0.0000000e+00 5.9932489e-16 0.0000000e+00], sampled 0.32949871481019655
[2019-03-26 21:58:22,624] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1891306.922935534 W.
[2019-03-26 21:58:22,634] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7287 2843176692.4016 1126.0000
[2019-03-26 21:58:22,635] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8666.2913 2779240214.1029 911.0000
[2019-03-26 21:58:22,727] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.0572 3164216232.4733 1758.0000
[2019-03-26 21:58:23,748] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1475000, evaluation results [1475000.0, 7883.057191900259, 3164216232.4732656, 1758.0, 8263.886626320176, 2927190142.149456, 1318.0, 8666.291298885964, 2779240214.102923, 911.0, 7998.199150697798, 3008241322.023315, 1768.0, 8497.728728170274, 2843176692.4015822, 1126.0]
[2019-03-26 21:58:28,876] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.124644e-30 0.000000e+00], sum to 1.0000
[2019-03-26 21:58:28,883] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6095
[2019-03-26 21:58:28,888] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 91.5, 1.0, 2.0, 0.4819867045926162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673493.5551222712, 673493.5551222712, 180821.801901838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7597800.0000, 
sim time next is 7598400.0000, 
raw observation next is [25.13333333333333, 92.0, 1.0, 2.0, 0.4824504968940798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674141.8307629039, 674141.8307629032, 180891.9996816422], 
processed observation next is [0.0, 0.9565217391304348, 0.3902053712480251, 0.92, 1.0, 1.0, 0.37644638180009615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1872616196563622, 0.187261619656362, 0.2699880592263316], 
reward next is 0.7300, 
noisyNet noise sample is [array([-1.4408172], dtype=float32), 0.58940196]. 
=============================================
[2019-03-26 21:58:29,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:29,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:29,175] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-03-26 21:58:29,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:29,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:29,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-03-26 21:58:30,295] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.3443252e-33 1.0000000e+00 6.6259568e-38 9.8559695e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:30,304] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4526
[2019-03-26 21:58:30,309] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2201513.907769398 W.
[2019-03-26 21:58:30,314] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 59.66666666666667, 1.0, 2.0, 0.9332050671494306, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98448846196521, 6.9112, 168.9125208822681, 2201513.907769398, 2149520.684548461, 444210.9268757968], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7742400.0000, 
sim time next is 7743000.0000, 
raw observation next is [31.7, 59.83333333333333, 1.0, 2.0, 0.7957843901510211, 1.0, 1.0, 0.7957843901510211, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2225544.648865488, 2225544.648865488, 417818.844995719], 
processed observation next is [1.0, 0.6086956521739131, 0.7014218009478673, 0.5983333333333333, 1.0, 1.0, 0.7539570965674953, 1.0, 0.5, 0.7539570965674953, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6182068469070799, 0.6182068469070799, 0.6236102164115208], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1245725], dtype=float32), 0.286298]. 
=============================================
[2019-03-26 21:58:30,328] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[39.772243]
 [39.937225]
 [38.122192]
 [36.81658 ]
 [36.404804]], R is [[37.18984985]
 [36.8179512 ]
 [36.88845825]
 [36.95373917]
 [37.00406265]].
[2019-03-26 21:58:34,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:34,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:34,270] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-03-26 21:58:35,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.9403036e-25 1.0000000e+00 1.4769138e-28 2.5576210e-11 2.5801111e-33], sum to 1.0000
[2019-03-26 21:58:35,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2143
[2019-03-26 21:58:35,703] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2205143.742921144 W.
[2019-03-26 21:58:35,709] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.55, 62.66666666666666, 1.0, 2.0, 0.7884964944183039, 1.0, 1.0, 0.7884964944183039, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2205143.742921144, 2205143.742921144, 414307.4088040201], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7733400.0000, 
sim time next is 7734000.0000, 
raw observation next is [31.6, 62.33333333333334, 1.0, 2.0, 0.7033424602570229, 1.0, 2.0, 0.7033424602570229, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1966779.632958987, 1966779.632958987, 375557.7857356243], 
processed observation next is [1.0, 0.5217391304347826, 0.6966824644549764, 0.6233333333333334, 1.0, 1.0, 0.6425812774180998, 1.0, 1.0, 0.6425812774180998, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5463276758219409, 0.5463276758219409, 0.5605340085606333], 
reward next is 0.4395, 
noisyNet noise sample is [array([-0.35661277], dtype=float32), -0.24511516]. 
=============================================
[2019-03-26 21:58:35,719] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[32.33656 ]
 [32.378063]
 [32.19761 ]
 [32.18463 ]
 [32.24612 ]], R is [[35.99461746]
 [36.0163002 ]
 [35.65613937]
 [35.29957962]
 [34.94658279]].
[2019-03-26 21:58:36,158] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8682131e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:36,166] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6913
[2019-03-26 21:58:36,178] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.98333333333333, 91.0, 1.0, 2.0, 0.4655241691031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703979.4639552551, 703979.4639552551, 184942.7072030428], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 114600.0000, 
sim time next is 115200.0000, 
raw observation next is [23.0, 91.0, 1.0, 2.0, 0.488432563251943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738230.8209667326, 738230.8209667332, 188631.9471678109], 
processed observation next is [1.0, 0.34782608695652173, 0.28909952606635075, 0.91, 1.0, 1.0, 0.3836536906649916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2050641169352035, 0.20506411693520366, 0.2815402196534491], 
reward next is 0.7185, 
noisyNet noise sample is [array([-0.7879407], dtype=float32), 1.2497447]. 
=============================================
[2019-03-26 21:58:37,763] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7082228e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:37,774] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6467
[2019-03-26 21:58:37,779] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 96.0, 1.0, 2.0, 0.7789358315697448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167888.554337311, 1167888.554337311, 249431.4727295514], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 144000.0000, 
sim time next is 144600.0000, 
raw observation next is [22.58333333333334, 96.0, 1.0, 2.0, 0.8156846133887776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1223659.216140579, 1223659.216140578, 259213.1658876538], 
processed observation next is [1.0, 0.6956521739130435, 0.2693522906793052, 0.96, 1.0, 1.0, 0.7779332691431056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33990533781682747, 0.33990533781682725, 0.3868853222203788], 
reward next is 0.6131, 
noisyNet noise sample is [array([-0.07336606], dtype=float32), -0.46050137]. 
=============================================
[2019-03-26 21:58:38,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:38,221] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:38,292] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-03-26 21:58:39,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:39,769] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:39,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-03-26 21:58:41,715] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:41,717] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:41,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-03-26 21:58:42,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:42,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:42,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-03-26 21:58:43,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:43,846] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:43,932] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-03-26 21:58:44,540] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:44,541] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:44,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-03-26 21:58:46,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:46,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:46,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-03-26 21:58:46,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:46,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:46,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:46,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:46,919] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-03-26 21:58:46,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-03-26 21:58:47,049] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:47,052] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:47,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-03-26 21:58:47,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:47,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:47,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-03-26 21:58:47,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:47,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:47,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-03-26 21:58:47,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 21:58:47,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:58:47,783] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-03-26 21:58:50,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6268022e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:50,595] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9583
[2019-03-26 21:58:50,601] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.8, 96.0, 1.0, 2.0, 0.2836317183056609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457673.3082664762, 457673.3082664762, 164114.6362203672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 190800.0000, 
sim time next is 191400.0000, 
raw observation next is [19.83333333333333, 95.83333333333333, 1.0, 2.0, 0.2835703086503259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457537.0063997328, 457537.0063997328, 164105.5407879313], 
processed observation next is [0.0, 0.21739130434782608, 0.13902053712480236, 0.9583333333333333, 1.0, 1.0, 0.13683169716906735, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12709361288881466, 0.12709361288881466, 0.24493364296706163], 
reward next is 0.7551, 
noisyNet noise sample is [array([-0.87624633], dtype=float32), -1.9018135]. 
=============================================
[2019-03-26 21:58:52,062] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6766995e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:52,072] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6163
[2019-03-26 21:58:52,080] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2855877156481978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458898.6671830605, 458898.6671830605, 164194.8710287105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 259200.0000, 
sim time next is 259800.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2860133779352211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459525.246932158, 459525.246932158, 164237.2160093909], 
processed observation next is [0.0, 0.0, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13977515413882058, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12764590192559944, 0.12764590192559944, 0.24513017314834462], 
reward next is 0.7549, 
noisyNet noise sample is [array([1.6185669], dtype=float32), -0.35124698]. 
=============================================
[2019-03-26 21:58:53,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.2162057e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 21:58:53,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1950
[2019-03-26 21:58:53,662] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 65.0, 1.0, 2.0, 0.2424079751973434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 399628.108440928, 399628.1084409273, 160144.5748213432], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 498600.0000, 
sim time next is 499200.0000, 
raw observation next is [22.2, 66.33333333333334, 1.0, 2.0, 0.2401018453830852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395993.0779871152, 395993.0779871159, 159918.0543452117], 
processed observation next is [1.0, 0.782608695652174, 0.2511848341232228, 0.6633333333333334, 1.0, 1.0, 0.0844600546784159, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1099980772186431, 0.1099980772186433, 0.23868366320180853], 
reward next is 0.7613, 
noisyNet noise sample is [array([1.861194], dtype=float32), 0.114015356]. 
=============================================
[2019-03-26 21:58:56,725] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 4.18323e-30 0.00000e+00], sum to 1.0000
[2019-03-26 21:58:56,731] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9184
[2019-03-26 21:58:56,739] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 96.0, 1.0, 2.0, 0.3851173570638002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 580498.9012723201, 580498.9012723208, 172932.974181683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 153000.0000, 
sim time next is 153600.0000, 
raw observation next is [22.43333333333333, 96.0, 1.0, 2.0, 0.3820765904361098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576234.0410331846, 576234.0410331846, 172561.660346285], 
processed observation next is [1.0, 0.782608695652174, 0.2622432859399683, 0.96, 1.0, 1.0, 0.25551396438085516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16006501139810683, 0.16006501139810683, 0.25755471693475374], 
reward next is 0.7424, 
noisyNet noise sample is [array([1.2776525], dtype=float32), 1.878377]. 
=============================================
[2019-03-26 21:59:00,571] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.722355e-31 0.000000e+00], sum to 1.0000
[2019-03-26 21:59:00,577] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5937
[2019-03-26 21:59:00,581] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 92.66666666666667, 1.0, 2.0, 0.3048412396151696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486642.5017893348, 486642.5017893342, 166115.4785106497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 210000.0000, 
sim time next is 210600.0000, 
raw observation next is [20.75, 92.5, 1.0, 2.0, 0.3023365824055306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482675.1279515924, 482675.1279515917, 165830.0677799411], 
processed observation next is [0.0, 0.43478260869565216, 0.18246445497630337, 0.925, 1.0, 1.0, 0.15944166554883202, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13407642443099788, 0.13407642443099768, 0.24750756385065836], 
reward next is 0.7525, 
noisyNet noise sample is [array([-0.48076126], dtype=float32), 0.84894043]. 
=============================================
[2019-03-26 21:59:01,912] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.366659e-33 0.000000e+00], sum to 1.0000
[2019-03-26 21:59:01,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6632
[2019-03-26 21:59:01,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 84.33333333333333, 1.0, 2.0, 0.2906628384787346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465296.2872185719, 465296.2872185726, 164619.6363289767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 328200.0000, 
sim time next is 328800.0000, 
raw observation next is [21.56666666666667, 84.66666666666667, 1.0, 2.0, 0.2895384935202305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463709.1940861639, 463709.1940861645, 164512.247423915], 
processed observation next is [0.0, 0.8260869565217391, 0.22116903633491333, 0.8466666666666667, 1.0, 1.0, 0.14402228134967526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12880810946837887, 0.12880810946837903, 0.24554066779688805], 
reward next is 0.7545, 
noisyNet noise sample is [array([0.36387032], dtype=float32), 1.1849092]. 
=============================================
[2019-03-26 21:59:05,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5498762e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 21:59:05,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9104
[2019-03-26 21:59:05,276] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.13333333333333, 77.33333333333334, 1.0, 2.0, 0.3051059986316448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 483043.7674686298, 483043.7674686298, 165789.0089884317], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 298200.0000, 
sim time next is 298800.0000, 
raw observation next is [23.2, 77.0, 1.0, 2.0, 0.3059573726402128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484192.1809023133, 484192.1809023133, 165867.9709453608], 
processed observation next is [0.0, 0.4782608695652174, 0.29857819905213273, 0.77, 1.0, 1.0, 0.1638040634219431, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13449782802842036, 0.13449782802842036, 0.2475641357393445], 
reward next is 0.7524, 
noisyNet noise sample is [array([-1.1196814], dtype=float32), -1.0163656]. 
=============================================
[2019-03-26 21:59:06,645] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.726873e-32 0.000000e+00], sum to 1.0000
[2019-03-26 21:59:06,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7587
[2019-03-26 21:59:06,659] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 84.0, 1.0, 2.0, 0.2923484757194712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467795.4244397502, 467795.4244397502, 164790.8867092818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 327600.0000, 
sim time next is 328200.0000, 
raw observation next is [21.63333333333333, 84.33333333333333, 1.0, 2.0, 0.2906628384787346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 465296.2872185719, 465296.2872185726, 164619.6363289767], 
processed observation next is [0.0, 0.8260869565217391, 0.2243285939968403, 0.8433333333333333, 1.0, 1.0, 0.1453769138298007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12924896867182553, 0.12924896867182573, 0.24570094974474133], 
reward next is 0.7543, 
noisyNet noise sample is [array([0.17089576], dtype=float32), 0.8104344]. 
=============================================
[2019-03-26 21:59:16,677] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 21:59:16,679] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 21:59:16,680] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:59:16,681] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 21:59:16,682] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:59:16,682] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 21:59:16,683] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 21:59:16,684] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:59:16,685] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:59:16,684] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 21:59:16,689] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 21:59:17,276] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-03-26 21:59:17,494] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-03-26 21:59:17,515] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-03-26 21:59:17,555] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-03-26 21:59:17,615] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-03-26 21:59:22,711] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 21:59:22,712] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.45889226666667, 91.57431577333334, 1.0, 2.0, 0.3023344205127972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 486012.2557562586, 486012.2557562586, 166089.7030396076]
[2019-03-26 21:59:22,713] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 21:59:22,719] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.35344125e-30
 0.00000000e+00], sampled 0.19578062569116295
[2019-03-26 21:59:54,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 21:59:54,351] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.61666666666667, 82.50000000000001, 1.0, 2.0, 0.4573112167023423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655450.6032004553, 655450.6032004553, 179286.7161106631]
[2019-03-26 21:59:54,352] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 21:59:54,355] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0176603e-29 0.0000000e+00], sampled 0.8721892502719721
[2019-03-26 22:00:31,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 22:00:31,539] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.85, 66.5, 1.0, 2.0, 0.6159125871725945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 860707.7708615352, 860707.7708615352, 203747.6255785714]
[2019-03-26 22:00:31,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:00:31,543] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8648078e-29 0.0000000e+00], sampled 0.6493172604542474
[2019-03-26 22:00:36,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 22:00:36,954] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.67369503666667, 75.33954938666668, 1.0, 2.0, 0.5988418328327985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837300.0877075377, 837300.0877075377, 200582.2021871126]
[2019-03-26 22:00:36,955] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:00:36,957] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.3058443e-29 0.0000000e+00], sampled 0.9210646818395599
[2019-03-26 22:01:07,548] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 22:01:07,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.00854528666667, 93.26316958166667, 1.0, 2.0, 0.3965311609856338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 591945.5830148972, 591945.5830148979, 173804.0233866617]
[2019-03-26 22:01:07,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:01:07,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.6479978e-29 0.0000000e+00], sampled 0.35474816098901796
[2019-03-26 22:01:11,781] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.77259934]
[2019-03-26 22:01:11,781] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.29927509833333, 79.15290220333334, 1.0, 2.0, 0.5342966889777204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746613.452362635, 746613.452362635, 189143.826630537]
[2019-03-26 22:01:11,783] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:01:11,788] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.2254534e-30 0.0000000e+00], sampled 0.3156287895962955
[2019-03-26 22:01:12,036] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.4951 2779831775.4409 925.0000
[2019-03-26 22:01:12,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5116 3008615590.0924 1766.0000
[2019-03-26 22:01:12,373] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.5268 2927885838.7004 1327.0000
[2019-03-26 22:01:12,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.8829 3164639021.3860 1775.0000
[2019-03-26 22:01:12,622] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5068 2843430346.4143 1131.0000
[2019-03-26 22:01:13,640] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1500000, evaluation results [1500000.0, 7874.8829052625315, 3164639021.385952, 1775.0, 8259.526848340773, 2927885838.7003746, 1327.0, 8659.495143010496, 2779831775.4408727, 925.0, 7997.511550894106, 3008615590.092444, 1766.0, 8493.50677167813, 2843430346.4142575, 1131.0]
[2019-03-26 22:01:15,772] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.709267e-27 0.000000e+00], sum to 1.0000
[2019-03-26 22:01:15,780] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1698
[2019-03-26 22:01:15,785] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 73.0, 1.0, 2.0, 0.3160070917888493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496657.4922854217, 496657.4922854217, 166706.1415441102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 924000.0000, 
sim time next is 924600.0000, 
raw observation next is [24.05, 73.5, 1.0, 2.0, 0.3166512713863182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497365.1924567967, 497365.1924567967, 166751.2736341544], 
processed observation next is [0.0, 0.6956521739130435, 0.3388625592417062, 0.735, 1.0, 1.0, 0.17668827877869664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13815699790466576, 0.13815699790466576, 0.24888249796142448], 
reward next is 0.7511, 
noisyNet noise sample is [array([1.6232138], dtype=float32), -0.8475021]. 
=============================================
[2019-03-26 22:01:28,022] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.426688e-32 0.000000e+00], sum to 1.0000
[2019-03-26 22:01:28,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4771
[2019-03-26 22:01:28,035] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 75.66666666666666, 1.0, 2.0, 0.2543079736086176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 418575.0686870357, 418575.0686870363, 161327.9664394889], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 718800.0000, 
sim time next is 719400.0000, 
raw observation next is [21.28333333333333, 74.83333333333334, 1.0, 2.0, 0.2638325361547078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433725.1113663855, 433725.1113663848, 162303.5312655763], 
processed observation next is [1.0, 0.30434782608695654, 0.20774091627172192, 0.7483333333333334, 1.0, 1.0, 0.11305124837916602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12047919760177374, 0.12047919760177354, 0.24224407651578553], 
reward next is 0.7578, 
noisyNet noise sample is [array([0.24727316], dtype=float32), -0.14273168]. 
=============================================
[2019-03-26 22:01:30,156] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.213681e-29 0.000000e+00], sum to 1.0000
[2019-03-26 22:01:30,168] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9075
[2019-03-26 22:01:30,173] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 85.0, 1.0, 2.0, 0.2564289679765918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 421080.2615530139, 421080.2615530132, 161551.1090656208], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 770400.0000, 
sim time next is 771000.0000, 
raw observation next is [19.93333333333333, 85.50000000000001, 1.0, 2.0, 0.2570120727208782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 422062.4172073957, 422062.4172073957, 161609.8068210982], 
processed observation next is [1.0, 0.9565217391304348, 0.14375987361769343, 0.8550000000000001, 1.0, 1.0, 0.10483382255527496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1172395603353877, 0.1172395603353877, 0.2412086668971615], 
reward next is 0.7588, 
noisyNet noise sample is [array([-0.22033434], dtype=float32), 1.5009191]. 
=============================================
[2019-03-26 22:01:30,194] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[75.05916 ]
 [74.985695]
 [75.031334]
 [74.9876  ]
 [74.89179 ]], R is [[74.99331665]
 [75.00226593]
 [75.0109787 ]
 [75.01954651]
 [75.02809143]].
[2019-03-26 22:01:32,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.758744e-27 0.000000e+00], sum to 1.0000
[2019-03-26 22:01:32,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3700
[2019-03-26 22:01:32,404] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 63.0, 1.0, 2.0, 0.2904307138657254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 465128.1484360214, 465128.1484360207, 164610.1315868421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 824400.0000, 
sim time next is 825000.0000, 
raw observation next is [24.76666666666667, 63.0, 1.0, 2.0, 0.2898091305429463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464338.8374976719, 464338.8374976725, 164557.6143666503], 
processed observation next is [0.0, 0.5652173913043478, 0.3728278041074251, 0.63, 1.0, 1.0, 0.14434835005174254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12898301041601998, 0.12898301041602014, 0.24560837965171686], 
reward next is 0.7544, 
noisyNet noise sample is [array([-2.1844678], dtype=float32), 0.2597546]. 
=============================================
[2019-03-26 22:01:32,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[78.10203 ]
 [77.9817  ]
 [77.96058 ]
 [77.920685]
 [77.87742 ]], R is [[78.07777405]
 [78.05130768]
 [78.02529144]
 [77.99967194]
 [77.97429657]].
[2019-03-26 22:01:36,357] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7074856e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:36,364] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2139
[2019-03-26 22:01:36,370] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.73333333333333, 96.66666666666667, 1.0, 2.0, 0.3568415315265809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548783.4748986302, 548783.4748986296, 170518.6940436217], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1016400.0000, 
sim time next is 1017000.0000, 
raw observation next is [21.75, 96.5, 1.0, 2.0, 0.3569259185812872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548965.748357719, 548965.7483577196, 170535.4452441361], 
processed observation next is [1.0, 0.782608695652174, 0.2298578199052133, 0.965, 1.0, 1.0, 0.22521195009793638, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15249048565492196, 0.15249048565492213, 0.25453051528975534], 
reward next is 0.7455, 
noisyNet noise sample is [array([1.4218165], dtype=float32), -0.8254868]. 
=============================================
[2019-03-26 22:01:36,399] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.3273 ]
 [72.41467]
 [72.54016]
 [72.42615]
 [72.50473]], R is [[72.36301422]
 [72.38487244]
 [72.40596008]
 [72.42733765]
 [72.44849396]].
[2019-03-26 22:01:36,551] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7766094e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:36,561] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5805
[2019-03-26 22:01:36,569] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2926940999104102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 467221.1734289101, 467221.1734289107, 164736.5207204623], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 894000.0000, 
sim time next is 894600.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2930747842748531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 467827.9844454972, 467827.9844454965, 164778.8066060361], 
processed observation next is [0.0, 0.34782608695652173, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14828287262030496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.129952217901527, 0.1299522179015268, 0.24593851732244193], 
reward next is 0.7541, 
noisyNet noise sample is [array([1.5334408], dtype=float32), 0.03620224]. 
=============================================
[2019-03-26 22:01:40,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0045733e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:40,946] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6204
[2019-03-26 22:01:40,952] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.66666666666666, 1.0, 2.0, 0.340495761810958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526394.0971710646, 526394.0971710652, 168768.8884986565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 952800.0000, 
sim time next is 953400.0000, 
raw observation next is [21.8, 94.83333333333333, 1.0, 2.0, 0.341666743957812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527903.3872326761, 527903.3872326761, 168880.8325417688], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.9483333333333333, 1.0, 1.0, 0.20682740235880961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1466398297868545, 0.1466398297868545, 0.2520609440921922], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.99792075], dtype=float32), -1.6282135]. 
=============================================
[2019-03-26 22:01:43,436] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2317465e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:43,440] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9789
[2019-03-26 22:01:43,448] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 58.33333333333333, 1.0, 2.0, 0.9201992396255323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1410577.967924249, 1410577.96792425, 293094.8122768563], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1179600.0000, 
sim time next is 1180200.0000, 
raw observation next is [27.6, 58.16666666666666, 1.0, 2.0, 0.9141797596363661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1402541.308959579, 1402541.308959579, 291385.2435093187], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.5816666666666666, 1.0, 1.0, 0.8966021200438146, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3895948080443275, 0.3895948080443275, 0.43490334852137114], 
reward next is 0.5651, 
noisyNet noise sample is [array([1.284673], dtype=float32), 0.18884884]. 
=============================================
[2019-03-26 22:01:47,814] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1106137e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:47,824] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7088
[2019-03-26 22:01:47,828] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 93.0, 1.0, 2.0, 0.3826004951318505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576206.3941894145, 576206.3941894138, 172534.4013985867], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1456200.0000, 
sim time next is 1456800.0000, 
raw observation next is [22.8, 93.33333333333334, 1.0, 2.0, 0.3817276113525269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575046.238524547, 575046.2385245475, 172435.9016830741], 
processed observation next is [0.0, 0.8695652173913043, 0.2796208530805688, 0.9333333333333335, 1.0, 1.0, 0.2550935076536469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1597350662568186, 0.15973506625681877, 0.257367017437424], 
reward next is 0.7426, 
noisyNet noise sample is [array([0.91047895], dtype=float32), 1.1280521]. 
=============================================
[2019-03-26 22:01:48,913] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6490663e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:48,922] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9958
[2019-03-26 22:01:48,927] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 61.0, 1.0, 2.0, 0.9160060260170056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1385697.653478942, 1385697.653478942, 289429.0482113513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1173600.0000, 
sim time next is 1174200.0000, 
raw observation next is [27.6, 60.66666666666667, 1.0, 2.0, 0.9738410104176051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1474473.84463667, 1474473.844636671, 307746.788348244], 
processed observation next is [1.0, 0.6086956521739131, 0.5071090047393366, 0.6066666666666667, 1.0, 1.0, 0.9684831450814518, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4095760679546306, 0.40957606795463086, 0.4593235646988716], 
reward next is 0.5407, 
noisyNet noise sample is [array([-1.351769], dtype=float32), -0.55563134]. 
=============================================
[2019-03-26 22:01:52,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6156635e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:52,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3786
[2019-03-26 22:01:52,081] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 57.66666666666667, 1.0, 2.0, 0.3549844035424509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544589.4044788288, 544589.4044788294, 170129.0385365109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1527600.0000, 
sim time next is 1528200.0000, 
raw observation next is [27.55, 58.0, 1.0, 2.0, 0.3522676077483729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541603.219326074, 541603.219326074, 169917.3253098239], 
processed observation next is [0.0, 0.6956521739130435, 0.504739336492891, 0.58, 1.0, 1.0, 0.21959952740767819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15044533870168722, 0.15044533870168722, 0.25360794822361776], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.2628233], dtype=float32), -0.25794783]. 
=============================================
[2019-03-26 22:01:54,795] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4632904e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 22:01:54,805] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4325
[2019-03-26 22:01:54,809] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 94.0, 1.0, 2.0, 0.4570678034078495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649487.8429542875, 649487.8429542875, 178537.2880311009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1301400.0000, 
sim time next is 1302000.0000, 
raw observation next is [24.23333333333333, 94.0, 1.0, 2.0, 0.4560888869921874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648564.8309758569, 648564.8309758569, 178453.6880327009], 
processed observation next is [1.0, 0.043478260869565216, 0.3475513428120062, 0.94, 1.0, 1.0, 0.3446854060146836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18015689749329358, 0.18015689749329358, 0.2663487881085088], 
reward next is 0.7337, 
noisyNet noise sample is [array([-0.12298106], dtype=float32), 0.76333135]. 
=============================================
[2019-03-26 22:01:54,819] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[76.57157]
 [76.52213]
 [76.41363]
 [76.36429]
 [76.2023 ]], R is [[76.55073547]
 [76.51876068]
 [76.48696899]
 [76.45537567]
 [76.42401123]].
[2019-03-26 22:02:09,271] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 22:02:09,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:02:09,275] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:02:09,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:02:09,277] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:02:09,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:02:09,277] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:02:09,279] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:02:09,280] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:02:09,280] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:02:09,282] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:02:09,309] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-03-26 22:02:09,330] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-03-26 22:02:09,353] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-03-26 22:02:09,354] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-03-26 22:02:09,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-03-26 22:02:24,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:02:24,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.95041185666667, 96.845376645, 1.0, 2.0, 0.4113670034383185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608267.548884563, 608267.548884563, 175147.9841516674]
[2019-03-26 22:02:24,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:02:24,485] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.748654e-21 0.000000e+00], sampled 0.7357985117747885
[2019-03-26 22:02:30,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:02:30,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.55, 47.0, 1.0, 2.0, 0.2905051374023661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 468007.6158141499, 468007.6158141499, 164822.1633143295]
[2019-03-26 22:02:30,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:02:30,503] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.9140094e-23 0.0000000e+00], sampled 0.1936583420266631
[2019-03-26 22:03:18,795] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:03:18,796] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.08333333333334, 54.66666666666666, 1.0, 2.0, 0.5341477129644158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 746405.2036933221, 746405.2036933221, 189119.260246572]
[2019-03-26 22:03:18,798] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:03:18,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.1386325e-23 0.0000000e+00], sampled 0.7423471390060354
[2019-03-26 22:03:20,682] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:03:20,683] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.3, 83.0, 1.0, 2.0, 0.5961650531864768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 833100.7333235051, 833100.7333235051, 200033.018305121]
[2019-03-26 22:03:20,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:03:20,687] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4284079e-21 0.0000000e+00], sampled 0.36393691661204675
[2019-03-26 22:03:30,580] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:03:30,583] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.03860333, 56.50557649, 1.0, 2.0, 0.4869675730668483, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680455.6915932068, 680455.6915932068, 181578.906656408]
[2019-03-26 22:03:30,585] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:03:30,588] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2919016e-22 0.0000000e+00], sampled 0.864722740049745
[2019-03-26 22:03:36,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.7208604]
[2019-03-26 22:03:36,325] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.8, 56.83333333333333, 1.0, 2.0, 0.7871750525500145, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.004843777033104, 6.9112, 168.9123224078661, 1997124.563356044, 1930690.694353679, 404727.8111435355]
[2019-03-26 22:03:36,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:03:36,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1813109e-38 1.0000000e+00 0.0000000e+00 1.6780391e-11 0.0000000e+00], sampled 0.8713149952887457
[2019-03-26 22:03:36,331] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1997124.563356044 W.
[2019-03-26 22:04:03,117] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8258.6919 2928094108.9632 1328.0000
[2019-03-26 22:04:03,154] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.3168 3008746041.1940 1766.0000
[2019-03-26 22:04:03,604] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8491.2394 2843610635.0832 1131.0000
[2019-03-26 22:04:03,612] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6712 2779981992.1168 924.0000
[2019-03-26 22:04:03,730] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7872.6220 3164826380.7574 1775.0000
[2019-03-26 22:04:04,748] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1525000, evaluation results [1525000.0, 7872.621951066925, 3164826380.7573576, 1775.0, 8258.69194646059, 2928094108.9631786, 1328.0, 8657.671160779117, 2779981992.116802, 924.0, 7997.316847757386, 3008746041.194046, 1766.0, 8491.239354585101, 2843610635.0831733, 1131.0]
[2019-03-26 22:04:04,904] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7488667e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:04,911] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3223
[2019-03-26 22:04:04,916] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.23333333333333, 99.00000000000001, 1.0, 2.0, 0.5172756652396943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747466.6913259211, 747466.6913259217, 189472.1891914051], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1650000.0000, 
sim time next is 1650600.0000, 
raw observation next is [23.25, 99.0, 1.0, 2.0, 0.4681029145885947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675971.529121182, 675971.5291211813, 181536.7750607812], 
processed observation next is [1.0, 0.08695652173913043, 0.30094786729857825, 0.99, 1.0, 1.0, 0.3591601380585479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18776986920032834, 0.18776986920032815, 0.2709504105384794], 
reward next is 0.7290, 
noisyNet noise sample is [array([-0.9748265], dtype=float32), -0.038107224]. 
=============================================
[2019-03-26 22:04:08,118] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9753231e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:08,131] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5799
[2019-03-26 22:04:08,137] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 96.0, 1.0, 2.0, 0.349355125073672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 539403.3320129109, 539403.3320129103, 169803.4621697461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [21.63333333333334, 96.0, 1.0, 2.0, 0.3475423436181706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537115.7424958579, 537115.7424958579, 169630.9236938214], 
processed observation next is [0.0, 0.0, 0.2243285939968408, 0.96, 1.0, 1.0, 0.21390643809418144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14919881735996052, 0.14919881735996052, 0.25318048312510655], 
reward next is 0.7468, 
noisyNet noise sample is [array([1.1712956], dtype=float32), 2.2798252]. 
=============================================
[2019-03-26 22:04:10,090] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5714127e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:10,100] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1835
[2019-03-26 22:04:10,106] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 62.83333333333333, 1.0, 2.0, 0.3593082526810152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548245.3583190968, 548245.3583190962, 170342.747135036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1504200.0000, 
sim time next is 1504800.0000, 
raw observation next is [27.2, 61.0, 1.0, 2.0, 0.357913139627097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547127.1069043688, 547127.1069043695, 170280.8052184762], 
processed observation next is [0.0, 0.43478260869565216, 0.4881516587677725, 0.61, 1.0, 1.0, 0.22640137304469515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15197975191788024, 0.15197975191788043, 0.25415045554996446], 
reward next is 0.7458, 
noisyNet noise sample is [array([0.25095162], dtype=float32), -0.24153833]. 
=============================================
[2019-03-26 22:04:10,329] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.001467e-22 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:10,339] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4332
[2019-03-26 22:04:10,347] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 85.0, 1.0, 2.0, 0.6383729827092052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 958392.0700820248, 958392.0700820254, 216470.8191801312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1602000.0000, 
sim time next is 1602600.0000, 
raw observation next is [24.01666666666667, 85.00000000000001, 1.0, 2.0, 0.650490053267038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975997.7207925948, 975997.7207925948, 219010.5520290937], 
processed observation next is [1.0, 0.5652173913043478, 0.33728278041074267, 0.8500000000000001, 1.0, 1.0, 0.5789036786349855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.271110477997943, 0.271110477997943, 0.32688142093894584], 
reward next is 0.6731, 
noisyNet noise sample is [array([-0.08092751], dtype=float32), 0.2552584]. 
=============================================
[2019-03-26 22:04:11,604] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.088691e-21 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:11,613] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3012
[2019-03-26 22:04:11,618] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.36666666666667, 94.0, 1.0, 2.0, 0.5013646783190098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 700579.8707358699, 700579.8707358692, 183810.1755656344], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1727400.0000, 
sim time next is 1728000.0000, 
raw observation next is [25.3, 94.0, 1.0, 2.0, 0.49910941371138, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697427.4515000603, 697427.4515000603, 183456.6416494632], 
processed observation next is [1.0, 0.0, 0.39810426540284366, 0.94, 1.0, 1.0, 0.39651736591732534, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19372984763890563, 0.19372984763890563, 0.2738158830589003], 
reward next is 0.7262, 
noisyNet noise sample is [array([-1.1523836], dtype=float32), -0.38668194]. 
=============================================
[2019-03-26 22:04:11,632] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.42749 ]
 [75.39727 ]
 [75.3636  ]
 [75.32901 ]
 [75.306854]], R is [[73.40802002]
 [73.39959717]
 [73.39081573]
 [73.38175964]
 [73.37237549]].
[2019-03-26 22:04:11,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.174084e-20 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:12,008] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0450
[2019-03-26 22:04:12,011] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 91.0, 1.0, 2.0, 0.3421657492197361, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531002.4731346571, 531002.4731346571, 169197.6043721182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1551600.0000, 
sim time next is 1552200.0000, 
raw observation next is [22.06666666666667, 91.00000000000001, 1.0, 2.0, 0.3411531859773741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 529851.4239202309, 529851.4239202302, 169116.6463757893], 
processed observation next is [0.0, 1.0, 0.2448657187993683, 0.9100000000000001, 1.0, 1.0, 0.20620865780406517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14718095108895304, 0.14718095108895285, 0.2524129050384915], 
reward next is 0.7476, 
noisyNet noise sample is [array([0.2661115], dtype=float32), -0.8912753]. 
=============================================
[2019-03-26 22:04:16,190] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.5563185e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:16,197] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3705
[2019-03-26 22:04:16,205] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 94.0, 1.0, 2.0, 0.5277877554670948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740526.6961008434, 740526.6961008434, 188449.1569334539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1737600.0000, 
sim time next is 1738200.0000, 
raw observation next is [24.51666666666667, 94.0, 1.0, 2.0, 0.5168699173348718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725773.0075960452, 725773.0075960445, 186732.0838326232], 
processed observation next is [1.0, 0.08695652173913043, 0.36097946287519767, 0.94, 1.0, 1.0, 0.4179155630540623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20160361322112366, 0.20160361322112347, 0.2787046027352585], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.6050574], dtype=float32), -1.3434045]. 
=============================================
[2019-03-26 22:04:19,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.9765657e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:19,446] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5291
[2019-03-26 22:04:19,452] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 93.66666666666667, 1.0, 2.0, 0.4727235308802371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661211.2445657701, 661211.2445657701, 179518.6265454997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2069400.0000, 
sim time next is 2070000.0000, 
raw observation next is [24.6, 94.0, 1.0, 2.0, 0.472007039567803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660407.1712579873, 660407.1712579873, 179437.8043038484], 
processed observation next is [0.0, 1.0, 0.36492890995260674, 0.94, 1.0, 1.0, 0.3638639030937385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18344643646055203, 0.18344643646055203, 0.2678176183639528], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.04146432], dtype=float32), 0.07344645]. 
=============================================
[2019-03-26 22:04:19,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.52665 ]
 [74.501434]
 [74.470024]
 [74.43601 ]
 [74.41307 ]], R is [[74.61438751]
 [74.60031128]
 [74.5863266 ]
 [74.57254028]
 [74.55886078]].
[2019-03-26 22:04:23,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0153253e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:23,972] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3744
[2019-03-26 22:04:23,979] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4629610546274237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656232.777746873, 656232.7777468737, 179198.3239041596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1742400.0000, 
sim time next is 1743000.0000, 
raw observation next is [24.28333333333333, 94.00000000000001, 1.0, 2.0, 0.5416862005381788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768554.3567444338, 768554.3567444343, 191871.5166570789], 
processed observation next is [1.0, 0.17391304347826086, 0.34992101105845175, 0.9400000000000002, 1.0, 1.0, 0.4478146994435889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21348732131789827, 0.21348732131789844, 0.28637539799564016], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.729262], dtype=float32), -0.090529844]. 
=============================================
[2019-03-26 22:04:23,998] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[66.9392 ]
 [67.11381]
 [67.13206]
 [67.13163]
 [67.0888 ]], R is [[66.29525757]
 [66.36484528]
 [66.43675995]
 [66.50744629]
 [66.57608795]].
[2019-03-26 22:04:30,322] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1165042e-27 9.9572831e-01 1.9523060e-34 4.2716381e-03 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:30,330] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3288
[2019-03-26 22:04:30,337] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 76.33333333333334, 1.0, 2.0, 0.4581375635303925, 1.0, 2.0, 0.4581375635303925, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1280695.424593117, 1280695.424593117, 287751.4318255591], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [27.0, 76.0, 1.0, 2.0, 0.8635980870008739, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1210671.198127168, 1210671.198127168, 260648.3922812851], 
processed observation next is [1.0, 0.5217391304347826, 0.4786729857819906, 0.76, 1.0, 1.0, 0.8356603457841855, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33629755503532444, 0.33629755503532444, 0.38902745116609716], 
reward next is 0.6110, 
noisyNet noise sample is [array([-1.4968737], dtype=float32), -1.0340192]. 
=============================================
[2019-03-26 22:04:31,900] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.572493e-17 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:31,908] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9147
[2019-03-26 22:04:31,913] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.81666666666666, 87.0, 1.0, 2.0, 0.48264653024892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674415.8407097274, 674415.8407097274, 180921.6819981368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1883400.0000, 
sim time next is 1884000.0000, 
raw observation next is [25.73333333333333, 87.0, 1.0, 2.0, 0.4800667094387691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670809.8449088709, 670809.8449088709, 180531.682876214], 
processed observation next is [1.0, 0.8260869565217391, 0.41864139020537117, 0.87, 1.0, 1.0, 0.3735743487214086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1863360680302419, 0.1863360680302419, 0.26945027294957313], 
reward next is 0.7305, 
noisyNet noise sample is [array([-0.30500388], dtype=float32), 0.0060091587]. 
=============================================
[2019-03-26 22:04:31,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[74.79044]
 [74.25785]
 [73.65198]
 [72.66181]
 [71.92944]], R is [[74.81784821]
 [74.79963684]
 [74.78093719]
 [74.76159668]
 [74.7416153 ]].
[2019-03-26 22:04:32,814] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.490372e-21 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:32,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9229
[2019-03-26 22:04:32,829] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 97.33333333333333, 1.0, 2.0, 0.4512865427725447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643499.0266313003, 643499.0266313003, 177979.0370970285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1986000.0000, 
sim time next is 1986600.0000, 
raw observation next is [23.83333333333333, 97.16666666666667, 1.0, 2.0, 0.4535014760161311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 645374.6808852643, 645374.6808852637, 178138.3604221271], 
processed observation next is [1.0, 1.0, 0.32859399684044216, 0.9716666666666667, 1.0, 1.0, 0.341568043392929, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1792707446903512, 0.17927074469035104, 0.2658781498837718], 
reward next is 0.7341, 
noisyNet noise sample is [array([-0.434137], dtype=float32), -0.048470985]. 
=============================================
[2019-03-26 22:04:38,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6480689e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:38,624] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6832
[2019-03-26 22:04:38,630] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 96.33333333333333, 1.0, 2.0, 0.4648949837240512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 654897.4471729493, 654897.4471729499, 178962.2610506622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2002200.0000, 
sim time next is 2002800.0000, 
raw observation next is [24.1, 96.66666666666666, 1.0, 2.0, 0.4660310343147669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656733.6792914147, 656733.6792914141, 179160.3661694448], 
processed observation next is [0.0, 0.17391304347826086, 0.3412322274881518, 0.9666666666666666, 1.0, 1.0, 0.35666389676477933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18242602202539296, 0.1824260220253928, 0.2674035315961863], 
reward next is 0.7326, 
noisyNet noise sample is [array([-1.7145557], dtype=float32), -1.1610925]. 
=============================================
[2019-03-26 22:04:45,701] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1363306e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:45,713] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7865
[2019-03-26 22:04:45,717] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 83.66666666666666, 1.0, 2.0, 0.539062313220602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753275.1832972945, 753275.1832972952, 189942.7619957579], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2238600.0000, 
sim time next is 2239200.0000, 
raw observation next is [27.8, 84.0, 1.0, 2.0, 0.5383594888326082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 752292.7223970952, 752292.7223970952, 189824.4975520402], 
processed observation next is [1.0, 0.9565217391304348, 0.5165876777251186, 0.84, 1.0, 1.0, 0.4438066130513352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20897020066585978, 0.20897020066585978, 0.28332014560006], 
reward next is 0.7167, 
noisyNet noise sample is [array([-1.4740766], dtype=float32), 0.21237576]. 
=============================================
[2019-03-26 22:04:46,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.622566e-29 0.000000e+00], sum to 1.0000
[2019-03-26 22:04:46,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7402
[2019-03-26 22:04:46,604] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
processed observation next is [0.0, 0.9130434782608695, 0.4715639810426541, 0.9, 1.0, 1.0, 0.4436370423865538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20891555011729224, 0.2089155501172924, 0.28328456911258376], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.88318366], dtype=float32), 0.5697995]. 
=============================================
[2019-03-26 22:04:46,620] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.63806 ]
 [78.634415]
 [78.62678 ]
 [78.49516 ]
 [78.45066 ]], R is [[78.61682129]
 [78.54714203]
 [78.47859955]
 [78.41040802]
 [78.34246826]].
[2019-03-26 22:04:48,257] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2976308e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 22:04:48,265] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9981
[2019-03-26 22:04:48,274] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 81.66666666666667, 1.0, 2.0, 0.8003493707167393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1118584.911423456, 1118584.911423456, 243995.5322233662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2342400.0000, 
sim time next is 2343000.0000, 
raw observation next is [27.63333333333334, 81.83333333333334, 1.0, 2.0, 0.7702478047778449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076493.012394116, 1076493.012394115, 236737.7817726357], 
processed observation next is [1.0, 0.08695652173913043, 0.5086887835703005, 0.8183333333333335, 1.0, 1.0, 0.7231901262383674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29902583677614336, 0.2990258367761431, 0.35333997279497864], 
reward next is 0.6467, 
noisyNet noise sample is [array([1.5913376], dtype=float32), 1.0095546]. 
=============================================
[2019-03-26 22:04:48,290] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[56.507477]
 [55.95011 ]
 [56.12144 ]
 [56.18828 ]
 [60.297382]], R is [[56.93408585]
 [57.0005722 ]
 [57.03077698]
 [57.03618622]
 [57.00300598]].
[2019-03-26 22:04:50,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.5414241e-15 3.7502914e-06 2.7508882e-17 9.9999630e-01 7.6737804e-19], sum to 1.0000
[2019-03-26 22:04:50,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2045
[2019-03-26 22:04:50,162] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.9, 65.66666666666666, 1.0, 2.0, 0.5825086563367791, 1.0, 2.0, 0.5825086563367791, 1.0, 1.0, 1.011624897500645, 6.9112, 6.9112, 170.5573041426782, 2443838.115003362, 2443838.115003362, 476890.1909181279], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2209200.0000, 
sim time next is 2209800.0000, 
raw observation next is [32.0, 65.33333333333334, 1.0, 2.0, 0.8706993164617335, 1.0, 2.0, 0.8706993164617335, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2435260.955561548, 2435260.955561548, 455759.6339456835], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.6533333333333334, 1.0, 1.0, 0.8442160439297993, 1.0, 1.0, 0.8442160439297993, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6764613765448745, 0.6764613765448745, 0.6802382596204231], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42269772], dtype=float32), 0.7232945]. 
=============================================
[2019-03-26 22:04:52,857] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1286909e-25 1.0000000e+00 9.2675990e-30 4.6311278e-08 3.6303600e-33], sum to 1.0000
[2019-03-26 22:04:52,867] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6035
[2019-03-26 22:04:52,878] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1878654.635695075 W.
[2019-03-26 22:04:52,881] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.11666666666667, 86.83333333333333, 1.0, 2.0, 0.6718555783036857, 1.0, 2.0, 0.6718555783036857, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1878654.635695075, 1878654.635695075, 362306.0382810892], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2451000.0000, 
sim time next is 2451600.0000, 
raw observation next is [27.0, 87.0, 1.0, 2.0, 0.4469766101899076, 1.0, 2.0, 0.4469766101899076, 1.0, 1.0, 0.7673304288421641, 6.9112, 6.9112, 170.5573041426782, 1874762.636721917, 1874762.636721917, 378991.3570706264], 
processed observation next is [1.0, 0.391304347826087, 0.4786729857819906, 0.87, 1.0, 1.0, 0.3337067592649489, 1.0, 1.0, 0.3337067592649489, 1.0, 0.5, 0.7162566205392245, 0.0, 0.0, 0.8375144448122397, 0.5207673990894214, 0.5207673990894214, 0.5656587418964573], 
reward next is 0.4343, 
noisyNet noise sample is [array([-0.29958913], dtype=float32), -0.3656273]. 
=============================================
[2019-03-26 22:04:58,043] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000e+00 1.0000e+00 0.0000e+00 1.0058e-37 0.0000e+00], sum to 1.0000
[2019-03-26 22:04:58,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3986
[2019-03-26 22:04:58,056] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 1.0, 1.0, 1.0, 0.28817071973506386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16784845999691392, 0.16784845999691392, 0.2608206684207806], 
reward next is 0.7392, 
noisyNet noise sample is [array([-0.311913], dtype=float32), 1.3651775]. 
=============================================
[2019-03-26 22:05:00,357] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 22:05:00,359] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:05:00,360] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:05:00,361] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:05:00,362] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:05:00,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:05:00,363] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:05:00,364] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:05:00,366] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:05:00,369] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:05:00,366] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:05:00,398] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-03-26 22:05:00,399] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-03-26 22:05:00,399] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-03-26 22:05:00,420] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-03-26 22:05:00,492] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-03-26 22:05:18,676] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.76666236]
[2019-03-26 22:05:18,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.9, 76.33333333333333, 1.0, 2.0, 0.2519800754159595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 414910.8428672841, 414910.8428672836, 161092.4680670016]
[2019-03-26 22:05:18,678] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:05:18,680] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7492764243981964
[2019-03-26 22:06:16,502] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.76666236]
[2019-03-26 22:06:16,503] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.725684933695556, 6.9112, 170.5573041426782, 3493458.08163267, 2910009.392497714, 549066.9949991213]
[2019-03-26 22:06:16,504] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:06:16,509] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2931639e-18 9.9999809e-01 8.3789862e-22 1.9071203e-06 1.3770449e-24], sampled 0.04565214382966187
[2019-03-26 22:06:16,510] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3493458.08163267 W.
[2019-03-26 22:06:25,777] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.76666236]
[2019-03-26 22:06:25,779] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.76666666666667, 88.0, 1.0, 2.0, 0.5579893811815649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779733.2071424638, 779733.2071424631, 193180.4869477111]
[2019-03-26 22:06:25,780] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:06:25,782] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.815386e-37 0.000000e+00], sampled 0.62323281979531
[2019-03-26 22:06:48,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.76666236]
[2019-03-26 22:06:48,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.12362202333333, 90.90022125333334, 1.0, 2.0, 0.3491608480331749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 541680.0958284553, 541680.0958284559, 170060.4245766422]
[2019-03-26 22:06:48,151] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:06:48,153] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.7373677e-37 0.0000000e+00], sampled 0.02807721487915249
[2019-03-26 22:06:52,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.76666236]
[2019-03-26 22:06:52,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.86666666666667, 63.0, 1.0, 2.0, 0.4079342377726718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 638855.6293466438, 638855.6293466438, 178651.9340375349]
[2019-03-26 22:06:52,852] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:06:52,855] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2315948e-36 0.0000000e+00], sampled 0.4441408333451605
[2019-03-26 22:06:54,217] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4158 2779682817.9169 928.0000
[2019-03-26 22:06:54,456] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6574 2928016837.6026 1337.0000
[2019-03-26 22:06:54,744] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.2709 3164360164.4135 1775.0000
[2019-03-26 22:06:54,908] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 22:06:54,934] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 22:06:55,954] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1550000, evaluation results [1550000.0, 7876.2708909448365, 3164360164.4135427, 1775.0, 8254.657435931651, 2928016837.6026235, 1337.0, 8661.415761986658, 2779682817.916868, 928.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 22:07:01,040] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1400211e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:07:01,050] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8324
[2019-03-26 22:07:01,058] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 93.16666666666667, 1.0, 2.0, 0.3912876182325191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 585308.0276393533, 585308.0276393533, 173233.1818880439], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2844600.0000, 
sim time next is 2845200.0000, 
raw observation next is [23.0, 92.33333333333334, 1.0, 2.0, 0.3880222021770862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582468.75346673, 582468.7534667306, 173038.3451551354], 
processed observation next is [1.0, 0.9565217391304348, 0.28909952606635075, 0.9233333333333335, 1.0, 1.0, 0.2626773520205858, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16179687596298056, 0.16179687596298073, 0.25826618679870955], 
reward next is 0.7417, 
noisyNet noise sample is [array([0.59698445], dtype=float32), 0.0065697185]. 
=============================================
[2019-03-26 22:07:02,133] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3529303e-15 4.9142539e-08 9.0039958e-18 1.0000000e+00 1.5039956e-18], sum to 1.0000
[2019-03-26 22:07:02,143] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0144
[2019-03-26 22:07:02,148] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.05, 88.5, 1.0, 2.0, 0.6663726927601872, 1.0, 2.0, 0.6663726927601872, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1863309.961559127, 1863309.961559127, 360052.5946428315], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2457000.0000, 
sim time next is 2457600.0000, 
raw observation next is [25.93333333333333, 88.66666666666666, 1.0, 2.0, 0.6443930490367189, 1.0, 2.0, 0.6443930490367189, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1801798.85177257, 1801798.85177257, 351218.8365424062], 
processed observation next is [1.0, 0.43478260869565216, 0.42812006319115314, 0.8866666666666666, 1.0, 1.0, 0.5715578904056854, 1.0, 1.0, 0.5715578904056854, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5004996810479361, 0.5004996810479361, 0.5242072187200092], 
reward next is 0.4758, 
noisyNet noise sample is [array([0.25185683], dtype=float32), -0.9356917]. 
=============================================
[2019-03-26 22:07:10,147] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0589328e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 22:07:10,156] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6857
[2019-03-26 22:07:10,162] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 97.0, 1.0, 2.0, 0.3120817245335519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494987.5331758604, 494987.5331758604, 166680.0862831681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3000600.0000, 
sim time next is 3001200.0000, 
raw observation next is [20.33333333333333, 98.0, 1.0, 2.0, 0.3118763279392696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 495249.3745411828, 495249.3745411834, 166709.224319104], 
processed observation next is [1.0, 0.7391304347826086, 0.16271721958925733, 0.98, 1.0, 1.0, 0.1709353348665899, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13756927070588412, 0.1375692707058843, 0.24881973778970745], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.29132035], dtype=float32), 0.917927]. 
=============================================
[2019-03-26 22:07:10,831] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:07:10,843] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2088
[2019-03-26 22:07:10,846] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.475867988442044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664941.0253757458, 664941.0253757464, 179901.4110463232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2703600.0000, 
sim time next is 2704200.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4770851508227688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666642.3273552274, 666642.3273552274, 180083.5644885299], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36998210942502263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18517842426534095, 0.18517842426534095, 0.26878143953511924], 
reward next is 0.7312, 
noisyNet noise sample is [array([1.5416757], dtype=float32), 0.69919556]. 
=============================================
[2019-03-26 22:07:26,168] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:07:26,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0885
[2019-03-26 22:07:26,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.314803195304462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497805.83781817, 497805.8378181707, 166861.4792520773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2954400.0000, 
sim time next is 2955000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3160220621309831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499731.7658908868, 499731.7658908868, 167005.2323440549], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17593019533853385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1388143794141352, 0.1388143794141352, 0.24926154081202226], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.5733209], dtype=float32), -0.48889598]. 
=============================================
[2019-03-26 22:07:26,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[73.19152]
 [73.29339]
 [73.27226]
 [73.24951]
 [73.2521 ]], R is [[73.20581818]
 [73.22471619]
 [73.24403381]
 [73.26332855]
 [73.28070831]].
[2019-03-26 22:07:30,229] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:07:30,239] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5003
[2019-03-26 22:07:30,245] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 71.5, 1.0, 2.0, 0.5325943095267229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744233.7545501533, 744233.7545501539, 188860.9576867862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3235800.0000, 
sim time next is 3236400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5304637583312806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741255.5376612237, 741255.537661223, 188507.1194313451], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.43429368473648255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20590431601700657, 0.20590431601700637, 0.28135390959902257], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.71525663], dtype=float32), -0.38388047]. 
=============================================
[2019-03-26 22:07:38,611] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:07:38,621] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0375
[2019-03-26 22:07:38,628] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 83.16666666666666, 1.0, 2.0, 0.5466986700205162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763949.9169458079, 763949.9169458079, 191236.2476275458], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3268200.0000, 
sim time next is 3268800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5452878886973932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761977.801584096, 761977.801584096, 190995.9706930356], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4521540827679436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21166050044002668, 0.21166050044002668, 0.28506861297468], 
reward next is 0.7149, 
noisyNet noise sample is [array([-1.8287877], dtype=float32), 1.8417475]. 
=============================================
[2019-03-26 22:07:45,273] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.975437e-36 0.000000e+00], sum to 1.0000
[2019-03-26 22:07:45,281] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4006
[2019-03-26 22:07:45,285] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 77.0, 1.0, 2.0, 0.5900030291869829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824486.3753322603, 824486.3753322603, 198898.6271320656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3342600.0000, 
sim time next is 3343200.0000, 
raw observation next is [30.33333333333334, 77.66666666666667, 1.0, 2.0, 0.5886233678286997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822557.652259809, 822557.652259809, 198645.9138852008], 
processed observation next is [0.0, 0.6956521739130435, 0.6366508688783573, 0.7766666666666667, 1.0, 1.0, 0.504365503408072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22848823673883584, 0.22848823673883584, 0.2964864386346281], 
reward next is 0.7035, 
noisyNet noise sample is [array([1.6679907], dtype=float32), 0.13895445]. 
=============================================
[2019-03-26 22:07:50,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5099516e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:07:50,123] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9778
[2019-03-26 22:07:50,129] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4210486180772605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616494.8503476972, 616494.8503476972, 175757.6330680395], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3301800.0000, 
sim time next is 3302400.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4214444541305093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617074.541790927, 617074.541790927, 175813.1796244131], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.83, 1.0, 1.0, 0.302945125458445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17140959494192415, 0.17140959494192415, 0.26240773078270613], 
reward next is 0.7376, 
noisyNet noise sample is [array([-1.9756328], dtype=float32), -1.4402839]. 
=============================================
[2019-03-26 22:07:51,565] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 22:07:51,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:07:51,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:07:51,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:07:51,569] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:07:51,569] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:07:51,571] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:07:51,570] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:07:51,572] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:07:51,574] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:07:51,576] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:07:51,600] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-03-26 22:07:51,621] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-03-26 22:07:51,644] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-03-26 22:07:51,644] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-03-26 22:07:51,645] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-03-26 22:07:53,523] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:07:53,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.08870602333333, 70.42781168, 1.0, 2.0, 0.8457740036945918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1244341.444948441, 1244341.444948441, 264226.522878728]
[2019-03-26 22:07:53,525] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:07:53,529] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0940197e-29 0.0000000e+00], sampled 0.9093349324613227
[2019-03-26 22:08:28,648] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:08:28,649] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.65244887166667, 68.17079780166665, 1.0, 2.0, 0.7342907090770058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1026215.341806445, 1026215.341806445, 228417.1836406752]
[2019-03-26 22:08:28,650] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:08:28,655] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.793541e-29 0.000000e+00], sampled 0.0047675510501383744
[2019-03-26 22:09:05,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:09:05,923] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 84.0, 1.0, 2.0, 0.6146994956877259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859011.8491990155, 859011.8491990155, 203516.5844316198]
[2019-03-26 22:09:05,926] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:09:05,927] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.1627797e-31 0.0000000e+00], sampled 0.3981361959330012
[2019-03-26 22:09:14,109] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:09:14,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.17715585, 84.66832583, 1.0, 2.0, 0.5255235744971148, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734349.8737607389, 734349.8737607383, 187691.3478874239]
[2019-03-26 22:09:14,111] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:09:14,116] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1154872e-33 0.0000000e+00], sampled 0.9957162302651202
[2019-03-26 22:09:17,090] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:09:17,091] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 73.0, 1.0, 2.0, 0.9104217609783128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1272516.558412635, 1272516.558412635, 272821.0565459684]
[2019-03-26 22:09:17,092] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:09:17,096] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.336821e-28 0.000000e+00], sampled 0.4368184560775956
[2019-03-26 22:09:35,761] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.80143046]
[2019-03-26 22:09:35,762] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.49742750333333, 74.21651075333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.255467282169779, 6.9112, 168.9109546754582, 1698154.363875843, 1453922.199637588, 311350.9919850268]
[2019-03-26 22:09:35,764] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:09:35,769] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.143634e-26 0.000000e+00], sampled 0.21677714842318707
[2019-03-26 22:09:35,771] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1698154.363875843 W.
[2019-03-26 22:09:45,393] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.1371 3164252005.0342 1772.0000
[2019-03-26 22:09:45,573] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1623 2779660315.0465 924.0000
[2019-03-26 22:09:45,943] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.0907 2927526317.3189 1326.0000
[2019-03-26 22:09:45,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.5074 3008235916.8904 1765.0000
[2019-03-26 22:09:46,028] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.0272 2843193022.3596 1131.0000
[2019-03-26 22:09:47,045] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1575000, evaluation results [1575000.0, 7878.137095812693, 3164252005.0342054, 1772.0, 8260.090685554242, 2927526317.318938, 1326.0, 8662.162266532097, 2779660315.0465465, 924.0, 7998.507350287605, 3008235916.8904333, 1765.0, 8497.027172431872, 2843193022.3596, 1131.0]
[2019-03-26 22:09:54,419] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.0481745e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:09:54,429] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9672
[2019-03-26 22:09:54,435] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5113292983090572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714508.5743503075, 714508.574350308, 185390.1024351827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3451200.0000, 
sim time next is 3451800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5110902771966108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714174.4647164326, 714174.4647164332, 185351.8733841149], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4109521412007359, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1983817957545646, 0.19838179575456477, 0.27664458714047], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.17511496], dtype=float32), 0.045098867]. 
=============================================
[2019-03-26 22:09:59,350] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2270521e-14 5.7139279e-11 1.9639715e-15 1.0000000e+00 1.0188950e-15], sum to 1.0000
[2019-03-26 22:09:59,362] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8671
[2019-03-26 22:09:59,368] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 62.33333333333333, 1.0, 2.0, 0.9029826731842905, 1.0, 2.0, 0.9029826731842905, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2525645.605056193, 2525645.605056193, 473124.6387767281], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3593400.0000, 
sim time next is 3594000.0000, 
raw observation next is [33.0, 61.66666666666667, 1.0, 2.0, 0.9550201090764718, 1.0, 2.0, 0.9550201090764718, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2671350.087350522, 2671350.087350522, 502398.722025735], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.6166666666666667, 1.0, 1.0, 0.9458073603330986, 1.0, 1.0, 0.9458073603330986, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7420416909307006, 0.7420416909307006, 0.7498488388443806], 
reward next is 0.2502, 
noisyNet noise sample is [array([0.31253335], dtype=float32), -0.54435194]. 
=============================================
[2019-03-26 22:09:59,380] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[25.991507]
 [26.171501]
 [26.162584]
 [26.12168 ]
 [27.897985]], R is [[25.98849106]
 [26.0224514 ]
 [26.04183197]
 [26.01982117]
 [25.75962257]].
[2019-03-26 22:10:02,978] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.451497e-25 0.000000e+00], sum to 1.0000
[2019-03-26 22:10:02,981] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1064
[2019-03-26 22:10:02,988] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.7130848300764111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 996564.9472169414, 996564.9472169414, 223691.9440317036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3654000.0000, 
sim time next is 3654600.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.7167963663609491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001754.416041167, 1001754.416041167, 224509.6117563216], 
processed observation next is [1.0, 0.30434782608695654, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.658790802844517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27826511556699085, 0.27826511556699085, 0.33508897277062927], 
reward next is 0.6649, 
noisyNet noise sample is [array([-2.1532602], dtype=float32), 0.96648407]. 
=============================================
[2019-03-26 22:10:07,873] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0425302e-12 9.9959213e-01 8.3154453e-15 4.0784909e-04 2.7067113e-17], sum to 1.0000
[2019-03-26 22:10:07,882] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7127
[2019-03-26 22:10:07,892] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2913696.349893423 W.
[2019-03-26 22:10:07,895] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.00000000000001, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 6.917288583977644, 6.9112, 170.5573041426782, 2913696.349893423, 2909334.849522475, 553599.2902076169], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4029600.0000, 
sim time next is 4030200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.200260510328816, 6.9112, 170.5573041426782, 3116636.733430116, 2909570.9316518, 552128.3293681194], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.028906051032881575, 0.0, 0.8375144448122397, 0.86573242595281, 0.8082141476810555, 0.8240721333852528], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48618814], dtype=float32), 2.050369]. 
=============================================
[2019-03-26 22:10:13,192] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:10:13,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2127
[2019-03-26 22:10:13,206] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 59.5, 1.0, 2.0, 0.6041731759856634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 844295.9964301536, 844295.996430153, 201526.8635109771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3947400.0000, 
sim time next is 3948000.0000, 
raw observation next is [34.33333333333333, 60.66666666666667, 1.0, 2.0, 0.6068088127332675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847980.6119475391, 847980.6119475391, 202022.2186594015], 
processed observation next is [0.0, 0.6956521739130435, 0.8262243285939966, 0.6066666666666667, 1.0, 1.0, 0.5262756779918885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23555016998542755, 0.23555016998542755, 0.30152569949164404], 
reward next is 0.6985, 
noisyNet noise sample is [array([-0.916037], dtype=float32), -1.2027476]. 
=============================================
[2019-03-26 22:10:13,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[79.77794]
 [79.56358]
 [79.11813]
 [79.6273 ]
 [79.47602]], R is [[79.77311707]
 [79.67459869]
 [79.57598114]
 [79.46749878]
 [79.37359619]].
[2019-03-26 22:10:22,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7987216e-15 9.9998105e-01 9.1876100e-18 1.9004148e-05 4.5453652e-20], sum to 1.0000
[2019-03-26 22:10:22,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0637
[2019-03-26 22:10:22,754] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1710861.016752009 W.
[2019-03-26 22:10:22,759] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.6118913478541329, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.948251318013581, 6.9112, 168.9127029091932, 1710861.016752009, 1684575.576009517, 366781.5405650897], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3991800.0000, 
sim time next is 3992400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.3941815075351575, 1.0, 1.0, 0.3941815075351575, 1.0, 2.0, 0.6845629207720425, 6.9112, 6.9112, 170.5573041426782, 1653152.302374055, 1653152.302374055, 349691.4638706222], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.2700982018495874, 1.0, 0.5, 0.2700982018495874, 1.0, 1.0, 0.6153206350878567, 0.0, 0.0, 0.8375144448122397, 0.45920897288168194, 0.45920897288168194, 0.5219275580158541], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.38454416], dtype=float32), 0.842148]. 
=============================================
[2019-03-26 22:10:33,048] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.889681e-31 0.000000e+00], sum to 1.0000
[2019-03-26 22:10:33,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3299
[2019-03-26 22:10:33,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6467782822224533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903859.4265226136, 903859.4265226136, 209777.0128541295], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4080600.0000, 
sim time next is 4081200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6869576680131998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 960034.6752812007, 960034.6752812007, 218055.4227884784], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.622840563871325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26667629868922244, 0.26667629868922244, 0.32545585490817674], 
reward next is 0.6745, 
noisyNet noise sample is [array([0.83210295], dtype=float32), -1.4239621]. 
=============================================
[2019-03-26 22:10:39,624] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8197565e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 22:10:39,632] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4650
[2019-03-26 22:10:39,636] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6035357201647397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 843404.8362381727, 843404.8362381734, 201407.0364804408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4465800.0000, 
sim time next is 4466400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6055076074234279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846161.5276253807, 846161.5276253807, 201776.9095753855], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5247079607511179, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23504486878482797, 0.23504486878482797, 0.30115956653042614], 
reward next is 0.6988, 
noisyNet noise sample is [array([-0.7543954], dtype=float32), -0.92138785]. 
=============================================
[2019-03-26 22:10:42,850] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 22:10:42,852] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:10:42,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:10:42,854] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:10:42,855] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:10:42,856] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:10:42,856] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:10:42,857] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:10:42,857] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:10:42,859] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:10:42,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:10:42,891] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-03-26 22:10:42,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-03-26 22:10:42,934] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-03-26 22:10:42,935] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-03-26 22:10:42,991] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-03-26 22:10:48,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:10:48,425] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.68031301, 78.14493455, 1.0, 2.0, 0.3300914741263706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515389.1498421024, 515389.1498421031, 168048.4087892857]
[2019-03-26 22:10:48,428] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:10:48,431] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.681299700518224
[2019-03-26 22:10:56,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:10:56,388] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.78953894333333, 78.03792608833334, 1.0, 2.0, 0.3048222472802665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485248.5958375676, 485248.5958375676, 165997.07849446]
[2019-03-26 22:10:56,389] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:10:56,391] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2047630628656153
[2019-03-26 22:11:01,482] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:11:01,484] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [17.55, 92.0, 1.0, 2.0, 0.2209791979974891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 368420.654628409, 368420.654628409, 157593.2255159258]
[2019-03-26 22:11:01,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:11:01,489] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04492925416985705
[2019-03-26 22:11:19,443] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:11:19,444] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.48035340666667, 97.17390031666666, 1.0, 2.0, 0.5277320943573445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737437.0601837133, 737437.0601837126, 188054.7361995234]
[2019-03-26 22:11:19,446] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:11:19,448] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2413532730471042
[2019-03-26 22:11:48,079] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:11:48,081] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 85.66666666666667, 1.0, 2.0, 0.6683488387162189, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.99795341528479, 6.9112, 168.9124453704888, 1830838.542117563, 1769292.869733582, 378244.9595247827]
[2019-03-26 22:11:48,083] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:11:48,086] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3732415e-32 1.0000000e+00 1.4874745e-37 4.7200354e-20 0.0000000e+00], sampled 0.03061543968564473
[2019-03-26 22:11:48,087] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1830838.542117563 W.
[2019-03-26 22:12:03,577] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:12:03,580] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.5, 1.0, 2.0, 0.5563401364978955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777427.7126325896, 777427.7126325902, 192894.3859055719]
[2019-03-26 22:12:03,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:12:03,584] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40413209637266756
[2019-03-26 22:12:06,876] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:12:06,878] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.90360206666667, 57.51952030666667, 1.0, 2.0, 0.7453917153247407, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005978117875388, 6.9112, 168.9123160158406, 1938649.773787565, 1871411.169916158, 394807.1021414134]
[2019-03-26 22:12:06,879] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:12:06,883] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4141848e-30 1.0000000e+00 1.1781148e-34 2.2529811e-18 0.0000000e+00], sampled 0.3080907725509603
[2019-03-26 22:12:06,884] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1938649.773787565 W.
[2019-03-26 22:12:26,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0296558]
[2019-03-26 22:12:26,028] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5704420546150114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797141.0768989144, 797141.0768989144, 195364.6506978352]
[2019-03-26 22:12:26,029] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:12:26,032] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3175864e-38 0.0000000e+00], sampled 0.6068056609560515
[2019-03-26 22:12:36,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7732 2779589745.4232 933.0000
[2019-03-26 22:12:36,950] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.6451 3164445220.6628 1778.0000
[2019-03-26 22:12:37,057] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.4752 2927712708.6915 1338.0000
[2019-03-26 22:12:37,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.3731 3008038374.3173 1766.0000
[2019-03-26 22:12:37,170] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9210 2842890288.6408 1131.0000
[2019-03-26 22:12:38,187] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1600000, evaluation results [1600000.0, 7879.6450561207575, 3164445220.6627817, 1778.0, 8254.475224188334, 2927712708.691521, 1338.0, 8660.773226357216, 2779589745.4231777, 933.0, 7998.373066976429, 3008038374.3172884, 1766.0, 8496.920990814182, 2842890288.6407876, 1131.0]
[2019-03-26 22:12:42,132] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5697254e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:12:42,142] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8108
[2019-03-26 22:12:42,147] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666666, 80.66666666666667, 1.0, 2.0, 0.8872529144389419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240114.038453437, 1240114.038453437, 266456.4811316155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4693200.0000, 
sim time next is 4693800.0000, 
raw observation next is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.8981540199752394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1255359.529208946, 1255359.529208946, 269431.6412660181], 
processed observation next is [1.0, 0.30434782608695654, 0.5655608214849924, 0.7983333333333335, 1.0, 1.0, 0.8772939999701679, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3487109803358183, 0.3487109803358183, 0.40213677800898223], 
reward next is 0.5979, 
noisyNet noise sample is [array([2.0888138], dtype=float32), 0.8361204]. 
=============================================
[2019-03-26 22:12:48,036] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5932182e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:12:48,046] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9572
[2019-03-26 22:12:48,051] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.8634290055778411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206796.419912639, 1206796.419912639, 260076.989897066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4692600.0000, 
sim time next is 4693200.0000, 
raw observation next is [28.66666666666666, 80.66666666666667, 1.0, 2.0, 0.8872529144389419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240114.038453437, 1240114.038453437, 266456.4811316155], 
processed observation next is [1.0, 0.30434782608695654, 0.5576619273301735, 0.8066666666666668, 1.0, 1.0, 0.8641601378782433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3444761217926214, 0.3444761217926214, 0.39769624049494856], 
reward next is 0.6023, 
noisyNet noise sample is [array([0.0724467], dtype=float32), 0.3614384]. 
=============================================
[2019-03-26 22:12:48,145] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3076261e-11 2.3937713e-10 4.1291254e-12 1.0000000e+00 1.5635489e-12], sum to 1.0000
[2019-03-26 22:12:48,152] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1417
[2019-03-26 22:12:48,158] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.9610412539707446, 1.0, 2.0, 0.9610412539707446, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2688210.350786336, 2688210.350786336, 505898.9763196867], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.9643571076472994, 1.0, 2.0, 0.9643571076472994, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2697495.420731307, 2697495.420731307, 507830.0166035445], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.9570567562015656, 1.0, 1.0, 0.9570567562015656, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7493042835364742, 0.7493042835364742, 0.7579552486620068], 
reward next is 0.2420, 
noisyNet noise sample is [array([1.5069734], dtype=float32), 1.0869831]. 
=============================================
[2019-03-26 22:12:48,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[6.313076 ]
 [6.198642 ]
 [5.7643814]
 [5.0568933]
 [4.9430494]], R is [[6.53135681]
 [6.7109704 ]
 [6.8900075 ]
 [7.07952261]
 [7.00872755]].
[2019-03-26 22:12:48,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0133786e-16 1.0000000e+00 2.5692875e-18 6.3218537e-09 7.7495805e-21], sum to 1.0000
[2019-03-26 22:12:48,327] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8001
[2019-03-26 22:12:48,337] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2087165.988005385 W.
[2019-03-26 22:12:48,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.851509922282928, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.991712301378927, 6.9112, 168.9124774384207, 2087165.988005385, 2030047.952386615, 421498.8405481244], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.018572848886866, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990338089912729, 6.9112, 168.9124207681489, 2320998.380657304, 2264855.274020373, 469658.3222622738], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 1.022376926369718, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00791380899127292, 0.0, 0.8294373144124337, 0.6447217724048067, 0.6291264650056592, 0.7009825705407072], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.81945527], dtype=float32), 1.2908854]. 
=============================================
[2019-03-26 22:12:53,482] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2714614e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 22:12:53,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9818
[2019-03-26 22:12:53,498] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.497565174219769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695268.9118359083, 695268.9118359083, 183214.9644337025], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4525200.0000, 
sim time next is 4525800.0000, 
raw observation next is [28.0, 74.83333333333334, 1.0, 2.0, 0.498187785142159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696139.1975420427, 696139.1975420427, 183312.3870787389], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.7483333333333334, 1.0, 1.0, 0.39540697005079395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1933719993172341, 0.1933719993172341, 0.273600577729461], 
reward next is 0.7264, 
noisyNet noise sample is [array([-0.209151], dtype=float32), 0.11401729]. 
=============================================
[2019-03-26 22:12:57,192] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:12:57,201] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0097
[2019-03-26 22:12:57,206] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5485688512610616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766564.2279348939, 766564.2279348939, 191555.4324358873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4579200.0000, 
sim time next is 4579800.0000, 
raw observation next is [28.0, 84.83333333333333, 1.0, 2.0, 0.5508464621512057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769748.0901731236, 769748.0901731236, 191946.0132456327], 
processed observation next is [1.0, 0.0, 0.5260663507109005, 0.8483333333333333, 1.0, 1.0, 0.4588511592183201, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2138189139369788, 0.2138189139369788, 0.28648658693378015], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.5655682], dtype=float32), 0.18421996]. 
=============================================
[2019-03-26 22:12:58,835] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3671546e-17 1.0000000e+00 4.2804781e-20 3.7955908e-10 5.2389843e-23], sum to 1.0000
[2019-03-26 22:12:58,844] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8902
[2019-03-26 22:12:58,856] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1865862.447906444 W.
[2019-03-26 22:12:58,862] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.6933780958770719, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.985410211837991, 6.9112, 168.9124541741552, 1865862.447906444, 1813215.32607982, 383975.3868139159], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4960800.0000, 
sim time next is 4961400.0000, 
raw observation next is [30.0, 69.33333333333334, 1.0, 2.0, 0.6176113405917582, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.93777229082873, 6.9112, 168.9127082940339, 1726867.248268649, 1708015.978308928, 368663.3765776408], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6933333333333335, 1.0, 1.0, 0.539290771797299, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0026572290828729807, 0.0, 0.829438726296159, 0.4796853467412914, 0.4744488828635911, 0.5502438456382699], 
reward next is 0.3169, 
noisyNet noise sample is [array([1.6992176], dtype=float32), -1.1760514]. 
=============================================
[2019-03-26 22:12:58,943] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9523762e-12 9.9875820e-01 4.8248962e-14 1.2418460e-03 8.9705825e-17], sum to 1.0000
[2019-03-26 22:12:58,950] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9004
[2019-03-26 22:12:58,958] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2045547.02863058 W.
[2019-03-26 22:12:58,962] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7314836982563919, 1.0, 2.0, 0.7314836982563919, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2045547.02863058, 2045547.02863058, 387892.9761226131], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4792200.0000, 
sim time next is 4792800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5401605898142392, 1.0, 2.0, 0.5401605898142392, 1.0, 1.0, 0.9352276486524708, 6.911199999999999, 6.9112, 170.5573041426782, 2266011.315666909, 2266011.31566691, 443468.2613601754], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4459766142340231, 1.0, 1.0, 0.4459766142340231, 1.0, 0.5, 0.9210093276249642, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6294475876852524, 0.6294475876852528, 0.6618929274032468], 
reward next is 0.3381, 
noisyNet noise sample is [array([-0.9061017], dtype=float32), -0.40059122]. 
=============================================
[2019-03-26 22:13:01,853] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:01,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6755
[2019-03-26 22:13:01,868] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4724233350370068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664883.6020439009, 664883.6020439015, 180001.7606253187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4767004995338715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668384.50316243, 668384.50316243, 180320.1372688563], 
processed observation next is [1.0, 0.9565217391304348, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.3695186741371945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856623619895639, 0.1856623619895639, 0.26913453323709896], 
reward next is 0.7309, 
noisyNet noise sample is [array([-1.1271684], dtype=float32), -0.6872705]. 
=============================================
[2019-03-26 22:13:01,878] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.79804 ]
 [70.59361 ]
 [70.62267 ]
 [71.08851 ]
 [70.957664]], R is [[70.7077179 ]
 [70.73197937]
 [70.75643921]
 [70.78114319]
 [70.80612183]].
[2019-03-26 22:13:10,327] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4269152e-17 9.4192362e-01 4.7916683e-21 5.8076318e-02 3.9632217e-25], sum to 1.0000
[2019-03-26 22:13:10,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1146
[2019-03-26 22:13:10,339] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2061486.034449783 W.
[2019-03-26 22:13:10,348] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7371779772943376, 1.0, 2.0, 0.7371779772943376, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2061486.034449783, 2061486.034449783, 390445.4199790885], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4789800.0000, 
sim time next is 4790400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.9960249533638038, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.991819899821698, 6.9112, 168.9124773631943, 2289438.24064879, 2232243.871233437, 462616.0917984137], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.66, 1.0, 1.0, 0.9952107871853058, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008061989982169759, 0.0, 0.829437592320016, 0.6359550668468861, 0.6200677420092882, 0.6904717788036026], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.16081724], dtype=float32), 1.1216618]. 
=============================================
[2019-03-26 22:13:11,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0204409e-20 9.9999094e-01 4.6655455e-24 9.1076208e-06 1.4598276e-28], sum to 1.0000
[2019-03-26 22:13:11,370] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7421
[2019-03-26 22:13:11,378] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1739448.231634422 W.
[2019-03-26 22:13:11,387] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4147414246580975, 1.0, 2.0, 0.4147414246580975, 1.0, 1.0, 0.7149300597746647, 6.9112, 6.9112, 170.5573041426782, 1739448.231634422, 1739448.231634422, 360420.0867283415], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4883400.0000, 
sim time next is 4884000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.6309859418607651, 1.0, 2.0, 0.6309859418607651, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1764280.139299631, 1764280.139299631, 345979.2879572887], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5554047492298374, 1.0, 1.0, 0.5554047492298374, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4900778164721197, 0.4900778164721197, 0.5163869969511772], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1570537], dtype=float32), -0.28331032]. 
=============================================
[2019-03-26 22:13:11,389] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.6676594e-17 2.0006928e-06 6.4574443e-20 9.9999797e-01 6.6521049e-23], sum to 1.0000
[2019-03-26 22:13:11,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9348
[2019-03-26 22:13:11,410] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[38.29971 ]
 [38.86958 ]
 [36.068966]
 [34.880146]
 [34.047268]], R is [[38.97825623]
 [39.05053329]
 [39.12358475]
 [39.09865189]
 [39.05294418]].
[2019-03-26 22:13:11,411] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.7834611638971285, 1.0, 2.0, 0.7834611638971285, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2191047.314705163, 2191047.314705164, 411897.9937518623], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4879800.0000, 
sim time next is 4880400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8207617068102425, 1.0, 2.0, 0.8207617068102425, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2295462.059007527, 2295462.059007528, 430102.6312450256], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7840502491689669, 1.0, 1.0, 0.7840502491689669, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.637628349724313, 0.6376283497243133, 0.6419442257388442], 
reward next is 0.3581, 
noisyNet noise sample is [array([0.41552842], dtype=float32), -1.6585586]. 
=============================================
[2019-03-26 22:13:17,425] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:17,429] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4267
[2019-03-26 22:13:17,434] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.5090546872603902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711329.0709204307, 711329.0709204313, 185026.9719908704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5044200.0000, 
sim time next is 5044800.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.5069270928919654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708355.0793879728, 708355.0793879734, 184688.7124087242], 
processed observation next is [0.0, 0.391304347826087, 0.6208530805687204, 0.66, 1.0, 1.0, 0.4059362564963438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19676529982999244, 0.1967652998299926, 0.2756547946398869], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.3450868], dtype=float32), 0.7620836]. 
=============================================
[2019-03-26 22:13:24,771] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:24,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8194
[2019-03-26 22:13:24,786] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4680428867001918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661094.291969632, 661094.2919696313, 179654.2606958098], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5025600.0000, 
sim time next is 5026200.0000, 
raw observation next is [25.16666666666667, 89.0, 1.0, 2.0, 0.469517856486125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 661399.3400960708, 661399.3400960714, 179646.1751554383], 
processed observation next is [0.0, 0.17391304347826086, 0.39178515007898923, 0.89, 1.0, 1.0, 0.3608648873326807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1837220389155752, 0.18372203891557537, 0.2681286196349825], 
reward next is 0.7319, 
noisyNet noise sample is [array([-0.6413325], dtype=float32), 0.6134322]. 
=============================================
[2019-03-26 22:13:26,262] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.2226521e-38 1.0000000e+00 0.0000000e+00 1.3539876e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:13:26,271] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6923
[2019-03-26 22:13:26,280] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2299518.365870678 W.
[2019-03-26 22:13:26,284] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 1.003226681995998, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005993490116253, 6.9112, 168.912393111448, 2299518.365870678, 2232268.825749603, 464289.3823144138], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5235600.0000, 
sim time next is 5236200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5220915302370318, 1.0, 1.0, 0.5220915302370318, 1.0, 2.0, 0.9067003297143013, 6.9112, 6.9112, 170.5573041426782, 2190140.649759296, 2190140.649759296, 430759.5337516088], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.67, 1.0, 1.0, 0.42420666293618287, 1.0, 0.5, 0.42420666293618287, 1.0, 1.0, 0.8862199142857332, 0.0, 0.0, 0.8375144448122397, 0.6083724027109155, 0.6083724027109155, 0.6429246772412072], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.41456723], dtype=float32), -0.16288157]. 
=============================================
[2019-03-26 22:13:28,241] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:28,252] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2051
[2019-03-26 22:13:28,256] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5212672859450231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728400.2327483637, 728400.232748363, 186995.1039861079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5169600.0000, 
sim time next is 5170200.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5226573742461251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730343.3601159235, 730343.3601159242, 187221.8609097044], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.42488840270617484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2028731555877565, 0.2028731555877567, 0.27943561329806627], 
reward next is 0.7206, 
noisyNet noise sample is [array([1.3057979], dtype=float32), -0.13308792]. 
=============================================
[2019-03-26 22:13:29,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:29,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0288
[2019-03-26 22:13:29,149] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.516471439461945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721696.412124088, 721696.4121240875, 186216.6336728786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5094600.0000, 
sim time next is 5095200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5170087466958854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722447.478955866, 722447.478955866, 186303.4505701619], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4180828273444402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20067985526551832, 0.20067985526551832, 0.2780648515972566], 
reward next is 0.7219, 
noisyNet noise sample is [array([-1.1230899], dtype=float32), -2.1178913]. 
=============================================
[2019-03-26 22:13:32,030] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:13:32,039] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0543
[2019-03-26 22:13:32,044] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 70.0, 1.0, 2.0, 0.5205520509824388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727400.4468327963, 727400.4468327957, 186878.614573153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5128200.0000, 
sim time next is 5128800.0000, 
raw observation next is [29.66666666666667, 68.66666666666667, 1.0, 2.0, 0.5190657316220347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725322.8092902369, 725322.8092902362, 186636.9319086387], 
processed observation next is [0.0, 0.34782608695652173, 0.6050552922590839, 0.6866666666666668, 1.0, 1.0, 0.4205611224361864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20147855813617693, 0.20147855813617674, 0.27856258493826674], 
reward next is 0.7214, 
noisyNet noise sample is [array([-1.54176], dtype=float32), 0.8912926]. 
=============================================
[2019-03-26 22:13:33,498] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2783971e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:13:33,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7662
[2019-03-26 22:13:33,518] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.9679506127614, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129563912569, 1352977.139345992, 1352977.139345991, 289310.4431353405], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5194800.0000, 
sim time next is 5195400.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.9637876046699443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104012, 1347154.498811997, 1347154.498811997, 288084.1002453836], 
processed observation next is [1.0, 0.13043478260869565, 0.4312796208530806, 0.89, 1.0, 1.0, 0.9563706080360774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451521562, 0.3742095830033325, 0.3742095830033325, 0.42997626902296066], 
reward next is 0.5700, 
noisyNet noise sample is [array([1.1646637], dtype=float32), 0.44478554]. 
=============================================
[2019-03-26 22:13:33,937] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-03-26 22:13:33,939] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:13:33,940] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:13:33,941] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:13:33,941] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:13:33,942] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:13:33,943] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:13:33,944] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:13:33,944] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:13:33,945] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:13:33,945] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:13:33,976] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-03-26 22:13:33,998] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-03-26 22:13:34,026] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-03-26 22:13:34,044] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-03-26 22:13:34,060] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-03-26 22:13:39,737] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:13:39,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.13333333333333, 50.0, 1.0, 2.0, 0.2089501840069923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 349079.4272074345, 349079.4272074352, 156230.0225022871]
[2019-03-26 22:13:39,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:13:39,742] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7337233541712088
[2019-03-26 22:13:54,310] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:13:54,311] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.6, 98.0, 1.0, 2.0, 0.3148380840668547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497111.9423132369, 497111.9423132376, 166794.2627271379]
[2019-03-26 22:13:54,312] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:13:54,313] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8533562911632012
[2019-03-26 22:13:57,953] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:13:57,954] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.2, 95.0, 1.0, 2.0, 0.4185450189628827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617287.5628999281, 617287.5628999281, 175956.7214807995]
[2019-03-26 22:13:57,956] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:13:57,958] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2697581989985234
[2019-03-26 22:14:02,771] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:02,772] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.23333333333334, 94.33333333333334, 1.0, 2.0, 0.5126213085758445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716314.5801362657, 716314.5801362657, 185596.2963116054]
[2019-03-26 22:14:02,773] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:14:02,774] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11360497582638784
[2019-03-26 22:14:03,540] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:03,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.1, 90.66666666666667, 1.0, 2.0, 0.4346598384036131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632913.2905331261, 632913.2905331261, 177260.4834737669]
[2019-03-26 22:14:03,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:14:03,548] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6732260145539921
[2019-03-26 22:14:12,085] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:12,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.38357069, 91.58452961333333, 1.0, 2.0, 0.4661264185659061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664757.8925929978, 664757.8925929983, 180182.4015499274]
[2019-03-26 22:14:12,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:14:12,093] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14580479964170867
[2019-03-26 22:14:30,073] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:30,075] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.15, 71.5, 1.0, 2.0, 0.5278227935845811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737563.844575171, 737563.844575171, 188069.2560270657]
[2019-03-26 22:14:30,077] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:14:30,079] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7054137511407446
[2019-03-26 22:14:39,784] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:39,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 70.0, 1.0, 2.0, 0.4826137343085554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 676107.6275046763, 676107.627504677, 181140.1339086928]
[2019-03-26 22:14:39,788] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:14:39,791] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5237234099448633
[2019-03-26 22:14:50,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:50,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.26666666666667, 76.66666666666666, 1.0, 2.0, 0.5556041888372137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776398.9261553636, 776398.9261553636, 192766.0923145489]
[2019-03-26 22:14:50,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:14:50,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6467347636560015
[2019-03-26 22:14:51,966] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:51,967] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.62616588666667, 79.68276271666667, 1.0, 2.0, 0.5526431686332923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 772259.7049896735, 772259.7049896728, 192254.5063899793]
[2019-03-26 22:14:51,972] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:14:51,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2054238679364675
[2019-03-26 22:14:56,349] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:14:56,350] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.86671922666667, 65.01245505666667, 1.0, 2.0, 0.454390692235396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660144.1099245191, 660144.1099245185, 179968.1837133011]
[2019-03-26 22:14:56,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:14:56,354] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8221929338014309
[2019-03-26 22:15:09,752] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.92829853]
[2019-03-26 22:15:09,755] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.76134094166667, 92.73500234666666, 1.0, 2.0, 0.5193182374297095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725675.7718727257, 725675.7718727263, 186677.575990712]
[2019-03-26 22:15:09,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:15:09,759] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6002746758885201
[2019-03-26 22:15:27,728] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.3726 3164618349.2034 1778.0000
[2019-03-26 22:15:28,043] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1744 2927914232.3319 1338.0000
[2019-03-26 22:15:28,209] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 22:15:28,230] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0759 3008237470.0170 1766.0000
[2019-03-26 22:15:28,283] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4620 2779798268.0133 933.0000
[2019-03-26 22:15:29,300] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1625000, evaluation results [1625000.0, 7877.372585301126, 3164618349.203418, 1778.0, 8254.174442635573, 2927914232.3318715, 1338.0, 8660.46199861073, 2779798268.0133233, 933.0, 7998.075909215718, 3008237470.0169654, 1766.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 22:15:36,062] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0050618e-17 9.9999928e-01 2.9163703e-20 6.9207891e-07 9.1994237e-25], sum to 1.0000
[2019-03-26 22:15:36,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3890
[2019-03-26 22:15:36,081] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2457783.852297361 W.
[2019-03-26 22:15:36,086] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.65, 74.0, 1.0, 2.0, 0.8787442052615174, 1.0, 1.0, 0.8787442052615174, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2457783.852297361, 2457783.852297361, 460037.5670139098], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5473800.0000, 
sim time next is 5474400.0000, 
raw observation next is [31.86666666666667, 73.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.897767550139083, 6.9112, 168.9063191957534, 2984201.032241535, 2284323.559407284, 473822.6894225812], 
processed observation next is [1.0, 0.34782608695652173, 0.7093206951026858, 0.7333333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.09865675501390828, 0.0, 0.82940735289866, 0.8289447311782041, 0.6345343220575789, 0.7071980439143003], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.2375688], dtype=float32), 0.13810275]. 
=============================================
[2019-03-26 22:15:42,024] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7502854e-25 0.0000000e+00 1.2299497e-24 1.0000000e+00 7.6013385e-22], sum to 1.0000
[2019-03-26 22:15:42,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3061
[2019-03-26 22:15:42,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3601445.395979505 W.
[2019-03-26 22:15:42,053] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.01666666666667, 60.66666666666666, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.876258036378407, 6.9112, 170.5573041426782, 3601445.395979505, 2910135.068470608, 548113.5546518322], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5395800.0000, 
sim time next is 5396400.0000, 
raw observation next is [35.2, 60.0, 1.0, 2.0, 0.9326326773408885, 1.0, 2.0, 0.7869063781847069, 1.0, 1.0, 1.03, 7.005116082200111, 6.9112, 170.5573041426782, 3302495.319174009, 3235219.409232098, 604883.9244097867], 
processed observation next is [1.0, 0.4782608695652174, 0.8672985781990523, 0.6, 1.0, 1.0, 0.9188345510131187, 1.0, 1.0, 0.7432606966080806, 1.0, 0.5, 1.0365853658536586, 0.009391608220011083, 0.0, 0.8375144448122397, 0.9173598108816692, 0.8986720581200273, 0.9028118274772936], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6086851], dtype=float32), 1.0508394]. 
=============================================
[2019-03-26 22:15:42,824] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.853401e-37 0.000000e+00], sum to 1.0000
[2019-03-26 22:15:42,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6194
[2019-03-26 22:15:42,836] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.11666666666667, 81.5, 1.0, 2.0, 0.537093460397986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750522.9747255193, 750522.9747255193, 189611.9083308448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5687400.0000, 
sim time next is 5688000.0000, 
raw observation next is [27.9, 83.0, 1.0, 2.0, 0.5381944802430946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752062.0610859754, 752062.0610859761, 189796.6617814766], 
processed observation next is [0.0, 0.8695652173913043, 0.5213270142180094, 0.83, 1.0, 1.0, 0.4436078075218007, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2089061280794376, 0.2089061280794378, 0.28327859967384567], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.65253276], dtype=float32), 1.7489854]. 
=============================================
[2019-03-26 22:15:42,851] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.05738 ]
 [77.023865]
 [76.967064]
 [76.90066 ]
 [76.884796]], R is [[77.15220642]
 [77.09767914]
 [77.04395294]
 [76.99095154]
 [76.93857574]].
[2019-03-26 22:15:47,520] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.0195776e-12 1.7098620e-03 2.3320027e-14 9.9829012e-01 1.3217674e-16], sum to 1.0000
[2019-03-26 22:15:47,531] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-26 22:15:47,538] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.1, 64.0, 1.0, 2.0, 0.8079692912912843, 1.0, 2.0, 0.8079692912912843, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2259652.577151449, 2259652.577151449, 423771.8383526159], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5568000.0000, 
sim time next is 5568600.0000, 
raw observation next is [32.3, 63.0, 1.0, 2.0, 0.8109417557736444, 1.0, 2.0, 0.8109417557736444, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2267973.229507399, 2267973.229507399, 425235.3521110727], 
processed observation next is [1.0, 0.43478260869565216, 0.7298578199052131, 0.63, 1.0, 1.0, 0.7722189828598125, 1.0, 1.0, 0.7722189828598125, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6299925637520553, 0.6299925637520553, 0.6346796300165264], 
reward next is 0.3653, 
noisyNet noise sample is [array([1.9536865], dtype=float32), 0.22336191]. 
=============================================
[2019-03-26 22:15:55,334] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:15:55,339] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5984
[2019-03-26 22:15:55,342] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 87.0, 1.0, 2.0, 0.5186722328170297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724772.7613659343, 724772.761365935, 186572.6404854767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5705400.0000, 
sim time next is 5706000.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5168852551501861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722274.8581099614, 722274.8581099614, 186283.3548427695], 
processed observation next is [0.0, 0.043478260869565216, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41793404234962184, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20063190503054482, 0.20063190503054482, 0.27803485797428285], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.48479113], dtype=float32), -0.4429152]. 
=============================================
[2019-03-26 22:15:55,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.263626]
 [80.323875]
 [80.485306]
 [80.72767 ]
 [80.94937 ]], R is [[80.21569061]
 [80.1350708 ]
 [80.05506897]
 [79.97634888]
 [79.89903259]].
[2019-03-26 22:15:55,552] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.7983655e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 22:15:55,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1357
[2019-03-26 22:15:55,569] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.05, 91.83333333333333, 1.0, 2.0, 0.5207622642880092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727694.291853428, 727694.291853428, 186912.7021412179], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5619000.0000, 
sim time next is 5619600.0000, 
raw observation next is [26.0, 92.0, 1.0, 2.0, 0.5186026217205423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724675.4562992429, 724675.4562992435, 186561.8068549785], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.92, 1.0, 1.0, 0.4200031586994485, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2012987378609008, 0.20129873786090097, 0.27845045799250523], 
reward next is 0.7215, 
noisyNet noise sample is [array([0.6033183], dtype=float32), -0.31020588]. 
=============================================
[2019-03-26 22:15:56,272] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:15:56,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5809
[2019-03-26 22:15:56,289] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 92.5, 1.0, 2.0, 0.5159975761444457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721034.0298569204, 721034.0298569197, 186140.2664358656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5621400.0000, 
sim time next is 5622000.0000, 
raw observation next is [25.8, 92.66666666666667, 1.0, 2.0, 0.5148374441949914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719412.359690964, 719412.3596909647, 185953.2053432669], 
processed observation next is [0.0, 0.043478260869565216, 0.42180094786729866, 0.9266666666666667, 1.0, 1.0, 0.41546680023492943, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19983676658082333, 0.19983676658082353, 0.277542097527264], 
reward next is 0.7225, 
noisyNet noise sample is [array([0.63505983], dtype=float32), -1.2130784]. 
=============================================
[2019-03-26 22:15:56,311] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.754974]
 [77.67158 ]
 [77.61298 ]
 [77.590294]
 [77.31217 ]], R is [[77.78682709]
 [77.73113251]
 [77.67580414]
 [77.62090302]
 [77.56624603]].
[2019-03-26 22:16:02,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.884587e-38 0.000000e+00], sum to 1.0000
[2019-03-26 22:16:02,556] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8797
[2019-03-26 22:16:02,560] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.76666666666667, 91.33333333333333, 1.0, 2.0, 0.5113428308367751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714527.4904560291, 714527.4904560298, 185391.8352907108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5719200.0000, 
sim time next is 5719800.0000, 
raw observation next is [25.73333333333333, 91.66666666666667, 1.0, 2.0, 0.5107758387180705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713734.9349794593, 713734.9349794593, 185301.2033035423], 
processed observation next is [0.0, 0.17391304347826086, 0.41864139020537117, 0.9166666666666667, 1.0, 1.0, 0.4105732996603259, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19825970416096092, 0.19825970416096092, 0.27656896015454074], 
reward next is 0.7234, 
noisyNet noise sample is [array([-1.1080565], dtype=float32), 0.81354237]. 
=============================================
[2019-03-26 22:16:13,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:16:13,420] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2558
[2019-03-26 22:16:13,426] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 84.33333333333334, 1.0, 2.0, 0.5373380642153414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750864.899704013, 750864.8997040137, 189652.5980651308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6297600.0000, 
sim time next is 6298200.0000, 
raw observation next is [27.55, 84.5, 1.0, 2.0, 0.5370017189397386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750394.7318814987, 750394.7318814987, 189596.2391537265], 
processed observation next is [0.0, 0.9130434782608695, 0.504739336492891, 0.845, 1.0, 1.0, 0.4421707457105284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2084429810781941, 0.2084429810781941, 0.28297946142347236], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.54060847], dtype=float32), -0.5368345]. 
=============================================
[2019-03-26 22:16:15,730] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:16:15,749] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5461
[2019-03-26 22:16:15,753] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 82.83333333333334, 1.0, 2.0, 0.5585933127316969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780577.4499351569, 780577.4499351569, 193285.6528329755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5946600.0000, 
sim time next is 5947200.0000, 
raw observation next is [28.5, 83.0, 1.0, 2.0, 0.5555106039364517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776268.1032273677, 776268.1032273677, 192750.558847794], 
processed observation next is [1.0, 0.8695652173913043, 0.5497630331753555, 0.83, 1.0, 1.0, 0.4644706071523514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2156300286742688, 0.2156300286742688, 0.2876874012653642], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.83706254], dtype=float32), 0.70250124]. 
=============================================
[2019-03-26 22:16:17,310] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.8675861e-23 9.9999928e-01 3.3228085e-27 6.6908774e-07 5.1004484e-31], sum to 1.0000
[2019-03-26 22:16:17,311] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7527
[2019-03-26 22:16:17,315] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2316598.564421105 W.
[2019-03-26 22:16:17,320] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.45, 68.0, 1.0, 2.0, 0.5522081622031038, 1.0, 2.0, 0.5522081622031038, 1.0, 1.0, 0.9545884618711977, 6.9112, 6.9112, 170.5573041426782, 2316598.564421105, 2316598.564421105, 452236.4934860124], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6107400.0000, 
sim time next is 6108000.0000, 
raw observation next is [30.4, 68.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.078350389093667, 6.9112, 168.9119117102915, 2413085.01512735, 2294503.509714643, 476246.5972769526], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6833333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.016715038909366698, 0.0, 0.8294348147053061, 0.6703013930909306, 0.6373620860318453, 0.7108158168312725], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.12866585], dtype=float32), 0.23354976]. 
=============================================
[2019-03-26 22:16:17,350] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[43.58687]
 [44.41288]
 [44.66009]
 [44.34782]
 [45.58541]], R is [[44.0065918 ]
 [43.56652451]
 [43.13085938]
 [42.69955063]
 [42.2725563 ]].
[2019-03-26 22:16:17,553] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.412220e-28 1.000000e+00 1.812739e-34 6.969391e-16 0.000000e+00], sum to 1.0000
[2019-03-26 22:16:17,564] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0335
[2019-03-26 22:16:17,573] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1985223.271569236 W.
[2019-03-26 22:16:17,578] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.2, 81.5, 1.0, 2.0, 0.4732880046743647, 1.0, 1.0, 0.4732880046743647, 1.0, 1.0, 0.8219447453844791, 6.9112, 6.9112, 170.5573041426782, 1985223.271569236, 1985223.271569236, 397165.8130594303], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5992200.0000, 
sim time next is 5992800.0000, 
raw observation next is [29.36666666666667, 80.66666666666667, 1.0, 2.0, 0.8095546512696267, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005982081726877, 6.9112, 168.9123930249604, 2028445.605670739, 1961204.159061167, 410198.2937984788], 
processed observation next is [1.0, 0.34782608695652173, 0.5908372827804109, 0.8066666666666668, 1.0, 1.0, 0.770547772614008, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009478208172687718, 0.0, 0.8294371781806773, 0.5634571126863164, 0.5447789330725463, 0.6122362594007146], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11662429], dtype=float32), 0.9839322]. 
=============================================
[2019-03-26 22:16:19,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9832702e-35 1.0000000e+00 0.0000000e+00 9.6512752e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:16:19,675] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2892
[2019-03-26 22:16:19,683] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1750831.122673877 W.
[2019-03-26 22:16:19,687] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.73333333333333, 83.66666666666667, 1.0, 2.0, 0.6261749183691829, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.940349242005841, 6.9112, 168.912762428557, 1750831.122673877, 1730151.670916275, 370709.4189010464], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6013200.0000, 
sim time next is 6013800.0000, 
raw observation next is [27.3, 82.5, 1.0, 2.0, 0.4282594856960594, 1.0, 1.0, 0.4282594856960594, 1.0, 2.0, 0.7298651386923469, 6.911200000000001, 6.9112, 170.5573041426782, 1796191.237586087, 1796191.237586087, 366957.7133724727], 
processed observation next is [1.0, 0.6086956521739131, 0.4928909952606636, 0.825, 1.0, 1.0, 0.31115600686272216, 1.0, 0.5, 0.31115600686272216, 1.0, 1.0, 0.6705672423077402, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.49894201044057973, 0.49894201044057973, 0.547698079660407], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48327076], dtype=float32), -0.8536634]. 
=============================================
[2019-03-26 22:16:23,689] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:16:23,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1390
[2019-03-26 22:16:23,701] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 80.0, 1.0, 2.0, 0.5279106497349693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737686.6547711431, 737686.6547711438, 188084.2933918417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6376800.0000, 
sim time next is 6377400.0000, 
raw observation next is [27.9, 80.5, 1.0, 2.0, 0.5284218663403171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738401.2619552452, 738401.2619552445, 188168.6547549789], 
processed observation next is [0.0, 0.8260869565217391, 0.5213270142180094, 0.805, 1.0, 1.0, 0.43183357390399646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20511146165423477, 0.20511146165423458, 0.28084873844026703], 
reward next is 0.7192, 
noisyNet noise sample is [array([1.1154021], dtype=float32), -0.1571882]. 
=============================================
[2019-03-26 22:16:25,056] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 22:16:25,056] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:16:25,058] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:16:25,058] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:16:25,059] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:16:25,060] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:16:25,060] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:16:25,063] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:16:25,061] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:16:25,064] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:16:25,066] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:16:25,089] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-03-26 22:16:25,113] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-03-26 22:16:25,114] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-03-26 22:16:25,152] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-03-26 22:16:25,153] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-03-26 22:16:29,296] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:16:29,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.0, 44.0, 1.0, 2.0, 0.2713196300449886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 443994.8438360208, 443994.8438360214, 163074.8963688035]
[2019-03-26 22:16:29,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:16:29,304] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5160906345532387
[2019-03-26 22:16:32,969] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:16:32,970] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 89.0, 1.0, 2.0, 0.3650628306022027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 555065.9674609876, 555065.9674609883, 170859.0613904948]
[2019-03-26 22:16:32,971] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:16:32,974] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6371521688499305
[2019-03-26 22:16:52,630] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:16:52,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.05414786666666, 89.58020965666667, 1.0, 2.0, 0.5329461588054287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744725.5921450668, 744725.5921450668, 188919.695508392]
[2019-03-26 22:16:52,633] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:16:52,639] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1152892e-37 0.0000000e+00], sampled 0.8457223354427941
[2019-03-26 22:16:59,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:16:59,554] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.7, 85.66666666666666, 1.0, 2.0, 0.7379706602272049, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031360.79186168, 1031360.79186168, 229255.3350940973]
[2019-03-26 22:16:59,556] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:16:59,558] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2979269e-32 0.0000000e+00], sampled 0.2325328498896453
[2019-03-26 22:17:22,750] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:17:22,751] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.605268785, 81.43195834000001, 1.0, 2.0, 0.615640571724349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860327.4885970138, 860327.4885970143, 203696.8496220754]
[2019-03-26 22:17:22,752] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:17:22,754] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.289085e-38 0.000000e+00], sampled 0.40663530024318606
[2019-03-26 22:17:41,553] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:17:41,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 77.0, 1.0, 2.0, 0.5910509194376669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825951.2951986777, 825951.2951986777, 199090.0541790062]
[2019-03-26 22:17:41,555] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:17:41,559] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0536559e-36 0.0000000e+00], sampled 0.18151141759785294
[2019-03-26 22:17:41,668] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:17:41,670] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 82.33333333333334, 1.0, 2.0, 1.04, 1.0, 1.0, 1.04, 1.0, 1.0, 1.03, 7.834808439379148, 6.9112, 178.6582176852504, 4433916.00730795, 3740873.009310106, 703747.2413819409]
[2019-03-26 22:17:41,672] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:17:41,676] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5899648e-17 1.3387995e-14 1.2362001e-18 1.0000000e+00 1.9141739e-19], sampled 0.4001565588596576
[2019-03-26 22:17:41,678] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4433916.00730795 W.
[2019-03-26 22:18:11,110] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.95887417]
[2019-03-26 22:18:11,111] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 70.5, 1.0, 2.0, 0.5296885268665537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740171.873005966, 740171.873005966, 188378.554910512]
[2019-03-26 22:18:11,113] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:18:11,117] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6690656e-38 0.0000000e+00], sampled 0.34881014999566784
[2019-03-26 22:18:15,693] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.1901 3164364723.5056 1774.0000
[2019-03-26 22:18:16,235] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1133 2843234548.3666 1131.0000
[2019-03-26 22:18:16,415] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.3322 2779812778.4665 929.0000
[2019-03-26 22:18:16,446] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3173 2928051448.2792 1338.0000
[2019-03-26 22:18:16,585] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 22:18:17,598] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1650000, evaluation results [1650000.0, 7877.190135679386, 3164364723.505603, 1774.0, 8254.317319695507, 2928051448.279249, 1338.0, 8660.332211994151, 2779812778.4664755, 929.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.11326267071, 2843234548.366585, 1131.0]
[2019-03-26 22:18:18,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.9313402e-30 1.0000000e+00 5.0272864e-36 1.9289240e-17 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:18,637] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0490
[2019-03-26 22:18:18,644] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1819712.668055421 W.
[2019-03-26 22:18:18,650] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.5, 76.0, 1.0, 2.0, 0.6603977436166667, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97845328945569, 6.9112, 168.9124985817876, 1819712.668055421, 1772000.999184242, 377272.3495277368], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6512400.0000, 
sim time next is 6513000.0000, 
raw observation next is [28.71666666666667, 74.33333333333334, 1.0, 2.0, 0.4179251831715237, 1.0, 1.0, 0.4179251831715237, 1.0, 2.0, 0.7141581932520819, 6.9112, 6.9112, 170.5573041426782, 1752812.00999815, 1752812.00999815, 361305.082917273], 
processed observation next is [1.0, 0.391304347826087, 0.5600315955766194, 0.7433333333333334, 1.0, 1.0, 0.2987050399656912, 1.0, 0.5, 0.2987050399656912, 1.0, 1.0, 0.6514124307952217, 0.0, 0.0, 0.8375144448122397, 0.4868922249994861, 0.4868922249994861, 0.5392613177869746], 
reward next is 0.4607, 
noisyNet noise sample is [array([-1.1329106], dtype=float32), -0.9571134]. 
=============================================
[2019-03-26 22:18:18,666] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[36.10797 ]
 [36.48217 ]
 [38.991287]
 [43.012157]
 [49.524544]], R is [[34.00465775]
 [33.76525116]
 [33.49345016]
 [33.6237793 ]
 [33.75410843]].
[2019-03-26 22:18:20,664] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2606167e-13 4.8935547e-01 5.6268261e-16 5.1064450e-01 9.9666930e-19], sum to 1.0000
[2019-03-26 22:18:20,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0913
[2019-03-26 22:18:20,684] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2091651.02411975 W.
[2019-03-26 22:18:20,687] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.96666666666667, 79.0, 1.0, 2.0, 0.4986362126250295, 1.0, 1.0, 0.4986362126250295, 1.0, 2.0, 0.8657926532314795, 6.9112, 6.9112, 170.5573041426782, 2091651.02411975, 2091651.02411975, 414172.6205763209], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6171600.0000, 
sim time next is 6172200.0000, 
raw observation next is [29.05, 78.5, 1.0, 2.0, 0.8072947949527881, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.997596298071159, 6.9112, 168.9124419798065, 2025282.813977124, 1963990.493299223, 409965.2259292343], 
processed observation next is [1.0, 0.43478260869565216, 0.575829383886256, 0.785, 1.0, 1.0, 0.7678250541599857, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008639629807115856, 0.0, 0.8294374185713832, 0.56257855943809, 0.5455529148053397, 0.611888396909305], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2238717], dtype=float32), 0.18816812]. 
=============================================
[2019-03-26 22:18:21,582] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:18:21,590] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1239
[2019-03-26 22:18:21,594] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 86.33333333333334, 1.0, 2.0, 0.5240657996925021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732312.1235873987, 732312.1235873987, 187452.4993230246], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6327600.0000, 
sim time next is 6328200.0000, 
raw observation next is [27.1, 85.66666666666667, 1.0, 2.0, 0.5237910485583507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731928.0632159087, 731928.0632159094, 187407.5278585231], 
processed observation next is [0.0, 0.21739130434782608, 0.4834123222748816, 0.8566666666666667, 1.0, 1.0, 0.42625427537150684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.203313350893308, 0.20331335089330818, 0.2797127281470494], 
reward next is 0.7203, 
noisyNet noise sample is [array([-0.7038004], dtype=float32), -1.2653931]. 
=============================================
[2019-03-26 22:18:25,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.439808e-37 1.000000e+00 0.000000e+00 2.039944e-20 0.000000e+00], sum to 1.0000
[2019-03-26 22:18:25,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5450
[2019-03-26 22:18:25,909] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 84.33333333333334, 1.0, 2.0, 0.348190643190551, 1.0, 1.0, 0.348190643190551, 1.0, 1.0, 0.5925051963145399, 6.9112, 6.9112, 170.5573041426782, 1460140.472583009, 1460140.472583009, 324602.6760409565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6403200.0000, 
sim time next is 6403800.0000, 
raw observation next is [26.91666666666667, 84.66666666666667, 1.0, 2.0, 0.9635853913976372, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564569191, 1346871.671639188, 1346871.671639188, 288025.3351443943], 
processed observation next is [1.0, 0.08695652173913043, 0.4747235387045816, 0.8466666666666667, 1.0, 1.0, 0.9561269775875147, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399448895347, 0.37413101989977443, 0.37413101989977443, 0.4298885599170064], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7126833], dtype=float32), -0.09851803]. 
=============================================
[2019-03-26 22:18:27,356] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.914304e-33 0.000000e+00], sum to 1.0000
[2019-03-26 22:18:27,365] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2442
[2019-03-26 22:18:27,370] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 90.0, 1.0, 2.0, 0.4974812046635599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695151.539223384, 695151.539223384, 183202.4392411917], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6654000.0000, 
sim time next is 6654600.0000, 
raw observation next is [25.75, 90.5, 1.0, 2.0, 0.4968094246478205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694212.5258494577, 694212.5258494583, 183097.758938083], 
processed observation next is [1.0, 0.0, 0.41943127962085314, 0.905, 1.0, 1.0, 0.3937462947564102, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19283681273596048, 0.19283681273596065, 0.2732802372210194], 
reward next is 0.7267, 
noisyNet noise sample is [array([-0.7637781], dtype=float32), 1.2416121]. 
=============================================
[2019-03-26 22:18:27,473] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0509507e-38 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:27,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9354
[2019-03-26 22:18:27,489] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.33333333333334, 1.0, 2.0, 0.5297373591888676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740240.133698212, 740240.1336982113, 188386.3710716541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6308400.0000, 
sim time next is 6309000.0000, 
raw observation next is [27.3, 85.5, 1.0, 2.0, 0.5304310276095777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741209.7846891206, 741209.78468912, 188501.2586362029], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.855, 1.0, 1.0, 0.43425425013202135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20589160685808905, 0.20589160685808888, 0.2813451621435864], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.09483522], dtype=float32), 1.4785179]. 
=============================================
[2019-03-26 22:18:27,514] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[77.5092  ]
 [77.64054 ]
 [79.14056 ]
 [82.766556]
 [82.76397 ]], R is [[77.31801605]
 [77.26366425]
 [77.21006012]
 [77.15716553]
 [77.10490417]].
[2019-03-26 22:18:33,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.615736e-32 0.000000e+00], sum to 1.0000
[2019-03-26 22:18:33,347] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5005
[2019-03-26 22:18:33,353] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 85.33333333333334, 1.0, 2.0, 0.3150565004258759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502268.2413615047, 502268.2413615054, 167255.5722339166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6754800.0000, 
sim time next is 6755400.0000, 
raw observation next is [21.65, 85.5, 1.0, 2.0, 0.3190473610214441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 508937.811772104, 508937.8117721034, 167758.3191546652], 
processed observation next is [1.0, 0.17391304347826086, 0.22511848341232227, 0.855, 1.0, 1.0, 0.17957513376077602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14137161438114, 0.14137161438113985, 0.25038555097711224], 
reward next is 0.7496, 
noisyNet noise sample is [array([-1.8311542], dtype=float32), -0.53562456]. 
=============================================
[2019-03-26 22:18:34,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000e+00 1.0000e+00 0.0000e+00 1.3909e-25 0.0000e+00], sum to 1.0000
[2019-03-26 22:18:34,599] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3701
[2019-03-26 22:18:34,602] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 68.0, 1.0, 2.0, 0.7824940040863644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1220329.309675551, 1220329.309675551, 255869.7009395369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6771600.0000, 
sim time next is 6772200.0000, 
raw observation next is [25.43333333333333, 66.66666666666667, 1.0, 2.0, 0.8505409820210375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1326035.591225285, 1326035.591225284, 274888.0411811259], 
processed observation next is [1.0, 0.391304347826087, 0.40442338072669815, 0.6666666666666667, 1.0, 1.0, 0.8199288940012499, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3683432197848014, 0.3683432197848011, 0.4102806584792924], 
reward next is 0.5897, 
noisyNet noise sample is [array([-0.428711], dtype=float32), -1.316344]. 
=============================================
[2019-03-26 22:18:39,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.4290205e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:39,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2183
[2019-03-26 22:18:39,605] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 95.0, 1.0, 2.0, 0.6289204452708832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878893.1386424365, 878893.1386424365, 206247.6123008078], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6667800.0000, 
sim time next is 6668400.0000, 
raw observation next is [24.73333333333333, 95.0, 1.0, 2.0, 0.5815770692908994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 812707.2037236409, 812707.2037236402, 197356.9951960758], 
processed observation next is [1.0, 0.17391304347826086, 0.3712480252764612, 0.95, 1.0, 1.0, 0.4958759870974691, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2257520010343447, 0.2257520010343445, 0.29456267939712805], 
reward next is 0.7054, 
noisyNet noise sample is [array([-0.8067065], dtype=float32), -0.23022747]. 
=============================================
[2019-03-26 22:18:44,570] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5549184e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:44,578] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1598
[2019-03-26 22:18:44,583] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 73.66666666666667, 1.0, 2.0, 0.4850318140300224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 677749.9294035789, 677749.9294035796, 181284.0457140137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6552600.0000, 
sim time next is 6553200.0000, 
raw observation next is [27.9, 74.33333333333334, 1.0, 2.0, 0.4874215611961167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681090.2673966298, 681090.2673966304, 181648.7144888597], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.7433333333333334, 1.0, 1.0, 0.3824356158989357, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18919174094350827, 0.18919174094350844, 0.2711174843117309], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.717366], dtype=float32), 0.8649105]. 
=============================================
[2019-03-26 22:18:44,608] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:18:44,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8305
[2019-03-26 22:18:44,622] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 78.33333333333333, 1.0, 2.0, 0.3801581559053236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573762.7834650266, 573762.7834650266, 172355.1561369583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6853800.0000, 
sim time next is 6854400.0000, 
raw observation next is [24.9, 78.0, 1.0, 2.0, 0.3824561498131069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576316.7930854935, 576316.7930854928, 172554.259451626], 
processed observation next is [0.0, 0.34782608695652173, 0.3791469194312796, 0.78, 1.0, 1.0, 0.25597126483506855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16008799807930374, 0.16008799807930355, 0.2575436708233224], 
reward next is 0.7425, 
noisyNet noise sample is [array([0.21579264], dtype=float32), -0.28011283]. 
=============================================
[2019-03-26 22:18:48,317] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1121923e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:48,327] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6485
[2019-03-26 22:18:48,335] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.46666666666667, 84.33333333333333, 1.0, 2.0, 0.7629382917603506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1088594.82446674, 1088594.82446674, 238089.1137964427], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7009800.0000, 
sim time next is 7010400.0000, 
raw observation next is [25.43333333333334, 84.66666666666667, 1.0, 2.0, 0.7030153623709926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1002518.760874684, 1002518.760874685, 224181.3914568223], 
processed observation next is [1.0, 0.13043478260869565, 0.40442338072669864, 0.8466666666666667, 1.0, 1.0, 0.6421871835795091, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2784774335763011, 0.2784774335763014, 0.33459909172660046], 
reward next is 0.6654, 
noisyNet noise sample is [array([-0.05772366], dtype=float32), -0.6188993]. 
=============================================
[2019-03-26 22:18:52,453] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0022086e-35 1.0000000e+00 0.0000000e+00 5.3641824e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:52,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4319
[2019-03-26 22:18:52,469] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2244687.024349267 W.
[2019-03-26 22:18:52,474] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 62.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 8.025359870030123, 6.9112, 168.9070774952426, 2244687.024349267, 1454291.176810344, 311347.5165623235], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6700800.0000, 
sim time next is 6701400.0000, 
raw observation next is [30.05, 62.0, 1.0, 2.0, 0.4858307651490548, 1.0, 1.0, 0.4858307651490548, 1.0, 1.0, 0.82151124503592, 6.9112, 6.9112, 170.5573041426782, 2037884.363537367, 2037884.363537367, 401526.7061429888], 
processed observation next is [1.0, 0.5652173913043478, 0.6232227488151659, 0.62, 1.0, 1.0, 0.3805189941554878, 1.0, 0.5, 0.3805189941554878, 1.0, 0.5, 0.7823307866291705, 0.0, 0.0, 0.8375144448122397, 0.5660789898714909, 0.5660789898714909, 0.5992935912581923], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06978149], dtype=float32), 0.2092826]. 
=============================================
[2019-03-26 22:18:58,103] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0741136e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 22:18:58,113] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2937
[2019-03-26 22:18:58,120] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.28333333333333, 47.33333333333334, 1.0, 2.0, 0.8613250427671466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1341622.094675504, 1341622.094675504, 277913.0741629426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6793800.0000, 
sim time next is 6794400.0000, 
raw observation next is [29.26666666666667, 47.66666666666667, 1.0, 2.0, 0.9181585291881118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1428329.924168902, 1428329.924168902, 294985.724372357], 
processed observation next is [1.0, 0.6521739130434783, 0.5860979462875199, 0.47666666666666674, 1.0, 1.0, 0.9013958182989299, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.39675831226913943, 0.39675831226913943, 0.44027720055575675], 
reward next is 0.5597, 
noisyNet noise sample is [array([-0.2085992], dtype=float32), 1.5107079]. 
=============================================
[2019-03-26 22:19:02,085] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9957366e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 22:19:02,092] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2047
[2019-03-26 22:19:02,097] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 85.0, 1.0, 2.0, 0.6868715659387925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 978950.8100961419, 978950.8100961419, 220571.1360970712], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7011000.0000, 
sim time next is 7011600.0000, 
raw observation next is [25.36666666666667, 85.33333333333333, 1.0, 2.0, 0.6777628954659383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965422.0169614085, 965422.0169614085, 218538.9102346063], 
processed observation next is [1.0, 0.13043478260869565, 0.40126382306477115, 0.8533333333333333, 1.0, 1.0, 0.6117625246577569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26817278248928017, 0.26817278248928017, 0.32617747796209895], 
reward next is 0.6738, 
noisyNet noise sample is [array([0.33381143], dtype=float32), -0.47490498]. 
=============================================
[2019-03-26 22:19:03,096] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:19:03,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3004
[2019-03-26 22:19:03,111] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333333, 77.33333333333333, 1.0, 2.0, 0.4098891426627436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604911.2989535672, 604911.2989535672, 174800.0212395065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6907200.0000, 
sim time next is 6907800.0000, 
raw observation next is [25.56666666666667, 78.16666666666667, 1.0, 2.0, 0.4123665948635596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607280.2525582149, 607280.2525582149, 174984.587054393], 
processed observation next is [0.0, 0.9565217391304348, 0.41074249605055313, 0.7816666666666667, 1.0, 1.0, 0.2920079456187465, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1686889590439486, 0.1686889590439486, 0.2611710254543179], 
reward next is 0.7388, 
noisyNet noise sample is [array([0.7116708], dtype=float32), 0.061866693]. 
=============================================
[2019-03-26 22:19:04,236] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:19:04,247] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0537
[2019-03-26 22:19:04,256] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 45.0, 1.0, 2.0, 0.31181910228717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 491540.4331209147, 491540.4331209154, 166362.4966810736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6884400.0000, 
sim time next is 6885000.0000, 
raw observation next is [29.3, 46.0, 1.0, 2.0, 0.3207212474971916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503496.4204848484, 503496.4204848491, 167206.9356454084], 
processed observation next is [0.0, 0.6956521739130435, 0.5876777251184835, 0.46, 1.0, 1.0, 0.18159186445444772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13986011680134677, 0.13986011680134697, 0.24956259051553492], 
reward next is 0.7504, 
noisyNet noise sample is [array([1.083379], dtype=float32), 0.27823338]. 
=============================================
[2019-03-26 22:19:04,273] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[80.972015]
 [81.05791 ]
 [81.11764 ]
 [81.04974 ]
 [81.109505]], R is [[80.79941559]
 [80.74312592]
 [80.6881485 ]
 [80.6338501 ]
 [80.58080292]].
[2019-03-26 22:19:08,702] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:19:08,704] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3949
[2019-03-26 22:19:08,708] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.23333333333333, 59.66666666666666, 1.0, 2.0, 0.3922925721318365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587481.8435461828, 587481.8435461835, 173451.3244544136], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6986400.0000, 
sim time next is 6987000.0000, 
raw observation next is [28.11666666666667, 60.83333333333334, 1.0, 2.0, 0.3956676475549482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590568.7387648047, 590568.7387648054, 173675.1042919391], 
processed observation next is [0.0, 0.8695652173913043, 0.5315955766192735, 0.6083333333333334, 1.0, 1.0, 0.2718887319939135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1640468718791124, 0.1640468718791126, 0.25921657357005834], 
reward next is 0.7408, 
noisyNet noise sample is [array([-0.5268558], dtype=float32), 0.32678828]. 
=============================================
[2019-03-26 22:19:08,729] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[81.292534]
 [81.283165]
 [81.28692 ]
 [81.32408 ]
 [81.29148 ]], R is [[81.20288849]
 [81.1319809 ]
 [81.0620575 ]
 [80.99328613]
 [80.92615509]].
[2019-03-26 22:19:13,306] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 22:19:13,308] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:19:13,309] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:19:13,309] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:19:13,310] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:19:13,311] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:19:13,312] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:19:13,312] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:19:13,314] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:19:13,315] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:19:13,316] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:19:13,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-03-26 22:19:13,369] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-03-26 22:19:13,370] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-03-26 22:19:13,388] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-03-26 22:19:13,418] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-03-26 22:19:21,135] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:19:21,137] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.32990985, 77.720724535, 1.0, 2.0, 0.2707957344229319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 441300.1997740294, 441300.1997740294, 162968.8048040112]
[2019-03-26 22:19:21,139] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:19:21,141] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.32911597193584774
[2019-03-26 22:19:29,638] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:19:29,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.76386836333333, 88.12997349333332, 1.0, 2.0, 0.2868826729187444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462606.5699467123, 462606.5699467123, 164450.5002008466]
[2019-03-26 22:19:29,641] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:19:29,643] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6226671547223076
[2019-03-26 22:19:43,357] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:19:43,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.48333333333333, 94.16666666666667, 1.0, 2.0, 0.4673698672905682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656852.309046034, 656852.309046034, 179131.1315331012]
[2019-03-26 22:19:43,359] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:19:43,362] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3458644e-35 0.0000000e+00], sampled 0.32880252558587975
[2019-03-26 22:19:43,416] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:19:43,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.38645046333333, 85.18797660999999, 1.0, 2.0, 0.489860781422137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684499.7686174918, 684499.7686174912, 182022.9176233225]
[2019-03-26 22:19:43,423] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:19:43,426] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.2994893e-37 0.0000000e+00], sampled 0.6657858849533638
[2019-03-26 22:19:44,836] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:19:44,837] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.84677927666667, 83.18511197500001, 1.0, 2.0, 0.5250516802905327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733690.2361253521, 733690.2361253527, 187615.0706892335]
[2019-03-26 22:19:44,839] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:19:44,841] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.075554e-37 0.000000e+00], sampled 0.03358394720244673
[2019-03-26 22:20:11,001] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:20:11,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.31642434833334, 76.52441896333333, 1.0, 2.0, 0.659864340273918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 922154.8644038808, 922154.8644038814, 212429.5410594174]
[2019-03-26 22:20:11,003] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:20:11,005] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3588439e-35 0.0000000e+00], sampled 0.5760105388271884
[2019-03-26 22:20:13,150] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:20:13,151] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.0, 50.0, 1.0, 2.0, 0.9836026564267562, 1.0, 2.0, 0.9836026564267562, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2751388.264747042, 2751388.264747043, 519156.880435914]
[2019-03-26 22:20:13,151] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:20:13,155] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.9555246e-28 1.0000000e+00 1.2613811e-32 9.7064986e-17 8.8787958e-38], sampled 0.03191765662922119
[2019-03-26 22:20:13,157] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2751388.264747042 W.
[2019-03-26 22:20:19,631] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:20:19,633] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.0, 61.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.203395327873448, 6.9112, 168.9112249626969, 2501054.386059414, 2293763.088869594, 475951.9076714903]
[2019-03-26 22:20:19,634] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:20:19,637] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.8906348e-20 9.9999964e-01 2.2814025e-23 3.2328609e-07 4.5160696e-27], sampled 0.29344439081358065
[2019-03-26 22:20:19,639] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2501054.386059414 W.
[2019-03-26 22:20:21,879] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:20:21,880] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.6773540024207381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 946607.4127502411, 946607.4127502404, 216031.3701620026]
[2019-03-26 22:20:21,881] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:20:21,885] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4129755e-33 0.0000000e+00], sampled 0.5562513709410931
[2019-03-26 22:20:56,871] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:20:56,873] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.87455465333333, 72.19568667, 1.0, 2.0, 0.5070373348992842, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760512.7415830878, 760512.7415830878, 191119.9463870736]
[2019-03-26 22:20:56,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:20:56,877] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.0655156e-36 0.0000000e+00], sampled 0.6418010163096618
[2019-03-26 22:21:07,100] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7889.6894 3163403249.7665 1750.0000
[2019-03-26 22:21:07,204] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.7607 2927737973.4830 1327.0000
[2019-03-26 22:21:07,213] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97060925]
[2019-03-26 22:21:07,214] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.64545458, 66.33870781, 1.0, 2.0, 0.5542023223942705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 774439.2498256566, 774439.249825656, 192520.2939038113]
[2019-03-26 22:21:07,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:21:07,217] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0111916e-36 0.0000000e+00], sampled 0.15874806810412279
[2019-03-26 22:21:07,602] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.3055 2842874686.3516 1121.0000
[2019-03-26 22:21:07,724] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.9723 3008177790.8767 1762.0000
[2019-03-26 22:21:07,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8664.2842 2779422068.8958 920.0000
[2019-03-26 22:21:08,923] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1675000, evaluation results [1675000.0, 7889.689355067585, 3163403249.76653, 1750.0, 8259.760744723142, 2927737973.483029, 1327.0, 8664.284234644037, 2779422068.895819, 920.0, 7999.972309892194, 3008177790.876713, 1762.0, 8498.30547158034, 2842874686.351606, 1121.0]
[2019-03-26 22:21:15,433] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.253622e-35 0.000000e+00], sum to 1.0000
[2019-03-26 22:21:15,441] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9792
[2019-03-26 22:21:15,446] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 84.0, 1.0, 2.0, 0.4712319309942674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664058.2664899619, 664058.2664899619, 179933.0783742315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7164000.0000, 
sim time next is 7164600.0000, 
raw observation next is [25.78333333333333, 84.16666666666667, 1.0, 2.0, 0.4705645336530451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663273.8861411756, 663273.8861411756, 179853.4742387891], 
processed observation next is [1.0, 0.9565217391304348, 0.4210110584518167, 0.8416666666666667, 1.0, 1.0, 0.36212594416029537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18424274615032657, 0.18424274615032657, 0.268438021251924], 
reward next is 0.7316, 
noisyNet noise sample is [array([2.6909997], dtype=float32), -2.1982243]. 
=============================================
[2019-03-26 22:21:40,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:40,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:40,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-03-26 22:21:41,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:41,505] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:41,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-03-26 22:21:43,059] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1168350e-33 1.0000000e+00 0.0000000e+00 2.3114826e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 22:21:43,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9417
[2019-03-26 22:21:43,078] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.4, 76.33333333333334, 1.0, 2.0, 0.4819135733933614, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673391.3344554967, 673391.3344554974, 180812.9041100988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7665600.0000, 
sim time next is 7666200.0000, 
raw observation next is [28.25, 77.0, 1.0, 2.0, 0.4816789110035518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673063.3302474434, 673063.3302474428, 180777.314184341], 
processed observation next is [1.0, 0.7391304347826086, 0.537914691943128, 0.77, 1.0, 1.0, 0.37551676024524316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1869620361798454, 0.18696203617984522, 0.2698168868423], 
reward next is 0.7302, 
noisyNet noise sample is [array([-0.07645804], dtype=float32), -0.5648495]. 
=============================================
[2019-03-26 22:21:43,185] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1651296e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:21:43,197] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1330
[2019-03-26 22:21:43,206] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.58333333333334, 59.66666666666666, 1.0, 2.0, 0.986055042668314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1378299.485932685, 1378299.485932685, 294702.6286454769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7650600.0000, 
sim time next is 7651200.0000, 
raw observation next is [30.66666666666667, 59.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.584305587778193, 6.9112, 168.9037514645026, 2641418.123513496, 1454524.119951271, 310281.3035789727], 
processed observation next is [1.0, 0.5652173913043478, 0.6524486571879939, 0.5933333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.16731055877781928, 0.0, 0.829394744162706, 0.7337272565315267, 0.40403447776424195, 0.4631064232521981], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.15869975], dtype=float32), -1.4426022]. 
=============================================
[2019-03-26 22:21:45,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:45,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:45,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-03-26 22:21:47,005] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8353939e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 22:21:47,014] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7145
[2019-03-26 22:21:47,019] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 79.0, 1.0, 2.0, 0.5261323101251307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735200.7958745282, 735200.7958745282, 187792.7899914839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7842600.0000, 
sim time next is 7843200.0000, 
raw observation next is [28.46666666666667, 80.0, 1.0, 2.0, 0.5262272140191933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735333.4575026871, 735333.4575026877, 187808.4575748914], 
processed observation next is [1.0, 0.782608695652174, 0.5481832543443919, 0.8, 1.0, 1.0, 0.4291894144809557, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20425929375074642, 0.20425929375074658, 0.2803111307087931], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.09534216], dtype=float32), 0.41828927]. 
=============================================
[2019-03-26 22:21:49,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:49,579] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:49,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-03-26 22:21:50,499] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:50,500] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:50,551] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-03-26 22:21:52,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:52,119] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:52,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-03-26 22:21:53,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:53,491] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:53,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-03-26 22:21:54,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:54,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:54,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-03-26 22:21:54,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:54,723] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:54,788] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-03-26 22:21:57,256] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.0453586e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 22:21:57,261] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9693
[2019-03-26 22:21:57,267] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.5, 1.0, 2.0, 0.9552732516788897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1425884.400256732, 1425884.400256732, 298995.2809642474], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 127800.0000, 
sim time next is 128400.0000, 
raw observation next is [22.8, 95.66666666666667, 1.0, 2.0, 0.9088440893745711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1355578.060794108, 1355578.060794109, 284677.1901405191], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9566666666666667, 1.0, 1.0, 0.8901736016561097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3765494613316967, 0.3765494613316969, 0.424891328567939], 
reward next is 0.5751, 
noisyNet noise sample is [array([-0.71257937], dtype=float32), -1.1911361]. 
=============================================
[2019-03-26 22:21:57,441] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:57,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:57,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-03-26 22:21:57,631] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:57,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:57,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-03-26 22:21:58,312] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:58,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:58,369] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-03-26 22:21:58,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6763086e-38 0.0000000e+00], sum to 1.0000
[2019-03-26 22:21:58,482] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5887
[2019-03-26 22:21:58,486] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 89.5, 1.0, 2.0, 0.5296601283048128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740132.1758280952, 740132.1758280959, 188373.6006343171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7943400.0000, 
sim time next is 7944000.0000, 
raw observation next is [26.63333333333333, 90.0, 1.0, 2.0, 0.528340222514659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738287.1356138209, 738287.1356138209, 188155.5502397603], 
processed observation next is [1.0, 0.9565217391304348, 0.46129541864139006, 0.9, 1.0, 1.0, 0.43173520784898667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20507975989272803, 0.20507975989272803, 0.2808291794623288], 
reward next is 0.7192, 
noisyNet noise sample is [array([-0.85085696], dtype=float32), -0.25764182]. 
=============================================
[2019-03-26 22:21:58,497] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[71.128044]
 [71.13345 ]
 [71.19331 ]
 [71.30689 ]
 [71.19273 ]], R is [[71.16603088]
 [71.17321777]
 [71.18034363]
 [71.1879425 ]
 [71.1957016 ]].
[2019-03-26 22:21:58,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:58,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:58,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-03-26 22:21:58,761] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:58,762] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:58,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-03-26 22:21:59,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:59,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:59,177] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-03-26 22:21:59,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:21:59,627] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:21:59,677] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-03-26 22:22:01,018] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-03-26 22:22:01,023] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:22:01,024] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:22:01,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:22:01,026] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:22:01,026] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:22:01,027] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:22:01,029] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:22:01,030] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:22:01,031] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:22:01,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:22:01,050] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-03-26 22:22:01,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-03-26 22:22:01,096] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-03-26 22:22:01,113] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-03-26 22:22:01,140] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-03-26 22:22:11,548] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0473833]
[2019-03-26 22:22:11,550] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.53867751666667, 59.23002340666667, 1.0, 2.0, 0.2152905764278489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 360136.082054398, 360136.082054398, 142070.2995723659]
[2019-03-26 22:22:11,552] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:22:11,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11038054150763521
[2019-03-26 22:22:55,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0473833]
[2019-03-26 22:22:55,904] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.77447453, 58.5429552, 1.0, 2.0, 0.7210878599282079, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.996022585943309, 6.9112, 168.9123755160181, 1904638.58623682, 1844462.731382617, 389666.6414589692]
[2019-03-26 22:22:55,904] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:22:55,908] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.493755e-32 1.000000e+00 9.615441e-38 3.838673e-12 0.000000e+00], sampled 0.31594366383629835
[2019-03-26 22:22:55,908] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1904638.58623682 W.
[2019-03-26 22:23:06,324] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0473833]
[2019-03-26 22:23:06,325] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.201090615, 57.396333195, 1.0, 2.0, 0.5818217143672246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813049.2063984413, 813049.2063984413, 197408.7402738647]
[2019-03-26 22:23:06,326] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:23:06,329] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3577922e-38 0.0000000e+00], sampled 0.3781831847689767
[2019-03-26 22:23:20,301] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0473833]
[2019-03-26 22:23:20,302] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.85, 73.0, 1.0, 2.0, 0.5699094366103854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796396.5124781496, 796396.5124781496, 195274.9081794913]
[2019-03-26 22:23:20,303] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:23:20,307] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4674992579358451
[2019-03-26 22:23:55,570] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0473833]
[2019-03-26 22:23:55,571] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 72.0, 1.0, 2.0, 0.8901363875255813, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986447906911978, 6.9112, 168.912448713151, 2141230.265879454, 2087846.97136788, 432052.0940336953]
[2019-03-26 22:23:55,572] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:23:55,574] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7084753e-22 9.3876648e-01 7.0779828e-27 6.1233539e-02 2.1817723e-31], sampled 0.39635465958803506
[2019-03-26 22:23:55,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2141230.265879454 W.
[2019-03-26 22:23:55,610] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8284.6778 2924947991.3877 1268.0000
[2019-03-26 22:23:55,688] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8684.7301 2777065661.5128 857.0000
[2019-03-26 22:23:55,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8056.2019 3002451274.9123 1622.0000
[2019-03-26 22:23:55,904] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7970.9912 3155630849.1997 1543.0000
[2019-03-26 22:23:55,947] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8525.0087 2839684381.1570 1047.0000
[2019-03-26 22:23:56,964] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 1700000, evaluation results [1700000.0, 7970.991249799608, 3155630849.199671, 1543.0, 8284.677835104865, 2924947991.3877387, 1268.0, 8684.730124127102, 2777065661.512752, 857.0, 8056.20188008874, 3002451274.9123344, 1622.0, 8525.008704274895, 2839684381.156953, 1047.0]
[2019-03-26 22:23:59,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.8124837e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 22:23:59,769] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2622
[2019-03-26 22:23:59,774] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.93333333333333, 96.0, 1.0, 2.0, 0.288645434796434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464586.690818226, 464586.6908182266, 164586.425793016], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 182400.0000, 
sim time next is 183000.0000, 
raw observation next is [19.91666666666666, 96.0, 1.0, 2.0, 0.2878720947516818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 463490.9715213497, 463490.9715213497, 164511.4154489585], 
processed observation next is [0.0, 0.08695652173913043, 0.14296998420221146, 0.96, 1.0, 1.0, 0.14201457198997805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1287474920892638, 0.1287474920892638, 0.24553942604322163], 
reward next is 0.7545, 
noisyNet noise sample is [array([-0.49860826], dtype=float32), -0.0997011]. 
=============================================
[2019-03-26 22:23:59,810] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[72.368744]
 [72.533615]
 [72.85655 ]
 [73.25147 ]
 [72.855705]], R is [[72.30391693]
 [72.33522797]
 [72.36606598]
 [72.39638519]
 [72.42619324]].
[2019-03-26 22:24:12,848] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:24:12,860] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5816
[2019-03-26 22:24:12,863] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.63333333333333, 80.0, 1.0, 2.0, 0.3022909784836246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 479864.490612335, 479864.4906123343, 165585.9281784119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 294000.0000, 
sim time next is 294600.0000, 
raw observation next is [22.71666666666667, 79.5, 1.0, 2.0, 0.3029723686281392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480799.7242377104, 480799.7242377104, 165650.3145247471], 
processed observation next is [0.0, 0.391304347826087, 0.2756714060031597, 0.795, 1.0, 1.0, 0.16020767304595085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13355547895491957, 0.13355547895491957, 0.24723927541007032], 
reward next is 0.7528, 
noisyNet noise sample is [array([-1.1148328], dtype=float32), -2.2406976]. 
=============================================
[2019-03-26 22:24:21,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5313035e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:21,274] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7898
[2019-03-26 22:24:21,278] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 81.0, 1.0, 2.0, 0.28165705529294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455468.6955350673, 455468.6955350667, 163961.1921212406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 410400.0000, 
sim time next is 411000.0000, 
raw observation next is [21.41666666666667, 81.0, 1.0, 2.0, 0.2815007482903867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 455917.5421540022, 455917.5421540016, 163984.7839345743], 
processed observation next is [1.0, 0.782608695652174, 0.2140600315955769, 0.81, 1.0, 1.0, 0.1343382509522731, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12664376170944505, 0.12664376170944489, 0.2447534088575736], 
reward next is 0.7552, 
noisyNet noise sample is [array([0.41484118], dtype=float32), -1.4350482]. 
=============================================
[2019-03-26 22:24:21,299] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[76.79111 ]
 [76.349075]
 [76.13565 ]
 [75.6149  ]
 [74.931046]], R is [[76.89530945]
 [76.88163757]
 [76.86763   ]
 [76.85323334]
 [76.83881378]].
[2019-03-26 22:24:21,827] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:24:21,837] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3271
[2019-03-26 22:24:21,841] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.15, 91.5, 1.0, 2.0, 0.2087223887586454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 348792.9853543976, 348792.9853543976, 156150.0011927825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 613800.0000, 
sim time next is 614400.0000, 
raw observation next is [17.13333333333333, 91.66666666666666, 1.0, 2.0, 0.206440420760213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 344979.0289928328, 344979.0289928322, 155960.3284788288], 
processed observation next is [1.0, 0.08695652173913043, 0.011058451816745531, 0.9166666666666665, 1.0, 1.0, 0.04390412139784696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09582750805356467, 0.09582750805356449, 0.23277660966989372], 
reward next is 0.7672, 
noisyNet noise sample is [array([-0.30110484], dtype=float32), -0.08074584]. 
=============================================
[2019-03-26 22:24:23,916] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7326925e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:23,929] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1749
[2019-03-26 22:24:23,933] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.16666666666667, 1.0, 2.0, 0.2156493060691995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 157168.2631591955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [17.6, 92.0, 1.0, 2.0, 0.2149129967709744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073988, 157091.67273694], 
processed observation next is [1.0, 0.17391304347826086, 0.0331753554502371, 0.92, 1.0, 1.0, 0.05411204430237877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09950699402983283, 0.099506994029833, 0.2344651831894627], 
reward next is 0.7655, 
noisyNet noise sample is [array([0.33725137], dtype=float32), 1.6474985]. 
=============================================
[2019-03-26 22:24:29,093] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1242741e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:29,102] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7857
[2019-03-26 22:24:29,110] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 57.5, 1.0, 2.0, 0.6350003701382125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1041487.492166851, 1041487.492166852, 223328.2821097738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 567000.0000, 
sim time next is 567600.0000, 
raw observation next is [24.06666666666667, 58.0, 1.0, 2.0, 0.6372104790311179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044985.718161123, 1044985.718161123, 223828.8376568352], 
processed observation next is [1.0, 0.5652173913043478, 0.3396524486571882, 0.58, 1.0, 1.0, 0.5629041916037565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29027381060031193, 0.29027381060031193, 0.3340728920251272], 
reward next is 0.6659, 
noisyNet noise sample is [array([0.64423364], dtype=float32), -1.4445462]. 
=============================================
[2019-03-26 22:24:30,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.252653e-30 0.000000e+00], sum to 1.0000
[2019-03-26 22:24:30,921] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7442
[2019-03-26 22:24:30,927] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 62.33333333333334, 1.0, 2.0, 0.2889146941406687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462546.0055829607, 462546.0055829613, 164430.3787668172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 822000.0000, 
sim time next is 822600.0000, 
raw observation next is [24.9, 62.5, 1.0, 2.0, 0.2886547487374136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462167.6222549282, 462167.6222549276, 164404.7663079722], 
processed observation next is [0.0, 0.5217391304347826, 0.3791469194312796, 0.625, 1.0, 1.0, 0.14295752859929348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1283798950708134, 0.1283798950708132, 0.24538024822085402], 
reward next is 0.7546, 
noisyNet noise sample is [array([-0.02138143], dtype=float32), -0.03509686]. 
=============================================
[2019-03-26 22:24:33,095] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4831988e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:33,105] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5275
[2019-03-26 22:24:33,112] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 50.0, 1.0, 2.0, 0.6286001612381256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1027981.185747482, 1027981.185747482, 221830.122651141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 739800.0000, 
sim time next is 740400.0000, 
raw observation next is [25.86666666666667, 49.33333333333334, 1.0, 2.0, 0.6256760036857524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1024025.362968306, 1024025.362968305, 221193.4819674156], 
processed observation next is [1.0, 0.5652173913043478, 0.42496050552922615, 0.4933333333333334, 1.0, 1.0, 0.5490072333563282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2844514897134183, 0.28445148971341805, 0.3301395253245009], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.14028068], dtype=float32), -0.4447834]. 
=============================================
[2019-03-26 22:24:35,832] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3310057e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:35,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6692
[2019-03-26 22:24:35,844] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.5204941257039716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806659.1760114438, 806659.1760114438, 196285.1326613611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 982800.0000, 
sim time next is 983400.0000, 
raw observation next is [21.91666666666667, 93.16666666666667, 1.0, 2.0, 0.499242761645745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773201.4396798844, 773201.4396798844, 192457.7354956286], 
processed observation next is [1.0, 0.391304347826087, 0.23775671406003188, 0.9316666666666668, 1.0, 1.0, 0.3966780260792108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21477817768885676, 0.21477817768885676, 0.28725035148601286], 
reward next is 0.7127, 
noisyNet noise sample is [array([-0.3777125], dtype=float32), -0.1700355]. 
=============================================
[2019-03-26 22:24:37,293] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8172014e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 22:24:37,300] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6946
[2019-03-26 22:24:37,305] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 76.5, 1.0, 2.0, 0.287851958334223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461785.5987357184, 461785.5987357184, 164387.0768557938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 808200.0000, 
sim time next is 808800.0000, 
raw observation next is [22.76666666666667, 75.33333333333334, 1.0, 2.0, 0.287771708005965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461659.387396554, 461659.3873965534, 164378.4425637016], 
processed observation next is [0.0, 0.34782608695652173, 0.2780410742496052, 0.7533333333333334, 1.0, 1.0, 0.1418936241035723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.128238718721265, 0.12823871872126483, 0.2453409590503009], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.9850224], dtype=float32), -1.6434853]. 
=============================================
[2019-03-26 22:24:39,698] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:24:39,705] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9521
[2019-03-26 22:24:39,712] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 80.66666666666666, 1.0, 2.0, 0.2966932051623078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473573.2531905661, 473573.2531905668, 165181.3424400975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 849000.0000, 
sim time next is 849600.0000, 
raw observation next is [22.2, 81.0, 1.0, 2.0, 0.2958970004740888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 472506.0193142867, 472506.0193142861, 165108.8733975341], 
processed observation next is [0.0, 0.8695652173913043, 0.2511848341232228, 0.81, 1.0, 1.0, 0.1516831331013118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1312516720317463, 0.13125167203174615, 0.24643115432467777], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.1202323], dtype=float32), -2.182715]. 
=============================================
[2019-03-26 22:24:42,915] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:24:42,925] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6510
[2019-03-26 22:24:42,931] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.63333333333333, 78.16666666666667, 1.0, 2.0, 0.2951472756626367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471027.3742975031, 471027.3742975031, 165001.1317824985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 904200.0000, 
sim time next is 904800.0000, 
raw observation next is [22.76666666666667, 77.33333333333334, 1.0, 2.0, 0.2960186946083676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472269.7981640099, 472269.7981640099, 165086.349841958], 
processed observation next is [0.0, 0.4782608695652174, 0.2780410742496052, 0.7733333333333334, 1.0, 1.0, 0.15182975254020192, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1311860550455583, 0.1311860550455583, 0.24639753707754924], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.22775738], dtype=float32), 1.4857001]. 
=============================================
[2019-03-26 22:24:52,923] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 22:24:52,925] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:24:52,925] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:24:52,927] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:24:52,928] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:24:52,929] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:24:52,929] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:24:52,930] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:24:52,931] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:24:52,931] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:24:52,933] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:24:52,957] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-03-26 22:24:52,979] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-03-26 22:24:53,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-03-26 22:24:53,032] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-03-26 22:24:53,062] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-03-26 22:25:22,922] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0916358]
[2019-03-26 22:25:22,924] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.63333333333333, 91.16666666666667, 1.0, 2.0, 0.4648821818516207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660784.9606828019, 660784.9606828013, 179716.6627090316]
[2019-03-26 22:25:22,925] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:25:22,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.3583867144677112
[2019-03-26 22:25:27,409] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0916358]
[2019-03-26 22:25:27,411] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 71.33333333333334, 1.0, 2.0, 0.5775932545384559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848144.8079690207, 848144.8079690207, 201825.9113971845]
[2019-03-26 22:25:27,412] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:25:27,414] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1055776e-38 0.0000000e+00], sampled 0.7239765775585498
[2019-03-26 22:25:35,821] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0916358]
[2019-03-26 22:25:35,822] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.3, 61.66666666666667, 1.0, 2.0, 0.5958434274523742, 0.0, 2.0, 0.0, 1.0, 2.0, 1.02726576265069, 6.9112, 6.9112, 168.9128852349116, 1665955.476049651, 1665955.476049651, 363414.5393948546]
[2019-03-26 22:25:35,824] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:25:35,828] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.540799e-27 0.000000e+00], sampled 0.22611241586430364
[2019-03-26 22:25:35,829] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1665955.476049651 W.
[2019-03-26 22:26:17,231] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0916358]
[2019-03-26 22:26:17,235] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.016911825, 86.53358144, 1.0, 2.0, 0.6326602883205923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 884121.6067298009, 884121.6067298015, 206986.1483559282]
[2019-03-26 22:26:17,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:26:17,240] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.04570083596819008
[2019-03-26 22:26:47,523] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 22:26:47,682] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.9734 2928048931.2162 1338.0000
[2019-03-26 22:26:47,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.6161 2779835940.2096 931.0000
[2019-03-26 22:26:47,891] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8585 3164735026.8284 1778.0000
[2019-03-26 22:26:48,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1133 2843234548.3666 1131.0000
[2019-03-26 22:26:49,024] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1725000, evaluation results [1725000.0, 7875.858472249107, 3164735026.8283834, 1778.0, 8253.973399524697, 2928048931.2161584, 1338.0, 8659.616110210765, 2779835940.2096453, 931.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.11326267071, 2843234548.366585, 1131.0]
[2019-03-26 22:26:50,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:26:50,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4715
[2019-03-26 22:26:50,077] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 90.66666666666667, 1.0, 2.0, 0.4689051040121825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659438.2118562575, 659438.2118562581, 179413.6465499122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1284600.0000, 
sim time next is 1285200.0000, 
raw observation next is [24.9, 91.0, 1.0, 2.0, 0.4702149066054256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660943.2037496103, 660943.2037496098, 179564.9241401693], 
processed observation next is [1.0, 0.9130434782608695, 0.3791469194312796, 0.91, 1.0, 1.0, 0.36170470675352484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18359533437489176, 0.1835953343748916, 0.26800734946293925], 
reward next is 0.7320, 
noisyNet noise sample is [array([-1.3291705], dtype=float32), 0.06037212]. 
=============================================
[2019-03-26 22:27:04,087] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.088443e-38 0.000000e+00], sum to 1.0000
[2019-03-26 22:27:04,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1613
[2019-03-26 22:27:04,101] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 94.16666666666667, 1.0, 2.0, 0.5283210951349117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772385.0555037736, 772385.0555037736, 192449.4217741873], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1321800.0000, 
sim time next is 1322400.0000, 
raw observation next is [23.43333333333334, 94.33333333333334, 1.0, 2.0, 0.4359369424408381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 638819.8099186216, 638819.8099186221, 177944.6119180975], 
processed observation next is [1.0, 0.30434782608695654, 0.30963665086887876, 0.9433333333333335, 1.0, 1.0, 0.3204059547479977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1774499471996171, 0.17744994719961726, 0.26558897301208584], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.37398443], dtype=float32), 1.2969879]. 
=============================================
[2019-03-26 22:27:06,577] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:27:06,584] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2276
[2019-03-26 22:27:06,589] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 99.0, 1.0, 2.0, 0.4681029145885947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675971.529121182, 675971.5291211813, 181536.7750607812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1650600.0000, 
sim time next is 1651200.0000, 
raw observation next is [23.26666666666667, 99.0, 1.0, 2.0, 0.472636319865294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682080.1074267359, 682080.1074267353, 182177.0432744259], 
processed observation next is [1.0, 0.08695652173913043, 0.3017377567140602, 0.99, 1.0, 1.0, 0.36462207212686015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18946669650742665, 0.18946669650742648, 0.2719060347379491], 
reward next is 0.7281, 
noisyNet noise sample is [array([-0.37076378], dtype=float32), -0.4848162]. 
=============================================
[2019-03-26 22:27:06,725] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.264849e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:27:06,733] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1603
[2019-03-26 22:27:06,737] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.53333333333333, 93.50000000000001, 1.0, 2.0, 0.8206010728133645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1155979.010444981, 1155979.01044498, 250321.3774954729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1674600.0000, 
sim time next is 1675200.0000, 
raw observation next is [24.66666666666667, 93.0, 1.0, 2.0, 0.8098102132885903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1137376.366077181, 1137376.366077181, 247119.4415666324], 
processed observation next is [1.0, 0.391304347826087, 0.36808846761453423, 0.93, 1.0, 1.0, 0.7708556786609521, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3159378794658836, 0.3159378794658836, 0.3688349874128842], 
reward next is 0.6312, 
noisyNet noise sample is [array([-0.9298073], dtype=float32), -0.43360895]. 
=============================================
[2019-03-26 22:27:19,406] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.899821e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:27:19,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3676
[2019-03-26 22:27:19,422] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 87.0, 1.0, 2.0, 0.4856036854836487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678549.2782577785, 678549.2782577785, 181371.1205841529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1882800.0000, 
sim time next is 1883400.0000, 
raw observation next is [25.81666666666666, 87.0, 1.0, 2.0, 0.48264653024892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674415.8407097274, 674415.8407097274, 180921.6819981368], 
processed observation next is [1.0, 0.8260869565217391, 0.4225908372827801, 0.87, 1.0, 1.0, 0.37668256656496385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18733773353047983, 0.18733773353047983, 0.27003236119124896], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.6600151], dtype=float32), 1.1957893]. 
=============================================
[2019-03-26 22:27:24,659] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:27:24,667] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5490
[2019-03-26 22:27:24,672] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 86.33333333333334, 1.0, 2.0, 0.7236314791354455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080989.417543761, 1080989.417543761, 235162.7322399884], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1606800.0000, 
sim time next is 1607400.0000, 
raw observation next is [23.9, 87.0, 1.0, 2.0, 0.7590619797247687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1133001.807641249, 1133001.807641248, 243730.3944333877], 
processed observation next is [1.0, 0.6086956521739131, 0.33175355450236965, 0.87, 1.0, 1.0, 0.7097132285840587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31472272434479137, 0.3147227243447911, 0.3637767081095339], 
reward next is 0.6362, 
noisyNet noise sample is [array([1.1743959], dtype=float32), 0.889546]. 
=============================================
[2019-03-26 22:27:24,963] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7594704e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 22:27:24,975] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8615
[2019-03-26 22:27:24,982] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.83333333333333, 1.0, 2.0, 0.3108920714226966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490465.1565361891, 490465.1565361884, 166292.216691375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1570200.0000, 
sim time next is 1570800.0000, 
raw observation next is [21.6, 89.66666666666667, 1.0, 2.0, 0.3063452297009812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 483532.9459328792, 483532.9459328799, 165790.6940724863], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.8966666666666667, 1.0, 1.0, 0.16427136108551954, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13431470720357758, 0.13431470720357777, 0.2474487971231139], 
reward next is 0.7526, 
noisyNet noise sample is [array([0.827969], dtype=float32), 1.6453496]. 
=============================================
[2019-03-26 22:27:38,185] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.242195e-24 0.000000e+00], sum to 1.0000
[2019-03-26 22:27:38,196] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4736
[2019-03-26 22:27:38,201] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 94.5, 1.0, 2.0, 0.562948585415229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 786665.7550940972, 786665.7550940979, 194041.5687225549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2179800.0000, 
sim time next is 2180400.0000, 
raw observation next is [25.36666666666667, 93.66666666666666, 1.0, 2.0, 0.5759891990431278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804895.6455888246, 804895.6455888246, 196352.5197220974], 
processed observation next is [1.0, 0.21739130434782608, 0.40126382306477115, 0.9366666666666665, 1.0, 1.0, 0.4891436133049733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2235821237746735, 0.2235821237746735, 0.2930634622717872], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.06638987], dtype=float32), 1.1001118]. 
=============================================
[2019-03-26 22:27:41,614] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5403899e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 22:27:41,620] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2784
[2019-03-26 22:27:41,627] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 88.0, 1.0, 2.0, 0.8802901067739083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238590.070452928, 1238590.070452928, 265779.7171240391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1854000.0000, 
sim time next is 1854600.0000, 
raw observation next is [25.4, 87.66666666666667, 1.0, 2.0, 1.007353601553031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1413716.224149723, 1413716.224149723, 302060.8261400507], 
processed observation next is [1.0, 0.4782608695652174, 0.4028436018957346, 0.8766666666666667, 1.0, 1.0, 1.0088597609072665, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.39269895115270087, 0.39269895115270087, 0.4508370539403742], 
reward next is 0.5492, 
noisyNet noise sample is [array([-0.350041], dtype=float32), 1.1121672]. 
=============================================
[2019-03-26 22:27:42,651] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0586966e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:27:42,659] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0101
[2019-03-26 22:27:42,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1918995.396001925 W.
[2019-03-26 22:27:42,673] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.566555968187854, 6.9112, 168.9092991121784, 1918995.396001925, 1454073.383528432, 311348.8221448578], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [27.03333333333333, 84.33333333333334, 1.0, 2.0, 0.6327289527537521, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.958064766692962, 6.9112, 168.9121405252895, 1769171.965829776, 1735924.647989005, 371849.1825278089], 
processed observation next is [1.0, 0.6086956521739131, 0.48025276461295413, 0.8433333333333334, 1.0, 1.0, 0.5575047623539181, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.00468647666929618, 0.0, 0.8294359382916983, 0.49143665717493773, 0.482201291108057, 0.5549987798922521], 
reward next is 0.2107, 
noisyNet noise sample is [array([1.2867891], dtype=float32), -0.31356585]. 
=============================================
[2019-03-26 22:27:45,069] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 22:27:45,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:27:45,073] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:27:45,073] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:27:45,077] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:27:45,078] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:27:45,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:27:45,080] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:27:45,081] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:27:45,082] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:27:45,081] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:27:45,116] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-03-26 22:27:45,139] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-03-26 22:27:45,141] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-03-26 22:27:45,162] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-03-26 22:27:45,163] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-03-26 22:29:12,361] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1320328]
[2019-03-26 22:29:12,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.36546891166667, 69.33014150833334, 1.0, 2.0, 0.5794367597224418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 809715.1531192866, 809715.1531192873, 196977.3592615327]
[2019-03-26 22:29:12,363] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:29:12,366] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.16842295e-30
 0.00000000e+00], sampled 0.5586444727281029
[2019-03-26 22:29:28,494] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1320328]
[2019-03-26 22:29:28,496] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.77982208, 74.14975835999999, 1.0, 2.0, 0.378730177028055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605941.881117263, 605941.881117263, 175675.8719061285]
[2019-03-26 22:29:28,498] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:29:28,500] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.536438e-31 0.000000e+00], sampled 0.6835107263205025
[2019-03-26 22:29:37,160] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1320328]
[2019-03-26 22:29:37,161] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.23112191333333, 72.21724542999999, 1.0, 2.0, 0.4640533002129067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648427.0577567363, 648427.0577567369, 178156.6417537002]
[2019-03-26 22:29:37,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:29:37,167] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.388066e-30 0.000000e+00], sampled 0.42287911350413465
[2019-03-26 22:29:38,002] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.2616 2843368869.3792 1131.0000
[2019-03-26 22:29:38,654] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.0081 3164870559.0756 1778.0000
[2019-03-26 22:29:38,784] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7003 2928231904.9171 1338.0000
[2019-03-26 22:29:38,849] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.4301 2780033067.3742 932.0000
[2019-03-26 22:29:38,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6078 3008551080.6251 1766.0000
[2019-03-26 22:29:39,882] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1750000, evaluation results [1750000.0, 7875.008135704462, 3164870559.075631, 1778.0, 8253.700304448676, 2928231904.9170933, 1338.0, 8658.430100356285, 2780033067.374156, 932.0, 7997.607833681256, 3008551080.6250534, 1766.0, 8494.261612308823, 2843368869.379151, 1131.0]
[2019-03-26 22:29:44,100] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4288977e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:29:44,108] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8688
[2019-03-26 22:29:44,113] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 85.33333333333334, 1.0, 2.0, 0.5507295009673426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769584.59041656, 769584.5904165606, 191926.0473097618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2144400.0000, 
sim time next is 2145000.0000, 
raw observation next is [27.78333333333333, 86.16666666666666, 1.0, 2.0, 0.5487331775684147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766793.9386372145, 766793.9386372138, 191583.8244840042], 
processed observation next is [0.0, 0.8260869565217391, 0.5157977883096366, 0.8616666666666666, 1.0, 1.0, 0.4563050332149574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21299831628811514, 0.21299831628811494, 0.2859460066925436], 
reward next is 0.7141, 
noisyNet noise sample is [array([1.9042797], dtype=float32), -1.3357465]. 
=============================================
[2019-03-26 22:29:44,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.91464 ]
 [74.85788 ]
 [74.808014]
 [74.775116]
 [74.7873  ]], R is [[74.92909241]
 [74.89334869]
 [74.8576355 ]
 [74.82195282]
 [74.78604126]].
[2019-03-26 22:29:47,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.2770352e-32 1.0000000e+00 1.3123912e-37 2.7986428e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 22:29:47,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0196
[2019-03-26 22:29:47,935] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1673039.557563231 W.
[2019-03-26 22:29:47,941] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.26666666666667, 68.0, 1.0, 2.0, 0.5983796589154178, 1.0, 2.0, 0.5983796589154178, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1673039.557563231, 1673039.557563231, 333659.2102641676], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2205600.0000, 
sim time next is 2206200.0000, 
raw observation next is [31.38333333333333, 67.5, 1.0, 2.0, 0.5967521351909496, 1.0, 2.0, 0.5967521351909496, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1668485.540332944, 1668485.540332944, 333060.5580589952], 
processed observation next is [1.0, 0.5217391304347826, 0.6864139020537123, 0.675, 1.0, 1.0, 0.5141591990252404, 1.0, 1.0, 0.5141591990252404, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.46346820564804, 0.46346820564804, 0.4971053105358137], 
reward next is 0.5029, 
noisyNet noise sample is [array([1.4669476], dtype=float32), 1.1391703]. 
=============================================
[2019-03-26 22:29:53,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5025795e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:29:53,904] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7908
[2019-03-26 22:29:53,909] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 90.33333333333334, 1.0, 2.0, 0.6942852340193061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970279.7476539674, 970279.7476539667, 219615.0888536948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2182800.0000, 
sim time next is 2183400.0000, 
raw observation next is [26.4, 89.5, 1.0, 2.0, 0.7177656982018268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003109.739681735, 1003109.739681735, 224725.4000877046], 
processed observation next is [1.0, 0.2608695652173913, 0.45023696682464454, 0.895, 1.0, 1.0, 0.6599586725323214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2786415943560375, 0.2786415943560375, 0.3354110449070218], 
reward next is 0.6646, 
noisyNet noise sample is [array([-0.5077373], dtype=float32), -0.36629033]. 
=============================================
[2019-03-26 22:29:56,395] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1279112e-18 9.9994671e-01 5.0023389e-22 5.3260850e-05 8.9836112e-27], sum to 1.0000
[2019-03-26 22:29:56,405] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7834
[2019-03-26 22:29:56,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1884143.013920815 W.
[2019-03-26 22:29:56,425] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.96666666666667, 63.33333333333334, 1.0, 2.0, 0.67381664028585, 1.0, 2.0, 0.67381664028585, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1884143.013920815, 1884143.013920815, 363119.937363203], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2290800.0000, 
sim time next is 2291400.0000, 
raw observation next is [31.95, 63.5, 1.0, 2.0, 0.6116733161022571, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.940665744447587, 6.9112, 168.9127365035694, 1710250.903922292, 1689346.917905713, 367023.1026267754], 
processed observation next is [1.0, 0.5217391304347826, 0.7132701421800948, 0.635, 1.0, 1.0, 0.532136525424406, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0029465744447587207, 0.0, 0.8294388648178926, 0.47506969553397, 0.46926303275158693, 0.5477956755623513], 
reward next is 0.3049, 
noisyNet noise sample is [array([0.6410692], dtype=float32), 0.4274024]. 
=============================================
[2019-03-26 22:29:57,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.411329e-23 0.000000e+00], sum to 1.0000
[2019-03-26 22:29:57,671] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2534
[2019-03-26 22:29:57,676] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 80.0, 1.0, 2.0, 0.5516432354518738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 770861.8983163009, 770861.8983163016, 192083.0174811172], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2422800.0000, 
sim time next is 2423400.0000, 
raw observation next is [28.76666666666667, 80.0, 1.0, 2.0, 0.5502540305683199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768919.9317527176, 768919.9317527176, 191844.3924685367], 
processed observation next is [1.0, 0.043478260869565216, 0.5624012638230649, 0.8, 1.0, 1.0, 0.45813738622689143, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21358886993131043, 0.21358886993131043, 0.28633491413214435], 
reward next is 0.7137, 
noisyNet noise sample is [array([-1.1730468], dtype=float32), -1.3939738]. 
=============================================
[2019-03-26 22:29:58,444] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1123673e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 22:29:58,452] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2053
[2019-03-26 22:29:58,459] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.7262191595118556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1014929.476273502, 1014929.476273502, 226603.468039346], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2258400.0000, 
sim time next is 2259000.0000, 
raw observation next is [26.1, 86.5, 1.0, 2.0, 0.7114756872993393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 994315.052190677, 994315.0521906777, 223338.8088254442], 
processed observation next is [1.0, 0.13043478260869565, 0.4360189573459717, 0.865, 1.0, 1.0, 0.6523803461437823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2761986256085214, 0.2761986256085216, 0.3333415057096182], 
reward next is 0.6667, 
noisyNet noise sample is [array([-0.56614685], dtype=float32), 0.24784397]. 
=============================================
[2019-03-26 22:29:58,477] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.079906]
 [62.31081 ]
 [63.39407 ]
 [63.358402]
 [62.997494]], R is [[62.07616043]
 [62.11718369]
 [62.14749146]
 [62.20656586]
 [62.2592659 ]].
[2019-03-26 22:30:12,452] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.085142e-15 0.000000e+00], sum to 1.0000
[2019-03-26 22:30:12,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2600
[2019-03-26 22:30:12,463] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7456871e-22 1.0000000e+00 2.7631184e-25 1.3112783e-10 2.2814560e-30], sum to 1.0000
[2019-03-26 22:30:12,466] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2155262.297216825 W.
[2019-03-26 22:30:12,469] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.46666666666667, 86.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.899379819576126, 6.9112, 168.9077422002604, 2155262.297216825, 1454235.163924476, 311350.9177910119], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2449200.0000, 
sim time next is 2449800.0000, 
raw observation next is [27.35, 86.5, 1.0, 2.0, 0.7121299032552998, 1.0, 1.0, 0.7121299032552998, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1991375.086080634, 1991375.086080634, 379358.7340800553], 
processed observation next is [1.0, 0.34782608695652173, 0.4952606635071091, 0.865, 1.0, 1.0, 0.6531685581389154, 1.0, 0.5, 0.6531685581389154, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5531597461335095, 0.5531597461335095, 0.5662070657911273], 
reward next is 0.4338, 
noisyNet noise sample is [array([-2.4665604], dtype=float32), -0.32686868]. 
=============================================
[2019-03-26 22:30:12,472] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5816
[2019-03-26 22:30:12,482] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1998916.596002042 W.
[2019-03-26 22:30:12,487] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.25, 69.0, 1.0, 2.0, 0.7148242872239496, 1.0, 2.0, 0.7148242872239496, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1998916.596002042, 1998916.596002042, 380533.5635667243], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2557800.0000, 
sim time next is 2558400.0000, 
raw observation next is [30.16666666666667, 69.33333333333333, 1.0, 2.0, 0.4935237406445926, 1.0, 2.0, 0.4935237406445926, 1.0, 1.0, 0.8514275757485958, 6.9112, 6.9112, 170.5573041426782, 2070184.793408245, 2070184.793408245, 409688.2115094162], 
processed observation next is [1.0, 0.6086956521739131, 0.6287519747235389, 0.6933333333333332, 1.0, 1.0, 0.38978763933083443, 1.0, 1.0, 0.38978763933083443, 1.0, 0.5, 0.8188141167665803, 0.0, 0.0, 0.8375144448122397, 0.5750513315022903, 0.5750513315022903, 0.6114749425513675], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06278577], dtype=float32), -1.7848864]. 
=============================================
[2019-03-26 22:30:21,155] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.7854355e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:30:21,166] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1219
[2019-03-26 22:30:21,175] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4770851508227688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666642.3273552274, 666642.3273552274, 180083.5644885299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2704200.0000, 
sim time next is 2704800.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4787173979121028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 668923.8227857114, 668923.8227857107, 180328.5257766519], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.37194867218325645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18581217299603095, 0.18581217299603076, 0.2691470533979879], 
reward next is 0.7309, 
noisyNet noise sample is [array([0.22593576], dtype=float32), 0.85461634]. 
=============================================
[2019-03-26 22:30:24,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5173794e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 22:30:24,371] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7180
[2019-03-26 22:30:24,377] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.361777469942939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 552609.0492098521, 552609.0492098528, 170729.5282432186], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2760000.0000, 
sim time next is 2760600.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3568873499027899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547143.1780567582, 547143.1780567582, 170330.9879352086], 
processed observation next is [0.0, 0.9565217391304348, 0.2417061611374408, 0.95, 1.0, 1.0, 0.22516548181059026, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1519842161268773, 0.1519842161268773, 0.25422535512717703], 
reward next is 0.7458, 
noisyNet noise sample is [array([0.5513086], dtype=float32), 0.4577393]. 
=============================================
[2019-03-26 22:30:30,547] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000e+00 1.0000e+00 0.0000e+00 9.5622e-27 0.0000e+00], sum to 1.0000
[2019-03-26 22:30:30,558] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0209
[2019-03-26 22:30:30,564] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3940222073388642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 587940.3788081195, 587940.3788081189, 173428.985432352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2740800.0000, 
sim time next is 2741400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3937032667045529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587464.4672788488, 587464.4672788488, 173385.4817238476], 
processed observation next is [0.0, 0.7391304347826086, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2695220080777746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16318457424412466, 0.16318457424412466, 0.25878430108036954], 
reward next is 0.7412, 
noisyNet noise sample is [array([0.4476843], dtype=float32), 0.5344628]. 
=============================================
[2019-03-26 22:30:33,855] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.233922e-25 0.000000e+00], sum to 1.0000
[2019-03-26 22:30:33,862] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7777
[2019-03-26 22:30:33,868] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 84.0, 1.0, 2.0, 0.7553082718076954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1107020.553110888, 1107020.553110888, 240178.0434369872], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2814600.0000, 
sim time next is 2815200.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.6960582949264096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1019228.509947903, 1019228.509947904, 226111.2174311034], 
processed observation next is [1.0, 0.6086956521739131, 0.38388625592417064, 0.83, 1.0, 1.0, 0.633805174610132, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28311903054108417, 0.28311903054108445, 0.33747942900164685], 
reward next is 0.6625, 
noisyNet noise sample is [array([0.8292777], dtype=float32), 2.1776927]. 
=============================================
[2019-03-26 22:30:35,474] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 22:30:35,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:30:35,478] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:30:35,479] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:30:35,479] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:30:35,481] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:30:35,482] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:30:35,481] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:30:35,484] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:30:35,482] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:30:35,485] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:30:35,510] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-03-26 22:30:35,534] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-03-26 22:30:35,535] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-03-26 22:30:35,581] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-03-26 22:30:35,581] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-03-26 22:30:49,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:30:49,349] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.66666666666666, 79.0, 1.0, 2.0, 0.3453903872341522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 536563.2445335628, 536563.2445335622, 169661.9658012919]
[2019-03-26 22:30:49,350] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:30:49,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.540808e-27 0.000000e+00], sampled 0.5639131275619921
[2019-03-26 22:30:51,738] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:30:51,739] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.1, 92.0, 1.0, 2.0, 0.309179572014055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490600.9813541404, 490600.981354141, 166361.1022286511]
[2019-03-26 22:30:51,742] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:30:51,746] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.3942982e-27 0.0000000e+00], sampled 0.20882385479462207
[2019-03-26 22:30:53,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:30:53,880] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [18.9, 87.0, 1.0, 2.0, 0.242235925038001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401473.13757976, 401473.13757976, 159986.6678764064]
[2019-03-26 22:30:53,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:30:53,883] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9050032e-27 0.0000000e+00], sampled 0.27823618815018636
[2019-03-26 22:31:06,921] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:06,923] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 90.5, 1.0, 2.0, 0.4252009587678136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622029.9816659256, 622029.9816659249, 176275.209738235]
[2019-03-26 22:31:06,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:31:06,930] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.722036e-28 0.000000e+00], sampled 0.5115756876434644
[2019-03-26 22:31:17,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:17,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.38333333333333, 56.66666666666666, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.023188441594606, 6.9112, 168.9121270303451, 1533257.401566534, 1453809.336901788, 311349.6504081668]
[2019-03-26 22:31:17,655] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:31:17,658] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5363755e-19 0.0000000e+00], sampled 0.8323798389392546
[2019-03-26 22:31:20,449] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:20,450] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 77.66666666666667, 1.0, 2.0, 0.5785170416906711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808429.4338046989, 808429.4338046989, 196811.5653438302]
[2019-03-26 22:31:20,451] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:31:20,455] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.987889e-28 0.000000e+00], sampled 0.975444258081871
[2019-03-26 22:31:32,547] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:32,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.28187011333333, 53.36184479166667, 1.0, 2.0, 0.6266324691104926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 875694.4581840336, 875694.4581840342, 205812.7813452245]
[2019-03-26 22:31:32,551] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:31:32,554] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9378456e-27 0.0000000e+00], sampled 0.47361604020175774
[2019-03-26 22:31:32,761] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:32,762] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.51666666666667, 72.0, 1.0, 2.0, 0.5768556483062026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 806106.8937853602, 806106.8937853602, 196512.4713393849]
[2019-03-26 22:31:32,764] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:31:32,766] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.8938674e-26 0.0000000e+00], sampled 0.27850620064566456
[2019-03-26 22:31:34,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:34,763] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.70826766666667, 73.67306379666667, 1.0, 2.0, 0.5631549324189616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786954.2117280976, 786954.2117280976, 194081.2969260614]
[2019-03-26 22:31:34,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:31:34,767] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5323434e-28 0.0000000e+00], sampled 0.2475192262078859
[2019-03-26 22:31:38,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:38,520] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 84.0, 1.0, 2.0, 0.6165534573315273, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861603.7188795534, 861603.7188795528, 203870.7025154484]
[2019-03-26 22:31:38,523] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:31:38,526] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.631656e-26 0.000000e+00], sampled 0.06869534518503229
[2019-03-26 22:31:38,595] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:38,597] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.23333333333333, 70.0, 1.0, 2.0, 0.5402511847323668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754937.0795682597, 754937.0795682597, 190143.1176444344]
[2019-03-26 22:31:38,598] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:31:38,601] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8574864e-26 0.0000000e+00], sampled 0.6587697855893907
[2019-03-26 22:31:42,295] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:31:42,298] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 1.0, 2.0, 0.918676411733252, 1.0, 1.0, 0.918676411733252, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2569586.181536476, 2569586.181536475, 481782.9465983783]
[2019-03-26 22:31:42,301] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:31:42,305] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.7196765e-24 1.0000000e+00 2.1797747e-28 5.4804633e-10 2.4140644e-33], sampled 0.7670103953656076
[2019-03-26 22:31:42,306] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2569586.181536476 W.
[2019-03-26 22:32:04,766] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:32:04,767] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.0, 80.0, 1.0, 2.0, 0.5677049197141256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793314.750548947, 793314.750548947, 194882.9815300843]
[2019-03-26 22:32:04,768] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:32:04,771] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.2669816e-26 0.0000000e+00], sampled 0.5788007296083146
[2019-03-26 22:32:07,685] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:32:07,687] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.12919423666667, 73.68785993, 1.0, 2.0, 0.5639026657048244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787999.483809995, 787999.4838099943, 194208.7072770786]
[2019-03-26 22:32:07,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:32:07,694] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3987337e-26 0.0000000e+00], sampled 0.010882263132173775
[2019-03-26 22:32:10,774] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:32:10,776] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.4, 90.66666666666667, 1.0, 2.0, 0.5024254328288744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702062.6015008674, 702062.6015008668, 183975.954123145]
[2019-03-26 22:32:10,781] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:32:10,786] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0622615e-26 0.0000000e+00], sampled 0.07778947811375092
[2019-03-26 22:32:15,447] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1117699]
[2019-03-26 22:32:15,449] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.53333333333333, 82.66666666666667, 1.0, 2.0, 0.5027453203639911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 702509.7431496605, 702509.7431496598, 184026.2914519113]
[2019-03-26 22:32:15,450] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:32:15,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8652936e-27 0.0000000e+00], sampled 0.8660711528959597
[2019-03-26 22:32:29,279] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8585 3164735026.8284 1778.0000
[2019-03-26 22:32:29,314] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7946 3008425931.0052 1766.0000
[2019-03-26 22:32:29,373] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.2822 2779898109.6615 932.0000
[2019-03-26 22:32:29,423] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.8896 2928105078.1461 1338.0000
[2019-03-26 22:32:29,653] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1133 2843234548.3666 1131.0000
[2019-03-26 22:32:30,670] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1775000, evaluation results [1775000.0, 7875.858472249107, 3164735026.8283834, 1778.0, 8253.889598136777, 2928105078.1460648, 1338.0, 8659.282224018918, 2779898109.66152, 932.0, 7997.794624158659, 3008425931.005194, 1766.0, 8495.11326267071, 2843234548.366585, 1131.0]
[2019-03-26 22:32:33,648] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9177455e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:32:33,656] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7891
[2019-03-26 22:32:33,664] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3604358247515858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570259.3613013607, 570259.3613013614, 172624.7549877047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2999400.0000, 
sim time next is 3000000.0000, 
raw observation next is [20.66666666666667, 96.0, 1.0, 2.0, 0.3155535954409784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499881.1584047105, 499881.1584047111, 167032.9784127846], 
processed observation next is [1.0, 0.7391304347826086, 0.17851500789889443, 0.96, 1.0, 1.0, 0.175365777639733, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1388558773346418, 0.13885587733464197, 0.2493029528549024], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.06325359], dtype=float32), -1.0707766]. 
=============================================
[2019-03-26 22:32:33,679] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[78.88923 ]
 [76.39047 ]
 [75.99598 ]
 [76.146835]
 [76.70465 ]], R is [[79.61701202]
 [79.56318665]
 [79.46291351]
 [79.35659027]
 [79.25640106]].
[2019-03-26 22:32:37,688] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2555185e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:32:37,697] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5496
[2019-03-26 22:32:37,701] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3076281445635441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489877.7509462302, 489877.7509462296, 166335.5080246674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3006600.0000, 
sim time next is 3007200.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3066347481513457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488296.1033477023, 488296.1033477029, 166220.340476851], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16462017849559726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13563780648547286, 0.13563780648547302, 0.24809006041321047], 
reward next is 0.7519, 
noisyNet noise sample is [array([-0.1754419], dtype=float32), -0.74118316]. 
=============================================
[2019-03-26 22:32:48,314] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.8139534e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:32:48,325] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4611
[2019-03-26 22:32:48,333] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 79.00000000000001, 1.0, 2.0, 0.5146523632729149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 719153.6477883166, 719153.647788316, 185923.4973853219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3276600.0000, 
sim time next is 3277200.0000, 
raw observation next is [27.66666666666667, 79.0, 1.0, 2.0, 0.5115656084541126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714838.8946300927, 714838.8946300927, 185427.6951711184], 
processed observation next is [0.0, 0.9565217391304348, 0.5102685624012641, 0.79, 1.0, 1.0, 0.41152482946278623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1985663596194702, 0.1985663596194702, 0.27675775398674385], 
reward next is 0.7232, 
noisyNet noise sample is [array([0.46971115], dtype=float32), 0.7289033]. 
=============================================
[2019-03-26 22:32:57,409] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1698430e-15 9.1242063e-01 1.3540196e-18 8.7579384e-02 7.2025506e-23], sum to 1.0000
[2019-03-26 22:32:57,418] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8708
[2019-03-26 22:32:57,429] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2155456.987513667 W.
[2019-03-26 22:32:57,434] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 70.66666666666667, 1.0, 2.0, 0.5138318574083348, 1.0, 2.0, 0.5138318574083348, 1.0, 1.0, 0.8923560095263989, 6.9112, 6.9112, 170.5573041426782, 2155456.987513667, 2155456.987513667, 424842.659694227], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3664200.0000, 
sim time next is 3664800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5319490088422987, 1.0, 2.0, 0.5319490088422987, 1.0, 2.0, 0.9238195101336606, 6.9112, 6.9112, 170.5573041426782, 2231532.406670272, 2231532.406670272, 437942.6530232458], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.7, 1.0, 1.0, 0.43608314318349234, 1.0, 1.0, 0.43608314318349234, 1.0, 1.0, 0.9070969635776349, 0.0, 0.0, 0.8375144448122397, 0.6198701129639644, 0.6198701129639644, 0.6536457507809639], 
reward next is 0.3464, 
noisyNet noise sample is [array([0.7067371], dtype=float32), -1.1451104]. 
=============================================
[2019-03-26 22:32:58,789] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.04433065e-24
 0.00000000e+00], sum to 1.0000
[2019-03-26 22:32:58,800] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9777
[2019-03-26 22:32:58,806] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 67.33333333333333, 1.0, 2.0, 0.5387763071972537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752875.3824124251, 752875.3824124257, 189894.6927182732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3611400.0000, 
sim time next is 3612000.0000, 
raw observation next is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.5355508790187717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748366.6449905266, 748366.6449905266, 189353.8163472899], 
processed observation next is [1.0, 0.8260869565217391, 0.6366508688783573, 0.6866666666666668, 1.0, 1.0, 0.4404227458057491, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2078796236084796, 0.2078796236084796, 0.28261763633923864], 
reward next is 0.7174, 
noisyNet noise sample is [array([1.330205], dtype=float32), 0.7280879]. 
=============================================
[2019-03-26 22:32:58,819] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.55449]
 [74.44618]
 [75.0906 ]
 [75.45964]
 [75.51448]], R is [[73.00952911]
 [72.99600983]
 [72.981987  ]
 [72.96686554]
 [72.95068359]].
[2019-03-26 22:33:09,964] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.3525826e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 22:33:09,981] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9084
[2019-03-26 22:33:09,992] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5433314895713639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759242.9792850231, 759242.9792850225, 190663.0707257695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3874200.0000, 
sim time next is 3874800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.538117117563818, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751953.9177811923, 751953.9177811918, 189783.508443075], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.44351459947447947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20887608827255344, 0.20887608827255327, 0.2832589678254851], 
reward next is 0.7167, 
noisyNet noise sample is [array([-0.8022149], dtype=float32), 0.17544146]. 
=============================================
[2019-03-26 22:33:10,978] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0804153e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:33:10,988] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8621
[2019-03-26 22:33:10,993] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5262438974779616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735356.7785158858, 735356.7785158858, 187809.8021203926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3619800.0000, 
sim time next is 3620400.0000, 
raw observation next is [28.33333333333333, 77.33333333333333, 1.0, 2.0, 0.5263584410286595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 735516.8935091477, 735516.8935091484, 187828.5552171551], 
processed observation next is [1.0, 0.9130434782608695, 0.541864139020537, 0.7733333333333333, 1.0, 1.0, 0.4293475193116379, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20431024819698546, 0.20431024819698565, 0.28034112718978377], 
reward next is 0.7197, 
noisyNet noise sample is [array([1.0811106], dtype=float32), -0.2603013]. 
=============================================
[2019-03-26 22:33:12,554] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0938853e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 22:33:12,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5160
[2019-03-26 22:33:12,575] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6156361652512266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860321.3282730926, 860321.3282730932, 203695.9286405581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3843000.0000, 
sim time next is 3843600.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6164534225815279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861463.8684486238, 861463.8684486238, 203852.0129842114], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5378956898572625, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23929551901350662, 0.23929551901350662, 0.3042567357973305], 
reward next is 0.6957, 
noisyNet noise sample is [array([0.40559897], dtype=float32), -0.049708523]. 
=============================================
[2019-03-26 22:33:12,861] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6386723e-14 9.7929376e-01 6.1706966e-17 2.0706179e-02 1.4138098e-20], sum to 1.0000
[2019-03-26 22:33:12,871] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4579
[2019-03-26 22:33:12,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2989814.37327748 W.
[2019-03-26 22:33:12,885] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 61.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.904249537023377, 6.9112, 168.9062840574441, 2989814.37327748, 2285338.683106428, 473835.9847423537], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3687600.0000, 
sim time next is 3688200.0000, 
raw observation next is [32.5, 63.0, 1.0, 2.0, 0.9103554669088283, 1.0, 1.0, 0.9103554669088283, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2546288.346439224, 2546288.346439224, 477173.4596753643], 
processed observation next is [1.0, 0.6956521739130435, 0.7393364928909952, 0.63, 1.0, 1.0, 0.8919945384443714, 1.0, 0.5, 0.8919945384443714, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7073023184553401, 0.7073023184553401, 0.7121991935453199], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8388728], dtype=float32), 0.16167918]. 
=============================================
[2019-03-26 22:33:13,662] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0857861e-14 9.9663889e-01 1.7723804e-17 3.3610475e-03 4.2212616e-22], sum to 1.0000
[2019-03-26 22:33:13,673] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4261
[2019-03-26 22:33:13,683] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2288563.079752453 W.
[2019-03-26 22:33:13,687] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.9953996920862568, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002276036960993, 6.9112, 168.9123429248503, 2288563.079752453, 2223950.839245409, 462030.5789842937], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3664800.0000, 
sim time next is 3665400.0000, 
raw observation next is [31.16666666666667, 68.83333333333334, 1.0, 2.0, 0.5822242957340624, 1.0, 1.0, 0.5822242957340624, 1.0, 2.0, 1.011131057173214, 6.9112, 6.9112, 170.5573041426782, 2442643.951634866, 2442643.951634866, 476661.2019642542], 
processed observation next is [1.0, 0.43478260869565216, 0.6761453396524489, 0.6883333333333335, 1.0, 1.0, 0.49665577799284627, 1.0, 0.5, 0.49665577799284627, 1.0, 1.0, 1.013574459967334, 0.0, 0.0, 0.8375144448122397, 0.6785122087874629, 0.6785122087874629, 0.7114346297973944], 
reward next is 0.2886, 
noisyNet noise sample is [array([0.6740242], dtype=float32), 0.4701331]. 
=============================================
[2019-03-26 22:33:15,184] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.071534e-18 0.000000e+00], sum to 1.0000
[2019-03-26 22:33:15,195] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4504
[2019-03-26 22:33:15,200] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.759871013071652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1061983.228120634, 1061983.228120635, 234295.6190430396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3640800.0000, 
sim time next is 3641400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.7736879900000767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1081303.439445454, 1081303.439445454, 237551.2278678445], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7273349277109358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3003620665126261, 0.3003620665126261, 0.35455407144454404], 
reward next is 0.6454, 
noisyNet noise sample is [array([-0.09502288], dtype=float32), -1.488611]. 
=============================================
[2019-03-26 22:33:17,224] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8753914e-14 9.9904507e-01 1.6321846e-16 9.5488544e-04 5.9037095e-20], sum to 1.0000
[2019-03-26 22:33:17,232] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7166
[2019-03-26 22:33:17,240] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3162776.767473289 W.
[2019-03-26 22:33:17,245] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.66666666666666, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.149269051744492, 6.9112, 168.9052030531042, 3162776.767473289, 2284488.288242885, 473139.643340097], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3760800.0000, 
sim time next is 3761400.0000, 
raw observation next is [33.83333333333334, 66.33333333333333, 1.0, 2.0, 0.6577973425729053, 1.0, 1.0, 0.6494887108007152, 1.0, 2.0, 1.03, 7.005094404751691, 6.9112, 170.5573041426782, 2725150.537812757, 2657890.156308932, 508331.3348478031], 
processed observation next is [1.0, 0.5217391304347826, 0.8025276461295423, 0.6633333333333333, 1.0, 1.0, 0.5877076416541028, 1.0, 0.5, 0.5776972419285725, 1.0, 1.0, 1.0365853658536586, 0.009389440475169143, 0.0, 0.8375144448122397, 0.7569862605035436, 0.7383028211969256, 0.7587034848474673], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0657713], dtype=float32), -1.282105]. 
=============================================
[2019-03-26 22:33:24,688] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4466599e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:33:24,699] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4276
[2019-03-26 22:33:24,704] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5445282443353512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760915.9047814367, 760915.9047814367, 190866.8922560311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4048800.0000, 
sim time next is 4049400.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5435760300077559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759584.8184712358, 759584.8184712352, 190705.3339932352], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.450091602418983, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2109957829086766, 0.21099578290867646, 0.2846348268555749], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.27656567], dtype=float32), 0.37012494]. 
=============================================
[2019-03-26 22:33:26,523] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 22:33:26,526] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:33:26,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:33:26,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:33:26,528] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:33:26,529] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:33:26,529] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:33:26,531] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:33:26,533] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:33:26,530] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:33:26,536] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:33:26,556] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-03-26 22:33:26,578] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-03-26 22:33:26,601] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-03-26 22:33:26,601] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-03-26 22:33:26,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-03-26 22:33:34,045] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:33:34,045] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.94267763833333, 78.64245755166667, 1.0, 2.0, 0.2266693423776321, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 376239.8817313439, 376239.8817313439, 158464.9555271]
[2019-03-26 22:33:34,048] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:33:34,052] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1481667e-34 0.0000000e+00], sampled 0.42050679330137886
[2019-03-26 22:33:41,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:33:41,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.45, 87.5, 1.0, 2.0, 0.2996686631306659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 477753.7306399799, 477753.7306399799, 165469.7268923429]
[2019-03-26 22:33:41,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:33:41,286] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9755182e-33 0.0000000e+00], sampled 0.5695023610740789
[2019-03-26 22:33:43,164] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:33:43,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.60119179333334, 82.93034093000001, 1.0, 2.0, 0.345863040543175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565905.2125560676, 565905.2125560676, 171935.3946906426]
[2019-03-26 22:33:43,168] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:33:43,172] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2067208e-33 0.0000000e+00], sampled 0.8555211911697422
[2019-03-26 22:33:48,663] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:33:48,665] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.622540355, 97.48927671, 1.0, 2.0, 0.4907563654168008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685751.6036875878, 685751.6036875871, 182160.3844703127]
[2019-03-26 22:33:48,667] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:33:48,671] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8736673e-35 0.0000000e+00], sampled 0.06274641801945391
[2019-03-26 22:34:40,684] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:34:40,687] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 0.5406710671499779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755524.0242093195, 755524.0242093201, 190213.936129436]
[2019-03-26 22:34:40,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:34:40,691] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 2.79069e-34 0.00000e+00], sampled 0.7019785201741704
[2019-03-26 22:34:47,511] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:34:47,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.25, 84.0, 1.0, 2.0, 0.6019291535486271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 841158.8671016792, 841158.8671016798, 201105.1397653916]
[2019-03-26 22:34:47,512] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:34:47,515] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.771131e-32 0.000000e+00], sampled 0.5607339395293706
[2019-03-26 22:35:00,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:35:00,003] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 92.0, 1.0, 2.0, 0.725539458453854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1013979.104788194, 1013979.104788194, 226453.5873591808]
[2019-03-26 22:35:00,005] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:35:00,007] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2852757e-29 0.0000000e+00], sampled 0.49127191450851226
[2019-03-26 22:35:15,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9596576]
[2019-03-26 22:35:15,019] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.31104200666667, 85.44759356833333, 1.0, 2.0, 0.3164624937122888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505869.4972276834, 505869.497227684, 167534.8932050991]
[2019-03-26 22:35:15,020] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:35:15,023] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0237469e-33 0.0000000e+00], sampled 0.2730230687162427
[2019-03-26 22:35:20,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5630 3008158537.7166 1768.0000
[2019-03-26 22:35:20,915] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.2013 2927823240.2823 1336.0000
[2019-03-26 22:35:21,134] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.2240 2779717374.9349 929.0000
[2019-03-26 22:35:21,136] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7877.8560 3164219432.5273 1773.0000
[2019-03-26 22:35:21,355] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 22:35:22,373] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1800000, evaluation results [1800000.0, 7877.855968777288, 3164219432.5273104, 1773.0, 8255.201254622527, 2927823240.282335, 1336.0, 8661.223973099577, 2779717374.934855, 929.0, 7997.563035027316, 3008158537.716564, 1768.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 22:35:30,609] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0774374e-11 9.9500436e-01 4.9622778e-14 4.9956529e-03 8.8797759e-17], sum to 1.0000
[2019-03-26 22:35:30,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4915
[2019-03-26 22:35:30,625] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2279781.843448518 W.
[2019-03-26 22:35:30,629] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.33333333333334, 62.00000000000001, 1.0, 2.0, 0.815160223430735, 1.0, 2.0, 0.815160223430735, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2279781.843448518, 2279781.843448518, 427326.5159069319], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4018800.0000, 
sim time next is 4019400.0000, 
raw observation next is [33.5, 61.5, 1.0, 2.0, 0.9130394196525838, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005988545928802, 6.9112, 168.9123931210871, 2173288.238802489, 2106042.206242621, 437692.4113493209], 
processed observation next is [1.0, 0.5217391304347826, 0.7867298578199052, 0.615, 1.0, 1.0, 0.8952282164488962, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009478854592880203, 0.0, 0.8294371786527034, 0.6036911774451358, 0.5850117239562836, 0.653272255745255], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.18381713], dtype=float32), 1.1436249]. 
=============================================
[2019-03-26 22:35:48,617] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.4534028e-10 8.7883964e-04 2.3892854e-12 9.9912113e-01 1.4737285e-13], sum to 1.0000
[2019-03-26 22:35:48,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7624
[2019-03-26 22:35:48,641] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3460593.470811601 W.
[2019-03-26 22:35:48,646] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.83333333333334, 71.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.679859859014325, 6.9112, 170.5573041426782, 3460593.470811601, 2909971.146715668, 549362.7984633876], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4269000.0000, 
sim time next is 4269600.0000, 
raw observation next is [34.0, 71.0, 1.0, 2.0, 0.9783120633723307, 1.0, 2.0, 0.8097460712004279, 1.0, 1.0, 1.03, 7.005119687072154, 6.9112, 170.5573041426782, 3398479.577420864, 3331201.085162471, 623664.1268799144], 
processed observation next is [1.0, 0.43478260869565216, 0.8104265402843602, 0.71, 1.0, 1.0, 0.9738699558702779, 1.0, 1.0, 0.77077839903666, 1.0, 0.5, 1.0365853658536586, 0.009391968707215436, 0.0, 0.8375144448122397, 0.9440221048391289, 0.9253336347673531, 0.9308419804177828], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24382171], dtype=float32), -0.57288486]. 
=============================================
[2019-03-26 22:35:56,516] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:35:56,527] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9025
[2019-03-26 22:35:56,532] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.532443530183544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744022.9855377565, 744022.9855377572, 188835.3935606821], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4558800.0000, 
sim time next is 4559400.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.5308248171535388, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741760.2475375492, 741760.2475375487, 188566.5813750938], 
processed observation next is [0.0, 0.782608695652174, 0.5971563981042655, 0.72, 1.0, 1.0, 0.43472869536570935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2060445132048748, 0.20604451320487463, 0.2814426587687967], 
reward next is 0.7186, 
noisyNet noise sample is [array([0.285044], dtype=float32), -0.6621114]. 
=============================================
[2019-03-26 22:35:56,534] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2577035e-35 1.0000000e+00 0.0000000e+00 4.5319504e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 22:35:56,543] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5760
[2019-03-26 22:35:56,548] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.020272149758052, 6.9112, 168.9122344970222, 1531187.125715393, 1453807.919459882, 311355.8249700929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4335000.0000, 
sim time next is 4335600.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.407735542162531, 6.9112, 168.9100738487074, 1806249.587345709, 1453996.195945949, 311355.908675315], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.04965355421625306, 0.0, 0.8294257899635517, 0.5017359964849192, 0.4038878322072081, 0.46471031145569397], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.33414024], dtype=float32), 0.74104154]. 
=============================================
[2019-03-26 22:35:57,641] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8706198e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 22:35:57,651] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5091
[2019-03-26 22:35:57,657] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5152865481673516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720040.1315345133, 720040.131534514, 186026.0277832874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4741800.0000, 
sim time next is 4742400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5164198616967963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 721624.3149491311, 721624.3149491318, 186208.848346045], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4173733273455377, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20045119859698085, 0.20045119859698105, 0.2779236542478284], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.92635155], dtype=float32), 0.73885036]. 
=============================================
[2019-03-26 22:35:58,649] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8779936e-21 1.0000000e+00 1.7970699e-25 4.5982592e-13 1.4605747e-30], sum to 1.0000
[2019-03-26 22:35:58,658] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8415
[2019-03-26 22:35:58,669] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2263585.72102134 W.
[2019-03-26 22:35:58,673] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 71.66666666666667, 1.0, 2.0, 0.8093743675376124, 1.0, 2.0, 0.8093743675376124, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2263585.72102134, 2263585.72102134, 424458.7780881649], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4783200.0000, 
sim time next is 4783800.0000, 
raw observation next is [30.0, 70.83333333333333, 1.0, 2.0, 0.5211143069064373, 1.0, 2.0, 0.5211143069064373, 1.0, 1.0, 0.9014281921922128, 6.911200000000001, 6.9112, 170.5573041426782, 2186037.077079962, 2186037.077079962, 429378.6073691213], 
processed observation next is [1.0, 0.34782608695652173, 0.6208530805687204, 0.7083333333333333, 1.0, 1.0, 0.42302928542944257, 1.0, 1.0, 0.42302928542944257, 1.0, 0.5, 0.8797904782831862, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6072325214111005, 0.6072325214111005, 0.6408635930882407], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.66229755], dtype=float32), -1.0576303]. 
=============================================
[2019-03-26 22:36:02,680] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:36:02,694] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2720
[2019-03-26 22:36:02,698] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.0, 1.0, 2.0, 0.5868582096367796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820090.0218771686, 820090.0218771686, 198323.3007647792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4435200.0000, 
sim time next is 4435800.0000, 
raw observation next is [30.16666666666666, 79.00000000000001, 1.0, 2.0, 0.5922119200917899, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 827574.3424542204, 827574.3424542211, 199304.188262228], 
processed observation next is [0.0, 0.34782608695652173, 0.6287519747235385, 0.7900000000000001, 1.0, 1.0, 0.508689060351554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.229881761792839, 0.2298817617928392, 0.2974689377048179], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.49220937], dtype=float32), -1.2657743]. 
=============================================
[2019-03-26 22:36:05,302] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.074003e-36 0.000000e+00], sum to 1.0000
[2019-03-26 22:36:05,309] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7937
[2019-03-26 22:36:05,314] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.6673898951587246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 932676.3819837064, 932676.381983707, 213962.9556299086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4866600.0000, 
sim time next is 4867200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.7316039499709531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1022458.62703462, 1022458.62703462, 227813.60955987], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.79, 1.0, 1.0, 0.6766312650252447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2840162852873945, 0.2840162852873945, 0.34002031277592537], 
reward next is 0.6600, 
noisyNet noise sample is [array([-0.20105702], dtype=float32), -1.3947077]. 
=============================================
[2019-03-26 22:36:09,168] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0113272e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:36:09,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7933
[2019-03-26 22:36:09,184] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 93.16666666666667, 1.0, 2.0, 0.9919741131948946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564686264, 1386578.516435549, 1386578.516435549, 296494.8538506472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4601400.0000, 
sim time next is 4602000.0000, 
raw observation next is [28.33333333333334, 92.33333333333334, 1.0, 2.0, 0.975150435183875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104205, 1363047.331558829, 1363047.331558828, 291450.8883961905], 
processed observation next is [1.0, 0.2608695652173913, 0.5418641390205374, 0.9233333333333335, 1.0, 1.0, 0.9700607652817771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451522512, 0.37862425876634137, 0.3786242587663411, 0.4350013259644634], 
reward next is 0.5650, 
noisyNet noise sample is [array([0.8087395], dtype=float32), -0.9547021]. 
=============================================
[2019-03-26 22:36:09,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[48.309982]
 [47.640697]
 [45.82655 ]
 [47.49137 ]
 [48.46619 ]], R is [[48.19428253]
 [48.26981354]
 [48.34202957]
 [47.85860825]
 [47.91918182]].
[2019-03-26 22:36:18,089] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:36:18,097] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5612
[2019-03-26 22:36:18,106] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666667, 71.33333333333334, 1.0, 2.0, 0.5301220517373679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740777.8797219268, 740777.8797219268, 188450.2965147253], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5077200.0000, 
sim time next is 5077800.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.5290435147289404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739270.2375146913, 739270.2375146907, 188271.8025317228], 
processed observation next is [0.0, 0.782608695652174, 0.5971563981042655, 0.72, 1.0, 1.0, 0.4325825478661932, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20535284375408092, 0.20535284375408075, 0.2810026903458549], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.26818871], dtype=float32), -0.67676]. 
=============================================
[2019-03-26 22:36:18,370] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 22:36:18,371] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:36:18,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:36:18,373] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:36:18,374] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:36:18,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:36:18,377] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:36:18,378] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:36:18,379] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:36:18,379] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:36:18,384] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:36:18,404] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-03-26 22:36:18,425] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-03-26 22:36:18,451] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-03-26 22:36:18,470] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-03-26 22:36:18,470] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-03-26 22:36:29,395] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.53353184]
[2019-03-26 22:36:29,396] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [17.77604556, 75.93798483333333, 1.0, 2.0, 0.1969950570805394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 329526.6142679707, 329526.6142679707, 132006.2340390309]
[2019-03-26 22:36:29,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:36:29,399] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8552446021829501
[2019-03-26 22:36:29,568] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.53353184]
[2019-03-26 22:36:29,569] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [15.98225671833334, 91.21345976166667, 1.0, 2.0, 0.2339363136289077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 391331.8288856575, 391331.8288856575, 136294.9437794629]
[2019-03-26 22:36:29,572] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:36:29,578] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2014740313590231
[2019-03-26 22:36:38,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.53353184]
[2019-03-26 22:36:38,919] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.55, 57.5, 1.0, 2.0, 0.2589899088380923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427513.4924810092, 427513.4924810092, 161761.7145360954]
[2019-03-26 22:36:38,919] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:36:38,921] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1517815799038168
[2019-03-26 22:37:17,542] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.53353184]
[2019-03-26 22:37:17,545] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [38.31816642, 63.38257391, 1.0, 2.0, 0.8833305336220818, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986733296615, 6.9112, 168.9123159712078, 2131704.122830879, 2064459.406924078, 429566.3905550102]
[2019-03-26 22:37:17,548] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:37:17,551] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.5461806e-26 1.0000000e+00 1.8220883e-30 2.6761489e-15 1.3066521e-36], sampled 0.6614989992550208
[2019-03-26 22:37:17,553] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2131704.122830879 W.
[2019-03-26 22:37:54,602] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.53353184]
[2019-03-26 22:37:54,603] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.2, 93.66666666666667, 1.0, 2.0, 0.6639687041702164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 927893.1840765299, 927893.1840765305, 213257.4504737729]
[2019-03-26 22:37:54,607] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:37:54,612] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2099462e-35 0.0000000e+00], sampled 0.970140805001494
[2019-03-26 22:38:12,735] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.2319 2779037928.2390 911.0000
[2019-03-26 22:38:12,861] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.8029 2927247657.6458 1321.0000
[2019-03-26 22:38:12,986] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7922.7064 3160323176.2196 1670.0000
[2019-03-26 22:38:13,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8006.0609 3007325610.1010 1741.0000
[2019-03-26 22:38:13,110] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8503.0212 2842297298.7360 1119.0000
[2019-03-26 22:38:14,125] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1825000, evaluation results [1825000.0, 7922.706357665364, 3160323176.2195597, 1670.0, 8262.802872760161, 2927247657.6458173, 1321.0, 8668.231873530991, 2779037928.2389545, 911.0, 8006.0608843335995, 3007325610.100958, 1741.0, 8503.02122601478, 2842297298.7359667, 1119.0]
[2019-03-26 22:38:22,911] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9120821e-15 1.7340248e-03 2.7636048e-17 9.9826604e-01 3.1618187e-20], sum to 1.0000
[2019-03-26 22:38:22,919] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8679
[2019-03-26 22:38:22,924] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5184535352259216, 1.0, 2.0, 0.5184535352259216, 1.0, 1.0, 0.8974208024527597, 6.9112, 6.9112, 170.5573041426782, 2174863.999472658, 2174863.999472658, 427585.4522014066], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4879200.0000, 
sim time next is 4879800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.7831663815303859, 1.0, 2.0, 0.7831663815303859, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2190222.076022087, 2190222.076022087, 411757.3757605405], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7387546765426336, 1.0, 1.0, 0.7387546765426336, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6083950211172464, 0.6083950211172464, 0.6145632474037918], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.7212553], dtype=float32), -0.23669893]. 
=============================================
[2019-03-26 22:38:37,693] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.716461e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:38:37,701] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6479
[2019-03-26 22:38:37,707] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.511471669805933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714707.5848065187, 714707.5848065187, 185412.3566504704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5100600.0000, 
sim time next is 5101200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5098171384776253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712394.8416375022, 712394.8416375016, 185147.8935073058], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40941823912966907, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19788745601041727, 0.1978874560104171, 0.27634013956314296], 
reward next is 0.7237, 
noisyNet noise sample is [array([0.21462457], dtype=float32), -1.7560279]. 
=============================================
[2019-03-26 22:38:37,948] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:38:37,957] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0126
[2019-03-26 22:38:37,964] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333333, 65.0, 1.0, 2.0, 0.5454243160662059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762168.5117103031, 762168.5117103037, 191019.3302135518], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5157600.0000, 
sim time next is 5158200.0000, 
raw observation next is [31.16666666666667, 65.5, 1.0, 2.0, 0.5437880349774372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759881.1769350927, 759881.1769350927, 190741.2701560888], 
processed observation next is [0.0, 0.6956521739130435, 0.6761453396524489, 0.655, 1.0, 1.0, 0.45034703009329785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21107810470419242, 0.21107810470419242, 0.28468846291953553], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.481558], dtype=float32), -0.8408472]. 
=============================================
[2019-03-26 22:38:41,242] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9010916e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 22:38:41,249] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0165
[2019-03-26 22:38:41,254] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.53333333333334, 84.66666666666667, 1.0, 2.0, 0.5661469796991381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791136.8637080308, 791136.8637080308, 194608.7906780089], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5271600.0000, 
sim time next is 5272200.0000, 
raw observation next is [28.55, 85.0, 1.0, 2.0, 0.5678159465288047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793469.958131984, 793469.958131984, 194903.4707997384], 
processed observation next is [1.0, 0.0, 0.552132701421801, 0.85, 1.0, 1.0, 0.4792963211190418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22040832170332889, 0.22040832170332889, 0.29090070268617674], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.10221345], dtype=float32), 0.115963]. 
=============================================
[2019-03-26 22:38:48,578] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9318451e-27 1.0000000e+00 2.0511067e-31 2.4178282e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 22:38:48,581] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5425
[2019-03-26 22:38:48,586] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.0, 1.0, 2.0, 0.3681754686746172, 1.0, 1.0, 0.3681754686746172, 1.0, 1.0, 0.6393990315997559, 6.9112, 6.9112, 170.5573041426782, 1544007.375083157, 1544007.375083157, 335972.8553058199], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5299200.0000, 
sim time next is 5299800.0000, 
raw observation next is [30.83333333333334, 79.16666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.208627911750746, 6.9112, 168.9112755461268, 1664902.936209839, 1453899.438791113, 311356.6490750291], 
processed observation next is [1.0, 0.34782608695652173, 0.6603475513428123, 0.7916666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.02974279117507459, 0.0, 0.8294316908480203, 0.4624730378360664, 0.4038609552197536, 0.46471141652989423], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9694522], dtype=float32), 0.85687536]. 
=============================================
[2019-03-26 22:38:50,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3286730e-13 3.1980799e-10 5.9969111e-15 1.0000000e+00 2.1923041e-17], sum to 1.0000
[2019-03-26 22:38:50,100] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1045
[2019-03-26 22:38:50,106] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.392736569867556, 6.9112, 170.5573041426782, 3254675.768082977, 2909731.535431542, 551058.9684817559], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5323200.0000, 
sim time next is 5323800.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.725686399515658, 6.9112, 170.5573041426782, 3493459.13288263, 2910009.393721109, 549066.9858468681], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.08144863995156584, 0.0, 0.8375144448122397, 0.9704053146896194, 0.808335942700308, 0.8195029639505494], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.64758295], dtype=float32), -1.7817625]. 
=============================================
[2019-03-26 22:38:50,148] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.815062e-24 0.000000e+00], sum to 1.0000
[2019-03-26 22:38:50,159] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9671
[2019-03-26 22:38:50,165] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 92.0, 1.0, 2.0, 0.9298339555523581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128951144307, 1299666.018307709, 1299666.01830771, 278278.9615278119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5460000.0000, 
sim time next is 5460600.0000, 
raw observation next is [27.61666666666667, 92.0, 1.0, 2.0, 0.8805637317985187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564951003, 1230759.144096507, 1230759.144096508, 264650.0432139775], 
processed observation next is [1.0, 0.17391304347826086, 0.5078988941548186, 0.92, 1.0, 1.0, 0.8561008816849622, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399450770219, 0.3418775400268075, 0.34187754002680776, 0.39500006449847386], 
reward next is 0.6050, 
noisyNet noise sample is [array([0.14171644], dtype=float32), -0.061907783]. 
=============================================
[2019-03-26 22:39:00,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.12243496e-32
 0.00000000e+00], sum to 1.0000
[2019-03-26 22:39:00,656] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5431
[2019-03-26 22:39:00,664] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4978311659426183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695640.7150467213, 695640.7150467207, 183256.7908125539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5634000.0000, 
sim time next is 5634600.0000, 
raw observation next is [25.76666666666667, 90.16666666666667, 1.0, 2.0, 0.4987063692652957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696864.0751059032, 696864.0751059025, 183393.5386969224], 
processed observation next is [0.0, 0.21739130434782608, 0.42022116903633505, 0.9016666666666667, 1.0, 1.0, 0.3960317701991515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19357335419608424, 0.19357335419608404, 0.27372169954764536], 
reward next is 0.7263, 
noisyNet noise sample is [array([1.3554235], dtype=float32), 0.66732514]. 
=============================================
[2019-03-26 22:39:03,244] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5943633e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:39:03,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9309
[2019-03-26 22:39:03,258] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 86.66666666666667, 1.0, 2.0, 0.808371934136521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129803.377829184, 1129803.377829185, 245976.7765173571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5554200.0000, 
sim time next is 5554800.0000, 
raw observation next is [27.7, 86.0, 1.0, 2.0, 0.8365889707277824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1169262.016483936, 1169262.016483935, 253091.0094335143], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.86, 1.0, 1.0, 0.8031192418407016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3247950045788711, 0.32479500457887084, 0.3777477752739019], 
reward next is 0.6223, 
noisyNet noise sample is [array([-0.92249733], dtype=float32), 0.14645013]. 
=============================================
[2019-03-26 22:39:03,533] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9501715e-38 0.0000000e+00], sum to 1.0000
[2019-03-26 22:39:03,541] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2385
[2019-03-26 22:39:03,544] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.13333333333333, 85.0, 1.0, 2.0, 0.5625300381840287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786080.6597273023, 786080.6597273023, 193972.4697935905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5523600.0000, 
sim time next is 5524200.0000, 
raw observation next is [28.0, 85.5, 1.0, 2.0, 0.5598755479021689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782369.903309312, 782369.903309312, 193508.456325155], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.855, 1.0, 1.0, 0.4697295757857456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21732497314147553, 0.21732497314147553, 0.2888185915300821], 
reward next is 0.7112, 
noisyNet noise sample is [array([-0.02252616], dtype=float32), -0.6733866]. 
=============================================
[2019-03-26 22:39:07,648] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:39:07,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7890
[2019-03-26 22:39:07,663] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 88.33333333333333, 1.0, 2.0, 0.5103670622577988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713163.5375938589, 713163.5375938582, 185236.0695604134], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5712000.0000, 
sim time next is 5712600.0000, 
raw observation next is [26.23333333333333, 88.66666666666667, 1.0, 2.0, 0.5107669127942012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713722.4581090902, 713722.4581090902, 185299.9764501694], 
processed observation next is [0.0, 0.08695652173913043, 0.44233807266982617, 0.8866666666666667, 1.0, 1.0, 0.4105625455351821, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19825623836363618, 0.19825623836363618, 0.2765671290301036], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.30433935], dtype=float32), 0.4872992]. 
=============================================
[2019-03-26 22:39:10,027] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-03-26 22:39:10,030] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:39:10,030] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:39:10,031] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:39:10,032] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:39:10,033] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:39:10,033] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:39:10,034] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:39:10,036] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:39:10,036] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:39:10,037] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:39:10,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-03-26 22:39:10,062] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-03-26 22:39:10,110] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-03-26 22:39:10,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-03-26 22:39:10,112] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-03-26 22:40:01,397] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.52287257]
[2019-03-26 22:40:01,398] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 79.0, 1.0, 2.0, 0.5538345692587809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773925.1664197636, 773925.1664197636, 192460.5614015208]
[2019-03-26 22:40:01,399] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:40:01,402] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7595876965497435
[2019-03-26 22:40:05,461] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.52287257]
[2019-03-26 22:40:05,463] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.27643652, 75.53160911666667, 1.0, 2.0, 0.5211321535528238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728211.3388588122, 728211.3388588122, 186972.4387394233]
[2019-03-26 22:40:05,465] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:40:05,468] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.16208552645708785
[2019-03-26 22:40:13,714] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.52287257]
[2019-03-26 22:40:13,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.09999999999999, 45.5, 1.0, 2.0, 0.594341388873312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830551.2881894335, 830551.2881894335, 199695.85424218]
[2019-03-26 22:40:13,716] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:40:13,718] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.299092e-37 0.000000e+00], sampled 0.021494658668601674
[2019-03-26 22:40:23,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.52287257]
[2019-03-26 22:40:23,159] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 63.0, 1.0, 2.0, 0.5524797156795057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772031.213966883, 772031.213966883, 192227.284804925]
[2019-03-26 22:40:23,159] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:40:23,163] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5127067224802084
[2019-03-26 22:40:46,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.52287257]
[2019-03-26 22:40:46,651] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.36013748666667, 87.55675255, 1.0, 2.0, 0.3694585705028604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560190.719401138, 560190.719401138, 171249.7401019242]
[2019-03-26 22:40:46,652] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:40:46,655] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.30445249240377725
[2019-03-26 22:41:03,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8682.2252 2777473544.9653 865.0000
[2019-03-26 22:41:03,173] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8066.2704 3000864681.6927 1590.0000
[2019-03-26 22:41:03,424] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8279.3211 2925520235.1328 1282.0000
[2019-03-26 22:41:03,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8533.2430 2839165192.2876 1030.0000
[2019-03-26 22:41:03,609] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8009.5391 3151654205.8466 1445.0000
[2019-03-26 22:41:04,627] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1850000, evaluation results [1850000.0, 8009.53905342267, 3151654205.846626, 1445.0, 8279.3210661927, 2925520235.1328053, 1282.0, 8682.225238861183, 2777473544.96529, 865.0, 8066.270447856331, 3000864681.6926928, 1590.0, 8533.242980334579, 2839165192.2875714, 1030.0]
[2019-03-26 22:41:09,553] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9312643e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:09,563] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2268
[2019-03-26 22:41:09,569] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 91.0, 1.0, 2.0, 0.5379244381609817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751684.5762346104, 751684.5762346097, 189751.4068265038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5964000.0000, 
sim time next is 5964600.0000, 
raw observation next is [26.73333333333333, 91.0, 1.0, 2.0, 0.5365059083643766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749701.652275915, 749701.6522759144, 189513.5127661979], 
processed observation next is [1.0, 0.0, 0.4660347551342811, 0.91, 1.0, 1.0, 0.4415733835715381, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20825045896553196, 0.2082504589655318, 0.2828559892032805], 
reward next is 0.7171, 
noisyNet noise sample is [array([1.1772926], dtype=float32), 0.34890172]. 
=============================================
[2019-03-26 22:41:15,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:41:15,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6504
[2019-03-26 22:41:15,948] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.66666666666667, 1.0, 2.0, 0.5579658161244295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779700.265294291, 779700.265294291, 193177.1577571181], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5856000.0000, 
sim time next is 5856600.0000, 
raw observation next is [29.8, 76.83333333333333, 1.0, 2.0, 0.5583851429277124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780286.446919334, 780286.446919334, 193250.050459017], 
processed observation next is [1.0, 0.782608695652174, 0.6113744075829385, 0.7683333333333333, 1.0, 1.0, 0.4679339071418221, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21674623525537057, 0.21674623525537057, 0.2884329111328612], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.8554319], dtype=float32), -0.7795122]. 
=============================================
[2019-03-26 22:41:20,572] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0055489e-33 1.0000000e+00 2.3231275e-38 2.7722103e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:20,583] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6553
[2019-03-26 22:41:20,594] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2221446.772646569 W.
[2019-03-26 22:41:20,600] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.73333333333333, 75.16666666666667, 1.0, 2.0, 0.947447024999106, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998530910708304, 6.9112, 168.9124372567458, 2221446.772646569, 2159491.409349087, 447836.6639492883], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5926200.0000, 
sim time next is 5926800.0000, 
raw observation next is [29.76666666666667, 75.33333333333334, 1.0, 2.0, 0.5107812647328819, 1.0, 1.0, 0.5107812647328819, 1.0, 2.0, 0.8870581388955511, 6.911199999999999, 6.9112, 170.5573041426782, 2142647.354179013, 2142647.354179013, 422681.3339021967], 
processed observation next is [1.0, 0.6086956521739131, 0.6097946287519749, 0.7533333333333334, 1.0, 1.0, 0.4105798370275685, 1.0, 0.5, 0.4105798370275685, 1.0, 1.0, 0.8622660230433549, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5951798206052814, 0.5951798206052814, 0.6308676625405921], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.12478677], dtype=float32), 0.5431572]. 
=============================================
[2019-03-26 22:41:22,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:41:22,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4098
[2019-03-26 22:41:22,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.95, 65.33333333333333, 1.0, 2.0, 0.5320050652225689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743410.071401003, 743410.071401003, 188762.9721525582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6360600.0000, 
sim time next is 6361200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5363770076742971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749521.4657144422, 749521.4657144416, 189492.4173407074], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44141808153529766, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2082004071429006, 0.20820040714290045, 0.28282450349359317], 
reward next is 0.7172, 
noisyNet noise sample is [array([0.3071834], dtype=float32), 0.9153478]. 
=============================================
[2019-03-26 22:41:24,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:41:24,092] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2497
[2019-03-26 22:41:24,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 82.0, 1.0, 2.0, 0.5276148445948841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737273.162009482, 737273.1620094826, 188035.4726312438], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6380400.0000, 
sim time next is 6381000.0000, 
raw observation next is [27.6, 82.0, 1.0, 2.0, 0.5264462443910909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735639.6296786687, 735639.6296786681, 187843.0045480424], 
processed observation next is [0.0, 0.8695652173913043, 0.5071090047393366, 0.82, 1.0, 1.0, 0.4294533064952902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20434434157740797, 0.2043443415774078, 0.2803626933552872], 
reward next is 0.7196, 
noisyNet noise sample is [array([-1.1939268], dtype=float32), -0.74023527]. 
=============================================
[2019-03-26 22:41:24,113] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[75.996056]
 [76.03639 ]
 [76.1027  ]
 [76.019966]
 [76.06332 ]], R is [[75.95506287]
 [75.91486359]
 [75.87477875]
 [75.83486938]
 [75.79537964]].
[2019-03-26 22:41:28,222] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3432062e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:28,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6374
[2019-03-26 22:41:28,237] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5250703621992528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733716.3506362874, 733716.3506362874, 187617.2523374262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6354000.0000, 
sim time next is 6354600.0000, 
raw observation next is [30.95, 62.83333333333333, 1.0, 2.0, 0.5292061733555125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739497.6111338534, 739497.611133854, 188298.0382992323], 
processed observation next is [0.0, 0.5652173913043478, 0.6658767772511848, 0.6283333333333333, 1.0, 1.0, 0.4327785221150752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20541600309273705, 0.20541600309273722, 0.2810418482078094], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.17878722], dtype=float32), 1.1180911]. 
=============================================
[2019-03-26 22:41:28,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6375087e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:28,873] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3670
[2019-03-26 22:41:28,879] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 84.33333333333334, 1.0, 2.0, 0.6946400925051264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970775.8972965084, 970775.8972965084, 219693.3994104363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6073800.0000, 
sim time next is 6074400.0000, 
raw observation next is [28.06666666666667, 83.66666666666667, 1.0, 2.0, 0.7124276490214955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 995646.0795226158, 995646.0795226158, 223551.506195066], 
processed observation next is [1.0, 0.30434782608695654, 0.529225908372828, 0.8366666666666667, 1.0, 1.0, 0.6535272879777053, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27656835542294883, 0.27656835542294883, 0.3336589644702478], 
reward next is 0.6663, 
noisyNet noise sample is [array([-0.43236098], dtype=float32), -1.6220218]. 
=============================================
[2019-03-26 22:41:29,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1615716e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:29,508] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0618
[2019-03-26 22:41:29,514] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.03333333333333, 79.50000000000001, 1.0, 2.0, 0.5286746343410111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738754.5953918314, 738754.5953918314, 188210.3184232313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6376200.0000, 
sim time next is 6376800.0000, 
raw observation next is [27.96666666666667, 80.0, 1.0, 2.0, 0.5279106497349693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737686.6547711431, 737686.6547711438, 188084.2933918417], 
processed observation next is [0.0, 0.8260869565217391, 0.524486571879937, 0.8, 1.0, 1.0, 0.43121765028309544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20491295965865086, 0.20491295965865106, 0.28072282595797265], 
reward next is 0.7193, 
noisyNet noise sample is [array([0.24917193], dtype=float32), 0.035464898]. 
=============================================
[2019-03-26 22:41:41,198] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5825919e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:41,212] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1438
[2019-03-26 22:41:41,216] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 91.0, 1.0, 2.0, 0.8121856203606963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1135136.342119585, 1135136.342119585, 246922.9730243001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6489000.0000, 
sim time next is 6489600.0000, 
raw observation next is [26.33333333333333, 91.0, 1.0, 2.0, 0.7272886022556856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1016424.794027235, 1016424.794027235, 226844.8089140306], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271704, 0.91, 1.0, 1.0, 0.6714320509104645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28234022056312086, 0.28234022056312086, 0.33857434166273226], 
reward next is 0.6614, 
noisyNet noise sample is [array([-0.39105162], dtype=float32), -0.94255286]. 
=============================================
[2019-03-26 22:41:45,288] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.404269e-35 0.000000e+00], sum to 1.0000
[2019-03-26 22:41:45,299] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6182
[2019-03-26 22:41:45,304] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.96666666666667, 70.5, 1.0, 2.0, 0.5333704921153345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745318.7528925957, 745318.7528925957, 188990.0026327585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6342600.0000, 
sim time next is 6343200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.5352173682689337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747900.4405693329, 747900.4405693322, 189298.1262683131], 
processed observation next is [0.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.4400209256252213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20775012238037024, 0.20775012238037005, 0.28253451681837777], 
reward next is 0.7175, 
noisyNet noise sample is [array([0.11410872], dtype=float32), -0.8660758]. 
=============================================
[2019-03-26 22:41:48,484] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0198535e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 22:41:48,489] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9087
[2019-03-26 22:41:48,493] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 66.33333333333333, 1.0, 2.0, 0.4660268026886323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651185.5008360879, 651185.5008360879, 178445.7562149227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6543600.0000, 
sim time next is 6544200.0000, 
raw observation next is [29.4, 66.66666666666667, 1.0, 2.0, 0.4728487781191061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660720.9030122767, 660720.9030122774, 179452.3924160616], 
processed observation next is [1.0, 0.7391304347826086, 0.5924170616113744, 0.6666666666666667, 1.0, 1.0, 0.3648780459266338, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18353358417007687, 0.18353358417007706, 0.2678393916657636], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.35205135], dtype=float32), 1.5121713]. 
=============================================
[2019-03-26 22:42:00,628] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-03-26 22:42:00,630] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:42:00,633] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:42:00,634] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:42:00,634] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:42:00,636] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:42:00,637] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:42:00,638] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:42:00,640] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:42:00,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:42:00,642] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:42:00,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-03-26 22:42:00,686] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-03-26 22:42:00,710] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-03-26 22:42:00,729] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-03-26 22:42:00,730] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-03-26 22:42:04,513] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:04,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.3, 50.0, 1.0, 2.0, 0.300698599477958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 481664.4559738073, 481664.4559738073, 165772.7214566341]
[2019-03-26 22:42:04,518] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:42:04,521] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.777171e-37 0.000000e+00], sampled 0.1233395006125888
[2019-03-26 22:42:14,616] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:14,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.8, 60.66666666666667, 1.0, 2.0, 0.3482466871893997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539083.6126654171, 539083.6126654171, 169816.3873517948]
[2019-03-26 22:42:14,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:42:14,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.730396e-35 0.000000e+00], sampled 0.8610173325925469
[2019-03-26 22:42:21,271] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:21,273] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.65, 85.5, 1.0, 2.0, 0.2292255346789918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 381276.3813270504, 381276.3813270504, 158561.4286122093]
[2019-03-26 22:42:21,274] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:42:21,278] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.423687e-35 0.000000e+00], sampled 0.21392803247918057
[2019-03-26 22:42:31,206] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:31,207] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.416819254629796, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614472.0860957283, 614472.086095729, 175681.6144706148]
[2019-03-26 22:42:31,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:42:31,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3746546e-31 0.0000000e+00], sampled 0.4528549295497608
[2019-03-26 22:42:32,029] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:32,031] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.32474652, 62.7432153, 1.0, 2.0, 0.905700561997496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1265913.700446945, 1265913.700446945, 271510.3652526987]
[2019-03-26 22:42:32,034] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:42:32,037] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5834925e-15 0.0000000e+00], sampled 0.5423487573222158
[2019-03-26 22:42:32,891] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:42:32,892] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.75, 83.66666666666667, 1.0, 2.0, 0.4488852416201645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636372.3650206315, 636372.3650206322, 177157.3205953385]
[2019-03-26 22:42:32,893] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:42:32,894] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.7397407e-29 0.0000000e+00], sampled 0.3509878846680271
[2019-03-26 22:43:26,654] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.63309294]
[2019-03-26 22:43:26,655] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.35961859, 54.63319005333333, 1.0, 2.0, 0.457957248868803, 1.0, 2.0, 0.457957248868803, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1332334.142353869, 1332334.142353869, 294065.8721839858]
[2019-03-26 22:43:26,657] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:43:26,662] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.2990565e-26 1.9142184e-02 2.2604072e-31 9.8085785e-01 5.0665437e-38], sampled 0.007923294762639532
[2019-03-26 22:43:52,250] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8912.4831 2761526043.9178 374.0000
[2019-03-26 22:43:52,763] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8631.6271 2900880932.0839 556.0000
[2019-03-26 22:43:52,892] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8366.0223 3119619798.8041 533.0000
[2019-03-26 22:43:52,984] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8551.6765 2961217688.2374 484.0000
[2019-03-26 22:43:53,077] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8799.6968 2816470382.1944 396.0000
[2019-03-26 22:43:54,094] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1875000, evaluation results [1875000.0, 8366.022340464257, 3119619798.804098, 533.0, 8631.627141434921, 2900880932.0838885, 556.0, 8912.483079022782, 2761526043.9178348, 374.0, 8551.676500522179, 2961217688.237371, 484.0, 8799.696814370209, 2816470382.1943655, 396.0]
[2019-03-26 22:44:02,303] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.7083867e-22 9.9998832e-01 1.8440058e-26 1.1702017e-05 1.6967713e-32], sum to 1.0000
[2019-03-26 22:44:02,310] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8907
[2019-03-26 22:44:02,317] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2060288.432940475 W.
[2019-03-26 22:44:02,326] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 61.5, 1.0, 2.0, 0.8323064009408588, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.965169840658858, 6.9112, 168.9125776483002, 2060288.432940475, 2022000.456356461, 417291.5728919918], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6705000.0000, 
sim time next is 6705600.0000, 
raw observation next is [30.06666666666667, 62.0, 1.0, 2.0, 0.8484944276185066, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.963313230405705, 6.9112, 168.9126022121701, 2082945.407136392, 2045974.56543485, 421532.4365377702], 
processed observation next is [1.0, 0.6086956521739131, 0.6240126382306479, 0.62, 1.0, 1.0, 0.8174631658054295, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.005211323040570459, 0.0, 0.8294382053856432, 0.5785959464267756, 0.5683262681763472, 0.6291528903548809], 
reward next is 0.1103, 
noisyNet noise sample is [array([0.59400684], dtype=float32), -0.86525834]. 
=============================================
[2019-03-26 22:44:22,418] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.072457e-28 0.000000e+00], sum to 1.0000
[2019-03-26 22:44:22,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7495
[2019-03-26 22:44:22,443] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2151868.761937186 W.
[2019-03-26 22:44:22,447] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.6, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.865800084982329, 6.9112, 168.9075676307972, 2151868.761937186, 1474664.212596213, 314626.0437618887], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7117800.0000, 
sim time next is 7118400.0000, 
raw observation next is [27.7, 71.33333333333334, 1.0, 2.0, 0.6166416644805994, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.911694863106829, 6.9112, 168.9121673675936, 1734552.224321608, 1734201.153041923, 369882.0315017942], 
processed observation next is [1.0, 0.391304347826087, 0.5118483412322274, 0.7133333333333334, 1.0, 1.0, 0.5381224873260234, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 4.948631068293352e-05, 0.0, 0.8294360700997, 0.4818200623115578, 0.48172254251164526, 0.5520627335847674], 
reward next is 0.4455, 
noisyNet noise sample is [array([-0.30988705], dtype=float32), 1.4172964]. 
=============================================
[2019-03-26 22:44:22,498] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:44:22,504] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9235
[2019-03-26 22:44:22,512] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.85, 90.5, 1.0, 2.0, 0.3412572469574073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533333.0381592724, 533333.0381592731, 169479.696774071], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7367400.0000, 
sim time next is 7368000.0000, 
raw observation next is [21.66666666666666, 90.66666666666667, 1.0, 2.0, 0.3343134563354721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524635.574303105, 524635.5743031057, 168836.3574700976], 
processed observation next is [1.0, 0.2608695652173913, 0.22590837282780388, 0.9066666666666667, 1.0, 1.0, 0.19796801968129168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14573210397308473, 0.14573210397308492, 0.25199456338820536], 
reward next is 0.7480, 
noisyNet noise sample is [array([-2.172658], dtype=float32), 1.0547941]. 
=============================================
[2019-03-26 22:44:22,537] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[73.8352  ]
 [73.61062 ]
 [73.526215]
 [73.242386]
 [73.20855 ]], R is [[73.98680878]
 [73.99398804]
 [74.00029755]
 [74.00266266]
 [74.00231934]].
[2019-03-26 22:44:25,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9620114e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 22:44:25,773] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0416
[2019-03-26 22:44:25,783] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.2, 80.66666666666666, 1.0, 2.0, 0.4036288715111631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 594362.9525237568, 594362.9525237568, 173779.8462242809], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7479600.0000, 
sim time next is 7480200.0000, 
raw observation next is [25.3, 80.33333333333334, 1.0, 2.0, 0.4055258622927768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596066.4046566807, 596066.4046566807, 173904.4246032597], 
processed observation next is [0.0, 0.5652173913043478, 0.39810426540284366, 0.8033333333333335, 1.0, 1.0, 0.2837660991479239, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1655740012935224, 0.1655740012935224, 0.25955884269143237], 
reward next is 0.7404, 
noisyNet noise sample is [array([0.3740526], dtype=float32), 1.6646917]. 
=============================================
[2019-03-26 22:44:30,106] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.523372e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:44:30,117] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0409
[2019-03-26 22:44:30,123] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333333, 89.16666666666667, 1.0, 2.0, 0.537329516017936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869358.8287553352, 869358.8287553352, 202392.1847829527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7404600.0000, 
sim time next is 7405200.0000, 
raw observation next is [20.4, 89.0, 1.0, 2.0, 0.5103061967884225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826402.2734230156, 826402.2734230156, 197352.1683664757], 
processed observation next is [1.0, 0.7391304347826086, 0.16587677725118483, 0.89, 1.0, 1.0, 0.41000746601014754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22955618706194877, 0.22955618706194877, 0.29455547517384434], 
reward next is 0.7054, 
noisyNet noise sample is [array([-0.3093209], dtype=float32), -0.09568838]. 
=============================================
[2019-03-26 22:44:38,415] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:44:38,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2245
[2019-03-26 22:44:38,429] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 76.0, 1.0, 2.0, 0.4240355883393812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 613336.4816620288, 613336.4816620282, 175236.5273522121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7492800.0000, 
sim time next is 7493400.0000, 
raw observation next is [26.25, 76.5, 1.0, 2.0, 0.4225702063350066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612096.67258929, 612096.67258929, 175143.6456559519], 
processed observation next is [0.0, 0.7391304347826086, 0.4431279620853081, 0.765, 1.0, 1.0, 0.3043014534156706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17002685349702498, 0.17002685349702498, 0.261408426352167], 
reward next is 0.7386, 
noisyNet noise sample is [array([-1.2313303], dtype=float32), 2.0377789]. 
=============================================
[2019-03-26 22:44:48,346] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2390642e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 22:44:48,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8390
[2019-03-26 22:44:48,360] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 83.66666666666667, 1.0, 2.0, 0.6462760089666977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 903157.2114684619, 903157.2114684612, 209674.6952352519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7888200.0000, 
sim time next is 7888800.0000, 
raw observation next is [27.1, 83.33333333333334, 1.0, 2.0, 0.661616269386269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924604.2363353607, 924604.2363353601, 212776.644352342], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.8333333333333335, 1.0, 1.0, 0.5923087582967097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2568345100931557, 0.25683451009315555, 0.3175770811228985], 
reward next is 0.6824, 
noisyNet noise sample is [array([-0.12954374], dtype=float32), -0.58793044]. 
=============================================
[2019-03-26 22:44:48,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0933463e-30 1.0000000e+00 3.3380280e-36 5.8578554e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 22:44:48,829] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9460
[2019-03-26 22:44:48,836] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 75.66666666666667, 1.0, 2.0, 0.2137140843430458, 1.0, 2.0, 0.2137140843430458, 1.0, 1.0, 0.3645573087093021, 6.9112, 6.9112, 170.5573041426782, 895976.2250611964, 895976.2250611964, 273372.9036402901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7665000.0000, 
sim time next is 7665600.0000, 
raw observation next is [28.4, 76.33333333333334, 1.0, 2.0, 0.4819135734778075, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673391.3345735336, 673391.3345735336, 180812.9038295734], 
processed observation next is [1.0, 0.7391304347826086, 0.5450236966824644, 0.7633333333333334, 1.0, 1.0, 0.3757994861178403, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18705314849264823, 0.18705314849264823, 0.2698700057157812], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4732162], dtype=float32), 0.97804004]. 
=============================================
[2019-03-26 22:44:49,959] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0370844e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 22:44:49,967] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8246
[2019-03-26 22:44:49,978] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 93.0, 1.0, 2.0, 0.4079176848427963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 599679.8845819202, 599679.8845819196, 174242.5415096132], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7519200.0000, 
sim time next is 7519800.0000, 
raw observation next is [23.51666666666667, 93.0, 1.0, 2.0, 0.4072154426557484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599021.1350886654, 599021.1350886661, 174192.8168834619], 
processed observation next is [0.0, 0.0, 0.31358609794628767, 0.93, 1.0, 1.0, 0.28580173813945586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1663947597468515, 0.1663947597468517, 0.25998927893054014], 
reward next is 0.7400, 
noisyNet noise sample is [array([1.110703], dtype=float32), -1.4301561]. 
=============================================
[2019-03-26 22:44:50,039] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 22:44:50,041] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:44:50,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:44:50,041] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:44:50,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:44:50,042] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:44:50,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:44:50,045] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:44:50,045] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:44:50,048] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:44:50,049] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:44:50,085] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-03-26 22:44:50,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-03-26 22:44:50,107] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-03-26 22:44:50,164] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-03-26 22:44:50,164] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-03-26 22:45:02,435] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:02,438] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.19671321333333, 54.57333321999999, 1.0, 2.0, 0.2553407667590317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 420672.247461836, 420672.247461836, 161422.0710650311]
[2019-03-26 22:45:02,441] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:45:02,444] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5502036e-35 0.0000000e+00], sampled 0.2625659890926648
[2019-03-26 22:45:14,705] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:14,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.1, 96.0, 1.0, 2.0, 0.4170158504287403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 614460.6113913963, 614460.611391397, 175672.1637420004]
[2019-03-26 22:45:14,708] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:45:14,711] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 1.72835e-33 0.00000e+00], sampled 0.3102157088546158
[2019-03-26 22:45:29,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:29,844] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.46666666666667, 57.33333333333333, 1.0, 2.0, 0.5652980708432912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789950.1527433685, 789950.1527433685, 194459.068545856]
[2019-03-26 22:45:29,846] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:45:29,849] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.8652062e-33 0.0000000e+00], sampled 0.2907248070499121
[2019-03-26 22:45:31,000] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:31,000] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.6, 82.5, 1.0, 2.0, 0.5381165728185272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104288, 751953.1562957632, 751953.1562957638, 189785.2133566381]
[2019-03-26 22:45:31,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:45:31,005] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8841627e-30 0.0000000e+00], sampled 0.9890618221214633
[2019-03-26 22:45:31,877] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:31,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.03333333333333, 59.33333333333333, 1.0, 2.0, 0.7463798506031426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043118.940731806, 1043118.940731806, 231172.4367606209]
[2019-03-26 22:45:31,879] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:45:31,881] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2116259e-30 0.0000000e+00], sampled 0.6421233290435099
[2019-03-26 22:45:32,248] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:32,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.6, 58.33333333333334, 1.0, 2.0, 0.5015344620416443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700817.1955825065, 700817.1955825065, 183837.5081670604]
[2019-03-26 22:45:32,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:45:32,255] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4797546e-33 0.0000000e+00], sampled 0.326354558720634
[2019-03-26 22:45:32,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:45:32,875] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.76079400666666, 99.8403438, 1.0, 2.0, 0.2441892109964124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403538.2969675728, 403538.2969675728, 160269.67354186]
[2019-03-26 22:45:32,876] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:45:32,878] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 7.393069e-35 0.000000e+00], sampled 0.3445408073190023
[2019-03-26 22:46:31,903] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:46:31,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.09641561666666, 75.16492611000001, 1.0, 2.0, 0.4965428925186828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693929.0002407687, 693929.0002407687, 183065.7033838156]
[2019-03-26 22:46:31,904] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:46:31,908] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1980976e-33 0.0000000e+00], sampled 0.9026211593323891
[2019-03-26 22:46:40,104] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:46:40,105] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.73333333333334, 78.0, 1.0, 2.0, 0.5608746490812058, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9583629664511745, 6.9112, 6.9112, 168.9126846917552, 1568111.880951329, 1568111.880951329, 339875.8486203603]
[2019-03-26 22:46:40,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:46:40,111] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.5396569e-38 1.0000000e+00 0.0000000e+00 1.9875407e-21 0.0000000e+00], sampled 0.7875272471961458
[2019-03-26 22:46:40,627] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.65023535]
[2019-03-26 22:46:40,628] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.08341127666667, 60.71572959333334, 1.0, 2.0, 0.4334252225478122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 627844.2918389116, 627844.291838911, 176674.7643597951]
[2019-03-26 22:46:40,629] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:46:40,634] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3666756e-34 0.0000000e+00], sampled 0.3549578826320626
[2019-03-26 22:46:44,360] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.9667 3008170325.9701 1766.0000
[2019-03-26 22:46:44,632] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.0992 2779442499.8228 920.0000
[2019-03-26 22:46:44,666] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.4258 2927842660.4315 1328.0000
[2019-03-26 22:46:44,674] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.8961 3164795308.4191 1773.0000
[2019-03-26 22:46:44,970] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.9746 2843132720.8611 1125.0000
[2019-03-26 22:46:45,988] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1900000, evaluation results [1900000.0, 7879.896125893719, 3164795308.4191008, 1773.0, 8259.425822792848, 2927842660.4314694, 1328.0, 8663.099169865789, 2779442499.8227763, 920.0, 7999.96670202967, 3008170325.9700766, 1766.0, 8496.974630063294, 2843132720.8611097, 1125.0]
[2019-03-26 22:46:48,368] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7266105e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:46:48,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1808
[2019-03-26 22:46:48,383] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 86.0, 1.0, 2.0, 0.4845946690601449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677138.8985563354, 677138.8985563348, 181217.898083836], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7588800.0000, 
sim time next is 7589400.0000, 
raw observation next is [26.13333333333333, 86.33333333333334, 1.0, 2.0, 0.4854868288901559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678385.9387319383, 678385.9387319383, 181353.6337841702], 
processed observation next is [0.0, 0.8695652173913043, 0.43759873617693507, 0.8633333333333334, 1.0, 1.0, 0.3801046131206698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18844053853664952, 0.18844053853664952, 0.27067706534950775], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.5164776], dtype=float32), 1.1874644]. 
=============================================
[2019-03-26 22:46:49,125] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:46:49,125] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:46:49,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-03-26 22:46:51,147] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:46:51,148] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:46:51,214] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-03-26 22:46:51,616] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.718372e-27 0.000000e+00], sum to 1.0000
[2019-03-26 22:46:51,622] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9688
[2019-03-26 22:46:51,626] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 88.0, 1.0, 2.0, 0.6431744756247157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 898821.0494038481, 898821.0494038475, 209054.8110721068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7788600.0000, 
sim time next is 7789200.0000, 
raw observation next is [25.83333333333334, 88.33333333333333, 1.0, 2.0, 0.6392658256620077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893356.5066685327, 893356.5066685327, 208279.128575393], 
processed observation next is [1.0, 0.13043478260869565, 0.42338072669826254, 0.8833333333333333, 1.0, 1.0, 0.5653805128457924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24815458518570355, 0.24815458518570355, 0.31086437100804926], 
reward next is 0.6891, 
noisyNet noise sample is [array([-0.40066862], dtype=float32), -0.6073164]. 
=============================================
[2019-03-26 22:46:52,591] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.7737196e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:46:52,599] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2472
[2019-03-26 22:46:52,603] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 87.0, 1.0, 2.0, 0.521490308296969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728711.9830900668, 728711.9830900675, 187031.7457191566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7765200.0000, 
sim time next is 7765800.0000, 
raw observation next is [26.86666666666667, 87.33333333333333, 1.0, 2.0, 0.5220659538473954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729516.6458953219, 729516.6458953219, 187125.6552924545], 
processed observation next is [1.0, 0.9130434782608695, 0.4723538704581361, 0.8733333333333333, 1.0, 1.0, 0.4241758480089101, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20264351274870052, 0.20264351274870052, 0.27929202282455895], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.92406887], dtype=float32), 1.9589391]. 
=============================================
[2019-03-26 22:46:53,564] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:46:53,565] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:46:53,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-03-26 22:46:53,963] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9397321e-18 9.9204344e-01 3.4814877e-21 7.9566110e-03 2.0931468e-25], sum to 1.0000
[2019-03-26 22:46:53,970] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0229
[2019-03-26 22:46:53,976] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2161321.248677373 W.
[2019-03-26 22:46:53,981] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.85, 66.5, 1.0, 2.0, 0.5152284091869699, 1.0, 1.0, 0.5152284091869699, 1.0, 1.0, 0.8818895063311966, 6.9112, 6.9112, 170.5573041426782, 2161321.248677373, 2161321.248677373, 423410.7633996017], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7659000.0000, 
sim time next is 7659600.0000, 
raw observation next is [29.73333333333333, 67.33333333333334, 1.0, 2.0, 0.4916575997651869, 1.0, 2.0, 0.4916575997651869, 1.0, 2.0, 0.8398815638720203, 6.9112, 6.9112, 170.5573041426782, 2062349.354319815, 2062349.354319815, 406927.0402718218], 
processed observation next is [1.0, 0.6521739130434783, 0.6082148499210109, 0.6733333333333335, 1.0, 1.0, 0.3875392768255264, 1.0, 1.0, 0.3875392768255264, 1.0, 1.0, 0.8047336144780736, 0.0, 0.0, 0.8375144448122397, 0.5728748206443931, 0.5728748206443931, 0.6073537914504803], 
reward next is 0.3926, 
noisyNet noise sample is [array([0.10622774], dtype=float32), -0.7809422]. 
=============================================
[2019-03-26 22:46:57,030] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0082597e-16 4.0078256e-05 2.4792865e-18 9.9995995e-01 7.0434167e-22], sum to 1.0000
[2019-03-26 22:46:57,037] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3944
[2019-03-26 22:46:57,040] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 62.33333333333334, 1.0, 2.0, 0.53813427541009, 1.0, 2.0, 0.53813427541009, 1.0, 2.0, 0.9179323278109877, 6.911200000000001, 6.9112, 170.5573041426782, 2257503.108039301, 2257503.1080393, 439225.0300007346], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7749600.0000, 
sim time next is 7750200.0000, 
raw observation next is [30.4, 62.66666666666666, 1.0, 2.0, 0.8110228386390813, 1.0, 2.0, 0.8110228386390813, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268200.200980565, 2268200.200980565, 425262.8868264363], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6266666666666666, 1.0, 1.0, 0.7723166730591341, 1.0, 1.0, 0.7723166730591341, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6300556113834903, 0.6300556113834903, 0.6347207266066214], 
reward next is 0.3653, 
noisyNet noise sample is [array([-0.5462153], dtype=float32), 0.30637538]. 
=============================================
[2019-03-26 22:46:58,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:46:58,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:46:58,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-03-26 22:46:58,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:46:58,450] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:46:58,511] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-03-26 22:47:00,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:00,926] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:01,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-03-26 22:47:01,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:01,647] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:01,698] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-03-26 22:47:02,526] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.542263e-25 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:02,534] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8148
[2019-03-26 22:47:02,538] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.3711299105779521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 567008.1271868538, 567008.1271868545, 171966.7106981881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 160800.0000, 
sim time next is 161400.0000, 
raw observation next is [21.9, 96.0, 1.0, 2.0, 0.3669706244901264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 562361.5014607024, 562361.501460703, 171613.6006957342], 
processed observation next is [1.0, 0.8695652173913043, 0.23696682464454974, 0.96, 1.0, 1.0, 0.23731400540979083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15621152818352843, 0.1562115281835286, 0.2561397025309466], 
reward next is 0.7439, 
noisyNet noise sample is [array([1.3376601], dtype=float32), 1.4118228]. 
=============================================
[2019-03-26 22:47:03,193] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:03,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:03,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-03-26 22:47:03,429] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:03,430] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:03,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-03-26 22:47:05,586] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:05,587] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:05,660] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-03-26 22:47:05,710] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:05,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:05,776] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-03-26 22:47:06,227] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.6424687e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:06,232] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9810
[2019-03-26 22:47:06,236] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333334, 85.33333333333334, 1.0, 2.0, 0.3577738573223444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552198.1601016809, 552198.1601016803, 170859.2472245966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 76200.0000, 
sim time next is 76800.0000, 
raw observation next is [22.96666666666667, 85.66666666666667, 1.0, 2.0, 0.3557073192929176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549347.9638717785, 549347.9638717785, 170629.8965344804], 
processed observation next is [1.0, 0.9130434782608695, 0.2875197472353872, 0.8566666666666667, 1.0, 1.0, 0.22374375818423806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15259665663104957, 0.15259665663104957, 0.2546714873648961], 
reward next is 0.7453, 
noisyNet noise sample is [array([1.6759744], dtype=float32), 0.12983577]. 
=============================================
[2019-03-26 22:47:07,196] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:07,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:07,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:07,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:07,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-03-26 22:47:07,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:07,308] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:07,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-03-26 22:47:07,405] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-03-26 22:47:07,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:07,914] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:07,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-03-26 22:47:08,616] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 22:47:08,617] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:08,679] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-03-26 22:47:10,184] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2015903e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:10,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6762
[2019-03-26 22:47:10,195] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.37936315630412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570915.4068064233, 570915.4068064233, 172052.029309508], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 148800.0000, 
sim time next is 149400.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3757333794794568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 565510.5378395348, 565510.5378395341, 171578.450878132], 
processed observation next is [1.0, 0.7391304347826086, 0.2654028436018958, 0.96, 1.0, 1.0, 0.2478715415415142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1570862605109819, 0.1570862605109817, 0.25608724011661493], 
reward next is 0.7439, 
noisyNet noise sample is [array([2.2873132], dtype=float32), 0.017497215]. 
=============================================
[2019-03-26 22:47:12,989] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.207985e-22 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:12,998] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4794
[2019-03-26 22:47:13,008] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2864132040095518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 461340.351837531, 461340.3518375317, 164364.5285493706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 187200.0000, 
sim time next is 187800.0000, 
raw observation next is [19.88333333333333, 96.0, 1.0, 2.0, 0.2862261540114023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 461138.5560566716, 461138.5560566716, 164350.7811383822], 
processed observation next is [0.0, 0.17391304347826086, 0.14139020537124788, 0.96, 1.0, 1.0, 0.14003151085711116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12809404334907545, 0.12809404334907545, 0.24529967334086897], 
reward next is 0.7547, 
noisyNet noise sample is [array([0.1775028], dtype=float32), 0.5259875]. 
=============================================
[2019-03-26 22:47:16,145] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 6.69218e-23 0.00000e+00], sum to 1.0000
[2019-03-26 22:47:16,153] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7181
[2019-03-26 22:47:16,157] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.13333333333333, 88.66666666666666, 1.0, 2.0, 0.256341999938463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 417466.0831842221, 417466.0831842227, 161473.3418926864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 358800.0000, 
sim time next is 359400.0000, 
raw observation next is [20.11666666666667, 88.83333333333334, 1.0, 2.0, 0.25656065928945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 417798.8141201269, 417798.8141201264, 161494.3555650893], 
processed observation next is [1.0, 0.13043478260869565, 0.15244865718799394, 0.8883333333333334, 1.0, 1.0, 0.10428995095114456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11605522614447969, 0.11605522614447956, 0.2410363515896855], 
reward next is 0.7590, 
noisyNet noise sample is [array([-0.13876815], dtype=float32), -0.44006798]. 
=============================================
[2019-03-26 22:47:16,702] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.949619e-25 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:16,710] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2144
[2019-03-26 22:47:16,722] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.55, 96.0, 1.0, 2.0, 0.7553360354431568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1134393.075448903, 1134393.075448903, 243666.2669851979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 145800.0000, 
sim time next is 146400.0000, 
raw observation next is [22.53333333333333, 96.0, 1.0, 2.0, 0.7250057222811997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089465.522283992, 1089465.522283992, 236280.4847419269], 
processed observation next is [1.0, 0.6956521739130435, 0.26698262243285936, 0.96, 1.0, 1.0, 0.6686815931098792, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30262931174555335, 0.30262931174555335, 0.3526574399133237], 
reward next is 0.6473, 
noisyNet noise sample is [array([-1.1496843], dtype=float32), 1.2996137]. 
=============================================
[2019-03-26 22:47:24,275] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4143701e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:24,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6866
[2019-03-26 22:47:24,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.13333333333333, 56.33333333333334, 1.0, 2.0, 0.2452469301751609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403336.2076768128, 403336.2076768128, 160441.8359325455], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 494400.0000, 
sim time next is 495000.0000, 
raw observation next is [23.9, 57.5, 1.0, 2.0, 0.2428323260854071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399503.9608848941, 399503.9608848948, 160206.8695239888], 
processed observation next is [1.0, 0.7391304347826086, 0.33175355450236965, 0.575, 1.0, 1.0, 0.0877497904643459, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11097332246802614, 0.11097332246802634, 0.2391147306328191], 
reward next is 0.7609, 
noisyNet noise sample is [array([0.02881467], dtype=float32), 1.1134335]. 
=============================================
[2019-03-26 22:47:24,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[77.20223 ]
 [75.69496 ]
 [71.996765]
 [71.95701 ]
 [71.67812 ]], R is [[77.868927  ]
 [77.85077667]
 [77.82234955]
 [77.73392487]
 [77.65019989]].
[2019-03-26 22:47:29,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9362378e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:29,505] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1530
[2019-03-26 22:47:29,515] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 77.16666666666667, 1.0, 2.0, 0.2526953037791749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 415038.7934701332, 415038.7934701332, 161177.4105638153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 762600.0000, 
sim time next is 763200.0000, 
raw observation next is [20.8, 79.0, 1.0, 2.0, 0.2538892108566228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416831.1548951771, 416831.1548951764, 161296.7499757389], 
processed observation next is [1.0, 0.8695652173913043, 0.1848341232227489, 0.79, 1.0, 1.0, 0.10107133838147325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11578643191532698, 0.11578643191532678, 0.24074141787423717], 
reward next is 0.7593, 
noisyNet noise sample is [array([0.6629932], dtype=float32), -1.7452658]. 
=============================================
[2019-03-26 22:47:30,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9896407e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:30,406] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1288
[2019-03-26 22:47:30,413] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 75.0, 1.0, 2.0, 0.238549696856602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394756.0296538163, 394756.0296538163, 159692.2469175723], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 592200.0000, 
sim time next is 592800.0000, 
raw observation next is [20.46666666666667, 75.66666666666666, 1.0, 2.0, 0.2382874569346272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 394479.4976514422, 394479.4976514422, 159654.2079338093], 
processed observation next is [1.0, 0.8695652173913043, 0.16903633491311232, 0.7566666666666666, 1.0, 1.0, 0.08227404449955082, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10957763823651172, 0.10957763823651172, 0.23828986258777507], 
reward next is 0.7617, 
noisyNet noise sample is [array([-1.1733944], dtype=float32), 0.5876712]. 
=============================================
[2019-03-26 22:47:30,434] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.723223e-29 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:30,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7011
[2019-03-26 22:47:30,449] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 53.0, 1.0, 2.0, 0.5412676434937735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887544.5930901167, 887544.5930901167, 203569.2083189036], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 487800.0000, 
sim time next is 488400.0000, 
raw observation next is [24.96666666666667, 53.0, 1.0, 2.0, 0.5553063906210898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 910904.0761430832, 910904.0761430832, 206340.8882840651], 
processed observation next is [1.0, 0.6521739130434783, 0.3823064770932071, 0.53, 1.0, 1.0, 0.4642245670133611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2530289100397453, 0.2530289100397453, 0.30797147505084344], 
reward next is 0.6920, 
noisyNet noise sample is [array([-1.2352883], dtype=float32), 0.7897005]. 
=============================================
[2019-03-26 22:47:32,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.154344e-23 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:32,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4343
[2019-03-26 22:47:32,161] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.86666666666667, 72.0, 1.0, 2.0, 0.4187583075914904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686689.8810980052, 686689.8810980045, 182272.7719313551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 721200.0000, 
sim time next is 721800.0000, 
raw observation next is [22.05, 71.0, 1.0, 2.0, 0.4842114114022496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 793551.9730142583, 793551.9730142589, 193007.2105690312], 
processed observation next is [1.0, 0.34782608695652173, 0.24407582938388633, 0.71, 1.0, 1.0, 0.3785679655448791, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22043110361507176, 0.22043110361507193, 0.28807046353586746], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.69257635], dtype=float32), 1.4122609]. 
=============================================
[2019-03-26 22:47:33,601] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.992019e-18 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:33,610] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3788
[2019-03-26 22:47:33,617] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.35, 53.0, 1.0, 2.0, 0.6158825450571871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005456.406062563, 1005456.406062563, 218956.4809668626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 483000.0000, 
sim time next is 483600.0000, 
raw observation next is [25.3, 53.0, 1.0, 2.0, 0.6183887266573648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1010166.130353235, 1010166.130353234, 219526.8675808142], 
processed observation next is [1.0, 0.6086956521739131, 0.39810426540284366, 0.53, 1.0, 1.0, 0.5402273815148972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28060170287589864, 0.28060170287589836, 0.3276520411653943], 
reward next is 0.6723, 
noisyNet noise sample is [array([-0.9709129], dtype=float32), 1.6433045]. 
=============================================
[2019-03-26 22:47:35,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.451823e-16 0.000000e+00], sum to 1.0000
[2019-03-26 22:47:35,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9314
[2019-03-26 22:47:35,915] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.0, 1.0, 2.0, 0.2599698969719779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425513.1070046628, 425513.1070046628, 161899.20165011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 784800.0000, 
sim time next is 785400.0000, 
raw observation next is [19.4, 92.0, 1.0, 2.0, 0.2598266316685817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425273.1627359597, 425273.1627359597, 161884.5354224691], 
processed observation next is [0.0, 0.08695652173913043, 0.11848341232227487, 0.92, 1.0, 1.0, 0.10822485743202616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11813143409332214, 0.11813143409332214, 0.2416187095857748], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.41299048], dtype=float32), 0.8430213]. 
=============================================
[2019-03-26 22:47:36,223] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5695818e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:47:36,234] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4337
[2019-03-26 22:47:36,244] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.18333333333333, 91.16666666666667, 1.0, 2.0, 0.245768296161783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 410712.8942951671, 410712.8942951671, 159480.1815492728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 612600.0000, 
sim time next is 613200.0000, 
raw observation next is [17.16666666666667, 91.33333333333334, 1.0, 2.0, 0.2182630173528393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 364737.7274437897, 364737.7274437897, 156963.1787796891], 
processed observation next is [1.0, 0.08695652173913043, 0.012638230647709612, 0.9133333333333334, 1.0, 1.0, 0.05814821367811961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1013160354010527, 0.1013160354010527, 0.23427340116371506], 
reward next is 0.7657, 
noisyNet noise sample is [array([0.41103858], dtype=float32), -0.08857035]. 
=============================================
[2019-03-26 22:47:38,819] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 22:47:38,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:47:38,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:38,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:47:38,824] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:47:38,825] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:38,826] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:47:38,827] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:38,825] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:47:38,830] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:38,831] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:47:38,860] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-03-26 22:47:38,884] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-03-26 22:47:38,905] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-03-26 22:47:38,923] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-03-26 22:47:38,925] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-03-26 22:47:42,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:47:42,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.68333333333333, 74.66666666666667, 1.0, 2.0, 0.2416961920936398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 399988.3317455532, 399988.3317455532, 159989.6215637999]
[2019-03-26 22:47:42,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:47:42,735] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.464626e-28 0.000000e+00], sampled 0.013581845824779615
[2019-03-26 22:47:45,113] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:47:45,114] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.423716175, 89.924490395, 1.0, 2.0, 0.3990375595010443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581960.5896409921, 581960.5896409921, 172441.7061126046]
[2019-03-26 22:47:45,115] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:47:45,117] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.8759557e-25 0.0000000e+00], sampled 0.041585856494965046
[2019-03-26 22:48:03,928] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:48:03,929] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.31859824333333, 77.91733303333334, 1.0, 2.0, 0.9645455076962872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1348214.546471264, 1348214.546471264, 288308.5385165588]
[2019-03-26 22:48:03,932] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:48:03,936] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5244657e-22 0.0000000e+00], sampled 0.8257702162773298
[2019-03-26 22:48:06,359] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:48:06,361] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 85.83333333333334, 1.0, 2.0, 0.5547067336108619, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9280430225934735, 6.911200000000001, 6.9112, 168.9129533137845, 1550854.81900146, 1550854.819001459, 332006.420136622]
[2019-03-26 22:48:06,362] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:48:06,365] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1480249e-29 1.0000000e+00 9.6733585e-36 1.3752705e-09 0.0000000e+00], sampled 0.9507903579696079
[2019-03-26 22:48:39,608] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:48:39,611] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.03333333333334, 47.66666666666666, 1.0, 2.0, 0.900152018214924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104309, 1258153.807493078, 1258153.807493078, 269982.1450204536]
[2019-03-26 22:48:39,612] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:48:39,617] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.444088e-23 0.000000e+00], sampled 0.568560267878479
[2019-03-26 22:48:42,004] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:48:42,005] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [36.16666666666666, 59.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.596840866874045, 6.9112, 170.5573041426782, 4118229.774477914, 2910736.654049505, 543041.9971297841]
[2019-03-26 22:48:42,006] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:48:42,009] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.2804915e-24 1.7572752e-19 1.1272356e-26 1.0000000e+00 1.7597013e-29], sampled 0.3150745919350523
[2019-03-26 22:48:42,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4118229.774477914 W.
[2019-03-26 22:48:43,962] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:48:43,963] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.96249928666666, 48.53862336, 1.0, 2.0, 0.5296032430867018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740052.6583329687, 740052.6583329693, 188365.4560861917]
[2019-03-26 22:48:43,964] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:48:43,966] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5237445e-27 0.0000000e+00], sampled 0.40818545020059027
[2019-03-26 22:49:32,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.6668794]
[2019-03-26 22:49:32,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.75, 85.5, 1.0, 2.0, 0.5388911077864368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753035.8593542437, 753035.8593542431, 189914.3213791161]
[2019-03-26 22:49:32,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:49:32,323] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.182628e-30 0.000000e+00], sampled 0.15130662886043889
[2019-03-26 22:49:32,816] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8543.2917 2837863971.2845 991.0000
[2019-03-26 22:49:33,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7993.2920 3152556226.7796 1458.0000
[2019-03-26 22:49:33,707] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8311.5357 2923307553.2258 1205.0000
[2019-03-26 22:49:33,723] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8078.9128 3000223360.3752 1554.0000
[2019-03-26 22:49:33,724] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8713.9604 2774183066.1262 776.0000
[2019-03-26 22:49:34,747] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1925000, evaluation results [1925000.0, 7993.292018411774, 3152556226.779594, 1458.0, 8311.53573285276, 2923307553.225771, 1205.0, 8713.960421484171, 2774183066.1261926, 776.0, 8078.912790956793, 3000223360.3751903, 1554.0, 8543.291699400212, 2837863971.2845483, 991.0]
[2019-03-26 22:49:36,694] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1502728e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:49:36,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4309
[2019-03-26 22:49:36,712] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.33333333333334, 1.0, 2.0, 0.5585032590207535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 918411.6264856434, 918411.626485644, 206990.280047354], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 660000.0000, 
sim time next is 660600.0000, 
raw observation next is [24.7, 53.5, 1.0, 2.0, 0.5948755988889678, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 977866.2161074358, 977866.2161074365, 214516.1061950178], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.535, 1.0, 1.0, 0.511898311914419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2716295044742877, 0.2716295044742879, 0.3201732928283848], 
reward next is 0.6798, 
noisyNet noise sample is [array([-0.92633927], dtype=float32), -0.8680921]. 
=============================================
[2019-03-26 22:49:39,902] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.9458363e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 22:49:39,911] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4315
[2019-03-26 22:49:39,918] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.36666666666667, 90.33333333333334, 1.0, 2.0, 0.2064099535450901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 344783.9959661341, 344783.9959661347, 156049.3884382827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 610800.0000, 
sim time next is 611400.0000, 
raw observation next is [17.28333333333333, 90.66666666666667, 1.0, 2.0, 0.2055258604055006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 343373.9307243232, 343373.9307243232, 155935.3899002669], 
processed observation next is [1.0, 0.043478260869565216, 0.018167456556082123, 0.9066666666666667, 1.0, 1.0, 0.042802241452410364, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09538164742342312, 0.09538164742342312, 0.2327393879108461], 
reward next is 0.7673, 
noisyNet noise sample is [array([-0.45807865], dtype=float32), -0.8257147]. 
=============================================
[2019-03-26 22:49:43,814] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.475922e-24 0.000000e+00], sum to 1.0000
[2019-03-26 22:49:43,824] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6294
[2019-03-26 22:49:43,831] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 71.5, 1.0, 2.0, 0.3807037680022441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627504.771609925, 627504.771609925, 176693.1715568655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 635400.0000, 
sim time next is 636000.0000, 
raw observation next is [21.66666666666667, 70.66666666666667, 1.0, 2.0, 0.3670622911218916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604694.740937864, 604694.740937864, 174821.7769256538], 
processed observation next is [1.0, 0.34782608695652173, 0.22590837282780438, 0.7066666666666667, 1.0, 1.0, 0.23742444713480915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16797076137162886, 0.16797076137162886, 0.26092802526216985], 
reward next is 0.7391, 
noisyNet noise sample is [array([-0.6752003], dtype=float32), 1.5107894]. 
=============================================
[2019-03-26 22:49:43,845] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[74.71029 ]
 [75.22767 ]
 [75.97944 ]
 [76.878105]
 [76.73812 ]], R is [[74.81877899]
 [74.80687714]
 [74.80135345]
 [74.80435944]
 [74.81706238]].
[2019-03-26 22:49:47,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8851782e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:49:47,569] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8127
[2019-03-26 22:49:47,575] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.71666666666667, 67.0, 1.0, 2.0, 0.7495945480135784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1159024.965604882, 1159024.965604882, 246130.2975684384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1095000.0000, 
sim time next is 1095600.0000, 
raw observation next is [25.73333333333333, 67.0, 1.0, 2.0, 0.7365807830015667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138469.343710732, 1138469.343710732, 242778.3475346128], 
processed observation next is [1.0, 0.6956521739130435, 0.41864139020537117, 0.67, 1.0, 1.0, 0.682627449399478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3162414843640922, 0.3162414843640922, 0.36235574258897435], 
reward next is 0.6376, 
noisyNet noise sample is [array([1.6105365], dtype=float32), -1.485337]. 
=============================================
[2019-03-26 22:49:50,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4089178e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:49:50,288] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2088
[2019-03-26 22:49:50,293] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.66666666666667, 1.0, 2.0, 0.8817825880218564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1356244.2926832, 1356244.2926832, 281994.2797926581], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1182000.0000, 
sim time next is 1182600.0000, 
raw observation next is [27.6, 57.5, 1.0, 2.0, 0.8694337795922499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1338358.83683975, 1338358.836839749, 278471.2090432497], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.575, 1.0, 1.0, 0.842691300713554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3717663435665972, 0.37176634356659694, 0.41562867021380556], 
reward next is 0.5844, 
noisyNet noise sample is [array([-2.5576298], dtype=float32), -1.3438295]. 
=============================================
[2019-03-26 22:49:54,844] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1998463e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 22:49:54,853] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6305
[2019-03-26 22:49:54,862] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 95.0, 1.0, 2.0, 0.3415342637599309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 527294.7460882227, 527294.7460882227, 168819.5140264291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 957600.0000, 
sim time next is 958200.0000, 
raw observation next is [21.8, 94.83333333333334, 1.0, 2.0, 0.4098947061082622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633078.9493693406, 633078.9493693413, 178043.0394659328], 
processed observation next is [1.0, 0.08695652173913043, 0.23222748815165886, 0.9483333333333335, 1.0, 1.0, 0.2890297663954966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17585526371370572, 0.1758552637137059, 0.26573587979989965], 
reward next is 0.7343, 
noisyNet noise sample is [array([1.1703967], dtype=float32), -0.99667555]. 
=============================================
[2019-03-26 22:49:58,674] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 6.56975e-32 0.00000e+00], sum to 1.0000
[2019-03-26 22:49:58,682] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2719
[2019-03-26 22:49:58,689] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.55, 95.0, 1.0, 2.0, 0.2985181058265153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 475780.7580911931, 475780.7580911931, 165327.4756442262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1061400.0000, 
sim time next is 1062000.0000, 
raw observation next is [20.6, 95.0, 1.0, 2.0, 0.3003555778695772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478186.0030671549, 478186.0030671549, 165490.4903227347], 
processed observation next is [1.0, 0.30434782608695654, 0.17535545023696694, 0.95, 1.0, 1.0, 0.1570549130958761, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13282944529643192, 0.13282944529643192, 0.24700073182497714], 
reward next is 0.7530, 
noisyNet noise sample is [array([0.8274122], dtype=float32), 1.2374355]. 
=============================================
[2019-03-26 22:49:58,700] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[75.55857 ]
 [75.56986 ]
 [75.577545]
 [75.55161 ]
 [75.47841 ]], R is [[75.5825882 ]
 [75.58000183]
 [75.57767487]
 [75.57558441]
 [75.57357788]].
[2019-03-26 22:50:01,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9583843e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:50:01,383] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2384
[2019-03-26 22:50:01,391] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 60.33333333333333, 1.0, 2.0, 0.3288082263344712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 507300.323228344, 507300.323228344, 167229.4643308986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1186800.0000, 
sim time next is 1187400.0000, 
raw observation next is [26.85, 61.16666666666667, 1.0, 2.0, 0.333174704884044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 513966.7101941255, 513966.7101941248, 167747.2171463742], 
processed observation next is [1.0, 0.7391304347826086, 0.4715639810426541, 0.6116666666666667, 1.0, 1.0, 0.19659602998077588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1427685306094793, 0.1427685306094791, 0.25036898081548387], 
reward next is 0.7496, 
noisyNet noise sample is [array([-1.9120882], dtype=float32), -0.6260541]. 
=============================================
[2019-03-26 22:50:11,064] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.371076e-27 0.000000e+00], sum to 1.0000
[2019-03-26 22:50:11,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1204
[2019-03-26 22:50:11,086] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.562827691920754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875034.8951867357, 875034.8951867357, 204526.2362519844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1342800.0000, 
sim time next is 1343400.0000, 
raw observation next is [21.91666666666667, 90.50000000000001, 1.0, 2.0, 0.5491828657643029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 856840.7394381252, 856840.7394381252, 202185.1770847914], 
processed observation next is [1.0, 0.5652173913043478, 0.23775671406003188, 0.9050000000000001, 1.0, 1.0, 0.45684682622205164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23801131651059032, 0.23801131651059032, 0.3017689210220767], 
reward next is 0.6982, 
noisyNet noise sample is [array([-0.025344], dtype=float32), 0.20941533]. 
=============================================
[2019-03-26 22:50:16,876] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1864656e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:50:16,888] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5216
[2019-03-26 22:50:16,895] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 95.0, 1.0, 2.0, 0.4111974519779803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606916.7836883408, 606916.7836883408, 174989.8254714457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [23.2, 95.0, 1.0, 2.0, 0.4105674953619807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605614.1123385016, 605614.1123385016, 174857.0122912386], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.95, 1.0, 1.0, 0.2898403558578081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16822614231625044, 0.16822614231625044, 0.2609806153600576], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.16623753], dtype=float32), -0.081633255]. 
=============================================
[2019-03-26 22:50:17,212] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.845163e-22 0.000000e+00], sum to 1.0000
[2019-03-26 22:50:17,219] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8475
[2019-03-26 22:50:17,227] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 88.5, 1.0, 2.0, 0.3846810340594198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583261.3342351102, 583261.3342351095, 173278.2983729321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1234200.0000, 
sim time next is 1234800.0000, 
raw observation next is [23.4, 88.0, 1.0, 2.0, 0.3865048400023153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584469.8819849034, 584469.8819849034, 173343.6761196216], 
processed observation next is [1.0, 0.30434782608695654, 0.30805687203791465, 0.88, 1.0, 1.0, 0.2608492048220666, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16235274499580649, 0.16235274499580649, 0.25872190465615164], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.23111446], dtype=float32), 2.1393895]. 
=============================================
[2019-03-26 22:50:17,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.9563566e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:50:17,250] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8046
[2019-03-26 22:50:17,256] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 93.16666666666667, 1.0, 2.0, 0.4692794729524349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660787.5824006919, 660787.5824006925, 179575.0919112472], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1289400.0000, 
sim time next is 1290000.0000, 
raw observation next is [24.53333333333333, 93.33333333333334, 1.0, 2.0, 0.4686659165150509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660260.844352134, 660260.8443521347, 179527.2354870941], 
processed observation next is [1.0, 0.9565217391304348, 0.36176935229067925, 0.9333333333333335, 1.0, 1.0, 0.3598384536325915, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18340579009781502, 0.1834057900978152, 0.2679510977419315], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.26360548], dtype=float32), -0.99241126]. 
=============================================
[2019-03-26 22:50:17,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.76064 ]
 [69.85218 ]
 [69.73424 ]
 [69.64879 ]
 [69.640656]], R is [[69.77578735]
 [69.81001282]
 [69.84391022]
 [69.87754822]
 [69.91078949]].
[2019-03-26 22:50:18,613] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 6.75714e-28 0.00000e+00], sum to 1.0000
[2019-03-26 22:50:18,618] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0827
[2019-03-26 22:50:18,622] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.0, 1.0, 2.0, 0.4283203187018617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617158.4046353626, 617158.404635362, 175536.2886674038], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1655400.0000, 
sim time next is 1656000.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4288213970175884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617880.6233303288, 617880.6233303295, 175606.5323677504], 
processed observation next is [1.0, 0.17391304347826086, 0.3033175355450238, 0.99, 1.0, 1.0, 0.31183300845492584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1716335064806469, 0.1716335064806471, 0.2620993020414185], 
reward next is 0.7379, 
noisyNet noise sample is [array([1.0238051], dtype=float32), -0.46394455]. 
=============================================
[2019-03-26 22:50:18,646] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[71.7583  ]
 [71.84827 ]
 [71.90325 ]
 [71.84894 ]
 [71.857925]], R is [[71.68003082]
 [71.70124054]
 [71.72227478]
 [71.74266815]
 [71.76104736]].
[2019-03-26 22:50:19,782] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.0467406e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:50:19,789] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8242
[2019-03-26 22:50:19,796] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 80.0, 1.0, 2.0, 0.4722703246041702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 659912.3674762726, 659912.3674762732, 179365.9376944303], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1274400.0000, 
sim time next is 1275000.0000, 
raw observation next is [26.75, 80.66666666666667, 1.0, 2.0, 0.4774357289756394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 667132.3522936073, 667132.3522936066, 180136.371535696], 
processed observation next is [1.0, 0.782608695652174, 0.4668246445497631, 0.8066666666666668, 1.0, 1.0, 0.37040449274173426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1853145423037798, 0.1853145423037796, 0.2688602560234269], 
reward next is 0.7311, 
noisyNet noise sample is [array([-1.02637], dtype=float32), -0.06809917]. 
=============================================
[2019-03-26 22:50:19,817] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[77.153564]
 [77.54776 ]
 [76.81281 ]
 [75.30864 ]
 [71.51779 ]], R is [[76.54171753]
 [76.5085907 ]
 [76.47684479]
 [76.44689178]
 [76.41875458]].
[2019-03-26 22:50:30,562] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 22:50:30,564] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:50:30,565] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:50:30,565] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:50:30,566] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:50:30,567] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:50:30,567] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:50:30,568] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:50:30,569] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:50:30,569] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:50:30,572] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:50:30,599] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-03-26 22:50:30,621] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-03-26 22:50:30,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-03-26 22:50:30,643] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-03-26 22:50:30,665] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-03-26 22:51:10,458] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.71726614]
[2019-03-26 22:51:10,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.52096995, 92.40158438666667, 1.0, 2.0, 0.4690171668448264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660578.6197217847, 660578.6197217853, 179556.3867193246]
[2019-03-26 22:51:10,460] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:51:10,462] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.0292546e-30 0.0000000e+00], sampled 0.43312716916008287
[2019-03-26 22:52:11,519] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.71726614]
[2019-03-26 22:52:11,521] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.98991777, 59.40956684, 1.0, 2.0, 0.6668588431242353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943394.6387228722, 943394.6387228722, 215372.4552300115]
[2019-03-26 22:52:11,524] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:52:11,526] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.737671e-28 0.000000e+00], sampled 0.012630646132508971
[2019-03-26 22:52:11,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.71726614]
[2019-03-26 22:52:11,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.78333333333333, 80.33333333333334, 1.0, 2.0, 0.9425174279900722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1317405.212890397, 1317405.212890397, 281902.174578989]
[2019-03-26 22:52:11,543] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:52:11,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5185511e-27 0.0000000e+00], sampled 0.3082482778274328
[2019-03-26 22:52:18,282] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.71726614]
[2019-03-26 22:52:18,283] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.25, 90.0, 1.0, 2.0, 0.3762866042624627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 566773.4268576526, 566773.4268576526, 171703.0952544681]
[2019-03-26 22:52:18,285] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:52:18,288] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4280861e-29 0.0000000e+00], sampled 0.5059248044751617
[2019-03-26 22:52:22,520] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.71726614]
[2019-03-26 22:52:22,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.1, 64.33333333333334, 1.0, 2.0, 0.3140309144760479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 497887.6905335666, 497887.6905335666, 166891.8592629499]
[2019-03-26 22:52:22,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:52:22,527] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.557044e-30 0.000000e+00], sampled 0.9247293827251449
[2019-03-26 22:52:24,314] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1443 2843334260.3044 1125.0000
[2019-03-26 22:52:24,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.6525 3164544440.0219 1773.0000
[2019-03-26 22:52:24,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.4243 2928046626.2477 1327.0000
[2019-03-26 22:52:24,528] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5574 3008577031.3205 1767.0000
[2019-03-26 22:52:24,572] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6127 2779609947.3343 919.0000
[2019-03-26 22:52:25,589] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1950000, evaluation results [1950000.0, 7875.65248485614, 3164544440.021899, 1773.0, 8259.424293184335, 2928046626.2477193, 1327.0, 8660.612728048825, 2779609947.3343077, 919.0, 7997.55740089204, 3008577031.320509, 1767.0, 8495.14426386514, 2843334260.304368, 1125.0]
[2019-03-26 22:52:26,739] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1891824e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:26,746] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9026
[2019-03-26 22:52:26,750] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5829598386777449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 921903.4441623366, 921903.4441623372, 209993.3208716435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1785600.0000, 
sim time next is 1786200.0000, 
raw observation next is [21.23333333333333, 92.33333333333334, 1.0, 2.0, 0.6651258961280077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1050948.669598726, 1050948.669598726, 227896.1378704305], 
processed observation next is [1.0, 0.6956521739130435, 0.2053712480252764, 0.9233333333333335, 1.0, 1.0, 0.5965372242506116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29193018599964615, 0.29193018599964615, 0.3401434893588515], 
reward next is 0.6599, 
noisyNet noise sample is [array([0.7376766], dtype=float32), -0.13225268]. 
=============================================
[2019-03-26 22:52:27,831] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.935433e-28 0.000000e+00], sum to 1.0000
[2019-03-26 22:52:27,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2923
[2019-03-26 22:52:27,845] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 89.33333333333334, 1.0, 2.0, 0.457008147605568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650229.9721294543, 650229.9721294548, 178634.0645105311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1892400.0000, 
sim time next is 1893000.0000, 
raw observation next is [24.76666666666667, 89.66666666666666, 1.0, 2.0, 0.4555482627739102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648744.9343583201, 648744.9343583207, 178495.5949080214], 
processed observation next is [1.0, 0.9130434782608695, 0.3728278041074251, 0.8966666666666666, 1.0, 1.0, 0.3440340515348316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18020692621064446, 0.18020692621064463, 0.266411335683614], 
reward next is 0.7336, 
noisyNet noise sample is [array([-0.7237565], dtype=float32), 0.9149645]. 
=============================================
[2019-03-26 22:52:27,855] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.7601  ]
 [72.476204]
 [72.10476 ]
 [71.940384]
 [71.96445 ]], R is [[72.9661026 ]
 [72.96982574]
 [72.97349548]
 [72.97692871]
 [72.97979736]].
[2019-03-26 22:52:28,879] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6955704e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:28,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2031
[2019-03-26 22:52:28,891] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.15, 95.0, 1.0, 2.0, 0.4113764989521568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607925.7783420607, 607925.7783420607, 175105.7364780429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1625400.0000, 
sim time next is 1626000.0000, 
raw observation next is [23.16666666666666, 95.0, 1.0, 2.0, 0.4116493389435469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607956.8027249908, 607956.8027249908, 175098.0413635816], 
processed observation next is [1.0, 0.8260869565217391, 0.2969984202211688, 0.95, 1.0, 1.0, 0.29114378185969503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16887688964583078, 0.16887688964583078, 0.26134036024415164], 
reward next is 0.7387, 
noisyNet noise sample is [array([2.0155637], dtype=float32), -0.20108046]. 
=============================================
[2019-03-26 22:52:28,920] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[70.59048 ]
 [70.648125]
 [70.7875  ]
 [70.85749 ]
 [70.80738 ]], R is [[70.47264862]
 [70.50656891]
 [70.54017639]
 [70.57354736]
 [70.60660553]].
[2019-03-26 22:52:31,746] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2029047e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:31,752] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1971
[2019-03-26 22:52:31,762] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 89.0, 1.0, 2.0, 0.6566177596595104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977771.4076126468, 977771.4076126468, 219455.7479137963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1609200.0000, 
sim time next is 1609800.0000, 
raw observation next is [23.65, 89.5, 1.0, 2.0, 0.6523175260850095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970834.3492990062, 970834.3492990062, 218459.0909019149], 
processed observation next is [1.0, 0.6521739130434783, 0.31990521327014215, 0.895, 1.0, 1.0, 0.5811054531144693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2696762081386128, 0.2696762081386128, 0.3260583446297237], 
reward next is 0.6739, 
noisyNet noise sample is [array([0.26393405], dtype=float32), 0.467113]. 
=============================================
[2019-03-26 22:52:35,345] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.112348e-33 0.000000e+00], sum to 1.0000
[2019-03-26 22:52:35,354] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4825
[2019-03-26 22:52:35,358] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 84.33333333333333, 1.0, 2.0, 0.4728815665828555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680478.1975876169, 680478.1975876162, 181969.988903834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1928400.0000, 
sim time next is 1929000.0000, 
raw observation next is [25.38333333333333, 83.66666666666667, 1.0, 2.0, 0.4781189451026186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687416.3430143271, 687416.3430143271, 182702.9621575311], 
processed observation next is [1.0, 0.30434782608695654, 0.4020537124802526, 0.8366666666666667, 1.0, 1.0, 0.37122764470195013, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19094898417064643, 0.19094898417064643, 0.2726909882948225], 
reward next is 0.7273, 
noisyNet noise sample is [array([0.9137497], dtype=float32), 0.069503784]. 
=============================================
[2019-03-26 22:52:35,371] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[69.935974]
 [70.02052 ]
 [70.17744 ]
 [69.719124]
 [70.3096  ]], R is [[69.93826294]
 [69.96728516]
 [69.99842834]
 [70.0340271 ]
 [70.05943298]].
[2019-03-26 22:52:37,102] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4261611e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:37,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9950
[2019-03-26 22:52:37,119] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.48333333333333, 95.66666666666666, 1.0, 2.0, 0.3424430376351973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532482.1838473212, 532482.1838473206, 169345.0301547416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1810200.0000, 
sim time next is 1810800.0000, 
raw observation next is [21.5, 96.0, 1.0, 2.0, 0.3442140665188314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534410.3794460463, 534410.3794460457, 169478.9182617124], 
processed observation next is [1.0, 1.0, 0.21800947867298584, 0.96, 1.0, 1.0, 0.20989646568533904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14844732762390175, 0.1484473276239016, 0.2529536093458394], 
reward next is 0.7470, 
noisyNet noise sample is [array([0.66705936], dtype=float32), -1.0209751]. 
=============================================
[2019-03-26 22:52:38,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.413653e-28 0.000000e+00], sum to 1.0000
[2019-03-26 22:52:38,959] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4844
[2019-03-26 22:52:38,968] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 85.5, 1.0, 2.0, 0.3115334276579558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491038.7222584822, 491038.7222584822, 166324.0711696194], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1791000.0000, 
sim time next is 1791600.0000, 
raw observation next is [22.13333333333333, 86.0, 1.0, 2.0, 0.3135263268917957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 494203.9631851632, 494203.9631851626, 166558.9988818591], 
processed observation next is [1.0, 0.7391304347826086, 0.24802527646129527, 0.86, 1.0, 1.0, 0.17292328541180205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13727887866254534, 0.13727887866254515, 0.24859552071919266], 
reward next is 0.7514, 
noisyNet noise sample is [array([-0.62378436], dtype=float32), 1.307707]. 
=============================================
[2019-03-26 22:52:49,119] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.0987622e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:49,129] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9420
[2019-03-26 22:52:49,134] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.03333333333334, 91.33333333333334, 1.0, 2.0, 0.4750164030414903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663750.7142235433, 663750.7142235427, 179774.0483028352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064000.0000, 
sim time next is 2064600.0000, 
raw observation next is [25.0, 91.5, 1.0, 2.0, 0.4756561449921946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664644.9191639184, 664644.9191639189, 179869.5470382831], 
processed observation next is [0.0, 0.9130434782608695, 0.38388625592417064, 0.915, 1.0, 1.0, 0.3682604156532466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.184623588656644, 0.18462358865664416, 0.26846201050490015], 
reward next is 0.7315, 
noisyNet noise sample is [array([1.0091726], dtype=float32), -0.1371498]. 
=============================================
[2019-03-26 22:52:52,454] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3064611e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:52,462] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0023
[2019-03-26 22:52:52,468] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 87.0, 1.0, 2.0, 0.4977559886810924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695535.6322628472, 695535.6322628465, 183245.615524486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1879200.0000, 
sim time next is 1879800.0000, 
raw observation next is [26.31666666666667, 87.0, 1.0, 2.0, 0.49862492834201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696750.2368443463, 696750.2368443457, 183381.0535122411], 
processed observation next is [1.0, 0.782608695652174, 0.4462875197472356, 0.87, 1.0, 1.0, 0.39593364860483127, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19354173245676284, 0.1935417324567627, 0.27370306494364344], 
reward next is 0.7263, 
noisyNet noise sample is [array([-0.47384217], dtype=float32), 0.27517062]. 
=============================================
[2019-03-26 22:52:53,821] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5842417e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 22:52:53,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0415
[2019-03-26 22:52:53,832] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 96.5, 1.0, 2.0, 0.470418714713117, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661346.4960762226, 661346.4960762219, 179610.3312836227], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2093400.0000, 
sim time next is 2094000.0000, 
raw observation next is [24.33333333333333, 96.0, 1.0, 2.0, 0.4764538974751408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667692.2994593258, 667692.2994593258, 180238.4013870275], 
processed observation next is [0.0, 0.21739130434782608, 0.35229067930489716, 0.96, 1.0, 1.0, 0.36922156322306116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18547008318314606, 0.18547008318314606, 0.26901253938362313], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.29593503], dtype=float32), -0.47794062]. 
=============================================
[2019-03-26 22:52:53,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.07508 ]
 [74.12698 ]
 [74.18711 ]
 [74.248085]
 [74.13365 ]], R is [[74.02809906]
 [74.01974487]
 [74.01267242]
 [74.00673676]
 [74.00104523]].
[2019-03-26 22:52:54,608] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.794464e-35 0.000000e+00], sum to 1.0000
[2019-03-26 22:52:54,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5089
[2019-03-26 22:52:54,622] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 93.33333333333334, 1.0, 2.0, 0.512815282090766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 716585.7216602801, 716585.7216602807, 185628.2625937236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2161200.0000, 
sim time next is 2161800.0000, 
raw observation next is [25.7, 93.5, 1.0, 2.0, 0.512143694793553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715646.9587259385, 715646.958725939, 185520.5706594243], 
processed observation next is [1.0, 0.0, 0.4170616113744076, 0.935, 1.0, 1.0, 0.4122213190283771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19879082186831623, 0.1987908218683164, 0.27689637411854373], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.29072204], dtype=float32), -0.34128475]. 
=============================================
[2019-03-26 22:53:01,198] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 1.32484e-38 0.00000e+00], sum to 1.0000
[2019-03-26 22:53:01,209] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8691
[2019-03-26 22:53:01,213] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.0, 1.0, 2.0, 0.5037652164897599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703935.3643046204, 703935.3643046204, 184188.2834954883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2025000.0000, 
sim time next is 2025600.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5033578552556454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703365.9504080118, 703365.9504080118, 184124.0527852743], 
processed observation next is [0.0, 0.43478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4016359701875245, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19537943066889216, 0.19537943066889216, 0.27481201908249897], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.4429619], dtype=float32), -2.48179]. 
=============================================
[2019-03-26 22:53:03,854] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.480064e-38 0.000000e+00], sum to 1.0000
[2019-03-26 22:53:03,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4787
[2019-03-26 22:53:03,872] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 93.5, 1.0, 2.0, 0.512143694793553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715646.9587259385, 715646.958725939, 185520.5706594243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2161800.0000, 
sim time next is 2162400.0000, 
raw observation next is [25.66666666666667, 93.66666666666667, 1.0, 2.0, 0.511557470526853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714827.5192284478, 714827.5192284472, 185426.6728663435], 
processed observation next is [1.0, 0.0, 0.4154818325434442, 0.9366666666666668, 1.0, 1.0, 0.41151502473114815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19856319978567993, 0.1985631997856798, 0.27675622815872164], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.81942767], dtype=float32), -0.3212592]. 
=============================================
[2019-03-26 22:53:10,132] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5338808e-13 1.7210772e-03 1.6513804e-15 9.9827898e-01 4.3315724e-19], sum to 1.0000
[2019-03-26 22:53:10,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3333
[2019-03-26 22:53:10,149] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 65.33333333333334, 1.0, 2.0, 0.8706993164617335, 1.0, 2.0, 0.8706993164617335, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2435260.955561548, 2435260.955561548, 455759.6638454327], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2209800.0000, 
sim time next is 2210400.0000, 
raw observation next is [32.1, 65.0, 1.0, 2.0, 0.898576015271232, 1.0, 2.0, 0.898576015271232, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2513307.771862936, 2513307.771862936, 470716.2851968266], 
processed observation next is [1.0, 0.6086956521739131, 0.7203791469194314, 0.65, 1.0, 1.0, 0.8778024280376289, 1.0, 1.0, 0.8778024280376289, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6981410477397045, 0.6981410477397045, 0.7025616196967561], 
reward next is 0.2974, 
noisyNet noise sample is [array([0.42574006], dtype=float32), 0.06393786]. 
=============================================
[2019-03-26 22:53:13,396] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4476955e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 22:53:13,405] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9295
[2019-03-26 22:53:13,411] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 75.66666666666666, 1.0, 2.0, 0.7034871674576918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983145.6160424062, 983145.6160424056, 221598.6202840613], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2357400.0000, 
sim time next is 2358000.0000, 
raw observation next is [28.7, 75.0, 1.0, 2.0, 0.7020375815863508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 981118.8380463118, 981118.8380463118, 221284.6010492296], 
processed observation next is [1.0, 0.30434782608695654, 0.5592417061611374, 0.75, 1.0, 1.0, 0.6410091344413864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27253301056841994, 0.27253301056841994, 0.330275523954074], 
reward next is 0.6697, 
noisyNet noise sample is [array([1.4001384], dtype=float32), -1.3915778]. 
=============================================
[2019-03-26 22:53:13,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[53.5316  ]
 [53.265057]
 [53.057693]
 [53.006077]
 [53.090767]], R is [[53.8864975 ]
 [54.01688766]
 [54.145401  ]
 [54.26890182]
 [54.39298248]].
[2019-03-26 22:53:17,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1779463e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 22:53:17,305] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9985
[2019-03-26 22:53:17,310] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.95, 81.0, 1.0, 2.0, 0.5278805504544675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737644.5803271668, 737644.5803271674, 188079.6796327491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2338200.0000, 
sim time next is 2338800.0000, 
raw observation next is [27.9, 81.0, 1.0, 2.0, 0.5258523360577524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 734809.4335724213, 734809.4335724219, 187745.7522814385], 
processed observation next is [1.0, 0.043478260869565216, 0.5213270142180094, 0.81, 1.0, 1.0, 0.4287377542864486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2041137315478948, 0.20411373154789497, 0.2802175407185649], 
reward next is 0.7198, 
noisyNet noise sample is [array([-1.2563199], dtype=float32), 1.3395432]. 
=============================================
[2019-03-26 22:53:21,531] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 22:53:21,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:53:21,535] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:53:21,535] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:53:21,537] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:53:21,537] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:53:21,538] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:53:21,539] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:53:21,540] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:53:21,541] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:53:21,541] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:53:21,569] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-03-26 22:53:21,594] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-03-26 22:53:21,595] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-03-26 22:53:21,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-03-26 22:53:21,639] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-03-26 22:53:24,531] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:53:24,532] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.41666666666666, 40.0, 1.0, 2.0, 0.3047688831896209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506091.232918842, 506091.232918842, 166572.2963882651]
[2019-03-26 22:53:24,532] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:53:24,536] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3155863e-33 0.0000000e+00], sampled 0.8816995523518438
[2019-03-26 22:53:32,887] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:53:32,888] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [16.54052664666667, 92.94260031666667, 1.0, 2.0, 0.2116526201660633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 353863.8598984272, 353863.8598984272, 152650.7947806478]
[2019-03-26 22:53:32,890] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:53:32,892] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.772547e-34 0.000000e+00], sampled 0.33904125655342643
[2019-03-26 22:54:18,294] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:54:18,296] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.9, 72.66666666666667, 1.0, 2.0, 0.5229614107196473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730768.3563197677, 730768.3563197671, 187271.2044333002]
[2019-03-26 22:54:18,298] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:54:18,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3885366e-31 0.0000000e+00], sampled 0.5805065083416332
[2019-03-26 22:54:22,464] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:54:22,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.85, 49.0, 1.0, 2.0, 0.6114507130651661, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.947382996961844, 6.9112, 168.912630372964, 1709628.00088594, 1683958.587081212, 366696.2304858152]
[2019-03-26 22:54:22,467] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:54:22,470] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1523724e-29 1.0000000e+00 1.6839446e-34 2.7433455e-15 0.0000000e+00], sampled 0.8765347322175077
[2019-03-26 22:54:22,471] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1709628.00088594 W.
[2019-03-26 22:54:27,513] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:54:27,515] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.65870583, 82.17711625, 1.0, 2.0, 0.5235806901998518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731634.0142622142, 731634.0142622136, 187370.8716294511]
[2019-03-26 22:54:27,516] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:54:27,519] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.0306296e-34 0.0000000e+00], sampled 0.6216242594011028
[2019-03-26 22:54:48,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:54:48,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 79.66666666666667, 1.0, 2.0, 0.5500332910040315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768611.3607103456, 768611.3607103461, 191805.7090399347]
[2019-03-26 22:54:48,541] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:54:48,544] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9181678e-31 0.0000000e+00], sampled 0.9731757571787721
[2019-03-26 22:54:50,180] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.72199607]
[2019-03-26 22:54:50,182] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.01666666666667, 84.66666666666667, 1.0, 2.0, 0.5578893812717798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779593.4161943315, 779593.4161943315, 193162.4493629913]
[2019-03-26 22:54:50,188] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:54:50,191] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.074101e-33 0.000000e+00], sampled 0.05533425294542327
[2019-03-26 22:55:15,829] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.0429 2779705169.3545 927.0000
[2019-03-26 22:55:15,937] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.8107 3008201681.2996 1768.0000
[2019-03-26 22:55:16,134] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9807 2843270489.3329 1131.0000
[2019-03-26 22:55:16,234] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.6574 2928016837.6026 1337.0000
[2019-03-26 22:55:16,315] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.7384 3164366738.6323 1774.0000
[2019-03-26 22:55:17,331] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1975000, evaluation results [1975000.0, 7876.738400208266, 3164366738.632291, 1774.0, 8254.657435931651, 2928016837.6026235, 1337.0, 8661.042934826895, 2779705169.354473, 927.0, 7998.810679533118, 3008201681.2995505, 1768.0, 8495.980682262947, 2843270489.3329268, 1131.0]
[2019-03-26 22:55:18,990] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.965766e-23 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:19,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6644
[2019-03-26 22:55:19,006] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 92.33333333333333, 1.0, 2.0, 0.7834872276045254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095005.878563655, 1095005.878563655, 239898.1733106222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2535000.0000, 
sim time next is 2535600.0000, 
raw observation next is [26.7, 91.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.976737658534601, 6.9112, 168.9073340352205, 2210174.842893007, 1454270.916980723, 311350.9248962943], 
processed observation next is [1.0, 0.34782608695652173, 0.46445497630331756, 0.9166666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.1065537658534601, 0.0, 0.8294123362250387, 0.6139374563591686, 0.4039641436057564, 0.46470287297954377], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.08465119], dtype=float32), 0.31618762]. 
=============================================
[2019-03-26 22:55:21,676] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9322723e-13 6.3829561e-04 2.1610982e-16 9.9936169e-01 2.9313026e-20], sum to 1.0000
[2019-03-26 22:55:21,685] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4332
[2019-03-26 22:55:21,690] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.28333333333333, 82.33333333333334, 1.0, 2.0, 0.7808494477337925, 1.0, 1.0, 0.7808494477337925, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2183735.877778471, 2183735.877778471, 410654.6825421769], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2479800.0000, 
sim time next is 2480400.0000, 
raw observation next is [28.4, 82.0, 1.0, 2.0, 0.7390984273338597, 1.0, 2.0, 0.7390984273338597, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2066861.670008772, 2066861.670008772, 391311.5882186133], 
processed observation next is [1.0, 0.7391304347826086, 0.5450236966824644, 0.82, 1.0, 1.0, 0.6856607558239273, 1.0, 1.0, 0.6856607558239273, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5741282416691034, 0.5741282416691034, 0.5840471465949452], 
reward next is 0.4160, 
noisyNet noise sample is [array([0.5052126], dtype=float32), -0.9852125]. 
=============================================
[2019-03-26 22:55:25,419] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.238433e-23 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:25,427] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0202
[2019-03-26 22:55:25,434] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.5, 93.0, 1.0, 2.0, 0.7620022220373106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064963.264275522, 1064963.264275522, 234797.8168785044], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2534400.0000, 
sim time next is 2535000.0000, 
raw observation next is [26.6, 92.33333333333333, 1.0, 2.0, 0.7834872276045254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095005.878563655, 1095005.878563655, 239898.1733106222], 
processed observation next is [1.0, 0.34782608695652173, 0.4597156398104266, 0.9233333333333333, 1.0, 1.0, 0.7391412380777415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3041682996010153, 0.3041682996010153, 0.3580569750904809], 
reward next is 0.6419, 
noisyNet noise sample is [array([0.48145598], dtype=float32), 1.234372]. 
=============================================
[2019-03-26 22:55:25,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.822998]
 [58.592957]
 [58.92701 ]
 [59.733513]
 [58.90954 ]], R is [[58.67248535]
 [58.73531723]
 [58.79174042]
 [58.85779572]
 [58.94815063]].
[2019-03-26 22:55:25,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.461911e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:25,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3097
[2019-03-26 22:55:25,635] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911062162407326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584174.1603886123, 584174.1603886123, 173103.7404002603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2747400.0000, 
sim time next is 2748000.0000, 
raw observation next is [22.66666666666667, 96.0, 1.0, 2.0, 0.3899526964879866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583306.3079049023, 583306.3079049016, 173051.4434765129], 
processed observation next is [0.0, 0.8260869565217391, 0.27330173775671435, 0.96, 1.0, 1.0, 0.26500324878070675, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16202952997358397, 0.16202952997358377, 0.25828573653210884], 
reward next is 0.7417, 
noisyNet noise sample is [array([-1.0651168], dtype=float32), 0.723382]. 
=============================================
[2019-03-26 22:55:25,658] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[83.922966]
 [83.93487 ]
 [83.78771 ]
 [83.752266]
 [83.68751 ]], R is [[83.88187408]
 [83.78469086]
 [83.68834686]
 [83.59275055]
 [83.4980011 ]].
[2019-03-26 22:55:25,844] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.8639517e-17 9.9999928e-01 1.8706136e-21 7.1398114e-07 2.0628265e-25], sum to 1.0000
[2019-03-26 22:55:25,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4449
[2019-03-26 22:55:25,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1891136.571417369 W.
[2019-03-26 22:55:25,866] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.86666666666667, 77.33333333333334, 1.0, 2.0, 0.6763155059389547, 1.0, 2.0, 0.6763155059389547, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1891136.571417369, 1891136.571417369, 364149.2557493706], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2547600.0000, 
sim time next is 2548200.0000, 
raw observation next is [28.98333333333333, 76.66666666666667, 1.0, 2.0, 0.4628523726451879, 1.0, 2.0, 0.4628523726451879, 1.0, 1.0, 0.7989973838742987, 6.911199999999999, 6.9112, 170.5573041426782, 1941410.983513399, 1941410.983513399, 389609.8708447718], 
processed observation next is [1.0, 0.4782608695652174, 0.5726698262243285, 0.7666666666666667, 1.0, 1.0, 0.352834183909865, 1.0, 1.0, 0.352834183909865, 1.0, 0.5, 0.7548748583832912, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.539280828753722, 0.539280828753722, 0.5815072699175698], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2435943], dtype=float32), -0.43496653]. 
=============================================
[2019-03-26 22:55:27,049] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2195768e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:27,059] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9927
[2019-03-26 22:55:27,066] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3103552240950783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490769.21008797, 490769.21008797, 166340.4255789926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2953200.0000, 
sim time next is 2953800.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3112988517276675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 492264.0480448005, 492264.0480448011, 166450.5901697146], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17023958039478015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13674001334577793, 0.1367400133457781, 0.2484337166712158], 
reward next is 0.7516, 
noisyNet noise sample is [array([-0.70368326], dtype=float32), -0.10403245]. 
=============================================
[2019-03-26 22:55:27,941] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.2265822e-15 3.9733506e-05 1.6179755e-17 9.9996030e-01 5.7858796e-20], sum to 1.0000
[2019-03-26 22:55:27,956] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0234
[2019-03-26 22:55:27,964] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.3, 80.66666666666667, 1.0, 2.0, 0.7318632580667299, 1.0, 2.0, 0.7318632580667299, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2046609.457074803, 2046609.457074804, 388061.4584726146], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2544600.0000, 
sim time next is 2545200.0000, 
raw observation next is [28.4, 80.0, 1.0, 2.0, 0.7117175419361105, 1.0, 2.0, 0.7117175419361105, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1990220.902268981, 1990220.902268981, 379179.6403167616], 
processed observation next is [1.0, 0.4782608695652174, 0.5450236966824644, 0.8, 1.0, 1.0, 0.6526717372724222, 1.0, 1.0, 0.6526717372724222, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5528391395191614, 0.5528391395191614, 0.5659397616668084], 
reward next is 0.4341, 
noisyNet noise sample is [array([0.4878444], dtype=float32), 0.8575885]. 
=============================================
[2019-03-26 22:55:30,464] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.4809276e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:30,476] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4865
[2019-03-26 22:55:30,484] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 90.0, 1.0, 2.0, 0.5545332603137373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864787.5775435641, 864787.5775435634, 203176.4031333039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [22.0, 89.0, 1.0, 2.0, 0.5395591959381556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844189.1259799454, 844189.1259799454, 200583.1075371584], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.89, 1.0, 1.0, 0.4452520432989826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23449697943887374, 0.23449697943887374, 0.29937777244352], 
reward next is 0.7006, 
noisyNet noise sample is [array([0.665052], dtype=float32), 0.7992257]. 
=============================================
[2019-03-26 22:55:30,500] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.39158 ]
 [66.932076]
 [67.20667 ]
 [67.63986 ]
 [67.6548  ]], R is [[67.69083405]
 [67.7106781 ]
 [67.72005463]
 [67.735672  ]
 [67.76023865]].
[2019-03-26 22:55:32,609] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4813637e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:32,615] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5728
[2019-03-26 22:55:32,621] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.331634081581932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 515089.818378539, 515089.818378539, 167947.7659570396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3043800.0000, 
sim time next is 3044400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3314771753901596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514847.1167386528, 514847.1167386528, 167928.8227189743], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.19455081372308383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14301308798295911, 0.14301308798295911, 0.25064003390891687], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.71876943], dtype=float32), 0.39215362]. 
=============================================
[2019-03-26 22:55:33,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3887447e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:33,565] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5764
[2019-03-26 22:55:33,570] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3959781094822624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 590857.7785036261, 590857.7785036255, 173696.3566230513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2674800.0000, 
sim time next is 2675400.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3955351687338005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 590196.8710161274, 590196.8710161281, 173635.6708689709], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.2717291189563862, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16394357528225761, 0.1639435752822578, 0.25915771771488194], 
reward next is 0.7408, 
noisyNet noise sample is [array([1.3553032], dtype=float32), 0.93267703]. 
=============================================
[2019-03-26 22:55:36,393] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.309448e-22 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:36,399] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4580
[2019-03-26 22:55:36,403] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 86.0, 1.0, 2.0, 0.5656816255115659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846588.7830245778, 846588.7830245785, 201492.78422965], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2809800.0000, 
sim time next is 2810400.0000, 
raw observation next is [24.0, 87.0, 1.0, 2.0, 0.5074931472829456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756014.4061528356, 756014.4061528356, 190594.5126949097], 
processed observation next is [1.0, 0.5217391304347826, 0.3364928909952607, 0.87, 1.0, 1.0, 0.40661824973848865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.210004001709121, 0.210004001709121, 0.284469421932701], 
reward next is 0.7155, 
noisyNet noise sample is [array([-0.27955425], dtype=float32), 0.09244749]. 
=============================================
[2019-03-26 22:55:43,009] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.124963e-30 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:43,017] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6972
[2019-03-26 22:55:43,023] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 98.0, 1.0, 2.0, 0.3012956036091493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478834.0511005365, 478834.0511005365, 165522.358767957], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3036000.0000, 
sim time next is 3036600.0000, 
raw observation next is [20.5, 97.0, 1.0, 2.0, 0.3043958692060172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 483192.4815268922, 483192.4815268916, 165825.3701243311], 
processed observation next is [1.0, 0.13043478260869565, 0.1706161137440759, 0.97, 1.0, 1.0, 0.16192273398315324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13422013375747005, 0.13422013375746988, 0.24750055242437477], 
reward next is 0.7525, 
noisyNet noise sample is [array([0.1541576], dtype=float32), 0.67900205]. 
=============================================
[2019-03-26 22:55:44,374] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.539332e-32 0.000000e+00], sum to 1.0000
[2019-03-26 22:55:44,383] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3730
[2019-03-26 22:55:44,388] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 97.0, 1.0, 2.0, 0.3152768574074239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494738.7411713288, 494738.7411713288, 166542.4331262729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2961000.0000, 
sim time next is 2961600.0000, 
raw observation next is [21.0, 98.0, 1.0, 2.0, 0.3208787822147572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502021.7171716538, 502021.7171716544, 167049.1650919695], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.98, 1.0, 1.0, 0.18178166531898454, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13945047699212607, 0.13945047699212623, 0.24932711207756644], 
reward next is 0.7507, 
noisyNet noise sample is [array([-0.78839135], dtype=float32), 0.3951772]. 
=============================================
[2019-03-26 22:55:49,806] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6239865e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:49,816] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9487
[2019-03-26 22:55:49,822] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.33333333333334, 98.0, 1.0, 2.0, 0.3925455952621777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589003.3438173933, 589003.3438173933, 173623.5774825238], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3098400.0000, 
sim time next is 3099000.0000, 
raw observation next is [22.16666666666667, 99.0, 1.0, 2.0, 0.3911575441158831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587864.5402136683, 587864.5402136683, 173547.3584021127], 
processed observation next is [1.0, 0.8695652173913043, 0.24960505529225935, 0.99, 1.0, 1.0, 0.2664548724287748, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16329570561490786, 0.16329570561490786, 0.2590259080628548], 
reward next is 0.7410, 
noisyNet noise sample is [array([-0.89680314], dtype=float32), 1.5876144]. 
=============================================
[2019-03-26 22:55:49,840] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.763916]
 [79.72056 ]
 [79.6727  ]
 [79.50522 ]
 [79.5549  ]], R is [[79.74681091]
 [79.69020081]
 [79.63387299]
 [79.57770538]
 [79.52163696]].
[2019-03-26 22:55:56,448] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7469497e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:56,456] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9114
[2019-03-26 22:55:56,464] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.6057840286348392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912274.0170086146, 912274.0170086146, 210012.8193050214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3058800.0000, 
sim time next is 3059400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.6208827562282309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 934995.076949588, 934995.0769495873, 213121.615267202], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 1.0, 1.0, 1.0, 0.5432322364195552, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2597208547082189, 0.2597208547082187, 0.31809196308537613], 
reward next is 0.6819, 
noisyNet noise sample is [array([-1.1671897], dtype=float32), -0.059942428]. 
=============================================
[2019-03-26 22:55:59,851] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7706912e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 22:55:59,861] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7736
[2019-03-26 22:55:59,868] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4290735432712089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 622794.3850525762, 622794.3850525768, 176215.0042449138], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3092400.0000, 
sim time next is 3093000.0000, 
raw observation next is [23.0, 99.00000000000001, 1.0, 2.0, 0.4264394184050594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621010.880074416, 621010.8800744154, 176098.6453409253], 
processed observation next is [1.0, 0.8260869565217391, 0.28909952606635075, 0.9900000000000001, 1.0, 1.0, 0.3089631547048908, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17250302224289335, 0.17250302224289316, 0.2628337990163064], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.7882193], dtype=float32), -1.5617956]. 
=============================================
[2019-03-26 22:55:59,880] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[73.30742]
 [73.04158]
 [73.0112 ]
 [72.86682]
 [72.7061 ]], R is [[73.28075409]
 [73.28494263]
 [73.28903961]
 [73.29301453]
 [73.29626465]].
[2019-03-26 22:56:07,154] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.4690646e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 22:56:07,164] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6735
[2019-03-26 22:56:07,171] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5350653751139779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747687.9740799333, 747687.9740799333, 189272.5834599001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3362400.0000, 
sim time next is 3363000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4419344861785306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20836683766327363, 0.20836683766327346, 0.2829308381689719], 
reward next is 0.7171, 
noisyNet noise sample is [array([-1.0099294], dtype=float32), 1.3725259]. 
=============================================
[2019-03-26 22:56:07,186] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[74.519714]
 [74.437   ]
 [74.43613 ]
 [74.38864 ]
 [74.33513 ]], R is [[74.4233551 ]
 [74.39662933]
 [74.37091827]
 [74.34611511]
 [74.32200623]].
[2019-03-26 22:56:13,158] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.6205604e-19 9.9999988e-01 3.3955365e-22 6.4519064e-08 1.4863085e-26], sum to 1.0000
[2019-03-26 22:56:13,167] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6370
[2019-03-26 22:56:13,178] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2275418.540975424 W.
[2019-03-26 22:56:13,185] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.9860085023360248, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599261138858, 6.9112, 168.9123159649507, 2275418.540975424, 2208169.654966179, 459022.7387068326], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3675600.0000, 
sim time next is 3676200.0000, 
raw observation next is [33.0, 62.33333333333333, 1.0, 2.0, 0.8524055659342172, 1.0, 1.0, 0.8524055659342172, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2384046.313856725, 2384046.313856725, 446195.3118756317], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.6233333333333333, 1.0, 1.0, 0.8221753806436353, 1.0, 0.5, 0.8221753806436353, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6622350871824236, 0.6622350871824236, 0.6659631520531816], 
reward next is 0.3340, 
noisyNet noise sample is [array([0.01066224], dtype=float32), -0.5529401]. 
=============================================
[2019-03-26 22:56:13,298] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-03-26 22:56:13,301] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:56:13,301] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:56:13,313] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:56:13,313] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:56:13,314] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:56:13,315] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:56:13,315] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:56:13,315] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:56:13,326] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:56:13,337] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:56:13,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-03-26 22:56:14,159] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-03-26 22:56:14,336] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-03-26 22:56:14,347] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-03-26 22:56:14,347] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-03-26 22:56:17,637] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:56:17,642] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.43333333333333, 41.33333333333334, 1.0, 2.0, 0.4250285092645089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704172.7655168639, 704172.7655168639, 183086.7167867599]
[2019-03-26 22:56:17,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:56:17,651] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.503747e-28 0.000000e+00], sampled 0.20460578148210573
[2019-03-26 22:56:50,659] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:56:50,661] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 94.0, 1.0, 2.0, 0.3700343603149981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561406.0748820656, 561406.0748820656, 171365.3923998176]
[2019-03-26 22:56:50,662] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:56:50,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4279069e-28 0.0000000e+00], sampled 0.6691532217853969
[2019-03-26 22:56:51,428] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:56:51,430] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.0, 94.0, 1.0, 2.0, 0.458608862296744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658575.7399060403, 658575.7399060403, 179638.8240602493]
[2019-03-26 22:56:51,432] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 22:56:51,434] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 1.773694e-27 0.000000e+00], sampled 0.7152410848375175
[2019-03-26 22:57:19,753] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:57:19,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.52164577, 62.42844892, 1.0, 2.0, 0.51558345560961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720455.158422162, 720455.158422162, 186072.9718297027]
[2019-03-26 22:57:19,755] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:57:19,757] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2018505e-28 0.0000000e+00], sampled 0.27332181362730734
[2019-03-26 22:57:27,514] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:57:27,515] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.09966686, 81.91798302, 1.0, 2.0, 0.5202906665608023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727035.0727822448, 727035.0727822448, 186835.1133700585]
[2019-03-26 22:57:27,516] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 22:57:27,518] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 4.22946e-29 0.00000e+00], sampled 0.45471712963776556
[2019-03-26 22:57:40,052] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:57:40,053] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 81.66666666666667, 1.0, 2.0, 0.7507893093396907, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005978454931819, 6.9112, 168.9123160302838, 1946203.397354845, 1878964.554359162, 396041.2103300421]
[2019-03-26 22:57:40,054] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 22:57:40,059] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3228263e-23 1.0000000e+00 2.1064745e-27 4.2969888e-11 4.7283481e-33], sampled 0.2707791943624789
[2019-03-26 22:57:40,061] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1946203.397354845 W.
[2019-03-26 22:57:45,020] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.69169325]
[2019-03-26 22:57:45,021] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.52738708333333, 88.29692529833335, 1.0, 2.0, 0.5789994581155024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 809103.8273852972, 809103.8273852966, 196899.4128964205]
[2019-03-26 22:57:45,023] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:57:45,025] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2312997e-29 0.0000000e+00], sampled 0.7401779250482237
[2019-03-26 22:58:08,969] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8410 2779752196.5928 927.0000
[2019-03-26 22:58:08,998] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.4885 3163710793.5127 1762.0000
[2019-03-26 22:58:09,068] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8262.4872 2927845301.4282 1331.0000
[2019-03-26 22:58:09,168] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.3905 2843207910.2454 1128.0000
[2019-03-26 22:58:09,283] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.5010 3008128232.6441 1766.0000
[2019-03-26 22:58:10,302] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2000000, evaluation results [2000000.0, 7880.488523407698, 3163710793.5126715, 1762.0, 8262.487158330163, 2927845301.4282284, 1331.0, 8660.841007161105, 2779752196.592776, 927.0, 7998.500992642304, 3008128232.644095, 1766.0, 8497.390495604257, 2843207910.2454023, 1128.0]
[2019-03-26 22:58:23,798] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2106246e-18 9.9999869e-01 2.3498286e-22 1.3049755e-06 2.0257740e-27], sum to 1.0000
[2019-03-26 22:58:23,809] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2576
[2019-03-26 22:58:23,818] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2429616.192095084 W.
[2019-03-26 22:58:23,824] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.8686830568858953, 1.0, 2.0, 0.8686830568858953, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2429616.192095084, 2429616.192095084, 454696.4527417223], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3688800.0000, 
sim time next is 3689400.0000, 
raw observation next is [32.16666666666666, 65.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.695547310635854, 6.9112, 168.9086839793133, 2840620.010311507, 2284191.107818201, 474320.1774371144], 
processed observation next is [1.0, 0.6956521739130435, 0.7235387045813582, 0.6566666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.07843473106358542, 0.0, 0.8294189650685416, 0.7890611139754187, 0.6344975299495004, 0.7079405633389767], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2216126], dtype=float32), 1.0072817]. 
=============================================
[2019-03-26 22:58:28,986] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.999321e-34 0.000000e+00], sum to 1.0000
[2019-03-26 22:58:28,994] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7322
[2019-03-26 22:58:28,998] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.5924232812887424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827869.8199108832, 827869.8199108832, 199343.5388423999], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.5089437123960752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2299638388641342, 0.2299638388641342, 0.2975276699140297], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.37805676], dtype=float32), -0.96129024]. 
=============================================
[2019-03-26 22:58:29,336] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 22:58:29,338] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3625
[2019-03-26 22:58:29,342] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 0.6050224737384846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845483.3117925103, 845483.3117925103, 201685.7757938854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3951600.0000, 
sim time next is 3952200.0000, 
raw observation next is [34.0, 60.5, 1.0, 2.0, 0.6015151681422277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840580.1191057914, 840580.1191057914, 201028.9345735524], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.605, 1.0, 1.0, 0.519897792942443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2334944775293865, 0.2334944775293865, 0.3000431859306752], 
reward next is 0.7000, 
noisyNet noise sample is [array([1.4055347], dtype=float32), 1.6375262]. 
=============================================
[2019-03-26 22:58:32,420] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.769439e-32 0.000000e+00], sum to 1.0000
[2019-03-26 22:58:32,434] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8281
[2019-03-26 22:58:32,442] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4884502095470469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682528.0935166902, 682528.0935166902, 181805.8168284014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3715200.0000, 
sim time next is 3715800.0000, 
raw observation next is [27.0, 78.16666666666667, 1.0, 2.0, 0.4854504547419786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678335.0957684433, 678335.0957684427, 181347.2992891828], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.7816666666666667, 1.0, 1.0, 0.3800607888457574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18842641549123426, 0.1884264154912341, 0.2706676108793773], 
reward next is 0.7293, 
noisyNet noise sample is [array([0.9237826], dtype=float32), 1.0773832]. 
=============================================
[2019-03-26 22:58:33,104] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6969578e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 22:58:33,113] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7962
[2019-03-26 22:58:33,119] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4579791345538589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653507.9042580575, 653507.9042580575, 179018.0397899096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3718800.0000, 
sim time next is 3719400.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.4547525033161577, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650008.1057268956, 650008.1057268963, 178683.9311878362], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.74, 1.0, 1.0, 0.34307530520019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18055780714635988, 0.18055780714636008, 0.2666924346087107], 
reward next is 0.7333, 
noisyNet noise sample is [array([-0.44861615], dtype=float32), 0.3604527]. 
=============================================
[2019-03-26 22:58:35,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0789017e-27 0.0000000e+00 5.9175426e-27 1.0000000e+00 6.7875612e-24], sum to 1.0000
[2019-03-26 22:58:35,772] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6206
[2019-03-26 22:58:35,780] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 4028046.640464425 W.
[2019-03-26 22:58:35,784] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 69.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 8.471093248040054, 6.9112, 170.5573041426782, 4028046.640464425, 2910631.654492899, 543986.3335597762], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4109400.0000, 
sim time next is 4110000.0000, 
raw observation next is [34.0, 68.33333333333333, 1.0, 2.0, 0.9982950254975045, 1.0, 2.0, 0.8197375522630148, 1.0, 1.0, 1.03, 7.005121264238202, 6.9112, 170.5573041426782, 3440471.292736926, 3373191.670690339, 632124.1650750979], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6833333333333332, 1.0, 1.0, 0.997945813852415, 1.0, 1.0, 0.7828163280277287, 1.0, 0.5, 1.0365853658536586, 0.009392126423820191, 0.0, 0.8375144448122397, 0.9556864702047017, 0.936997686302872, 0.9434689030971611], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14798512], dtype=float32), -0.68031865]. 
=============================================
[2019-03-26 22:58:35,797] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[26.040045]
 [26.082794]
 [25.0881  ]
 [29.269943]
 [28.62125 ]], R is [[25.33442688]
 [25.0810833 ]
 [24.83027267]
 [24.58197021]
 [24.33615112]].
[2019-03-26 22:58:41,242] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8672412e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 22:58:41,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5296
[2019-03-26 22:58:41,257] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 58.0, 1.0, 2.0, 0.6138748972006653, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857859.0486237034, 857859.0486237034, 203359.9688358732], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3853800.0000, 
sim time next is 3854400.0000, 
raw observation next is [35.0, 57.33333333333333, 1.0, 2.0, 0.6093395870336394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851518.6419786054, 851518.6419786054, 202499.3785488599], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5733333333333333, 1.0, 1.0, 0.5293248036549872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23653295610516817, 0.23653295610516817, 0.3022378784311342], 
reward next is 0.6978, 
noisyNet noise sample is [array([-1.0533303], dtype=float32), 0.45005536]. 
=============================================
[2019-03-26 22:58:43,219] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4461633e-18 3.9454765e-23 1.5993277e-19 1.0000000e+00 8.0478437e-21], sum to 1.0000
[2019-03-26 22:58:43,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1113
[2019-03-26 22:58:43,233] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666666, 76.5, 1.0, 2.0, 0.8880689242181234, 1.0, 2.0, 0.8880689242181234, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2483890.337310926, 2483890.337310926, 465043.3961326481], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4265400.0000, 
sim time next is 4266000.0000, 
raw observation next is [33.0, 75.0, 1.0, 2.0, 1.032099889880493, 1.0, 2.0, 1.032099889880493, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2887204.204755269, 2887204.204755268, 548701.288547394], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.75, 1.0, 1.0, 1.038674566121076, 1.0, 1.0, 1.038674566121076, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.8020011679875747, 0.8020011679875745, 0.8189571470856627], 
reward next is 0.1810, 
noisyNet noise sample is [array([-2.0110424], dtype=float32), -0.704699]. 
=============================================
[2019-03-26 22:58:43,246] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[24.644188]
 [23.523891]
 [23.05407 ]
 [25.020004]
 [27.753334]], R is [[22.92319107]
 [22.99986649]
 [23.05749702]
 [23.02584839]
 [23.09661674]].
[2019-03-26 22:58:44,965] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0929597e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 22:58:44,975] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9726
[2019-03-26 22:58:44,979] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 93.16666666666667, 1.0, 2.0, 0.5674161613056328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792911.0869564826, 792911.0869564819, 194831.853932362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.5635043300668936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787442.641918431, 787442.6419184317, 194143.037041534], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.47410160249023325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21873406719956417, 0.21873406719956437, 0.28976572692766267], 
reward next is 0.7102, 
noisyNet noise sample is [array([-1.0533085], dtype=float32), 1.0845243]. 
=============================================
[2019-03-26 22:58:46,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1589119e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 22:58:46,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9606
[2019-03-26 22:58:46,387] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5812969940711078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812315.6714570721, 812315.6714570721, 197313.3088154392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4146600.0000, 
sim time next is 4147200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5809453277480887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811824.0583276125, 811824.058327613, 197249.7826141115], 
processed observation next is [1.0, 0.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4951148527085406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22550668286878123, 0.2255066828687814, 0.29440266061807685], 
reward next is 0.7056, 
noisyNet noise sample is [array([-0.35230312], dtype=float32), 1.989172]. 
=============================================
[2019-03-26 22:58:58,965] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.677958e-22 0.000000e+00], sum to 1.0000
[2019-03-26 22:58:58,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5715
[2019-03-26 22:58:58,976] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 1.007196723122814, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.91295651035, 1407870.745796616, 1407870.745796616, 301129.7500658007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4156800.0000, 
sim time next is 4157400.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.9698636581406972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104309, 1355652.853614371, 1355652.853614371, 289881.576540006], 
processed observation next is [1.0, 0.08695652173913043, 0.5734597156398105, 0.84, 1.0, 1.0, 0.9636911543863822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523021, 0.37657023711510307, 0.37657023711510307, 0.4326590694626955], 
reward next is 0.5673, 
noisyNet noise sample is [array([0.22001119], dtype=float32), -0.16714716]. 
=============================================
[2019-03-26 22:59:06,057] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 22:59:06,060] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 22:59:06,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:59:06,064] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 22:59:06,064] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 22:59:06,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 22:59:06,066] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:59:06,065] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 22:59:06,067] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:59:06,068] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:59:06,069] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 22:59:06,092] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-03-26 22:59:06,116] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-03-26 22:59:06,140] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-03-26 22:59:06,163] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-03-26 22:59:06,164] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-03-26 22:59:49,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.921761]
[2019-03-26 22:59:49,531] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.33333333333334, 98.0, 1.0, 2.0, 0.4701993980493784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747117.9361224582, 747117.9361224582, 189293.4138336418]
[2019-03-26 22:59:49,532] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 22:59:49,537] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5260567e-27 0.0000000e+00], sampled 0.4298482927528985
[2019-03-26 22:59:59,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.921761]
[2019-03-26 22:59:59,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.15412684, 62.51163607333334, 1.0, 2.0, 0.6372453237399002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890531.7267270143, 890531.7267270143, 207882.2613086526]
[2019-03-26 22:59:59,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 22:59:59,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3812654e-26 0.0000000e+00], sampled 0.12135875142896069
[2019-03-26 23:00:13,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.921761]
[2019-03-26 23:00:13,946] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.6, 78.33333333333333, 1.0, 2.0, 0.6521539905812702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 911375.0923430439, 911375.0923430439, 210864.6799467234]
[2019-03-26 23:00:13,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:00:13,952] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.9962076e-28 0.0000000e+00], sampled 0.9169725681901079
[2019-03-26 23:00:30,173] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.921761]
[2019-03-26 23:00:30,174] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.7, 62.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 9.821664181185202, 6.9112, 168.8959599322934, 4350159.999348397, 2285584.304083108, 467822.9965221548]
[2019-03-26 23:00:30,174] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:00:30,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7539093e-23 3.5434310e-36 8.5946011e-23 1.0000000e+00 1.5846599e-21], sampled 0.5334604560917546
[2019-03-26 23:00:30,184] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 0, 0, 1, 0] has been changed to [0, 0, 0, 0, 1] for the demand 4350159.999348397 W.
[2019-03-26 23:00:55,618] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.921761]
[2019-03-26 23:00:55,619] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 59.0, 1.0, 2.0, 0.4137290871491028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605774.6591884283, 605774.6591884283, 174739.0961470108]
[2019-03-26 23:00:55,621] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:00:55,624] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.8425164e-29 0.0000000e+00], sampled 0.03483273771064621
[2019-03-26 23:01:00,419] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8851 2779690392.4968 932.0000
[2019-03-26 23:01:00,857] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7157 2843027813.7524 1131.0000
[2019-03-26 23:01:00,865] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1751 3008171027.5419 1766.0000
[2019-03-26 23:01:00,952] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2748 2927846980.7461 1338.0000
[2019-03-26 23:01:00,957] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8871 3164216397.0078 1776.0000
[2019-03-26 23:01:01,973] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2025000, evaluation results [2025000.0, 7878.887125165954, 3164216397.007843, 1776.0, 8254.274818136657, 2927846980.7461457, 1338.0, 8660.885141065537, 2779690392.4967556, 932.0, 7998.175077088882, 3008171027.5419445, 1766.0, 8496.715729453535, 2843027813.7524204, 1131.0]
[2019-03-26 23:01:06,096] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3690713e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:06,104] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6301
[2019-03-26 23:01:06,111] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 52.5, 1.0, 2.0, 0.5339334248901505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 746105.6575136452, 746105.657513646, 189084.4687387619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4546200.0000, 
sim time next is 4546800.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5366912403143218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749960.7225235285, 749960.7225235285, 189545.519842363], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.44179667507749615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20832242292320235, 0.20832242292320235, 0.28290376095875075], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.1587367], dtype=float32), -0.42780653]. 
=============================================
[2019-03-26 23:01:14,575] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.187136e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:01:14,583] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5035
[2019-03-26 23:01:14,591] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 89.83333333333333, 1.0, 2.0, 0.9787719358586844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1368112.660358884, 1368112.660358883, 292529.9889960755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4603800.0000, 
sim time next is 4604400.0000, 
raw observation next is [29.0, 89.0, 1.0, 2.0, 1.001329001628105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1399663.372522734, 1399663.372522734, 299336.5388062986], 
processed observation next is [1.0, 0.30434782608695654, 0.5734597156398105, 0.89, 1.0, 1.0, 1.0016012067808493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.388795381256315, 0.388795381256315, 0.44677095344223666], 
reward next is 0.5532, 
noisyNet noise sample is [array([-0.29590383], dtype=float32), -1.0125847]. 
=============================================
[2019-03-26 23:01:15,240] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1443582e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:15,247] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8701
[2019-03-26 23:01:15,254] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4518132224550012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 647699.9483840843, 647699.9483840849, 178493.5296850415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4654800.0000, 
sim time next is 4655400.0000, 
raw observation next is [24.08333333333333, 94.00000000000001, 1.0, 2.0, 0.4527319079443171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648668.7607734681, 648668.7607734681, 178584.2729055045], 
processed observation next is [1.0, 0.9130434782608695, 0.34044233807266966, 0.9400000000000002, 1.0, 1.0, 0.34064085294496044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1801857668815189, 0.1801857668815189, 0.2665436909037381], 
reward next is 0.7335, 
noisyNet noise sample is [array([0.24048309], dtype=float32), -0.035046168]. 
=============================================
[2019-03-26 23:01:15,570] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.09581246e-29
 0.00000000e+00], sum to 1.0000
[2019-03-26 23:01:15,576] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0156
[2019-03-26 23:01:15,581] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.507745637686434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709499.2557531429, 709499.2557531429, 184818.0670325271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4500600.0000, 
sim time next is 4501200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5079039030453599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 709720.4820082138, 709720.4820082145, 184843.2213397491], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.4071131361992288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19714457833561494, 0.19714457833561513, 0.27588540498470016], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.6884093], dtype=float32), -0.76921815]. 
=============================================
[2019-03-26 23:01:20,017] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8023208e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:20,026] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7831
[2019-03-26 23:01:20,034] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 53.0, 1.0, 2.0, 0.5366912403143218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749960.7225235285, 749960.7225235285, 189545.519842363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4546800.0000, 
sim time next is 4547400.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.5523590424186343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 771862.5247446579, 771862.5247446572, 192206.0057901137], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.4606735450826919, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21440625687351608, 0.2144062568735159, 0.28687463550763237], 
reward next is 0.7131, 
noisyNet noise sample is [array([-1.4026397], dtype=float32), 0.7616288]. 
=============================================
[2019-03-26 23:01:22,895] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.9173343e-21 1.0000000e+00 2.0619008e-24 2.4854956e-09 3.2679742e-29], sum to 1.0000
[2019-03-26 23:01:22,903] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2325
[2019-03-26 23:01:22,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2203349.957728705 W.
[2019-03-26 23:01:22,917] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.66666666666667, 68.33333333333334, 1.0, 2.0, 0.5252371646917522, 1.0, 2.0, 0.5252371646917522, 1.0, 1.0, 0.91216325648494, 6.9112, 6.9112, 170.5573041426782, 2203349.957728705, 2203349.957728705, 433038.3034090598], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4623600.0000, 
sim time next is 4624200.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.8714256681598106, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.00598594625265, 6.9112, 168.9123931726456, 2115041.103040164, 2047796.914753355, 426260.9964611707], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.8450911664576031, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009478594625265036, 0.0, 0.8294371789058792, 0.5875114175111567, 0.5688324763203764, 0.6362104424793593], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1279489], dtype=float32), -0.22761041]. 
=============================================
[2019-03-26 23:01:23,809] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3146582e-28 1.0000000e+00 4.4346935e-33 2.6705109e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:23,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5287
[2019-03-26 23:01:23,829] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1707105.888414596 W.
[2019-03-26 23:01:23,832] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.33333333333334, 65.0, 1.0, 2.0, 0.6105541325306986, 1.0, 2.0, 0.6105541325306986, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1707105.888414596, 1707105.888414596, 338184.5945905052], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4796400.0000, 
sim time next is 4797000.0000, 
raw observation next is [31.5, 64.5, 1.0, 2.0, 0.3811311809600543, 1.0, 2.0, 0.3811311809600543, 1.0, 1.0, 0.6598833546619115, 6.9112, 6.9112, 170.5573041426782, 1598379.905626517, 1598379.905626517, 342424.3896489601], 
processed observation next is [1.0, 0.5217391304347826, 0.6919431279620853, 0.645, 1.0, 1.0, 0.25437491681934254, 1.0, 1.0, 0.25437491681934254, 1.0, 0.5, 0.5852236032462335, 0.0, 0.0, 0.8375144448122397, 0.4439944182295881, 0.4439944182295881, 0.5110811785805375], 
reward next is 0.4889, 
noisyNet noise sample is [array([0.8524791], dtype=float32), 1.3103225]. 
=============================================
[2019-03-26 23:01:23,843] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[37.499554]
 [31.768263]
 [30.884275]
 [30.829023]
 [30.73705 ]], R is [[35.54959106]
 [35.6893425 ]
 [35.71863937]
 [35.70503235]
 [35.3479805 ]].
[2019-03-26 23:01:26,305] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.6709281e-16 9.8700935e-01 4.2991231e-19 1.2990720e-02 1.8200750e-23], sum to 1.0000
[2019-03-26 23:01:26,314] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3143
[2019-03-26 23:01:26,323] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1772013.707378828 W.
[2019-03-26 23:01:26,327] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4224996857489516, 1.0, 2.0, 0.4224996857489516, 1.0, 2.0, 0.7303818734438439, 6.911199999999999, 6.9112, 170.5573041426782, 1772013.707378828, 1772013.707378829, 365193.1525628199], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4884600.0000, 
sim time next is 4885200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4587244869324448, 1.0, 2.0, 0.4587244869324448, 1.0, 2.0, 0.793419266771219, 6.9112, 6.9112, 170.5573041426782, 1924081.227449416, 1924081.227449416, 387247.9457274922], 
processed observation next is [1.0, 0.5652173913043478, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3478608276294516, 1.0, 1.0, 0.3478608276294516, 1.0, 1.0, 0.7480722765502671, 0.0, 0.0, 0.8375144448122397, 0.5344670076248378, 0.5344670076248378, 0.5779820085484958], 
reward next is 0.4220, 
noisyNet noise sample is [array([-1.0947084], dtype=float32), -0.7877565]. 
=============================================
[2019-03-26 23:01:27,739] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4691369e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:27,747] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9720
[2019-03-26 23:01:27,752] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5159538375002802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 720972.8905128329, 720972.8905128335, 186134.0273546086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5050800.0000, 
sim time next is 5051400.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5169274923730585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722333.8987893036, 722333.8987893036, 186291.2622685379], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.41798493056994995, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.200648305219251, 0.200648305219251, 0.27804666010229534], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.8181613], dtype=float32), -0.69535345]. 
=============================================
[2019-03-26 23:01:33,758] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9952395e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 23:01:33,766] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8675
[2019-03-26 23:01:33,771] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5086982487952844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710830.8339312291, 710830.8339312284, 184970.2242233995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4927200.0000, 
sim time next is 4927800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5091368395051649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711443.9050444425, 711443.9050444419, 185040.0746899869], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4085986018134516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976233069567896, 0.19762330695678942, 0.27617921595520434], 
reward next is 0.7238, 
noisyNet noise sample is [array([0.19818655], dtype=float32), 0.06573415]. 
=============================================
[2019-03-26 23:01:33,812] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1933314e-16 2.3574469e-07 2.1468988e-19 9.9999976e-01 6.1514545e-22], sum to 1.0000
[2019-03-26 23:01:33,821] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8695
[2019-03-26 23:01:33,826] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.7609561404668769, 1.0, 2.0, 0.7609561404668769, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2128046.677674421, 2128046.677674422, 401307.3508656398], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873200.0000, 
sim time next is 4873800.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.761434826985342, 1.0, 2.0, 0.761434826985342, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2129386.675729333, 2129386.675729333, 401529.7827027079], 
processed observation next is [1.0, 0.391304347826087, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.7125720807052314, 1.0, 1.0, 0.7125720807052314, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5914962988137036, 0.5914962988137036, 0.59929818313837], 
reward next is 0.4007, 
noisyNet noise sample is [array([0.7737418], dtype=float32), 0.64842427]. 
=============================================
[2019-03-26 23:01:38,298] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1397121e-18 9.9999547e-01 6.0258936e-22 4.5523352e-06 1.5343073e-26], sum to 1.0000
[2019-03-26 23:01:38,309] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3915
[2019-03-26 23:01:38,319] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2195661.304943267 W.
[2019-03-26 23:01:38,324] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.76666666666667, 63.16666666666666, 1.0, 2.0, 0.9290233606718896, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.979305316303464, 6.9112, 168.9125510308072, 2195661.304943267, 2147345.165537839, 443173.1777230577], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4978200.0000, 
sim time next is 4978800.0000, 
raw observation next is [30.8, 63.0, 1.0, 2.0, 0.7994750978048557, 1.0, 1.0, 0.7994750978048557, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2235875.565225339, 2235875.565225339, 419608.6996047688], 
processed observation next is [1.0, 0.6521739130434783, 0.6587677725118484, 0.63, 1.0, 1.0, 0.7584037322950069, 1.0, 0.5, 0.7584037322950069, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6210765458959275, 0.6210765458959275, 0.6262816412011474], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6106502], dtype=float32), 0.22071554]. 
=============================================
[2019-03-26 23:01:50,028] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:01:50,037] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2452
[2019-03-26 23:01:50,043] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5308752258061428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741830.7118224282, 741830.7118224282, 188575.3002265263], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5076000.0000, 
sim time next is 5076600.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.5286455297675907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738713.9113636625, 738713.9113636625, 188206.3161386328], 
processed observation next is [0.0, 0.782608695652174, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.4321030479127599, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20519830871212846, 0.20519830871212846, 0.280904949460646], 
reward next is 0.7191, 
noisyNet noise sample is [array([-0.15206416], dtype=float32), 0.27857026]. 
=============================================
[2019-03-26 23:01:57,709] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 23:01:57,711] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:01:57,713] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:01:57,714] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:01:57,714] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:01:57,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:01:57,717] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:01:57,718] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:01:57,719] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:01:57,721] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:01:57,722] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:01:57,744] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-03-26 23:01:57,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-03-26 23:01:57,794] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-03-26 23:01:57,795] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-03-26 23:01:57,842] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-03-26 23:02:47,016] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:02:47,017] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.39284566333333, 90.18993788, 1.0, 2.0, 0.5808181714640378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811646.2998248974, 811646.2998248974, 197224.9741532488]
[2019-03-26 23:02:47,019] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:02:47,022] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.071754e-26 0.000000e+00], sampled 0.14193941968460644
[2019-03-26 23:02:54,637] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:02:54,638] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.56666666666667, 70.33333333333334, 1.0, 2.0, 0.5324311992088461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744005.7485142675, 744005.7485142675, 188832.6622188912]
[2019-03-26 23:02:54,639] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:02:54,641] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8977274e-27 0.0000000e+00], sampled 0.6492198574154738
[2019-03-26 23:02:57,313] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:02:57,314] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.08333333333334, 74.5, 1.0, 2.0, 0.5690752939659715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 795230.4377850593, 795230.4377850586, 195125.8053285497]
[2019-03-26 23:02:57,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:02:57,318] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.457964e-28 0.000000e+00], sampled 0.18854573282305576
[2019-03-26 23:03:23,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:03:23,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.6, 91.0, 1.0, 2.0, 0.5099978713708332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712647.4741212225, 712647.4741212225, 185176.4405137003]
[2019-03-26 23:03:23,848] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:03:23,853] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0201192e-28 0.0000000e+00], sampled 0.7579701675319025
[2019-03-26 23:03:27,919] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:03:27,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.76666666666667, 77.33333333333333, 1.0, 2.0, 0.5475340926732652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 765117.746612348, 765117.7466123486, 191377.9076500839]
[2019-03-26 23:03:27,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:03:27,926] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 5.113688e-28 0.000000e+00], sampled 0.2586885604274419
[2019-03-26 23:03:42,581] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:03:42,582] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.08333333333334, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 11.82995482021193, 6.9112, 172.7326615868083, 5024303.511885328, 1455859.142375098, 299822.0807267075]
[2019-03-26 23:03:42,583] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:03:42,587] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9665249e-18 1.0000000e+00 2.3692206e-21 4.1108770e-09 2.4522855e-26], sampled 0.8060044999715279
[2019-03-26 23:03:42,590] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 5024303.511885328 W.
[2019-03-26 23:03:51,471] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1141969]
[2019-03-26 23:03:51,473] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.4, 61.0, 1.0, 2.0, 0.304638308605519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485223.0393143223, 485223.039314323, 165999.0006009082]
[2019-03-26 23:03:51,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:03:51,479] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7817495e-28 0.0000000e+00], sampled 0.5049589082511141
[2019-03-26 23:03:51,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7893.4074 3163449145.3654 1744.0000
[2019-03-26 23:03:52,347] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.5371 2779484207.9225 925.0000
[2019-03-26 23:03:52,400] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8260.3638 2927736054.4781 1333.0000
[2019-03-26 23:03:52,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.2049 2843092853.5063 1130.0000
[2019-03-26 23:03:52,560] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.9726 3007943833.8963 1763.0000
[2019-03-26 23:03:53,578] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2050000, evaluation results [2050000.0, 7893.4074033810175, 3163449145.3653994, 1744.0, 8260.363770447639, 2927736054.4781265, 1333.0, 8662.537146881557, 2779484207.9224505, 925.0, 7998.972596723362, 3007943833.896321, 1763.0, 8498.204886615578, 2843092853.5063224, 1130.0]
[2019-03-26 23:04:08,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.775456e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:04:08,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6045
[2019-03-26 23:04:08,416] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 92.0, 1.0, 2.0, 0.9402653770175735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1314255.456720984, 1314255.456720984, 281256.94440863], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5457000.0000, 
sim time next is 5457600.0000, 
raw observation next is [27.7, 92.0, 1.0, 2.0, 0.9211757236426917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287556.723439644, 1287556.723439643, 275832.0907661408], 
processed observation next is [1.0, 0.17391304347826086, 0.5118483412322274, 0.92, 1.0, 1.0, 0.9050309923405925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35765464539990116, 0.35765464539990083, 0.4116896877106579], 
reward next is 0.5883, 
noisyNet noise sample is [array([-0.5502205], dtype=float32), -0.2984261]. 
=============================================
[2019-03-26 23:04:08,455] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2189054e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 23:04:08,467] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0841
[2019-03-26 23:04:08,472] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.4, 56.66666666666667, 1.0, 2.0, 0.5211209160873153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728195.6306460764, 728195.6306460764, 186971.8663684944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5744400.0000, 
sim time next is 5745000.0000, 
raw observation next is [32.59999999999999, 55.83333333333334, 1.0, 2.0, 0.5210302913278715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728068.9514670399, 728068.9514670399, 186957.141096288], 
processed observation next is [0.0, 0.4782608695652174, 0.7440758293838856, 0.5583333333333335, 1.0, 1.0, 0.422928061840809, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20224137540751108, 0.20224137540751108, 0.2790405090989373], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.14281258], dtype=float32), -1.1818936]. 
=============================================
[2019-03-26 23:04:08,485] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[81.039856]
 [80.911644]
 [80.78476 ]
 [80.64372 ]
 [80.67907 ]], R is [[81.05935669]
 [80.96970367]
 [80.88083649]
 [80.79257965]
 [80.70444489]].
[2019-03-26 23:04:09,605] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.947334e-32 0.000000e+00], sum to 1.0000
[2019-03-26 23:04:09,613] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1158
[2019-03-26 23:04:09,618] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.06666666666666, 63.83333333333334, 1.0, 2.0, 0.5572636009613726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778718.6315937896, 778718.6315937903, 193054.878127043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5763000.0000, 
sim time next is 5763600.0000, 
raw observation next is [31.9, 65.0, 1.0, 2.0, 0.5598090003584952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782276.8755501541, 782276.8755501541, 193497.828176362], 
processed observation next is [0.0, 0.7391304347826086, 0.7109004739336492, 0.65, 1.0, 1.0, 0.4696493980222834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21729913209726504, 0.21729913209726504, 0.2888027286214358], 
reward next is 0.7112, 
noisyNet noise sample is [array([1.0084289], dtype=float32), -0.9082533]. 
=============================================
[2019-03-26 23:04:19,197] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2296228e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:04:19,205] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1737
[2019-03-26 23:04:19,208] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.43333333333333, 86.83333333333333, 1.0, 2.0, 0.5034418228043765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703483.3211220966, 703483.3211220972, 184137.1717802832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5637000.0000, 
sim time next is 5637600.0000, 
raw observation next is [26.6, 86.0, 1.0, 2.0, 0.5047848553754679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705360.6277993605, 705360.6277993611, 184349.2608425079], 
processed observation next is [0.0, 0.2608695652173913, 0.4597156398104266, 0.86, 1.0, 1.0, 0.40335524744032275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1959335077220446, 0.19593350772204476, 0.27514815051120584], 
reward next is 0.7249, 
noisyNet noise sample is [array([1.408137], dtype=float32), 0.3647677]. 
=============================================
[2019-03-26 23:04:25,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.40134130e-21 2.71074743e-12 2.41847201e-24 1.00000000e+00
 1.00061155e-28], sum to 1.0000
[2019-03-26 23:04:25,211] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8921
[2019-03-26 23:04:25,216] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.7, 62.33333333333334, 1.0, 2.0, 0.9934446574096452, 1.0, 2.0, 0.9934446574096452, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2778949.4801711, 2778949.4801711, 525035.1793760505], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5836800.0000, 
sim time next is 5837400.0000, 
raw observation next is [32.75, 62.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 6.913023401920479, 6.9112, 170.5573041426782, 2910637.468361725, 2909331.291394838, 553615.5063894382], 
processed observation next is [1.0, 0.5652173913043478, 0.7511848341232228, 0.62, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.00018234019204790285, 0.0, 0.8375144448122397, 0.808510407878257, 0.8081475809430106, 0.826291800581251], 
reward next is 0.1646, 
noisyNet noise sample is [array([-1.0810125], dtype=float32), 0.34305805]. 
=============================================
[2019-03-26 23:04:32,091] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4903397e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:04:32,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0778
[2019-03-26 23:04:32,102] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.18333333333333, 93.0, 1.0, 2.0, 0.7186882424336557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1004399.64630749, 1004399.64630749, 224930.2274513614], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6059400.0000, 
sim time next is 6060000.0000, 
raw observation next is [26.16666666666667, 93.0, 1.0, 2.0, 0.6952603385547894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 971643.1022969646, 971643.1022969646, 219824.931110276], 
processed observation next is [1.0, 0.13043478260869565, 0.4391785150078992, 0.93, 1.0, 1.0, 0.6328437813913125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26990086174915684, 0.26990086174915684, 0.32809691210488956], 
reward next is 0.6719, 
noisyNet noise sample is [array([-0.6401129], dtype=float32), -0.94997245]. 
=============================================
[2019-03-26 23:04:32,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[65.33693 ]
 [64.95308 ]
 [65.205795]
 [64.791794]
 [63.821285]], R is [[65.09642029]
 [65.10974121]
 [65.10923004]
 [65.10774994]
 [65.0899353 ]].
[2019-03-26 23:04:32,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.5200015e-21 9.9786770e-01 3.2931636e-26 2.1322779e-03 5.6725690e-32], sum to 1.0000
[2019-03-26 23:04:33,002] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3847
[2019-03-26 23:04:33,011] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2181105.515115284 W.
[2019-03-26 23:04:33,017] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.86666666666667, 75.83333333333333, 1.0, 2.0, 0.7799098523255277, 1.0, 2.0, 0.7799098523255277, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2181105.515115284, 2181105.515115284, 410211.6514113431], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5928600.0000, 
sim time next is 5929200.0000, 
raw observation next is [29.9, 76.0, 1.0, 2.0, 0.9812507486086827, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.00386221184684, 6.9112, 168.9124054391703, 2268759.321894529, 2203021.774089801, 457660.7406986263], 
processed observation next is [1.0, 0.6521739130434783, 0.6161137440758293, 0.76, 1.0, 1.0, 0.9774105404923887, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.009266221184684032, 0.0, 0.8294372391401311, 0.6302109227484802, 0.6119504928027225, 0.6830757323860094], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0812823], dtype=float32), -0.72878206]. 
=============================================
[2019-03-26 23:04:39,723] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1827204e-28 1.0000000e+00 1.7872453e-33 8.0522739e-19 0.0000000e+00], sum to 1.0000
[2019-03-26 23:04:39,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6350
[2019-03-26 23:04:39,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2224631.927054792 W.
[2019-03-26 23:04:39,746] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.63333333333334, 66.83333333333333, 1.0, 2.0, 0.9497227853321286, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988677852761832, 6.9112, 168.9124954913223, 2224631.927054792, 2169666.621742489, 448860.0996014317], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6105000.0000, 
sim time next is 6105600.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.5251409035204454, 1.0, 1.0, 0.5251409035204454, 1.0, 2.0, 0.9063236649289839, 6.911199999999999, 6.9112, 170.5573041426782, 2202945.730512795, 2202945.730512796, 431884.2188522732], 
processed observation next is [1.0, 0.6956521739130435, 0.6492890995260664, 0.67, 1.0, 1.0, 0.42788060665113903, 1.0, 0.5, 0.42788060665113903, 1.0, 1.0, 0.8857605669865657, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6119293695868875, 0.6119293695868878, 0.6446033117198108], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.00483836], dtype=float32), -0.10334681]. 
=============================================
[2019-03-26 23:04:40,697] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:04:40,705] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0009
[2019-03-26 23:04:40,710] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.31666666666666, 89.16666666666667, 1.0, 2.0, 0.6866506204013002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959605.3773132082, 959605.3773132089, 217991.2683076609], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6159000.0000, 
sim time next is 6159600.0000, 
raw observation next is [27.4, 89.0, 1.0, 2.0, 0.6881651335305607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 961722.8923189238, 961722.8923189233, 218312.6852338259], 
processed observation next is [1.0, 0.30434782608695654, 0.4976303317535545, 0.89, 1.0, 1.0, 0.6242953416030852, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26714524786636773, 0.26714524786636756, 0.32583982870720285], 
reward next is 0.6742, 
noisyNet noise sample is [array([0.04834886], dtype=float32), -0.46343425]. 
=============================================
[2019-03-26 23:04:46,997] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.6301977e-12 9.7220314e-01 1.3489578e-14 2.7796833e-02 9.3990324e-18], sum to 1.0000
[2019-03-26 23:04:47,005] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5159
[2019-03-26 23:04:47,016] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2293500.564112303 W.
[2019-03-26 23:04:47,022] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.8, 66.0, 1.0, 2.0, 0.5467073338894204, 1.0, 1.0, 0.5467073338894204, 1.0, 2.0, 0.9441954120988322, 6.911200000000001, 6.9112, 170.5573041426782, 2293500.564112303, 2293500.564112302, 447899.5874842076], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6102000.0000, 
sim time next is 6102600.0000, 
raw observation next is [30.76666666666667, 66.16666666666667, 1.0, 2.0, 0.689975716133261, 1.0, 2.0, 0.689975716133261, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1929368.111225646, 1929368.111225646, 369861.4213768077], 
processed observation next is [1.0, 0.6521739130434783, 0.6571879936808849, 0.6616666666666667, 1.0, 1.0, 0.6264767664256157, 1.0, 1.0, 0.6264767664256157, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5359355864515684, 0.5359355864515684, 0.5520319722041905], 
reward next is 0.4480, 
noisyNet noise sample is [array([0.19783492], dtype=float32), 0.3221099]. 
=============================================
[2019-03-26 23:04:49,039] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.934523e-38 0.000000e+00], sum to 1.0000
[2019-03-26 23:04:49,048] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7090
[2019-03-26 23:04:49,054] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 89.83333333333333, 1.0, 2.0, 0.5360785903220822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749104.3168114576, 749104.3168114576, 189442.0911609878], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6137400.0000, 
sim time next is 6138000.0000, 
raw observation next is [26.9, 90.0, 1.0, 2.0, 0.5359879313480116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748977.5873112571, 748977.5873112571, 189426.8981917034], 
processed observation next is [1.0, 0.043478260869565216, 0.4739336492890995, 0.9, 1.0, 1.0, 0.44094931487712236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20804932980868254, 0.20804932980868254, 0.2827267137189603], 
reward next is 0.7173, 
noisyNet noise sample is [array([-2.1616066], dtype=float32), 0.1311803]. 
=============================================
[2019-03-26 23:04:49,072] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[64.9208  ]
 [65.02714 ]
 [65.104195]
 [65.1941  ]
 [65.190384]], R is [[64.92517853]
 [64.99317169]
 [65.06044769]
 [65.12696838]
 [65.19273376]].
[2019-03-26 23:04:49,487] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 23:04:49,488] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:04:49,488] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:04:49,490] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:04:49,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:04:49,492] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:04:49,491] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:04:49,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:04:49,494] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:04:49,496] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:04:49,495] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:04:49,531] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-03-26 23:04:49,553] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-03-26 23:04:49,579] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-03-26 23:04:49,580] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-03-26 23:04:49,599] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-03-26 23:05:34,038] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:05:34,040] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.352294785, 90.03805788, 1.0, 2.0, 0.5279130048236021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790712.2921783733, 790712.2921783733, 194635.805972822]
[2019-03-26 23:05:34,043] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:05:34,046] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6889424042499024
[2019-03-26 23:05:46,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:05:46,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.88408487, 95.524249955, 1.0, 2.0, 0.8571843704993869, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005985056661358, 6.9112, 168.9123931558965, 2095108.167342404, 2027864.610166838, 422458.1145990779]
[2019-03-26 23:05:46,757] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:05:46,760] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1161947e-28 1.0000000e+00 2.3181771e-32 3.1756585e-20 0.0000000e+00], sampled 0.01232537718202198
[2019-03-26 23:05:46,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2095108.167342404 W.
[2019-03-26 23:05:50,134] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:05:50,135] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6216299483060438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868700.7713449328, 868700.7713449328, 204845.1600819521]
[2019-03-26 23:05:50,136] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:05:50,138] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.5633812084258035
[2019-03-26 23:05:54,053] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:05:54,054] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.7835879753519386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1095146.756755355, 1095146.756755355, 239918.6676532144]
[2019-03-26 23:05:54,055] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:05:54,057] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.030752201484236963
[2019-03-26 23:05:56,937] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:05:56,939] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.00011803666667, 76.15922095333333, 1.0, 2.0, 0.6228503762682469, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870406.9650950729, 870406.9650950729, 205071.7658927401]
[2019-03-26 23:05:56,941] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:05:56,944] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.09427359380748679
[2019-03-26 23:06:09,180] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:09,304] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.53333333333333, 57.0, 1.0, 2.0, 0.5950481530887788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831539.3292738762, 831539.3292738762, 199827.2195000483]
[2019-03-26 23:06:09,305] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:06:09,308] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8824078946427941
[2019-03-26 23:06:10,364] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:10,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 81.83333333333333, 1.0, 2.0, 0.9218570103631246, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.9129563643707, 1288509.557498151, 1288509.55749815, 276023.264586437]
[2019-03-26 23:06:10,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:06:10,367] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.077994e-36 0.000000e+00], sampled 0.94379042927837
[2019-03-26 23:06:16,063] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:16,064] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.1, 68.5, 1.0, 2.0, 0.5325866243670174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744223.0117412278, 744223.0117412278, 188858.9975813133]
[2019-03-26 23:06:16,065] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:06:16,070] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.1554827592485548
[2019-03-26 23:06:22,476] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:22,479] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.96666666666667, 69.66666666666667, 1.0, 2.0, 0.3764670437434964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570601.8994316269, 570601.8994316269, 172149.2990639542]
[2019-03-26 23:06:22,480] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:06:22,482] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.24421099833648074
[2019-03-26 23:06:22,812] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:22,813] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 84.66666666666667, 1.0, 2.0, 0.3306257402689252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525818.6063668791, 525818.6063668791, 169039.1116392021]
[2019-03-26 23:06:22,814] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:06:22,817] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6250371950062904
[2019-03-26 23:06:33,845] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1346339]
[2019-03-26 23:06:33,846] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.08116889333333, 93.95465866833334, 1.0, 2.0, 0.5307321556678909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741630.7196813175, 741630.7196813168, 188551.0661726234]
[2019-03-26 23:06:33,850] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:06:33,853] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.15901037530311513
[2019-03-26 23:06:40,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7157 2843027813.7524 1131.0000
[2019-03-26 23:06:40,282] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.4273 3164465094.6898 1776.0000
[2019-03-26 23:06:40,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.1751 3008171027.5419 1766.0000
[2019-03-26 23:06:40,978] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.5671 2779727878.6853 933.0000
[2019-03-26 23:06:41,026] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.2748 2927846980.7461 1338.0000
[2019-03-26 23:06:42,042] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2075000, evaluation results [2075000.0, 7879.427262013278, 3164465094.689829, 1776.0, 8254.274818136657, 2927846980.7461457, 1338.0, 8660.567057309232, 2779727878.685327, 933.0, 7998.175077088882, 3008171027.5419445, 1766.0, 8496.715729453535, 2843027813.7524204, 1131.0]
[2019-03-26 23:06:42,488] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:06:42,499] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3162
[2019-03-26 23:06:42,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 88.33333333333334, 1.0, 2.0, 0.5217189503055122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729031.5889271157, 729031.5889271162, 187069.1724624268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6218400.0000, 
sim time next is 6219000.0000, 
raw observation next is [26.75, 88.5, 1.0, 2.0, 0.5218063898653075, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729153.8158078616, 729153.8158078616, 187083.4496020554], 
processed observation next is [1.0, 1.0, 0.4668246445497631, 0.885, 1.0, 1.0, 0.4238631203196475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2025427266132949, 0.2025427266132949, 0.2792290292567991], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.10535301], dtype=float32), -1.9247282]. 
=============================================
[2019-03-26 23:06:42,518] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[71.39038 ]
 [71.38658 ]
 [71.430885]
 [71.2867  ]
 [71.26553 ]], R is [[71.41977692]
 [71.42637634]
 [71.43299103]
 [71.43956757]
 [71.445961  ]].
[2019-03-26 23:06:45,508] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:06:45,516] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0718
[2019-03-26 23:06:45,521] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.71666666666667, 66.5, 1.0, 2.0, 0.5334403555462484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745416.4125799548, 745416.4125799541, 189001.6663214388], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6264600.0000, 
sim time next is 6265200.0000, 
raw observation next is [30.73333333333333, 66.0, 1.0, 2.0, 0.5312558886775424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742362.8257202522, 742362.8257202522, 188638.4128364908], 
processed observation next is [0.0, 0.5217391304347826, 0.6556082148499209, 0.66, 1.0, 1.0, 0.4352480586476414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2062118960334034, 0.2062118960334034, 0.28154986990521014], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.94524693], dtype=float32), -0.12211014]. 
=============================================
[2019-03-26 23:06:49,069] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:06:49,079] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2876
[2019-03-26 23:06:49,085] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.63333333333333, 68.33333333333334, 1.0, 2.0, 0.5423444730103086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757863.2456525544, 757863.2456525537, 190496.8040392407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6261600.0000, 
sim time next is 6262200.0000, 
raw observation next is [30.65, 68.0, 1.0, 2.0, 0.5401834855089805, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754842.4443038256, 754842.444303825, 190131.8280194779], 
processed observation next is [0.0, 0.4782608695652174, 0.6516587677725119, 0.68, 1.0, 1.0, 0.44600419940841024, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20967845675106267, 0.2096784567510625, 0.2837788477902655], 
reward next is 0.7162, 
noisyNet noise sample is [array([-2.0430686], dtype=float32), -0.03663808]. 
=============================================
[2019-03-26 23:06:53,087] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4611928e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:06:53,100] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3933
[2019-03-26 23:06:53,109] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.35, 85.0, 1.0, 2.0, 0.5270079968073125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736424.87716059, 736424.8771605907, 187935.9629441486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6474600.0000, 
sim time next is 6475200.0000, 
raw observation next is [27.3, 85.66666666666667, 1.0, 2.0, 0.5281113209620603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737967.1641867617, 737967.1641867611, 188117.8992521968], 
processed observation next is [1.0, 0.9565217391304348, 0.4928909952606636, 0.8566666666666667, 1.0, 1.0, 0.4314594228458557, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20499087894076715, 0.20499087894076698, 0.2807729839585027], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.04314804], dtype=float32), -2.0332048]. 
=============================================
[2019-03-26 23:06:56,130] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8524145e-20 9.9993336e-01 9.3192329e-23 6.6668566e-05 3.3620592e-29], sum to 1.0000
[2019-03-26 23:06:56,140] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9550
[2019-03-26 23:06:56,146] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2120897.343529826 W.
[2019-03-26 23:06:56,151] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 67.0, 1.0, 2.0, 0.8756096715797939, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.977167785191011, 6.9112, 168.9125099159008, 2120897.343529826, 2074097.650014957, 428387.3752284585], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6451800.0000, 
sim time next is 6452400.0000, 
raw observation next is [30.0, 67.0, 1.0, 2.0, 0.7620534915639534, 1.0, 1.0, 0.7620534915639534, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2131118.520033865, 2131118.520033866, 401813.936759761], 
processed observation next is [1.0, 0.6956521739130435, 0.6208530805687204, 0.67, 1.0, 1.0, 0.7133174597156066, 1.0, 0.5, 0.7133174597156066, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5919773666760736, 0.5919773666760739, 0.599722293671285], 
reward next is 0.4003, 
noisyNet noise sample is [array([-0.9653632], dtype=float32), -0.27212092]. 
=============================================
[2019-03-26 23:06:56,314] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.859262e-38 0.000000e+00], sum to 1.0000
[2019-03-26 23:06:56,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6450
[2019-03-26 23:06:56,325] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 90.0, 1.0, 2.0, 0.6755937162824901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 944146.3058865666, 944146.3058865673, 215663.5895416749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6588000.0000, 
sim time next is 6588600.0000, 
raw observation next is [26.18333333333334, 89.66666666666667, 1.0, 2.0, 0.6752231728762547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943628.2391805528, 943628.2391805528, 215586.3919657916], 
processed observation next is [1.0, 0.2608695652173913, 0.4399684044233811, 0.8966666666666667, 1.0, 1.0, 0.6087026179231984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26211895532793134, 0.26211895532793134, 0.3217707342773009], 
reward next is 0.6782, 
noisyNet noise sample is [array([0.1364427], dtype=float32), 1.2766949]. 
=============================================
[2019-03-26 23:06:58,102] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9255438e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:06:58,110] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4651
[2019-03-26 23:06:58,116] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 87.16666666666667, 1.0, 2.0, 0.5316882047698023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742967.1439800152, 742967.1439800158, 188709.9675605736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6477000.0000, 
sim time next is 6477600.0000, 
raw observation next is [27.1, 87.33333333333334, 1.0, 2.0, 0.5325411868914179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 744159.4963270944, 744159.4963270938, 188851.6051236656], 
processed observation next is [1.0, 1.0, 0.4834123222748816, 0.8733333333333334, 1.0, 1.0, 0.4367966107125517, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20671097120197066, 0.2067109712019705, 0.28186806734875464], 
reward next is 0.7181, 
noisyNet noise sample is [array([0.893647], dtype=float32), -0.18892074]. 
=============================================
[2019-03-26 23:06:59,038] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.649555e-32 0.000000e+00], sum to 1.0000
[2019-03-26 23:06:59,048] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6040
[2019-03-26 23:06:59,052] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 88.0, 1.0, 2.0, 0.6854201674262496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957885.0238343929, 957885.0238343923, 217728.3920143394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6591600.0000, 
sim time next is 6592200.0000, 
raw observation next is [26.66666666666667, 87.50000000000001, 1.0, 2.0, 0.658464865065084, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 920198.2623011803, 920198.262301181, 212134.1025921805], 
processed observation next is [1.0, 0.30434782608695654, 0.4628751974723541, 0.8750000000000001, 1.0, 1.0, 0.5885118856205831, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2556106284169945, 0.2556106284169947, 0.31661806357041866], 
reward next is 0.6834, 
noisyNet noise sample is [array([0.34938273], dtype=float32), 0.4812429]. 
=============================================
[2019-03-26 23:07:00,275] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:07:00,285] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6990
[2019-03-26 23:07:00,290] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.3, 70.0, 1.0, 2.0, 0.5023734663010554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 701989.9622427048, 701989.9622427048, 183969.324635408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6458400.0000, 
sim time next is 6459000.0000, 
raw observation next is [29.2, 70.5, 1.0, 2.0, 0.5075011301951279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709157.4787359153, 709157.4787359147, 184779.8942074119], 
processed observation next is [1.0, 0.782608695652174, 0.5829383886255924, 0.705, 1.0, 1.0, 0.40662786770497333, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19698818853775424, 0.19698818853775407, 0.2757908868767342], 
reward next is 0.7242, 
noisyNet noise sample is [array([-1.2376343], dtype=float32), -0.9647986]. 
=============================================
[2019-03-26 23:07:00,304] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[73.49863 ]
 [71.14656 ]
 [69.45885 ]
 [66.06284 ]
 [59.926113]], R is [[73.34350586]
 [73.33548737]
 [73.33010101]
 [73.32750702]
 [73.32700348]].
[2019-03-26 23:07:07,798] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5129590e-34 1.0000000e+00 0.0000000e+00 8.6531453e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:07:07,808] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3470
[2019-03-26 23:07:07,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1918958.848306214 W.
[2019-03-26 23:07:07,820] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 61.33333333333334, 1.0, 2.0, 0.6862565127235332, 1.0, 1.0, 0.6862565127235332, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1918958.848306214, 1918958.848306214, 368285.4078142087], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6702000.0000, 
sim time next is 6702600.0000, 
raw observation next is [30.15, 60.66666666666666, 1.0, 2.0, 0.7412668380413378, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.962594799242521, 6.9112, 168.9126501335574, 1932877.277133275, 1896416.103812102, 395226.2973299751], 
processed observation next is [1.0, 0.5652173913043478, 0.6279620853080569, 0.6066666666666666, 1.0, 1.0, 0.6882732988449852, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.005139479924252122, 0.0, 0.8294384407015933, 0.5369103547592431, 0.5267822510589172, 0.5898899960148882], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.8841684], dtype=float32), 1.4716308]. 
=============================================
[2019-03-26 23:07:10,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:07:10,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1372
[2019-03-26 23:07:10,432] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.85, 81.0, 1.0, 2.0, 0.3253539464378399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 512050.2102336524, 512050.2102336518, 167889.4562810859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6744600.0000, 
sim time next is 6745200.0000, 
raw observation next is [22.7, 81.66666666666666, 1.0, 2.0, 0.3241838804815212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 510900.7135724161, 510900.7135724155, 167815.926422494], 
processed observation next is [1.0, 0.043478260869565216, 0.27488151658767773, 0.8166666666666665, 1.0, 1.0, 0.1857637114235195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1419168648812267, 0.14191686488122654, 0.2504715319738716], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.7157843], dtype=float32), -0.43461072]. 
=============================================
[2019-03-26 23:07:13,654] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:07:13,667] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8176
[2019-03-26 23:07:13,671] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 94.0, 1.0, 2.0, 0.4932284750188914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689207.0923656292, 689207.0923656285, 182541.7730666905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6660000.0000, 
sim time next is 6660600.0000, 
raw observation next is [25.06666666666667, 94.16666666666667, 1.0, 2.0, 0.8839607967448336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0120118691975, 1235509.544592586, 1235509.544592586, 265578.6771098351], 
processed observation next is [1.0, 0.08695652173913043, 0.38704581358609813, 0.9416666666666668, 1.0, 1.0, 0.8601937310178719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8299263523115856, 0.3431970957201628, 0.3431970957201628, 0.39638608523855984], 
reward next is 0.6036, 
noisyNet noise sample is [array([0.9632825], dtype=float32), -1.1746475]. 
=============================================
[2019-03-26 23:07:15,092] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:07:15,101] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2898
[2019-03-26 23:07:15,107] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 76.5, 1.0, 2.0, 0.4075228384696103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 602701.5259398881, 602701.5259398888, 174631.7254999224], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6906600.0000, 
sim time next is 6907200.0000, 
raw observation next is [25.63333333333333, 77.33333333333333, 1.0, 2.0, 0.4098891426627436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604911.2989535672, 604911.2989535672, 174800.0212395065], 
processed observation next is [0.0, 0.9565217391304348, 0.4139020537124801, 0.7733333333333333, 1.0, 1.0, 0.2890230634490887, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16803091637599088, 0.16803091637599088, 0.26089555408881565], 
reward next is 0.7391, 
noisyNet noise sample is [array([0.14376675], dtype=float32), 0.65894926]. 
=============================================
[2019-03-26 23:07:22,229] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:07:22,240] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6075
[2019-03-26 23:07:22,244] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.63333333333333, 52.0, 1.0, 2.0, 0.4580598965596291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646346.3453054957, 646346.3453054951, 178099.7782756368], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6964800.0000, 
sim time next is 6965400.0000, 
raw observation next is [31.81666666666667, 52.0, 1.0, 2.0, 0.4655127809537518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652273.3864311778, 652273.3864311778, 178603.0257078125], 
processed observation next is [0.0, 0.6086956521739131, 0.7069510268562403, 0.52, 1.0, 1.0, 0.35603949512500216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1811870517864383, 0.1811870517864383, 0.2665716801609142], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.6006977], dtype=float32), 1.0242645]. 
=============================================
[2019-03-26 23:07:37,942] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 23:07:37,944] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:07:37,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:07:37,945] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:07:37,947] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:07:37,949] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:07:37,945] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:07:37,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:07:37,952] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:07:37,954] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:07:37,955] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:07:37,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-03-26 23:07:37,999] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-03-26 23:07:38,018] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-03-26 23:07:38,037] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-03-26 23:07:38,060] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-03-26 23:07:48,554] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:07:48,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.92056897666667, 51.21386822666666, 1.0, 2.0, 0.4618174228512561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751259.4022995488, 751259.4022995494, 189011.0405475118]
[2019-03-26 23:07:48,556] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:07:48,562] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.834747e-37 0.000000e+00], sampled 0.843028887621273
[2019-03-26 23:08:01,281] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:01,282] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.02974596666667, 91.09559960666667, 1.0, 2.0, 0.3254350466078788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516958.9488414446, 516958.9488414446, 168347.0333075939]
[2019-03-26 23:08:01,284] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:08:01,285] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7519808446171051
[2019-03-26 23:08:12,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:12,163] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.77949678666667, 80.02530643166666, 1.0, 2.0, 0.5319710412930737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743362.5106088116, 743362.5106088116, 188755.7459864289]
[2019-03-26 23:08:12,164] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:08:12,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.1037463e-36 0.0000000e+00], sampled 0.3132396967238913
[2019-03-26 23:08:18,250] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:18,252] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.99922746, 94.76598743, 1.0, 2.0, 0.3451352568908524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533437.6193380662, 533437.6193380668, 169333.3155668387]
[2019-03-26 23:08:18,256] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:08:18,258] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14855136779704559
[2019-03-26 23:08:18,594] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:18,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.95590240666667, 94.87846518666667, 1.0, 2.0, 0.2887437361548876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464916.2221747101, 464916.2221747108, 164609.086422778]
[2019-03-26 23:08:18,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:08:18,600] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.03295098952051845
[2019-03-26 23:08:27,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:27,927] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.08333333333334, 83.83333333333333, 1.0, 2.0, 0.5215780358245633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728834.6124680545, 728834.6124680545, 187045.3360129432]
[2019-03-26 23:08:27,928] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:08:27,931] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2731543e-35 0.0000000e+00], sampled 0.955053640309697
[2019-03-26 23:08:31,797] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:31,798] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.71585222666667, 75.66039386666667, 1.0, 2.0, 0.5291908469668339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739476.1870208273, 739476.187020828, 188295.7417734073]
[2019-03-26 23:08:31,798] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:08:31,802] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2280026e-34 0.0000000e+00], sampled 0.883359639007661
[2019-03-26 23:08:53,900] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:08:53,903] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.66666666666667, 68.0, 1.0, 2.0, 0.9947208982549747, 1.0, 2.0, 0.9947208982549747, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2782523.4675411, 2782523.4675411, 525802.5257678573]
[2019-03-26 23:08:53,903] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:08:53,906] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3558344e-16 1.9121454e-11 4.1357811e-18 1.0000000e+00 2.5049851e-20], sampled 0.3239096039759467
[2019-03-26 23:09:27,840] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1197253]
[2019-03-26 23:09:27,842] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.1, 72.0, 1.0, 2.0, 0.8535211178566979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1192940.595817367, 1192940.595817368, 257473.476903864]
[2019-03-26 23:09:27,844] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:09:27,846] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2258347e-28 0.0000000e+00], sampled 0.6772405095888755
[2019-03-26 23:09:32,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8729.2391 2772771454.5263 748.0000
[2019-03-26 23:09:32,520] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8612.3657 2831687779.1173 841.0000
[2019-03-26 23:09:32,521] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8238.8815 2984386903.9982 1167.0000
[2019-03-26 23:09:32,526] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8348.9066 2919198794.9777 1111.0000
[2019-03-26 23:09:32,821] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8129.8620 3139408435.8485 1125.0000
[2019-03-26 23:09:33,840] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2100000, evaluation results [2100000.0, 8129.862010177169, 3139408435.8484983, 1125.0, 8348.906608422876, 2919198794.977691, 1111.0, 8729.239123941588, 2772771454.5262513, 748.0, 8238.881480106696, 2984386903.998203, 1167.0, 8612.36572927855, 2831687779.117336, 841.0]
[2019-03-26 23:09:38,632] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:09:38,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5968
[2019-03-26 23:09:38,644] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 84.0, 1.0, 2.0, 0.4712319309942674, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664058.2664899619, 664058.2664899619, 179933.0783742315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7164000.0000, 
sim time next is 7164600.0000, 
raw observation next is [25.78333333333333, 84.16666666666667, 1.0, 2.0, 0.4705645336530451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 663273.8861411756, 663273.8861411756, 179853.4742387891], 
processed observation next is [1.0, 0.9565217391304348, 0.4210110584518167, 0.8416666666666667, 1.0, 1.0, 0.36212594416029537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18424274615032657, 0.18424274615032657, 0.268438021251924], 
reward next is 0.7316, 
noisyNet noise sample is [array([-1.2964077], dtype=float32), -1.1760144]. 
=============================================
[2019-03-26 23:09:41,500] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:09:41,509] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6859
[2019-03-26 23:09:41,516] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 90.66666666666667, 1.0, 2.0, 0.3311335112969416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522813.9779869437, 522813.9779869437, 168755.0481316826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7274400.0000, 
sim time next is 7275000.0000, 
raw observation next is [21.43333333333333, 90.83333333333334, 1.0, 2.0, 0.3283024194652454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518500.3955674687, 518500.3955674687, 168422.2432845208], 
processed observation next is [1.0, 0.17391304347826086, 0.21484992101105835, 0.9083333333333334, 1.0, 1.0, 0.190725806584633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1440278876576302, 0.1440278876576302, 0.2513764825142102], 
reward next is 0.7486, 
noisyNet noise sample is [array([-1.7139455], dtype=float32), -0.96233064]. 
=============================================
[2019-03-26 23:09:41,534] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.809456]
 [78.90691 ]
 [79.03761 ]
 [78.96194 ]
 [79.48388 ]], R is [[78.85842133]
 [78.81796265]
 [78.77706909]
 [78.73781586]
 [78.69315338]].
[2019-03-26 23:09:42,553] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.1051614e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:09:42,558] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9005
[2019-03-26 23:09:42,566] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 74.0, 1.0, 2.0, 0.7306945428776342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131166.859759534, 1131166.859759534, 241497.8793449228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7292400.0000, 
sim time next is 7293000.0000, 
raw observation next is [24.78333333333333, 73.0, 1.0, 2.0, 0.7518275171007635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161936.169716239, 1161936.169716239, 246643.2278884322], 
processed observation next is [1.0, 0.391304347826087, 0.37361769352290675, 0.73, 1.0, 1.0, 0.7009970085551367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32276004714339973, 0.32276004714339973, 0.36812422072900325], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.51217455], dtype=float32), 0.09804804]. 
=============================================
[2019-03-26 23:09:42,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[66.02067 ]
 [67.01824 ]
 [67.74759 ]
 [68.602295]
 [68.994095]], R is [[65.29473877]
 [65.28134918]
 [65.28321838]
 [65.295578  ]
 [65.32007599]].
[2019-03-26 23:09:50,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:09:50,489] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4556
[2019-03-26 23:09:50,493] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 62.0, 1.0, 2.0, 0.4674771543322592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 653212.7197094756, 653212.7197094751, 178657.5924909291], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7571400.0000, 
sim time next is 7572000.0000, 
raw observation next is [29.66666666666667, 62.0, 1.0, 2.0, 0.4643887633404882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648895.9474373192, 648895.9474373192, 178204.6994065632], 
processed observation next is [0.0, 0.6521739130434783, 0.6050552922590839, 0.62, 1.0, 1.0, 0.3546852570367327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18024887428814423, 0.18024887428814423, 0.2659771632933779], 
reward next is 0.7340, 
noisyNet noise sample is [array([-0.43766737], dtype=float32), 1.1872433]. 
=============================================
[2019-03-26 23:09:50,509] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[75.126   ]
 [75.211975]
 [75.21374 ]
 [75.30482 ]
 [75.34736 ]], R is [[75.15142822]
 [75.13326263]
 [75.11536407]
 [75.09925842]
 [75.0845108 ]].
[2019-03-26 23:09:54,760] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:09:54,771] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9994
[2019-03-26 23:09:54,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 92.0, 1.0, 2.0, 0.394226100906987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586920.5135751214, 586920.513575122, 173294.9515662149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7527600.0000, 
sim time next is 7528200.0000, 
raw observation next is [23.28333333333333, 91.83333333333334, 1.0, 2.0, 0.3921844485271709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584765.3535057231, 584765.3535057236, 173125.8966830855], 
processed observation next is [0.0, 0.13043478260869565, 0.3025276461295418, 0.9183333333333334, 1.0, 1.0, 0.26769210665924204, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1624348204182564, 0.16243482041825658, 0.25839686072102314], 
reward next is 0.7416, 
noisyNet noise sample is [array([-0.7122254], dtype=float32), 1.2202159]. 
=============================================
[2019-03-26 23:09:59,783] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:09:59,794] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2832
[2019-03-26 23:09:59,799] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 82.0, 1.0, 2.0, 0.394913388322563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 585719.1098138968, 585719.1098138962, 173115.0791195052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7477200.0000, 
sim time next is 7477800.0000, 
raw observation next is [24.9, 81.66666666666667, 1.0, 2.0, 0.3972185431418468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 588120.0993658563, 588120.099365857, 173303.1534490435], 
processed observation next is [0.0, 0.5652173913043478, 0.3791469194312796, 0.8166666666666668, 1.0, 1.0, 0.2737572808937913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1633666942682934, 0.1633666942682936, 0.2586614230582739], 
reward next is 0.7413, 
noisyNet noise sample is [array([-1.9734012], dtype=float32), 0.7817164]. 
=============================================
[2019-03-26 23:10:03,843] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:03,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:03,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-03-26 23:10:05,550] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2114333: loss 6.1277
[2019-03-26 23:10:05,558] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2114333: learning rate 0.0010
[2019-03-26 23:10:05,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:05,674] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:05,743] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-03-26 23:10:07,276] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.7856636e-16 1.4553498e-03 7.0497905e-19 9.9854469e-01 3.4129155e-22], sum to 1.0000
[2019-03-26 23:10:07,285] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5050
[2019-03-26 23:10:07,290] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.03333333333333, 75.66666666666667, 1.0, 2.0, 0.7016633908505452, 1.0, 2.0, 0.7016633908505452, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1962080.096556378, 1962080.096556378, 374835.7127077064], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7723200.0000, 
sim time next is 7723800.0000, 
raw observation next is [29.25, 74.5, 1.0, 2.0, 0.7156389483546995, 1.0, 2.0, 0.7156389483546995, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2001196.821550614, 2001196.821550614, 380889.528154503], 
processed observation next is [1.0, 0.391304347826087, 0.5853080568720379, 0.745, 1.0, 1.0, 0.6573963233189151, 1.0, 1.0, 0.6573963233189151, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5558880059862816, 0.5558880059862816, 0.5684918330664224], 
reward next is 0.4315, 
noisyNet noise sample is [array([0.6994333], dtype=float32), -0.28976992]. 
=============================================
[2019-03-26 23:10:07,408] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2115279: loss 1.3699
[2019-03-26 23:10:07,412] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2115279: learning rate 0.0010
[2019-03-26 23:10:08,397] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:08,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:08,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-03-26 23:10:10,193] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2116664: loss 82.4937
[2019-03-26 23:10:10,194] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2116664: learning rate 0.0010
[2019-03-26 23:10:12,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:12,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:12,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-03-26 23:10:13,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:13,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:13,104] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-03-26 23:10:14,350] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2118715: loss 103.8647
[2019-03-26 23:10:14,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2118715: learning rate 0.0010
[2019-03-26 23:10:14,849] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2118938: loss 65.8095
[2019-03-26 23:10:14,851] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2118939: learning rate 0.0010
[2019-03-26 23:10:14,881] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:14,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:14,952] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-03-26 23:10:15,448] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:15,449] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:15,503] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-03-26 23:10:16,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:16,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:16,446] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2119823: loss 4.9438
[2019-03-26 23:10:16,448] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2119823: learning rate 0.0010
[2019-03-26 23:10:16,462] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-03-26 23:10:17,021] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2120149: loss 25.1458
[2019-03-26 23:10:17,022] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2120149: learning rate 0.0010
[2019-03-26 23:10:17,795] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:17,796] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:17,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-03-26 23:10:18,100] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2120708: loss 9.8652
[2019-03-26 23:10:18,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2120708: learning rate 0.0010
[2019-03-26 23:10:19,133] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:19,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:19,213] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-03-26 23:10:19,248] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:19,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:19,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-03-26 23:10:19,484] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2121445: loss 4.5245
[2019-03-26 23:10:19,486] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2121445: learning rate 0.0010
[2019-03-26 23:10:19,603] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2121514: loss 0.0130
[2019-03-26 23:10:19,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2121514: learning rate 0.0010
[2019-03-26 23:10:20,833] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2122215: loss 1.2941
[2019-03-26 23:10:20,834] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2122215: learning rate 0.0010
[2019-03-26 23:10:20,939] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2122264: loss 4.6270
[2019-03-26 23:10:20,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2122265: learning rate 0.0010
[2019-03-26 23:10:21,241] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2122402: loss 0.0076
[2019-03-26 23:10:21,244] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2122402: learning rate 0.0010
[2019-03-26 23:10:21,419] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:21,420] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:21,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-03-26 23:10:21,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:21,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:21,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:21,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:21,687] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-03-26 23:10:21,722] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-03-26 23:10:22,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:22,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:22,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-03-26 23:10:22,986] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2123362: loss 0.8757
[2019-03-26 23:10:22,987] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2123362: learning rate 0.0010
[2019-03-26 23:10:23,265] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2123526: loss 3.9285
[2019-03-26 23:10:23,265] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2123526: loss 4.6499
[2019-03-26 23:10:23,267] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2123526: learning rate 0.0010
[2019-03-26 23:10:23,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2123527: learning rate 0.0010
[2019-03-26 23:10:23,306] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2123547: loss 0.0100
[2019-03-26 23:10:23,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2123547: learning rate 0.0010
[2019-03-26 23:10:23,463] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:10:23,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:23,534] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-03-26 23:10:23,988] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2123924: loss 1.7826
[2019-03-26 23:10:23,989] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2123925: learning rate 0.0010
[2019-03-26 23:10:24,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.913347e-21 0.000000e+00], sum to 1.0000
[2019-03-26 23:10:24,384] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7569
[2019-03-26 23:10:24,392] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 96.0, 1.0, 2.0, 0.334621014927417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 524335.8812284133, 524335.8812284126, 168795.4524456441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 166200.0000, 
sim time next is 166800.0000, 
raw observation next is [21.0, 96.0, 1.0, 2.0, 0.3335006658281129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 523905.1669742728, 523905.1669742721, 168790.5164609165], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.96, 1.0, 1.0, 0.19698875400977456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1455292130484091, 0.14552921304840893, 0.25192614397151714], 
reward next is 0.7481, 
noisyNet noise sample is [array([-2.3460793], dtype=float32), 0.0038962609]. 
=============================================
[2019-03-26 23:10:24,445] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.902172e-32 0.000000e+00], sum to 1.0000
[2019-03-26 23:10:24,446] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0911
[2019-03-26 23:10:24,450] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 69.5, 1.0, 2.0, 0.7432295329360913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1142410.468538642, 1142410.468538642, 243752.7884550088], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 37800.0000, 
sim time next is 38400.0000, 
raw observation next is [25.76666666666667, 68.66666666666667, 1.0, 2.0, 0.8336674078865801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1278676.334116864, 1278676.334116864, 267608.4755021951], 
processed observation next is [1.0, 0.43478260869565216, 0.42022116903633505, 0.6866666666666668, 1.0, 1.0, 0.7995992866103375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.35518787058801776, 0.35518787058801776, 0.3994156350779032], 
reward next is 0.6006, 
noisyNet noise sample is [array([-0.09868561], dtype=float32), 0.03457521]. 
=============================================
[2019-03-26 23:10:25,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7239083e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 23:10:25,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6369
[2019-03-26 23:10:25,196] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333334, 92.33333333333334, 1.0, 2.0, 0.2848866203252448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 457874.7427059273, 457874.7427059267, 164125.7888438743], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 267000.0000, 
sim time next is 267600.0000, 
raw observation next is [20.36666666666667, 92.66666666666667, 1.0, 2.0, 0.2846646876911634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457752.9761656316, 457752.9761656316, 164118.6441627186], 
processed observation next is [0.0, 0.08695652173913043, 0.1642969984202214, 0.9266666666666667, 1.0, 1.0, 0.13815022613393177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12715360449045324, 0.12715360449045324, 0.2449532002428636], 
reward next is 0.7550, 
noisyNet noise sample is [array([0.31284428], dtype=float32), -1.0578232]. 
=============================================
[2019-03-26 23:10:25,255] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2124583: loss 0.6346
[2019-03-26 23:10:25,257] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2124584: learning rate 0.0010
[2019-03-26 23:10:26,196] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 23:10:26,198] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:10:26,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:26,199] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:10:26,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:10:26,201] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:26,203] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:10:26,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:26,202] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:10:26,207] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:26,208] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:10:26,231] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-03-26 23:10:26,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-03-26 23:10:26,283] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-03-26 23:10:26,303] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-03-26 23:10:26,304] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-03-26 23:10:36,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:10:36,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.85, 84.33333333333333, 1.0, 2.0, 0.3105403303680755, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 494917.2707464259, 494917.2707464259, 166709.6488244838]
[2019-03-26 23:10:36,129] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:10:36,130] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.40658864077038825
[2019-03-26 23:11:03,626] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:03,629] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 66.33333333333333, 1.0, 2.0, 0.9049184561409643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1264819.886102872, 1264819.886102872, 271292.1019393342]
[2019-03-26 23:11:03,630] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:11:03,634] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.2655937032802608
[2019-03-26 23:11:06,705] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:06,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.03333333333333, 80.66666666666667, 1.0, 2.0, 0.5720213417927215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799348.8180317055, 799348.8180317055, 195648.6915702939]
[2019-03-26 23:11:06,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:11:06,709] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6523619540461701
[2019-03-26 23:11:15,039] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:15,040] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.96666666666667, 57.0, 1.0, 2.0, 0.5245977084879303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733055.6517210628, 733055.6517210621, 187539.0661590225]
[2019-03-26 23:11:15,042] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:11:15,044] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.6189933293854522
[2019-03-26 23:11:40,707] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:40,710] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.8, 85.0, 1.0, 2.0, 0.6327211049123415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884206.6312616653, 884206.6312616653, 206999.5149399539]
[2019-03-26 23:11:40,711] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:11:40,715] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.01574604522721057
[2019-03-26 23:11:51,720] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:51,722] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.24761802333333, 82.77409081, 1.0, 2.0, 0.5600577080434939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 782624.5476071058, 782624.5476071051, 193539.7813292411]
[2019-03-26 23:11:51,724] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:11:51,726] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.28114718985352916
[2019-03-26 23:11:58,279] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:11:58,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.40000000000001, 62.66666666666667, 1.0, 2.0, 0.5729615570309236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800663.1801856619, 800663.1801856619, 195816.4042435031]
[2019-03-26 23:11:58,283] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:11:58,285] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.10559564465271298
[2019-03-26 23:12:18,639] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0254654]
[2019-03-26 23:12:18,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.43296728166667, 94.99371885500001, 1.0, 2.0, 0.5876427238945738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821186.7450930744, 821186.7450930738, 198466.0056691425]
[2019-03-26 23:12:18,640] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:12:18,643] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.624284165398945
[2019-03-26 23:12:21,007] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.2308 2927315925.6204 1318.0000
[2019-03-26 23:12:21,069] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.9889 2842628700.7373 1114.0000
[2019-03-26 23:12:21,142] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7886.6720 3163865214.8434 1754.0000
[2019-03-26 23:12:21,143] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8668.4498 2778910188.1883 905.0000
[2019-03-26 23:12:21,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.9242 3007815566.5926 1752.0000
[2019-03-26 23:12:22,163] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2125000, evaluation results [2125000.0, 7886.672030766345, 3163865214.8433704, 1754.0, 8263.230773654925, 2927315925.6203804, 1318.0, 8668.44980733942, 2778910188.188306, 905.0, 8002.924244942965, 3007815566.5926166, 1752.0, 8498.98886953507, 2842628700.7373075, 1114.0]
[2019-03-26 23:12:22,173] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:22,173] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8142
[2019-03-26 23:12:22,178] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.2, 96.0, 1.0, 2.0, 0.2614105203169333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426112.745323793, 426112.745323793, 162000.0584266495], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 277200.0000, 
sim time next is 277800.0000, 
raw observation next is [19.35, 95.33333333333333, 1.0, 2.0, 0.2623846589048661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427461.9519288958, 427461.9519288958, 162091.0591451025], 
processed observation next is [0.0, 0.21739130434782608, 0.11611374407582951, 0.9533333333333333, 1.0, 1.0, 0.11130681795767002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11873943109135994, 0.11873943109135994, 0.2419269539479142], 
reward next is 0.7581, 
noisyNet noise sample is [array([1.6402813], dtype=float32), -0.54752976]. 
=============================================
[2019-03-26 23:12:23,295] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.2787326e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:12:23,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0694
[2019-03-26 23:12:23,309] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 80.33333333333334, 1.0, 2.0, 0.3754797772127559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570268.6303970864, 570268.630397087, 172154.5278498729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 69600.0000, 
sim time next is 70200.0000, 
raw observation next is [24.1, 81.0, 1.0, 2.0, 0.3727560468232467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 567340.9216961456, 567340.9216961456, 171934.3372624855], 
processed observation next is [1.0, 0.8260869565217391, 0.3412322274881518, 0.81, 1.0, 1.0, 0.24428439376294783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15759470047115154, 0.15759470047115154, 0.2566184138246052], 
reward next is 0.7434, 
noisyNet noise sample is [array([0.85590595], dtype=float32), 0.30100578]. 
=============================================
[2019-03-26 23:12:23,809] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2125741: loss 0.0242
[2019-03-26 23:12:23,811] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2125741: learning rate 0.0010
[2019-03-26 23:12:24,228] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2125927: loss 0.0093
[2019-03-26 23:12:24,234] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2125928: learning rate 0.0010
[2019-03-26 23:12:25,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:25,274] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8788
[2019-03-26 23:12:25,282] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.25, 81.66666666666667, 1.0, 2.0, 0.2481922374714207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 408419.1267361342, 408419.1267361335, 160724.5840904671], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 420600.0000, 
sim time next is 421200.0000, 
raw observation next is [20.2, 82.0, 1.0, 2.0, 0.2491420668945658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 410017.1736102046, 410017.173610204, 160817.0308902212], 
processed observation next is [1.0, 0.9130434782608695, 0.15639810426540288, 0.82, 1.0, 1.0, 0.09535188782477808, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11389365933616795, 0.11389365933616778, 0.24002541923913615], 
reward next is 0.7600, 
noisyNet noise sample is [array([-1.1757199], dtype=float32), -0.976541]. 
=============================================
[2019-03-26 23:12:26,562] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2126973: loss 0.0050
[2019-03-26 23:12:26,566] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2126973: learning rate 0.0010
[2019-03-26 23:12:27,673] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2127471: loss 0.0023
[2019-03-26 23:12:27,675] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2127471: learning rate 0.0010
[2019-03-26 23:12:27,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:28,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9637
[2019-03-26 23:12:28,007] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 86.0, 1.0, 2.0, 0.3176992795855395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501055.3737746762, 501055.3737746769, 167077.0104738689], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 225000.0000, 
sim time next is 225600.0000, 
raw observation next is [22.06666666666667, 86.0, 1.0, 2.0, 0.3163282975316293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499282.2942377378, 499282.2942377378, 166952.4391187992], 
processed observation next is [0.0, 0.6086956521739131, 0.2448657187993683, 0.86, 1.0, 1.0, 0.17629915365256538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13868952617714939, 0.13868952617714939, 0.24918274495343165], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.23433506], dtype=float32), -1.0826231]. 
=============================================
[2019-03-26 23:12:28,919] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2128024: loss 0.0671
[2019-03-26 23:12:28,921] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2128024: learning rate 0.0010
[2019-03-26 23:12:30,832] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2128879: loss 0.1342
[2019-03-26 23:12:30,834] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2128879: learning rate 0.0010
[2019-03-26 23:12:30,880] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2128900: loss 0.0460
[2019-03-26 23:12:30,890] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2128903: learning rate 0.0010
[2019-03-26 23:12:32,768] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2129748: loss 0.0024
[2019-03-26 23:12:32,772] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2129749: learning rate 0.0010
[2019-03-26 23:12:33,173] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2129934: loss 0.0272
[2019-03-26 23:12:33,175] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2129935: learning rate 0.0010
[2019-03-26 23:12:33,240] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2129966: loss 0.4921
[2019-03-26 23:12:33,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2129967: learning rate 0.0010
[2019-03-26 23:12:35,663] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5702151e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:12:35,673] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6479
[2019-03-26 23:12:35,681] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333334, 87.0, 1.0, 2.0, 0.2606786183408782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424511.086626421, 424511.086626421, 161910.4406527069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 351600.0000, 
sim time next is 352200.0000, 
raw observation next is [20.31666666666667, 87.0, 1.0, 2.0, 0.2605029842532295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424335.7549007521, 424335.7549007521, 161896.8102515732], 
processed observation next is [1.0, 0.043478260869565216, 0.1619273301737759, 0.87, 1.0, 1.0, 0.1090397400641319, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11787104302798669, 0.11787104302798669, 0.24163703022622865], 
reward next is 0.7584, 
noisyNet noise sample is [array([-0.01121861], dtype=float32), 0.30096057]. 
=============================================
[2019-03-26 23:12:36,246] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2131297: loss 0.0029
[2019-03-26 23:12:36,251] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2131298: learning rate 0.0010
[2019-03-26 23:12:36,460] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2131390: loss 0.2248
[2019-03-26 23:12:36,464] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2131390: learning rate 0.0010
[2019-03-26 23:12:36,677] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2131485: loss 0.0051
[2019-03-26 23:12:36,679] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2131485: learning rate 0.0010
[2019-03-26 23:12:36,884] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2131578: loss 0.0114
[2019-03-26 23:12:36,887] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2131579: learning rate 0.0010
[2019-03-26 23:12:37,770] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2131973: loss 0.0092
[2019-03-26 23:12:37,773] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2131973: learning rate 0.0010
[2019-03-26 23:12:39,326] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2132665: loss 0.0141
[2019-03-26 23:12:39,330] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2132665: learning rate 0.0010
[2019-03-26 23:12:41,654] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2133714: loss 0.3035
[2019-03-26 23:12:41,656] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2133715: learning rate 0.0010
[2019-03-26 23:12:41,970] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2133857: loss 0.3079
[2019-03-26 23:12:41,974] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2133858: learning rate 0.0010
[2019-03-26 23:12:44,484] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2134981: loss 0.3391
[2019-03-26 23:12:44,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2134981: learning rate 0.0010
[2019-03-26 23:12:45,449] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2135415: loss 0.2898
[2019-03-26 23:12:45,450] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2135415: learning rate 0.0010
[2019-03-26 23:12:46,948] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2136086: loss 0.4605
[2019-03-26 23:12:46,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2136088: learning rate 0.0010
[2019-03-26 23:12:47,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:47,031] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9921
[2019-03-26 23:12:47,035] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.36666666666667, 55.16666666666666, 1.0, 2.0, 0.3100019470312426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509637.4464753962, 509637.4464753969, 167449.2733693903], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 493800.0000, 
sim time next is 494400.0000, 
raw observation next is [24.13333333333333, 56.33333333333334, 1.0, 2.0, 0.2452469301751609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403336.2076768128, 403336.2076768128, 160441.8359325455], 
processed observation next is [1.0, 0.7391304347826086, 0.3428120063191152, 0.5633333333333335, 1.0, 1.0, 0.09065895201826613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11203783546578133, 0.11203783546578133, 0.2394654267649933], 
reward next is 0.7605, 
noisyNet noise sample is [array([0.7053852], dtype=float32), -0.08948868]. 
=============================================
[2019-03-26 23:12:48,001] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:48,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2235
[2019-03-26 23:12:48,015] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 61.0, 1.0, 2.0, 0.2450129090338873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403502.7665618007, 403502.7665618007, 160408.1908090874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 496800.0000, 
sim time next is 497400.0000, 
raw observation next is [22.95, 62.33333333333334, 1.0, 2.0, 0.2468573924072627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 406673.5049488174, 406673.504948818, 160583.7274971596], 
processed observation next is [1.0, 0.782608695652174, 0.28672985781990523, 0.6233333333333334, 1.0, 1.0, 0.09259926796055747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11296486248578262, 0.11296486248578277, 0.23967720521964117], 
reward next is 0.7603, 
noisyNet noise sample is [array([1.4727947], dtype=float32), -0.04358289]. 
=============================================
[2019-03-26 23:12:48,597] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2136824: loss 0.0026
[2019-03-26 23:12:48,599] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2136825: learning rate 0.0010
[2019-03-26 23:12:48,732] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2136884: loss 0.5831
[2019-03-26 23:12:48,741] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2136888: learning rate 0.0010
[2019-03-26 23:12:49,576] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2362854e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 23:12:49,586] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9064
[2019-03-26 23:12:49,592] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.1, 80.0, 1.0, 2.0, 0.2428836359520094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 401307.3486973058, 401307.3486973058, 160148.350047122], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 680400.0000, 
sim time next is 681000.0000, 
raw observation next is [20.0, 80.66666666666667, 1.0, 2.0, 0.2415022161778914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 399074.9289187366, 399074.9289187372, 160012.917123301], 
processed observation next is [1.0, 0.9130434782608695, 0.1469194312796209, 0.8066666666666668, 1.0, 1.0, 0.08614724840709805, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11085414692187127, 0.11085414692187145, 0.2388252494377627], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.19789484], dtype=float32), 0.27375796]. 
=============================================
[2019-03-26 23:12:49,629] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[73.604256]
 [73.4029  ]
 [73.426476]
 [73.558754]
 [73.7445  ]], R is [[73.5954895 ]
 [73.62050629]
 [73.64498901]
 [73.66911316]
 [73.69319153]].
[2019-03-26 23:12:50,623] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2137731: loss 0.1388
[2019-03-26 23:12:50,625] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2137731: learning rate 0.0010
[2019-03-26 23:12:50,979] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2137889: loss 0.0053
[2019-03-26 23:12:50,982] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2137890: learning rate 0.0010
[2019-03-26 23:12:51,135] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2137962: loss 0.7038
[2019-03-26 23:12:51,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2137962: learning rate 0.0010
[2019-03-26 23:12:51,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:51,208] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8773
[2019-03-26 23:12:51,211] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 54.33333333333334, 1.0, 2.0, 0.5346861525628435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 878580.5556977735, 878580.5556977735, 202307.5717364682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 652800.0000, 
sim time next is 653400.0000, 
raw observation next is [24.6, 54.0, 1.0, 2.0, 0.5738309843027856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943131.678212051, 943131.678212051, 210106.825288552], 
processed observation next is [1.0, 0.5652173913043478, 0.36492890995260674, 0.54, 1.0, 1.0, 0.4865433545816693, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26198102172556975, 0.26198102172556975, 0.3135922765500776], 
reward next is 0.6864, 
noisyNet noise sample is [array([-0.48535702], dtype=float32), 2.225615]. 
=============================================
[2019-03-26 23:12:54,064] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2139271: loss 0.1852
[2019-03-26 23:12:54,067] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2139272: learning rate 0.0010
[2019-03-26 23:12:54,454] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2139436: loss 0.0033
[2019-03-26 23:12:54,456] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2139436: learning rate 0.0010
[2019-03-26 23:12:54,667] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2139532: loss 0.4133
[2019-03-26 23:12:54,669] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2139532: learning rate 0.0010
[2019-03-26 23:12:54,870] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2139626: loss 0.2214
[2019-03-26 23:12:54,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2139627: learning rate 0.0010
[2019-03-26 23:12:55,489] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2139899: loss 0.2719
[2019-03-26 23:12:55,493] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2139901: learning rate 0.0010
[2019-03-26 23:12:55,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:55,992] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9874
[2019-03-26 23:12:55,999] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 97.5, 1.0, 2.0, 0.3724396340667769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 564686.0933807458, 564686.0933807465, 171638.2772160558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1035000.0000, 
sim time next is 1035600.0000, 
raw observation next is [22.13333333333333, 97.33333333333333, 1.0, 2.0, 0.372787209659153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 564968.6092827584, 564968.6092827584, 171655.3101044312], 
processed observation next is [1.0, 1.0, 0.24802527646129527, 0.9733333333333333, 1.0, 1.0, 0.24432193934837712, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15693572480076623, 0.15693572480076623, 0.25620195537974805], 
reward next is 0.7438, 
noisyNet noise sample is [array([-1.3549906], dtype=float32), 2.115115]. 
=============================================
[2019-03-26 23:12:57,235] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2140680: loss 0.1978
[2019-03-26 23:12:57,239] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2140681: learning rate 0.0010
[2019-03-26 23:12:57,456] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:12:57,462] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9753
[2019-03-26 23:12:57,466] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.0, 1.0, 2.0, 0.2594014098224554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424576.881604003, 424576.881604003, 161841.3151887065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 786000.0000, 
sim time next is 786600.0000, 
raw observation next is [19.4, 92.0, 1.0, 2.0, 0.2590417686379294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 423988.3716942275, 423988.3716942275, 161804.8166700756], 
processed observation next is [0.0, 0.08695652173913043, 0.11848341232227487, 0.92, 1.0, 1.0, 0.10727923932280647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11777454769284097, 0.11777454769284097, 0.24149972637324718], 
reward next is 0.7585, 
noisyNet noise sample is [array([-1.5619973], dtype=float32), -1.0331451]. 
=============================================
[2019-03-26 23:12:59,527] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2141707: loss 0.0086
[2019-03-26 23:12:59,531] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2141708: learning rate 0.0010
[2019-03-26 23:12:59,873] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2141859: loss 0.0016
[2019-03-26 23:12:59,875] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2141860: learning rate 0.0010
[2019-03-26 23:13:02,481] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2143033: loss 0.0047
[2019-03-26 23:13:02,483] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2143033: learning rate 0.0010
[2019-03-26 23:13:03,343] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2143423: loss 0.0025
[2019-03-26 23:13:03,346] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2143424: learning rate 0.0010
[2019-03-26 23:13:04,931] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2144134: loss 0.0204
[2019-03-26 23:13:04,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2144138: learning rate 0.0010
[2019-03-26 23:13:06,411] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2144797: loss 2.0070
[2019-03-26 23:13:06,412] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2144797: learning rate 0.0010
[2019-03-26 23:13:06,619] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2144890: loss 0.0113
[2019-03-26 23:13:06,622] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2144891: learning rate 0.0010
[2019-03-26 23:13:08,441] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2145704: loss 0.0014
[2019-03-26 23:13:08,443] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2145706: learning rate 0.0010
[2019-03-26 23:13:08,740] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2145836: loss 1.5762
[2019-03-26 23:13:08,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2145839: learning rate 0.0010
[2019-03-26 23:13:09,001] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2145955: loss 0.0047
[2019-03-26 23:13:09,005] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2145956: learning rate 0.0010
[2019-03-26 23:13:11,891] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2147248: loss 0.0166
[2019-03-26 23:13:11,893] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2147248: learning rate 0.0010
[2019-03-26 23:13:12,270] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2147422: loss 0.0073
[2019-03-26 23:13:12,272] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2147422: learning rate 0.0010
[2019-03-26 23:13:12,332] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2147445: loss 1.4086
[2019-03-26 23:13:12,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2147447: learning rate 0.0010
[2019-03-26 23:13:12,765] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2147639: loss 0.0392
[2019-03-26 23:13:12,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2147640: learning rate 0.0010
[2019-03-26 23:13:13,264] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2147860: loss 0.0017
[2019-03-26 23:13:13,267] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2147862: learning rate 0.0010
[2019-03-26 23:13:13,431] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:13:13,448] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2453
[2019-03-26 23:13:13,451] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 93.33333333333334, 1.0, 2.0, 0.5372174152532702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 831187.1474571837, 831187.1474571843, 199218.4408116657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 984000.0000, 
sim time next is 984600.0000, 
raw observation next is [21.95, 93.5, 1.0, 2.0, 0.5693733268232177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 880004.0057918355, 880004.0057918349, 205284.1763980274], 
processed observation next is [1.0, 0.391304347826087, 0.2393364928909953, 0.935, 1.0, 1.0, 0.4811726829195394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24444555716439875, 0.24444555716439859, 0.30639429313138417], 
reward next is 0.6936, 
noisyNet noise sample is [array([-0.5672183], dtype=float32), 0.094168]. 
=============================================
[2019-03-26 23:13:15,356] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2148788: loss 0.0029
[2019-03-26 23:13:15,359] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2148788: learning rate 0.0010
[2019-03-26 23:13:17,299] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2149658: loss 1.3826
[2019-03-26 23:13:17,301] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2149658: learning rate 0.0010
[2019-03-26 23:13:17,562] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2149775: loss 1.0761
[2019-03-26 23:13:17,564] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2149775: learning rate 0.0010
[2019-03-26 23:13:18,064] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 23:13:18,066] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:13:18,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:13:18,069] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:13:18,070] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:13:18,070] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:13:18,073] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:13:18,074] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:13:18,075] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:13:18,076] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:13:18,071] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:13:18,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-03-26 23:13:18,119] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-03-26 23:13:18,143] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-03-26 23:13:18,144] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-03-26 23:13:18,165] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-03-26 23:13:28,846] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:13:28,852] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [17.6, 92.66666666666667, 1.0, 2.0, 0.2168974310259757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 361364.6682528363, 361364.6682528363, 157314.7848902274]
[2019-03-26 23:13:28,853] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:13:28,856] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.43696793551104873
[2019-03-26 23:13:38,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:13:38,250] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.86666666666667, 78.66666666666667, 1.0, 2.0, 0.2312608336530948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383545.4728140257, 383545.4728140257, 158925.0859017005]
[2019-03-26 23:13:38,251] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:13:38,256] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9175936350996632
[2019-03-26 23:13:51,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:13:51,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.81666666666667, 64.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.180545450742226, 6.9112, 168.9112620169963, 2479151.505766663, 2288070.489752357, 475718.5718566757]
[2019-03-26 23:13:51,351] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:13:51,357] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.7929566e-27 0.0000000e+00], sampled 0.26464682779933213
[2019-03-26 23:13:51,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2479151.505766663 W.
[2019-03-26 23:13:58,108] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:13:58,109] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.37669642, 90.85678826, 1.0, 2.0, 0.325625786025385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514326.3124493457, 514326.312449345, 168100.9434213701]
[2019-03-26 23:13:58,110] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:13:58,112] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.4424025257630453
[2019-03-26 23:13:58,706] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:13:58,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.83283661, 94.91430183, 1.0, 2.0, 0.2791259268475709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 451896.8001733717, 451896.8001733717, 163718.2229664762]
[2019-03-26 23:13:58,710] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:13:58,717] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7540561066948575
[2019-03-26 23:14:00,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:14:00,503] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.97922538333334, 85.52652725333334, 1.0, 2.0, 0.4081400552033779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595168.4430659538, 595168.4430659538, 173670.0433704134]
[2019-03-26 23:14:00,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:14:00,508] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.017250331344835
[2019-03-26 23:14:27,015] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:14:27,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.18179275, 68.10133912333333, 1.0, 2.0, 0.5238638163217112, 0.0, 2.0, 0.0, 1.0, 1.0, 0.8818411890921728, 6.911199999999999, 6.9112, 168.9126869423089, 1464564.384435424, 1464564.384435424, 315916.3318993571]
[2019-03-26 23:14:27,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:14:27,021] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.9639727e-34 0.0000000e+00], sampled 0.8204220677408922
[2019-03-26 23:14:38,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:14:38,740] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.76666666666667, 94.0, 1.0, 2.0, 0.6383706585097045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 892105.0096710057, 892105.0096710057, 208111.9444316458]
[2019-03-26 23:14:38,740] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:14:38,744] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.33052863251785247
[2019-03-26 23:15:02,195] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:15:02,196] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.40756106, 82.93981129, 1.0, 2.0, 0.6763106271015824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 945148.638909252, 945148.638909252, 215816.5355208974]
[2019-03-26 23:15:02,198] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:15:02,200] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9424627286842076
[2019-03-26 23:15:06,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9943953]
[2019-03-26 23:15:06,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.06666666666667, 74.66666666666667, 1.0, 2.0, 0.4664172587223063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662110.5747056825, 662110.5747056832, 179836.5364543395]
[2019-03-26 23:15:06,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:15:06,878] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.47012766776078696
[2019-03-26 23:15:12,746] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.6154 3164676497.9081 1778.0000
[2019-03-26 23:15:12,954] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0319 2779830318.5638 932.0000
[2019-03-26 23:15:12,964] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 23:15:12,999] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 23:15:13,084] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.9766 3008303976.5521 1766.0000
[2019-03-26 23:15:14,104] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2150000, evaluation results [2150000.0, 7876.615402862808, 3164676497.9080544, 1778.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8660.03188781958, 2779830318.5637875, 932.0, 7997.976645730424, 3008303976.5521116, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 23:15:16,274] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2150983: loss 1.1973
[2019-03-26 23:15:16,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2150984: learning rate 0.0010
[2019-03-26 23:15:17,234] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2151411: loss 1.0381
[2019-03-26 23:15:17,236] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2151412: learning rate 0.0010
[2019-03-26 23:15:18,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:18,660] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0899
[2019-03-26 23:15:18,665] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.6, 88.0, 1.0, 2.0, 0.3503547184393271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541939.3885808345, 541939.3885808345, 170039.5037553105], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1209600.0000, 
sim time next is 1210200.0000, 
raw observation next is [22.56666666666667, 88.16666666666667, 1.0, 2.0, 0.3486060157665278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539487.9497954753, 539487.9497954753, 169845.2876534384], 
processed observation next is [1.0, 0.0, 0.26856240126382325, 0.8816666666666667, 1.0, 1.0, 0.2151879708030455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1498577638320765, 0.1498577638320765, 0.25350042933349015], 
reward next is 0.7465, 
noisyNet noise sample is [array([-2.7482836], dtype=float32), 0.10131244]. 
=============================================
[2019-03-26 23:15:18,753] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2152089: loss 1.3982
[2019-03-26 23:15:18,757] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2152089: learning rate 0.0010
[2019-03-26 23:15:20,305] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2152781: loss 1.1624
[2019-03-26 23:15:20,307] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2152781: learning rate 0.0010
[2019-03-26 23:15:20,604] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2152915: loss 0.0309
[2019-03-26 23:15:20,608] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2152915: learning rate 0.0010
[2019-03-26 23:15:20,677] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:20,685] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4548
[2019-03-26 23:15:20,692] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 76.66666666666667, 1.0, 2.0, 0.9627624593586369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1386511.577371322, 1386511.577371322, 293996.219844892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1244400.0000, 
sim time next is 1245000.0000, 
raw observation next is [26.6, 75.83333333333333, 1.0, 2.0, 0.9844087125408639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1413843.379648927, 1413843.379648927, 300052.910774734], 
processed observation next is [1.0, 0.391304347826087, 0.4597156398104266, 0.7583333333333333, 1.0, 1.0, 0.9812153163142939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3927342721247019, 0.3927342721247019, 0.44784016533542387], 
reward next is 0.5522, 
noisyNet noise sample is [array([1.1959659], dtype=float32), -0.8320513]. 
=============================================
[2019-03-26 23:15:20,705] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[67.92341]
 [68.12659]
 [67.93556]
 [68.3638 ]
 [68.15161]], R is [[67.7191925 ]
 [67.60320282]
 [67.49961853]
 [67.39666748]
 [67.31336975]].
[2019-03-26 23:15:21,355] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:21,363] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0161
[2019-03-26 23:15:21,372] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.45, 91.33333333333334, 1.0, 2.0, 0.3483192195694967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560963.5639090809, 560963.5639090802, 171829.06402252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1131000.0000, 
sim time next is 1131600.0000, 
raw observation next is [20.4, 91.66666666666667, 1.0, 2.0, 0.2968436553141644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478136.7622501294, 478136.7622501287, 165529.0784781773], 
processed observation next is [1.0, 0.08695652173913043, 0.16587677725118483, 0.9166666666666667, 1.0, 1.0, 0.15282368110140288, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1328157672917026, 0.1328157672917024, 0.24705832608683181], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.94750535], dtype=float32), -0.34190512]. 
=============================================
[2019-03-26 23:15:21,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:21,871] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5162
[2019-03-26 23:15:21,878] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 67.0, 1.0, 2.0, 0.3443976029830777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532773.1660277712, 532773.1660277712, 169292.8305185069], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1191600.0000, 
sim time next is 1192200.0000, 
raw observation next is [25.55, 68.0, 1.0, 2.0, 0.3459098047016798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535056.3827896409, 535056.3827896416, 169476.4749503628], 
processed observation next is [1.0, 0.8260869565217391, 0.40995260663507116, 0.68, 1.0, 1.0, 0.21193952373696362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14862677299712249, 0.14862677299712268, 0.2529499626124818], 
reward next is 0.7471, 
noisyNet noise sample is [array([-0.7195884], dtype=float32), 0.84384763]. 
=============================================
[2019-03-26 23:15:22,143] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2153592: loss 1.0459
[2019-03-26 23:15:22,143] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2153592: learning rate 0.0010
[2019-03-26 23:15:22,474] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:22,481] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3673
[2019-03-26 23:15:22,487] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 67.0, 1.0, 2.0, 0.7717465397206623, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1190770.815690985, 1190770.815690985, 251603.0383437812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1098000.0000, 
sim time next is 1098600.0000, 
raw observation next is [25.66666666666666, 67.5, 1.0, 2.0, 0.4232138857200688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 653470.7032023642, 653470.7032023648, 179974.1591324478], 
processed observation next is [1.0, 0.7391304347826086, 0.4154818325434437, 0.675, 1.0, 1.0, 0.3050769707470708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1815196397784345, 0.1815196397784347, 0.2686181479588773], 
reward next is 0.7314, 
noisyNet noise sample is [array([0.8188471], dtype=float32), -1.425081]. 
=============================================
[2019-03-26 23:15:22,764] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2153868: loss 0.0358
[2019-03-26 23:15:22,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2153868: learning rate 0.0010
[2019-03-26 23:15:23,112] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2154009: loss 1.0639
[2019-03-26 23:15:23,117] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2154011: learning rate 0.0010
[2019-03-26 23:15:23,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:23,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0678
[2019-03-26 23:15:23,822] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 58.33333333333333, 1.0, 2.0, 0.3491524393875517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 537979.8451074867, 537979.845107486, 169654.2106731027], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1528800.0000, 
sim time next is 1529400.0000, 
raw observation next is [27.25, 58.66666666666667, 1.0, 2.0, 0.346292124444137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534723.2371210987, 534723.2371210987, 169422.5073354292], 
processed observation next is [0.0, 0.6956521739130435, 0.490521327014218, 0.5866666666666667, 1.0, 1.0, 0.21240014993269515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1485342325336385, 0.1485342325336385, 0.2528694139334764], 
reward next is 0.7471, 
noisyNet noise sample is [array([0.07513345], dtype=float32), 0.1009501]. 
=============================================
[2019-03-26 23:15:25,422] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:25,432] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6572
[2019-03-26 23:15:25,439] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.33333333333334, 1.0, 2.0, 0.3382249340770461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524194.9318888809, 524194.9318888815, 168632.8292670927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1491600.0000, 
sim time next is 1492200.0000, 
raw observation next is [22.1, 92.5, 1.0, 2.0, 0.3419022963034121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 528449.00397552, 528449.0039755206, 168930.2571604968], 
processed observation next is [0.0, 0.2608695652173913, 0.24644549763033188, 0.925, 1.0, 1.0, 0.20711120036555677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14679138999320002, 0.14679138999320016, 0.252134712179846], 
reward next is 0.7479, 
noisyNet noise sample is [array([0.07021601], dtype=float32), -1.3746101]. 
=============================================
[2019-03-26 23:15:25,745] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2155189: loss 0.7121
[2019-03-26 23:15:25,748] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2155190: learning rate 0.0010
[2019-03-26 23:15:26,149] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2155374: loss 0.8019
[2019-03-26 23:15:26,150] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2155375: learning rate 0.0010
[2019-03-26 23:15:26,555] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2155552: loss 0.0719
[2019-03-26 23:15:26,558] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2155552: learning rate 0.0010
[2019-03-26 23:15:26,697] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2155618: loss 1.1810
[2019-03-26 23:15:26,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2155618: learning rate 0.0010
[2019-03-26 23:15:27,205] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2155842: loss 1.0371
[2019-03-26 23:15:27,209] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2155846: learning rate 0.0010
[2019-03-26 23:15:28,869] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8592495e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:15:28,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6419
[2019-03-26 23:15:28,883] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.46666666666667, 88.33333333333334, 1.0, 2.0, 0.3838733222462882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576914.3328353864, 576914.3328353864, 172560.3639169473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1420800.0000, 
sim time next is 1421400.0000, 
raw observation next is [23.33333333333333, 88.16666666666667, 1.0, 2.0, 0.3777673281179953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 570617.3028294251, 570617.3028294244, 172091.4384280974], 
processed observation next is [0.0, 0.43478260869565216, 0.30489731437598716, 0.8816666666666667, 1.0, 1.0, 0.25032208206987383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15850480634150696, 0.15850480634150677, 0.2568528931762648], 
reward next is 0.7431, 
noisyNet noise sample is [array([1.0450244], dtype=float32), 0.5229434]. 
=============================================
[2019-03-26 23:15:29,157] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2156713: loss 0.9266
[2019-03-26 23:15:29,160] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2156713: learning rate 0.0010
[2019-03-26 23:15:30,706] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:30,719] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1595
[2019-03-26 23:15:30,724] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.56666666666667, 93.66666666666667, 1.0, 2.0, 0.7483828928246812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1132725.695171514, 1132725.695171513, 242998.8884753696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1338000.0000, 
sim time next is 1338600.0000, 
raw observation next is [22.48333333333333, 93.33333333333333, 1.0, 2.0, 0.7187401081279726, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1092218.434412109, 1092218.434412109, 236227.0199749477], 
processed observation next is [1.0, 0.4782608695652174, 0.26461295418641384, 0.9333333333333332, 1.0, 1.0, 0.6611326603951477, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30339400955891915, 0.30339400955891915, 0.3525776417536533], 
reward next is 0.6474, 
noisyNet noise sample is [array([0.07194799], dtype=float32), -0.54100883]. 
=============================================
[2019-03-26 23:15:30,844] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:30,846] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0018
[2019-03-26 23:15:30,851] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.41666666666667, 85.00000000000001, 1.0, 2.0, 0.3914057450573952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598482.06138325, 598482.0613832506, 174773.1832725219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1584600.0000, 
sim time next is 1585200.0000, 
raw observation next is [23.43333333333333, 85.0, 1.0, 2.0, 0.4809180021870427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 734962.9368181376, 734962.9368181371, 188286.2661040828], 
processed observation next is [1.0, 0.34782608695652173, 0.30963665086887826, 0.85, 1.0, 1.0, 0.3746000026349912, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20415637133837156, 0.20415637133837142, 0.2810242777672878], 
reward next is 0.7190, 
noisyNet noise sample is [array([0.55576736], dtype=float32), 0.041972395]. 
=============================================
[2019-03-26 23:15:31,347] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2157690: loss 0.0434
[2019-03-26 23:15:31,348] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2157690: learning rate 0.0010
[2019-03-26 23:15:31,586] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2157796: loss 0.0587
[2019-03-26 23:15:31,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2157796: learning rate 0.0010
[2019-03-26 23:15:34,255] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2158994: loss 0.0330
[2019-03-26 23:15:34,259] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2158994: learning rate 0.0010
[2019-03-26 23:15:35,200] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2159421: loss 0.0602
[2019-03-26 23:15:35,201] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2159421: learning rate 0.0010
[2019-03-26 23:15:36,733] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2160105: loss 0.0212
[2019-03-26 23:15:36,737] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2160106: learning rate 0.0010
[2019-03-26 23:15:37,864] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:37,869] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1410
[2019-03-26 23:15:37,874] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 51.0, 1.0, 2.0, 0.3406595343048348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526195.087827559, 526195.087827559, 168738.9939266162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1508400.0000, 
sim time next is 1509000.0000, 
raw observation next is [28.85, 51.0, 1.0, 2.0, 0.3404076519279594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525745.5316471308, 525745.5316471303, 168701.1901762203], 
processed observation next is [0.0, 0.4782608695652174, 0.5663507109004741, 0.51, 1.0, 1.0, 0.20531042400958965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14604042545753634, 0.14604042545753618, 0.25179282115853774], 
reward next is 0.7482, 
noisyNet noise sample is [array([0.83986294], dtype=float32), 0.73560417]. 
=============================================
[2019-03-26 23:15:37,899] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[76.31601 ]
 [76.13838 ]
 [76.05867 ]
 [75.974335]
 [75.88135 ]], R is [[76.30297852]
 [76.2881012 ]
 [76.27285767]
 [76.25734711]
 [76.24171448]].
[2019-03-26 23:15:38,089] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2160711: loss 0.0314
[2019-03-26 23:15:38,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2160711: learning rate 0.0010
[2019-03-26 23:15:38,556] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2160917: loss 0.0213
[2019-03-26 23:15:38,557] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2160918: learning rate 0.0010
[2019-03-26 23:15:40,279] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2161691: loss 0.0159
[2019-03-26 23:15:40,280] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2161691: learning rate 0.0010
[2019-03-26 23:15:40,552] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2161811: loss 0.0266
[2019-03-26 23:15:40,555] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2161812: learning rate 0.0010
[2019-03-26 23:15:41,052] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:41,060] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6622
[2019-03-26 23:15:41,065] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 98.0, 1.0, 2.0, 0.4493481692295083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 177726.5250660207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1669200.0000, 
sim time next is 1669800.0000, 
raw observation next is [23.68333333333333, 98.0, 1.0, 2.0, 0.4636623486231898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660898.7693431445, 660898.7693431451, 179770.3064371699], 
processed observation next is [1.0, 0.30434782608695654, 0.32148499210110576, 0.98, 1.0, 1.0, 0.3538100585821563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18358299148420681, 0.18358299148420695, 0.2683138902047312], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.6002788], dtype=float32), -1.4591079]. 
=============================================
[2019-03-26 23:15:41,075] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2162048: loss 0.0377
[2019-03-26 23:15:41,083] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2162049: learning rate 0.0010
[2019-03-26 23:15:43,687] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2163201: loss 0.0101
[2019-03-26 23:15:43,690] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2163202: learning rate 0.0010
[2019-03-26 23:15:44,256] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2163452: loss 0.0348
[2019-03-26 23:15:44,258] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2163453: learning rate 0.0010
[2019-03-26 23:15:44,297] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:44,307] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1479
[2019-03-26 23:15:44,313] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 61.0, 1.0, 2.0, 0.357913139627097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 547127.1069043688, 547127.1069043695, 170280.8052184762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1504800.0000, 
sim time next is 1505400.0000, 
raw observation next is [27.46666666666667, 59.33333333333334, 1.0, 2.0, 0.3557121950948179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544564.9627604837, 544564.9627604837, 170091.8296220152], 
processed observation next is [0.0, 0.43478260869565216, 0.500789889415482, 0.5933333333333334, 1.0, 1.0, 0.2237496326443589, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15126804521124548, 0.15126804521124548, 0.25386840242091824], 
reward next is 0.7461, 
noisyNet noise sample is [array([0.315872], dtype=float32), -0.13363314]. 
=============================================
[2019-03-26 23:15:44,378] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2163510: loss 0.0025
[2019-03-26 23:15:44,384] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2163511: learning rate 0.0010
[2019-03-26 23:15:44,709] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2163660: loss 0.0144
[2019-03-26 23:15:44,714] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2163661: learning rate 0.0010
[2019-03-26 23:15:45,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4545293e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:15:45,114] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9435
[2019-03-26 23:15:45,119] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.66666666666667, 1.0, 2.0, 0.4158999286635896, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611613.6734360954, 611613.6734360954, 175368.4103524756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1629600.0000, 
sim time next is 1630200.0000, 
raw observation next is [23.2, 95.83333333333333, 1.0, 2.0, 0.4164248089475659, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611907.3553737071, 611907.3553737071, 175382.6263626735], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.9583333333333333, 1.0, 1.0, 0.2968973601777902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1699742653815853, 0.1699742653815853, 0.26176511397413954], 
reward next is 0.7382, 
noisyNet noise sample is [array([-0.37243584], dtype=float32), 0.10594404]. 
=============================================
[2019-03-26 23:15:45,286] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2163913: loss 0.0153
[2019-03-26 23:15:45,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2163913: learning rate 0.0010
[2019-03-26 23:15:47,321] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2164824: loss 0.0705
[2019-03-26 23:15:47,323] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2164824: learning rate 0.0010
[2019-03-26 23:15:49,202] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2165670: loss 0.0035
[2019-03-26 23:15:49,208] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2165670: learning rate 0.0010
[2019-03-26 23:15:49,578] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2165839: loss 0.0016
[2019-03-26 23:15:49,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2165840: learning rate 0.0010
[2019-03-26 23:15:52,170] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2167000: loss 0.0176
[2019-03-26 23:15:52,172] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2167000: learning rate 0.0010
[2019-03-26 23:15:53,151] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2167437: loss 0.0101
[2019-03-26 23:15:53,152] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2167437: learning rate 0.0010
[2019-03-26 23:15:54,622] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2168103: loss 0.0066
[2019-03-26 23:15:54,624] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2168103: learning rate 0.0010
[2019-03-26 23:15:55,813] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2168635: loss 0.0068
[2019-03-26 23:15:55,816] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2168636: learning rate 0.0010
[2019-03-26 23:15:56,327] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:56,331] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6864
[2019-03-26 23:15:56,337] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 86.0, 1.0, 2.0, 0.5067137639541339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 708056.8846347855, 708056.8846347849, 184654.8703867525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1713600.0000, 
sim time next is 1714200.0000, 
raw observation next is [26.65, 86.5, 1.0, 2.0, 0.5082943590738291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710266.2688861516, 710266.2688861516, 184905.9869181053], 
processed observation next is [1.0, 0.8695652173913043, 0.462085308056872, 0.865, 1.0, 1.0, 0.4075835651491917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19729618580170877, 0.19729618580170877, 0.275979084952396], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.4150301], dtype=float32), -0.77510524]. 
=============================================
[2019-03-26 23:15:56,777] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2169067: loss 0.0008
[2019-03-26 23:15:56,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2169067: learning rate 0.0010
[2019-03-26 23:15:57,587] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:15:57,597] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6887
[2019-03-26 23:15:57,602] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.66666666666667, 1.0, 2.0, 0.460793783209259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651413.0798743238, 651413.0798743238, 178654.516414052], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1903800.0000, 
sim time next is 1904400.0000, 
raw observation next is [24.3, 95.0, 1.0, 2.0, 0.4625432617026303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652647.5764196299, 652647.5764196299, 178752.9072299544], 
processed observation next is [1.0, 0.043478260869565216, 0.3507109004739337, 0.95, 1.0, 1.0, 0.35246176108750643, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1812909934498972, 0.1812909934498972, 0.2667953839253051], 
reward next is 0.7332, 
noisyNet noise sample is [array([0.03290909], dtype=float32), -0.102912426]. 
=============================================
[2019-03-26 23:15:58,035] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2169632: loss 0.0572
[2019-03-26 23:15:58,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2169632: learning rate 0.0010
[2019-03-26 23:15:58,574] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2169871: loss 0.0016
[2019-03-26 23:15:58,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2169872: learning rate 0.0010
[2019-03-26 23:15:58,809] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2169966: loss 0.0141
[2019-03-26 23:15:58,811] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2169967: learning rate 0.0010
[2019-03-26 23:16:01,357] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2171106: loss 0.0088
[2019-03-26 23:16:01,361] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2171107: learning rate 0.0010
[2019-03-26 23:16:02,071] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2171426: loss 0.0013
[2019-03-26 23:16:02,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2171426: learning rate 0.0010
[2019-03-26 23:16:02,383] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2171559: loss 0.0026
[2019-03-26 23:16:02,387] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2171560: learning rate 0.0010
[2019-03-26 23:16:02,472] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2171603: loss 0.0062
[2019-03-26 23:16:02,479] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2171604: learning rate 0.0010
[2019-03-26 23:16:02,966] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2171820: loss 0.0014
[2019-03-26 23:16:02,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2171820: learning rate 0.0010
[2019-03-26 23:16:04,987] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2172733: loss 0.0018
[2019-03-26 23:16:04,994] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2172735: learning rate 0.0010
[2019-03-26 23:16:07,011] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2173638: loss 0.0018
[2019-03-26 23:16:07,013] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2173638: learning rate 0.0010
[2019-03-26 23:16:07,297] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2173766: loss 0.0045
[2019-03-26 23:16:07,298] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2173766: learning rate 0.0010
[2019-03-26 23:16:08,702] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.0301656e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 23:16:08,709] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0517
[2019-03-26 23:16:08,713] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.914983226132993, 6.9112, 168.9125319223211, 1486490.029086788, 1483806.085005962, 316152.0432385501], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1934400.0000, 
sim time next is 1935000.0000, 
raw observation next is [25.95, 81.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.051251240683174, 6.9112, 168.9120686360958, 1583186.047197754, 1483829.399126598, 316145.1216261043], 
processed observation next is [1.0, 0.391304347826087, 0.42890995260663506, 0.81, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.014005124068317443, 0.0, 0.8294355852828462, 0.4397739019993761, 0.4121748330907216, 0.4718583904867228], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09165016], dtype=float32), -0.45135742]. 
=============================================
[2019-03-26 23:16:08,742] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.697704]
 [58.186954]
 [59.54958 ]
 [59.597404]
 [59.789257]], R is [[57.31159592]
 [57.24769592]
 [56.71377945]
 [56.70538712]
 [56.70359802]].
[2019-03-26 23:16:09,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2829920e-38 1.0000000e+00 0.0000000e+00 3.1392461e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:16:09,456] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0226
[2019-03-26 23:16:09,463] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 76.33333333333334, 1.0, 2.0, 0.9135515312639516, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565026114, 1279372.389155146, 1279372.389155146, 274060.7430413824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1945200.0000, 
sim time next is 1945800.0000, 
raw observation next is [27.0, 76.0, 1.0, 2.0, 0.8595524681475536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510429, 1212077.767834585, 1212077.767834584, 260607.7232843322], 
processed observation next is [1.0, 0.5217391304347826, 0.4786729857819906, 0.76, 1.0, 1.0, 0.8307861062018718, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451522929, 0.33668826884294023, 0.33668826884294, 0.38896675117064505], 
reward next is 0.6110, 
noisyNet noise sample is [array([0.7714524], dtype=float32), 0.9389345]. 
=============================================
[2019-03-26 23:16:09,802] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2174888: loss 0.0022
[2019-03-26 23:16:09,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2174888: learning rate 0.0010
[2019-03-26 23:16:10,060] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 23:16:10,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:16:10,063] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:16:10,064] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:16:10,065] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:16:10,065] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:16:10,069] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:16:10,071] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:16:10,069] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:16:10,072] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:16:10,073] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:16:10,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-03-26 23:16:10,116] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-03-26 23:16:10,137] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-03-26 23:16:10,169] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-03-26 23:16:10,170] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-03-26 23:16:13,920] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:16:13,922] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.46298718, 95.51337135, 1.0, 2.0, 0.2869528360745866, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 465398.8921878293, 465398.8921878293, 164616.9114672383]
[2019-03-26 23:16:13,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:16:13,926] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6285565e-37 0.0000000e+00], sampled 0.46230184799685003
[2019-03-26 23:16:13,948] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:16:13,949] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.71666666666667, 77.5, 1.0, 2.0, 0.2515482453591628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 414440.4423740222, 414440.4423740229, 161043.543836468]
[2019-03-26 23:16:13,950] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:16:13,955] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 8.5190034e-38 0.0000000e+00], sampled 0.02808767556627889
[2019-03-26 23:16:36,812] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:16:36,813] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.7, 85.0, 1.0, 2.0, 0.4294143458402478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628798.4264270249, 628798.4264270255, 176948.3737707685]
[2019-03-26 23:16:36,815] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:16:36,817] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6629213e-37 0.0000000e+00], sampled 0.512417087821132
[2019-03-26 23:16:38,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:16:38,605] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.7, 95.0, 1.0, 2.0, 0.3979959721855331, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597332.0483119456, 597332.0483119456, 174389.881239449]
[2019-03-26 23:16:38,606] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:16:38,609] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4736437e-36 0.0000000e+00], sampled 0.6434907547308754
[2019-03-26 23:17:33,103] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:17:33,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.92144933, 89.36220644666668, 1.0, 2.0, 0.498280768022126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696469.0944685491, 696469.0944685485, 183350.7065653153]
[2019-03-26 23:17:33,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:17:33,106] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.1957986e-37 0.0000000e+00], sampled 0.28768030538229394
[2019-03-26 23:17:33,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9546218]
[2019-03-26 23:17:33,878] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.21666666666667, 62.66666666666666, 1.0, 2.0, 0.9000194444617586, 1.0, 1.0, 0.9000194444617586, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 2517371.852367327, 2517371.852367327, 471232.5774634933]
[2019-03-26 23:17:33,879] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:17:33,881] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3261271e-34 1.0000000e+00 0.0000000e+00 2.3958330e-20 0.0000000e+00], sampled 0.8821652518722012
[2019-03-26 23:17:33,882] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2517371.852367327 W.
[2019-03-26 23:18:04,684] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 23:18:04,702] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.1133 2843234548.3666 1131.0000
[2019-03-26 23:18:04,848] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7875.8585 3164735026.8284 1778.0000
[2019-03-26 23:18:04,878] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.9734 2928048931.2162 1338.0000
[2019-03-26 23:18:04,986] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.9641 2779935595.8501 933.0000
[2019-03-26 23:18:06,002] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2175000, evaluation results [2175000.0, 7875.858472249107, 3164735026.8283834, 1778.0, 8253.973399524697, 2928048931.2161584, 1338.0, 8658.964140262613, 2779935595.8500915, 933.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.11326267071, 2843234548.366585, 1131.0]
[2019-03-26 23:18:06,702] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.7918310e-24 9.9999857e-01 6.3023969e-29 1.4401569e-06 3.5210470e-35], sum to 1.0000
[2019-03-26 23:18:06,709] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9371
[2019-03-26 23:18:06,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2278426.338813884 W.
[2019-03-26 23:18:06,724] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 64.0, 1.0, 2.0, 0.5431173265026401, 1.0, 2.0, 0.5431173265026401, 1.0, 2.0, 0.9432151844905846, 6.911200000000001, 6.9112, 170.5573041426782, 2278426.338813884, 2278426.338813883, 446243.3711822697], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2306400.0000, 
sim time next is 2307000.0000, 
raw observation next is [32.36666666666667, 64.0, 1.0, 2.0, 0.8236172517183842, 1.0, 2.0, 0.8236172517183842, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2303455.652983228, 2303455.652983229, 431536.0770380599], 
processed observation next is [1.0, 0.6956521739130435, 0.7330173775671407, 0.64, 1.0, 1.0, 0.7874906647209448, 1.0, 1.0, 0.7874906647209448, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6398487924953412, 0.6398487924953414, 0.6440836970717312], 
reward next is 0.3559, 
noisyNet noise sample is [array([2.8156185], dtype=float32), -0.67637724]. 
=============================================
[2019-03-26 23:18:06,738] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[39.353207]
 [39.442368]
 [39.33931 ]
 [39.34106 ]
 [41.4191  ]], R is [[40.48467636]
 [40.41379547]
 [40.36258316]
 [40.29336548]
 [39.89043045]].
[2019-03-26 23:18:07,048] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2175473: loss 0.0008
[2019-03-26 23:18:07,050] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2175473: learning rate 0.0010
[2019-03-26 23:18:08,414] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2176085: loss 0.0009
[2019-03-26 23:18:08,419] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2176087: learning rate 0.0010
[2019-03-26 23:18:09,129] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:18:09,138] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1772
[2019-03-26 23:18:09,146] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.85, 92.33333333333334, 1.0, 2.0, 0.4743413212499119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 662807.1135258719, 662807.1135258725, 179673.2635288055], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2067000.0000, 
sim time next is 2067600.0000, 
raw observation next is [24.8, 92.66666666666667, 1.0, 2.0, 0.4732750241149288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 661416.11784795, 661416.11784795, 179527.3883171952], 
processed observation next is [0.0, 0.9565217391304348, 0.3744075829383887, 0.9266666666666667, 1.0, 1.0, 0.36539159531919135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18372669940220832, 0.18372669940220832, 0.26795132584656], 
reward next is 0.7320, 
noisyNet noise sample is [array([-0.28526855], dtype=float32), -0.5383466]. 
=============================================
[2019-03-26 23:18:09,593] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2176609: loss 0.0136
[2019-03-26 23:18:09,596] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2176609: learning rate 0.0010
[2019-03-26 23:18:10,081] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:18:10,092] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1556
[2019-03-26 23:18:10,098] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 76.0, 1.0, 2.0, 0.5622836267046358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785736.1964479298, 785736.1964479298, 193930.428104229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2120400.0000, 
sim time next is 2121000.0000, 
raw observation next is [30.0, 76.16666666666667, 1.0, 2.0, 0.5627679389941809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786413.2258867561, 786413.2258867561, 194015.3221651925], 
processed observation next is [0.0, 0.5652173913043478, 0.6208530805687204, 0.7616666666666667, 1.0, 1.0, 0.47321438433033836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2184481183018767, 0.2184481183018767, 0.28957510770924255], 
reward next is 0.7104, 
noisyNet noise sample is [array([1.190044], dtype=float32), -0.17518653]. 
=============================================
[2019-03-26 23:18:10,110] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.934265]
 [74.83149 ]
 [74.824844]
 [74.8136  ]
 [74.79327 ]], R is [[74.85504913]
 [74.81704712]
 [74.7795105 ]
 [74.74253082]
 [74.70631409]].
[2019-03-26 23:18:11,238] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2177343: loss 0.0235
[2019-03-26 23:18:11,241] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2177343: learning rate 0.0010
[2019-03-26 23:18:11,725] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2177564: loss 0.0060
[2019-03-26 23:18:11,727] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2177565: learning rate 0.0010
[2019-03-26 23:18:12,516] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2177913: loss 0.0110
[2019-03-26 23:18:12,519] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2177913: learning rate 0.0010
[2019-03-26 23:18:13,225] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2178224: loss 0.0467
[2019-03-26 23:18:13,227] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2178224: learning rate 0.0010
[2019-03-26 23:18:15,001] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2179020: loss 0.0008
[2019-03-26 23:18:15,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2179021: learning rate 0.0010
[2019-03-26 23:18:15,706] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2179338: loss 0.0068
[2019-03-26 23:18:15,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2179339: learning rate 0.0010
[2019-03-26 23:18:16,087] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2179507: loss 0.0081
[2019-03-26 23:18:16,091] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2179508: learning rate 0.0010
[2019-03-26 23:18:16,413] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2179654: loss 0.0014
[2019-03-26 23:18:16,416] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2179656: learning rate 0.0010
[2019-03-26 23:18:16,660] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.2762341e-30 1.0000000e+00 1.7132316e-34 4.8242275e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:16,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4274
[2019-03-26 23:18:16,679] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1927512.023942027 W.
[2019-03-26 23:18:16,684] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.9, 64.0, 1.0, 2.0, 0.459541695647545, 1.0, 2.0, 0.459541695647545, 1.0, 2.0, 0.7980719525787552, 6.9112, 6.9112, 170.5573041426782, 1927512.023942027, 1927512.023942027, 388295.8126663791], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2293200.0000, 
sim time next is 2293800.0000, 
raw observation next is [31.88333333333333, 64.16666666666667, 1.0, 2.0, 0.4110871988136197, 1.0, 2.0, 0.4110871988136197, 1.0, 2.0, 0.7139225157251932, 6.911199999999999, 6.9112, 170.5573041426782, 1724109.879161139, 1724109.879161139, 359110.7106805468], 
processed observation next is [1.0, 0.5652173913043478, 0.7101105845181673, 0.6416666666666667, 1.0, 1.0, 0.29046650459472256, 1.0, 1.0, 0.29046650459472256, 1.0, 1.0, 0.6511250191770649, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4789194108780942, 0.4789194108780942, 0.5359861353440998], 
reward next is 0.4640, 
noisyNet noise sample is [array([0.8604166], dtype=float32), -0.48348087]. 
=============================================
[2019-03-26 23:18:16,792] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2179828: loss 0.0124
[2019-03-26 23:18:16,795] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2179829: learning rate 0.0010
[2019-03-26 23:18:16,924] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3263985e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:16,932] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0054
[2019-03-26 23:18:16,935] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 94.0, 1.0, 2.0, 0.552070865847513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771459.6826550601, 771459.6826550601, 192156.6470472417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2503200.0000, 
sim time next is 2503800.0000, 
raw observation next is [26.75, 94.0, 1.0, 2.0, 0.551534643858489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770710.0982300511, 770710.0982300511, 192064.3922580315], 
processed observation next is [1.0, 1.0, 0.4668246445497631, 0.94, 1.0, 1.0, 0.4596802938054085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21408613839723642, 0.21408613839723642, 0.28666327202691266], 
reward next is 0.7133, 
noisyNet noise sample is [array([1.1708449], dtype=float32), 1.3695914]. 
=============================================
[2019-03-26 23:18:18,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6347592e-17 7.3264749e-04 3.8447739e-20 9.9926740e-01 1.4395211e-23], sum to 1.0000
[2019-03-26 23:18:18,431] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4565
[2019-03-26 23:18:18,436] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 66.0, 1.0, 2.0, 0.6201289388881844, 1.0, 2.0, 0.6201289388881844, 1.0, 1.0, 1.03, 6.963990683154925, 6.9112, 170.5573041426782, 2601833.367104813, 2564017.253373244, 495392.2647828822], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2208600.0000, 
sim time next is 2209200.0000, 
raw observation next is [31.9, 65.66666666666666, 1.0, 2.0, 0.877716367005347, 1.0, 2.0, 0.877716367005347, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2454906.239261363, 2454906.239261363, 459481.251139132], 
processed observation next is [1.0, 0.5652173913043478, 0.7109004739336492, 0.6566666666666666, 1.0, 1.0, 0.8526703216931892, 1.0, 1.0, 0.8526703216931892, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.681918399794823, 0.681918399794823, 0.6857929121479582], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.05903276], dtype=float32), 0.066952646]. 
=============================================
[2019-03-26 23:18:18,589] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2180619: loss 0.0084
[2019-03-26 23:18:18,592] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2180619: learning rate 0.0010
[2019-03-26 23:18:21,264] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2181811: loss 0.0636
[2019-03-26 23:18:21,266] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2181811: learning rate 0.0010
[2019-03-26 23:18:21,479] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2181910: loss 0.0538
[2019-03-26 23:18:21,481] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2181911: learning rate 0.0010
[2019-03-26 23:18:23,819] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2182952: loss 0.0744
[2019-03-26 23:18:23,823] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2182952: learning rate 0.0010
[2019-03-26 23:18:24,237] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5903849e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:24,246] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9368
[2019-03-26 23:18:24,251] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 85.0, 1.0, 2.0, 0.5227447013851051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730465.4300006226, 730465.430000622, 187236.4340996137], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2244000.0000, 
sim time next is 2244600.0000, 
raw observation next is [27.15, 85.0, 1.0, 2.0, 0.5202554624139636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726985.8629665411, 726985.8629665418, 186830.5868944517], 
processed observation next is [1.0, 1.0, 0.485781990521327, 0.85, 1.0, 1.0, 0.42199453302887174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20194051749070585, 0.20194051749070605, 0.2788516222305249], 
reward next is 0.7211, 
noisyNet noise sample is [array([-0.15555301], dtype=float32), -0.32198504]. 
=============================================
[2019-03-26 23:18:25,243] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2183585: loss 0.0117
[2019-03-26 23:18:25,247] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2183586: learning rate 0.0010
[2019-03-26 23:18:26,417] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2184115: loss 0.0016
[2019-03-26 23:18:26,420] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2184117: learning rate 0.0010
[2019-03-26 23:18:27,471] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2184588: loss 0.0148
[2019-03-26 23:18:27,477] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2184590: learning rate 0.0010
[2019-03-26 23:18:28,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1549422e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:28,897] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2370
[2019-03-26 23:18:28,903] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 100.0, 1.0, 2.0, 0.4503384064312532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.9493691779, 643133.9493691786, 177966.7730345486], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2709600.0000, 
sim time next is 2710200.0000, 
raw observation next is [23.16666666666667, 100.0, 1.0, 2.0, 0.4427102211633037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636735.7042807669, 636735.7042807676, 177437.2248689152], 
processed observation next is [0.0, 0.34782608695652173, 0.2969984202211693, 1.0, 1.0, 1.0, 0.3285665315220526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1768710289668797, 0.1768710289668799, 0.26483167890882864], 
reward next is 0.7352, 
noisyNet noise sample is [array([0.1261795], dtype=float32), 2.627957]. 
=============================================
[2019-03-26 23:18:29,000] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2185276: loss 0.0189
[2019-03-26 23:18:29,004] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2185276: learning rate 0.0010
[2019-03-26 23:18:29,687] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2185582: loss 0.2238
[2019-03-26 23:18:29,691] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2185584: learning rate 0.0010
[2019-03-26 23:18:30,031] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8060893e-22 1.0000000e+00 2.7891992e-27 1.5238282e-09 5.3602276e-34], sum to 1.0000
[2019-03-26 23:18:30,045] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6384
[2019-03-26 23:18:30,050] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1748825.386776997 W.
[2019-03-26 23:18:30,054] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.56666666666667, 72.66666666666666, 1.0, 2.0, 0.6254631338653431, 1.0, 2.0, 0.6254631338653431, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1748825.386776997, 1748825.386776997, 343846.4817092184], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2551200.0000, 
sim time next is 2551800.0000, 
raw observation next is [29.68333333333333, 71.83333333333334, 1.0, 2.0, 0.4139243759210203, 1.0, 2.0, 0.4139243759210203, 1.0, 1.0, 0.7127004253968818, 6.911200000000001, 6.9112, 170.5573041426782, 1736018.708751088, 1736018.708751087, 359834.2836298554], 
processed observation next is [1.0, 0.5217391304347826, 0.6058451816745655, 0.7183333333333334, 1.0, 1.0, 0.29388479026628955, 1.0, 1.0, 0.29388479026628955, 1.0, 0.5, 0.6496346651181486, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4822274190975244, 0.4822274190975242, 0.5370660949699334], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.98308325], dtype=float32), -0.59023887]. 
=============================================
[2019-03-26 23:18:30,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:18:30,529] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4723
[2019-03-26 23:18:30,534] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.475867988442044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664941.0253757458, 664941.0253757464, 179901.4110463232], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2703600.0000, 
sim time next is 2704200.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.4770851508227688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666642.3273552274, 666642.3273552274, 180083.5644885299], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36998210942502263, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18517842426534095, 0.18517842426534095, 0.26878143953511924], 
reward next is 0.7312, 
noisyNet noise sample is [array([0.8281728], dtype=float32), 0.9587427]. 
=============================================
[2019-03-26 23:18:30,561] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2185967: loss 0.0245
[2019-03-26 23:18:30,563] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2185967: learning rate 0.0010
[2019-03-26 23:18:30,946] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2186141: loss 0.0620
[2019-03-26 23:18:30,948] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2186141: learning rate 0.0010
[2019-03-26 23:18:32,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7236599e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:32,318] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9475
[2019-03-26 23:18:32,324] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.16666666666666, 72.33333333333333, 1.0, 2.0, 0.5792358332091772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 809434.2678781697, 809434.2678781691, 196942.0413811374], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2403600.0000, 
sim time next is 2404200.0000, 
raw observation next is [31.03333333333333, 73.16666666666667, 1.0, 2.0, 0.5830151288377925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814717.5470553037, 814717.547055303, 197624.7943381713], 
processed observation next is [1.0, 0.8260869565217391, 0.669826224328594, 0.7316666666666667, 1.0, 1.0, 0.49760858896119575, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22631042973758436, 0.22631042973758417, 0.2949623796092109], 
reward next is 0.7050, 
noisyNet noise sample is [array([0.19747625], dtype=float32), -0.1061068]. 
=============================================
[2019-03-26 23:18:32,485] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8141163e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:32,494] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8396
[2019-03-26 23:18:32,500] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 92.5, 1.0, 2.0, 0.46358690747478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 656448.3001028345, 656448.3001028338, 179205.0183926996], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2597400.0000, 
sim time next is 2598000.0000, 
raw observation next is [24.43333333333333, 92.33333333333333, 1.0, 2.0, 0.4589023605588368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652311.1562969316, 652311.1562969309, 178834.2932940064], 
processed observation next is [0.0, 0.043478260869565216, 0.3570300157977882, 0.9233333333333333, 1.0, 1.0, 0.3480751332034178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18119754341581432, 0.18119754341581412, 0.26691685566269613], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.82374406], dtype=float32), -0.59948826]. 
=============================================
[2019-03-26 23:18:32,527] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.89055]
 [74.67813]
 [74.54794]
 [74.62138]
 [74.5401 ]], R is [[75.11325836]
 [75.0946579 ]
 [75.07568359]
 [75.05634308]
 [75.03662109]].
[2019-03-26 23:18:32,761] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2186946: loss 0.1471
[2019-03-26 23:18:32,763] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2186946: learning rate 0.0010
[2019-03-26 23:18:33,552] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2187298: loss 0.0202
[2019-03-26 23:18:33,558] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2187299: learning rate 0.0010
[2019-03-26 23:18:33,847] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2187433: loss 0.0286
[2019-03-26 23:18:33,852] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2187433: learning rate 0.0010
[2019-03-26 23:18:34,391] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2187671: loss 0.0916
[2019-03-26 23:18:34,393] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2187671: learning rate 0.0010
[2019-03-26 23:18:34,515] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2187724: loss 0.0196
[2019-03-26 23:18:34,520] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2187725: learning rate 0.0010
[2019-03-26 23:18:36,523] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2188626: loss 0.0644
[2019-03-26 23:18:36,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2188626: learning rate 0.0010
[2019-03-26 23:18:37,702] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.136348e-20 0.000000e+00], sum to 1.0000
[2019-03-26 23:18:37,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4952
[2019-03-26 23:18:37,719] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 89.66666666666667, 1.0, 2.0, 0.4503228765350923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642213.2562571305, 642213.2562571305, 177850.370632033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2616600.0000, 
sim time next is 2617200.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4573981071325827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648801.6628119738, 648801.6628119744, 178438.1370797681], 
processed observation next is [0.0, 0.30434782608695654, 0.38388625592417064, 0.89, 1.0, 1.0, 0.34626277967781055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18022268411443718, 0.18022268411443734, 0.26632557773099713], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.51692736], dtype=float32), -0.5801721]. 
=============================================
[2019-03-26 23:18:39,071] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2189762: loss 0.0247
[2019-03-26 23:18:39,073] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2189762: learning rate 0.0010
[2019-03-26 23:18:39,205] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7513072e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:39,221] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6717
[2019-03-26 23:18:39,225] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 91.0, 1.0, 2.0, 0.5827558170493199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 894439.6703646185, 894439.6703646185, 207286.7940517343], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2889000.0000, 
sim time next is 2889600.0000, 
raw observation next is [22.56666666666667, 90.66666666666667, 1.0, 2.0, 0.621225282094197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 952801.9272920782, 952801.9272920782, 215142.7409349323], 
processed observation next is [1.0, 0.43478260869565216, 0.26856240126382325, 0.9066666666666667, 1.0, 1.0, 0.5436449181857795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26466720202557725, 0.26466720202557725, 0.32110856855960046], 
reward next is 0.6789, 
noisyNet noise sample is [array([-0.613448], dtype=float32), -2.1094463]. 
=============================================
[2019-03-26 23:18:39,446] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2189925: loss 0.0318
[2019-03-26 23:18:39,448] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2189925: learning rate 0.0010
[2019-03-26 23:18:40,814] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2776064e-14 9.4817281e-01 2.0620111e-17 5.1827185e-02 2.2692969e-22], sum to 1.0000
[2019-03-26 23:18:40,823] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3859
[2019-03-26 23:18:40,829] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2169975.017893974 W.
[2019-03-26 23:18:40,835] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.28333333333333, 82.33333333333334, 1.0, 2.0, 0.775933886803608, 1.0, 2.0, 0.775933886803608, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2169975.017893974, 2169975.017893974, 408323.9518030626], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2479800.0000, 
sim time next is 2480400.0000, 
raw observation next is [28.4, 82.0, 1.0, 2.0, 0.7388032227007859, 1.0, 2.0, 0.7388032227007859, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2066035.345001784, 2066035.345001784, 391178.4672257247], 
processed observation next is [1.0, 0.7391304347826086, 0.5450236966824644, 0.82, 1.0, 1.0, 0.6853050875913083, 1.0, 1.0, 0.6853050875913083, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5738987069449399, 0.5738987069449399, 0.5838484585458578], 
reward next is 0.4162, 
noisyNet noise sample is [array([0.0311249], dtype=float32), -0.3904631]. 
=============================================
[2019-03-26 23:18:41,791] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2190973: loss 0.1002
[2019-03-26 23:18:41,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2190976: learning rate 0.0010
[2019-03-26 23:18:42,103] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.761519e-31 0.000000e+00], sum to 1.0000
[2019-03-26 23:18:42,111] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3656
[2019-03-26 23:18:42,117] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3959294546072412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590785.4356171512, 590785.4356171512, 173689.7185016228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2673600.0000, 
sim time next is 2674200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3960511450640312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590966.8437370453, 590966.8437370453, 173706.3800040062], 
processed observation next is [0.0, 0.9565217391304348, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27235077718557976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16415745659362369, 0.16415745659362369, 0.2592632537373227], 
reward next is 0.7407, 
noisyNet noise sample is [array([0.30469653], dtype=float32), 0.7948298]. 
=============================================
[2019-03-26 23:18:42,769] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1638699e-13 1.0361781e-03 4.7450346e-16 9.9896383e-01 4.7312730e-19], sum to 1.0000
[2019-03-26 23:18:42,783] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1944
[2019-03-26 23:18:42,789] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666667, 69.33333333333333, 1.0, 2.0, 0.4969851911528562, 1.0, 1.0, 0.4969851911528562, 1.0, 2.0, 0.8570536874918137, 6.9112, 6.9112, 170.5573041426782, 2084718.673509038, 2084718.673509038, 411981.8534458398], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2558400.0000, 
sim time next is 2559000.0000, 
raw observation next is [30.08333333333333, 69.66666666666667, 1.0, 2.0, 0.7512903835423326, 1.0, 2.0, 0.7512903835423326, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2100989.463191955, 2100989.463191955, 396851.7377542303], 
processed observation next is [1.0, 0.6086956521739131, 0.6248025276461293, 0.6966666666666668, 1.0, 1.0, 0.7003498596895573, 1.0, 1.0, 0.7003498596895573, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5836081842199875, 0.5836081842199875, 0.5923160264988512], 
reward next is 0.4077, 
noisyNet noise sample is [array([-1.2358193], dtype=float32), -0.4333458]. 
=============================================
[2019-03-26 23:18:42,803] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[31.085785]
 [31.695576]
 [32.010796]
 [31.621637]
 [31.13293 ]], R is [[31.06473923]
 [31.13919258]
 [30.82864952]
 [30.53427887]
 [30.69685745]].
[2019-03-26 23:18:43,274] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2191632: loss 0.0915
[2019-03-26 23:18:43,276] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2191633: learning rate 0.0010
[2019-03-26 23:18:43,311] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.768531e-32 0.000000e+00], sum to 1.0000
[2019-03-26 23:18:43,324] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8428
[2019-03-26 23:18:43,328] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 99.0, 1.0, 2.0, 0.308800462644727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491248.2889387395, 491248.2889387395, 166428.6719962212], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2924400.0000, 
sim time next is 2925000.0000, 
raw observation next is [20.25, 98.5, 1.0, 2.0, 0.3098181933047057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 492572.1679135509, 492572.1679135509, 166521.3419519773], 
processed observation next is [1.0, 0.8695652173913043, 0.1587677725118484, 0.985, 1.0, 1.0, 0.1684556545839828, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13682560219820858, 0.13682560219820858, 0.24853931634623475], 
reward next is 0.7515, 
noisyNet noise sample is [array([-1.8552998], dtype=float32), 0.32088283]. 
=============================================
[2019-03-26 23:18:43,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[73.706184]
 [73.480644]
 [73.53922 ]
 [73.111305]
 [72.86207 ]], R is [[73.85904694]
 [73.87206268]
 [73.88496399]
 [73.8977356 ]
 [73.91017151]].
[2019-03-26 23:18:44,310] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2192090: loss 0.0218
[2019-03-26 23:18:44,311] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2192091: learning rate 0.0010
[2019-03-26 23:18:45,479] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2192618: loss 0.0707
[2019-03-26 23:18:45,480] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2192619: learning rate 0.0010
[2019-03-26 23:18:45,900] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4780496e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:45,910] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6276
[2019-03-26 23:18:45,917] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3969508427242628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592308.4251204082, 592308.4251204075, 173829.7584087286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2665800.0000, 
sim time next is 2666400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3968518920899767, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 592161.0428946876, 592161.042894687, 173816.1965682209], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2733155326385261, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16448917858185766, 0.16448917858185752, 0.25942715905704616], 
reward next is 0.7406, 
noisyNet noise sample is [array([-0.19856113], dtype=float32), 0.4580032]. 
=============================================
[2019-03-26 23:18:46,394] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2193023: loss 0.1949
[2019-03-26 23:18:46,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2193023: learning rate 0.0010
[2019-03-26 23:18:46,409] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2118626e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:46,421] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8674
[2019-03-26 23:18:46,426] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.34953729368586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538453.7168682074, 538453.7168682081, 169689.5798246755], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2763600.0000, 
sim time next is 2764200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3498789882288919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538979.1598952005, 538979.1598952012, 169732.6897413997], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.94, 1.0, 1.0, 0.21672167256493, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971643330422235, 0.14971643330422255, 0.25333237274835774], 
reward next is 0.7467, 
noisyNet noise sample is [array([1.8378785], dtype=float32), -0.07629345]. 
=============================================
[2019-03-26 23:18:47,779] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2193646: loss 0.1104
[2019-03-26 23:18:47,781] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2193648: learning rate 0.0010
[2019-03-26 23:18:48,437] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2193946: loss 0.1070
[2019-03-26 23:18:48,439] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2193946: learning rate 0.0010
[2019-03-26 23:18:48,653] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2194042: loss 0.1651
[2019-03-26 23:18:48,660] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2194044: learning rate 0.0010
[2019-03-26 23:18:51,054] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2195117: loss 0.0392
[2019-03-26 23:18:51,056] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2195119: learning rate 0.0010
[2019-03-26 23:18:51,953] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2195518: loss 0.0279
[2019-03-26 23:18:51,958] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2195519: learning rate 0.0010
[2019-03-26 23:18:51,989] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2195537: loss 0.0296
[2019-03-26 23:18:51,992] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2195537: learning rate 0.0010
[2019-03-26 23:18:52,114] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2195592: loss 0.0100
[2019-03-26 23:18:52,116] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2195593: learning rate 0.0010
[2019-03-26 23:18:52,479] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2195753: loss 0.0440
[2019-03-26 23:18:52,485] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2195754: learning rate 0.0010
[2019-03-26 23:18:54,845] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2196801: loss 0.1077
[2019-03-26 23:18:54,850] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2196804: learning rate 0.0010
[2019-03-26 23:18:56,160] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2986412e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:18:56,168] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9583
[2019-03-26 23:18:56,172] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8386034150715587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1216916.286248577, 1216916.286248577, 259959.349771019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3078000.0000, 
sim time next is 3078600.0000, 
raw observation next is [23.16666666666667, 99.00000000000001, 1.0, 2.0, 0.7888085287301458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1143192.121185025, 1143192.121185024, 246814.0474492084], 
processed observation next is [1.0, 0.6521739130434783, 0.2969984202211693, 0.9900000000000001, 1.0, 1.0, 0.7455524442531878, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31755336699584025, 0.31755336699584, 0.36837917529732594], 
reward next is 0.6316, 
noisyNet noise sample is [array([0.06407663], dtype=float32), -0.7025259]. 
=============================================
[2019-03-26 23:18:56,851] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2197702: loss 0.0080
[2019-03-26 23:18:56,853] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2197702: learning rate 0.0010
[2019-03-26 23:18:57,181] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2197850: loss 0.0143
[2019-03-26 23:18:57,187] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2197853: learning rate 0.0010
[2019-03-26 23:18:59,555] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2198918: loss 0.0149
[2019-03-26 23:18:59,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2198918: learning rate 0.0010
[2019-03-26 23:19:01,172] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2199644: loss 0.0319
[2019-03-26 23:19:01,174] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2199644: learning rate 0.0010
[2019-03-26 23:19:01,965] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 23:19:01,968] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:19:01,969] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:19:01,969] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:19:01,970] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:19:01,970] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:19:01,971] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:19:01,970] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:19:01,972] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:19:01,973] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:19:01,973] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:19:02,001] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-03-26 23:19:02,026] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-03-26 23:19:02,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-03-26 23:19:02,071] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-03-26 23:19:02,100] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-03-26 23:19:19,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9786278]
[2019-03-26 23:19:19,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.43333333333333, 88.83333333333334, 1.0, 2.0, 0.2775245645776383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 449587.4021602792, 449587.4021602799, 163561.8796473338]
[2019-03-26 23:19:19,730] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:19:19,734] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.3369992e-29 0.0000000e+00], sampled 0.2021661699121896
[2019-03-26 23:20:21,705] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.9786278]
[2019-03-26 23:20:21,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.2, 83.0, 1.0, 2.0, 0.9913371706984299, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992896755798, 6.9112, 168.9123159437557, 2282876.885518054, 2215627.797068689, 460640.5435557585]
[2019-03-26 23:20:21,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:20:21,712] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6363751e-25 1.0000000e+00 7.5963800e-30 4.2917445e-12 1.0717275e-36], sampled 0.3523040032191215
[2019-03-26 23:20:21,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2282876.885518054 W.
[2019-03-26 23:20:56,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 23:20:56,367] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 23:20:56,476] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0319 2779830318.5638 932.0000
[2019-03-26 23:20:56,591] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.0753 3164427800.2261 1778.0000
[2019-03-26 23:20:56,628] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 23:20:57,649] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2200000, evaluation results [2200000.0, 7876.075266015484, 3164427800.2260675, 1778.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8660.03188781958, 2779830318.5637875, 932.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
[2019-03-26 23:20:57,933] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2200132: loss 0.0315
[2019-03-26 23:20:57,938] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2200133: learning rate 0.0010
[2019-03-26 23:20:59,039] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2200630: loss 0.0176
[2019-03-26 23:20:59,042] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2200630: learning rate 0.0010
[2019-03-26 23:20:59,932] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2201032: loss 0.0045
[2019-03-26 23:20:59,933] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2201032: learning rate 0.0010
[2019-03-26 23:21:01,167] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2201588: loss 0.0103
[2019-03-26 23:21:01,172] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2201589: learning rate 0.0010
[2019-03-26 23:21:02,006] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2201964: loss 0.0286
[2019-03-26 23:21:02,008] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2201966: learning rate 0.0010
[2019-03-26 23:21:02,163] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2202032: loss 0.0215
[2019-03-26 23:21:02,166] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2202032: learning rate 0.0010
[2019-03-26 23:21:03,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 8.27948e-24 0.00000e+00], sum to 1.0000
[2019-03-26 23:21:03,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5924
[2019-03-26 23:21:03,755] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3907757072072625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584246.9615002999, 584246.9615002999, 173127.884081503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3109800.0000, 
sim time next is 3110400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3896586440802436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581716.8524090784, 581716.8524090784, 172871.7893211565], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26464896877137783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16158801455807734, 0.16158801455807734, 0.25801759600172613], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.09724151], dtype=float32), -0.5203422]. 
=============================================
[2019-03-26 23:21:04,627] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2203130: loss 0.0112
[2019-03-26 23:21:04,630] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2203131: learning rate 0.0010
[2019-03-26 23:21:04,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.552641e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:21:04,704] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4713
[2019-03-26 23:21:04,711] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 89.0, 1.0, 2.0, 0.6362739479298782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 965837.7640789308, 965837.7640789314, 217252.5311161857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3143400.0000, 
sim time next is 3144000.0000, 
raw observation next is [23.33333333333334, 89.0, 1.0, 2.0, 0.6353656039245327, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959454.6949367183, 959454.6949367183, 216482.4968872991], 
processed observation next is [1.0, 0.391304347826087, 0.3048973143759877, 0.89, 1.0, 1.0, 0.5606814505114851, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2665151930379773, 0.2665151930379773, 0.3231082043094016], 
reward next is 0.6769, 
noisyNet noise sample is [array([1.7473934], dtype=float32), -0.4604433]. 
=============================================
[2019-03-26 23:21:04,730] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[69.033325]
 [68.76009 ]
 [68.36935 ]
 [68.57216 ]
 [68.901276]], R is [[69.08660889]
 [69.07148743]
 [69.04898834]
 [69.01792908]
 [68.9912262 ]].
[2019-03-26 23:21:05,440] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2203491: loss 0.0129
[2019-03-26 23:21:05,442] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2203492: learning rate 0.0010
[2019-03-26 23:21:05,656] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2203588: loss 0.0275
[2019-03-26 23:21:05,661] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2203589: learning rate 0.0010
[2019-03-26 23:21:05,668] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2203592: loss 0.0239
[2019-03-26 23:21:05,673] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2203592: learning rate 0.0010
[2019-03-26 23:21:05,906] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2203696: loss 0.0113
[2019-03-26 23:21:05,907] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2203696: learning rate 0.0010
[2019-03-26 23:21:05,935] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.170966e-35 0.000000e+00], sum to 1.0000
[2019-03-26 23:21:05,945] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9018
[2019-03-26 23:21:05,952] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3510038208817862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542613.456095416, 542613.456095416, 170085.9618704006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3126600.0000, 
sim time next is 3127200.0000, 
raw observation next is [21.33333333333334, 98.0, 1.0, 2.0, 0.3470690678087817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537272.8732681094, 537272.8732681094, 169668.783994089], 
processed observation next is [1.0, 0.17391304347826086, 0.2101105845181678, 0.98, 1.0, 1.0, 0.2133362262756406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14924246479669706, 0.14924246479669706, 0.2532369910359537], 
reward next is 0.7468, 
noisyNet noise sample is [array([-1.3462876], dtype=float32), -0.26843995]. 
=============================================
[2019-03-26 23:21:06,572] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4647736e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:06,586] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3308
[2019-03-26 23:21:06,592] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.5159348702410322, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820788.1294222798, 820788.1294222791, 197403.8990551119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2984400.0000, 
sim time next is 2985000.0000, 
raw observation next is [20.16666666666667, 99.00000000000001, 1.0, 2.0, 0.4749872591779489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755568.6569742446, 755568.6569742446, 190168.9957770277], 
processed observation next is [1.0, 0.5652173913043478, 0.15481832543443946, 0.9900000000000001, 1.0, 1.0, 0.36745452913005894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20988018249284573, 0.20988018249284573, 0.28383432205526526], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.03858988], dtype=float32), -0.28068164]. 
=============================================
[2019-03-26 23:21:06,609] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.9855 ]
 [74.54341]
 [74.36135]
 [74.20923]
 [73.48446]], R is [[74.47470856]
 [74.43533325]
 [74.41012573]
 [74.38380432]
 [74.3568573 ]].
[2019-03-26 23:21:08,218] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2204724: loss 0.0124
[2019-03-26 23:21:08,222] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2204725: learning rate 0.0010
[2019-03-26 23:21:10,302] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2205653: loss 0.0340
[2019-03-26 23:21:10,306] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2205655: learning rate 0.0010
[2019-03-26 23:21:10,434] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.2577653e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:10,442] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6511
[2019-03-26 23:21:10,446] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5521977402739913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771637.0404237891, 771637.0404237891, 192178.3293658529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3266400.0000, 
sim time next is 3267000.0000, 
raw observation next is [28.5, 81.5, 1.0, 2.0, 0.5497492113644545, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768214.2469366328, 768214.2469366335, 191757.7863229321], 
processed observation next is [0.0, 0.8260869565217391, 0.5497630331753555, 0.815, 1.0, 1.0, 0.4575291703186199, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2133928463712869, 0.21339284637128708, 0.28620565122825686], 
reward next is 0.7138, 
noisyNet noise sample is [array([1.1193718], dtype=float32), 1.9185778]. 
=============================================
[2019-03-26 23:21:10,462] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.13277]
 [77.06097]
 [77.05004]
 [76.81525]
 [76.67032]], R is [[77.14853668]
 [77.09021759]
 [77.03132629]
 [76.97303772]
 [76.91345215]].
[2019-03-26 23:21:10,854] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2205899: loss 0.0073
[2019-03-26 23:21:10,857] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2205899: learning rate 0.0010
[2019-03-26 23:21:13,091] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2206898: loss 0.0062
[2019-03-26 23:21:13,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2206900: learning rate 0.0010
[2019-03-26 23:21:14,847] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2207684: loss 0.0049
[2019-03-26 23:21:14,849] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2207684: learning rate 0.0010
[2019-03-26 23:21:15,104] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.5391944e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:15,117] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8433
[2019-03-26 23:21:15,124] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 99.0, 1.0, 2.0, 0.3432213617658628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532063.7906343807, 532063.79063438, 169267.1162802483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3127800.0000, 
sim time next is 3128400.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.342328538041652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531439.1029385976, 531439.1029385976, 169237.7812907807], 
processed observation next is [1.0, 0.21739130434782608, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20762474462849634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14762197303849933, 0.14762197303849933, 0.25259370341907567], 
reward next is 0.7474, 
noisyNet noise sample is [array([1.2640624], dtype=float32), 0.016359992]. 
=============================================
[2019-03-26 23:21:15,140] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.4078368e-38 1.0000000e+00 0.0000000e+00 1.1649738e-15 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:15,147] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9981
[2019-03-26 23:21:15,151] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 84.0, 1.0, 2.0, 0.3670756045091388, 1.0, 1.0, 0.3670756045091388, 1.0, 1.0, 0.6189928215452329, 6.9112, 6.9112, 170.5573041426782, 1539391.592414516, 1539391.592414516, 333021.0021072107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3165600.0000, 
sim time next is 3166200.0000, 
raw observation next is [26.5, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.244916923818481, 6.9112, 168.9110318731211, 1690664.646581587, 1453917.072767633, 311347.502990708], 
processed observation next is [1.0, 0.6521739130434783, 0.4549763033175356, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.03337169238184812, 0.0, 0.8294304943020082, 0.4696290684948852, 0.40386585354656473, 0.4646977656577731], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9683937], dtype=float32), 1.7815081]. 
=============================================
[2019-03-26 23:21:15,784] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2208099: loss 0.0050
[2019-03-26 23:21:15,785] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2208099: learning rate 0.0010
[2019-03-26 23:21:16,904] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2208561: loss 0.0047
[2019-03-26 23:21:16,905] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2208561: learning rate 0.0010
[2019-03-26 23:21:18,522] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2209293: loss 3.1315
[2019-03-26 23:21:18,525] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2209293: learning rate 0.0010
[2019-03-26 23:21:19,116] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2209556: loss 0.0309
[2019-03-26 23:21:19,119] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2209557: learning rate 0.0010
[2019-03-26 23:21:19,887] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2209905: loss 0.0147
[2019-03-26 23:21:19,892] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2209905: learning rate 0.0010
[2019-03-26 23:21:20,601] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2210223: loss 0.7134
[2019-03-26 23:21:20,604] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2210224: learning rate 0.0010
[2019-03-26 23:21:20,785] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6914086e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:20,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8615
[2019-03-26 23:21:20,802] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5566791690368277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777901.6489690338, 777901.6489690333, 192952.5473174102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3265800.0000, 
sim time next is 3266400.0000, 
raw observation next is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5521977402739913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771637.0404237891, 771637.0404237891, 192178.3293658529], 
processed observation next is [0.0, 0.8260869565217391, 0.5576619273301741, 0.8066666666666668, 1.0, 1.0, 0.46047920514938706, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21434362233994142, 0.21434362233994142, 0.28683332741172074], 
reward next is 0.7132, 
noisyNet noise sample is [array([0.5393575], dtype=float32), 0.9488033]. 
=============================================
[2019-03-26 23:21:21,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.06135334e-20
 0.00000000e+00], sum to 1.0000
[2019-03-26 23:21:21,587] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2609
[2019-03-26 23:21:21,595] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 89.0, 1.0, 2.0, 0.765910022265203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1070427.501924285, 1070427.501924285, 235716.0615192117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3392400.0000, 
sim time next is 3393000.0000, 
raw observation next is [27.5, 89.0, 1.0, 2.0, 0.7733679151593492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080855.876382472, 1080855.876382473, 237480.0817162002], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.89, 1.0, 1.0, 0.7269492953727099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30023774343957554, 0.3002377434395758, 0.3544478831585077], 
reward next is 0.6456, 
noisyNet noise sample is [array([0.9979468], dtype=float32), 0.37442705]. 
=============================================
[2019-03-26 23:21:21,605] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.7451  ]
 [63.96774 ]
 [63.359264]
 [63.62252 ]
 [63.83428 ]], R is [[63.90391159]
 [63.91305923]
 [63.92053986]
 [63.91562653]
 [63.90911484]].
[2019-03-26 23:21:22,291] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2210976: loss 0.0281
[2019-03-26 23:21:22,295] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2210977: learning rate 0.0010
[2019-03-26 23:21:22,765] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4175464e-12 2.0714186e-02 2.7746019e-15 9.7928578e-01 6.0534624e-18], sum to 1.0000
[2019-03-26 23:21:22,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8042
[2019-03-26 23:21:22,781] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 69.0, 1.0, 2.0, 0.9335845835884348, 1.0, 2.0, 0.9335845835884348, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2611328.697212625, 2611328.697212624, 490153.2295001256], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3411000.0000, 
sim time next is 3411600.0000, 
raw observation next is [32.66666666666667, 68.33333333333333, 1.0, 2.0, 0.9953740970768249, 1.0, 2.0, 0.9953740970768249, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2784352.690760687, 2784352.690760687, 526202.1802908197], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.6833333333333332, 1.0, 1.0, 0.9944266229841264, 1.0, 1.0, 0.9944266229841264, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7734313029890797, 0.7734313029890797, 0.7853763884937608], 
reward next is 0.2146, 
noisyNet noise sample is [array([-1.823378], dtype=float32), -0.70408905]. 
=============================================
[2019-03-26 23:21:23,146] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2211357: loss 0.0250
[2019-03-26 23:21:23,149] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2211357: learning rate 0.0010
[2019-03-26 23:21:23,487] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2211508: loss 0.0045
[2019-03-26 23:21:23,489] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2211508: learning rate 0.0010
[2019-03-26 23:21:23,716] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2211609: loss 0.0045
[2019-03-26 23:21:23,719] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2211609: learning rate 0.0010
[2019-03-26 23:21:24,059] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2211759: loss 0.8377
[2019-03-26 23:21:24,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2211761: learning rate 0.0010
[2019-03-26 23:21:25,918] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2212590: loss 0.0045
[2019-03-26 23:21:25,921] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2212590: learning rate 0.0010
[2019-03-26 23:21:28,489] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2213742: loss 1.6923
[2019-03-26 23:21:28,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2213742: learning rate 0.0010
[2019-03-26 23:21:29,095] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2214012: loss 0.0326
[2019-03-26 23:21:29,097] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2214012: learning rate 0.0010
[2019-03-26 23:21:31,331] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2215019: loss 0.0533
[2019-03-26 23:21:31,334] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2215021: learning rate 0.0010
[2019-03-26 23:21:33,089] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2215797: loss 1.0952
[2019-03-26 23:21:33,091] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2215797: learning rate 0.0010
[2019-03-26 23:21:33,884] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2216147: loss 0.5189
[2019-03-26 23:21:33,887] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2216147: learning rate 0.0010
[2019-03-26 23:21:34,272] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7313517e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:21:34,282] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5848
[2019-03-26 23:21:34,289] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.5524456496319025, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 771983.5930162227, 771983.5930162227, 192221.322358043], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3696600.0000, 
sim time next is 3697200.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5508000066866207, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769683.1501883166, 769683.1501883172, 191938.2722912116], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.79, 1.0, 1.0, 0.458795188779061, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21380087505231016, 0.21380087505231032, 0.28647503327046503], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.52509904], dtype=float32), 0.47512853]. 
=============================================
[2019-03-26 23:21:34,862] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2216584: loss 0.1418
[2019-03-26 23:21:34,865] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2216584: learning rate 0.0010
[2019-03-26 23:21:36,424] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2217286: loss 0.0098
[2019-03-26 23:21:36,426] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2217286: learning rate 0.0010
[2019-03-26 23:21:37,195] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2217630: loss 2.1279
[2019-03-26 23:21:37,199] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2217631: learning rate 0.0010
[2019-03-26 23:21:37,939] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2217969: loss 1.3765
[2019-03-26 23:21:37,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2217969: learning rate 0.0010
[2019-03-26 23:21:38,375] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2218158: loss 0.0077
[2019-03-26 23:21:38,383] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2218159: learning rate 0.0010
[2019-03-26 23:21:38,707] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.726321e-26 0.000000e+00], sum to 1.0000
[2019-03-26 23:21:38,715] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9392
[2019-03-26 23:21:38,720] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 54.66666666666667, 1.0, 2.0, 0.603812365442395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843791.5849227291, 843791.5849227291, 201457.7535669962], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3859800.0000, 
sim time next is 3860400.0000, 
raw observation next is [35.0, 54.33333333333334, 1.0, 2.0, 0.5852473548498202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817838.1073454967, 817838.1073454967, 198030.1991720698], 
processed observation next is [0.0, 0.6956521739130435, 0.8578199052132701, 0.5433333333333334, 1.0, 1.0, 0.5002980178913495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22717725204041575, 0.22717725204041575, 0.29556746145085044], 
reward next is 0.7044, 
noisyNet noise sample is [array([0.5875719], dtype=float32), 0.83870023]. 
=============================================
[2019-03-26 23:21:40,144] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2218955: loss 0.4153
[2019-03-26 23:21:40,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2218955: learning rate 0.0010
[2019-03-26 23:21:41,145] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2219399: loss 0.3975
[2019-03-26 23:21:41,147] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2219399: learning rate 0.0010
[2019-03-26 23:21:41,380] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2219502: loss 1.0341
[2019-03-26 23:21:41,382] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2219502: learning rate 0.0010
[2019-03-26 23:21:41,600] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2219596: loss 0.1434
[2019-03-26 23:21:41,603] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2219596: learning rate 0.0010
[2019-03-26 23:21:41,743] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2219665: loss 0.0087
[2019-03-26 23:21:41,745] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2219666: learning rate 0.0010
[2019-03-26 23:21:43,761] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2220565: loss 4.0520
[2019-03-26 23:21:43,766] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2220565: learning rate 0.0010
[2019-03-26 23:21:44,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.5129217e-12 8.5338211e-01 1.8358160e-15 1.4661793e-01 3.9205269e-20], sum to 1.0000
[2019-03-26 23:21:44,087] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2499
[2019-03-26 23:21:44,097] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2524893.942956453 W.
[2019-03-26 23:21:44,102] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 62.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.250902268773552, 6.9112, 168.9112065036225, 2524893.942956453, 2283899.958660108, 475348.67047089], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3676200.0000, 
sim time next is 3676800.0000, 
raw observation next is [33.0, 61.66666666666667, 1.0, 2.0, 0.9766688847227905, 1.0, 1.0, 0.9766688847227905, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2731971.523900695, 2731971.523900695, 515046.3058737481], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.6166666666666667, 1.0, 1.0, 0.9718902225575788, 1.0, 0.5, 0.9718902225575788, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7588809788613042, 0.7588809788613042, 0.7687258296623106], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5534746], dtype=float32), 1.2997365]. 
=============================================
[2019-03-26 23:21:46,175] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2221638: loss 0.0084
[2019-03-26 23:21:46,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2221638: learning rate 0.0010
[2019-03-26 23:21:47,034] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2222011: loss 0.0167
[2019-03-26 23:21:47,036] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2222012: learning rate 0.0010
[2019-03-26 23:21:49,273] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2223021: loss 0.0084
[2019-03-26 23:21:49,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2223021: learning rate 0.0010
[2019-03-26 23:21:50,691] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2223651: loss 0.0096
[2019-03-26 23:21:50,694] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2223652: learning rate 0.0010
[2019-03-26 23:21:51,616] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2224070: loss 0.0217
[2019-03-26 23:21:51,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2224071: learning rate 0.0010
[2019-03-26 23:21:52,504] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2224470: loss 0.0134
[2019-03-26 23:21:52,507] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2224471: learning rate 0.0010
[2019-03-26 23:21:53,703] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 23:21:53,704] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:21:53,704] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:21:53,705] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:21:53,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:21:53,706] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:21:53,707] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:21:53,705] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:21:53,709] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:21:53,710] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:21:53,714] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:21:53,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-03-26 23:21:53,764] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-03-26 23:21:53,765] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-03-26 23:21:53,806] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-03-26 23:21:53,831] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-03-26 23:21:57,511] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:21:57,514] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.46666666666667, 93.0, 1.0, 2.0, 0.2929322179929746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469890.6680970922, 469890.6680970922, 164946.9693351423]
[2019-03-26 23:21:57,515] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:21:57,518] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0936205e-28 0.0000000e+00], sampled 0.19628951051723775
[2019-03-26 23:22:07,097] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:22:07,100] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.78771746, 69.53731509666667, 1.0, 2.0, 0.2494570350854661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 411454.4534261586, 411454.4534261586, 160821.814027028]
[2019-03-26 23:22:07,101] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:22:07,105] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.187731e-30 0.000000e+00], sampled 0.9249239561726902
[2019-03-26 23:22:23,802] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:22:23,804] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.2, 90.0, 1.0, 2.0, 0.4889708901759187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683255.8922545578, 683255.8922545578, 181885.1647133034]
[2019-03-26 23:22:23,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:22:23,809] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9562104e-29 0.0000000e+00], sampled 0.5842807076844491
[2019-03-26 23:22:47,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:22:47,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.1, 50.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.468846935718801, 6.9112, 168.9098982960653, 1849632.660947965, 1454025.894718989, 311354.7342196154]
[2019-03-26 23:22:47,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:22:47,816] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8654385e-33 1.0000000e+00 0.0000000e+00 9.3481541e-17 0.0000000e+00], sampled 0.9036851905628586
[2019-03-26 23:22:47,817] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1849632.660947965 W.
[2019-03-26 23:23:30,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:23:30,854] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.91217868333333, 80.35799521833334, 1.0, 2.0, 0.4646777926459721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664648.3477967514, 664648.3477967521, 180213.6095697662]
[2019-03-26 23:23:30,856] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:23:30,861] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2885088e-29 0.0000000e+00], sampled 0.5088454629273046
[2019-03-26 23:23:31,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:23:31,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.49070147666666, 52.167177105, 1.0, 2.0, 0.6966280115117595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1005347.683332921, 1005347.683332921, 224340.5928428179]
[2019-03-26 23:23:31,693] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:23:31,696] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.8051805e-27 0.0000000e+00], sampled 0.7172157977311522
[2019-03-26 23:23:33,351] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.0044782]
[2019-03-26 23:23:33,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.09113113333333, 36.79254704666667, 1.0, 2.0, 0.2820156778757094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454202.092905194, 454202.0929051947, 163880.9701937292]
[2019-03-26 23:23:33,352] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:23:33,357] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7675922e-30 0.0000000e+00], sampled 0.022746182804963655
[2019-03-26 23:23:48,164] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8263.7475 2927580254.1712 1327.0000
[2019-03-26 23:23:48,658] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8002.2136 3007764677.3979 1758.0000
[2019-03-26 23:23:48,660] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.1658 2779508305.0014 924.0000
[2019-03-26 23:23:48,704] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8499.4966 2842617879.4684 1123.0000
[2019-03-26 23:23:48,762] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7897.1916 3162791774.1136 1733.0000
[2019-03-26 23:23:49,780] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2225000, evaluation results [2225000.0, 7897.191617960918, 3162791774.113635, 1733.0, 8263.747477946785, 2927580254.171225, 1327.0, 8662.165788974999, 2779508305.0013585, 924.0, 8002.213583924667, 3007764677.397899, 1758.0, 8499.49660135402, 2842617879.4684305, 1123.0]
[2019-03-26 23:23:49,950] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 6.76608e-31 0.00000e+00], sum to 1.0000
[2019-03-26 23:23:49,963] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1523
[2019-03-26 23:23:49,967] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.83333333333334, 56.66666666666667, 1.0, 2.0, 0.5939899216010776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 830059.9448879989, 830059.9448879989, 199632.40688564], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3934200.0000, 
sim time next is 3934800.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5948395220642603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831247.6675191096, 831247.6675191096, 199789.2505918717], 
processed observation next is [0.0, 0.5652173913043478, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5118548458605545, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2309021298664193, 0.2309021298664193, 0.2981929113311518], 
reward next is 0.7018, 
noisyNet noise sample is [array([-1.0626327], dtype=float32), -0.43269885]. 
=============================================
[2019-03-26 23:23:50,116] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5607941e-29 0.0000000e+00 8.5633213e-29 1.0000000e+00 6.9603412e-28], sum to 1.0000
[2019-03-26 23:23:50,124] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4884
[2019-03-26 23:23:50,131] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.16666666666666, 52.5, 1.0, 2.0, 0.8772055293075626, 1.0, 2.0, 0.7591928041680439, 1.0, 1.0, 1.03, 7.005111708809259, 6.9112, 170.5573041426782, 3186038.387019526, 3118765.609915381, 583144.0132854871], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4204200.0000, 
sim time next is 4204800.0000, 
raw observation next is [36.0, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.344899798848039, 6.9112, 170.5573041426782, 3220368.426030594, 2909691.618335872, 551330.1618663034], 
processed observation next is [1.0, 0.6956521739130435, 0.9052132701421801, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.043369979884803914, 0.0, 0.8375144448122397, 0.8945467850084983, 0.8082476717599645, 0.822880838606423], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.63711375], dtype=float32), 2.2626016]. 
=============================================
[2019-03-26 23:23:50,578] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2225330: loss 380.5125
[2019-03-26 23:23:50,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2225331: learning rate 0.0010
[2019-03-26 23:23:51,137] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2225582: loss 0.0173
[2019-03-26 23:23:51,139] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2225583: learning rate 0.0010
[2019-03-26 23:23:51,986] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2225966: loss 0.0201
[2019-03-26 23:23:51,987] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2225966: learning rate 0.0010
[2019-03-26 23:23:52,791] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2226323: loss 405.3852
[2019-03-26 23:23:52,793] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2226324: learning rate 0.0010
[2019-03-26 23:23:53,702] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5352339e-18 0.0000000e+00], sum to 1.0000
[2019-03-26 23:23:53,712] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9537
[2019-03-26 23:23:53,723] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 85.66666666666666, 1.0, 2.0, 0.7594872664644381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1061446.642131792, 1061446.642131793, 234210.5042758792], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4089000.0000, 
sim time next is 4089600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.735177323856685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027455.038609861, 1027455.038609861, 228622.2823998151], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.84, 1.0, 1.0, 0.6809365347670904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28540417739162804, 0.28540417739162804, 0.34122728716390316], 
reward next is 0.6588, 
noisyNet noise sample is [array([-1.7134984], dtype=float32), -0.5231088]. 
=============================================
[2019-03-26 23:23:54,113] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2226919: loss 0.0250
[2019-03-26 23:23:54,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2226920: learning rate 0.0010
[2019-03-26 23:23:55,131] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2227374: loss 0.0293
[2019-03-26 23:23:55,136] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2227375: learning rate 0.0010
[2019-03-26 23:23:55,373] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2227482: loss 0.0177
[2019-03-26 23:23:55,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2227484: learning rate 0.0010
[2019-03-26 23:23:55,732] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2227643: loss 0.0161
[2019-03-26 23:23:55,734] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2227644: learning rate 0.0010
[2019-03-26 23:23:55,787] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2227666: loss 335.6622
[2019-03-26 23:23:55,790] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2227667: learning rate 0.0010
[2019-03-26 23:23:57,441] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.4761489e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:23:57,450] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4194
[2019-03-26 23:23:57,457] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 60.5, 1.0, 2.0, 0.6015151681422277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840580.1191057914, 840580.1191057914, 201028.9345735524], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3952200.0000, 
sim time next is 3952800.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.59795932202276, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835609.0901588723, 835609.0901588729, 200366.724353474], 
processed observation next is [0.0, 0.782608695652174, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5156136409912772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2321136361552423, 0.23211363615524247, 0.29905481246787163], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.6687744], dtype=float32), -0.28855973]. 
=============================================
[2019-03-26 23:23:57,642] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2228500: loss 0.0180
[2019-03-26 23:23:57,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2228501: learning rate 0.0010
[2019-03-26 23:24:00,413] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2229737: loss 293.7649
[2019-03-26 23:24:00,415] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2229737: learning rate 0.0010
[2019-03-26 23:24:01,047] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2230023: loss 275.8860
[2019-03-26 23:24:01,049] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2230024: learning rate 0.0010
[2019-03-26 23:24:03,420] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2231088: loss 293.4590
[2019-03-26 23:24:03,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2231089: learning rate 0.0010
[2019-03-26 23:24:04,843] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2231726: loss 300.6135
[2019-03-26 23:24:04,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2231726: learning rate 0.0010
[2019-03-26 23:24:05,704] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2232122: loss 380.8771
[2019-03-26 23:24:05,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2232124: learning rate 0.0010
[2019-03-26 23:24:06,426] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2232456: loss 369.0759
[2019-03-26 23:24:06,428] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2232456: learning rate 0.0010
[2019-03-26 23:24:08,341] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.315977e-17 0.000000e+00], sum to 1.0000
[2019-03-26 23:24:08,355] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5102
[2019-03-26 23:24:08,360] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 79.0, 1.0, 2.0, 0.9187025758693804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1284097.835336189, 1284097.835336189, 275136.2849042831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4256400.0000, 
sim time next is 4257000.0000, 
raw observation next is [29.5, 79.0, 1.0, 2.0, 0.9834529039382823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1374659.88324726, 1374659.883247259, 293927.35968535], 
processed observation next is [1.0, 0.2608695652173913, 0.5971563981042655, 0.79, 1.0, 1.0, 0.9800637396846775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.38184996756868334, 0.38184996756868306, 0.4386975517691791], 
reward next is 0.5613, 
noisyNet noise sample is [array([-0.55444235], dtype=float32), 1.8121765]. 
=============================================
[2019-03-26 23:24:08,369] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[57.01552 ]
 [55.895714]
 [58.311104]
 [58.306725]
 [58.77832 ]], R is [[56.04972458]
 [56.07857513]
 [56.0701828 ]
 [56.12606812]
 [56.17635345]].
[2019-03-26 23:24:08,436] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2233349: loss 0.0354
[2019-03-26 23:24:08,439] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2233351: learning rate 0.0010
[2019-03-26 23:24:08,930] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2233573: loss 340.4552
[2019-03-26 23:24:08,935] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2233574: learning rate 0.0010
[2019-03-26 23:24:09,871] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2233993: loss 330.7590
[2019-03-26 23:24:09,874] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2233994: learning rate 0.0010
[2019-03-26 23:24:10,600] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2234311: loss 0.0176
[2019-03-26 23:24:10,602] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2234311: learning rate 0.0010
[2019-03-26 23:24:12,091] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2234976: loss 362.9049
[2019-03-26 23:24:12,096] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2234976: learning rate 0.0010
[2019-03-26 23:24:12,900] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2235337: loss 298.5574
[2019-03-26 23:24:12,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2235338: learning rate 0.0010
[2019-03-26 23:24:13,277] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2235505: loss 379.9947
[2019-03-26 23:24:13,284] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2235508: learning rate 0.0010
[2019-03-26 23:24:13,460] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2235588: loss 0.0307
[2019-03-26 23:24:13,463] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2235588: learning rate 0.0010
[2019-03-26 23:24:13,647] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2235670: loss 416.8506
[2019-03-26 23:24:13,652] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2235670: learning rate 0.0010
[2019-03-26 23:24:15,473] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2236485: loss 266.5378
[2019-03-26 23:24:15,474] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2236485: learning rate 0.0010
[2019-03-26 23:24:17,973] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2237598: loss 0.0280
[2019-03-26 23:24:17,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2237599: learning rate 0.0010
[2019-03-26 23:24:18,892] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2238009: loss 0.0266
[2019-03-26 23:24:18,895] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2238009: learning rate 0.0010
[2019-03-26 23:24:21,151] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2239023: loss 0.0280
[2019-03-26 23:24:21,157] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2239026: learning rate 0.0010
[2019-03-26 23:24:22,497] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2239626: loss 0.0198
[2019-03-26 23:24:22,498] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2239626: learning rate 0.0010
[2019-03-26 23:24:23,624] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2240130: loss 0.0192
[2019-03-26 23:24:23,625] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2240130: learning rate 0.0010
[2019-03-26 23:24:24,125] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2240356: loss 0.0304
[2019-03-26 23:24:24,126] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2240357: learning rate 0.0010
[2019-03-26 23:24:26,670] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2241476: loss 0.0376
[2019-03-26 23:24:26,671] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2241477: learning rate 0.0010
[2019-03-26 23:24:26,775] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2241522: loss 8.3109
[2019-03-26 23:24:26,777] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2241522: learning rate 0.0010
[2019-03-26 23:24:27,691] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2241929: loss 0.0204
[2019-03-26 23:24:27,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2241931: learning rate 0.0010
[2019-03-26 23:24:28,565] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2242320: loss -77.2982
[2019-03-26 23:24:28,568] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2242321: learning rate 0.0010
[2019-03-26 23:24:29,948] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2242939: loss 0.0264
[2019-03-26 23:24:29,950] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2242939: learning rate 0.0010
[2019-03-26 23:24:30,970] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2243395: loss 0.0124
[2019-03-26 23:24:30,974] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2243395: learning rate 0.0010
[2019-03-26 23:24:31,466] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2243619: loss 0.0361
[2019-03-26 23:24:31,468] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2243620: learning rate 0.0010
[2019-03-26 23:24:31,582] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2243669: loss 1.2152
[2019-03-26 23:24:31,587] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2243670: learning rate 0.0010
[2019-03-26 23:24:31,602] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2243679: loss 0.0941
[2019-03-26 23:24:31,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2243679: learning rate 0.0010
[2019-03-26 23:24:32,639] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.7286301e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 23:24:32,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1890
[2019-03-26 23:24:32,652] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.5088732889015167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711075.5085487235, 711075.508548723, 184997.4550269174], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4495200.0000, 
sim time next is 4495800.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5088754442486395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711078.5213370813, 711078.5213370813, 184997.7982765822], 
processed observation next is [0.0, 0.0, 0.4312796208530806, 0.89, 1.0, 1.0, 0.40828366776944514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1975218114825226, 0.1975218114825226, 0.27611611683071974], 
reward next is 0.7239, 
noisyNet noise sample is [array([0.4116542], dtype=float32), 1.3821309]. 
=============================================
[2019-03-26 23:24:33,606] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2244571: loss 0.0093
[2019-03-26 23:24:33,607] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2244572: learning rate 0.0010
[2019-03-26 23:24:34,427] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.8448375e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:24:34,438] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9987
[2019-03-26 23:24:34,443] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.493160040738451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 182531.3177667337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4909200.0000, 
sim time next is 4909800.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.491514018420261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686810.6418548356, 686810.6418548356, 182277.1349528615], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.38736628725332645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19078073384856542, 0.19078073384856542, 0.2720554253027784], 
reward next is 0.7279, 
noisyNet noise sample is [array([-0.83884484], dtype=float32), 0.68878204]. 
=============================================
[2019-03-26 23:24:36,174] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2245722: loss 2.4133
[2019-03-26 23:24:36,178] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2245723: learning rate 0.0010
[2019-03-26 23:24:36,815] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2246007: loss -17.7564
[2019-03-26 23:24:36,819] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2246008: learning rate 0.0010
[2019-03-26 23:24:39,146] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2247050: loss -0.0212
[2019-03-26 23:24:39,148] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2247050: learning rate 0.0010
[2019-03-26 23:24:40,146] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6674171e-28 1.0000000e+00 2.1092595e-32 1.5792276e-15 1.6015407e-38], sum to 1.0000
[2019-03-26 23:24:40,155] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6743
[2019-03-26 23:24:40,169] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2103787.308935588 W.
[2019-03-26 23:24:40,173] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.8633853094822579, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.97450220937616, 6.9112, 168.9125270502221, 2103787.308935588, 2058878.657090512, 425185.2851860771], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4957200.0000, 
sim time next is 4957800.0000, 
raw observation next is [30.0, 66.66666666666667, 1.0, 2.0, 0.4706330706873372, 1.0, 1.0, 0.4706330706873372, 1.0, 2.0, 0.8013557215323094, 6.911200000000001, 6.9112, 170.5573041426782, 1974076.797104976, 1974076.797104975, 392703.8624193915], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.6666666666666667, 1.0, 1.0, 0.36220851890040623, 1.0, 0.5, 0.36220851890040623, 1.0, 1.0, 0.7577508799174504, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5483546658624933, 0.548354665862493, 0.5861251677901366], 
reward next is 0.4139, 
noisyNet noise sample is [array([-1.4719115], dtype=float32), 0.22459011]. 
=============================================
[2019-03-26 23:24:40,627] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2247713: loss 0.0656
[2019-03-26 23:24:40,632] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2247714: learning rate 0.0010
[2019-03-26 23:24:41,577] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2248139: loss -2.3620
[2019-03-26 23:24:41,579] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2248139: learning rate 0.0010
[2019-03-26 23:24:42,220] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2248426: loss 56.5234
[2019-03-26 23:24:42,223] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2248427: learning rate 0.0010
[2019-03-26 23:24:42,298] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3689478e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 23:24:42,308] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9681
[2019-03-26 23:24:42,312] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4724233350370068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664883.6020439009, 664883.6020439015, 180001.7606253187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4658400.0000, 
sim time next is 4659000.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4767004995338715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668384.50316243, 668384.50316243, 180320.1372688563], 
processed observation next is [1.0, 0.9565217391304348, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.3695186741371945, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1856623619895639, 0.1856623619895639, 0.26913453323709896], 
reward next is 0.7309, 
noisyNet noise sample is [array([-0.13858658], dtype=float32), -0.09102224]. 
=============================================
[2019-03-26 23:24:42,340] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.93382 ]
 [70.00278 ]
 [70.09283 ]
 [70.253525]
 [70.22769 ]], R is [[69.81253815]
 [69.8457489 ]
 [69.8790741 ]
 [69.91255188]
 [69.94621277]].
[2019-03-26 23:24:44,352] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2249382: loss 0.4861
[2019-03-26 23:24:44,355] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2249382: learning rate 0.0010
[2019-03-26 23:24:44,639] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2249509: loss -11.1418
[2019-03-26 23:24:44,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2249510: learning rate 0.0010
[2019-03-26 23:24:44,840] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.114417e-33 0.000000e+00], sum to 1.0000
[2019-03-26 23:24:44,850] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5437
[2019-03-26 23:24:44,857] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.708105031886024, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989602.2348464536, 989602.2348464536, 222601.007603375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4765800.0000, 
sim time next is 4766400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6332765027489061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884983.1045701196, 884983.1045701191, 207098.9061980101], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5581644611432603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24582864015836656, 0.24582864015836642, 0.30910284507165686], 
reward next is 0.6909, 
noisyNet noise sample is [array([1.579458], dtype=float32), -0.12071757]. 
=============================================
[2019-03-26 23:24:45,636] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2249949: loss 6.0600
[2019-03-26 23:24:45,644] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2249952: learning rate 0.0010
[2019-03-26 23:24:45,753] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 23:24:45,754] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:24:45,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:24:45,756] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:24:45,757] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:24:45,757] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:24:45,758] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:24:45,758] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:24:45,759] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:24:45,760] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:24:45,760] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:24:45,790] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-03-26 23:24:45,811] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-03-26 23:24:45,834] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-03-26 23:24:45,834] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-03-26 23:24:45,873] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-03-26 23:24:57,345] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2461038]
[2019-03-26 23:24:57,346] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 60.66666666666667, 1.0, 2.0, 0.4803175319889053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671160.4365752094, 671160.4365752094, 180569.6297423654]
[2019-03-26 23:24:57,348] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:24:57,352] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.2322856e-36 0.0000000e+00], sampled 0.06799405410268877
[2019-03-26 23:25:15,269] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2461038]
[2019-03-26 23:25:15,270] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.01738041833334, 93.97526149166666, 1.0, 2.0, 0.3984835703852239, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596380.1976151111, 596380.1976151104, 174256.3756927193]
[2019-03-26 23:25:15,273] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:25:15,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4039874e-36 0.0000000e+00], sampled 0.6898656765557117
[2019-03-26 23:25:46,612] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2461038]
[2019-03-26 23:25:46,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.76666666666667, 49.33333333333333, 1.0, 2.0, 1.020033976811433, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005994433572415, 6.9112, 168.9123159153375, 2323043.540808242, 2255793.362103725, 469517.0786251436]
[2019-03-26 23:25:46,616] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:25:46,618] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6361431e-27 1.0000000e+00 2.8527029e-31 6.5355834e-16 1.8596916e-38], sampled 0.5766179459834089
[2019-03-26 23:25:46,619] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2323043.540808242 W.
[2019-03-26 23:25:49,921] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2461038]
[2019-03-26 23:25:49,923] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 84.0, 1.0, 2.0, 0.5862638969763434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 819259.1945285236, 819259.1945285242, 198214.4589014251]
[2019-03-26 23:25:49,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:25:49,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8900396e-34 0.0000000e+00], sampled 0.7071199893018525
[2019-03-26 23:25:59,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2461038]
[2019-03-26 23:25:59,083] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.38270196333333, 92.09211082666667, 1.0, 2.0, 0.5538625472744042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773964.2769777926, 773964.2769777926, 192466.3234934626]
[2019-03-26 23:25:59,083] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:25:59,086] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.0402954e-34 0.0000000e+00], sampled 0.2787268114193042
[2019-03-26 23:26:39,757] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6877 2779657219.3097 927.0000
[2019-03-26 23:26:40,215] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.7580 2927949454.2277 1337.0000
[2019-03-26 23:26:40,298] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.5726 2843130846.9544 1130.0000
[2019-03-26 23:26:40,428] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.4638 3008225044.2517 1768.0000
[2019-03-26 23:26:40,476] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.5059 3164269387.1460 1772.0000
[2019-03-26 23:26:41,493] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2250000, evaluation results [2250000.0, 7878.505888797981, 3164269387.145979, 1772.0, 8254.75800813299, 2927949454.2277265, 1337.0, 8660.68770181798, 2779657219.309736, 927.0, 7997.463771542022, 3008225044.2517104, 1768.0, 8496.572616262636, 2843130846.9543915, 1130.0]
[2019-03-26 23:26:42,095] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2250276: loss 0.4343
[2019-03-26 23:26:42,096] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2250276: learning rate 0.0010
[2019-03-26 23:26:43,374] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2250854: loss -5.2181
[2019-03-26 23:26:43,376] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2250854: learning rate 0.0010
[2019-03-26 23:26:44,783] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2251482: loss 0.9306
[2019-03-26 23:26:44,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2251483: learning rate 0.0010
[2019-03-26 23:26:44,860] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2251510: loss 0.5377
[2019-03-26 23:26:44,865] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2251512: learning rate 0.0010
[2019-03-26 23:26:45,132] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2251637: loss -15.2654
[2019-03-26 23:26:45,133] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2251637: learning rate 0.0010
[2019-03-26 23:26:45,197] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2251661: loss -11.6521
[2019-03-26 23:26:45,200] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2251661: learning rate 0.0010
[2019-03-26 23:26:47,249] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2252580: loss 0.1417
[2019-03-26 23:26:47,252] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2252580: learning rate 0.0010
[2019-03-26 23:26:49,874] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2253739: loss 0.3004
[2019-03-26 23:26:49,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2253740: learning rate 0.0010
[2019-03-26 23:26:50,330] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2253939: loss 0.3090
[2019-03-26 23:26:50,332] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2253940: learning rate 0.0010
[2019-03-26 23:26:52,873] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2255081: loss 0.7475
[2019-03-26 23:26:52,876] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2255081: learning rate 0.0010
[2019-03-26 23:26:54,274] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2255709: loss 0.2115
[2019-03-26 23:26:54,279] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2255710: learning rate 0.0010
[2019-03-26 23:26:54,952] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2256015: loss 0.3452
[2019-03-26 23:26:54,954] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2256015: learning rate 0.0010
[2019-03-26 23:26:55,869] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2256422: loss 0.7860
[2019-03-26 23:26:55,875] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2256424: learning rate 0.0010
[2019-03-26 23:26:57,869] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2257318: loss -69.3678
[2019-03-26 23:26:57,874] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2257321: learning rate 0.0010
[2019-03-26 23:26:58,315] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2257519: loss 0.4660
[2019-03-26 23:26:58,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2257519: learning rate 0.0010
[2019-03-26 23:26:58,506] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:26:58,514] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7270
[2019-03-26 23:26:58,521] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.9, 74.33333333333333, 1.0, 2.0, 0.560677956927136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 783491.6029556579, 783491.6029556579, 193651.3025116534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5421000.0000, 
sim time next is 5421600.0000, 
raw observation next is [30.9, 75.0, 1.0, 2.0, 0.5725982609448191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800155.3147649188, 800155.3147649188, 195753.8019869955], 
processed observation next is [1.0, 0.782608695652174, 0.6635071090047393, 0.75, 1.0, 1.0, 0.4850581457166495, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22226536521247744, 0.22226536521247744, 0.2921698537119336], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.74444413], dtype=float32), 1.2664826]. 
=============================================
[2019-03-26 23:26:59,227] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2257926: loss 0.1313
[2019-03-26 23:26:59,229] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2257926: learning rate 0.0010
[2019-03-26 23:27:00,033] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2258281: loss -31.7666
[2019-03-26 23:27:00,034] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2258282: learning rate 0.0010
[2019-03-26 23:27:01,398] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2258895: loss 0.0827
[2019-03-26 23:27:01,404] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2258895: learning rate 0.0010
[2019-03-26 23:27:02,665] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2259468: loss 0.2166
[2019-03-26 23:27:02,667] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2259468: learning rate 0.0010
[2019-03-26 23:27:02,992] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2259617: loss -35.4487
[2019-03-26 23:27:02,994] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2259619: loss 0.1787
[2019-03-26 23:27:02,994] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2259619: learning rate 0.0010
[2019-03-26 23:27:02,997] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2259619: learning rate 0.0010
[2019-03-26 23:27:03,313] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2259761: loss 0.2301
[2019-03-26 23:27:03,315] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2259761: learning rate 0.0010
[2019-03-26 23:27:05,212] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2260604: loss 0.4759
[2019-03-26 23:27:05,215] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2260604: learning rate 0.0010
[2019-03-26 23:27:06,870] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.7932181e-21 1.4138598e-33 2.0051016e-21 1.0000000e+00 1.1856886e-20], sum to 1.0000
[2019-03-26 23:27:06,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1497
[2019-03-26 23:27:06,880] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.53333333333333, 71.33333333333333, 1.0, 2.0, 0.97765759531617, 1.0, 2.0, 0.97765759531617, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2734740.20597868, 2734740.20597868, 515639.7927685225], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5476200.0000, 
sim time next is 5476800.0000, 
raw observation next is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.9548745306143527, 1.0, 2.0, 0.9548745306143527, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2670942.444900033, 2670942.444900033, 502324.7359627879], 
processed observation next is [1.0, 0.391304347826087, 0.7519747235387049, 0.7066666666666667, 1.0, 1.0, 0.9456319645956056, 1.0, 1.0, 0.9456319645956056, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7419284569166759, 0.7419284569166759, 0.749738411884758], 
reward next is 0.2503, 
noisyNet noise sample is [array([-0.26047403], dtype=float32), 0.3298885]. 
=============================================
[2019-03-26 23:27:07,846] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2261778: loss 5.6424
[2019-03-26 23:27:07,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2261779: learning rate 0.0010
[2019-03-26 23:27:08,462] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2262053: loss -30.3279
[2019-03-26 23:27:08,464] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2262054: learning rate 0.0010
[2019-03-26 23:27:09,568] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.054147e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:27:09,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1757
[2019-03-26 23:27:09,583] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 83.0, 1.0, 2.0, 0.56496112017837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789479.121331899, 789479.121331899, 194399.8114578796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5521200.0000, 
sim time next is 5521800.0000, 
raw observation next is [28.55, 83.5, 1.0, 2.0, 0.563520643250032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787465.4464596465, 787465.4464596465, 194146.6071224975], 
processed observation next is [1.0, 0.9130434782608695, 0.552132701421801, 0.835, 1.0, 1.0, 0.4741212569277494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21874040179434623, 0.21874040179434623, 0.28977105540671266], 
reward next is 0.7102, 
noisyNet noise sample is [array([-0.15217903], dtype=float32), -1.623252]. 
=============================================
[2019-03-26 23:27:09,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.6242515e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:09,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4400
[2019-03-26 23:27:09,762] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 90.0, 1.0, 2.0, 0.549222067031194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767477.354408749, 767477.3544087496, 191667.1488789463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5608800.0000, 
sim time next is 5609400.0000, 
raw observation next is [27.01666666666667, 90.0, 1.0, 2.0, 0.5464752681517986, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763637.6257394119, 763637.6257394119, 191197.798214859], 
processed observation next is [1.0, 0.9565217391304348, 0.4794628751974725, 0.9, 1.0, 1.0, 0.4535846604238537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2121215627053922, 0.2121215627053922, 0.2853698480818791], 
reward next is 0.7146, 
noisyNet noise sample is [array([-0.16050132], dtype=float32), -1.2682744]. 
=============================================
[2019-03-26 23:27:10,834] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2263116: loss -31.0249
[2019-03-26 23:27:10,838] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2263116: learning rate 0.0010
[2019-03-26 23:27:12,026] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.7606967e-23 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:12,028] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2584
[2019-03-26 23:27:12,038] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.46666666666667, 82.16666666666667, 1.0, 2.0, 0.62326405176578, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870985.2960817601, 870985.2960817601, 205160.4808125163], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5431800.0000, 
sim time next is 5432400.0000, 
raw observation next is [30.4, 82.0, 1.0, 2.0, 0.6206111471085819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867276.4597691364, 867276.4597691364, 204648.9368202178], 
processed observation next is [1.0, 0.9130434782608695, 0.6398104265402843, 0.82, 1.0, 1.0, 0.5429049965163637, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.240910127713649, 0.240910127713649, 0.305446174358534], 
reward next is 0.6946, 
noisyNet noise sample is [array([-0.3661321], dtype=float32), 0.6815599]. 
=============================================
[2019-03-26 23:27:12,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 5.80003e-28 0.00000e+00], sum to 1.0000
[2019-03-26 23:27:12,143] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2865
[2019-03-26 23:27:12,151] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 82.00000000000001, 1.0, 2.0, 0.6182834709956576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864022.3128965896, 864022.3128965896, 204201.8626631692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5433000.0000, 
sim time next is 5433600.0000, 
raw observation next is [30.26666666666667, 82.0, 1.0, 2.0, 0.6158046336834332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860556.8500148747, 860556.8500148754, 203727.530860143], 
processed observation next is [1.0, 0.9130434782608695, 0.6334913112164299, 0.82, 1.0, 1.0, 0.537114016486064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2390435694485763, 0.2390435694485765, 0.30407094158230297], 
reward next is 0.6959, 
noisyNet noise sample is [array([-0.12308007], dtype=float32), -2.267956]. 
=============================================
[2019-03-26 23:27:12,313] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2263774: loss -39.1618
[2019-03-26 23:27:12,315] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2263774: learning rate 0.0010
[2019-03-26 23:27:12,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0854398e-23 1.2145045e-37 1.8368620e-23 1.0000000e+00 6.0873752e-23], sum to 1.0000
[2019-03-26 23:27:12,751] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7558
[2019-03-26 23:27:12,755] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.5, 53.83333333333333, 1.0, 2.0, 0.8589123711890123, 1.0, 2.0, 0.7500462251087687, 1.0, 2.0, 1.03, 7.005110265595649, 6.9112, 170.5573041426782, 3147605.362066561, 3080333.618795031, 576222.691235153], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5411400.0000, 
sim time next is 5412000.0000, 
raw observation next is [37.0, 53.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.589358879587701, 6.9112, 170.5573041426782, 3395688.408397961, 2909895.617209792, 549910.1051901687], 
processed observation next is [1.0, 0.6521739130434783, 0.95260663507109, 0.5366666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.06781588795877011, 0.0, 0.8375144448122397, 0.9432467801105447, 0.8083043381138312, 0.8207613510301026], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7168219], dtype=float32), -0.19815196]. 
=============================================
[2019-03-26 23:27:12,770] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[19.608305]
 [19.813034]
 [20.03766 ]
 [20.187815]
 [20.114742]], R is [[19.45521545]
 [19.26066399]
 [19.06805801]
 [18.87737846]
 [18.68860435]].
[2019-03-26 23:27:12,771] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2263975: loss -27.5703
[2019-03-26 23:27:12,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2263978: learning rate 0.0010
[2019-03-26 23:27:13,854] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2264460: loss -38.2001
[2019-03-26 23:27:13,857] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2264461: learning rate 0.0010
[2019-03-26 23:27:15,074] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.982678e-17 1.000000e+00 5.444179e-20 1.022451e-09 1.498985e-24], sum to 1.0000
[2019-03-26 23:27:15,086] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1292
[2019-03-26 23:27:15,096] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2278123.665235632 W.
[2019-03-26 23:27:15,101] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.2, 52.0, 1.0, 2.0, 0.543045242727517, 1.0, 2.0, 0.543045242727517, 1.0, 1.0, 0.9430899988116646, 6.9112, 6.9112, 170.5573041426782, 2278123.665235632, 2278123.665235632, 446190.4017228375], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5317200.0000, 
sim time next is 5317800.0000, 
raw observation next is [36.18333333333334, 52.16666666666667, 1.0, 2.0, 0.5940430649833044, 1.0, 2.0, 0.5940430649833044, 1.0, 2.0, 1.03, 6.913061970501674, 6.9112, 170.5573041426782, 2492277.461770789, 2490943.656560322, 485829.2778614647], 
processed observation next is [1.0, 0.5652173913043478, 0.9139020537124805, 0.5216666666666667, 1.0, 1.0, 0.5108952590160294, 1.0, 1.0, 0.5108952590160294, 1.0, 1.0, 1.0365853658536586, 0.0001861970501673582, 0.0, 0.8375144448122397, 0.6922992949363304, 0.6919287934889783, 0.7251183251663652], 
reward next is 0.2656, 
noisyNet noise sample is [array([-0.29953432], dtype=float32), 1.9883473]. 
=============================================
[2019-03-26 23:27:15,609] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2265248: loss 1.5965
[2019-03-26 23:27:15,612] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2265248: learning rate 0.0010
[2019-03-26 23:27:16,381] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2265594: loss -6.9297
[2019-03-26 23:27:16,382] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2265594: learning rate 0.0010
[2019-03-26 23:27:16,959] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5231032e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:16,968] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4383
[2019-03-26 23:27:16,978] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6245572213472688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872793.1887043887, 872793.1887043887, 205411.2944045779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5341200.0000, 
sim time next is 5341800.0000, 
raw observation next is [31.75, 76.5, 1.0, 2.0, 0.6264332755560198, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 875415.9782338598, 875415.9782338605, 205774.8840607514], 
processed observation next is [1.0, 0.8260869565217391, 0.7037914691943128, 0.765, 1.0, 1.0, 0.5499196091036382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24317110506496104, 0.24317110506496123, 0.3071266926279872], 
reward next is 0.6929, 
noisyNet noise sample is [array([0.13485627], dtype=float32), 0.9962244]. 
=============================================
[2019-03-26 23:27:17,216] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2265967: loss -10.6984
[2019-03-26 23:27:17,217] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2265967: learning rate 0.0010
[2019-03-26 23:27:17,503] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 9.6981436e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:17,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7331
[2019-03-26 23:27:17,514] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.26666666666667, 84.5, 1.0, 2.0, 0.5625131380973192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786057.034772072, 786057.034772072, 193969.6634648023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5523000.0000, 
sim time next is 5523600.0000, 
raw observation next is [28.13333333333333, 85.0, 1.0, 2.0, 0.5625300381840287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786080.6597273023, 786080.6597273023, 193972.4697935905], 
processed observation next is [1.0, 0.9565217391304348, 0.532385466034755, 0.85, 1.0, 1.0, 0.4729277568482273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21835573881313952, 0.21835573881313952, 0.2895111489456575], 
reward next is 0.7105, 
noisyNet noise sample is [array([-0.8371035], dtype=float32), -0.94983304]. 
=============================================
[2019-03-26 23:27:17,665] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2266168: loss 1.6203
[2019-03-26 23:27:17,670] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2266168: learning rate 0.0010
[2019-03-26 23:27:18,232] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1941963e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:18,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7588
[2019-03-26 23:27:18,240] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.03333333333333, 90.0, 1.0, 2.0, 0.5099661090461131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712603.0760283974, 712603.0760283968, 185172.0625785768], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5721600.0000, 
sim time next is 5722200.0000, 
raw observation next is [26.2, 89.0, 1.0, 2.0, 0.5105956776912598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713483.1016054852, 713483.1016054852, 185272.6464170006], 
processed observation next is [0.0, 0.21739130434782608, 0.44075829383886256, 0.89, 1.0, 1.0, 0.4103562381822407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19818975044596812, 0.19818975044596812, 0.27652633793582176], 
reward next is 0.7235, 
noisyNet noise sample is [array([0.28930137], dtype=float32), -0.880967]. 
=============================================
[2019-03-26 23:27:19,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2392721e-11 9.7420424e-01 4.7427047e-13 2.5795743e-02 3.6864372e-16], sum to 1.0000
[2019-03-26 23:27:19,175] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6093
[2019-03-26 23:27:19,186] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2342141.352389129 W.
[2019-03-26 23:27:19,192] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.13333333333333, 54.33333333333333, 1.0, 2.0, 0.5582911025016618, 1.0, 2.0, 0.5582911025016618, 1.0, 1.0, 0.9633279977979357, 6.911199999999999, 6.9112, 170.5573041426782, 2342141.352389129, 2342141.35238913, 456521.0438018866], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5589600.0000, 
sim time next is 5590200.0000, 
raw observation next is [33.06666666666666, 54.66666666666667, 1.0, 2.0, 0.5619791985342025, 1.0, 2.0, 0.5619791985342025, 1.0, 2.0, 0.9705582461905758, 6.911199999999999, 6.9112, 170.5573041426782, 2357628.238142055, 2357628.238142056, 459538.0624952348], 
processed observation next is [1.0, 0.6956521739130435, 0.7661927330173774, 0.5466666666666667, 1.0, 1.0, 0.47226409461952107, 1.0, 1.0, 0.47226409461952107, 1.0, 1.0, 0.964095422183629, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6548967328172375, 0.6548967328172377, 0.6858777052167683], 
reward next is 0.3141, 
noisyNet noise sample is [array([-0.27183172], dtype=float32), -2.1884406]. 
=============================================
[2019-03-26 23:27:19,567] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2267013: loss -4.5521
[2019-03-26 23:27:19,570] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2267015: learning rate 0.0010
[2019-03-26 23:27:20,543] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2267435: loss -5.5009
[2019-03-26 23:27:20,545] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2267435: learning rate 0.0010
[2019-03-26 23:27:20,793] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2267547: loss 1.6008
[2019-03-26 23:27:20,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2267547: learning rate 0.0010
[2019-03-26 23:27:20,891] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2267592: loss -6.1516
[2019-03-26 23:27:20,893] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2267592: learning rate 0.0010
[2019-03-26 23:27:21,172] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6409113e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:21,185] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1301
[2019-03-26 23:27:21,191] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.36666666666667, 87.33333333333333, 1.0, 2.0, 0.8011567345204668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1119713.894792561, 1119713.894792561, 244195.3564651177], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5553600.0000, 
sim time next is 5554200.0000, 
raw observation next is [27.53333333333333, 86.66666666666667, 1.0, 2.0, 0.808371934136521, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1129803.377829184, 1129803.377829185, 245976.7765173571], 
processed observation next is [1.0, 0.2608695652173913, 0.5039494470774091, 0.8666666666666667, 1.0, 1.0, 0.7691228122126759, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3138342716192178, 0.31383427161921806, 0.36712951719008524], 
reward next is 0.6329, 
noisyNet noise sample is [array([-0.16264491], dtype=float32), 0.098872304]. 
=============================================
[2019-03-26 23:27:21,316] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2267781: loss -15.3790
[2019-03-26 23:27:21,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2267781: learning rate 0.0010
[2019-03-26 23:27:23,280] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2268660: loss -9.0133
[2019-03-26 23:27:23,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2268660: learning rate 0.0010
[2019-03-26 23:27:23,893] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:27:23,903] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9370
[2019-03-26 23:27:23,910] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.26666666666667, 60.0, 1.0, 2.0, 0.5599378463623756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782456.9912215542, 782456.9912215542, 193518.6832922029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5670600.0000, 
sim time next is 5671200.0000, 
raw observation next is [32.23333333333333, 60.00000000000001, 1.0, 2.0, 0.5412211549960072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756292.9807215903, 756292.980721591, 190306.8403208943], 
processed observation next is [0.0, 0.6521739130434783, 0.7266982622432857, 0.6000000000000001, 1.0, 1.0, 0.4472544036096472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2100813835337751, 0.21008138353377528, 0.28404006018043926], 
reward next is 0.7160, 
noisyNet noise sample is [array([1.0536472], dtype=float32), 1.7598424]. 
=============================================
[2019-03-26 23:27:24,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 5.89457e-32 0.00000e+00], sum to 1.0000
[2019-03-26 23:27:24,122] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5821
[2019-03-26 23:27:24,127] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 89.5, 1.0, 2.0, 0.5498509575134044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768356.4774477064, 768356.4774477064, 191775.1292037778], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5529000.0000, 
sim time next is 5529600.0000, 
raw observation next is [27.2, 90.0, 1.0, 2.0, 0.5495751823582499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 767970.9725694183, 767970.9725694183, 191727.8888013666], 
processed observation next is [1.0, 0.0, 0.4881516587677725, 0.9, 1.0, 1.0, 0.4573194968171685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21332527015817174, 0.21332527015817174, 0.28616102806174115], 
reward next is 0.7138, 
noisyNet noise sample is [array([-0.61874247], dtype=float32), -0.6974805]. 
=============================================
[2019-03-26 23:27:25,673] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2269733: loss 1.7986
[2019-03-26 23:27:25,676] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2269734: learning rate 0.0010
[2019-03-26 23:27:26,224] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2269983: loss 1.4646
[2019-03-26 23:27:26,226] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2269983: learning rate 0.0010
[2019-03-26 23:27:26,573] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.6692630e-11 1.3886844e-01 1.9848428e-13 8.6113155e-01 1.5342130e-16], sum to 1.0000
[2019-03-26 23:27:26,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0080
[2019-03-26 23:27:26,592] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2386481.494445075 W.
[2019-03-26 23:27:26,596] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.13333333333334, 48.33333333333333, 1.0, 2.0, 0.8532754237363194, 1.0, 2.0, 0.8532754237363194, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2386481.494445075, 2386481.494445075, 446632.9605916767], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5580600.0000, 
sim time next is 5581200.0000, 
raw observation next is [34.06666666666667, 48.66666666666666, 1.0, 2.0, 0.5345262309939475, 1.0, 2.0, 0.5345262309939475, 1.0, 1.0, 0.9171201016511101, 6.911200000000001, 6.9112, 170.5573041426782, 2242353.58316446, 2242353.58316446, 437657.0011325612], 
processed observation next is [1.0, 0.6086956521739131, 0.8135860979462877, 0.4866666666666666, 1.0, 1.0, 0.4391882301131897, 1.0, 1.0, 0.4391882301131897, 1.0, 0.5, 0.8989269532330612, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6228759953234612, 0.6228759953234612, 0.6532194046754645], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06296326], dtype=float32), -0.9863322]. 
=============================================
[2019-03-26 23:27:28,628] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1494493e-16 9.9999988e-01 8.3517299e-19 1.7057816e-07 1.2372542e-22], sum to 1.0000
[2019-03-26 23:27:28,638] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7388
[2019-03-26 23:27:28,649] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2271063: loss 1.6587
[2019-03-26 23:27:28,652] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2132187.598456188 W.
[2019-03-26 23:27:28,655] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2271064: learning rate 0.0010
[2019-03-26 23:27:28,659] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.76666666666667, 75.33333333333334, 1.0, 2.0, 0.7624353966146591, 1.0, 2.0, 0.7624353966146591, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2132187.598456188, 2132187.598456188, 401999.4999156024], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5926800.0000, 
sim time next is 5927400.0000, 
raw observation next is [29.8, 75.5, 1.0, 2.0, 0.4922105356395978, 1.0, 2.0, 0.4922105356395978, 1.0, 1.0, 0.8548069238944744, 6.911200000000001, 6.9112, 170.5573041426782, 2064670.981942749, 2064670.981942748, 409800.9344253234], 
processed observation next is [1.0, 0.6086956521739131, 0.6113744075829385, 0.755, 1.0, 1.0, 0.38820546462602146, 1.0, 1.0, 0.38820546462602146, 1.0, 0.5, 0.8229352730420421, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5735197172063191, 0.5735197172063189, 0.6116431857094379], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.370792], dtype=float32), -1.3000607]. 
=============================================
[2019-03-26 23:27:29,147] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5819245e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 23:27:29,155] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6316
[2019-03-26 23:27:29,159] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.40000000000001, 63.5, 1.0, 2.0, 0.5530221974118666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772789.5498299884, 772789.5498299884, 192321.9829320914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5509800.0000, 
sim time next is 5510400.0000, 
raw observation next is [32.13333333333333, 65.0, 1.0, 2.0, 0.5551705497552017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775792.7394602232, 775792.7394602232, 192692.963921618], 
processed observation next is [1.0, 0.782608695652174, 0.7219589257503949, 0.65, 1.0, 1.0, 0.46406090331952016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21549798318339533, 0.21549798318339533, 0.2876014386889821], 
reward next is 0.7124, 
noisyNet noise sample is [array([-1.7586721], dtype=float32), 0.31973794]. 
=============================================
[2019-03-26 23:27:30,101] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2271709: loss 1.9310
[2019-03-26 23:27:30,105] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2271710: learning rate 0.0010
[2019-03-26 23:27:30,490] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2271884: loss 1.8366
[2019-03-26 23:27:30,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2271884: learning rate 0.0010
[2019-03-26 23:27:31,025] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.329462e-36 0.000000e+00], sum to 1.0000
[2019-03-26 23:27:31,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1912
[2019-03-26 23:27:31,042] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 77.66666666666667, 1.0, 2.0, 0.5208447252183488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727809.5592404157, 727809.5592404157, 186926.6160775571], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5644200.0000, 
sim time next is 5644800.0000, 
raw observation next is [28.5, 77.0, 1.0, 2.0, 0.5224477891720196, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730050.3925329795, 730050.3925329789, 187188.1157313306], 
processed observation next is [0.0, 0.34782608695652173, 0.5497630331753555, 0.77, 1.0, 1.0, 0.4246358905686983, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2027917757036054, 0.20279177570360526, 0.27938524736019493], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.43299794], dtype=float32), -0.4545429]. 
=============================================
[2019-03-26 23:27:31,817] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2272482: loss 1.6659
[2019-03-26 23:27:31,820] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2272482: learning rate 0.0010
[2019-03-26 23:27:33,066] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:27:33,077] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2335
[2019-03-26 23:27:33,083] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.81666666666667, 65.66666666666667, 1.0, 2.0, 0.5311470644255317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742210.7044822134, 742210.7044822134, 188620.4143638385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5680200.0000, 
sim time next is 5680800.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.5343108440677539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746633.2392995686, 746633.239299568, 189146.6838414601], 
processed observation next is [0.0, 0.782608695652174, 0.6492890995260664, 0.67, 1.0, 1.0, 0.4389287277924745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20739812202765795, 0.20739812202765778, 0.2823084833454628], 
reward next is 0.7177, 
noisyNet noise sample is [array([-0.21285649], dtype=float32), 0.4011381]. 
=============================================
[2019-03-26 23:27:33,702] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2273325: loss -21.1019
[2019-03-26 23:27:33,707] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2273326: learning rate 0.0010
[2019-03-26 23:27:34,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2273618: loss 1.6449
[2019-03-26 23:27:34,361] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2273618: learning rate 0.0010
[2019-03-26 23:27:34,939] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2273880: loss 1.8389
[2019-03-26 23:27:34,943] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2273881: learning rate 0.0010
[2019-03-26 23:27:35,623] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2274178: loss -111.6462
[2019-03-26 23:27:35,628] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2274179: learning rate 0.0010
[2019-03-26 23:27:37,382] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2274966: loss 1.9165
[2019-03-26 23:27:37,387] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2274967: learning rate 0.0010
[2019-03-26 23:27:37,470] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 23:27:37,474] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:27:37,476] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:27:37,478] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:27:37,479] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:27:37,481] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:27:37,483] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:27:37,484] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:27:37,484] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:27:37,485] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:27:37,482] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:27:37,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-03-26 23:27:37,527] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-03-26 23:27:37,549] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-03-26 23:27:37,578] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-03-26 23:27:37,579] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-03-26 23:28:15,217] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:28:15,218] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 94.0, 1.0, 2.0, 0.4838468794673307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680584.6426011422, 680584.6426011422, 181681.8817722224]
[2019-03-26 23:28:15,220] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:28:15,223] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9220694241398045
[2019-03-26 23:28:15,564] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:28:15,565] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.71519483, 94.07794545, 1.0, 2.0, 0.6212290812716372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868140.3481818213, 868140.3481818213, 204768.214093223]
[2019-03-26 23:28:15,566] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:28:15,569] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.4031343e-38 0.0000000e+00], sampled 0.3778307457421488
[2019-03-26 23:28:23,259] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:28:23,259] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.31110752, 94.32756692666666, 1.0, 2.0, 0.3598888241737124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 552010.180168318, 552010.1801683174, 170748.0413349285]
[2019-03-26 23:28:23,260] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:28:23,263] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.9613693166902858
[2019-03-26 23:28:28,789] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:28:28,792] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.4, 64.5, 1.0, 2.0, 0.5638357726987666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104262, 787905.9726284618, 787905.9726284618, 194203.2068987637]
[2019-03-26 23:28:28,793] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:28:28,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2732015e-35 0.0000000e+00], sampled 0.5047913115734552
[2019-03-26 23:28:42,485] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:28:42,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5263370743355337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735487.0260210639, 735487.0260210639, 187825.1955856867]
[2019-03-26 23:28:42,487] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:28:42,489] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.05545002088030704
[2019-03-26 23:29:10,703] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:29:10,704] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.25, 89.5, 1.0, 2.0, 0.6709937728360992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 937715.0190934654, 937715.0190934661, 214709.9073613459]
[2019-03-26 23:29:10,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:29:10,708] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2020326e-34 0.0000000e+00], sampled 0.6637300966292685
[2019-03-26 23:29:10,758] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:29:10,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.90038140666667, 64.64484768, 1.0, 2.0, 0.4962898501183982, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 693486.2657077018, 693486.2657077018, 183016.8734497038]
[2019-03-26 23:29:10,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:29:10,764] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.760716e-38 0.000000e+00], sampled 0.4637929477456394
[2019-03-26 23:29:13,220] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578895]
[2019-03-26 23:29:13,221] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.95076747, 84.37318579, 1.0, 2.0, 0.4375773098345188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636857.4100256924, 636857.4100256931, 177642.9596039797]
[2019-03-26 23:29:13,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:29:13,225] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.14297843445315117
[2019-03-26 23:29:31,004] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8267.6584 2926612326.3139 1306.0000
[2019-03-26 23:29:31,176] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7970.5775 3155556521.9112 1556.0000
[2019-03-26 23:29:31,325] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8671.3834 2778594586.1625 894.0000
[2019-03-26 23:29:31,502] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8043.5005 3003879556.8858 1663.0000
[2019-03-26 23:29:31,569] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8512.3803 2841009335.1624 1083.0000
[2019-03-26 23:29:32,586] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2275000, evaluation results [2275000.0, 7970.577494866972, 3155556521.911192, 1556.0, 8267.658381614216, 2926612326.313939, 1306.0, 8671.38341242458, 2778594586.1625443, 894.0, 8043.500496430873, 3003879556.885794, 1663.0, 8512.380258486191, 2841009335.1623507, 1083.0]
[2019-03-26 23:29:33,492] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2275411: loss 2.0213
[2019-03-26 23:29:33,493] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2275411: learning rate 0.0010
[2019-03-26 23:29:33,660] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2673507e-19 2.7953808e-17 2.0745035e-21 1.0000000e+00 2.7927396e-23], sum to 1.0000
[2019-03-26 23:29:33,668] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5485
[2019-03-26 23:29:33,675] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8481033859987173, 1.0, 2.0, 0.8481033859987173, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2372002.363285846, 2372002.363285847, 443970.8067586973], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5824800.0000, 
sim time next is 5825400.0000, 
raw observation next is [31.15, 69.5, 1.0, 2.0, 0.8353424446151421, 1.0, 2.0, 0.8353424446151421, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2336278.797053175, 2336278.797053175, 437446.2903097179], 
processed observation next is [1.0, 0.43478260869565216, 0.6753554502369667, 0.695, 1.0, 1.0, 0.8016174031507736, 1.0, 1.0, 0.8016174031507736, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6489663325147709, 0.6489663325147709, 0.6529049109100267], 
reward next is 0.3471, 
noisyNet noise sample is [array([-1.5137918], dtype=float32), -2.4361908]. 
=============================================
[2019-03-26 23:29:33,699] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2275502: loss -96.3098
[2019-03-26 23:29:33,701] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2275502: learning rate 0.0010
[2019-03-26 23:29:33,988] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2275631: loss 2.2473
[2019-03-26 23:29:33,994] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2275633: learning rate 0.0010
[2019-03-26 23:29:34,358] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2275798: loss 2.2387
[2019-03-26 23:29:34,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2275798: learning rate 0.0010
[2019-03-26 23:29:35,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:29:35,156] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3945
[2019-03-26 23:29:35,161] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.81666666666667, 53.83333333333334, 1.0, 2.0, 0.5579740119941156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779711.722389029, 779711.722389029, 193177.2254882831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5757000.0000, 
sim time next is 5757600.0000, 
raw observation next is [33.63333333333333, 54.66666666666667, 1.0, 2.0, 0.5401031139084347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754730.0946113296, 754730.0946113289, 190118.7845046877], 
processed observation next is [0.0, 0.6521739130434783, 0.7930489731437599, 0.5466666666666667, 1.0, 1.0, 0.4459073661547406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20964724850314712, 0.20964724850314692, 0.28375937985774285], 
reward next is 0.7162, 
noisyNet noise sample is [array([-1.5259688], dtype=float32), 0.9389475]. 
=============================================
[2019-03-26 23:29:35,881] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:29:35,891] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9792
[2019-03-26 23:29:35,896] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 87.5, 1.0, 2.0, 0.5594376765184098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781757.7968147881, 781757.7968147881, 193432.5290155013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5952600.0000, 
sim time next is 5953200.0000, 
raw observation next is [27.76666666666667, 88.0, 1.0, 2.0, 0.5579893811815649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 779733.2071424638, 779733.2071424631, 193180.4869477111], 
processed observation next is [1.0, 0.9130434782608695, 0.515007898894155, 0.88, 1.0, 1.0, 0.4674570857609216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21659255753957327, 0.21659255753957307, 0.2883290849965837], 
reward next is 0.7117, 
noisyNet noise sample is [array([1.2297196], dtype=float32), -1.2627409]. 
=============================================
[2019-03-26 23:29:36,315] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2276677: loss 2.0672
[2019-03-26 23:29:36,318] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2276677: learning rate 0.0010
[2019-03-26 23:29:37,586] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:29:37,595] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3244
[2019-03-26 23:29:37,601] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 86.0, 1.0, 2.0, 0.5443056818944126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760604.7878911024, 760604.7878911024, 190828.5858248018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5781600.0000, 
sim time next is 5782200.0000, 
raw observation next is [27.45, 86.16666666666667, 1.0, 2.0, 0.5434275459420019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759377.2549444822, 759377.2549444829, 190679.5765445616], 
processed observation next is [0.0, 0.9565217391304348, 0.5, 0.8616666666666667, 1.0, 1.0, 0.44991270595421917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2109381263734673, 0.21093812637346748, 0.28459638290233075], 
reward next is 0.7154, 
noisyNet noise sample is [array([0.4203637], dtype=float32), 1.306423]. 
=============================================
[2019-03-26 23:29:38,571] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2277666: loss -214.8040
[2019-03-26 23:29:38,574] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2277668: learning rate 0.0010
[2019-03-26 23:29:39,402] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2278030: loss -263.9944
[2019-03-26 23:29:39,406] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2278030: learning rate 0.0010
[2019-03-26 23:29:40,342] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.125100e-21 1.000000e+00 6.943807e-24 2.074723e-08 9.502306e-28], sum to 1.0000
[2019-03-26 23:29:40,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4100
[2019-03-26 23:29:40,347] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3698673.011771567 W.
[2019-03-26 23:29:40,353] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 76.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 10.0739548631696, 6.9112, 168.8949309045779, 3698673.011771567, 1455145.302622348, 306049.6748903951], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5818800.0000, 
sim time next is 5819400.0000, 
raw observation next is [29.65, 76.0, 1.0, 2.0, 0.6722750550297466, 1.0, 1.0, 0.6567275670291359, 1.0, 1.0, 1.03, 7.005095546171716, 6.9112, 170.5573041426782, 2755557.115693608, 2688295.916544189, 512713.8068548669], 
processed observation next is [1.0, 0.34782608695652173, 0.6042654028436019, 0.76, 1.0, 1.0, 0.605150668710538, 1.0, 0.5, 0.5864187554567902, 1.0, 0.5, 1.0365853658536586, 0.009389554617171569, 0.0, 0.8375144448122397, 0.7654325321371133, 0.7467488657067193, 0.765244487843085], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.435441], dtype=float32), -0.37087536]. 
=============================================
[2019-03-26 23:29:41,841] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2279120: loss -218.8188
[2019-03-26 23:29:41,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2279122: learning rate 0.0010
[2019-03-26 23:29:43,141] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2279703: loss -283.6709
[2019-03-26 23:29:43,143] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2279703: learning rate 0.0010
[2019-03-26 23:29:43,619] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2279920: loss -289.3223
[2019-03-26 23:29:43,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2279920: learning rate 0.0010
[2019-03-26 23:29:43,776] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:29:43,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5644
[2019-03-26 23:29:43,791] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 90.0, 1.0, 2.0, 0.5293183546834279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739654.4246990532, 739654.4246990532, 188317.2765606468], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5875200.0000, 
sim time next is 5875800.0000, 
raw observation next is [26.63333333333333, 90.33333333333333, 1.0, 2.0, 0.5301574870471409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740827.4133099086, 740827.4133099093, 188456.0373991262], 
processed observation next is [1.0, 0.0, 0.46129541864139006, 0.9033333333333333, 1.0, 1.0, 0.4339246831893264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20578539258608572, 0.2057853925860859, 0.2812776677598898], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.17460193], dtype=float32), 0.11467789]. 
=============================================
[2019-03-26 23:29:44,917] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2280499: loss -230.2961
[2019-03-26 23:29:44,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2280499: learning rate 0.0010
[2019-03-26 23:29:46,370] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.5202733e-35 0.0000000e+00], sum to 1.0000
[2019-03-26 23:29:46,379] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5182
[2019-03-26 23:29:46,385] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.6993744570760846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 977395.3280066288, 977395.3280066282, 220709.5056729922], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6065400.0000, 
sim time next is 6066000.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.6735150494075706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941240.0690682312, 941240.0690682305, 215231.6120407878], 
processed observation next is [1.0, 0.21739130434782608, 0.4360189573459717, 0.93, 1.0, 1.0, 0.6066446378404464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2614555747411753, 0.26145557474117515, 0.3212412120011758], 
reward next is 0.6788, 
noisyNet noise sample is [array([2.5411575], dtype=float32), -0.14538936]. 
=============================================
[2019-03-26 23:29:46,403] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[58.93976 ]
 [58.908752]
 [58.948128]
 [59.65275 ]
 [59.29612 ]], R is [[59.04761887]
 [59.12772369]
 [59.20249939]
 [59.2710228 ]
 [59.35381699]].
[2019-03-26 23:29:46,618] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2281256: loss 0.2080
[2019-03-26 23:29:46,621] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2281258: learning rate 0.0010
[2019-03-26 23:29:47,356] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2281588: loss -264.4017
[2019-03-26 23:29:47,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2281589: learning rate 0.0010
[2019-03-26 23:29:47,877] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2281823: loss -86.1358
[2019-03-26 23:29:47,879] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2281824: learning rate 0.0010
[2019-03-26 23:29:48,700] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2282185: loss 0.2002
[2019-03-26 23:29:48,704] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2282186: learning rate 0.0010
[2019-03-26 23:29:50,426] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2282957: loss -284.2942
[2019-03-26 23:29:50,428] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2282957: learning rate 0.0010
[2019-03-26 23:29:51,478] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2283426: loss -210.8949
[2019-03-26 23:29:51,482] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2283428: learning rate 0.0010
[2019-03-26 23:29:51,497] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2283437: loss 0.2626
[2019-03-26 23:29:51,499] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2283437: learning rate 0.0010
[2019-03-26 23:29:52,078] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2283695: loss -182.0520
[2019-03-26 23:29:52,082] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2283696: learning rate 0.0010
[2019-03-26 23:29:52,486] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2283875: loss -77.2998
[2019-03-26 23:29:52,490] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2283876: learning rate 0.0010
[2019-03-26 23:29:53,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4011343e-17 7.4558970e-10 1.5488795e-20 1.0000000e+00 4.0223450e-22], sum to 1.0000
[2019-03-26 23:29:53,502] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6889
[2019-03-26 23:29:53,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.3, 72.33333333333334, 1.0, 2.0, 0.8141781186090354, 1.0, 2.0, 0.8141781186090354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2277032.660648823, 2277032.660648823, 426826.5994531778], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6430200.0000, 
sim time next is 6430800.0000, 
raw observation next is [29.4, 71.66666666666667, 1.0, 2.0, 0.8190879312356849, 1.0, 2.0, 0.8190879312356849, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2290776.643470604, 2290776.643470604, 429264.6647312967], 
processed observation next is [1.0, 0.43478260869565216, 0.5924170616113744, 0.7166666666666667, 1.0, 1.0, 0.7820336520911866, 1.0, 1.0, 0.7820336520911866, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6363268454085012, 0.6363268454085012, 0.6406935294496966], 
reward next is 0.3593, 
noisyNet noise sample is [array([-0.15897126], dtype=float32), -0.22177969]. 
=============================================
[2019-03-26 23:29:54,134] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2284600: loss -65.2885
[2019-03-26 23:29:54,144] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2284603: learning rate 0.0010
[2019-03-26 23:29:56,401] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2285611: loss 0.0880
[2019-03-26 23:29:56,402] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2285611: learning rate 0.0010
[2019-03-26 23:29:57,309] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2286013: loss 0.1466
[2019-03-26 23:29:57,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2286016: learning rate 0.0010
[2019-03-26 23:29:59,721] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2287091: loss 0.0608
[2019-03-26 23:29:59,727] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2287094: learning rate 0.0010
[2019-03-26 23:30:01,145] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2287728: loss 0.1680
[2019-03-26 23:30:01,146] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2287728: learning rate 0.0010
[2019-03-26 23:30:01,638] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2287950: loss 0.0313
[2019-03-26 23:30:01,639] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2287950: learning rate 0.0010
[2019-03-26 23:30:02,869] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2288502: loss 0.1373
[2019-03-26 23:30:02,870] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2288502: learning rate 0.0010
[2019-03-26 23:30:04,626] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2289287: loss 1450.4358
[2019-03-26 23:30:04,631] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2289287: learning rate 0.0010
[2019-03-26 23:30:05,083] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2289496: loss 0.0252
[2019-03-26 23:30:05,088] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2289496: learning rate 0.0010
[2019-03-26 23:30:05,269] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:30:05,276] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0703
[2019-03-26 23:30:05,281] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.96666666666667, 80.0, 1.0, 2.0, 0.5279106497349693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 737686.6547711431, 737686.6547711438, 188084.2933918417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6376800.0000, 
sim time next is 6377400.0000, 
raw observation next is [27.9, 80.5, 1.0, 2.0, 0.5284218663403171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 738401.2619552452, 738401.2619552445, 188168.6547549789], 
processed observation next is [0.0, 0.8260869565217391, 0.5213270142180094, 0.805, 1.0, 1.0, 0.43183357390399646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20511146165423477, 0.20511146165423458, 0.28084873844026703], 
reward next is 0.7192, 
noisyNet noise sample is [array([-1.0703797], dtype=float32), 1.561841]. 
=============================================
[2019-03-26 23:30:05,843] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2289835: loss 0.2621
[2019-03-26 23:30:05,846] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2289835: learning rate 0.0010
[2019-03-26 23:30:06,655] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2290201: loss 1354.9266
[2019-03-26 23:30:06,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2290201: learning rate 0.0010
[2019-03-26 23:30:08,198] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2290893: loss 0.0399
[2019-03-26 23:30:08,199] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2290893: learning rate 0.0010
[2019-03-26 23:30:08,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9419315e-26 1.0000000e+00 2.7179383e-31 2.9697107e-19 2.3991162e-35], sum to 1.0000
[2019-03-26 23:30:08,538] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7805
[2019-03-26 23:30:08,547] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1757309.165639929 W.
[2019-03-26 23:30:08,554] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.15, 71.0, 1.0, 2.0, 0.6284948507479771, 1.0, 2.0, 0.6284948507479771, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1757309.165639929, 1757309.165639929, 345009.5973936422], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6514200.0000, 
sim time next is 6514800.0000, 
raw observation next is [29.36666666666667, 69.33333333333334, 1.0, 2.0, 0.4395596170379151, 1.0, 2.0, 0.4395596170379151, 1.0, 1.0, 0.7495088953183096, 6.9112, 6.9112, 170.5573041426782, 1843626.634524863, 1843626.634524863, 373681.2742778424], 
processed observation next is [1.0, 0.391304347826087, 0.5908372827804109, 0.6933333333333335, 1.0, 1.0, 0.3247706229372471, 1.0, 1.0, 0.3247706229372471, 1.0, 0.5, 0.6945230430711092, 0.0, 0.0, 0.8375144448122397, 0.5121185095902397, 0.5121185095902397, 0.5577332451908096], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.96802264], dtype=float32), -0.8720762]. 
=============================================
[2019-03-26 23:30:09,382] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2291408: loss 0.0128
[2019-03-26 23:30:09,385] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2291409: learning rate 0.0010
[2019-03-26 23:30:09,690] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2291545: loss 540.2828
[2019-03-26 23:30:09,693] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2291546: learning rate 0.0010
[2019-03-26 23:30:09,920] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2291648: loss 0.0156
[2019-03-26 23:30:09,924] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2291648: learning rate 0.0010
[2019-03-26 23:30:10,528] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2291922: loss 0.0187
[2019-03-26 23:30:10,529] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2291922: learning rate 0.0010
[2019-03-26 23:30:12,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2292703: loss 0.0079
[2019-03-26 23:30:12,277] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2292704: learning rate 0.0010
[2019-03-26 23:30:14,440] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:30:14,448] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7494
[2019-03-26 23:30:14,454] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.9, 74.33333333333334, 1.0, 2.0, 0.4874215611961167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681090.2673966298, 681090.2673966304, 181648.7144888597], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6553200.0000, 
sim time next is 6553800.0000, 
raw observation next is [27.85, 75.0, 1.0, 2.0, 0.4895861670088543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 684115.9166385791, 684115.9166385791, 181980.4981491886], 
processed observation next is [1.0, 0.8695652173913043, 0.5189573459715641, 0.75, 1.0, 1.0, 0.3850435747094631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19003219906627197, 0.19003219906627197, 0.2716126838047591], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.2002792], dtype=float32), -1.3661405]. 
=============================================
[2019-03-26 23:30:14,707] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2293795: loss 333.3724
[2019-03-26 23:30:14,709] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2293795: learning rate 0.0010
[2019-03-26 23:30:15,287] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2294053: loss 564.4950
[2019-03-26 23:30:15,288] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2294053: learning rate 0.0010
[2019-03-26 23:30:17,927] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2295240: loss 1084.0112
[2019-03-26 23:30:17,929] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2295241: learning rate 0.0010
[2019-03-26 23:30:18,829] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.8282574e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 23:30:18,837] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6439
[2019-03-26 23:30:18,840] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 83.0, 1.0, 2.0, 0.5221058914024111, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729572.4723983603, 729572.472398361, 187132.1495908636], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6472800.0000, 
sim time next is 6473400.0000, 
raw observation next is [27.45, 83.66666666666667, 1.0, 2.0, 0.5240114768023031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732236.1884234525, 732236.1884234525, 187443.7484477302], 
processed observation next is [1.0, 0.9565217391304348, 0.5, 0.8366666666666667, 1.0, 1.0, 0.4265198515690398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2033989412287368, 0.2033989412287368, 0.2797667887279555], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.68730795], dtype=float32), 0.098809116]. 
=============================================
[2019-03-26 23:30:19,029] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2295731: loss 743.2202
[2019-03-26 23:30:19,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2295733: learning rate 0.0010
[2019-03-26 23:30:19,570] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2295970: loss 1472.3370
[2019-03-26 23:30:19,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2295970: learning rate 0.0010
[2019-03-26 23:30:19,951] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:30:19,958] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1833
[2019-03-26 23:30:19,965] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 86.66666666666667, 1.0, 2.0, 0.5128216749064335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716594.6577153507, 716594.65771535, 185629.3045243928], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6644400.0000, 
sim time next is 6645000.0000, 
raw observation next is [26.63333333333333, 86.83333333333333, 1.0, 2.0, 0.5122795829055696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715836.9067466138, 715836.9067466138, 185542.3660197811], 
processed observation next is [1.0, 0.9130434782608695, 0.46129541864139006, 0.8683333333333333, 1.0, 1.0, 0.4123850396452646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19884358520739273, 0.19884358520739273, 0.276928904507136], 
reward next is 0.7231, 
noisyNet noise sample is [array([2.143235], dtype=float32), 0.9930136]. 
=============================================
[2019-03-26 23:30:19,982] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[63.472248]
 [63.45091 ]
 [63.522312]
 [63.45764 ]
 [63.633812]], R is [[63.9262085 ]
 [64.0098877 ]
 [64.09283447]
 [64.17512512]
 [64.25672913]].
[2019-03-26 23:30:20,730] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2296488: loss 480.6634
[2019-03-26 23:30:20,732] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2296488: learning rate 0.0010
[2019-03-26 23:30:22,247] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2297161: loss 16.5944
[2019-03-26 23:30:22,250] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2297163: learning rate 0.0010
[2019-03-26 23:30:23,003] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2297500: loss 819.1821
[2019-03-26 23:30:23,007] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2297501: learning rate 0.0010
[2019-03-26 23:30:23,763] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2297843: loss 1375.3496
[2019-03-26 23:30:23,767] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2297845: learning rate 0.0010
[2019-03-26 23:30:24,317] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:30:24,325] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6946
[2019-03-26 23:30:24,334] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2298092: loss 11.0810
[2019-03-26 23:30:24,336] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 85.0, 1.0, 2.0, 0.518071380718566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 723932.8674980478, 723932.8674980478, 186476.4307149933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6632400.0000, 
sim time next is 6633000.0000, 
raw observation next is [27.25, 85.0, 1.0, 2.0, 0.5196585907159637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726151.5313130427, 726151.5313130427, 186733.8434060903], 
processed observation next is [1.0, 0.782608695652174, 0.490521327014218, 0.85, 1.0, 1.0, 0.421275410501161, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20170875869806743, 0.20170875869806743, 0.2787072289643139], 
reward next is 0.7213, 
noisyNet noise sample is [array([-0.06571288], dtype=float32), 0.5068146]. 
=============================================
[2019-03-26 23:30:24,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2298092: learning rate 0.0010
[2019-03-26 23:30:24,369] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[62.372032]
 [62.250282]
 [61.66072 ]
 [60.33592 ]
 [59.426277]], R is [[61.89946365]
 [62.00214386]
 [62.10427094]
 [62.20663452]
 [62.30918884]].
[2019-03-26 23:30:25,975] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2298828: loss 534.9620
[2019-03-26 23:30:25,978] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2298830: learning rate 0.0010
[2019-03-26 23:30:27,131] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2299344: loss 15.8365
[2019-03-26 23:30:27,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2299344: learning rate 0.0010
[2019-03-26 23:30:27,259] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2299399: loss 869.7921
[2019-03-26 23:30:27,263] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2299402: learning rate 0.0010
[2019-03-26 23:30:27,963] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2299714: loss 1392.3296
[2019-03-26 23:30:27,966] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2299715: learning rate 0.0010
[2019-03-26 23:30:28,555] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2299975: loss 517.9114
[2019-03-26 23:30:28,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2299975: learning rate 0.0010
[2019-03-26 23:30:28,618] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-03-26 23:30:28,619] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:30:28,621] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:30:28,621] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:30:28,623] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:30:28,622] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:30:28,625] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:30:28,626] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:30:28,627] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:30:28,623] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:30:28,629] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:30:28,653] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-03-26 23:30:28,679] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-03-26 23:30:28,700] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-03-26 23:30:28,701] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-03-26 23:30:28,702] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-03-26 23:31:17,138] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578093]
[2019-03-26 23:31:17,139] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.33333333333334, 64.33333333333333, 1.0, 2.0, 0.8829438955290907, 1.0, 2.0, 0.8829438955290907, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2469541.688865676, 2469541.688865676, 462273.8001314937]
[2019-03-26 23:31:17,142] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:31:17,145] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0852500e-25 1.0000000e+00 6.4631170e-30 1.8273059e-18 5.2339988e-35], sampled 0.47970010289512643
[2019-03-26 23:31:17,146] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2469541.688865676 W.
[2019-03-26 23:31:18,972] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578093]
[2019-03-26 23:31:18,974] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.6450945540044076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 901505.4521294803, 901505.4521294803, 209436.7986959239]
[2019-03-26 23:31:18,975] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:31:18,978] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.8535543654569543
[2019-03-26 23:31:41,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2578093]
[2019-03-26 23:31:41,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.64206554833333, 73.62419093333332, 1.0, 2.0, 0.8073078207291213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1128315.352794613, 1128315.352794613, 245720.8777627849]
[2019-03-26 23:31:41,487] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:31:41,489] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.06721814579884733
[2019-03-26 23:32:20,007] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8257.6097 2927643248.4466 1331.0000
[2019-03-26 23:32:20,345] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.9890 2842794159.4554 1124.0000
[2019-03-26 23:32:20,466] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.6075 3008012635.3836 1763.0000
[2019-03-26 23:32:20,621] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.3876 3164088655.7384 1764.0000
[2019-03-26 23:32:20,727] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8663.4360 2779422802.9378 923.0000
[2019-03-26 23:32:21,747] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2300000, evaluation results [2300000.0, 7885.387555666899, 3164088655.7383957, 1764.0, 8257.609702783173, 2927643248.446561, 1331.0, 8663.4360049064, 2779422802.937841, 923.0, 7997.607502150157, 3008012635.3836417, 1763.0, 8498.989012794147, 2842794159.4553633, 1124.0]
[2019-03-26 23:32:22,089] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3699627e-36 1.0000000e+00 0.0000000e+00 4.5474368e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 23:32:22,097] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7938
[2019-03-26 23:32:22,100] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 48.0, 1.0, 2.0, 0.958603078955185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1489234.480603889, 1489234.480603889, 307681.268783711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6795000.0000, 
sim time next is 6795600.0000, 
raw observation next is [29.23333333333333, 48.33333333333333, 1.0, 2.0, 0.9685269977396771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1502575.608151765, 1502575.608151765, 310683.3331273543], 
processed observation next is [1.0, 0.6521739130434783, 0.5845181674565559, 0.4833333333333333, 1.0, 1.0, 0.9620807201682856, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4173821133754903, 0.4173821133754903, 0.46370646735426013], 
reward next is 0.5363, 
noisyNet noise sample is [array([0.12896742], dtype=float32), -1.2939492]. 
=============================================
[2019-03-26 23:32:23,248] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2300675: loss 965.1583
[2019-03-26 23:32:23,250] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2300675: learning rate 0.0010
[2019-03-26 23:32:25,445] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2301666: loss 14.6304
[2019-03-26 23:32:25,448] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2301666: learning rate 0.0010
[2019-03-26 23:32:26,147] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2301976: loss 13.0655
[2019-03-26 23:32:26,149] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2301977: learning rate 0.0010
[2019-03-26 23:32:28,830] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2303151: loss 17.3638
[2019-03-26 23:32:28,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2303152: learning rate 0.0010
[2019-03-26 23:32:29,915] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2303641: loss 14.3400
[2019-03-26 23:32:29,917] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2303642: learning rate 0.0010
[2019-03-26 23:32:30,330] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2303830: loss 14.5330
[2019-03-26 23:32:30,332] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2303830: learning rate 0.0010
[2019-03-26 23:32:31,691] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2304439: loss 14.3577
[2019-03-26 23:32:31,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2304439: learning rate 0.0010
[2019-03-26 23:32:31,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:32:31,809] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5612
[2019-03-26 23:32:31,814] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.15, 94.5, 1.0, 2.0, 0.5293945642474459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753184.2030502071, 753184.2030502065, 190045.4234577006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7101000.0000, 
sim time next is 7101600.0000, 
raw observation next is [24.1, 94.66666666666666, 1.0, 2.0, 0.5528481404135185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787519.6662528536, 787519.6662528529, 194206.4304038058], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.9466666666666665, 1.0, 1.0, 0.46126281977532346, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2187554628480149, 0.2187554628480147, 0.2898603438862773], 
reward next is 0.7101, 
noisyNet noise sample is [array([0.16962421], dtype=float32), -1.1533461]. 
=============================================
[2019-03-26 23:32:31,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:32:31,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4217
[2019-03-26 23:32:31,902] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.66666666666667, 1.0, 2.0, 0.5778778499485238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807535.8769784406, 807535.8769784413, 196691.0986482591], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7184400.0000, 
sim time next is 7185000.0000, 
raw observation next is [25.8, 89.83333333333333, 1.0, 2.0, 0.5707306822529502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797544.5592902916, 797544.5592902916, 195414.4910698795], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.8983333333333333, 1.0, 1.0, 0.4828080509071689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22154015535841434, 0.22154015535841434, 0.29166341950728286], 
reward next is 0.7083, 
noisyNet noise sample is [array([0.12945004], dtype=float32), -1.0133328]. 
=============================================
[2019-03-26 23:32:31,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[63.5481  ]
 [63.682076]
 [64.04895 ]
 [63.176525]
 [64.15927 ]], R is [[63.58032608]
 [63.6509552 ]
 [63.72109985]
 [63.79633331]
 [63.84642029]].
[2019-03-26 23:32:33,665] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2305323: loss -52.4427
[2019-03-26 23:32:33,666] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2305323: learning rate 0.0010
[2019-03-26 23:32:34,077] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2305506: loss 13.2882
[2019-03-26 23:32:34,080] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2305508: learning rate 0.0010
[2019-03-26 23:32:34,810] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2305835: loss 12.4870
[2019-03-26 23:32:34,813] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2305835: learning rate 0.0010
[2019-03-26 23:32:35,456] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2306128: loss -6.0477
[2019-03-26 23:32:35,459] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2306129: learning rate 0.0010
[2019-03-26 23:32:37,050] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2306838: loss 15.9913
[2019-03-26 23:32:37,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2306838: learning rate 0.0010
[2019-03-26 23:32:38,215] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2307362: loss 14.3052
[2019-03-26 23:32:38,218] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2307363: learning rate 0.0010
[2019-03-26 23:32:38,633] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2307547: loss 1.5140
[2019-03-26 23:32:38,635] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2307548: learning rate 0.0010
[2019-03-26 23:32:38,874] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2307657: loss 11.2302
[2019-03-26 23:32:38,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2307657: learning rate 0.0010
[2019-03-26 23:32:39,731] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2308039: loss 12.8141
[2019-03-26 23:32:39,733] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2308040: learning rate 0.0010
[2019-03-26 23:32:41,311] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2308745: loss 14.7762
[2019-03-26 23:32:41,314] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2308746: learning rate 0.0010
[2019-03-26 23:32:43,801] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2309854: loss -123.9550
[2019-03-26 23:32:43,805] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2309856: learning rate 0.0010
[2019-03-26 23:32:44,282] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2310070: loss -13.7023
[2019-03-26 23:32:44,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2310071: learning rate 0.0010
[2019-03-26 23:32:46,918] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2311257: loss 0.8529
[2019-03-26 23:32:46,920] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2311257: learning rate 0.0010
[2019-03-26 23:32:48,000] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2311717: loss -62.9192
[2019-03-26 23:32:48,005] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2311718: learning rate 0.0010
[2019-03-26 23:32:48,501] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2311940: loss -116.9003
[2019-03-26 23:32:48,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2311942: learning rate 0.0010
[2019-03-26 23:32:48,785] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.0918388e-18 1.1419162e-11 3.3105988e-21 1.0000000e+00 9.2425912e-25], sum to 1.0000
[2019-03-26 23:32:48,794] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8688
[2019-03-26 23:32:48,799] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.93333333333334, 52.0, 1.0, 2.0, 0.4646643458483691, 1.0, 1.0, 0.4646643458483691, 1.0, 2.0, 0.7746251240618942, 6.911199999999999, 6.9112, 170.5573041426782, 1949018.127032355, 1949018.127032356, 386002.2835959479], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7040400.0000, 
sim time next is 7041000.0000, 
raw observation next is [31.01666666666667, 51.5, 1.0, 2.0, 0.6989863032638222, 1.0, 2.0, 0.6989863032638222, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1959266.626361361, 1959266.626361361, 374266.4335372274], 
processed observation next is [1.0, 0.4782608695652174, 0.6690363349131123, 0.515, 1.0, 1.0, 0.6373328954985809, 1.0, 1.0, 0.6373328954985809, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5442407295448225, 0.5442407295448225, 0.5586066172197424], 
reward next is 0.4414, 
noisyNet noise sample is [array([1.0045831], dtype=float32), 0.30152103]. 
=============================================
[2019-03-26 23:32:48,817] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[42.060806]
 [45.26581 ]
 [44.693104]
 [43.712887]
 [43.096786]], R is [[42.83432007]
 [42.40597916]
 [42.21376801]
 [42.2477417 ]
 [41.82526398]].
[2019-03-26 23:32:49,754] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2312502: loss -4.9760
[2019-03-26 23:32:49,757] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2312502: learning rate 0.0010
[2019-03-26 23:32:51,232] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2313168: loss 0.0476
[2019-03-26 23:32:51,234] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2313169: learning rate 0.0010
[2019-03-26 23:32:52,144] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2313572: loss -127.1804
[2019-03-26 23:32:52,145] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2313572: learning rate 0.0010
[2019-03-26 23:32:52,993] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2313950: loss 0.0103
[2019-03-26 23:32:52,998] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2313951: learning rate 0.0010
[2019-03-26 23:32:53,016] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2313960: loss -25.4253
[2019-03-26 23:32:53,018] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2313960: learning rate 0.0010
[2019-03-26 23:32:55,103] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2314887: loss -124.9183
[2019-03-26 23:32:55,111] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2314889: learning rate 0.0010
[2019-03-26 23:32:56,160] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2315363: loss -190.0223
[2019-03-26 23:32:56,163] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2315364: learning rate 0.0010
[2019-03-26 23:32:56,253] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2315404: loss 0.0489
[2019-03-26 23:32:56,255] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2315404: learning rate 0.0010
[2019-03-26 23:32:56,824] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2315662: loss 1.5990
[2019-03-26 23:32:56,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2315663: learning rate 0.0010
[2019-03-26 23:32:57,795] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2316098: loss -363.3468
[2019-03-26 23:32:57,796] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2316098: learning rate 0.0010
[2019-03-26 23:32:59,039] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2316658: loss -85.2527
[2019-03-26 23:32:59,042] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2316659: learning rate 0.0010
[2019-03-26 23:33:01,465] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2317744: loss 0.0089
[2019-03-26 23:33:01,467] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2317745: learning rate 0.0010
[2019-03-26 23:33:02,135] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2318042: loss 0.0031
[2019-03-26 23:33:02,139] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2318044: learning rate 0.0010
[2019-03-26 23:33:02,695] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:33:02,708] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6342
[2019-03-26 23:33:02,714] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.0, 1.0, 2.0, 0.4480936154768843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660530.4493628057, 660530.4493628057, 180205.2251658387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7621200.0000, 
sim time next is 7621800.0000, 
raw observation next is [23.28333333333333, 94.83333333333334, 1.0, 2.0, 0.4266384581965208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628160.9977933567, 628160.9977933567, 176974.5281979145], 
processed observation next is [1.0, 0.21739130434782608, 0.3025276461295418, 0.9483333333333335, 1.0, 1.0, 0.3092029616825551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17448916605371018, 0.17448916605371018, 0.264141086862559], 
reward next is 0.7359, 
noisyNet noise sample is [array([0.76646245], dtype=float32), 0.03874392]. 
=============================================
[2019-03-26 23:33:04,664] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2319167: loss 0.0056
[2019-03-26 23:33:04,667] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2319169: learning rate 0.0010
[2019-03-26 23:33:05,672] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2319623: loss 0.0285
[2019-03-26 23:33:05,676] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2319623: learning rate 0.0010
[2019-03-26 23:33:06,260] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2319891: loss 0.0072
[2019-03-26 23:33:06,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2319892: learning rate 0.0010
[2019-03-26 23:33:06,300] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:33:06,307] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1590
[2019-03-26 23:33:06,315] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.63333333333333, 90.33333333333334, 1.0, 2.0, 0.6114700420070801, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 983143.4715254193, 983143.4715254187, 217231.662735039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7400400.0000, 
sim time next is 7401000.0000, 
raw observation next is [20.61666666666667, 90.16666666666666, 1.0, 2.0, 0.5934007741524251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954769.8846048559, 954769.8846048552, 213409.0474973637], 
processed observation next is [1.0, 0.6521739130434783, 0.1761453396524489, 0.9016666666666666, 1.0, 1.0, 0.510121414641476, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2652138568346822, 0.265213856834682, 0.3185209664139757], 
reward next is 0.6815, 
noisyNet noise sample is [array([-0.65732634], dtype=float32), 0.6166834]. 
=============================================
[2019-03-26 23:33:06,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[72.00625 ]
 [71.69276 ]
 [72.386696]
 [72.60657 ]
 [73.39544 ]], R is [[72.15856171]
 [72.11274719]
 [72.06060791]
 [72.02555847]
 [71.996315  ]].
[2019-03-26 23:33:07,496] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2320435: loss 0.0344
[2019-03-26 23:33:07,500] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2320439: learning rate 0.0010
[2019-03-26 23:33:07,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:33:07,782] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9512
[2019-03-26 23:33:07,788] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.83333333333333, 1.0, 2.0, 0.4075648238224539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 598425.8607098386, 598425.860709838, 174103.2615890897], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7516200.0000, 
sim time next is 7516800.0000, 
raw observation next is [23.6, 93.0, 1.0, 2.0, 0.4084714586318052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599274.7086393032, 599274.7086393038, 174167.2460016372], 
processed observation next is [0.0, 0.0, 0.3175355450236968, 0.93, 1.0, 1.0, 0.28731501039976526, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1664651968442509, 0.16646519684425107, 0.2599511134352794], 
reward next is 0.7400, 
noisyNet noise sample is [array([-1.1478797], dtype=float32), -1.2691832]. 
=============================================
[2019-03-26 23:33:09,469] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2321308: loss -2.1532
[2019-03-26 23:33:09,469] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2321308: learning rate 0.0010
[2019-03-26 23:33:09,867] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2321486: loss 0.0040
[2019-03-26 23:33:09,869] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2321486: learning rate 0.0010
[2019-03-26 23:33:10,807] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2321905: loss 0.0087
[2019-03-26 23:33:10,808] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2321906: learning rate 0.0010
[2019-03-26 23:33:11,225] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2322088: loss -28.1938
[2019-03-26 23:33:11,230] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2322088: learning rate 0.0010
[2019-03-26 23:33:13,002] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2322888: loss 0.0026
[2019-03-26 23:33:13,003] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2322888: learning rate 0.0010
[2019-03-26 23:33:13,933] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2323300: loss 0.0356
[2019-03-26 23:33:13,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2323300: learning rate 0.0010
[2019-03-26 23:33:14,526] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2323564: loss -111.9049
[2019-03-26 23:33:14,530] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2323564: learning rate 0.0010
[2019-03-26 23:33:14,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.5207284e-18 9.0640879e-06 2.4094102e-22 9.9999094e-01 4.3977062e-27], sum to 1.0000
[2019-03-26 23:33:14,599] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8510
[2019-03-26 23:33:14,604] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.7266206949582174, 1.0, 2.0, 0.7266206949582174, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2031935.056284019, 2031935.056284018, 385727.5281103501], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7727400.0000, 
sim time next is 7728000.0000, 
raw observation next is [30.7, 67.0, 1.0, 2.0, 0.7380721436267708, 1.0, 2.0, 0.7380721436267708, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2063988.940429311, 2063988.940429311, 390847.3160865484], 
processed observation next is [1.0, 0.43478260869565216, 0.6540284360189573, 0.67, 1.0, 1.0, 0.6844242694298442, 1.0, 1.0, 0.6844242694298442, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5733302612303641, 0.5733302612303641, 0.5833542031142513], 
reward next is 0.4166, 
noisyNet noise sample is [array([-0.19900167], dtype=float32), -0.85112137]. 
=============================================
[2019-03-26 23:33:14,616] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[45.945698]
 [45.82683 ]
 [45.33714 ]
 [44.55903 ]
 [45.56511 ]], R is [[46.20022964]
 [46.16251373]
 [46.13503265]
 [46.10543442]
 [45.64437866]].
[2019-03-26 23:33:14,672] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2323626: loss 0.0107
[2019-03-26 23:33:14,676] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2323626: learning rate 0.0010
[2019-03-26 23:33:15,643] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2324061: loss 0.0065
[2019-03-26 23:33:15,645] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2324061: learning rate 0.0010
[2019-03-26 23:33:15,739] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.235854e-29 0.000000e+00], sum to 1.0000
[2019-03-26 23:33:15,747] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9908
[2019-03-26 23:33:15,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2073278.14897024 W.
[2019-03-26 23:33:15,763] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.8, 61.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.783888787997312, 6.9112, 168.9085003689616, 2073278.14897024, 1454179.020198623, 311350.7647980494], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7736400.0000, 
sim time next is 7737000.0000, 
raw observation next is [31.86666666666667, 60.66666666666667, 1.0, 2.0, 0.4192348812598136, 1.0, 1.0, 0.4192348812598136, 1.0, 1.0, 0.7222411323003048, 6.9112, 6.9112, 170.5573041426782, 1758309.496361674, 1758309.496361674, 362929.645839092], 
processed observation next is [1.0, 0.5652173913043478, 0.7093206951026858, 0.6066666666666667, 1.0, 1.0, 0.300282989469655, 1.0, 0.5, 0.300282989469655, 1.0, 0.5, 0.6612696735369572, 0.0, 0.0, 0.8375144448122397, 0.48841930454490945, 0.48841930454490945, 0.5416860385658089], 
reward next is 0.4583, 
noisyNet noise sample is [array([2.9749875], dtype=float32), -0.7852377]. 
=============================================
[2019-03-26 23:33:15,774] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[51.254585]
 [51.582256]
 [46.940933]
 [46.769638]
 [46.407772]], R is [[48.54539871]
 [48.05994415]
 [47.5793457 ]
 [47.61192703]
 [47.17024612]].
[2019-03-26 23:33:16,796] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2324573: loss 0.0184
[2019-03-26 23:33:16,798] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2324575: learning rate 0.0010
[2019-03-26 23:33:17,750] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-03-26 23:33:17,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:33:17,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:33:17,756] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:33:17,756] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:33:17,757] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:33:17,759] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:33:17,760] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:33:17,760] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:33:17,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:33:17,763] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:33:17,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-03-26 23:33:17,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-03-26 23:33:17,835] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-03-26 23:33:17,835] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-03-26 23:33:17,872] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-03-26 23:34:18,256] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2171892]
[2019-03-26 23:34:18,257] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.76666666666667, 49.33333333333333, 1.0, 2.0, 0.6643847747818513, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005973032980366, 6.9112, 168.9123637683246, 1825291.660396713, 1758056.644903444, 377108.5765921691]
[2019-03-26 23:34:18,260] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:34:18,263] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2500867e-23 1.0000000e+00 1.9504226e-28 2.2255936e-12 1.3803809e-33], sampled 0.8937647147899792
[2019-03-26 23:34:18,264] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1825291.660396713 W.
[2019-03-26 23:34:27,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2171892]
[2019-03-26 23:34:27,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 66.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.367280860551963, 6.9112, 168.9103458064624, 2618593.578452015, 2295039.119791543, 475652.5586953456]
[2019-03-26 23:34:27,060] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:34:27,064] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.14891714e-17 9.99862909e-01 9.20186935e-22 1.37110197e-04
 7.16151657e-26], sampled 0.9491422477334166
[2019-03-26 23:34:27,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2618593.578452015 W.
[2019-03-26 23:35:12,156] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8266.1473 2926934398.9143 1315.0000
[2019-03-26 23:35:12,571] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7905.9950 3161879651.0893 1712.0000
[2019-03-26 23:35:12,581] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8504.1420 2841844394.0272 1104.0000
[2019-03-26 23:35:12,726] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8010.7353 3006973699.6060 1739.0000
[2019-03-26 23:35:12,801] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8667.0852 2779022333.6364 909.0000
[2019-03-26 23:35:13,821] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 2325000, evaluation results [2325000.0, 7905.995013405302, 3161879651.089297, 1712.0, 8266.147298684913, 2926934398.914342, 1315.0, 8667.085230189376, 2779022333.636372, 909.0, 8010.7353100544415, 3006973699.6059737, 1739.0, 8504.142005622065, 2841844394.0271764, 1104.0]
[2019-03-26 23:35:14,039] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:35:14,047] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0791
[2019-03-26 23:35:14,056] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 90.83333333333334, 1.0, 2.0, 0.3804741892488303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571961.4395648274, 571961.4395648274, 172124.7326947786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7531800.0000, 
sim time next is 7532400.0000, 
raw observation next is [23.13333333333333, 90.66666666666667, 1.0, 2.0, 0.3780343696340843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 569319.488506724, 569319.4885067233, 171924.0297224739], 
processed observation next is [0.0, 0.17391304347826086, 0.29541864139020524, 0.9066666666666667, 1.0, 1.0, 0.25064381883624615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15814430236297888, 0.1581443023629787, 0.25660302943652824], 
reward next is 0.7434, 
noisyNet noise sample is [array([-1.8014505], dtype=float32), 0.44357064]. 
=============================================
[2019-03-26 23:35:15,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:15,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:15,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-03-26 23:35:15,563] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2325814: loss -291.5854
[2019-03-26 23:35:15,566] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2325814: learning rate 0.0010
[2019-03-26 23:35:16,004] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2326063: loss 64.6029
[2019-03-26 23:35:16,005] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2326063: learning rate 0.0010
[2019-03-26 23:35:16,551] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:16,552] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:16,732] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-03-26 23:35:17,908] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2327077: loss 183.3609
[2019-03-26 23:35:17,911] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2327077: learning rate 0.0010
[2019-03-26 23:35:18,620] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2327396: loss 205.6321
[2019-03-26 23:35:18,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2327398: learning rate 0.0010
[2019-03-26 23:35:19,035] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:19,035] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:19,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-03-26 23:35:19,203] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2327650: loss -121.2316
[2019-03-26 23:35:19,207] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2327650: learning rate 0.0010
[2019-03-26 23:35:20,065] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9799675e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:35:20,071] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2660
[2019-03-26 23:35:20,076] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.6, 72.0, 1.0, 2.0, 0.4854204658592268, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678293.1779866985, 678293.1779866978, 181346.3968766941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7838400.0000, 
sim time next is 7839000.0000, 
raw observation next is [29.45, 73.0, 1.0, 2.0, 0.4821109437376629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 673667.2130141779, 673667.2130141773, 180844.0317032763], 
processed observation next is [1.0, 0.7391304347826086, 0.5947867298578199, 0.73, 1.0, 1.0, 0.37603728161164207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18712978139282718, 0.18712978139282702, 0.2699164652287706], 
reward next is 0.7301, 
noisyNet noise sample is [array([1.1680866], dtype=float32), 0.034617897]. 
=============================================
[2019-03-26 23:35:20,084] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[57.539665]
 [43.71452 ]
 [31.06975 ]
 [31.066906]
 [31.203423]], R is [[63.89761734]
 [63.98797226]
 [63.34809494]
 [62.71461487]
 [62.47928238]].
[2019-03-26 23:35:20,152] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.4101687e-18 9.9999988e-01 1.2828756e-21 1.3094306e-07 1.5245949e-25], sum to 1.0000
[2019-03-26 23:35:20,155] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2328188: loss 23.8837
[2019-03-26 23:35:20,156] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2328188: learning rate 0.0010
[2019-03-26 23:35:20,160] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2643
[2019-03-26 23:35:20,168] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1963888.287781478 W.
[2019-03-26 23:35:20,171] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.81666666666667, 76.83333333333333, 1.0, 2.0, 0.7023094296092415, 1.0, 2.0, 0.7023094296092415, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1963888.287781478, 1963888.287781478, 375112.8649678684], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7722600.0000, 
sim time next is 7723200.0000, 
raw observation next is [29.03333333333333, 75.66666666666667, 1.0, 2.0, 0.4677755939003634, 1.0, 2.0, 0.4677755939003634, 1.0, 1.0, 0.8064408945072046, 6.9112, 6.9112, 170.5573041426782, 1962080.096556378, 1962080.096556378, 392582.8760408943], 
processed observation next is [1.0, 0.391304347826087, 0.5750394944707741, 0.7566666666666667, 1.0, 1.0, 0.35876577578357033, 1.0, 1.0, 0.35876577578357033, 1.0, 0.5, 0.7639523103746397, 0.0, 0.0, 0.8375144448122397, 0.5450222490434383, 0.5450222490434383, 0.5859445911058124], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.58713114], dtype=float32), -0.27047068]. 
=============================================
[2019-03-26 23:35:22,604] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2329291: loss 141.4633
[2019-03-26 23:35:22,609] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2329293: learning rate 0.0010
[2019-03-26 23:35:23,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:35:23,134] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9163
[2019-03-26 23:35:23,139] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 90.0, 1.0, 2.0, 0.5267780883679298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736103.4988143583, 736103.498814359, 187897.9156275762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7770600.0000, 
sim time next is 7771200.0000, 
raw observation next is [26.5, 90.33333333333334, 1.0, 2.0, 0.527664243299526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 737342.2142358475, 737342.2142358475, 188043.839472033], 
processed observation next is [1.0, 0.9565217391304348, 0.4549763033175356, 0.9033333333333334, 1.0, 1.0, 0.43092077505966986, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20481728173217986, 0.20481728173217986, 0.2806624469731836], 
reward next is 0.7193, 
noisyNet noise sample is [array([1.3873857], dtype=float32), 0.32576638]. 
=============================================
[2019-03-26 23:35:23,301] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2329601: loss -121.7953
[2019-03-26 23:35:23,304] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2329602: learning rate 0.0010
[2019-03-26 23:35:23,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:23,813] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:23,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-03-26 23:35:24,383] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:24,384] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:24,391] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-03-26 23:35:24,914] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:35:24,921] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9228
[2019-03-26 23:35:24,926] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 86.0, 1.0, 2.0, 0.4989857216511807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697254.5544403295, 697254.5544403295, 183437.3057700585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7781400.0000, 
sim time next is 7782000.0000, 
raw observation next is [26.4, 85.66666666666667, 1.0, 2.0, 0.4970971705667859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694614.7366797907, 694614.7366797907, 183142.50619799], 
processed observation next is [1.0, 0.043478260869565216, 0.45023696682464454, 0.8566666666666667, 1.0, 1.0, 0.39409297658648906, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19294853796660852, 0.19294853796660852, 0.27334702417610446], 
reward next is 0.7267, 
noisyNet noise sample is [array([0.23236096], dtype=float32), 0.77190024]. 
=============================================
[2019-03-26 23:35:24,932] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2330439: loss -179.5357
[2019-03-26 23:35:24,933] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2330439: learning rate 0.0010
[2019-03-26 23:35:24,937] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[65.606865]
 [65.658585]
 [65.66716 ]
 [65.8139  ]
 [65.78237 ]], R is [[65.61338806]
 [65.68346405]
 [65.75234222]
 [65.81999969]
 [65.88650513]].
[2019-03-26 23:35:25,664] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2330857: loss -5.3476
[2019-03-26 23:35:25,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2330857: learning rate 0.0010
[2019-03-26 23:35:25,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7521760e-17 9.9999893e-01 1.0202720e-20 1.0806339e-06 9.8735300e-25], sum to 1.0000
[2019-03-26 23:35:25,911] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5085
[2019-03-26 23:35:25,917] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2109511.698932658 W.
[2019-03-26 23:35:25,920] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 65.5, 1.0, 2.0, 0.5028898948221071, 1.0, 2.0, 0.5028898948221071, 1.0, 2.0, 0.8642953490052258, 6.911199999999999, 6.9112, 170.5573041426782, 2109511.698932658, 2109511.698932658, 415497.1320842575], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7918200.0000, 
sim time next is 7918800.0000, 
raw observation next is [30.53333333333333, 65.0, 1.0, 2.0, 0.7029041787850758, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.982661256744142, 6.9112, 168.9125298427332, 1879192.776183423, 1828495.829201367, 386119.5309510415], 
processed observation next is [1.0, 0.6521739130434783, 0.646129541864139, 0.65, 1.0, 1.0, 0.6420532274518985, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007146125674414172, 0.0, 0.8294378500185772, 0.5219979933842841, 0.5079155081114908, 0.5762978073896142], 
reward next is 0.0664, 
noisyNet noise sample is [array([0.06674245], dtype=float32), 1.8507035]. 
=============================================
[2019-03-26 23:35:26,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:26,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:26,223] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-03-26 23:35:26,375] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2331171: loss 139.6837
[2019-03-26 23:35:26,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2331171: learning rate 0.0010
[2019-03-26 23:35:26,759] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2331388: loss -113.1334
[2019-03-26 23:35:26,761] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2331388: learning rate 0.0010
[2019-03-26 23:35:26,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:26,816] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:26,867] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-03-26 23:35:27,333] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:27,334] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:27,383] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-03-26 23:35:27,572] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2331851: loss -78.9857
[2019-03-26 23:35:27,573] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2331852: learning rate 0.0010
[2019-03-26 23:35:28,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:28,216] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:28,278] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-03-26 23:35:29,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:29,701] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:29,781] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-03-26 23:35:30,274] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:30,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:30,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-03-26 23:35:31,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:31,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:31,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-03-26 23:35:32,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:32,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:32,462] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-03-26 23:35:33,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:33,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:33,144] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-03-26 23:35:33,589] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:33,590] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:33,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-03-26 23:35:34,432] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-03-26 23:35:34,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:35:34,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-03-26 23:35:36,937] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2525024e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:35:36,946] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4947
[2019-03-26 23:35:36,951] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.33333333333333, 95.66666666666667, 1.0, 2.0, 0.26342732567264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428744.169440457, 428744.1694404576, 162181.6441650866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 276000.0000, 
sim time next is 276600.0000, 
raw observation next is [19.26666666666667, 95.83333333333333, 1.0, 2.0, 0.2622963044690849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 427232.0739510572, 427232.0739510572, 162078.7265452268], 
processed observation next is [0.0, 0.17391304347826086, 0.1121642969984204, 0.9583333333333333, 1.0, 1.0, 0.11120036683022279, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11867557609751589, 0.11867557609751589, 0.24190854708242804], 
reward next is 0.7581, 
noisyNet noise sample is [array([-0.5959097], dtype=float32), -0.84494174]. 
=============================================
[2019-03-26 23:35:49,807] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:35:49,816] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9282
[2019-03-26 23:35:49,820] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.93333333333334, 94.33333333333334, 1.0, 2.0, 0.2776022783389498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 448295.1869766206, 448295.1869766201, 163486.0269349681], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 271200.0000, 
sim time next is 271800.0000, 
raw observation next is [19.85, 94.5, 1.0, 2.0, 0.2754767701685673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 445380.6546891804, 445380.6546891811, 163291.4703041285], 
processed observation next is [0.0, 0.13043478260869565, 0.1398104265402845, 0.945, 1.0, 1.0, 0.12708044598622567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12371684852477233, 0.12371684852477252, 0.24371861239422166], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.5161424], dtype=float32), -0.27375117]. 
=============================================
[2019-03-26 23:35:52,966] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:35:52,975] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9777
[2019-03-26 23:35:52,982] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 75.0, 1.0, 2.0, 0.2397413199632716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 396109.0358317908, 396109.0358317908, 159848.4993827649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 502800.0000, 
sim time next is 503400.0000, 
raw observation next is [20.53333333333333, 76.5, 1.0, 2.0, 0.2398221583705169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396352.3710722703, 396352.3710722709, 159849.4220526468], 
processed observation next is [1.0, 0.8260869565217391, 0.17219589257503945, 0.765, 1.0, 1.0, 0.08412308237411674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11009788085340842, 0.11009788085340859, 0.23858122694424896], 
reward next is 0.7614, 
noisyNet noise sample is [array([0.08599357], dtype=float32), 1.8162668]. 
=============================================
[2019-03-26 23:36:01,801] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3076634e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:36:01,812] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7136
[2019-03-26 23:36:01,820] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 85.33333333333334, 1.0, 2.0, 0.2749936752716465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445417.9380949528, 445417.9380949528, 163287.7934301915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 801600.0000, 
sim time next is 802200.0000, 
raw observation next is [21.05, 84.66666666666666, 1.0, 2.0, 0.2768149100475821, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 447865.6756368405, 447865.6756368412, 163453.3268960881], 
processed observation next is [0.0, 0.2608695652173913, 0.1966824644549764, 0.8466666666666666, 1.0, 1.0, 0.1286926627079302, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12440713212134459, 0.12440713212134479, 0.24396018939714642], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.76836824], dtype=float32), 0.8566886]. 
=============================================
[2019-03-26 23:36:01,990] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1097622e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:36:02,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6052
[2019-03-26 23:36:02,012] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 54.66666666666667, 1.0, 2.0, 0.4082328409843701, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668352.9711337485, 668352.9711337485, 180665.8312832849], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 562800.0000, 
sim time next is 563400.0000, 
raw observation next is [24.7, 55.0, 1.0, 2.0, 0.3563486808448915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583751.8053821806, 583751.80538218, 173354.4350673942], 
processed observation next is [1.0, 0.5217391304347826, 0.3696682464454976, 0.55, 1.0, 1.0, 0.22451648294565243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16215327927282794, 0.16215327927282777, 0.2587379627871555], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.6043081], dtype=float32), 0.31154123]. 
=============================================
[2019-03-26 23:36:03,450] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:36:03,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2405
[2019-03-26 23:36:03,471] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 66.33333333333334, 1.0, 2.0, 0.2401018453830852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 395993.0779871152, 395993.0779871159, 159918.0543452117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 499200.0000, 
sim time next is 499800.0000, 
raw observation next is [21.95, 67.66666666666666, 1.0, 2.0, 0.2407609549877426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 397262.7553255308, 397262.7553255308, 159973.0869926063], 
processed observation next is [1.0, 0.782608695652174, 0.2393364928909953, 0.6766666666666665, 1.0, 1.0, 0.08525416263583446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11035076536820299, 0.11035076536820299, 0.23876580148150192], 
reward next is 0.7612, 
noisyNet noise sample is [array([0.5947785], dtype=float32), 1.2406604]. 
=============================================
[2019-03-26 23:36:05,967] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-03-26 23:36:05,969] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:36:05,971] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:36:05,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:36:05,973] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:36:05,972] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:36:05,975] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:36:05,973] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:36:05,976] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:36:05,976] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:36:05,978] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:36:06,000] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-03-26 23:36:06,026] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-03-26 23:36:06,026] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-03-26 23:36:06,045] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-03-26 23:36:06,084] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-03-26 23:36:34,979] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2156411]
[2019-03-26 23:36:34,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.05, 80.33333333333334, 1.0, 2.0, 0.5477416377115489, 0.0, 2.0, 0.0, 1.0, 1.0, 0.920913615185417, 6.9112, 6.9112, 168.9127102001379, 1554147.089488171, 1554147.089488171, 330646.1882107974]
[2019-03-26 23:36:34,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:36:34,983] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.932567e-29 0.000000e+00], sampled 0.2934515097141823
[2019-03-26 23:36:39,284] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2156411]
[2019-03-26 23:36:39,287] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 90.33333333333333, 1.0, 2.0, 0.3815996265069066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 576687.3470192816, 576687.3470192822, 172637.0702049189]
[2019-03-26 23:36:39,290] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:36:39,292] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.7542129158701375
[2019-03-26 23:37:28,772] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.2156411]
[2019-03-26 23:37:28,775] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.63333333333333, 75.83333333333333, 1.0, 2.0, 0.5236745060107719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731765.1544703929, 731765.1544703934, 187388.4692998539]
[2019-03-26 23:37:28,776] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:37:28,780] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0. 1. 0. 0. 0.], sampled 0.11382251086246054
[2019-03-26 23:38:00,596] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.2185 2779607616.2075 924.0000
[2019-03-26 23:38:00,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8259.6246 2927820334.4347 1327.0000
[2019-03-26 23:38:00,983] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8549 2843203615.0074 1130.0000
[2019-03-26 23:38:01,070] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.4864 3164440628.4411 1775.0000
[2019-03-26 23:38:01,121] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.7044 3008486396.2608 1766.0000
[2019-03-26 23:38:02,138] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2350000, evaluation results [2350000.0, 7876.486443509165, 3164440628.441078, 1775.0, 8259.62461590143, 2927820334.4347343, 1327.0, 8660.218483230228, 2779607616.2075377, 924.0, 7997.704377508541, 3008486396.260774, 1766.0, 8495.854862915825, 2843203615.007414, 1130.0]
[2019-03-26 23:38:03,320] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:38:03,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8954
[2019-03-26 23:38:03,335] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 89.0, 1.0, 2.0, 0.3370803587496009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522510.1086791033, 522510.1086791033, 168501.4591834703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 939600.0000, 
sim time next is 940200.0000, 
raw observation next is [22.35, 89.5, 1.0, 2.0, 0.3372148457385539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522615.8658173371, 522615.8658173371, 168506.8035884477], 
processed observation next is [0.0, 0.9130434782608695, 0.25829383886255936, 0.895, 1.0, 1.0, 0.2014636695645228, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1451710738381492, 0.1451710738381492, 0.2515026919230563], 
reward next is 0.7485, 
noisyNet noise sample is [array([-0.05290702], dtype=float32), -0.13817115]. 
=============================================
[2019-03-26 23:38:05,570] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:38:05,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2381
[2019-03-26 23:38:05,588] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.31666666666667, 56.5, 1.0, 2.0, 0.4380545207939711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718684.7162382543, 718684.716238255, 185290.6793592843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 565800.0000, 
sim time next is 566400.0000, 
raw observation next is [24.23333333333333, 57.0, 1.0, 2.0, 0.5954843412428402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 976858.2892104761, 976858.2892104761, 214635.2878927916], 
processed observation next is [1.0, 0.5652173913043478, 0.3475513428120062, 0.57, 1.0, 1.0, 0.5126317364371569, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2713495247806878, 0.2713495247806878, 0.32035117595939044], 
reward next is 0.6796, 
noisyNet noise sample is [array([0.66168076], dtype=float32), 0.68187726]. 
=============================================
[2019-03-26 23:38:08,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:38:08,478] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8566
[2019-03-26 23:38:08,483] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 89.0, 1.0, 2.0, 0.2935083253260662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469225.7463031584, 469225.7463031584, 164885.6876700621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 871200.0000, 
sim time next is 871800.0000, 
raw observation next is [21.08333333333334, 89.0, 1.0, 2.0, 0.292641747443764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468052.1249192206, 468052.12491922, 164806.3618896688], 
processed observation next is [0.0, 0.08695652173913043, 0.1982622432859403, 0.89, 1.0, 1.0, 0.1477611414985108, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13001447914422792, 0.13001447914422778, 0.24597964461144597], 
reward next is 0.7540, 
noisyNet noise sample is [array([-1.3289013], dtype=float32), 1.5101807]. 
=============================================
[2019-03-26 23:38:21,655] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6213993e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:21,664] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0986
[2019-03-26 23:38:21,668] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.03333333333333, 83.5, 1.0, 2.0, 0.3008289006429775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478878.6753059169, 478878.6753059163, 165538.9578176553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 852600.0000, 
sim time next is 853200.0000, 
raw observation next is [22.0, 84.0, 1.0, 2.0, 0.3029436973056214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481905.6715583483, 481905.6715583489, 165750.4613694883], 
processed observation next is [0.0, 0.9130434782608695, 0.2417061611374408, 0.84, 1.0, 1.0, 0.16017312928388117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13386268654398564, 0.1338626865439858, 0.2473887483126691], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.25562885], dtype=float32), 1.0797418]. 
=============================================
[2019-03-26 23:38:22,608] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.1802426e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:22,617] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7347
[2019-03-26 23:38:22,623] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 92.33333333333334, 1.0, 2.0, 0.4696578998260643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660665.1396174054, 660665.1396174054, 179547.1106673018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1287600.0000, 
sim time next is 1288200.0000, 
raw observation next is [24.65, 92.66666666666666, 1.0, 2.0, 0.469250363620599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660271.5983369732, 660271.5983369732, 179509.6665299839], 
processed observation next is [1.0, 0.9130434782608695, 0.3672985781990521, 0.9266666666666665, 1.0, 1.0, 0.360542606771806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1834087773158259, 0.1834087773158259, 0.2679248754178864], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.7924717], dtype=float32), -0.049705073]. 
=============================================
[2019-03-26 23:38:24,488] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 1.215095e-34 0.000000e+00], sum to 1.0000
[2019-03-26 23:38:24,492] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3515
[2019-03-26 23:38:24,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6562439726076529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1010589.281588501, 1010589.281588501, 223221.2628403766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 990000.0000, 
sim time next is 990600.0000, 
raw observation next is [21.98333333333333, 94.16666666666667, 1.0, 2.0, 0.5779668196676535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889981.6723193923, 889981.6723193923, 206641.2591889056], 
processed observation next is [1.0, 0.4782608695652174, 0.24091627172195884, 0.9416666666666668, 1.0, 1.0, 0.491526288756209, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2472171311998312, 0.2472171311998312, 0.30841978983418744], 
reward next is 0.6916, 
noisyNet noise sample is [array([1.7987356], dtype=float32), 0.25466776]. 
=============================================
[2019-03-26 23:38:30,121] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:38:30,128] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5932
[2019-03-26 23:38:30,133] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 91.66666666666667, 1.0, 2.0, 0.3541920629782502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545006.4198378765, 545006.4198378765, 170212.5421992257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [22.5, 91.0, 1.0, 2.0, 0.3613032816597336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554878.4009200283, 554878.4009200289, 171009.83021727], 
processed observation next is [1.0, 0.2608695652173913, 0.2654028436018958, 0.91, 1.0, 1.0, 0.2304858815177513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1541328891444523, 0.15413288914445247, 0.2552385525630896], 
reward next is 0.7448, 
noisyNet noise sample is [array([1.4587185], dtype=float32), 2.4709895]. 
=============================================
[2019-03-26 23:38:31,124] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 2.803454e-32 0.000000e+00], sum to 1.0000
[2019-03-26 23:38:31,136] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4325
[2019-03-26 23:38:31,142] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.26666666666667, 69.0, 1.0, 2.0, 0.3304070660160498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512324.9343247216, 512324.9343247223, 167705.3275194789], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1100400.0000, 
sim time next is 1101000.0000, 
raw observation next is [25.13333333333333, 69.5, 1.0, 2.0, 0.332286620568118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515907.662497577, 515907.6624975776, 168005.839660841], 
processed observation next is [1.0, 0.7391304347826086, 0.3902053712480251, 0.695, 1.0, 1.0, 0.19552604887725056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14330768402710473, 0.1433076840271049, 0.25075498456841944], 
reward next is 0.7492, 
noisyNet noise sample is [array([0.3097272], dtype=float32), -0.66746765]. 
=============================================
[2019-03-26 23:38:31,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[72.707245]
 [72.60657 ]
 [72.08807 ]
 [70.75511 ]
 [66.27425 ]], R is [[72.59430695]
 [72.61805725]
 [72.64221191]
 [72.66529083]
 [72.67002106]].
[2019-03-26 23:38:31,506] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.4784238e-38 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:31,516] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8793
[2019-03-26 23:38:31,522] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.63333333333333, 95.66666666666667, 1.0, 2.0, 0.5612675163992599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868538.8901138138, 868538.8901138138, 203809.6542918139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 999600.0000, 
sim time next is 1000200.0000, 
raw observation next is [21.61666666666667, 95.83333333333333, 1.0, 2.0, 0.5704838590048266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 882692.041178323, 882692.041178323, 205603.0482875124], 
processed observation next is [1.0, 0.5652173913043478, 0.22353870458135885, 0.9583333333333333, 1.0, 1.0, 0.48251067349979104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24519223366064527, 0.24519223366064527, 0.3068702213246454], 
reward next is 0.6931, 
noisyNet noise sample is [array([-0.7991801], dtype=float32), 0.80115217]. 
=============================================
[2019-03-26 23:38:31,983] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:38:31,991] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9335
[2019-03-26 23:38:31,997] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.23333333333333, 82.66666666666667, 1.0, 2.0, 0.3105179619863199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 493141.9930866007, 493141.9930866014, 166554.6920804694], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1114800.0000, 
sim time next is 1115400.0000, 
raw observation next is [22.11666666666667, 83.33333333333333, 1.0, 2.0, 0.3094614475552037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491753.6787675507, 491753.6787675507, 166457.3621773816], 
processed observation next is [1.0, 0.9130434782608695, 0.24723538704581383, 0.8333333333333333, 1.0, 1.0, 0.1680258404279563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13659824410209742, 0.13659824410209742, 0.2484438241453457], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.61704236], dtype=float32), -0.46636328]. 
=============================================
[2019-03-26 23:38:42,234] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3668993e-37 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:42,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1973
[2019-03-26 23:38:42,247] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.81666666666667, 92.16666666666667, 1.0, 2.0, 0.3646134778447146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581773.2967046147, 581773.2967046147, 173611.6546320042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1357800.0000, 
sim time next is 1358400.0000, 
raw observation next is [20.83333333333334, 92.33333333333334, 1.0, 2.0, 0.3012415671543864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480381.7677212527, 480381.7677212527, 165658.9165345882], 
processed observation next is [1.0, 0.7391304347826086, 0.1864139020537128, 0.9233333333333335, 1.0, 1.0, 0.15812237006552576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13343937992257018, 0.13343937992257018, 0.24725211423072865], 
reward next is 0.7527, 
noisyNet noise sample is [array([-0.51171696], dtype=float32), 2.0113733]. 
=============================================
[2019-03-26 23:38:46,525] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.7398585e-25 9.8160249e-01 2.6879780e-31 1.8397518e-02 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:46,533] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0223
[2019-03-26 23:38:46,541] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1970140.711666942 W.
[2019-03-26 23:38:46,548] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.7045433155579759, 1.0, 2.0, 0.7045433155579759, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1970140.711666942, 1970140.711666943, 376066.0869215743], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.4388601972229799, 1.0, 2.0, 0.4388601972229799, 1.0, 1.0, 0.7445280717793048, 6.9112, 6.9112, 170.5573041426782, 1840690.56728043, 1840690.56728043, 372652.8738278935], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.32392794846142164, 1.0, 1.0, 0.32392794846142164, 1.0, 0.5, 0.6884488680235424, 0.0, 0.0, 0.8375144448122397, 0.5113029353556751, 0.5113029353556751, 0.5561983191461097], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.85046345], dtype=float32), -0.60972464]. 
=============================================
[2019-03-26 23:38:47,520] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2541343e-32 1.0000000e+00 0.0000000e+00 3.1603565e-14 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:47,530] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1514
[2019-03-26 23:38:47,542] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1970141.293700308 W.
[2019-03-26 23:38:47,547] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 75.0, 1.0, 2.0, 0.7045435235081066, 1.0, 2.0, 0.7045435235081066, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1970141.293700308, 1970141.293700308, 376066.213212619], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1267200.0000, 
sim time next is 1267800.0000, 
raw observation next is [27.96666666666667, 75.16666666666667, 1.0, 2.0, 0.6582902958858446, 1.0, 2.0, 0.6582902958858446, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1840690.567424206, 1840690.567424206, 356770.8136670886], 
processed observation next is [1.0, 0.6956521739130435, 0.524486571879937, 0.7516666666666667, 1.0, 1.0, 0.5883015613082465, 1.0, 1.0, 0.5883015613082465, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5113029353956128, 0.5113029353956128, 0.5324937517419233], 
reward next is 0.4675, 
noisyNet noise sample is [array([1.1364216], dtype=float32), 0.41765434]. 
=============================================
[2019-03-26 23:38:50,216] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.2453334e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:50,226] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9617
[2019-03-26 23:38:50,234] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 57.66666666666667, 1.0, 2.0, 0.3549844035424509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544589.4044788288, 544589.4044788294, 170129.0385365109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1527600.0000, 
sim time next is 1528200.0000, 
raw observation next is [27.55, 58.0, 1.0, 2.0, 0.3522676077483729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541603.219326074, 541603.219326074, 169917.3253098239], 
processed observation next is [0.0, 0.6956521739130435, 0.504739336492891, 0.58, 1.0, 1.0, 0.21959952740767819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15044533870168722, 0.15044533870168722, 0.25360794822361776], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.5255627], dtype=float32), 2.057165]. 
=============================================
[2019-03-26 23:38:50,850] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1417485e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:50,860] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1479
[2019-03-26 23:38:50,867] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 70.0, 1.0, 2.0, 0.4273507734078753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617285.1253990417, 617285.1253990423, 175593.3555833625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1434600.0000, 
sim time next is 1435200.0000, 
raw observation next is [27.46666666666667, 69.66666666666667, 1.0, 2.0, 0.4273767754111061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617098.6455318729, 617098.6455318729, 175568.7072981955], 
processed observation next is [0.0, 0.6086956521739131, 0.500789889415482, 0.6966666666666668, 1.0, 1.0, 0.3100925004953085, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17141629042552023, 0.17141629042552023, 0.26204284671372463], 
reward next is 0.7380, 
noisyNet noise sample is [array([-0.4856765], dtype=float32), -2.070207]. 
=============================================
[2019-03-26 23:38:51,061] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1688342e-28 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:51,072] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7570
[2019-03-26 23:38:51,076] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333333, 94.0, 1.0, 2.0, 0.449903455275776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636792.6927127681, 636792.6927127681, 177172.4196123565], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1741800.0000, 
sim time next is 1742400.0000, 
raw observation next is [24.3, 94.0, 1.0, 2.0, 0.4629610546274237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656232.777746873, 656232.7777468737, 179198.3239041596], 
processed observation next is [1.0, 0.17391304347826086, 0.3507109004739337, 0.94, 1.0, 1.0, 0.35296512605713704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1822868827074647, 0.1822868827074649, 0.26746018493158147], 
reward next is 0.7325, 
noisyNet noise sample is [array([-2.0422268], dtype=float32), -0.6732315]. 
=============================================
[2019-03-26 23:38:53,462] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.4151173e-16 0.0000000e+00], sum to 1.0000
[2019-03-26 23:38:53,472] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3391
[2019-03-26 23:38:53,477] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.4, 94.0, 1.0, 2.0, 0.332216873817985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 520286.0949662406, 520286.0949662399, 168470.6441603976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1807200.0000, 
sim time next is 1807800.0000, 
raw observation next is [21.41666666666666, 94.33333333333334, 1.0, 2.0, 0.3335875844193171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 521827.7258664079, 521827.7258664072, 168577.0882926433], 
processed observation next is [1.0, 0.9565217391304348, 0.21406003159557638, 0.9433333333333335, 1.0, 1.0, 0.19709347520399648, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1449521460740022, 0.144952146074002, 0.25160759446663183], 
reward next is 0.7484, 
noisyNet noise sample is [array([-0.14055234], dtype=float32), -0.07439726]. 
=============================================
[2019-03-26 23:38:57,939] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-03-26 23:38:57,943] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:38:57,944] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:38:57,945] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:38:57,946] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:38:57,944] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:38:57,948] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:38:57,949] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:38:57,946] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:38:57,950] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:38:57,952] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:38:57,973] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-03-26 23:38:57,996] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-03-26 23:38:58,019] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-03-26 23:38:58,019] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-03-26 23:38:58,060] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-03-26 23:40:39,216] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1717694]
[2019-03-26 23:40:39,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.2, 82.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.455411442243141, 6.9112, 168.909631844, 1840094.095353021, 1454019.367063779, 311350.9394442691]
[2019-03-26 23:40:39,219] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:40:39,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2224765e-30 0.0000000e+00], sampled 0.2856810214227681
[2019-03-26 23:40:39,225] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1840094.095353021 W.
[2019-03-26 23:40:44,699] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1717694]
[2019-03-26 23:40:44,703] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.913435025, 92.37440165999999, 1.0, 2.0, 0.6038558766780958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 877647.7727965269, 877647.7727965263, 205789.2939206049]
[2019-03-26 23:40:44,703] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:40:44,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2636392e-33 0.0000000e+00], sampled 0.433889229881652
[2019-03-26 23:40:52,463] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.5116 3008615590.0924 1766.0000
[2019-03-26 23:40:52,717] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.5068 2843430346.4143 1131.0000
[2019-03-26 23:40:52,718] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.6028 2928297227.5999 1338.0000
[2019-03-26 23:40:52,723] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7874.2549 3164933419.7733 1778.0000
[2019-03-26 23:40:52,731] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8657.6753 2780094883.5524 932.0000
[2019-03-26 23:40:53,748] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2375000, evaluation results [2375000.0, 7874.254934002473, 3164933419.7732573, 1778.0, 8253.602807907135, 2928297227.5999246, 1338.0, 8657.675256733233, 2780094883.5523534, 932.0, 7997.511550894106, 3008615590.092444, 1766.0, 8493.50677167813, 2843430346.4142575, 1131.0]
[2019-03-26 23:41:07,745] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.714349e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:41:07,750] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2318
[2019-03-26 23:41:07,756] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.16666666666667, 1.0, 2.0, 0.8863361545837083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1238831.934994498, 1238831.934994498, 266203.9479840854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.5883644471], 
processed observation next is [1.0, 0.5652173913043478, 0.4549763033175356, 0.85, 1.0, 1.0, 0.695185568603881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2899976926052511, 0.2899976926052511, 0.3452471468126076], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.14170113], dtype=float32), 0.37800598]. 
=============================================
[2019-03-26 23:41:10,669] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.1758382e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:10,683] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9992
[2019-03-26 23:41:10,687] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 94.0, 1.0, 2.0, 0.4911164248082961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686254.890200783, 686254.8902007823, 182215.8789517085], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1730400.0000, 
sim time next is 1731000.0000, 
raw observation next is [25.05, 94.0, 1.0, 2.0, 0.4892350096337261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683625.0742379685, 683625.0742379691, 181926.5683401431], 
processed observation next is [1.0, 0.0, 0.3862559241706162, 0.94, 1.0, 1.0, 0.38462049353460975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18989585395499126, 0.18989585395499142, 0.2715321915524524], 
reward next is 0.7285, 
noisyNet noise sample is [array([0.08736956], dtype=float32), 0.2954428]. 
=============================================
[2019-03-26 23:41:10,707] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[66.799446]
 [66.770744]
 [66.59472 ]
 [66.1165  ]
 [66.23637 ]], R is [[66.8474884 ]
 [66.90705109]
 [66.9655838 ]
 [67.02307892]
 [67.07952881]].
[2019-03-26 23:41:11,058] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.259855e-31 0.000000e+00], sum to 1.0000
[2019-03-26 23:41:11,061] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3709
[2019-03-26 23:41:11,069] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 96.66666666666667, 1.0, 2.0, 0.4589800935241213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 650967.111097416, 650967.1110974167, 178659.9968710756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1987800.0000, 
sim time next is 1988400.0000, 
raw observation next is [24.03333333333333, 96.33333333333334, 1.0, 2.0, 0.4605817607332746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652516.0638483983, 652516.0638483976, 178803.1379552874], 
processed observation next is [0.0, 0.0, 0.3380726698262243, 0.9633333333333334, 1.0, 1.0, 0.3500985069075597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18125446218011063, 0.18125446218011043, 0.2668703551571454], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.6266241], dtype=float32), -0.20938547]. 
=============================================
[2019-03-26 23:41:13,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.0539459e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:13,632] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3826
[2019-03-26 23:41:13,636] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666666, 96.83333333333334, 1.0, 2.0, 0.4200953839593341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 615447.9821304696, 615447.9821304696, 175667.3530107417], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1979400.0000, 
sim time next is 1980000.0000, 
raw observation next is [23.2, 97.0, 1.0, 2.0, 0.4219258349270911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616846.4755540118, 616846.4755540125, 175765.1468554756], 
processed observation next is [1.0, 0.9565217391304348, 0.29857819905213273, 0.97, 1.0, 1.0, 0.3035251023217965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17134624320944772, 0.1713462432094479, 0.2623360400827994], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.14860287], dtype=float32), 0.5388891]. 
=============================================
[2019-03-26 23:41:13,646] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[69.897095]
 [70.1516  ]
 [70.17736 ]
 [69.92082 ]
 [69.74054 ]], R is [[69.94869232]
 [69.98701477]
 [70.02519226]
 [70.06339264]
 [70.10152435]].
[2019-03-26 23:41:13,695] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0604665e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:13,704] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5102
[2019-03-26 23:41:13,710] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.31666666666667, 91.33333333333334, 1.0, 2.0, 0.3173504913265095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502100.6510673204, 502100.6510673204, 167187.8860500929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1800600.0000, 
sim time next is 1801200.0000, 
raw observation next is [21.33333333333334, 91.66666666666667, 1.0, 2.0, 0.3188246731998339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503774.7514364708, 503774.7514364708, 167301.3087464913], 
processed observation next is [1.0, 0.8695652173913043, 0.2101105845181678, 0.9166666666666667, 1.0, 1.0, 0.17930683518052273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13993743095457523, 0.13993743095457523, 0.2497034458902855], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.15252452], dtype=float32), 0.7041983]. 
=============================================
[2019-03-26 23:41:26,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5687793e-31 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:26,908] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9490
[2019-03-26 23:41:26,915] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.66666666666666, 63.33333333333333, 1.0, 2.0, 0.5388995160993995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753047.613130675, 753047.613130675, 189918.383318463], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2396400.0000, 
sim time next is 2397000.0000, 
raw observation next is [32.53333333333333, 64.16666666666667, 1.0, 2.0, 0.5462107230847445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763267.8209920973, 763267.8209920973, 191155.6971237608], 
processed observation next is [1.0, 0.7391304347826086, 0.7409162717219588, 0.6416666666666667, 1.0, 1.0, 0.45326593142740296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21201883916447145, 0.21201883916447145, 0.2853070106324788], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.6717153], dtype=float32), 0.5020613]. 
=============================================
[2019-03-26 23:41:26,926] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.52757 ]
 [68.63202 ]
 [62.68266 ]
 [49.97735 ]
 [33.768757]], R is [[72.20890045]
 [72.20334625]
 [72.20098114]
 [71.47897339]
 [71.37903595]].
[2019-03-26 23:41:28,528] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.4799192e-23 1.0000000e+00 3.5729951e-28 8.1963096e-14 1.4797240e-33], sum to 1.0000
[2019-03-26 23:41:28,536] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2908
[2019-03-26 23:41:28,549] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2033722.709891187 W.
[2019-03-26 23:41:28,554] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.01666666666667, 62.83333333333333, 1.0, 2.0, 0.7272593543048038, 1.0, 2.0, 0.7272593543048038, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2033722.709891187, 2033722.709891187, 386014.2025707008], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2289000.0000, 
sim time next is 2289600.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5291036846560565, 1.0, 2.0, 0.5291036846560565, 1.0, 1.0, 0.9188781229852451, 6.9112, 6.9112, 170.5573041426782, 2219585.638818665, 2219585.638818665, 435855.0367795338], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.63, 1.0, 1.0, 0.43265504175428493, 1.0, 1.0, 0.43265504175428493, 1.0, 0.5, 0.9010708816893231, 0.0, 0.0, 0.8375144448122397, 0.616551566338518, 0.616551566338518, 0.6505299056410951], 
reward next is 0.3495, 
noisyNet noise sample is [array([0.42065832], dtype=float32), -0.5043343]. 
=============================================
[2019-03-26 23:41:29,949] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0.00000000e+00 1.00000000e+00 0.00000000e+00 1.09727064e-32
 0.00000000e+00], sum to 1.0000
[2019-03-26 23:41:29,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9410
[2019-03-26 23:41:29,968] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.56666666666667, 75.33333333333333, 1.0, 2.0, 0.5865709141981166, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 819688.3932872053, 819688.3932872047, 198270.7730353634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2407200.0000, 
sim time next is 2407800.0000, 
raw observation next is [30.48333333333333, 75.66666666666667, 1.0, 2.0, 0.5849447147882951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817415.0284312845, 817415.0284312845, 197974.7118572583], 
processed observation next is [1.0, 0.8695652173913043, 0.6437598736176934, 0.7566666666666667, 1.0, 1.0, 0.4999333913111989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22705973011980124, 0.22705973011980124, 0.2954846445630721], 
reward next is 0.7045, 
noisyNet noise sample is [array([1.0255152], dtype=float32), 0.4615652]. 
=============================================
[2019-03-26 23:41:31,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.3687847e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:31,194] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5908
[2019-03-26 23:41:31,204] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 93.0, 1.0, 2.0, 0.5542222135223954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 774467.0557109504, 774467.0557109511, 192527.4416422963], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2496600.0000, 
sim time next is 2497200.0000, 
raw observation next is [26.9, 93.0, 1.0, 2.0, 0.553248472815269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 773105.860647302, 773105.8606473013, 192359.4388407102], 
processed observation next is [1.0, 0.9130434782608695, 0.4739336492890995, 0.93, 1.0, 1.0, 0.4617451479702036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2147516279575839, 0.2147516279575837, 0.2871036400607615], 
reward next is 0.7129, 
noisyNet noise sample is [array([-0.45367867], dtype=float32), 0.21070042]. 
=============================================
[2019-03-26 23:41:38,222] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.075896e-18 0.000000e+00], sum to 1.0000
[2019-03-26 23:41:38,229] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8743
[2019-03-26 23:41:38,238] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 84.66666666666666, 1.0, 2.0, 0.6981956445813882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975747.1478178694, 975747.1478178694, 220453.3247028223], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2266800.0000, 
sim time next is 2267400.0000, 
raw observation next is [26.53333333333333, 83.83333333333334, 1.0, 2.0, 0.7371720842473377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1030244.189916009, 1030244.189916009, 229070.5233628539], 
processed observation next is [1.0, 0.21739130434782608, 0.45655608214849913, 0.8383333333333334, 1.0, 1.0, 0.6833398605389611, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.28617894164333585, 0.28617894164333585, 0.3418963035266476], 
reward next is 0.6581, 
noisyNet noise sample is [array([1.653516], dtype=float32), 0.40500107]. 
=============================================
[2019-03-26 23:41:42,894] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.0579552e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:41:42,906] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0331
[2019-03-26 23:41:42,913] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 96.0, 1.0, 2.0, 0.6430358996279928, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 898627.3108131714, 898627.310813172, 209030.992515251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2520000.0000, 
sim time next is 2520600.0000, 
raw observation next is [26.28333333333333, 96.16666666666666, 1.0, 2.0, 0.70030999022607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 978703.3641504358, 978703.3641504358, 220913.043317088], 
processed observation next is [1.0, 0.17391304347826086, 0.4447077409162717, 0.9616666666666666, 1.0, 1.0, 0.6389276990675542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2718620455973433, 0.2718620455973433, 0.3297209601747582], 
reward next is 0.6703, 
noisyNet noise sample is [array([-0.5979882], dtype=float32), 0.07936711]. 
=============================================
[2019-03-26 23:41:44,377] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.5913412e-15 9.8991781e-01 2.1653363e-19 1.0082155e-02 1.5456993e-23], sum to 1.0000
[2019-03-26 23:41:44,386] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3018
[2019-03-26 23:41:44,395] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1990220.902268981 W.
[2019-03-26 23:41:44,399] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.4, 80.0, 1.0, 2.0, 0.7117175419361105, 1.0, 2.0, 0.7117175419361105, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1990220.902268981, 1990220.902268981, 379179.6361972781], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2545200.0000, 
sim time next is 2545800.0000, 
raw observation next is [28.51666666666667, 79.33333333333334, 1.0, 2.0, 0.4399851847245604, 1.0, 2.0, 0.4399851847245604, 1.0, 1.0, 0.7590371556543856, 6.9112, 6.9112, 170.5573041426782, 1845413.112505562, 1845413.112505562, 375334.4437600373], 
processed observation next is [1.0, 0.4782608695652174, 0.5505529225908374, 0.7933333333333334, 1.0, 1.0, 0.32528335508983175, 1.0, 1.0, 0.32528335508983175, 1.0, 0.5, 0.7061428727492507, 0.0, 0.0, 0.8375144448122397, 0.5126147534737672, 0.5126147534737672, 0.5602006623284139], 
reward next is 0.4398, 
noisyNet noise sample is [array([-0.71724373], dtype=float32), -0.8626281]. 
=============================================
[2019-03-26 23:41:49,652] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-03-26 23:41:49,654] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:41:49,655] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:41:49,656] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:41:49,657] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:41:49,660] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:41:49,659] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:41:49,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:41:49,664] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:41:49,663] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:41:49,662] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:41:49,687] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-03-26 23:41:49,712] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-03-26 23:41:49,713] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-03-26 23:41:49,713] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-03-26 23:41:49,750] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-03-26 23:41:59,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:41:59,244] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.13333333333333, 56.16666666666667, 1.0, 2.0, 0.3593500294664954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583890.2424039101, 583890.2424039096, 173600.4202698442]
[2019-03-26 23:41:59,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:41:59,247] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 4.132714e-29 0.000000e+00], sampled 0.1414406582756198
[2019-03-26 23:42:21,801] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:42:21,805] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.16666666666666, 90.50000000000001, 1.0, 2.0, 0.4274149214719164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 621886.5286581442, 621886.5286581436, 176168.5886796799]
[2019-03-26 23:42:21,806] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:42:21,807] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1498668e-27 0.0000000e+00], sampled 0.23996511342611326
[2019-03-26 23:42:26,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:42:26,493] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.505942955, 95.511365275, 1.0, 2.0, 0.5357072915758163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748585.2893645591, 748585.2893645597, 189378.2307952791]
[2019-03-26 23:42:26,494] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:42:26,496] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.9904216e-28 0.0000000e+00], sampled 0.5906033992305908
[2019-03-26 23:42:40,013] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:42:40,014] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.89926311, 73.27900624, 1.0, 2.0, 0.5516333854428858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770848.1289971077, 770848.1289971077, 192078.7216720426]
[2019-03-26 23:42:40,015] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:42:40,019] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5180801e-29 0.0000000e+00], sampled 0.6498686840550189
[2019-03-26 23:43:01,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:43:01,613] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.9, 59.66666666666667, 1.0, 2.0, 0.5692098982529606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795418.6053382198, 795418.6053382198, 195149.5295596384]
[2019-03-26 23:43:01,614] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:43:01,617] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4723132e-27 0.0000000e+00], sampled 0.8840277818640818
[2019-03-26 23:43:40,328] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:43:40,330] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.5, 94.0, 1.0, 2.0, 0.4868345722466326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684259.3430898135, 684259.3430898128, 182072.2308730523]
[2019-03-26 23:43:40,333] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:43:40,338] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 6.75717e-27 0.00000e+00], sampled 0.6366945328143251
[2019-03-26 23:43:42,762] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1925627]
[2019-03-26 23:43:42,764] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.27689129, 68.46143072, 1.0, 2.0, 0.6100316393736518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 890352.038111744, 890352.038111744, 207468.4416282234]
[2019-03-26 23:43:42,765] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:43:42,770] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.661716e-27 0.000000e+00], sampled 0.6195154566131547
[2019-03-26 23:43:43,869] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.5885 3163607924.6798 1754.0000
[2019-03-26 23:43:44,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.7288 2928048845.8087 1333.0000
[2019-03-26 23:43:44,363] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.9511 2779690712.1360 925.0000
[2019-03-26 23:43:44,484] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.6355 3008110996.9785 1761.0000
[2019-03-26 23:43:44,514] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.7945 2843171731.6309 1125.0000
[2019-03-26 23:43:45,532] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2400000, evaluation results [2400000.0, 7885.588510065972, 3163607924.6798344, 1754.0, 8256.728752803589, 2928048845.808738, 1333.0, 8660.951127580147, 2779690712.1359887, 925.0, 8000.635513758486, 3008110996.9784813, 1761.0, 8497.79446046936, 2843171731.630934, 1125.0]
[2019-03-26 23:43:50,946] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.3311458e-20 0.0000000e+00], sum to 1.0000
[2019-03-26 23:43:50,959] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8086
[2019-03-26 23:43:50,963] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3959294546072412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590785.4356171512, 590785.4356171512, 173689.7185016228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2673600.0000, 
sim time next is 2674200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3960511450640312, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590966.8437370453, 590966.8437370453, 173706.3800040062], 
processed observation next is [0.0, 0.9565217391304348, 0.28909952606635075, 0.94, 1.0, 1.0, 0.27235077718557976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16415745659362369, 0.16415745659362369, 0.2592632537373227], 
reward next is 0.7407, 
noisyNet noise sample is [array([0.0322089], dtype=float32), 2.467469]. 
=============================================
[2019-03-26 23:43:54,761] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.9394267e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:43:54,771] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2114
[2019-03-26 23:43:54,775] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4801442827383569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670918.2743513243, 670918.2743513243, 180543.3134476939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2707200.0000, 
sim time next is 2707800.0000, 
raw observation next is [23.83333333333333, 100.0, 1.0, 2.0, 0.4767107555814358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666119.0120372076, 666119.0120372076, 180027.0951194802], 
processed observation next is [0.0, 0.34782608695652173, 0.32859399684044216, 1.0, 1.0, 1.0, 0.36953103082100697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18503305889922433, 0.18503305889922433, 0.26869715689474655], 
reward next is 0.7313, 
noisyNet noise sample is [array([0.6850103], dtype=float32), 0.74086136]. 
=============================================
[2019-03-26 23:43:56,283] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.980453e-25 0.000000e+00], sum to 1.0000
[2019-03-26 23:43:56,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7050
[2019-03-26 23:43:56,295] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4835738062725922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712301.6879033858, 712301.6879033853, 185650.9953942904], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2826600.0000, 
sim time next is 2827200.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4047702488944265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596324.3255622144, 596324.3255622144, 173971.008130057], 
processed observation next is [1.0, 0.7391304347826086, 0.3364928909952607, 0.89, 1.0, 1.0, 0.28285572155954997, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.165645645989504, 0.165645645989504, 0.2596582210896373], 
reward next is 0.7403, 
noisyNet noise sample is [array([0.21619849], dtype=float32), -0.5581235]. 
=============================================
[2019-03-26 23:44:00,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 9.952668e-29 0.000000e+00], sum to 1.0000
[2019-03-26 23:44:00,213] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1210
[2019-03-26 23:44:00,217] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3937624702918596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587552.7706215467, 587552.7706215472, 173393.5499916874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2736600.0000, 
sim time next is 2737200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3932790645978489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586831.5939398808, 586831.5939398803, 173327.68430533], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2690109212022276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16300877609441133, 0.1630087760944112, 0.258698036276612], 
reward next is 0.7413, 
noisyNet noise sample is [array([-0.81587094], dtype=float32), -1.4357752]. 
=============================================
[2019-03-26 23:44:09,190] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.2536485e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:44:09,199] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3764
[2019-03-26 23:44:09,202] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4554664323995708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644889.3542668258, 644889.3542668258, 178005.615019745], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214200.0000, 
sim time next is 3214800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3435988257961173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1790261510823746, 0.1790261510823746, 0.2656194289465004], 
reward next is 0.7344, 
noisyNet noise sample is [array([0.6250333], dtype=float32), 0.273075]. 
=============================================
[2019-03-26 23:44:11,683] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6421849e-25 0.0000000e+00], sum to 1.0000
[2019-03-26 23:44:11,692] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4972
[2019-03-26 23:44:11,700] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.00000000000001, 1.0, 2.0, 0.6323485296520729, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 999692.4680557448, 999692.4680557442, 220503.448472093], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2992200.0000, 
sim time next is 2992800.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5695950113844704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900436.0167523412, 900436.0167523412, 207253.4065095986], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.481439772752374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2501211157645392, 0.2501211157645392, 0.3093334425516397], 
reward next is 0.6907, 
noisyNet noise sample is [array([0.9305612], dtype=float32), -0.7895591]. 
=============================================
[2019-03-26 23:44:24,020] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2377875e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:44:24,029] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9071
[2019-03-26 23:44:24,036] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 97.0, 1.0, 2.0, 0.7606526954542268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1097199.113658899, 1097199.113658899, 239146.6568780977], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3083400.0000, 
sim time next is 3084000.0000, 
raw observation next is [23.33333333333333, 98.0, 1.0, 2.0, 0.8182030209497433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1182308.866422852, 1182308.866422852, 253874.7332175714], 
processed observation next is [1.0, 0.6956521739130435, 0.30489731437598716, 0.98, 1.0, 1.0, 0.7809674951201726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32841912956190333, 0.32841912956190333, 0.37891751226503195], 
reward next is 0.6211, 
noisyNet noise sample is [array([-0.6408626], dtype=float32), 0.77379864]. 
=============================================
[2019-03-26 23:44:24,057] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[73.572235]
 [73.4003  ]
 [72.42584 ]
 [72.65512 ]
 [72.31665 ]], R is [[72.88825226]
 [72.80243683]
 [72.7144928 ]
 [72.60231018]
 [72.49748993]].
[2019-03-26 23:44:30,998] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.5042817e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:44:31,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4724
[2019-03-26 23:44:31,014] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.516806662117134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722164.9980060417, 722164.9980060423, 186271.7918337891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3319200.0000, 
sim time next is 3319800.0000, 
raw observation next is [29.33333333333334, 73.33333333333334, 1.0, 2.0, 0.5209711584512375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727986.2930018047, 727986.2930018047, 186947.8906946398], 
processed observation next is [0.0, 0.43478260869565216, 0.5892575039494474, 0.7333333333333334, 1.0, 1.0, 0.4228568174111295, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20221841472272353, 0.20221841472272353, 0.2790267025293131], 
reward next is 0.7210, 
noisyNet noise sample is [array([-0.56354874], dtype=float32), -0.19665374]. 
=============================================
[2019-03-26 23:44:33,547] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 3.512719e-36 0.000000e+00], sum to 1.0000
[2019-03-26 23:44:33,557] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5331
[2019-03-26 23:44:33,563] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.66666666666666, 77.66666666666667, 1.0, 2.0, 0.5705677971913229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 797316.8567040372, 797316.8567040366, 195390.7187670969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3264000.0000, 
sim time next is 3264600.0000, 
raw observation next is [29.33333333333333, 78.33333333333334, 1.0, 2.0, 0.5638418887388965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787914.5223730415, 787914.5223730408, 194202.8663751448], 
processed observation next is [0.0, 0.782608695652174, 0.5892575039494469, 0.7833333333333334, 1.0, 1.0, 0.47450829968541747, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21886514510362265, 0.21886514510362245, 0.28985502444051464], 
reward next is 0.7101, 
noisyNet noise sample is [array([-0.15611608], dtype=float32), -0.7795698]. 
=============================================
[2019-03-26 23:44:34,526] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9587783e-13 9.9334705e-01 6.0077843e-17 6.6530271e-03 1.9759989e-20], sum to 1.0000
[2019-03-26 23:44:34,535] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2223
[2019-03-26 23:44:34,543] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2640314.492407996 W.
[2019-03-26 23:44:34,554] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.94393645093804, 1.0, 2.0, 0.94393645093804, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2640314.492407996, 2640314.492407995, 496038.1096868927], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3420000.0000, 
sim time next is 3420600.0000, 
raw observation next is [34.0, 62.5, 1.0, 2.0, 0.6954829375346477, 1.0, 2.0, 0.6683315082815864, 1.0, 1.0, 1.03, 7.005097375992508, 6.9112, 170.5573041426782, 2804300.633995178, 2737038.124070693, 519906.9713060684], 
processed observation next is [1.0, 0.6086956521739131, 0.8104265402843602, 0.625, 1.0, 1.0, 0.6331119729333106, 1.0, 1.0, 0.6003994075681763, 1.0, 0.5, 1.0365853658536586, 0.009389737599250835, 0.0, 0.8375144448122397, 0.7789723983319939, 0.7602883677974147, 0.7759805541881618], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6445561], dtype=float32), 1.1928862]. 
=============================================
[2019-03-26 23:44:41,423] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-03-26 23:44:41,425] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:44:41,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:44:41,428] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:44:41,430] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:44:41,430] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:44:41,431] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:44:41,430] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:44:41,435] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:44:41,437] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:44:41,438] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:44:41,465] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-03-26 23:44:41,488] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-03-26 23:44:41,507] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-03-26 23:44:41,530] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-03-26 23:44:41,531] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-03-26 23:44:50,433] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:44:50,436] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.71666666666667, 59.66666666666666, 1.0, 2.0, 0.259140656112393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425492.9259189648, 425492.9259189648, 161825.4797858947]
[2019-03-26 23:44:50,437] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:44:50,439] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.0577375e-26 0.0000000e+00], sampled 0.8523617755741331
[2019-03-26 23:44:55,535] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:44:55,536] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.98333333333333, 63.66666666666666, 1.0, 2.0, 0.3396836859525645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529289.0621836962, 529289.0621836962, 169117.2617830907]
[2019-03-26 23:44:55,538] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:44:55,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 7.54499e-26 0.00000e+00], sampled 0.3580491711032481
[2019-03-26 23:45:23,533] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:45:23,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.0, 89.0, 1.0, 2.0, 0.7212752840298917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1098952.566744534, 1098952.566744534, 237177.9138975594]
[2019-03-26 23:45:23,534] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:45:23,536] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 6.242011e-23 0.000000e+00], sampled 0.3290830732595291
[2019-03-26 23:45:25,007] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:45:25,008] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.04625884, 90.91635653, 1.0, 2.0, 0.2654858178130099, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 431511.7959980057, 431511.7959980051, 162368.6559096845]
[2019-03-26 23:45:25,009] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:45:25,013] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.6526772e-25 0.0000000e+00], sampled 0.44300258633810075
[2019-03-26 23:45:59,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:45:59,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.1, 69.33333333333334, 1.0, 2.0, 0.57475464125299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 803169.8043964162, 803169.8043964155, 196136.4615455901]
[2019-03-26 23:45:59,318] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:45:59,323] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2681763e-25 0.0000000e+00], sampled 0.8634723208474294
[2019-03-26 23:46:05,667] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:46:05,668] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.10335436, 85.26927496, 1.0, 2.0, 0.7900989668321095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1104251.284872451, 1104251.284872451, 241496.5645762735]
[2019-03-26 23:46:05,669] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:46:05,671] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.8219555e-22 0.0000000e+00], sampled 0.9448897419089786
[2019-03-26 23:46:06,080] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:46:06,082] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [35.19979782999999, 62.87926624333333, 1.0, 2.0, 0.9976338437091753, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00599323396182, 6.9112, 168.9123159282677, 2291690.169120119, 2224440.841452271, 462608.9744508548]
[2019-03-26 23:46:06,083] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:46:06,085] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8447337e-19 1.0000000e+00 1.6047023e-23 3.6212636e-08 2.7724900e-28], sampled 0.6302596247148099
[2019-03-26 23:46:06,088] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2291690.169120119 W.
[2019-03-26 23:46:08,488] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1645956]
[2019-03-26 23:46:08,489] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.271649865, 97.777818465, 1.0, 2.0, 0.8824271502117089, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.978180317702877, 6.9112, 168.9125574247105, 2130439.66163394, 2082921.631240627, 430204.053486709]
[2019-03-26 23:46:08,492] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:46:08,496] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8719907e-25 1.0000000e+00 1.5356755e-30 6.8934663e-12 1.0032447e-36], sampled 0.7427587797514651
[2019-03-26 23:46:08,497] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2130439.66163394 W.
[2019-03-26 23:46:35,511] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8043.9788 3003666339.0526 1656.0000
[2019-03-26 23:46:35,727] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7954.9320 3157559663.3829 1591.0000
[2019-03-26 23:46:35,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8675.1186 2778209967.8930 883.0000
[2019-03-26 23:46:35,983] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8291.5639 2924312245.9834 1252.0000
[2019-03-26 23:46:36,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8514.0269 2841309703.4568 1088.0000
[2019-03-26 23:46:37,175] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 2425000, evaluation results [2425000.0, 7954.93203454244, 3157559663.382935, 1591.0, 8291.563939750362, 2924312245.983362, 1252.0, 8675.118603130257, 2778209967.8930287, 883.0, 8043.978773880467, 3003666339.052628, 1656.0, 8514.026872615925, 2841309703.4568186, 1088.0]
[2019-03-26 23:46:37,269] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.2629406e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:46:37,280] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6072
[2019-03-26 23:46:37,285] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5387947595640994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752901.1765240583, 752901.176524059, 189897.467371552], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3368400.0000, 
sim time next is 3369000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5388179412332851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752933.5816124796, 752933.5816124802, 189901.3645148258], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4443589653413073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20914821711457768, 0.20914821711457784, 0.2834348724101878], 
reward next is 0.7166, 
noisyNet noise sample is [array([-1.7787194], dtype=float32), 0.33982733]. 
=============================================
[2019-03-26 23:46:37,298] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[72.05586 ]
 [71.98429 ]
 [71.911674]
 [71.836525]
 [71.84262 ]], R is [[72.1140976 ]
 [72.10952759]
 [72.10533142]
 [72.10137177]
 [72.09738159]].
[2019-03-26 23:46:40,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.539604e-35 0.000000e+00], sum to 1.0000
[2019-03-26 23:46:40,730] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9763
[2019-03-26 23:46:40,735] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5184873768171205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 724514.3625625372, 724514.3625625379, 186543.3496807762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3800400.0000, 
sim time next is 3801000.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.518497269532667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 724528.1909779884, 724528.1909779878, 186544.9532383817], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41987622835261074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20125783082721901, 0.20125783082721885, 0.27842530334086824], 
reward next is 0.7216, 
noisyNet noise sample is [array([0.7611638], dtype=float32), 0.65846115]. 
=============================================
[2019-03-26 23:46:40,750] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.50232 ]
 [67.45663 ]
 [67.40567 ]
 [67.37594 ]
 [67.422295]], R is [[67.55892944]
 [67.60491943]
 [67.650177  ]
 [67.69474792]
 [67.73911285]].
[2019-03-26 23:46:46,299] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2519053e-15 8.3854271e-15 8.4297340e-19 1.0000000e+00 7.3970150e-21], sum to 1.0000
[2019-03-26 23:46:46,311] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8043
[2019-03-26 23:46:46,318] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.9407285170852454, 1.0, 2.0, 0.9407285170852454, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2631332.030124995, 2631332.030124995, 494196.7033857099], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3679200.0000, 
sim time next is 3679800.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.8675780727728671, 1.0, 2.0, 0.8675780727728671, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2426522.665736567, 2426522.665736567, 454111.5500416305], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.59, 1.0, 1.0, 0.8404555093649002, 1.0, 1.0, 0.8404555093649002, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6740340738157131, 0.6740340738157131, 0.677778432897956], 
reward next is 0.3222, 
noisyNet noise sample is [array([1.1504469], dtype=float32), -0.24044056]. 
=============================================
[2019-03-26 23:46:50,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.2158324e-29 0.0000000e+00], sum to 1.0000
[2019-03-26 23:46:50,018] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5775
[2019-03-26 23:46:50,025] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5959326189443598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 832775.7947019509, 832775.7947019509, 199982.5151668325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3562800.0000, 
sim time next is 3563400.0000, 
raw observation next is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.6021312725522473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 841441.4278245873, 841441.4278245867, 201135.2269374539], 
processed observation next is [1.0, 0.21739130434782608, 0.470774091627172, 0.7983333333333335, 1.0, 1.0, 0.5206400874123461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23373372995127425, 0.2337337299512741, 0.3002018312499312], 
reward next is 0.6998, 
noisyNet noise sample is [array([1.2507145], dtype=float32), -0.43562755]. 
=============================================
[2019-03-26 23:46:52,289] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9118789e-30 1.0000000e+00 4.8651095e-36 2.0648405e-09 0.0000000e+00], sum to 1.0000
[2019-03-26 23:46:52,298] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9023
[2019-03-26 23:46:52,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2610590.191310199 W.
[2019-03-26 23:46:52,313] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.54087167379041, 6.9112, 168.9040445680557, 2610590.191310199, 1454506.015821188, 310381.3891037142], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.7925475435686825, 1.0, 1.0, 0.7925475435686825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2216484.236871239, 2216484.236871239, 416254.8260226678], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.72, 1.0, 1.0, 0.7500572814080512, 1.0, 0.5, 0.7500572814080512, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6156900657975664, 0.6156900657975664, 0.6212758597353251], 
reward next is 0.3787, 
noisyNet noise sample is [array([2.3113391], dtype=float32), -1.7006878]. 
=============================================
[2019-03-26 23:46:52,326] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[43.297745]
 [60.232014]
 [60.604187]
 [60.635746]
 [60.175983]], R is [[36.67840576]
 [36.31162262]
 [36.56733322]
 [36.83515549]
 [37.10637665]].
[2019-03-26 23:46:54,682] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4758279e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:46:54,696] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2759
[2019-03-26 23:46:54,702] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5434979214518272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759475.6317513906, 759475.6317513912, 190692.0933461548], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3826800.0000, 
sim time next is 3827400.0000, 
raw observation next is [28.33333333333334, 82.5, 1.0, 2.0, 0.547343449913913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764851.2487339274, 764851.2487339268, 191346.3796811566], 
processed observation next is [0.0, 0.30434782608695654, 0.5418641390205374, 0.825, 1.0, 1.0, 0.45463066254688317, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2124586802038687, 0.21245868020386854, 0.2855916114644128], 
reward next is 0.7144, 
noisyNet noise sample is [array([2.816427], dtype=float32), 0.88572174]. 
=============================================
[2019-03-26 23:46:55,072] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 6.625239e-11 0.000000e+00], sum to 1.0000
[2019-03-26 23:46:55,081] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1880
[2019-03-26 23:46:55,085] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5261972484939502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735291.5701220223, 735291.5701220216, 187802.2786802547], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3618600.0000, 
sim time next is 3619200.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5251488687914434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733826.0911034971, 733826.0911034965, 187630.0401750312], 
processed observation next is [1.0, 0.9130434782608695, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4278902033631848, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2038405808620825, 0.20384058086208237, 0.2800448360821361], 
reward next is 0.7200, 
noisyNet noise sample is [array([-1.6234541], dtype=float32), -1.631743]. 
=============================================
[2019-03-26 23:47:01,293] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.241553e-18 0.000000e+00], sum to 1.0000
[2019-03-26 23:47:01,301] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6665
[2019-03-26 23:47:01,305] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.5942153171896634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830375.0427994833, 830375.0427994839, 199674.0139918176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3938400.0000, 
sim time next is 3939000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.6102890567413877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852846.0065062258, 852846.0065062258, 202678.21304662], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5304687430619128, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2369016684739516, 0.2369016684739516, 0.30250479559197013], 
reward next is 0.6975, 
noisyNet noise sample is [array([-0.6447386], dtype=float32), 1.2631991]. 
=============================================
[2019-03-26 23:47:01,320] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[74.494095]
 [74.3431  ]
 [74.25439 ]
 [74.151535]
 [74.0094  ]], R is [[74.3434906 ]
 [74.3020401 ]
 [74.26091003]
 [74.22006989]
 [74.17935944]].
[2019-03-26 23:47:02,984] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.78355209e-35 1.00000000e+00 0.00000000e+00 1.07161904e-10
 0.00000000e+00], sum to 1.0000
[2019-03-26 23:47:02,991] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9987
[2019-03-26 23:47:02,998] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 83.16666666666667, 1.0, 2.0, 0.957621953381459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1338530.896345231, 1338530.896345231, 286286.2908198257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4173000.0000, 
sim time next is 4173600.0000, 
raw observation next is [30.66666666666667, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.916186142425426, 6.9112, 168.9128263400949, 1457294.688481161, 1453757.350160787, 311357.4961964511], 
processed observation next is [1.0, 0.30434782608695654, 0.6524486571879939, 0.8233333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0004986142425425832, 0.0, 0.8294393059563603, 0.4048040801336558, 0.4038214861557742, 0.4647126808902255], 
reward next is 0.5104, 
noisyNet noise sample is [array([-1.498805], dtype=float32), -1.7062489]. 
=============================================
[2019-03-26 23:47:04,511] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 8.536473e-23 0.000000e+00], sum to 1.0000
[2019-03-26 23:47:04,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9079
[2019-03-26 23:47:04,527] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.5924232812887424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827869.8199108832, 827869.8199108832, 199343.5388423999], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.5089437123960752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2299638388641342, 0.2299638388641342, 0.2975276699140297], 
reward next is 0.7025, 
noisyNet noise sample is [array([-0.28819466], dtype=float32), -2.6731133]. 
=============================================
[2019-03-26 23:47:05,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.045835e-11 0.000000e+00], sum to 1.0000
[2019-03-26 23:47:05,690] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9542
[2019-03-26 23:47:05,694] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.83333333333334, 63.66666666666666, 1.0, 2.0, 0.6123736076711256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 855760.2274261215, 855760.2274261221, 203074.7337283009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3840600.0000, 
sim time next is 3841200.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6134717693504033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 857295.4705314585, 857295.4705314592, 203283.5157147623], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5343033365667509, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23813763070318292, 0.2381376307031831, 0.30340823241009296], 
reward next is 0.6966, 
noisyNet noise sample is [array([0.939796], dtype=float32), 0.8915919]. 
=============================================
[2019-03-26 23:47:08,935] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 5.556173e-28 0.000000e+00], sum to 1.0000
[2019-03-26 23:47:08,941] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5435
[2019-03-26 23:47:08,947] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6084379794734014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850258.1905649487, 850258.1905649487, 202328.6956724103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3967200.0000, 
sim time next is 3967800.0000, 
raw observation next is [31.83333333333334, 72.33333333333334, 1.0, 2.0, 0.6140346197662262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858082.3429757889, 858082.3429757889, 203389.8077019726], 
processed observation next is [0.0, 0.9565217391304348, 0.7077409162717223, 0.7233333333333334, 1.0, 1.0, 0.5349814695978629, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23835620638216357, 0.23835620638216357, 0.3035668771671233], 
reward next is 0.6964, 
noisyNet noise sample is [array([1.5685813], dtype=float32), 0.80670536]. 
=============================================
[2019-03-26 23:47:10,486] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.8292436e-18 9.9999464e-01 8.8048489e-23 5.3594881e-06 1.1026720e-27], sum to 1.0000
[2019-03-26 23:47:10,494] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6072
[2019-03-26 23:47:10,502] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.83333333333334, 48.33333333333333, 1.0, 2.0, 0.4454534457089321, 1.0, 2.0, 0.4454534457089321, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1245217.159633947, 1245217.159633947, 284196.4411472306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4295400.0000, 
sim time next is 4296000.0000, 
raw observation next is [36.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5627929816518753, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564928153, 786448.2335100379, 786448.2335100379, 194021.6947992255], 
processed observation next is [1.0, 0.7391304347826086, 0.9368088467614536, 0.4866666666666666, 1.0, 1.0, 0.47324455620707867, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399450658015, 0.2184578426416772, 0.2184578426416772, 0.2895846191033216], 
reward next is 0.7104, 
noisyNet noise sample is [array([0.39785564], dtype=float32), -1.1503487]. 
=============================================
[2019-03-26 23:47:10,515] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[28.02286 ]
 [24.785616]
 [25.02476 ]
 [25.395922]
 [25.112782]], R is [[48.34159088]
 [48.43400192]
 [47.94966125]
 [47.47016525]
 [46.99546432]].
[2019-03-26 23:47:20,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.00000e+00 1.00000e+00 0.00000e+00 9.44229e-20 0.00000e+00], sum to 1.0000
[2019-03-26 23:47:20,569] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3304
[2019-03-26 23:47:20,577] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 88.16666666666667, 1.0, 2.0, 0.8433435168777597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1178707.775708078, 1178707.775708079, 254828.3455030523], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4075800.0000, 
sim time next is 4076400.0000, 
raw observation next is [27.13333333333334, 88.33333333333334, 1.0, 2.0, 0.7576638047186189, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1058896.932026893, 1058896.932026892, 233784.456891426], 
processed observation next is [1.0, 0.17391304347826086, 0.4849921011058455, 0.8833333333333334, 1.0, 1.0, 0.7080286803838782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2941380366741369, 0.2941380366741367, 0.3489320252110836], 
reward next is 0.6511, 
noisyNet noise sample is [array([0.2160192], dtype=float32), -2.0603793]. 
=============================================
[2019-03-26 23:47:31,434] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 4.119692e-24 0.000000e+00], sum to 1.0000
[2019-03-26 23:47:31,442] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1656
[2019-03-26 23:47:31,448] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 83.16666666666666, 1.0, 2.0, 0.6178550359089731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863423.3514857844, 863423.3514857844, 204119.8234795695], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4402200.0000, 
sim time next is 4402800.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6170007420431912, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862229.0314055202, 862229.0314055202, 203956.2245229629], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5385551108954111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23950806427931115, 0.23950806427931115, 0.30441227540740734], 
reward next is 0.6956, 
noisyNet noise sample is [array([-1.843631], dtype=float32), 1.4367813]. 
=============================================
[2019-03-26 23:47:31,832] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.7415539e-12 9.9975020e-01 2.4699178e-14 2.4979369e-04 3.1985155e-16], sum to 1.0000
[2019-03-26 23:47:31,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4431
[2019-03-26 23:47:31,846] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2635700.247046844 W.
[2019-03-26 23:47:31,853] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.0, 54.00000000000001, 1.0, 2.0, 0.9422885544530585, 1.0, 2.0, 0.9422885544530585, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2635700.247046844, 2635700.247046844, 495103.181274681], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4364400.0000, 
sim time next is 4365000.0000, 
raw observation next is [37.0, 54.0, 1.0, 2.0, 0.8808890015560283, 1.0, 2.0, 0.8808890015560283, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2463788.606134657, 2463788.606134657, 461189.2330866578], 
processed observation next is [1.0, 0.5217391304347826, 0.95260663507109, 0.54, 1.0, 1.0, 0.8564927729590703, 1.0, 1.0, 0.8564927729590703, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6843857239262936, 0.6843857239262936, 0.6883421389353102], 
reward next is 0.3117, 
noisyNet noise sample is [array([1.0297439], dtype=float32), -0.47523794]. 
=============================================
[2019-03-26 23:47:31,879] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[13.454661]
 [ 9.65212 ]
 [ 9.206717]
 [ 9.253279]
 [ 9.228843]], R is [[21.18720818]
 [21.23637581]
 [21.02401161]
 [20.8137722 ]
 [20.60563469]].
[2019-03-26 23:47:33,036] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-03-26 23:47:33,038] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:47:33,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:47:33,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:47:33,042] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:47:33,042] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:47:33,043] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:47:33,042] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:47:33,044] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:47:33,046] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:47:33,046] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:47:33,075] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-03-26 23:47:33,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-03-26 23:47:33,117] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-03-26 23:47:33,149] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-03-26 23:47:33,150] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-03-26 23:47:36,694] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97531325]
[2019-03-26 23:47:36,695] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.73333333333333, 73.5, 1.0, 2.0, 0.2422615275990829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 401293.7976108338, 401293.7976108344, 160011.3028301084]
[2019-03-26 23:47:36,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:47:36,700] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.5706706e-32 0.0000000e+00], sampled 0.4399878984676979
[2019-03-26 23:48:04,492] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97531325]
[2019-03-26 23:48:04,492] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.26666666666667, 89.0, 1.0, 2.0, 0.4815483853681319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674482.2097482833, 674482.2097482826, 180961.779296526]
[2019-03-26 23:48:04,494] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:48:04,495] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2561325e-30 0.0000000e+00], sampled 0.9928340077232222
[2019-03-26 23:49:03,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97531325]
[2019-03-26 23:49:03,457] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.9921557, 79.04535378499999, 1.0, 2.0, 0.5269685320197348, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736369.7111571619, 736369.7111571613, 187931.3573058637]
[2019-03-26 23:49:03,459] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-03-26 23:49:03,463] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.5175972e-31 0.0000000e+00], sampled 0.5272620719279811
[2019-03-26 23:49:13,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97531325]
[2019-03-26 23:49:13,902] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.85, 94.5, 1.0, 2.0, 0.5327905196420738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744508.0296650664, 744508.0296650664, 188892.3760748432]
[2019-03-26 23:49:13,903] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:49:13,905] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 3.8466214e-29 0.0000000e+00], sampled 0.2673896153499762
[2019-03-26 23:49:22,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -0.97531325]
[2019-03-26 23:49:22,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.86666666666667, 72.5, 1.0, 2.0, 0.550313776741697, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769003.4507288414, 769003.4507288414, 191854.269720937]
[2019-03-26 23:49:22,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:49:22,712] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 2.990679e-31 0.000000e+00], sampled 0.5152519864023714
[2019-03-26 23:49:27,152] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.8875 3164502873.7783 1778.0000
[2019-03-26 23:49:27,382] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8190 2842958600.4139 1131.0000
[2019-03-26 23:49:27,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2741 3008104656.7243 1766.0000
[2019-03-26 23:49:27,518] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6708 2779658361.4638 933.0000
[2019-03-26 23:49:27,574] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3751 2927779800.6357 1338.0000
[2019-03-26 23:49:28,592] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 2450000, evaluation results [2450000.0, 7878.887539409773, 3164502873.778303, 1778.0, 8254.375086958147, 2927779800.6357465, 1338.0, 8660.670814356308, 2779658361.463785, 933.0, 7998.274138010797, 3008104656.724262, 1766.0, 8496.819032943851, 2842958600.4139094, 1131.0]
[2019-03-26 23:49:29,449] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.0357104e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:29,457] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1929
[2019-03-26 23:49:29,463] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6212517794063316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 868172.0807918498, 868172.0807918498, 204772.4759817558], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4314600.0000, 
sim time next is 4315200.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.621937577563629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869130.8457266962, 869130.8457266962, 204904.5806662329], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5445031054983481, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24142523492408227, 0.24142523492408227, 0.3058277323376611], 
reward next is 0.6942, 
noisyNet noise sample is [array([0.14882025], dtype=float32), 0.64054424]. 
=============================================
[2019-03-26 23:49:33,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3160475e-24 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:33,201] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8853
[2019-03-26 23:49:33,205] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5483205362909488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766217.1101081015, 766217.1101081009, 191513.7958283018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4388400.0000, 
sim time next is 4389000.0000, 
raw observation next is [31.83333333333334, 63.0, 1.0, 2.0, 0.5460196378479318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763000.7050435139, 763000.7050435132, 191120.8788285246], 
processed observation next is [1.0, 0.8260869565217391, 0.7077409162717223, 0.63, 1.0, 1.0, 0.45303570825052025, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21194464028986498, 0.2119446402898648, 0.28525504302764865], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.1909332], dtype=float32), -0.04971002]. 
=============================================
[2019-03-26 23:49:33,218] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[67.785095]
 [66.80112 ]
 [66.954254]
 [66.45515 ]
 [66.80216 ]], R is [[67.90122223]
 [67.93637085]
 [67.9706192 ]
 [68.0039978 ]
 [68.03638458]].
[2019-03-26 23:49:37,461] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 5.6274163e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:37,471] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5455
[2019-03-26 23:49:37,474] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6174361852499398, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862837.7896362707, 862837.7896362707, 204039.5391622155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4405200.0000, 
sim time next is 4405800.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6174544488798052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862863.3225603158, 862863.3225603158, 204043.0348285168], 
processed observation next is [1.0, 1.0, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5391017456383195, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2396842562667544, 0.2396842562667544, 0.30454184302763704], 
reward next is 0.6955, 
noisyNet noise sample is [array([-0.65911674], dtype=float32), 0.53625846]. 
=============================================
[2019-03-26 23:49:40,483] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5046507e-22 1.0000000e+00 6.8703113e-27 1.7218341e-08 6.5822423e-32], sum to 1.0000
[2019-03-26 23:49:40,493] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1135
[2019-03-26 23:49:40,501] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1830838.542117563 W.
[2019-03-26 23:49:40,508] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 85.66666666666667, 1.0, 2.0, 0.6683488387162189, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.99795341528479, 6.9112, 168.9124453704888, 1830838.542117563, 1769292.869733582, 378244.9595247827], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4588800.0000, 
sim time next is 4589400.0000, 
raw observation next is [28.0, 84.83333333333333, 1.0, 2.0, 0.6055743569011941, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.923757031123818, 6.9112, 168.9127622407448, 1693184.487643747, 1684276.107830545, 366067.8204460182], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8483333333333333, 1.0, 1.0, 0.5247883818086676, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.001255703112381834, 0.0, 0.8294389911992064, 0.47032902434548524, 0.4678544743973736, 0.5463698812627137], 
reward next is 0.3908, 
noisyNet noise sample is [array([-0.3954421], dtype=float32), -2.8274026]. 
=============================================
[2019-03-26 23:49:41,337] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 7.1100546e-22 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:41,354] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0409
[2019-03-26 23:49:41,360] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 85.66666666666667, 1.0, 2.0, 0.6910342311826162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965734.3298855813, 965734.3298855813, 218921.6578950011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4864800.0000, 
sim time next is 4865400.0000, 
raw observation next is [27.5, 84.0, 1.0, 2.0, 0.6866836397159045, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959651.5431896322, 959651.5431896322, 217996.7302538599], 
processed observation next is [1.0, 0.30434782608695654, 0.5023696682464456, 0.84, 1.0, 1.0, 0.6225104092962704, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26656987310823116, 0.26656987310823116, 0.3253682541102386], 
reward next is 0.6746, 
noisyNet noise sample is [array([0.72709644], dtype=float32), -0.37252864]. 
=============================================
[2019-03-26 23:49:43,591] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2004626e-26 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:43,592] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4920
[2019-03-26 23:49:43,604] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 58.0, 1.0, 2.0, 0.5295253535408311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739943.7797659085, 739943.7797659079, 188351.7016867764], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4553400.0000, 
sim time next is 4554000.0000, 
raw observation next is [32.0, 59.0, 1.0, 2.0, 0.5268508633309894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736205.227550024, 736205.2275500234, 187910.214314848], 
processed observation next is [0.0, 0.7391304347826086, 0.7156398104265403, 0.59, 1.0, 1.0, 0.4299407991939631, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2045014520972289, 0.2045014520972287, 0.28046300644007166], 
reward next is 0.7195, 
noisyNet noise sample is [array([0.80124885], dtype=float32), 0.08198979]. 
=============================================
[2019-03-26 23:49:43,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[77.92223]
 [77.77394]
 [77.61256]
 [77.43342]
 [77.10656]], R is [[78.0775528 ]
 [78.01565552]
 [77.95368958]
 [77.89186096]
 [77.8301239 ]].
[2019-03-26 23:49:54,521] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.7169853e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:54,530] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6926
[2019-03-26 23:49:54,535] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 82.33333333333333, 1.0, 2.0, 0.5090420893356211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711311.4612790997, 711311.4612790997, 185024.7927164774], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4749000.0000, 
sim time next is 4749600.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5061244382373603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707233.1157376021, 707233.1157376014, 184561.0802186313], 
processed observation next is [1.0, 1.0, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.4049692026956148, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19645364326044504, 0.19645364326044484, 0.2754642988337781], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.25777006], dtype=float32), 1.3681548]. 
=============================================
[2019-03-26 23:49:55,365] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.4412444e-20 9.9999928e-01 9.1542999e-25 7.1768324e-07 4.4280876e-30], sum to 1.0000
[2019-03-26 23:49:55,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8925
[2019-03-26 23:49:55,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2009636.943392741 W.
[2019-03-26 23:49:55,390] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.7186543539164791, 1.0, 2.0, 0.7186543539164791, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2009636.943392741, 2009636.943392741, 382208.0231903305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [30.0, 67.33333333333334, 1.0, 2.0, 0.690919323890608, 1.0, 2.0, 0.690919323890608, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1932009.085643579, 1932009.085643578, 370256.4556930488], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6733333333333335, 1.0, 1.0, 0.6276136432416963, 1.0, 1.0, 0.6276136432416963, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5366691904565497, 0.5366691904565495, 0.5526215756612668], 
reward next is 0.4474, 
noisyNet noise sample is [array([-0.33714035], dtype=float32), -1.5471417]. 
=============================================
[2019-03-26 23:49:57,444] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 4.6582395e-33 0.0000000e+00], sum to 1.0000
[2019-03-26 23:49:57,450] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3966
[2019-03-26 23:49:57,454] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5340302231083116, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746240.9685164052, 746240.9685164046, 189099.758521216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5164200.0000, 
sim time next is 5164800.0000, 
raw observation next is [29.66666666666666, 71.33333333333333, 1.0, 2.0, 0.5319240645207146, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 743296.8435203222, 743296.8435203228, 188749.0839518509], 
processed observation next is [0.0, 0.782608695652174, 0.6050552922590835, 0.7133333333333333, 1.0, 1.0, 0.4360530897839935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20647134542231174, 0.20647134542231188, 0.28171505067440433], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.9419591], dtype=float32), 1.845703]. 
=============================================
[2019-03-26 23:50:05,751] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3734976e-30 0.0000000e+00], sum to 1.0000
[2019-03-26 23:50:05,758] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0093
[2019-03-26 23:50:05,767] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5100935444767921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712781.2081546375, 712781.2081546369, 185192.638322905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4922400.0000, 
sim time next is 4923000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5097186140793517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712257.1220534906, 712257.1220534906, 185132.8167083495], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4092995350353635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19784920057041405, 0.19784920057041405, 0.2763176368781336], 
reward next is 0.7237, 
noisyNet noise sample is [array([0.167519], dtype=float32), 0.4188979]. 
=============================================
[2019-03-26 23:50:05,786] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[74.49626 ]
 [74.49891 ]
 [74.572426]
 [74.48457 ]
 [74.51454 ]], R is [[74.48336029]
 [74.46212006]
 [74.4410553 ]
 [74.42024994]
 [74.39977264]].
[2019-03-26 23:50:23,494] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9521844e-11 4.1411218e-04 4.0389506e-14 9.9958593e-01 7.2417777e-17], sum to 1.0000
[2019-03-26 23:50:23,502] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9113
[2019-03-26 23:50:23,506] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.76666666666667, 63.33333333333334, 1.0, 2.0, 0.9657998340901767, 1.0, 2.0, 0.9657998340901767, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2701535.372031679, 2701535.372031678, 508669.1607246746], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5305200.0000, 
sim time next is 5305800.0000, 
raw observation next is [34.08333333333334, 61.66666666666666, 1.0, 2.0, 0.9692847768743139, 1.0, 2.0, 0.9692847768743139, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2711294.032069545, 2711294.032069545, 510708.3474797612], 
processed observation next is [1.0, 0.391304347826087, 0.8143759873617699, 0.6166666666666666, 1.0, 1.0, 0.9629937070774867, 1.0, 1.0, 0.9629937070774867, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7531372311304291, 0.7531372311304291, 0.762251264895166], 
reward next is 0.2377, 
noisyNet noise sample is [array([0.78906506], dtype=float32), -0.20830582]. 
=============================================
[2019-03-26 23:50:24,747] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-03-26 23:50:24,749] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:50:24,751] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:50:24,753] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:50:24,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:50:24,755] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:50:24,756] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:50:24,752] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:50:24,754] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:50:24,758] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:50:24,759] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:50:24,782] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-03-26 23:50:24,783] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-03-26 23:50:24,783] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-03-26 23:50:24,851] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-03-26 23:50:24,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-03-26 23:50:34,876] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:50:34,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.11666666666667, 63.0, 1.0, 2.0, 0.3431124054324728, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 534071.5260210686, 534071.526021068, 169487.2017563391]
[2019-03-26 23:50:34,879] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:50:34,882] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 3.892839e-37 0.000000e+00], sampled 0.5378129081212631
[2019-03-26 23:50:59,746] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:50:59,748] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.95, 69.0, 1.0, 2.0, 0.5929662733961909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874693.4138118197, 874693.4138118197, 205261.9691035615]
[2019-03-26 23:50:59,750] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:50:59,753] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.00000e+00 1.00000e+00 0.00000e+00 8.24061e-36 0.00000e+00], sampled 0.7959935981924773
[2019-03-26 23:51:04,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:51:04,705] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.26666666666667, 79.5, 1.0, 2.0, 0.5741417793593042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802313.059557121, 802313.059557121, 196026.8049645943]
[2019-03-26 23:51:04,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:04,709] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.6094481e-37 0.0000000e+00], sampled 0.3929582701469466
[2019-03-26 23:51:07,315] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:51:07,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 94.0, 1.0, 2.0, 0.6084503108895621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937004.2227734138, 937004.2227734138, 212865.1197646986]
[2019-03-26 23:51:07,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:07,322] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2428444e-34 0.0000000e+00], sampled 0.703496991680848
[2019-03-26 23:51:45,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:51:45,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.90536603166667, 78.31691871999999, 1.0, 2.0, 0.5884202198303835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 822273.658060076, 822273.6580600754, 198600.5964491233]
[2019-03-26 23:51:45,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:51:45,954] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.2498515e-33 0.0000000e+00], sampled 0.24482325758971013
[2019-03-26 23:51:46,739] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:51:46,740] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.1, 82.0, 1.0, 2.0, 0.566637347757696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791822.3621106668, 791822.3621106668, 194695.6462683202]
[2019-03-26 23:51:46,741] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-03-26 23:51:46,746] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.912484e-33 0.000000e+00], sampled 0.005494942254498092
[2019-03-26 23:52:02,700] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1097972]
[2019-03-26 23:52:02,702] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 81.0, 1.0, 2.0, 0.6244935311834086, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.97252416512244, 6.9112, 168.9125711222648, 1746125.974331915, 1702620.600240332, 369192.0642224727]
[2019-03-26 23:52:02,703] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-03-26 23:52:02,705] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5118243e-33 1.0000000e+00 0.0000000e+00 3.0815707e-21 0.0000000e+00], sampled 0.07511361748690037
[2019-03-26 23:52:02,706] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1746125.974331915 W.
[2019-03-26 23:52:14,280] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.1744 2927914232.3319 1338.0000
[2019-03-26 23:52:14,603] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.4620 2779798268.0133 933.0000
[2019-03-26 23:52:15,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.0759 3008237470.0170 1766.0000
[2019-03-26 23:52:15,247] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.6111 2843097899.6270 1131.0000
[2019-03-26 23:52:15,314] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7878.1300 3164560515.3511 1778.0000
[2019-03-26 23:52:16,334] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2475000, evaluation results [2475000.0, 7878.129995644267, 3164560515.3511353, 1778.0, 8254.174442635573, 2927914232.3318715, 1338.0, 8660.46199861073, 2779798268.0133233, 933.0, 7998.075909215718, 3008237470.0169654, 1766.0, 8496.611123670547, 2843097899.6270223, 1131.0]
[2019-03-26 23:52:20,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2490421e-16 1.3515405e-18 2.2691028e-17 1.0000000e+00 1.4896864e-18], sum to 1.0000
[2019-03-26 23:52:20,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2971
[2019-03-26 23:52:20,369] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.28333333333333, 63.33333333333334, 1.0, 2.0, 0.8517413086791193, 1.0, 2.0, 0.7464606938538223, 1.0, 1.0, 1.03, 7.00510969986873, 6.9112, 170.5573041426782, 3132539.635336063, 3065268.297317737, 573541.9186019069], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5393400.0000, 
sim time next is 5394000.0000, 
raw observation next is [34.46666666666667, 62.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.962943259051481, 6.9112, 170.5573041426782, 3663613.903936611, 2910207.42529758, 547545.2212495831], 
processed observation next is [1.0, 0.43478260869565216, 0.8325434439178516, 0.6266666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.10517432590514808, 0.0, 0.8375144448122397, 1.0176705288712808, 0.80839095147155, 0.8172316735068403], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6163586], dtype=float32), 1.0239159]. 
=============================================
[2019-03-26 23:52:20,379] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[21.94989 ]
 [22.75503 ]
 [22.599762]
 [26.47219 ]
 [26.959099]], R is [[22.06521225]
 [21.84456062]
 [21.6261158 ]
 [21.40985489]
 [21.46998024]].
[2019-03-26 23:52:20,505] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.9841309e-11 9.9996781e-01 1.8170053e-14 3.2176955e-05 8.8507366e-18], sum to 1.0000
[2019-03-26 23:52:20,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9438
[2019-03-26 23:52:20,523] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2658689.159426244 W.
[2019-03-26 23:52:20,528] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.06666666666666, 57.00000000000001, 1.0, 2.0, 0.9504985793568872, 1.0, 2.0, 0.9504985793568872, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2658689.159426244, 2658689.159426243, 499798.4573662746], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5307600.0000, 
sim time next is 5308200.0000, 
raw observation next is [35.4, 55.5, 1.0, 2.0, 0.6455847184304166, 1.0, 2.0, 0.6433823987294709, 1.0, 1.0, 1.03, 7.005093441953965, 6.9112, 170.5573041426782, 2699501.749908326, 2632242.058095675, 504692.1297183076], 
processed observation next is [1.0, 0.43478260869565216, 0.8767772511848341, 0.555, 1.0, 1.0, 0.5729936366631526, 1.0, 1.0, 0.5703402394330974, 1.0, 0.5, 1.0365853658536586, 0.009389344195396544, 0.0, 0.8375144448122397, 0.7498615971967573, 0.7311783494710208, 0.7532718354004591], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.28191012], dtype=float32), -0.8636407]. 
=============================================
[2019-03-26 23:52:22,341] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:52:22,351] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8704
[2019-03-26 23:52:22,358] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.90000000000001, 84.33333333333334, 1.0, 2.0, 0.5078105802265798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709590.0336350915, 709590.0336350915, 184828.9231312728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5638800.0000, 
sim time next is 5639400.0000, 
raw observation next is [27.05, 83.5, 1.0, 2.0, 0.5084926615102403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710543.4599184075, 710543.4599184075, 184937.4575291791], 
processed observation next is [0.0, 0.2608695652173913, 0.4810426540284361, 0.835, 1.0, 1.0, 0.4078224837472775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19737318331066875, 0.19737318331066875, 0.27602605601370017], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.06535813], dtype=float32), 0.8056615]. 
=============================================
[2019-03-26 23:52:26,289] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6404987e-20 6.3831217e-29 1.7862825e-20 1.0000000e+00 3.0087183e-20], sum to 1.0000
[2019-03-26 23:52:26,297] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7402
[2019-03-26 23:52:26,301] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.5, 62.0, 1.0, 2.0, 0.9731125870056264, 1.0, 2.0, 0.9731125870056264, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2722012.889226072, 2722012.889226072, 512972.620551136], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5401800.0000, 
sim time next is 5402400.0000, 
raw observation next is [36.66666666666666, 61.33333333333334, 1.0, 2.0, 0.9749534505380296, 1.0, 2.0, 0.9749534505380296, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2727167.815392972, 2727167.815392972, 514057.0586616262], 
processed observation next is [1.0, 0.5217391304347826, 0.9368088467614529, 0.6133333333333334, 1.0, 1.0, 0.9698234343831682, 1.0, 1.0, 0.9698234343831682, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7575466153869367, 0.7575466153869367, 0.7672493412860092], 
reward next is 0.2328, 
noisyNet noise sample is [array([-2.5507245], dtype=float32), -1.0645815]. 
=============================================
[2019-03-26 23:52:27,209] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.7024541e-22 5.6417245e-31 1.8186999e-22 1.0000000e+00 5.8189960e-22], sum to 1.0000
[2019-03-26 23:52:27,220] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5585
[2019-03-26 23:52:27,228] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.5, 62.0, 1.0, 2.0, 0.9731125870056264, 1.0, 2.0, 0.9731125870056264, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2722012.889226072, 2722012.889226072, 512972.620551136], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5401800.0000, 
sim time next is 5402400.0000, 
raw observation next is [36.66666666666666, 61.33333333333334, 1.0, 2.0, 0.9749534505380296, 1.0, 2.0, 0.9749534505380296, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2727167.815392972, 2727167.815392972, 514057.0586616262], 
processed observation next is [1.0, 0.5217391304347826, 0.9368088467614529, 0.6133333333333334, 1.0, 1.0, 0.9698234343831682, 1.0, 1.0, 0.9698234343831682, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7575466153869367, 0.7575466153869367, 0.7672493412860092], 
reward next is 0.2328, 
noisyNet noise sample is [array([0.3731233], dtype=float32), 0.08484283]. 
=============================================
[2019-03-26 23:52:27,759] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2611768e-34 0.0000000e+00], sum to 1.0000
[2019-03-26 23:52:27,772] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0409
[2019-03-26 23:52:27,775] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 85.66666666666667, 1.0, 2.0, 0.5906428527188955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825380.8296423217, 825380.8296423217, 199015.6882775268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5443800.0000, 
sim time next is 5444400.0000, 
raw observation next is [28.9, 86.33333333333334, 1.0, 2.0, 0.5921734180530719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827520.5176142306, 827520.5176142306, 199296.8405240625], 
processed observation next is [1.0, 0.0, 0.5687203791469194, 0.8633333333333334, 1.0, 1.0, 0.5086426723530987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2298668104483974, 0.2298668104483974, 0.2974579709314366], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.5759537], dtype=float32), -0.28738087]. 
=============================================
[2019-03-26 23:52:34,108] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:52:34,117] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1122
[2019-03-26 23:52:34,122] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 92.0, 1.0, 2.0, 0.5432124238834983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759076.5392937129, 759076.5392937136, 190643.3372058023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5533200.0000, 
sim time next is 5533800.0000, 
raw observation next is [26.63333333333333, 92.5, 1.0, 2.0, 0.5424073010377316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757951.0718281234, 757951.071828124, 190507.0367575094], 
processed observation next is [1.0, 0.043478260869565216, 0.46129541864139006, 0.925, 1.0, 1.0, 0.4486834952261826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21054196439670095, 0.21054196439670111, 0.2843388608321036], 
reward next is 0.7157, 
noisyNet noise sample is [array([1.2693832], dtype=float32), -0.942915]. 
=============================================
[2019-03-26 23:52:35,660] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.3056596e-27 0.0000000e+00], sum to 1.0000
[2019-03-26 23:52:35,669] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2485
[2019-03-26 23:52:35,677] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.85, 95.0, 1.0, 2.0, 0.8429484127294752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1178155.247943872, 1178155.247943872, 254725.3105663981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5543400.0000, 
sim time next is 5544000.0000, 
raw observation next is [25.8, 95.0, 1.0, 2.0, 0.8468101052220499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1183555.589092286, 1183555.589092286, 255724.3951900941], 
processed observation next is [1.0, 0.17391304347826086, 0.42180094786729866, 0.95, 1.0, 1.0, 0.8154338617133131, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3287654414145239, 0.3287654414145239, 0.38167820177625983], 
reward next is 0.6183, 
noisyNet noise sample is [array([0.46614856], dtype=float32), 0.046973873]. 
=============================================
[2019-03-26 23:52:35,689] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[53.105263]
 [53.115776]
 [53.17364 ]
 [53.631794]
 [52.894974]], R is [[52.65818405]
 [52.75141525]
 [52.83594513]
 [52.91697693]
 [52.9991684 ]].
[2019-03-26 23:52:38,170] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5136789e-12 1.2934827e-02 5.3085744e-15 9.8706520e-01 3.0364540e-17], sum to 1.0000
[2019-03-26 23:52:38,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5231
[2019-03-26 23:52:38,184] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.86666666666666, 49.66666666666667, 1.0, 2.0, 0.8962368741880538, 1.0, 1.0, 0.8962368741880538, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2506758.6557408, 2506758.6557408, 469433.921460723], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5583000.0000, 
sim time next is 5583600.0000, 
raw observation next is [33.8, 50.0, 1.0, 2.0, 0.8560442451608473, 1.0, 2.0, 0.8560442451608473, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2394232.885133368, 2394232.885133367, 448070.3154559614], 
processed observation next is [1.0, 0.6521739130434783, 0.800947867298578, 0.5, 1.0, 1.0, 0.8265593315190931, 1.0, 1.0, 0.8265593315190931, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6650646903148244, 0.6650646903148242, 0.6687616648596438], 
reward next is 0.3312, 
noisyNet noise sample is [array([-0.3370068], dtype=float32), 0.08509243]. 
=============================================
[2019-03-26 23:52:39,107] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0.000000e+00 1.000000e+00 0.000000e+00 7.434845e-34 0.000000e+00], sum to 1.0000
[2019-03-26 23:52:39,115] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6631
[2019-03-26 23:52:39,119] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 83.66666666666667, 1.0, 2.0, 0.5383356371834498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752259.380780187, 752259.3807801863, 189820.2847899604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5689200.0000, 
sim time next is 5689800.0000, 
raw observation next is [27.7, 84.0, 1.0, 2.0, 0.5376063055533814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751239.8670519596, 751239.8670519596, 189697.8121916432], 
processed observation next is [0.0, 0.8695652173913043, 0.5118483412322274, 0.84, 1.0, 1.0, 0.442899163317327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20867774084776655, 0.20867774084776655, 0.28313106297260177], 
reward next is 0.7169, 
noisyNet noise sample is [array([0.3393072], dtype=float32), -0.6041208]. 
=============================================
[2019-03-26 23:52:39,173] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 2.2635024e-38 0.0000000e+00], sum to 1.0000
[2019-03-26 23:52:39,182] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8433
[2019-03-26 23:52:39,186] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4993119367968658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697710.5388176047, 697710.5388176052, 183488.0833109441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5632200.0000, 
sim time next is 5632800.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4983819537638847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696410.6066368707, 696410.6066368707, 183342.7465845901], 
processed observation next is [0.0, 0.17391304347826086, 0.4123222748815167, 0.91, 1.0, 1.0, 0.3956409081492587, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19344739073246406, 0.19344739073246406, 0.27364589042476134], 
reward next is 0.7264, 
noisyNet noise sample is [array([-0.74474996], dtype=float32), -0.7130494]. 
=============================================
[2019-03-26 23:52:41,845] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4821035e-24 1.0000000e+00 2.5691948e-29 7.0374327e-13 1.9169580e-34], sum to 1.0000
[2019-03-26 23:52:41,854] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4556
[2019-03-26 23:52:41,864] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2259932.912836162 W.
[2019-03-26 23:52:41,871] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.9749445816394311, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.995519313961501, 6.9112, 168.9124551103709, 2259932.912836162, 2200114.067031433, 456089.0244741811], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6015600.0000, 
sim time next is 6016200.0000, 
raw observation next is [29.5, 77.66666666666667, 1.0, 2.0, 0.7327433887123487, 1.0, 1.0, 0.7327433887123487, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2049073.040799799, 2049073.040799799, 388459.9945877941], 
processed observation next is [1.0, 0.6521739130434783, 0.5971563981042655, 0.7766666666666667, 1.0, 1.0, 0.6780040827859622, 1.0, 0.5, 0.6780040827859622, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5691869557777219, 0.5691869557777219, 0.5797910366982002], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2148584], dtype=float32), -0.10338312]. 
=============================================
[2019-03-26 23:52:41,939] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:52:41,948] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4241
[2019-03-26 23:52:41,953] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.53333333333333, 64.83333333333333, 1.0, 2.0, 0.5189555364224218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725168.7741283949, 725168.7741283949, 186619.4367274168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5737800.0000, 
sim time next is 5738400.0000, 
raw observation next is [30.7, 64.0, 1.0, 2.0, 0.5193628575900634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725738.1437058876, 725738.1437058869, 186685.5544897759], 
processed observation next is [0.0, 0.43478260869565216, 0.6540284360189573, 0.64, 1.0, 1.0, 0.42091910553019685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20159392880719101, 0.20159392880719082, 0.2786351559548894], 
reward next is 0.7214, 
noisyNet noise sample is [array([0.03180866], dtype=float32), 1.8358034]. 
=============================================
[2019-03-26 23:52:44,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [0. 1. 0. 0. 0.], sum to 1.0000
[2019-03-26 23:52:44,027] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4338
[2019-03-26 23:52:44,035] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 74.0, 1.0, 2.0, 0.5315173219168408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742728.2732834636, 742728.2732834629, 188681.5681455161], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5684400.0000, 
sim time next is 5685000.0000, 
raw observation next is [28.98333333333333, 75.5, 1.0, 2.0, 0.5346733109947537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747139.920075564, 747139.9200755634, 189206.8853414786], 
processed observation next is [0.0, 0.8260869565217391, 0.5726698262243285, 0.755, 1.0, 1.0, 0.43936543493343816, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20753886668765664, 0.2075388666876565, 0.28239833633056505], 
reward next is 0.7176, 
noisyNet noise sample is [array([1.4291242], dtype=float32), -0.93417174]. 
=============================================
[2019-03-26 23:52:44,056] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.03234]
 [73.93018]
 [73.9292 ]
 [73.93092]
 [73.90354]], R is [[73.93960571]
 [73.91859436]
 [73.89761353]
 [73.87667847]
 [73.85600281]].
[2019-03-26 23:52:45,581] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 6.1384615e-32 0.0000000e+00], sum to 1.0000
[2019-03-26 23:52:45,590] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5195
[2019-03-26 23:52:45,595] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.56666666666667, 80.66666666666667, 1.0, 2.0, 0.5706608332825299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797446.9150249014, 797446.9150249014, 195408.2689505969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5941200.0000, 
sim time next is 5941800.0000, 
raw observation next is [29.45, 81.0, 1.0, 2.0, 0.5688148956551572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794866.4189164727, 794866.4189164727, 195080.8698226975], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.81, 1.0, 1.0, 0.4804998742833219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22079622747679797, 0.22079622747679797, 0.29116547734730974], 
reward next is 0.7088, 
noisyNet noise sample is [array([1.8052082], dtype=float32), -0.18225546]. 
=============================================
[2019-03-26 23:52:54,873] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2441411e-35 1.0000000e+00 0.0000000e+00 9.2012744e-21 0.0000000e+00], sum to 1.0000
[2019-03-26 23:52:54,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7121
[2019-03-26 23:52:54,895] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1963679.944062809 W.
[2019-03-26 23:52:54,901] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.6, 83.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.629502893913581, 6.9112, 168.9087034292206, 1963679.944062809, 1454103.98004513, 311352.6526017103], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5902200.0000, 
sim time next is 5902800.0000, 
raw observation next is [28.8, 82.66666666666667, 1.0, 2.0, 0.3457305946144949, 1.0, 1.0, 0.3457305946144949, 1.0, 1.0, 0.6004197079905996, 6.9112, 6.9112, 170.5573041426782, 1449817.260646099, 1449817.260646099, 324881.8412771264], 
processed observation next is [1.0, 0.30434782608695654, 0.5639810426540285, 0.8266666666666667, 1.0, 1.0, 0.21172360796927095, 1.0, 0.5, 0.21172360796927095, 1.0, 0.5, 0.5127069609641458, 0.0, 0.0, 0.8375144448122397, 0.4027270168461386, 0.4027270168461386, 0.4848982705628752], 
reward next is 0.5151, 
noisyNet noise sample is [array([-0.06700359], dtype=float32), 1.2651951]. 
=============================================
[2019-03-26 23:52:57,118] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.04745046e-17 8.42996091e-02 4.72194013e-22 9.15700436e-01
 1.60600448e-26], sum to 1.0000
[2019-03-26 23:52:57,123] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6428
[2019-03-26 23:52:57,132] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.4, 79.0, 1.0, 2.0, 0.8349706591639859, 1.0, 2.0, 0.8349706591639859, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2335238.019070949, 2335238.019070949, 437263.6432969697], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5936400.0000, 
sim time next is 5937000.0000, 
raw observation next is [30.3, 79.16666666666667, 1.0, 2.0, 0.3985840414520514, 1.0, 2.0, 0.3985840414520514, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1114130.709791446, 1114130.709791446, 271784.2483161376], 
processed observation next is [1.0, 0.7391304347826086, 0.6350710900473934, 0.7916666666666667, 1.0, 1.0, 0.2754024595807848, 1.0, 1.0, 0.2754024595807848, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.30948075271984615, 0.30948075271984615, 0.4056481318151307], 
reward next is 0.5944, 
noisyNet noise sample is [array([0.01350722], dtype=float32), 0.965173]. 
=============================================
[2019-03-26 23:52:57,148] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[40.34336 ]
 [39.99129 ]
 [40.895355]
 [39.93987 ]
 [41.673378]], R is [[47.59620667]
 [47.12024689]
 [46.64904404]
 [46.18255234]
 [46.04715347]].
[2019-03-26 23:53:04,856] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4968765e-24 1.0000000e+00 1.1754366e-31 7.9957512e-16 8.8257791e-38], sum to 1.0000
[2019-03-26 23:53:04,865] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3162
[2019-03-26 23:53:04,871] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2268500.381314809 W.
[2019-03-26 23:53:04,877] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.5, 73.5, 1.0, 2.0, 0.9810657457205876, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991165890967915, 6.9112, 168.9124804770152, 2268500.381314809, 2211769.98592764, 458086.8459385913], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6192600.0000, 
sim time next is 6193200.0000, 
raw observation next is [29.40000000000001, 74.0, 1.0, 2.0, 0.5442759611312823, 1.0, 1.0, 0.5442759611312823, 1.0, 2.0, 0.9404560416712734, 6.9112, 6.9112, 170.5573041426782, 2283291.356127293, 2283291.356127293, 446167.2967663676], 
processed observation next is [1.0, 0.6956521739130435, 0.5924170616113749, 0.74, 1.0, 1.0, 0.45093489292925576, 1.0, 0.5, 0.45093489292925576, 1.0, 1.0, 0.9273854166722845, 0.0, 0.0, 0.8375144448122397, 0.634247598924248, 0.634247598924248, 0.6659213384572651], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.20615613], dtype=float32), 0.9954667]. 
=============================================
[2019-03-26 23:53:06,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [8.8142307e-16 7.6581407e-01 3.1552252e-20 2.3418593e-01 1.5907857e-24], sum to 1.0000
[2019-03-26 23:53:06,352] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3198
[2019-03-26 23:53:06,363] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2129937.661096368 W.
[2019-03-26 23:53:06,369] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.13333333333333, 85.0, 1.0, 2.0, 0.7616316549085643, 1.0, 2.0, 0.7616316549085643, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2129937.661096368, 2129937.661096369, 401625.6697651451], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6165600.0000, 
sim time next is 6166200.0000, 
raw observation next is [28.21666666666667, 84.5, 1.0, 2.0, 0.8557499177315474, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998291786171195, 6.9112, 168.9124382053084, 2093100.445319304, 2031314.72433001, 422371.2230468337], 
processed observation next is [1.0, 0.34782608695652173, 0.5363349131121644, 0.845, 1.0, 1.0, 0.8262047201584909, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008709178617119484, 0.0, 0.8294374000368696, 0.5814167903664733, 0.5642540900916694, 0.6304048105176623], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.73810333], dtype=float32), -1.1254966]. 
=============================================
[2019-03-26 23:53:07,984] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.1957353e-36 0.0000000e+00], sum to 1.0000
[2019-03-26 23:53:07,992] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4903
[2019-03-26 23:53:07,997] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 86.16666666666667, 1.0, 2.0, 0.5340542116640158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746274.5013215193, 746274.5013215187, 189103.4894171029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6315000.0000, 
sim time next is 6315600.0000, 
raw observation next is [27.23333333333333, 86.33333333333334, 1.0, 2.0, 0.5327938524652397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744512.6885001892, 744512.6885001892, 188893.5925057874], 
processed observation next is [0.0, 0.08695652173913043, 0.4897314375987361, 0.8633333333333334, 1.0, 1.0, 0.4371010270665538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20680908013894145, 0.20680908013894145, 0.28193073508326477], 
reward next is 0.7181, 
noisyNet noise sample is [array([-1.3045999], dtype=float32), -0.5026384]. 
=============================================
[2019-03-26 23:53:12,032] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-03-26 23:53:12,036] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-03-26 23:53:12,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:53:12,038] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-03-26 23:53:12,039] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-03-26 23:53:12,039] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:53:12,040] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:53:12,041] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-03-26 23:53:12,042] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-03-26 23:53:12,042] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:53:12,045] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-03-26 23:53:12,835] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-03-26 23:53:12,966] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-03-26 23:53:13,003] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-03-26 23:53:13,008] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-03-26 23:53:13,069] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/7/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-03-26 23:53:25,729] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:53:25,730] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.9, 88.66666666666667, 1.0, 2.0, 0.4617881810516305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 657957.0042874261, 657957.0042874255, 179457.0283121427]
[2019-03-26 23:53:25,730] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:53:25,736] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.3575234e-34 0.0000000e+00], sampled 0.9509566146718578
[2019-03-26 23:53:57,519] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:53:57,523] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.01666666666667, 70.5, 1.0, 2.0, 0.5737215416554504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801725.5923920145, 801725.5923920152, 195952.2989591273]
[2019-03-26 23:53:57,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:53:57,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.824427e-34 0.000000e+00], sampled 0.4612193181530506
[2019-03-26 23:54:13,820] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:54:13,822] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.4, 43.0, 1.0, 2.0, 0.8310089749142419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1161458.846938262, 1161458.846938262, 251665.2352505539]
[2019-03-26 23:54:13,823] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-03-26 23:54:13,826] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4525845e-30 0.0000000e+00], sampled 0.10102560751662448
[2019-03-26 23:54:15,325] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:54:15,326] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.00093156333333, 59.57509354333334, 1.0, 2.0, 0.531628573641645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742883.7878684163, 742883.7878684163, 188700.1654267009]
[2019-03-26 23:54:15,327] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:54:15,330] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 9.723007e-36 0.000000e+00], sampled 0.43012093224049164
[2019-03-26 23:54:16,446] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:54:16,449] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.53601158333334, 61.39770114333334, 1.0, 2.0, 0.5017864247170576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701169.3908281142, 701169.3908281137, 183876.1863166973]
[2019-03-26 23:54:16,450] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:54:16,452] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.4555547e-34 0.0000000e+00], sampled 0.9360216993137936
[2019-03-26 23:55:04,934] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.13253538], dtype=float32), -1.1938792]
[2019-03-26 23:55:04,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.49638819666667, 67.28402142666667, 1.0, 2.0, 1.011615284907947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1420161.336340105, 1420161.336340105, 303449.8650504508]
[2019-03-26 23:55:04,936] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-03-26 23:55:04,939] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.000000e+00 1.000000e+00 0.000000e+00 8.133988e-28 0.000000e+00], sampled 0.4114974479658128
[2019-03-26 23:55:06,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.8629 2843167093.8007 1131.0000
[2019-03-26 23:55:06,995] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.0319 2779830318.5638 932.0000
[2019-03-26 23:55:07,167] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.9350 3164186355.8058 1774.0000
[2019-03-26 23:55:07,258] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.0740 2927981547.8413 1338.0000
[2019-03-26 23:55:07,516] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7997.8772 3008370595.0632 1766.0000
[2019-03-26 23:55:08,536] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2500000, evaluation results [2500000.0, 7876.934976147337, 3164186355.8058395, 1774.0, 8254.073971726037, 2927981547.8412614, 1338.0, 8660.03188781958, 2779830318.5637875, 932.0, 7997.87721511679, 3008370595.063247, 1766.0, 8495.862932155293, 2843167093.8007364, 1131.0]
