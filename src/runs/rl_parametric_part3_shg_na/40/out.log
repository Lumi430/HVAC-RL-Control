Using TensorFlow backend.
[2019-04-27 22:18:34,691] A3C_AGENT_MAIN INFO:Namespace(action_repeat_n=1, action_space='part3_v1', activation='relu', agent_num=5, check_args_only=False, clip_norm=5.0, debug_log_prob=0.0005, decay_steps=1000000, dropout_prob=0.0, end_e=0.0, env='Part3-NA-Shg-Train-v1', eval_act_func='part3_shg_det_v1', eval_env_res_max_keep=100, eval_epi_num=1, eval_freq=25000, forecast_dim=0, gamma=0.99, h_decay_bounds=[], h_regu_frac=[0.0], init_e=0.0, isNoisyNet=True, isNoisyNetEval_rmNoise=True, is_add_time_to_state=True, is_greedy_policy=False, is_learning_rate_decay_staircase=False, is_r_term_zero=True, is_warm_start=False, job_mode='Train', learning_rate=5e-05, learning_rate_decay_rate=1.0, learning_rate_decay_steps=100000, max_interactions=2500000, metric_func='part3_v1', model_dir='None', model_param=[256, 8], model_type='nn', num_threads=16, output='./Part3-NA-Shg-Train-v1-res1', p_loss_frac=1.0, raw_state_prcs_func='cslDx_1', reward_func='part3_v3', rmsprop_decay=0.99, rmsprop_epsil=1e-10, rmsprop_momet=0.0, rwd_e_para=1.0, rwd_p_para=1.0, save_freq=500000, save_max_to_keep=5, save_scope='all', sharedNet_type='Dense', state_dim=17, test_env=['Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'], test_mode='Multiple', train_act_func='part3_shg_sto_v1', train_freq=5, v_loss_frac=0.5, violation_penalty_scl=50.0, weight_initer='glorot_uniform', window_len=29)
[2019-04-27 22:18:34,691] A3C_AGENT_MAIN INFO:Start compiling...
2019-04-27 22:18:34.790450: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA
[2019-04-27 22:19:06,695] A3C_AGENT_MAIN INFO:Start the learning...
[2019-04-27 22:19:06,696] A3C_AGENT_MAIN INFO:Prepare the evaluation environments ['Part3-NA-Shg-Train-v1', 'Part3-NA-Shg-Test-v1', 'Part3-NA-Shg-Test-v2', 'Part3-NA-Shg-Test-v3', 'Part3-NA-Shg-Test-v4'] ...
[2019-04-27 22:19:06,706] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation worker starts!
[2019-04-27 22:19:06,711] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation worker starts!
[2019-04-27 22:19:06,716] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation worker starts!
[2019-04-27 22:19:06,721] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation worker starts!
[2019-04-27 22:19:06,724] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation worker starts!
[2019-04-27 22:19:06,724] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:06,725] A3C_AGENT_WORKER-Thread-2 INFO:Local worker starts!
[2019-04-27 22:19:06,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:06,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run1
[2019-04-27 22:19:07,726] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:07,729] A3C_AGENT_WORKER-Thread-3 INFO:Local worker starts!
[2019-04-27 22:19:07,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:07,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run1
[2019-04-27 22:19:08,232] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-27 22:19:08,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:19:08,234] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:19:08,234] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,234] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:19:08,235] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,235] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:19:08,235] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:19:08,236] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,236] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,236] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,240] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run1
[2019-04-27 22:19:08,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run1
[2019-04-27 22:19:08,241] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run1
[2019-04-27 22:19:08,253] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run1
[2019-04-27 22:19:08,284] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run1
[2019-04-27 22:19:08,729] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:08,730] A3C_AGENT_WORKER-Thread-9 INFO:Local worker starts!
[2019-04-27 22:19:08,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:08,848] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run1
[2019-04-27 22:19:09,732] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:09,733] A3C_AGENT_WORKER-Thread-10 INFO:Local worker starts!
[2019-04-27 22:19:09,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:09,960] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run1
[2019-04-27 22:19:10,734] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:10,737] A3C_AGENT_WORKER-Thread-11 INFO:Local worker starts!
[2019-04-27 22:19:10,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:10,886] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run1
[2019-04-27 22:19:11,736] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:11,741] A3C_AGENT_WORKER-Thread-12 INFO:Local worker starts!
[2019-04-27 22:19:11,849] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:11,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run1
[2019-04-27 22:19:12,739] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:12,746] A3C_AGENT_WORKER-Thread-13 INFO:Local worker starts!
[2019-04-27 22:19:12,866] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:12,868] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run1
[2019-04-27 22:19:13,745] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:13,753] A3C_AGENT_WORKER-Thread-14 INFO:Local worker starts!
[2019-04-27 22:19:13,861] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:13,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run1
[2019-04-27 22:19:14,638] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:19:14,639] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.362710815, 98.01161515000001, 1.0, 2.0, 0.2710052551562356, 1.0, 1.0, 0.2710052551562356, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 757392.2990403676, 757392.2990403676, 244707.5311718741]
[2019-04-27 22:19:14,640] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:19:14,644] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.18341266 0.20351295 0.21329848 0.18486111 0.21491483], sampled 0.11847072515201229
[2019-04-27 22:19:14,750] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:14,756] A3C_AGENT_WORKER-Thread-15 INFO:Local worker starts!
[2019-04-27 22:19:14,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:14,847] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run1
[2019-04-27 22:19:15,756] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:15,759] A3C_AGENT_WORKER-Thread-16 INFO:Local worker starts!
[2019-04-27 22:19:15,850] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:15,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run1
[2019-04-27 22:19:16,761] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:16,765] A3C_AGENT_WORKER-Thread-17 INFO:Local worker starts!
[2019-04-27 22:19:16,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:16,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run1
[2019-04-27 22:19:17,765] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:17,770] A3C_AGENT_WORKER-Thread-18 INFO:Local worker starts!
[2019-04-27 22:19:17,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:17,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run1
[2019-04-27 22:19:18,769] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:18,771] A3C_AGENT_WORKER-Thread-19 INFO:Local worker starts!
[2019-04-27 22:19:18,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:18,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run1
[2019-04-27 22:19:19,772] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:19,776] A3C_AGENT_WORKER-Thread-20 INFO:Local worker starts!
[2019-04-27 22:19:19,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:19,864] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run1
[2019-04-27 22:19:20,776] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:20,781] A3C_AGENT_WORKER-Thread-21 INFO:Local worker starts!
[2019-04-27 22:19:20,871] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:20,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run1
[2019-04-27 22:19:21,780] A3C_AGENT_MAIN INFO:Prepare the local workers ...
[2019-04-27 22:19:21,786] A3C_AGENT_WORKER-Thread-22 INFO:Local worker starts!
[2019-04-27 22:19:21,874] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:19:21,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run1
[2019-04-27 22:19:37,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:19:37,806] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.6, 89.0, 1.0, 2.0, 0.17, 1.0, 1.0, 0.17, 1.0, 2.0, 0.1850429065126847, 6.9112, 6.9112, 170.5573041426782, 484599.379881648, 484599.379881648, 228039.3275299871]
[2019-04-27 22:19:37,808] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:19:37,811] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.18091236 0.20379964 0.21548037 0.18882373 0.21098387], sampled 0.012759242688169636
[2019-04-27 22:19:49,089] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:19:49,090] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.85745514, 83.17065577, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9242472107456254, 6.911200000000001, 6.9112, 168.912956510431, 757443.0502172369, 757443.0502172362, 228917.1803977135]
[2019-04-27 22:19:49,091] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:19:49,096] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.1811762  0.199562   0.21330371 0.18681633 0.21914174], sampled 0.5533468782158346
[2019-04-27 22:20:01,676] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:01,678] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.8, 58.5, 1.0, 2.0, 0.293403465810432, 0.0, 2.0, 0.0, 1.0, 1.0, 0.5095447901038713, 6.9112, 6.9112, 168.9129565056218, 820018.337061982, 820018.337061982, 219858.7390711227]
[2019-04-27 22:20:01,679] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:20:01,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.17946744 0.20578128 0.22263815 0.1853173  0.20679586], sampled 0.3476077764697286
[2019-04-27 22:20:06,429] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:06,430] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 77.33333333333334, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.8651891884586932, 6.9112, 6.9112, 168.912956510431, 714989.9181316332, 714989.9181316332, 215532.9106572285]
[2019-04-27 22:20:06,431] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:20:06,434] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.18200547 0.2017883  0.21242924 0.18471092 0.21906605], sampled 0.07072198763475446
[2019-04-27 22:20:11,478] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:11,480] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.33333333333334, 51.0, 1.0, 2.0, 0.1928183216772606, 1.0, 1.0, 0.1928183216772606, 1.0, 2.0, 0.3348616587600214, 6.9112, 6.9112, 169.0403247858759, 808342.4220677047, 808342.4220677047, 267601.9276895838]
[2019-04-27 22:20:11,483] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:20:11,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [0.18424849 0.20149364 0.21108606 0.19009165 0.21308017], sampled 0.3744533895689306
[2019-04-27 22:20:12,530] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:12,532] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.31613600666667, 95.00908838000001, 1.0, 2.0, 0.2749825109299968, 0.0, 2.0, 0.0, 1.0, 1.0, 0.4696621033495873, 6.9112, 6.9112, 168.912956510431, 768515.9275377775, 768515.9275377775, 213549.8813379684]
[2019-04-27 22:20:12,533] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:20:12,537] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.1786565  0.20598207 0.21733035 0.18446024 0.2135709 ], sampled 0.1797330286970067
[2019-04-27 22:20:23,454] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:23,456] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 81.5, 1.0, 2.0, 0.6105353936930136, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 853190.3873601814, 853190.3873601807, 202717.0609208821]
[2019-04-27 22:20:23,457] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:20:23,459] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [0.18189944 0.2019883  0.21385019 0.19028674 0.21197537], sampled 0.7032339442148535
[2019-04-27 22:20:36,507] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:36,509] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.32237404, 80.75907397333333, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 1.0, 2.0, 0.7305773250865467, 6.9112, 6.9112, 168.912956510431, 624669.6647298745, 624669.6647298745, 188338.2559340417]
[2019-04-27 22:20:36,510] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:20:36,512] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [0.18107428 0.2020621  0.21877399 0.18622294 0.21186668], sampled 0.34518711116420153
[2019-04-27 22:20:40,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.0]
[2019-04-27 22:20:40,905] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.51031080666667, 74.26844011, 1.0, 2.0, 0.6026314070410581, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.918872287828771, 6.9112, 168.9127654756145, 1684949.474655592, 1679506.495765925, 365483.7564571052]
[2019-04-27 22:20:40,906] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:20:40,907] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [0.18334004 0.20269784 0.2158072  0.18672268 0.2114323 ], sampled 0.217859204852213
[2019-04-27 22:20:40,907] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1684949.474655592 W.
[2019-04-27 22:21:03,822] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 3410.2766 3489132469.9939 1377.0000
[2019-04-27 22:21:03,939] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 3666.6048 3151572199.5011 632.0000
[2019-04-27 22:21:04,170] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 3506.1475 3333910149.6805 1219.0000
[2019-04-27 22:21:04,390] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 3569.7936 3277255328.6951 961.0000
[2019-04-27 22:21:04,600] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 3507.7274 3193249084.3307 799.0000
[2019-04-27 22:21:05,613] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 0, evaluation results [0.0, 3410.2766214449, 3489132469.993927, 1377.0, 3569.793603342031, 3277255328.6951303, 961.0, 3666.604808979073, 3151572199.501123, 632.0, 3506.147462178064, 3333910149.6804566, 1219.0, 3507.727374980858, 3193249084.330739, 799.0]
[2019-04-27 22:21:09,648] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [0.17779975 0.1969134  0.22464336 0.18886122 0.21178234], sum to 1.0000
[2019-04-27 22:21:09,655] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1011
[2019-04-27 22:21:09,813] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 0, 
current raw observation is [21.1, 85.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 517775.2873553025, 517775.2873553025, 236001.3256237899], 
current ob forecast is [], 
actual action is [1, 0, 0, 0, 0], 
sim time this is 12600.0000, 
sim time next is 13200.0000, 
raw observation next is [21.13333333333333, 85.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.5729199607352039, 6.911199999999999, 6.9112, 168.912956510431, 505720.444757449, 505720.4447574496, 161862.9723843477], 
processed observation next is [1.0, 0.13043478260869565, 0.20063191153238533, 0.85, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 0.4791706838234194, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1404779013215136, 0.14047790132151378, 0.24158652594678762], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.46067202], dtype=float32), -0.72723424]. 
=============================================
[2019-04-27 22:21:23,541] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.2050275e-04 9.9924880e-01 3.0623862e-05 7.0887712e-10 5.8803185e-10], sum to 1.0000
[2019-04-27 22:21:23,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5568
[2019-04-27 22:21:23,710] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 95.5, 1.0, 2.0, 0.2650526154486161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 431053.1406491161, 431053.1406491161, 162334.7419220064], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 275400.0000, 
sim time next is 276000.0000, 
raw observation next is [19.33333333333333, 95.66666666666667, 1.0, 2.0, 0.26342732567264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 428744.169440457, 428744.1694404576, 162181.6441650866], 
processed observation next is [0.0, 0.17391304347826086, 0.11532385466034739, 0.9566666666666667, 1.0, 1.0, 0.11256304297908432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11909560262234917, 0.11909560262234933, 0.24206215547027848], 
reward next is 0.7579, 
noisyNet noise sample is [array([2.2811396], dtype=float32), -0.26238957]. 
=============================================
[2019-04-27 22:21:23,744] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[62.145863]
 [62.12984 ]
 [62.195034]
 [62.12443 ]
 [62.055065]], R is [[62.58367538]
 [62.71554947]
 [62.84584808]
 [62.97457504]
 [63.10177994]].
[2019-04-27 22:21:24,599] A3C_AGENT_WORKER-Thread-10 INFO:Local step 500, global step 7806: loss 1.7171
[2019-04-27 22:21:24,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 500, global step 7807: learning rate 0.0000
[2019-04-27 22:21:24,696] A3C_AGENT_WORKER-Thread-11 INFO:Local step 500, global step 7815: loss 1.3694
[2019-04-27 22:21:24,699] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 500, global step 7816: learning rate 0.0000
[2019-04-27 22:21:24,750] A3C_AGENT_WORKER-Thread-13 INFO:Local step 500, global step 7840: loss 0.1447
[2019-04-27 22:21:24,752] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 500, global step 7840: learning rate 0.0000
[2019-04-27 22:21:24,884] A3C_AGENT_WORKER-Thread-2 INFO:Local step 500, global step 7912: loss 0.4301
[2019-04-27 22:21:24,892] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 500, global step 7913: learning rate 0.0000
[2019-04-27 22:21:24,957] A3C_AGENT_WORKER-Thread-14 INFO:Local step 500, global step 7947: loss 0.6802
[2019-04-27 22:21:24,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 500, global step 7948: learning rate 0.0000
[2019-04-27 22:21:25,027] A3C_AGENT_WORKER-Thread-22 INFO:Local step 500, global step 7981: loss 0.4014
[2019-04-27 22:21:25,035] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 500, global step 7983: learning rate 0.0000
[2019-04-27 22:21:25,041] A3C_AGENT_WORKER-Thread-12 INFO:Local step 500, global step 7987: loss 0.3635
[2019-04-27 22:21:25,046] A3C_AGENT_WORKER-Thread-16 INFO:Local step 500, global step 7990: loss 0.0300
[2019-04-27 22:21:25,047] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 500, global step 7990: learning rate 0.0000
[2019-04-27 22:21:25,048] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 500, global step 7990: learning rate 0.0000
[2019-04-27 22:21:25,086] A3C_AGENT_WORKER-Thread-20 INFO:Local step 500, global step 8005: loss 0.0150
[2019-04-27 22:21:25,089] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 500, global step 8006: learning rate 0.0000
[2019-04-27 22:21:25,108] A3C_AGENT_WORKER-Thread-18 INFO:Local step 500, global step 8017: loss 0.0036
[2019-04-27 22:21:25,111] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 500, global step 8017: learning rate 0.0000
[2019-04-27 22:21:25,115] A3C_AGENT_WORKER-Thread-15 INFO:Local step 500, global step 8020: loss 0.0043
[2019-04-27 22:21:25,118] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 500, global step 8021: learning rate 0.0000
[2019-04-27 22:21:25,155] A3C_AGENT_WORKER-Thread-17 INFO:Local step 500, global step 8040: loss 0.0469
[2019-04-27 22:21:25,159] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 500, global step 8041: learning rate 0.0000
[2019-04-27 22:21:25,209] A3C_AGENT_WORKER-Thread-3 INFO:Local step 500, global step 8073: loss 0.6118
[2019-04-27 22:21:25,211] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 500, global step 8075: learning rate 0.0000
[2019-04-27 22:21:25,230] A3C_AGENT_WORKER-Thread-9 INFO:Local step 500, global step 8084: loss 0.9858
[2019-04-27 22:21:25,233] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 500, global step 8084: learning rate 0.0000
[2019-04-27 22:21:25,276] A3C_AGENT_WORKER-Thread-21 INFO:Local step 500, global step 8100: loss 1.6051
[2019-04-27 22:21:25,280] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 500, global step 8101: learning rate 0.0000
[2019-04-27 22:21:25,464] A3C_AGENT_WORKER-Thread-19 INFO:Local step 500, global step 8199: loss 0.1091
[2019-04-27 22:21:25,466] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 500, global step 8200: learning rate 0.0000
[2019-04-27 22:21:26,107] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.3744230e-06 9.9986458e-01 1.3300458e-04 1.9925899e-09 4.2823065e-09], sum to 1.0000
[2019-04-27 22:21:26,115] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8753
[2019-04-27 22:21:26,285] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333333, 78.16666666666667, 1.0, 2.0, 0.3057860900630685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484764.9369811529, 484764.9369811529, 165926.998357975], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 317400.0000, 
sim time next is 318000.0000, 
raw observation next is [22.86666666666667, 78.33333333333334, 1.0, 2.0, 0.3051888988695611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484268.9324126781, 484268.9324126774, 165899.7505884923], 
processed observation next is [0.0, 0.6956521739130435, 0.28278041074249627, 0.7833333333333334, 1.0, 1.0, 0.16287819140910972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13451914789241057, 0.13451914789241037, 0.2476115680425258], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.05170062], dtype=float32), -0.46726906]. 
=============================================
[2019-04-27 22:21:26,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[53.61639 ]
 [53.57123 ]
 [53.518192]
 [53.481106]
 [53.47507 ]], R is [[53.83914566]
 [54.05310059]
 [54.26483154]
 [54.47425842]
 [54.68141556]].
[2019-04-27 22:21:32,469] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0882793e-07 9.9999869e-01 1.0904768e-06 1.7211176e-10 3.3581051e-11], sum to 1.0000
[2019-04-27 22:21:32,479] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1930
[2019-04-27 22:21:32,643] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.0, 1.0, 2.0, 0.2397853028395814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 395649.4862059378, 395649.4862059372, 159880.4258597444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 436800.0000, 
sim time next is 437400.0000, 
raw observation next is [19.6, 85.0, 1.0, 2.0, 0.2393795938962013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 394980.12064865, 394980.1206486494, 159841.9166773104], 
processed observation next is [1.0, 0.043478260869565216, 0.127962085308057, 0.85, 1.0, 1.0, 0.08358987216409795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10971670018018055, 0.10971670018018038, 0.23857002489150805], 
reward next is 0.7614, 
noisyNet noise sample is [array([0.48892152], dtype=float32), 0.20745444]. 
=============================================
[2019-04-27 22:21:33,366] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.6377441e-05 9.9995267e-01 9.8545570e-07 1.0528189e-09 1.1820855e-08], sum to 1.0000
[2019-04-27 22:21:33,380] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3577
[2019-04-27 22:21:33,546] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 81.5, 1.0, 2.0, 0.2323528972158815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 383386.1076044721, 383386.1076044721, 159184.1578109189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 455400.0000, 
sim time next is 456000.0000, 
raw observation next is [20.1, 81.33333333333334, 1.0, 2.0, 0.2356152286388764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 388645.8454703358, 388645.8454703358, 159492.8250227705], 
processed observation next is [1.0, 0.2608695652173913, 0.15165876777251197, 0.8133333333333335, 1.0, 1.0, 0.07905449233599567, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1079571792973155, 0.1079571792973155, 0.23804899257129924], 
reward next is 0.7620, 
noisyNet noise sample is [array([-0.93685454], dtype=float32), -0.60323167]. 
=============================================
[2019-04-27 22:21:33,563] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[54.668926]
 [54.648018]
 [54.64378 ]
 [54.615505]
 [54.58131 ]], R is [[54.89536667]
 [55.10882568]
 [55.32036972]
 [55.52987289]
 [55.73729324]].
[2019-04-27 22:21:39,009] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4103911e-10 1.0000000e+00 7.1318063e-09 3.2018452e-14 5.2640496e-14], sum to 1.0000
[2019-04-27 22:21:39,020] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7787
[2019-04-27 22:21:39,175] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 55.0, 1.0, 2.0, 0.633009382082244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1039117.682835463, 1039117.682835462, 222885.9367263796], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 560400.0000, 
sim time next is 561000.0000, 
raw observation next is [24.78333333333333, 54.5, 1.0, 2.0, 0.6412729187898063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1051154.283686693, 1051154.283686693, 224749.5555619333], 
processed observation next is [1.0, 0.4782608695652174, 0.37361769352290675, 0.545, 1.0, 1.0, 0.567798697337116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2919873010240814, 0.2919873010240814, 0.33544709785363175], 
reward next is 0.6646, 
noisyNet noise sample is [array([0.60335714], dtype=float32), 0.48002583]. 
=============================================
[2019-04-27 22:21:39,198] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[86.75387 ]
 [86.46642 ]
 [86.38239 ]
 [86.31196 ]
 [86.134674]], R is [[86.76901245]
 [86.56865692]
 [86.37643433]
 [86.1869812 ]
 [85.99984741]].
[2019-04-27 22:21:40,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0451158e-06 9.9999452e-01 3.8147653e-07 2.6491842e-10 2.9401206e-09], sum to 1.0000
[2019-04-27 22:21:40,099] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3952
[2019-04-27 22:21:40,104] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 65.5, 1.0, 2.0, 0.2548724681904998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416457.5175737196, 416457.5175737189, 161370.4445274106], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 581400.0000, 
sim time next is 582000.0000, 
raw observation next is [22.96666666666667, 66.0, 1.0, 2.0, 0.2559756984143901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418568.5634198869, 418568.5634198862, 161487.9069930623], 
processed observation next is [1.0, 0.7391304347826086, 0.2875197472353872, 0.66, 1.0, 1.0, 0.10358517881251819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11626904539441303, 0.11626904539441284, 0.24102672685531687], 
reward next is 0.7590, 
noisyNet noise sample is [array([0.5602107], dtype=float32), 0.16091447]. 
=============================================
[2019-04-27 22:21:40,126] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[73.02682 ]
 [72.93019 ]
 [73.09114 ]
 [73.117096]
 [73.06682 ]], R is [[72.90596771]
 [72.93605804]
 [72.96537781]
 [72.98208618]
 [72.92523193]].
[2019-04-27 22:21:41,003] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1000, global step 15824: loss 0.7017
[2019-04-27 22:21:41,005] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1000, global step 15825: learning rate 0.0000
[2019-04-27 22:21:41,058] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1000, global step 15846: loss 0.1450
[2019-04-27 22:21:41,060] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1000, global step 15848: learning rate 0.0000
[2019-04-27 22:21:41,131] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1000, global step 15883: loss 0.0243
[2019-04-27 22:21:41,133] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1000, global step 15883: learning rate 0.0000
[2019-04-27 22:21:41,158] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1000, global step 15897: loss 0.0146
[2019-04-27 22:21:41,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1000, global step 15900: learning rate 0.0000
[2019-04-27 22:21:41,177] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1000, global step 15906: loss 0.0896
[2019-04-27 22:21:41,179] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1000, global step 15906: learning rate 0.0000
[2019-04-27 22:21:41,188] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1000, global step 15910: loss 0.1748
[2019-04-27 22:21:41,190] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1000, global step 15910: learning rate 0.0000
[2019-04-27 22:21:41,200] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1000, global step 15913: loss 0.1669
[2019-04-27 22:21:41,202] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1000, global step 15913: learning rate 0.0000
[2019-04-27 22:21:41,408] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1000, global step 16013: loss 0.2179
[2019-04-27 22:21:41,410] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1000, global step 16013: learning rate 0.0000
[2019-04-27 22:21:41,443] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1000, global step 16030: loss 0.0494
[2019-04-27 22:21:41,445] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1000, global step 16030: loss 0.1155
[2019-04-27 22:21:41,449] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1000, global step 16030: learning rate 0.0000
[2019-04-27 22:21:41,450] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1000, global step 16030: learning rate 0.0000
[2019-04-27 22:21:41,451] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1000, global step 16032: loss 0.0682
[2019-04-27 22:21:41,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1000, global step 16039: learning rate 0.0000
[2019-04-27 22:21:41,475] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1000, global step 16044: loss 0.0005
[2019-04-27 22:21:41,477] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1000, global step 16044: learning rate 0.0000
[2019-04-27 22:21:41,486] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1000, global step 16049: loss 0.0078
[2019-04-27 22:21:41,487] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1000, global step 16049: learning rate 0.0000
[2019-04-27 22:21:41,526] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1000, global step 16065: loss 0.0011
[2019-04-27 22:21:41,529] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1000, global step 16065: learning rate 0.0000
[2019-04-27 22:21:41,573] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1000, global step 16087: loss 0.3083
[2019-04-27 22:21:41,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1000, global step 16087: learning rate 0.0000
[2019-04-27 22:21:41,942] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1000, global step 16272: loss 0.9098
[2019-04-27 22:21:41,944] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1000, global step 16272: learning rate 0.0000
[2019-04-27 22:21:45,135] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6759373e-07 9.9999964e-01 1.8094332e-07 4.4449902e-10 1.4908334e-08], sum to 1.0000
[2019-04-27 22:21:45,146] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0432
[2019-04-27 22:21:45,302] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 68.0, 1.0, 2.0, 0.2483964544502939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 409016.1123429117, 409016.1123429111, 160739.05740801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 673200.0000, 
sim time next is 673800.0000, 
raw observation next is [21.88333333333333, 69.16666666666667, 1.0, 2.0, 0.2474632813174532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 407621.5253254219, 407621.5253254219, 160644.1875541037], 
processed observation next is [1.0, 0.8260869565217391, 0.2361769352290678, 0.6916666666666668, 1.0, 1.0, 0.0933292545993412, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11322820147928386, 0.11322820147928386, 0.23976744411060255], 
reward next is 0.7602, 
noisyNet noise sample is [array([-0.8370197], dtype=float32), 0.2397497]. 
=============================================
[2019-04-27 22:21:47,240] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.9512727e-07 9.9999976e-01 7.3356354e-08 4.9953860e-11 4.6712917e-10], sum to 1.0000
[2019-04-27 22:21:47,250] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6004
[2019-04-27 22:21:47,397] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.36666666666667, 88.0, 1.0, 2.0, 0.2197230140345048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 365581.3865665816, 365581.386566581, 157686.2080604718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 711600.0000, 
sim time next is 712200.0000, 
raw observation next is [18.58333333333334, 87.0, 1.0, 2.0, 0.221775458374074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368721.5013603071, 368721.5013603077, 157924.3054245826], 
processed observation next is [1.0, 0.21739130434782608, 0.07977883096366543, 0.87, 1.0, 1.0, 0.06238007033020963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10242263926675198, 0.10242263926675213, 0.23570791854415313], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.01822713], dtype=float32), 0.448]. 
=============================================
[2019-04-27 22:21:57,696] A3C_AGENT_WORKER-Thread-11 INFO:Local step 1500, global step 23785: loss 0.3684
[2019-04-27 22:21:57,698] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 1500, global step 23785: learning rate 0.0000
[2019-04-27 22:21:57,814] A3C_AGENT_WORKER-Thread-10 INFO:Local step 1500, global step 23850: loss 0.7039
[2019-04-27 22:21:57,817] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 1500, global step 23850: learning rate 0.0000
[2019-04-27 22:21:57,837] A3C_AGENT_WORKER-Thread-13 INFO:Local step 1500, global step 23857: loss 0.5075
[2019-04-27 22:21:57,838] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 1500, global step 23857: learning rate 0.0000
[2019-04-27 22:21:57,856] A3C_AGENT_WORKER-Thread-12 INFO:Local step 1500, global step 23863: loss 0.1695
[2019-04-27 22:21:57,857] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 1500, global step 23863: learning rate 0.0000
[2019-04-27 22:21:57,911] A3C_AGENT_WORKER-Thread-18 INFO:Local step 1500, global step 23892: loss 0.0034
[2019-04-27 22:21:57,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 1500, global step 23892: learning rate 0.0000
[2019-04-27 22:21:57,974] A3C_AGENT_WORKER-Thread-20 INFO:Local step 1500, global step 23927: loss 0.0082
[2019-04-27 22:21:57,979] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 1500, global step 23928: learning rate 0.0000
[2019-04-27 22:21:57,997] A3C_AGENT_WORKER-Thread-9 INFO:Local step 1500, global step 23938: loss 0.2533
[2019-04-27 22:21:58,001] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 1500, global step 23939: learning rate 0.0000
[2019-04-27 22:21:58,003] A3C_AGENT_WORKER-Thread-2 INFO:Local step 1500, global step 23939: loss 0.2540
[2019-04-27 22:21:58,007] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 1500, global step 23939: learning rate 0.0000
[2019-04-27 22:21:58,089] A3C_AGENT_WORKER-Thread-17 INFO:Local step 1500, global step 23981: loss 0.5353
[2019-04-27 22:21:58,093] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 1500, global step 23982: learning rate 0.0000
[2019-04-27 22:21:58,110] A3C_AGENT_WORKER-Thread-22 INFO:Local step 1500, global step 23992: loss 0.6758
[2019-04-27 22:21:58,113] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 1500, global step 23993: learning rate 0.0000
[2019-04-27 22:21:58,199] A3C_AGENT_WORKER-Thread-14 INFO:Local step 1500, global step 24028: loss 0.1219
[2019-04-27 22:21:58,201] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 1500, global step 24028: learning rate 0.0000
[2019-04-27 22:21:58,258] A3C_AGENT_WORKER-Thread-15 INFO:Local step 1500, global step 24058: loss 0.0024
[2019-04-27 22:21:58,262] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 1500, global step 24060: learning rate 0.0000
[2019-04-27 22:21:58,330] A3C_AGENT_WORKER-Thread-3 INFO:Local step 1500, global step 24095: loss 0.0718
[2019-04-27 22:21:58,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 1500, global step 24096: learning rate 0.0000
[2019-04-27 22:21:58,374] A3C_AGENT_WORKER-Thread-21 INFO:Local step 1500, global step 24111: loss 0.7529
[2019-04-27 22:21:58,378] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 1500, global step 24111: learning rate 0.0000
[2019-04-27 22:21:58,407] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1732945e-13 1.0000000e+00 8.1668074e-16 2.7271996e-18 2.8253234e-17], sum to 1.0000
[2019-04-27 22:21:58,417] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0330
[2019-04-27 22:21:58,422] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.76666666666667, 77.33333333333334, 1.0, 2.0, 0.2960186946083676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472269.7981640099, 472269.7981640099, 165086.349841958], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 904800.0000, 
sim time next is 905400.0000, 
raw observation next is [22.9, 76.5, 1.0, 2.0, 0.2965128329127142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 472918.5376668115, 472918.5376668122, 165130.0208093765], 
processed observation next is [0.0, 0.4782608695652174, 0.2843601895734597, 0.765, 1.0, 1.0, 0.15242509989483635, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1313662604630032, 0.1313662604630034, 0.24646271762593508], 
reward next is 0.7535, 
noisyNet noise sample is [array([0.33982965], dtype=float32), 0.85437477]. 
=============================================
[2019-04-27 22:21:58,517] A3C_AGENT_WORKER-Thread-16 INFO:Local step 1500, global step 24179: loss 0.3481
[2019-04-27 22:21:58,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 1500, global step 24179: learning rate 0.0000
[2019-04-27 22:21:58,806] A3C_AGENT_WORKER-Thread-19 INFO:Local step 1500, global step 24315: loss 0.3435
[2019-04-27 22:21:58,808] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 1500, global step 24316: learning rate 0.0000
[2019-04-27 22:22:00,244] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.5234310e-14 1.0000000e+00 2.7748298e-12 1.1313333e-17 4.4813347e-13], sum to 1.0000
[2019-04-27 22:22:00,253] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6763
[2019-04-27 22:22:00,255] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 22:22:00,255] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:22:00,257] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:22:00,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:22:00,257] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:22:00,258] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:22:00,259] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:22:00,261] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:22:00,263] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:22:00,264] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:22:00,264] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:22:00,269] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run2
[2019-04-27 22:22:00,269] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run2
[2019-04-27 22:22:00,318] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run2
[2019-04-27 22:22:00,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run2
[2019-04-27 22:22:00,343] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run2
[2019-04-27 22:22:00,601] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 85.33333333333334, 1.0, 2.0, 0.3345505630127777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 519477.9307774201, 519477.9307774206, 168288.1086678219], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 934800.0000, 
sim time next is 935400.0000, 
raw observation next is [22.76666666666667, 86.16666666666666, 1.0, 2.0, 0.3353269215834158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520066.7343381367, 520066.7343381373, 168316.2579304135], 
processed observation next is [0.0, 0.8260869565217391, 0.2780410742496052, 0.8616666666666666, 1.0, 1.0, 0.19918906214869375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14446298176059352, 0.1444629817605937, 0.25121829541852764], 
reward next is 0.7488, 
noisyNet noise sample is [array([-0.9336779], dtype=float32), -0.18834072]. 
=============================================
[2019-04-27 22:22:04,967] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:22:04,968] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.3, 75.0, 1.0, 2.0, 0.2592200509348497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425685.7886239318, 425685.7886239318, 161833.378254622]
[2019-04-27 22:22:04,970] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:22:04,974] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0198957e-12 1.0000000e+00 1.8821691e-12 1.5668526e-17 2.0878793e-15], sampled 0.6902362586090571
[2019-04-27 22:22:10,338] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:22:10,339] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.13333333333333, 53.66666666666667, 1.0, 2.0, 0.3006745135718655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484550.0038267861, 484550.0038267861, 165982.8027743507]
[2019-04-27 22:22:10,340] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:22:10,342] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.1540878e-13 1.0000000e+00 4.9344730e-13 2.3096492e-18 3.9054492e-16], sampled 0.7129637346025542
[2019-04-27 22:22:12,629] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:22:12,632] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.21702311, 61.59341714, 1.0, 2.0, 0.2830744643394749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458877.1364477484, 458877.1364477484, 164178.399894292]
[2019-04-27 22:22:12,634] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:22:12,637] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1715761e-12 1.0000000e+00 1.3437845e-12 9.6788362e-18 1.3699020e-15], sampled 0.05670731670071205
[2019-04-27 22:22:34,113] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:22:34,114] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.53333333333333, 93.50000000000001, 1.0, 2.0, 0.3785914467894154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575319.7690804836, 575319.7690804836, 172608.2004566886]
[2019-04-27 22:22:34,115] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:22:34,121] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.7047639e-13 1.0000000e+00 3.4148685e-13 1.3668672e-18 2.4699671e-16], sampled 0.33074856978376854
[2019-04-27 22:23:13,031] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:23:13,034] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.1, 80.66666666666667, 1.0, 2.0, 0.6060134591086744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 846868.7076386745, 846868.7076386738, 201871.4250107468]
[2019-04-27 22:23:13,036] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:23:13,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6716026e-13 1.0000000e+00 1.5719896e-13 4.5105797e-19 9.3599291e-17], sampled 0.7559219421790057
[2019-04-27 22:23:24,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:23:24,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.5, 81.0, 1.0, 2.0, 0.8679480224899567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1213116.162332437, 1213116.162332436, 261276.5560661483]
[2019-04-27 22:23:24,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:23:24,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.7273918e-14 1.0000000e+00 3.8345639e-14 6.0039690e-20 1.5983962e-17], sampled 0.7252718615668373
[2019-04-27 22:23:29,000] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.029886736]
[2019-04-27 22:23:29,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.45, 68.0, 1.0, 2.0, 0.8615348050083911, 1.0, 1.0, 0.8615348050083911, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2409603.998580009, 2409603.998580009, 450935.935444388]
[2019-04-27 22:23:29,002] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:23:29,006] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.5967443e-14 1.0000000e+00 1.4448813e-14 1.4967996e-20 4.7450980e-18], sampled 0.671210878253041
[2019-04-27 22:23:29,007] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2409603.998580009 W.
[2019-04-27 22:23:57,318] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:23:57,412] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7167 2779131942.0695 933.0000
[2019-04-27 22:23:57,556] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:23:57,705] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8618 2842433197.1644 1131.0000
[2019-04-27 22:23:57,949] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:23:58,965] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 25000, evaluation results [25000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.716715102415, 2779131942.069542, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.861828886107, 2842433197.16442, 1131.0]
[2019-04-27 22:24:06,149] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.6280009e-10 1.0000000e+00 1.2703383e-10 6.4381903e-15 3.2086469e-12], sum to 1.0000
[2019-04-27 22:24:06,165] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5819
[2019-04-27 22:24:06,309] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.36666666666667, 97.0, 1.0, 2.0, 0.3802825846655978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 572803.730547473, 572803.730547473, 172234.9270182879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1039800.0000, 
sim time next is 1040400.0000, 
raw observation next is [22.4, 97.0, 1.0, 2.0, 0.3809588480346077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573181.2049272379, 573181.2049272379, 172248.3978779776], 
processed observation next is [1.0, 0.043478260869565216, 0.2606635071090047, 0.97, 1.0, 1.0, 0.254167286788684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1592170013686772, 0.1592170013686772, 0.2570871610119069], 
reward next is 0.7429, 
noisyNet noise sample is [array([-1.0261383], dtype=float32), -1.0111628]. 
=============================================
[2019-04-27 22:24:14,803] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2000, global step 31763: loss 0.1041
[2019-04-27 22:24:14,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2000, global step 31764: learning rate 0.0000
[2019-04-27 22:24:15,025] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2000, global step 31859: loss 0.2763
[2019-04-27 22:24:15,030] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2000, global step 31862: learning rate 0.0000
[2019-04-27 22:24:15,053] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2000, global step 31875: loss 0.4338
[2019-04-27 22:24:15,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2000, global step 31875: learning rate 0.0000
[2019-04-27 22:24:15,066] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2000, global step 31879: loss 0.3027
[2019-04-27 22:24:15,068] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2000, global step 31879: learning rate 0.0000
[2019-04-27 22:24:15,112] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2000, global step 31901: loss 0.7962
[2019-04-27 22:24:15,117] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2000, global step 31901: learning rate 0.0000
[2019-04-27 22:24:15,117] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2000, global step 31903: loss 0.8850
[2019-04-27 22:24:15,123] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2000, global step 31904: learning rate 0.0000
[2019-04-27 22:24:15,156] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2000, global step 31917: loss 0.9978
[2019-04-27 22:24:15,159] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2000, global step 31917: learning rate 0.0000
[2019-04-27 22:24:15,194] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2000, global step 31932: loss 0.8435
[2019-04-27 22:24:15,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2000, global step 31934: learning rate 0.0000
[2019-04-27 22:24:15,286] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2000, global step 31975: loss 0.6606
[2019-04-27 22:24:15,290] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2000, global step 31975: learning rate 0.0000
[2019-04-27 22:24:15,357] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2000, global step 32000: loss 0.0862
[2019-04-27 22:24:15,359] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2000, global step 32000: learning rate 0.0000
[2019-04-27 22:24:15,366] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2000, global step 32003: loss 0.2261
[2019-04-27 22:24:15,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2000, global step 32003: learning rate 0.0000
[2019-04-27 22:24:15,502] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2000, global step 32059: loss 0.0291
[2019-04-27 22:24:15,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2000, global step 32060: learning rate 0.0000
[2019-04-27 22:24:15,529] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2000, global step 32076: loss 0.0429
[2019-04-27 22:24:15,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2000, global step 32076: learning rate 0.0000
[2019-04-27 22:24:15,671] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2000, global step 32129: loss 0.3343
[2019-04-27 22:24:15,677] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2000, global step 32130: learning rate 0.0000
[2019-04-27 22:24:15,742] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2000, global step 32161: loss 0.1867
[2019-04-27 22:24:15,744] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2000, global step 32163: learning rate 0.0000
[2019-04-27 22:24:16,187] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2000, global step 32356: loss 0.0685
[2019-04-27 22:24:16,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2000, global step 32356: learning rate 0.0000
[2019-04-27 22:24:20,088] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6049199e-10 1.0000000e+00 3.4205676e-12 3.4983329e-12 5.1763285e-12], sum to 1.0000
[2019-04-27 22:24:20,097] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8021
[2019-04-27 22:24:20,248] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 82.0, 1.0, 2.0, 0.478739987687154, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668955.3979834877, 668955.3979834877, 180331.9605838366], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1276200.0000, 
sim time next is 1276800.0000, 
raw observation next is [26.3, 82.66666666666667, 1.0, 2.0, 0.4766174885734988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665988.6470284841, 665988.6470284841, 180013.4500430102], 
processed observation next is [1.0, 0.782608695652174, 0.4454976303317536, 0.8266666666666667, 1.0, 1.0, 0.36941866093192627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18499684639680114, 0.18499684639680114, 0.26867679110897047], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.2380097], dtype=float32), 0.22704744]. 
=============================================
[2019-04-27 22:24:25,145] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.3039550e-18 1.0000000e+00 1.4724692e-21 1.1866946e-25 2.6768733e-23], sum to 1.0000
[2019-04-27 22:24:25,157] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2811
[2019-04-27 22:24:25,312] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333334, 91.33333333333334, 1.0, 2.0, 0.6037842607230055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962711.6791604629, 962711.6791604629, 214989.1359678286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1354800.0000, 
sim time next is 1355400.0000, 
raw observation next is [20.9, 91.5, 1.0, 2.0, 0.5898153121947395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940732.0345272601, 940732.0345272601, 212064.7947096643], 
processed observation next is [1.0, 0.6956521739130435, 0.1895734597156398, 0.915, 1.0, 1.0, 0.5058015809575174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26131445403535003, 0.26131445403535003, 0.3165146189696482], 
reward next is 0.6835, 
noisyNet noise sample is [array([0.24620225], dtype=float32), 0.5377401]. 
=============================================
[2019-04-27 22:24:33,630] A3C_AGENT_WORKER-Thread-11 INFO:Local step 2500, global step 39705: loss 0.0017
[2019-04-27 22:24:33,635] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 2500, global step 39706: learning rate 0.0000
[2019-04-27 22:24:33,870] A3C_AGENT_WORKER-Thread-18 INFO:Local step 2500, global step 39809: loss 0.0955
[2019-04-27 22:24:33,871] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 2500, global step 39810: learning rate 0.0000
[2019-04-27 22:24:33,881] A3C_AGENT_WORKER-Thread-13 INFO:Local step 2500, global step 39813: loss 0.1228
[2019-04-27 22:24:33,885] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 2500, global step 39813: learning rate 0.0000
[2019-04-27 22:24:33,950] A3C_AGENT_WORKER-Thread-2 INFO:Local step 2500, global step 39839: loss 0.0111
[2019-04-27 22:24:33,954] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 2500, global step 39842: learning rate 0.0000
[2019-04-27 22:24:34,045] A3C_AGENT_WORKER-Thread-9 INFO:Local step 2500, global step 39879: loss 0.2520
[2019-04-27 22:24:34,048] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 2500, global step 39880: learning rate 0.0000
[2019-04-27 22:24:34,108] A3C_AGENT_WORKER-Thread-12 INFO:Local step 2500, global step 39906: loss 0.7118
[2019-04-27 22:24:34,110] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 2500, global step 39906: learning rate 0.0000
[2019-04-27 22:24:34,142] A3C_AGENT_WORKER-Thread-20 INFO:Local step 2500, global step 39917: loss 1.0816
[2019-04-27 22:24:34,145] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 2500, global step 39917: learning rate 0.0000
[2019-04-27 22:24:34,297] A3C_AGENT_WORKER-Thread-10 INFO:Local step 2500, global step 39984: loss 0.0746
[2019-04-27 22:24:34,299] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 2500, global step 39984: learning rate 0.0000
[2019-04-27 22:24:34,435] A3C_AGENT_WORKER-Thread-22 INFO:Local step 2500, global step 40039: loss 0.0505
[2019-04-27 22:24:34,439] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 2500, global step 40040: learning rate 0.0000
[2019-04-27 22:24:34,451] A3C_AGENT_WORKER-Thread-15 INFO:Local step 2500, global step 40042: loss 0.2034
[2019-04-27 22:24:34,454] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 2500, global step 40043: learning rate 0.0000
[2019-04-27 22:24:34,487] A3C_AGENT_WORKER-Thread-14 INFO:Local step 2500, global step 40059: loss 0.1876
[2019-04-27 22:24:34,491] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 2500, global step 40059: learning rate 0.0000
[2019-04-27 22:24:34,545] A3C_AGENT_WORKER-Thread-17 INFO:Local step 2500, global step 40084: loss 0.5309
[2019-04-27 22:24:34,546] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 2500, global step 40085: learning rate 0.0000
[2019-04-27 22:24:34,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 2500, global step 40094: loss 0.4365
[2019-04-27 22:24:34,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 2500, global step 40094: learning rate 0.0000
[2019-04-27 22:24:34,615] A3C_AGENT_WORKER-Thread-3 INFO:Local step 2500, global step 40115: loss 0.1987
[2019-04-27 22:24:34,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 2500, global step 40115: learning rate 0.0000
[2019-04-27 22:24:34,775] A3C_AGENT_WORKER-Thread-21 INFO:Local step 2500, global step 40186: loss 0.0024
[2019-04-27 22:24:34,776] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 2500, global step 40186: learning rate 0.0000
[2019-04-27 22:24:35,245] A3C_AGENT_WORKER-Thread-19 INFO:Local step 2500, global step 40386: loss 0.3607
[2019-04-27 22:24:35,248] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 2500, global step 40386: learning rate 0.0000
[2019-04-27 22:24:37,968] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2013522e-13 1.0000000e+00 2.6076596e-17 1.4067436e-16 3.5175785e-18], sum to 1.0000
[2019-04-27 22:24:37,978] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1518
[2019-04-27 22:24:37,986] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 91.0, 1.0, 2.0, 0.3260588268832261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511414.5032452514, 511414.5032452508, 167800.4732867103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1560600.0000, 
sim time next is 1561200.0000, 
raw observation next is [21.7, 91.0, 1.0, 2.0, 0.3261569680534232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511568.5270110629, 511568.5270110629, 167812.3356408702], 
processed observation next is [1.0, 0.043478260869565216, 0.2274881516587678, 0.91, 1.0, 1.0, 0.18814092536557012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14210236861418413, 0.14210236861418413, 0.2504661725983137], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.469815], dtype=float32), -0.1566952]. 
=============================================
[2019-04-27 22:24:38,765] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4617496e-13 1.0000000e+00 1.3751528e-16 5.4096898e-20 1.3218649e-18], sum to 1.0000
[2019-04-27 22:24:38,776] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8697
[2019-04-27 22:24:38,787] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 90.0, 1.0, 2.0, 0.3265040407911278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513596.4148940176, 513596.4148940182, 168002.772027921], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1566000.0000, 
sim time next is 1566600.0000, 
raw observation next is [21.68333333333333, 90.0, 1.0, 2.0, 0.3742980431237734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 589032.2880080118, 589032.2880080125, 174214.6120921314], 
processed observation next is [1.0, 0.13043478260869565, 0.22669826224328585, 0.9, 1.0, 1.0, 0.2461422206310523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1636200800022255, 0.1636200800022257, 0.2600218090927334], 
reward next is 0.7400, 
noisyNet noise sample is [array([-0.13649364], dtype=float32), 1.9647799]. 
=============================================
[2019-04-27 22:24:39,212] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.3834506e-17 1.0000000e+00 3.6413113e-21 9.0625485e-24 1.7215436e-21], sum to 1.0000
[2019-04-27 22:24:39,219] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1440
[2019-04-27 22:24:39,226] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 88.0, 1.0, 2.0, 0.3212464011813297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502674.9943207598, 502674.9943207592, 167100.9012685133], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1576800.0000, 
sim time next is 1577400.0000, 
raw observation next is [22.3, 87.66666666666667, 1.0, 2.0, 0.3527663989668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551096.573161713, 551096.573161713, 170923.8904187363], 
processed observation next is [1.0, 0.2608695652173913, 0.25592417061611383, 0.8766666666666667, 1.0, 1.0, 0.22020048068292628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15308238143380914, 0.15308238143380914, 0.2551102842070691], 
reward next is 0.7449, 
noisyNet noise sample is [array([0.4566578], dtype=float32), -0.084724605]. 
=============================================
[2019-04-27 22:24:42,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4006347e-15 1.0000000e+00 2.7073675e-20 2.5841655e-21 2.9683018e-22], sum to 1.0000
[2019-04-27 22:24:42,437] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-27 22:24:42,592] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.16666666666667, 96.33333333333333, 1.0, 2.0, 0.4164400034122114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611138.6189723732, 611138.6189723732, 175287.062005289], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1632000.0000, 
sim time next is 1632600.0000, 
raw observation next is [23.15, 96.5, 1.0, 2.0, 0.4165286868239017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611177.3138799083, 611177.3138799083, 175288.096109286], 
processed observation next is [1.0, 0.9130434782608695, 0.2962085308056872, 0.965, 1.0, 1.0, 0.2970225142456647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1697714760777523, 0.1697714760777523, 0.26162402404371043], 
reward next is 0.7384, 
noisyNet noise sample is [array([-0.00618212], dtype=float32), 0.24879678]. 
=============================================
[2019-04-27 22:24:44,674] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.7807776e-16 1.0000000e+00 3.6685987e-20 9.8381065e-21 2.9948498e-21], sum to 1.0000
[2019-04-27 22:24:44,684] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-27 22:24:44,691] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 94.0, 1.0, 2.0, 0.9075780463729445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1281737.819993569, 1281737.819993569, 273997.0819213995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1674000.0000, 
sim time next is 1674600.0000, 
raw observation next is [24.53333333333333, 93.50000000000001, 1.0, 2.0, 0.8206010728133645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1155979.010444981, 1155979.01044498, 250321.3774954729], 
processed observation next is [1.0, 0.391304347826087, 0.36176935229067925, 0.9350000000000002, 1.0, 1.0, 0.7838567142329692, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3211052806791614, 0.32110528067916116, 0.37361399626189984], 
reward next is 0.6264, 
noisyNet noise sample is [array([0.6335075], dtype=float32), -0.024959885]. 
=============================================
[2019-04-27 22:24:50,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4736527e-19 1.0000000e+00 3.0392982e-28 6.6731272e-26 1.0287385e-27], sum to 1.0000
[2019-04-27 22:24:50,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9176
[2019-04-27 22:24:50,881] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 88.0, 1.0, 2.0, 0.8490209401339973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1241531.474424428, 1241531.474424428, 264079.1401714162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1764000.0000, 
sim time next is 1764600.0000, 
raw observation next is [24.21666666666667, 87.00000000000001, 1.0, 2.0, 0.8346412022571453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1228578.621944959, 1228578.621944959, 261279.0360663521], 
processed observation next is [1.0, 0.43478260869565216, 0.34676145339652464, 0.8700000000000001, 1.0, 1.0, 0.8007725328399341, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3412718394291553, 0.3412718394291553, 0.38996871054679416], 
reward next is 0.6100, 
noisyNet noise sample is [array([0.82047284], dtype=float32), -0.8272541]. 
=============================================
[2019-04-27 22:24:52,473] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3000, global step 47692: loss 0.0454
[2019-04-27 22:24:52,476] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3000, global step 47692: learning rate 0.0000
[2019-04-27 22:24:52,603] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3000, global step 47745: loss 0.0358
[2019-04-27 22:24:52,607] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3000, global step 47747: learning rate 0.0000
[2019-04-27 22:24:52,715] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3000, global step 47795: loss 0.4169
[2019-04-27 22:24:52,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3000, global step 47796: learning rate 0.0000
[2019-04-27 22:24:52,722] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3000, global step 47797: loss 0.4790
[2019-04-27 22:24:52,727] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3000, global step 47797: learning rate 0.0000
[2019-04-27 22:24:52,844] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3000, global step 47848: loss 0.0379
[2019-04-27 22:24:52,847] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3000, global step 47848: learning rate 0.0000
[2019-04-27 22:24:53,007] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3000, global step 47914: loss 0.3165
[2019-04-27 22:24:53,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3000, global step 47915: learning rate 0.0000
[2019-04-27 22:24:53,125] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3000, global step 47965: loss 0.5628
[2019-04-27 22:24:53,127] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3000, global step 47967: learning rate 0.0000
[2019-04-27 22:24:53,165] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3000, global step 47983: loss 0.3124
[2019-04-27 22:24:53,168] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3000, global step 47983: learning rate 0.0000
[2019-04-27 22:24:53,315] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3000, global step 48045: loss 0.0559
[2019-04-27 22:24:53,317] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3000, global step 48045: learning rate 0.0000
[2019-04-27 22:24:53,365] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3000, global step 48068: loss 0.1040
[2019-04-27 22:24:53,369] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3000, global step 48068: learning rate 0.0000
[2019-04-27 22:24:53,417] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3000, global step 48093: loss 0.1725
[2019-04-27 22:24:53,421] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3000, global step 48093: learning rate 0.0000
[2019-04-27 22:24:53,423] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3000, global step 48093: loss 0.1919
[2019-04-27 22:24:53,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3000, global step 48094: learning rate 0.0000
[2019-04-27 22:24:53,467] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3000, global step 48111: loss 0.1883
[2019-04-27 22:24:53,470] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3000, global step 48111: learning rate 0.0000
[2019-04-27 22:24:53,524] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3000, global step 48133: loss 0.0651
[2019-04-27 22:24:53,527] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3000, global step 48133: learning rate 0.0000
[2019-04-27 22:24:53,627] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3000, global step 48177: loss 0.0589
[2019-04-27 22:24:53,631] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3000, global step 48178: learning rate 0.0000
[2019-04-27 22:24:54,071] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3000, global step 48367: loss 0.1230
[2019-04-27 22:24:54,072] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3000, global step 48367: learning rate 0.0000
[2019-04-27 22:24:57,860] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 22:24:57,861] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:24:57,862] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:24:57,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:24:57,863] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:24:57,863] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:24:57,863] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:24:57,866] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:24:57,864] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:24:57,867] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:24:57,869] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:24:57,886] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run3
[2019-04-27 22:24:57,888] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run3
[2019-04-27 22:24:57,911] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run3
[2019-04-27 22:24:57,937] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run3
[2019-04-27 22:24:57,992] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run3
[2019-04-27 22:26:11,835] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.05154472]
[2019-04-27 22:26:11,837] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.4, 45.0, 1.0, 2.0, 0.5955230855523637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 832203.2751005228, 832203.2751005221, 199914.5105976886]
[2019-04-27 22:26:11,837] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:26:11,839] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8112873e-10 1.0000000e+00 9.2345413e-13 3.2628810e-13 3.6679262e-13], sampled 0.9067823733972913
[2019-04-27 22:26:38,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.05154472]
[2019-04-27 22:26:38,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.86666666666667, 76.0, 1.0, 2.0, 0.7165433914054832, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1001400.705735501, 1001400.705735501, 224456.4286251475]
[2019-04-27 22:26:38,503] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:26:38,508] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4053722e-10 1.0000000e+00 3.8315171e-13 1.3142008e-13 1.4832374e-13], sampled 0.07218180898793902
[2019-04-27 22:27:05,782] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.05154472]
[2019-04-27 22:27:05,783] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.3, 81.0, 1.0, 2.0, 0.8895568457675237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104092, 1243336.130448786, 1243336.130448786, 267084.1449024701]
[2019-04-27 22:27:05,786] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:27:05,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4671425e-10 1.0000000e+00 4.0543054e-13 1.3907068e-13 1.5633035e-13], sampled 0.8705902687988043
[2019-04-27 22:27:07,886] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.6903 2779149183.8826 933.0000
[2019-04-27 22:27:08,154] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:27:08,522] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:27:08,637] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2953 3007596012.4260 1766.0000
[2019-04-27 22:27:08,730] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1761 3163990863.4184 1778.0000
[2019-04-27 22:27:09,747] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 50000, evaluation results [50000.0, 7884.1761076780385, 3163990863.418371, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8660.69031035388, 2779149183.882583, 933.0, 7998.295259250187, 3007596012.4259605, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:27:17,832] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.2064564e-19 1.0000000e+00 1.1371927e-23 3.7130127e-23 1.0785305e-23], sum to 1.0000
[2019-04-27 22:27:17,848] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3714
[2019-04-27 22:27:17,855] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 95.16666666666667, 1.0, 2.0, 0.4854643778384652, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 678354.5571582321, 678354.5571582327, 181350.1802974536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2011800.0000, 
sim time next is 2012400.0000, 
raw observation next is [25.0, 95.0, 1.0, 2.0, 0.4874043180397513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 681066.1652401134, 681066.165240114, 181646.3651873764], 
processed observation next is [0.0, 0.30434782608695654, 0.38388625592417064, 0.95, 1.0, 1.0, 0.3824148410117486, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1891850459000315, 0.18918504590003166, 0.2711139778916066], 
reward next is 0.7289, 
noisyNet noise sample is [array([-0.2184606], dtype=float32), -1.088625]. 
=============================================
[2019-04-27 22:27:22,976] A3C_AGENT_WORKER-Thread-11 INFO:Local step 3500, global step 55685: loss 0.0304
[2019-04-27 22:27:22,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 3500, global step 55686: learning rate 0.0000
[2019-04-27 22:27:23,149] A3C_AGENT_WORKER-Thread-2 INFO:Local step 3500, global step 55760: loss 0.0662
[2019-04-27 22:27:23,150] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 3500, global step 55760: learning rate 0.0000
[2019-04-27 22:27:23,163] A3C_AGENT_WORKER-Thread-18 INFO:Local step 3500, global step 55766: loss 0.1264
[2019-04-27 22:27:23,164] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 3500, global step 55766: learning rate 0.0000
[2019-04-27 22:27:23,264] A3C_AGENT_WORKER-Thread-13 INFO:Local step 3500, global step 55805: loss 0.3761
[2019-04-27 22:27:23,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 3500, global step 55805: learning rate 0.0000
[2019-04-27 22:27:23,338] A3C_AGENT_WORKER-Thread-9 INFO:Local step 3500, global step 55838: loss 0.3744
[2019-04-27 22:27:23,340] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 3500, global step 55839: learning rate 0.0000
[2019-04-27 22:27:23,410] A3C_AGENT_WORKER-Thread-20 INFO:Local step 3500, global step 55865: loss 0.0577
[2019-04-27 22:27:23,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 3500, global step 55865: learning rate 0.0000
[2019-04-27 22:27:23,745] A3C_AGENT_WORKER-Thread-14 INFO:Local step 3500, global step 56010: loss 0.3310
[2019-04-27 22:27:23,748] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 3500, global step 56010: learning rate 0.0000
[2019-04-27 22:27:23,767] A3C_AGENT_WORKER-Thread-10 INFO:Local step 3500, global step 56017: loss 0.1156
[2019-04-27 22:27:23,771] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 3500, global step 56018: learning rate 0.0000
[2019-04-27 22:27:23,817] A3C_AGENT_WORKER-Thread-15 INFO:Local step 3500, global step 56036: loss 0.0238
[2019-04-27 22:27:23,820] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 3500, global step 56036: learning rate 0.0000
[2019-04-27 22:27:23,896] A3C_AGENT_WORKER-Thread-22 INFO:Local step 3500, global step 56071: loss 0.0003
[2019-04-27 22:27:23,899] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 3500, global step 56072: learning rate 0.0000
[2019-04-27 22:27:23,910] A3C_AGENT_WORKER-Thread-12 INFO:Local step 3500, global step 56076: loss 0.0490
[2019-04-27 22:27:23,913] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 3500, global step 56076: learning rate 0.0000
[2019-04-27 22:27:23,982] A3C_AGENT_WORKER-Thread-21 INFO:Local step 3500, global step 56106: loss 0.2453
[2019-04-27 22:27:23,986] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 3500, global step 56106: learning rate 0.0000
[2019-04-27 22:27:24,000] A3C_AGENT_WORKER-Thread-3 INFO:Local step 3500, global step 56111: loss 0.4033
[2019-04-27 22:27:24,001] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 3500, global step 56112: learning rate 0.0000
[2019-04-27 22:27:24,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 3500, global step 56117: loss 0.4068
[2019-04-27 22:27:24,022] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 3500, global step 56117: learning rate 0.0000
[2019-04-27 22:27:24,240] A3C_AGENT_WORKER-Thread-16 INFO:Local step 3500, global step 56215: loss 0.0051
[2019-04-27 22:27:24,245] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 3500, global step 56215: learning rate 0.0000
[2019-04-27 22:27:24,514] A3C_AGENT_WORKER-Thread-19 INFO:Local step 3500, global step 56335: loss 0.0150
[2019-04-27 22:27:24,515] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 3500, global step 56335: learning rate 0.0000
[2019-04-27 22:27:30,014] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5515288e-10 1.0000000e+00 1.2206260e-11 1.3635123e-12 5.6198375e-12], sum to 1.0000
[2019-04-27 22:27:30,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6750
[2019-04-27 22:27:30,033] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2172992.329359962 W.
[2019-04-27 22:27:30,037] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.8, 75.0, 1.0, 2.0, 0.9128280173662581, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997613433625435, 6.9112, 168.9123712120371, 2172992.329359962, 2111687.877851328, 437955.9011384965], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2197200.0000, 
sim time next is 2197800.0000, 
raw observation next is [29.9, 74.5, 1.0, 2.0, 0.9543352494407801, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997898798112523, 6.9112, 168.9123706272609, 2231087.560341526, 2169580.6623587, 449879.2249470839], 
processed observation next is [1.0, 0.43478260869565216, 0.6161137440758293, 0.745, 1.0, 1.0, 0.9449822282419037, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00866987981125229, 0.0, 0.8294370681977191, 0.6197465445393128, 0.6026612950996388, 0.671461529771767], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.32017475], dtype=float32), -2.2938507]. 
=============================================
[2019-04-27 22:27:34,408] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2119273e-17 1.0000000e+00 1.8326615e-24 5.2424004e-24 5.9288659e-26], sum to 1.0000
[2019-04-27 22:27:34,419] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4864
[2019-04-27 22:27:34,428] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1939735.734566016 W.
[2019-04-27 22:27:34,435] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.6, 70.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.595772099824298, 6.9112, 168.9091801916553, 1939735.734566016, 1454087.583479727, 311349.7706229984], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2278200.0000, 
sim time next is 2278800.0000, 
raw observation next is [29.8, 70.0, 1.0, 2.0, 0.6632111398665066, 1.0, 1.0, 0.6632111398665066, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1854461.983457253, 1854461.983457253, 358770.2476391255], 
processed observation next is [1.0, 0.391304347826087, 0.6113744075829385, 0.7, 1.0, 1.0, 0.5942302889957911, 1.0, 0.5, 0.5942302889957911, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5151283287381259, 0.5151283287381259, 0.5354779815509336], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46137217], dtype=float32), -0.3169259]. 
=============================================
[2019-04-27 22:27:35,033] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.5796304e-14 1.0000000e+00 1.7408950e-19 8.2956604e-18 5.4705978e-20], sum to 1.0000
[2019-04-27 22:27:35,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0007
[2019-04-27 22:27:35,053] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1940168.88227819 W.
[2019-04-27 22:27:35,058] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.7, 63.0, 1.0, 2.0, 0.7464772291782619, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.992760865780296, 6.9112, 168.9124714803414, 1940168.88227819, 1882306.963135079, 395550.077715416], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2284800.0000, 
sim time next is 2285400.0000, 
raw observation next is [31.9, 62.5, 1.0, 2.0, 0.8044740006466403, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993012440722895, 6.9112, 168.9124036082292, 2021334.973460241, 1963294.602228489, 409437.3504063888], 
processed observation next is [1.0, 0.43478260869565216, 0.7109004739336492, 0.625, 1.0, 1.0, 0.7644265068031811, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008181244072289484, 0.0, 0.8294372301493721, 0.5614819370722892, 0.5453596117301358, 0.6111005229946102], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.6166444], dtype=float32), -1.9852773]. 
=============================================
[2019-04-27 22:27:37,986] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7885799e-13 1.0000000e+00 1.5428963e-17 6.3342661e-15 2.4605135e-18], sum to 1.0000
[2019-04-27 22:27:37,991] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0163
[2019-04-27 22:27:38,147] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 80.66666666666667, 1.0, 2.0, 0.553764987147905, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773827.8973694084, 773827.8973694084, 192448.6338663476], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2328000.0000, 
sim time next is 2328600.0000, 
raw observation next is [28.68333333333333, 80.83333333333333, 1.0, 2.0, 0.5521788851898377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 771610.6828985944, 771610.682898595, 192175.1877840316], 
processed observation next is [1.0, 0.9565217391304348, 0.5584518167456555, 0.8083333333333332, 1.0, 1.0, 0.4604564881805273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2143363008051651, 0.21433630080516528, 0.2868286384836292], 
reward next is 0.7132, 
noisyNet noise sample is [array([1.1579155], dtype=float32), -0.8455489]. 
=============================================
[2019-04-27 22:27:41,876] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4000, global step 63702: loss 6.0505
[2019-04-27 22:27:41,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4000, global step 63703: learning rate 0.0000
[2019-04-27 22:27:41,949] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4000, global step 63728: loss 6.4888
[2019-04-27 22:27:41,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4000, global step 63728: learning rate 0.0000
[2019-04-27 22:27:42,099] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4000, global step 63794: loss 4.3969
[2019-04-27 22:27:42,103] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4000, global step 63794: learning rate 0.0000
[2019-04-27 22:27:42,124] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4000, global step 63803: loss 8.0275
[2019-04-27 22:27:42,125] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4000, global step 63803: learning rate 0.0000
[2019-04-27 22:27:42,210] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4000, global step 63844: loss 5.2479
[2019-04-27 22:27:42,212] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4000, global step 63844: learning rate 0.0000
[2019-04-27 22:27:42,219] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4000, global step 63847: loss 4.2234
[2019-04-27 22:27:42,220] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4000, global step 63847: learning rate 0.0000
[2019-04-27 22:27:42,452] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1853656e-10 1.0000000e+00 8.1813151e-14 5.5641675e-10 2.0079714e-14], sum to 1.0000
[2019-04-27 22:27:42,457] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9871
[2019-04-27 22:27:42,462] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.81666666666667, 74.33333333333334, 1.0, 2.0, 0.5856821183125834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818445.8898779831, 818445.8898779831, 198109.0939281125], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2405400.0000, 
sim time next is 2406000.0000, 
raw observation next is [30.73333333333333, 74.66666666666667, 1.0, 2.0, 0.5842371234322484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 816425.8441769247, 816425.8441769254, 197846.3589911139], 
processed observation next is [1.0, 0.8695652173913043, 0.6556082148499209, 0.7466666666666667, 1.0, 1.0, 0.49908087160511855, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22678495671581242, 0.2267849567158126, 0.2952930731210655], 
reward next is 0.7047, 
noisyNet noise sample is [array([0.49958017], dtype=float32), -0.852074]. 
=============================================
[2019-04-27 22:27:42,480] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[32.86306 ]
 [33.12343 ]
 [33.232563]
 [33.626007]
 [34.09133 ]], R is [[32.72802734]
 [33.10506439]
 [33.478405  ]
 [33.8486557 ]
 [34.21622849]].
[2019-04-27 22:27:42,609] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4000, global step 64008: loss 7.7083
[2019-04-27 22:27:42,613] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4000, global step 64009: learning rate 0.0000
[2019-04-27 22:27:42,655] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4000, global step 64026: loss 5.1918
[2019-04-27 22:27:42,658] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4000, global step 64026: learning rate 0.0000
[2019-04-27 22:27:42,763] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4000, global step 64072: loss 4.8498
[2019-04-27 22:27:42,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4000, global step 64073: learning rate 0.0000
[2019-04-27 22:27:42,814] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4000, global step 64093: loss 4.6972
[2019-04-27 22:27:42,818] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4000, global step 64093: learning rate 0.0000
[2019-04-27 22:27:42,836] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4000, global step 64101: loss 0.6930
[2019-04-27 22:27:42,837] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4000, global step 64101: learning rate 0.0000
[2019-04-27 22:27:42,887] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4000, global step 64124: loss 2.0864
[2019-04-27 22:27:42,889] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4000, global step 64124: learning rate 0.0000
[2019-04-27 22:27:42,929] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4000, global step 64146: loss 2.0602
[2019-04-27 22:27:42,931] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4000, global step 64147: learning rate 0.0000
[2019-04-27 22:27:42,968] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4000, global step 64162: loss 2.4630
[2019-04-27 22:27:42,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4000, global step 64162: learning rate 0.0000
[2019-04-27 22:27:43,068] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4000, global step 64197: loss 2.0013
[2019-04-27 22:27:43,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4000, global step 64197: learning rate 0.0000
[2019-04-27 22:27:43,375] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4000, global step 64333: loss 1.8745
[2019-04-27 22:27:43,378] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4000, global step 64333: learning rate 0.0000
[2019-04-27 22:27:50,940] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.1740447e-13 1.0000000e+00 6.7302008e-22 5.8759390e-16 3.6001621e-23], sum to 1.0000
[2019-04-27 22:27:50,951] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5999
[2019-04-27 22:27:50,962] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1987915.571882851 W.
[2019-04-27 22:27:50,969] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.45, 86.5, 1.0, 2.0, 0.7108939030975842, 1.0, 1.0, 0.7108939030975842, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1987915.571882851, 1987915.571882851, 378821.991339915], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2539800.0000, 
sim time next is 2540400.0000, 
raw observation next is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.8567930310013762, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98981535015186, 6.9112, 168.9124892993235, 2094560.4302822, 2038788.149418788, 422957.1260188213], 
processed observation next is [1.0, 0.391304347826087, 0.505529225908373, 0.8566666666666667, 1.0, 1.0, 0.8274614831341881, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007861535015185961, 0.0, 0.8294376509318747, 0.5818223417450555, 0.5663300415052189, 0.6312792925654049], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5064928], dtype=float32), 0.8051507]. 
=============================================
[2019-04-27 22:27:57,932] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6858964e-14 1.0000000e+00 3.2072339e-23 3.5369462e-15 3.0391321e-23], sum to 1.0000
[2019-04-27 22:27:57,948] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2972
[2019-04-27 22:27:57,954] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333333, 89.0, 1.0, 2.0, 0.482814047242684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674649.991339704, 674649.9913397034, 180946.5945343779], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2655600.0000, 
sim time next is 2656200.0000, 
raw observation next is [25.16666666666667, 89.0, 1.0, 2.0, 0.4750088158621437, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666012.16823512, 666012.16823512, 180066.1552623873], 
processed observation next is [0.0, 0.7391304347826086, 0.39178515007898923, 0.89, 1.0, 1.0, 0.3674805010387273, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1850033800653111, 0.1850033800653111, 0.26875545561550346], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.5228378], dtype=float32), 1.044108]. 
=============================================
[2019-04-27 22:28:00,376] A3C_AGENT_WORKER-Thread-11 INFO:Local step 4500, global step 71633: loss 0.4995
[2019-04-27 22:28:00,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 4500, global step 71633: learning rate 0.0000
[2019-04-27 22:28:00,463] A3C_AGENT_WORKER-Thread-2 INFO:Local step 4500, global step 71666: loss 0.5363
[2019-04-27 22:28:00,468] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 4500, global step 71667: learning rate 0.0000
[2019-04-27 22:28:00,662] A3C_AGENT_WORKER-Thread-9 INFO:Local step 4500, global step 71747: loss 0.1407
[2019-04-27 22:28:00,665] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 4500, global step 71748: learning rate 0.0000
[2019-04-27 22:28:00,692] A3C_AGENT_WORKER-Thread-13 INFO:Local step 4500, global step 71764: loss 0.1893
[2019-04-27 22:28:00,696] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 4500, global step 71764: learning rate 0.0000
[2019-04-27 22:28:00,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 4500, global step 71772: loss 0.0960
[2019-04-27 22:28:00,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 4500, global step 71773: learning rate 0.0000
[2019-04-27 22:28:00,943] A3C_AGENT_WORKER-Thread-20 INFO:Local step 4500, global step 71872: loss 0.0118
[2019-04-27 22:28:00,944] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 4500, global step 71873: learning rate 0.0000
[2019-04-27 22:28:01,163] A3C_AGENT_WORKER-Thread-21 INFO:Local step 4500, global step 71960: loss 0.0269
[2019-04-27 22:28:01,164] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 4500, global step 71961: learning rate 0.0000
[2019-04-27 22:28:01,336] A3C_AGENT_WORKER-Thread-14 INFO:Local step 4500, global step 72036: loss 0.0696
[2019-04-27 22:28:01,344] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 4500, global step 72037: learning rate 0.0000
[2019-04-27 22:28:01,384] A3C_AGENT_WORKER-Thread-15 INFO:Local step 4500, global step 72055: loss 0.0256
[2019-04-27 22:28:01,387] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 4500, global step 72057: learning rate 0.0000
[2019-04-27 22:28:01,436] A3C_AGENT_WORKER-Thread-3 INFO:Local step 4500, global step 72081: loss 0.0147
[2019-04-27 22:28:01,439] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 4500, global step 72081: learning rate 0.0000
[2019-04-27 22:28:01,518] A3C_AGENT_WORKER-Thread-17 INFO:Local step 4500, global step 72111: loss 0.0044
[2019-04-27 22:28:01,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 4500, global step 72111: learning rate 0.0000
[2019-04-27 22:28:01,589] A3C_AGENT_WORKER-Thread-12 INFO:Local step 4500, global step 72142: loss 0.0369
[2019-04-27 22:28:01,595] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 4500, global step 72142: learning rate 0.0000
[2019-04-27 22:28:01,667] A3C_AGENT_WORKER-Thread-10 INFO:Local step 4500, global step 72175: loss 0.2074
[2019-04-27 22:28:01,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 4500, global step 72175: learning rate 0.0000
[2019-04-27 22:28:01,700] A3C_AGENT_WORKER-Thread-22 INFO:Local step 4500, global step 72188: loss 0.1187
[2019-04-27 22:28:01,702] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 4500, global step 72188: learning rate 0.0000
[2019-04-27 22:28:01,840] A3C_AGENT_WORKER-Thread-16 INFO:Local step 4500, global step 72252: loss 0.0098
[2019-04-27 22:28:01,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 4500, global step 72252: learning rate 0.0000
[2019-04-27 22:28:02,024] A3C_AGENT_WORKER-Thread-19 INFO:Local step 4500, global step 72336: loss 0.3539
[2019-04-27 22:28:02,027] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 4500, global step 72337: learning rate 0.0000
[2019-04-27 22:28:08,199] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 22:28:08,202] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:28:08,202] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:28:08,203] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:28:08,204] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:28:08,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:28:08,207] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:28:08,208] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:28:08,209] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:28:08,210] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:28:08,211] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:28:08,220] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run4
[2019-04-27 22:28:08,257] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run4
[2019-04-27 22:28:08,259] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run4
[2019-04-27 22:28:08,306] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run4
[2019-04-27 22:28:08,330] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run4
[2019-04-27 22:28:12,652] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:28:12,654] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.0, 55.00000000000001, 1.0, 2.0, 0.2209031874462655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 367764.3892049208, 367764.3892049214, 157738.7251265381]
[2019-04-27 22:28:12,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:28:12,659] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1644573e-13 1.0000000e+00 1.0282629e-21 1.9884881e-14 3.8102413e-22], sampled 0.6784765929020026
[2019-04-27 22:28:16,365] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:28:16,369] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.16666666666667, 90.66666666666667, 1.0, 2.0, 0.2760389075105141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447730.8383659978, 447730.8383659978, 163432.4422000369]
[2019-04-27 22:28:16,370] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:28:16,376] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6913225e-14 1.0000000e+00 9.4542058e-23 4.1988821e-15 3.3434027e-23], sampled 0.20472210982541528
[2019-04-27 22:28:48,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:28:48,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.6, 78.0, 1.0, 2.0, 0.346975192843594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542139.4029648753, 542139.4029648753, 170189.640959426]
[2019-04-27 22:28:48,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:28:48,321] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0815421e-14 1.0000000e+00 6.2099484e-23 3.1943531e-15 2.1780847e-23], sampled 0.3073907618720303
[2019-04-27 22:28:48,581] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:28:48,582] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.340435935, 78.33856764999999, 1.0, 2.0, 0.5379989645267347, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751788.7547028683, 751788.754702869, 189763.0529501319]
[2019-04-27 22:28:48,583] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:28:48,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.54041700e-14 1.00000000e+00 3.79987545e-23 2.32056831e-15
 1.32058645e-23], sampled 0.7948983253242103
[2019-04-27 22:28:59,736] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:28:59,739] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.9353257, 92.83878997, 1.0, 2.0, 0.5112067441794412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714337.2651379352, 714337.2651379359, 185370.9315299765]
[2019-04-27 22:28:59,740] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:28:59,744] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.18084039e-15 1.00000000e+00 1.09570705e-23 1.03160566e-15
 3.71585909e-24], sampled 0.6051882771596833
[2019-04-27 22:29:00,543] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:00,546] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.28790432, 89.56977940499999, 1.0, 2.0, 0.9073966513875497, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1268285.770565472, 1268285.770565472, 271981.4623861819]
[2019-04-27 22:29:00,549] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:29:00,552] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9588098e-14 1.0000000e+00 5.6145881e-23 2.9913650e-15 1.9646229e-23], sampled 0.5765347671468137
[2019-04-27 22:29:19,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:19,236] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.7, 43.0, 1.0, 2.0, 0.9452455424723649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1321220.811966497, 1321220.811966498, 282687.9171014322]
[2019-04-27 22:29:19,237] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:29:19,239] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9607373e-14 1.0000000e+00 1.0995305e-22 4.6325337e-15 3.8969675e-23], sampled 0.9415157274424811
[2019-04-27 22:29:23,700] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:23,700] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.20246698333333, 79.62922178333334, 1.0, 2.0, 0.5260074026616244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735026.1937125725, 735026.1937125725, 187769.1082049695]
[2019-04-27 22:29:23,701] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:29:23,704] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0017110e-14 1.0000000e+00 1.8839328e-23 1.4690626e-15 6.4583064e-24], sampled 0.27346057770359455
[2019-04-27 22:29:30,821] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:30,823] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.63248955166667, 86.76109764666667, 1.0, 2.0, 0.5416382460113699, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756876.0240643956, 756876.0240643962, 190377.5058728419]
[2019-04-27 22:29:30,827] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:29:30,830] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.7955664e-15 1.0000000e+00 1.5241313e-23 1.2793415e-15 5.2026753e-24], sampled 0.7317954966890846
[2019-04-27 22:29:39,517] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:39,519] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.1, 58.5, 1.0, 2.0, 0.6055551717698311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 846228.0225011668, 846228.0225011668, 201784.6988459735]
[2019-04-27 22:29:39,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:29:39,527] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.7605409e-15 1.0000000e+00 2.3007079e-24 3.7299766e-16 7.5814325e-25], sampled 0.4442513557135708
[2019-04-27 22:29:51,708] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:29:51,712] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.92081173, 64.58091172333334, 1.0, 2.0, 0.4980327353906827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695922.4688932078, 695922.4688932085, 183286.3274613515]
[2019-04-27 22:29:51,713] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:29:51,716] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5662695e-14 1.0000000e+00 3.9012513e-23 2.3599460e-15 1.3565009e-23], sampled 0.10051736861639904
[2019-04-27 22:30:11,871] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.061933134]
[2019-04-27 22:30:11,873] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.75, 86.0, 1.0, 2.0, 0.5197412662897591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726267.0985699301, 726267.0985699301, 186746.3974021032]
[2019-04-27 22:30:11,874] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:30:11,877] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5773027e-14 1.0000000e+00 1.5015332e-22 5.6726521e-15 5.3598895e-23], sampled 0.8401693306641779
[2019-04-27 22:30:18,897] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:30:19,068] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:30:19,299] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:30:19,300] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:30:19,436] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:30:20,455] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 75000, evaluation results [75000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:30:21,352] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.0844925e-14 1.0000000e+00 1.3068489e-25 1.7780057e-16 2.9303030e-26], sum to 1.0000
[2019-04-27 22:30:21,363] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2981
[2019-04-27 22:30:21,374] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666666, 90.66666666666667, 1.0, 2.0, 0.4064164472869222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600874.1257394182, 600874.1257394182, 174456.3596540048], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2838000.0000, 
sim time next is 2838600.0000, 
raw observation next is [23.5, 91.5, 1.0, 2.0, 0.402772868211731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596731.3529397737, 596731.3529397737, 174111.2858917953], 
processed observation next is [1.0, 0.8695652173913043, 0.31279620853080575, 0.915, 1.0, 1.0, 0.2804492388093145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16575870914993715, 0.16575870914993715, 0.25986759088327654], 
reward next is 0.7401, 
noisyNet noise sample is [array([1.3165913], dtype=float32), 0.84561515]. 
=============================================
[2019-04-27 22:30:28,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2036591e-15 1.0000000e+00 1.4001735e-25 4.6211471e-16 1.9460555e-25], sum to 1.0000
[2019-04-27 22:30:28,753] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8918
[2019-04-27 22:30:28,765] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3103552240950783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490769.21008797, 490769.21008797, 166340.4255789926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2953200.0000, 
sim time next is 2953800.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3112988517276675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 492264.0480448005, 492264.0480448011, 166450.5901697146], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17023958039478015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13674001334577793, 0.1367400133457781, 0.2484337166712158], 
reward next is 0.7516, 
noisyNet noise sample is [array([0.1712486], dtype=float32), -0.7006567]. 
=============================================
[2019-04-27 22:30:30,682] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2168503e-13 1.0000000e+00 2.8071404e-21 9.2668877e-16 3.3758026e-22], sum to 1.0000
[2019-04-27 22:30:30,693] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5009
[2019-04-27 22:30:30,845] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.590560166453826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 917938.325498177, 917938.3254981777, 210056.5198575559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2971800.0000, 
sim time next is 2972400.0000, 
raw observation next is [22.0, 90.0, 1.0, 2.0, 0.5545332603137373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864787.5775435641, 864787.5775435634, 203176.4031333039], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9, 1.0, 1.0, 0.4632930847153461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24021877153987892, 0.24021877153987872, 0.3032483628855282], 
reward next is 0.6968, 
noisyNet noise sample is [array([-1.4525245], dtype=float32), -0.004206773]. 
=============================================
[2019-04-27 22:30:31,148] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1019943e-16 1.0000000e+00 1.8228626e-27 1.1885155e-18 3.9602702e-30], sum to 1.0000
[2019-04-27 22:30:31,159] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1437
[2019-04-27 22:30:31,166] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.5507602230073979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852111.9518531389, 852111.9518531389, 201769.2983555433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2968200.0000, 
sim time next is 2968800.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.5597298578214112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864746.8019493389, 864746.8019493383, 203365.8018382049], 
processed observation next is [1.0, 0.34782608695652173, 0.22590837282780438, 0.96, 1.0, 1.0, 0.4695540455679652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24020744498592747, 0.2402074449859273, 0.30353104751970883], 
reward next is 0.6965, 
noisyNet noise sample is [array([0.6048186], dtype=float32), 0.71017075]. 
=============================================
[2019-04-27 22:30:31,432] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5000, global step 79624: loss 0.0250
[2019-04-27 22:30:31,436] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5000, global step 79625: learning rate 0.0000
[2019-04-27 22:30:31,582] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5000, global step 79647: loss 0.1925
[2019-04-27 22:30:31,583] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5000, global step 79647: learning rate 0.0000
[2019-04-27 22:30:31,713] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5000, global step 79661: loss 0.1890
[2019-04-27 22:30:31,722] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5000, global step 79662: learning rate 0.0000
[2019-04-27 22:30:32,034] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5000, global step 79762: loss 0.1422
[2019-04-27 22:30:32,035] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5000, global step 79762: learning rate 0.0000
[2019-04-27 22:30:32,193] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5000, global step 79794: loss 0.0353
[2019-04-27 22:30:32,195] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5000, global step 79795: learning rate 0.0000
[2019-04-27 22:30:32,325] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5000, global step 79807: loss 0.0187
[2019-04-27 22:30:32,333] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5000, global step 79809: learning rate 0.0000
[2019-04-27 22:30:32,832] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5000, global step 79981: loss 0.2255
[2019-04-27 22:30:32,835] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5000, global step 79982: learning rate 0.0000
[2019-04-27 22:30:32,989] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5000, global step 80011: loss 0.4581
[2019-04-27 22:30:32,990] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5000, global step 80011: learning rate 0.0000
[2019-04-27 22:30:33,139] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5000, global step 80033: loss 0.2790
[2019-04-27 22:30:33,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5000, global step 80033: learning rate 0.0000
[2019-04-27 22:30:33,298] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7051200e-14 1.0000000e+00 2.5068933e-23 7.6603082e-16 1.7302455e-23], sum to 1.0000
[2019-04-27 22:30:33,303] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3779
[2019-04-27 22:30:33,310] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5695950113844704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 900436.0167523412, 900436.0167523412, 207253.4065095986], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2992800.0000, 
sim time next is 2993400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.6067918806907895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959263.9464168317, 959263.9464168324, 214947.5562249226], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.5262552779407103, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2664622073380088, 0.266462207338009, 0.3208172480968994], 
reward next is 0.6792, 
noisyNet noise sample is [array([0.38993505], dtype=float32), 1.6497995]. 
=============================================
[2019-04-27 22:30:33,366] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5000, global step 80096: loss 0.0575
[2019-04-27 22:30:33,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5000, global step 80096: learning rate 0.0000
[2019-04-27 22:30:33,371] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5000, global step 80097: loss 0.0113
[2019-04-27 22:30:33,478] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5000, global step 80097: learning rate 0.0000
[2019-04-27 22:30:33,696] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5000, global step 80156: loss 0.1211
[2019-04-27 22:30:33,698] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5000, global step 80156: loss 0.2328
[2019-04-27 22:30:33,699] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5000, global step 80156: learning rate 0.0000
[2019-04-27 22:30:33,701] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5000, global step 80157: learning rate 0.0000
[2019-04-27 22:30:34,089] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5000, global step 80246: loss 0.0109
[2019-04-27 22:30:34,091] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5000, global step 80247: learning rate 0.0000
[2019-04-27 22:30:34,255] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5000, global step 80274: loss 0.0680
[2019-04-27 22:30:34,259] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5000, global step 80275: learning rate 0.0000
[2019-04-27 22:30:34,445] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5000, global step 80322: loss 0.3054
[2019-04-27 22:30:34,447] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5000, global step 80322: learning rate 0.0000
[2019-04-27 22:30:42,830] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.2534575e-15 1.0000000e+00 6.4074608e-24 8.6786724e-17 1.5853876e-26], sum to 1.0000
[2019-04-27 22:30:42,841] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4859
[2019-04-27 22:30:42,848] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.342347309670555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 528301.0408940149, 528301.0408940143, 168892.7728461347], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3131400.0000, 
sim time next is 3132000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3438449554861333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 529919.3263179462, 529919.3263179462, 169001.789415034], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20945175359775098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14719981286609615, 0.14719981286609615, 0.2522414767388567], 
reward next is 0.7478, 
noisyNet noise sample is [array([-1.011471], dtype=float32), 0.99622416]. 
=============================================
[2019-04-27 22:30:42,871] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[78.48971]
 [78.49229]
 [78.53603]
 [78.46266]
 [78.46535]], R is [[78.42854309]
 [78.3921814 ]
 [78.35631561]
 [78.32108307]
 [78.28663635]].
[2019-04-27 22:30:43,469] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2547985e-14 1.0000000e+00 4.0710767e-23 6.6639015e-16 1.8152333e-23], sum to 1.0000
[2019-04-27 22:30:43,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3763
[2019-04-27 22:30:43,478] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 82.33333333333334, 1.0, 2.0, 0.7645639183483103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117259.567119488, 1117259.567119488, 242017.6803069109], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3150600.0000, 
sim time next is 3151200.0000, 
raw observation next is [25.33333333333334, 81.66666666666667, 1.0, 2.0, 0.8276831794280174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1206354.169971211, 1206354.169971211, 257779.7054740786], 
processed observation next is [1.0, 0.4782608695652174, 0.3996840442338076, 0.8166666666666668, 1.0, 1.0, 0.7923893728048402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33509838054755864, 0.33509838054755864, 0.38474582906578897], 
reward next is 0.6153, 
noisyNet noise sample is [array([0.3143184], dtype=float32), 0.12883636]. 
=============================================
[2019-04-27 22:30:49,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2879241e-14 1.0000000e+00 1.6492941e-22 3.7768843e-15 6.0170026e-24], sum to 1.0000
[2019-04-27 22:30:49,267] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1841
[2019-04-27 22:30:49,273] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 67.5, 1.0, 2.0, 0.558430229368891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780349.4737786412, 780349.4737786412, 193258.3575492141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3239400.0000, 
sim time next is 3240000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5661714416204972, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 791171.0596606085, 791171.0596606091, 194614.2034068987], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4773149899042134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21976973879461348, 0.21976973879461364, 0.29046896030880404], 
reward next is 0.7095, 
noisyNet noise sample is [array([-1.2195007], dtype=float32), -0.11947724]. 
=============================================
[2019-04-27 22:30:49,301] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[70.13514 ]
 [70.12762 ]
 [70.12621 ]
 [70.112045]
 [70.10622 ]], R is [[70.18160248]
 [70.19134521]
 [70.20285034]
 [70.21580505]
 [70.22995758]].
[2019-04-27 22:30:50,981] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.99352275e-15 1.00000000e+00 1.65918708e-24 1.36272886e-17
 1.49344859e-24], sum to 1.0000
[2019-04-27 22:30:50,992] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7450
[2019-04-27 22:30:50,999] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.5969005510065787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 834128.9458531226, 834128.9458531233, 200170.9127907281], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3258000.0000, 
sim time next is 3258600.0000, 
raw observation next is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.5996724398922827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 838004.0048726617, 838004.0048726617, 200685.6172654667], 
processed observation next is [0.0, 0.7391304347826086, 0.7077409162717223, 0.7166666666666667, 1.0, 1.0, 0.5176776384244369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23277889024240603, 0.23277889024240603, 0.29953077203801], 
reward next is 0.7005, 
noisyNet noise sample is [array([-1.5173682], dtype=float32), -0.038560946]. 
=============================================
[2019-04-27 22:30:51,659] A3C_AGENT_WORKER-Thread-11 INFO:Local step 5500, global step 87588: loss 0.2954
[2019-04-27 22:30:51,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 5500, global step 87588: learning rate 0.0000
[2019-04-27 22:30:51,805] A3C_AGENT_WORKER-Thread-18 INFO:Local step 5500, global step 87649: loss 0.1674
[2019-04-27 22:30:51,807] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 5500, global step 87650: learning rate 0.0000
[2019-04-27 22:30:51,839] A3C_AGENT_WORKER-Thread-2 INFO:Local step 5500, global step 87664: loss 0.0922
[2019-04-27 22:30:51,841] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 5500, global step 87664: learning rate 0.0000
[2019-04-27 22:30:52,072] A3C_AGENT_WORKER-Thread-9 INFO:Local step 5500, global step 87767: loss 0.4452
[2019-04-27 22:30:52,074] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 5500, global step 87767: learning rate 0.0000
[2019-04-27 22:30:52,078] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1411557e-12 1.0000000e+00 5.9562029e-19 1.5763441e-12 2.9962110e-20], sum to 1.0000
[2019-04-27 22:30:52,089] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8591
[2019-04-27 22:30:52,095] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 79.83333333333334, 1.0, 2.0, 0.4842734569432579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 676689.9161190981, 676689.9161190981, 181168.4367642173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3283800.0000, 
sim time next is 3284400.0000, 
raw observation next is [26.66666666666667, 80.66666666666667, 1.0, 2.0, 0.4829951644860992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 674903.1520813071, 674903.1520813066, 180974.3376002834], 
processed observation next is [0.0, 0.0, 0.4628751974723541, 0.8066666666666668, 1.0, 1.0, 0.3771026078145774, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1874730978003631, 0.18747309780036292, 0.27011095164221405], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.16821735], dtype=float32), 0.30815083]. 
=============================================
[2019-04-27 22:30:52,102] A3C_AGENT_WORKER-Thread-20 INFO:Local step 5500, global step 87778: loss 0.3848
[2019-04-27 22:30:52,104] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 5500, global step 87778: learning rate 0.0000
[2019-04-27 22:30:52,146] A3C_AGENT_WORKER-Thread-13 INFO:Local step 5500, global step 87794: loss 0.3177
[2019-04-27 22:30:52,149] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 5500, global step 87794: learning rate 0.0000
[2019-04-27 22:30:52,479] A3C_AGENT_WORKER-Thread-14 INFO:Local step 5500, global step 87934: loss 0.0926
[2019-04-27 22:30:52,482] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 5500, global step 87934: learning rate 0.0000
[2019-04-27 22:30:52,693] A3C_AGENT_WORKER-Thread-21 INFO:Local step 5500, global step 88025: loss 0.0048
[2019-04-27 22:30:52,696] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 5500, global step 88026: learning rate 0.0000
[2019-04-27 22:30:52,738] A3C_AGENT_WORKER-Thread-15 INFO:Local step 5500, global step 88047: loss 0.0465
[2019-04-27 22:30:52,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 5500, global step 88047: learning rate 0.0000
[2019-04-27 22:30:52,750] A3C_AGENT_WORKER-Thread-3 INFO:Local step 5500, global step 88049: loss 0.0013
[2019-04-27 22:30:52,753] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 5500, global step 88049: learning rate 0.0000
[2019-04-27 22:30:52,982] A3C_AGENT_WORKER-Thread-17 INFO:Local step 5500, global step 88144: loss 0.1544
[2019-04-27 22:30:52,984] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 5500, global step 88144: learning rate 0.0000
[2019-04-27 22:30:53,034] A3C_AGENT_WORKER-Thread-10 INFO:Local step 5500, global step 88162: loss 0.0482
[2019-04-27 22:30:53,041] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 5500, global step 88164: learning rate 0.0000
[2019-04-27 22:30:53,181] A3C_AGENT_WORKER-Thread-22 INFO:Local step 5500, global step 88225: loss 0.1180
[2019-04-27 22:30:53,187] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 5500, global step 88227: learning rate 0.0000
[2019-04-27 22:30:53,336] A3C_AGENT_WORKER-Thread-12 INFO:Local step 5500, global step 88288: loss 0.4062
[2019-04-27 22:30:53,338] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 5500, global step 88289: learning rate 0.0000
[2019-04-27 22:30:53,391] A3C_AGENT_WORKER-Thread-16 INFO:Local step 5500, global step 88310: loss 0.2852
[2019-04-27 22:30:53,392] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 5500, global step 88311: learning rate 0.0000
[2019-04-27 22:30:53,654] A3C_AGENT_WORKER-Thread-19 INFO:Local step 5500, global step 88423: loss 0.2324
[2019-04-27 22:30:53,662] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 5500, global step 88423: learning rate 0.0000
[2019-04-27 22:30:56,803] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2473935e-15 1.0000000e+00 7.5557868e-27 6.2859461e-18 1.5268516e-27], sum to 1.0000
[2019-04-27 22:30:56,814] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7156
[2019-04-27 22:30:56,818] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 79.00000000000001, 1.0, 2.0, 0.5827288349026739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814317.3205550575, 814317.3205550575, 197572.4307188338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3348600.0000, 
sim time next is 3349200.0000, 
raw observation next is [29.66666666666667, 79.0, 1.0, 2.0, 0.5774209010486471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 806897.086632314, 806897.0866323146, 196614.797051636], 
processed observation next is [0.0, 0.782608695652174, 0.6050552922590839, 0.79, 1.0, 1.0, 0.4908685554802977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22413807962008722, 0.2241380796200874, 0.29345492097259107], 
reward next is 0.7065, 
noisyNet noise sample is [array([0.3574103], dtype=float32), -1.0178266]. 
=============================================
[2019-04-27 22:31:00,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1019191e-08 1.0000000e+00 2.1111795e-13 3.2009819e-08 1.4617320e-14], sum to 1.0000
[2019-04-27 22:31:00,776] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5501
[2019-04-27 22:31:00,921] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 70.66666666666667, 1.0, 2.0, 0.5429697960851061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758737.373915605, 758737.373915605, 190603.789200072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3435000.0000, 
sim time next is 3435600.0000, 
raw observation next is [30.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5430648807823452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758870.2912250569, 758870.2912250574, 190619.353949634], 
processed observation next is [1.0, 0.782608695652174, 0.6366508688783573, 0.7133333333333334, 1.0, 1.0, 0.4494757599787291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21079730311807135, 0.21079730311807152, 0.2845064984322896], 
reward next is 0.7155, 
noisyNet noise sample is [array([0.22133584], dtype=float32), 0.11757673]. 
=============================================
[2019-04-27 22:31:10,763] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6000, global step 95640: loss -4.3308
[2019-04-27 22:31:10,765] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6000, global step 95641: learning rate 0.0000
[2019-04-27 22:31:10,823] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6000, global step 95664: loss 75.7943
[2019-04-27 22:31:10,825] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6000, global step 95665: learning rate 0.0000
[2019-04-27 22:31:11,024] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6000, global step 95748: loss -12.4224
[2019-04-27 22:31:11,028] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6000, global step 95748: learning rate 0.0000
[2019-04-27 22:31:11,088] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6000, global step 95776: loss 2.3902
[2019-04-27 22:31:11,090] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6000, global step 95776: learning rate 0.0000
[2019-04-27 22:31:11,114] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6000, global step 95785: loss -9.8076
[2019-04-27 22:31:11,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6000, global step 95786: learning rate 0.0000
[2019-04-27 22:31:11,348] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6000, global step 95886: loss 74.8547
[2019-04-27 22:31:11,352] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6000, global step 95886: learning rate 0.0000
[2019-04-27 22:31:11,412] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6000, global step 95908: loss 36.5649
[2019-04-27 22:31:11,419] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6000, global step 95910: learning rate 0.0000
[2019-04-27 22:31:11,589] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6000, global step 95984: loss -38.7096
[2019-04-27 22:31:11,589] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6000, global step 95984: learning rate 0.0000
[2019-04-27 22:31:11,690] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6000, global step 96026: loss -103.3158
[2019-04-27 22:31:11,692] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6000, global step 96027: learning rate 0.0000
[2019-04-27 22:31:11,969] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6000, global step 96140: loss 27.1224
[2019-04-27 22:31:11,970] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6000, global step 96140: learning rate 0.0000
[2019-04-27 22:31:12,026] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6000, global step 96161: loss -51.1195
[2019-04-27 22:31:12,028] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6000, global step 96161: learning rate 0.0000
[2019-04-27 22:31:12,201] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6000, global step 96236: loss -21.6941
[2019-04-27 22:31:12,203] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6000, global step 96236: learning rate 0.0000
[2019-04-27 22:31:12,207] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6000, global step 96239: loss 50.7760
[2019-04-27 22:31:12,208] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6000, global step 96239: loss 7.2408
[2019-04-27 22:31:12,209] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6000, global step 96239: learning rate 0.0000
[2019-04-27 22:31:12,211] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6000, global step 96239: learning rate 0.0000
[2019-04-27 22:31:12,334] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6000, global step 96288: loss 72.1303
[2019-04-27 22:31:12,336] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6000, global step 96288: learning rate 0.0000
[2019-04-27 22:31:12,591] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6000, global step 96401: loss -131.0109
[2019-04-27 22:31:12,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6000, global step 96402: learning rate 0.0000
[2019-04-27 22:31:21,182] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 22:31:21,183] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:31:21,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:31:21,186] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:31:21,186] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:31:21,186] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:31:21,187] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:31:21,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:31:21,189] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:31:21,189] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:31:21,190] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:31:21,204] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run5
[2019-04-27 22:31:21,205] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run5
[2019-04-27 22:31:21,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run5
[2019-04-27 22:31:21,291] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run5
[2019-04-27 22:31:21,292] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run5
[2019-04-27 22:31:24,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:31:24,218] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.04571582166667, 93.54513474166667, 1.0, 2.0, 0.3782655749985362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 581882.8580423885, 581882.8580423878, 173368.2640536825]
[2019-04-27 22:31:24,220] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:31:24,226] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.2874456e-09 9.9984407e-01 6.3721767e-14 1.5586965e-04 1.0777892e-18], sampled 0.3041694026922033
[2019-04-27 22:31:31,242] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:31:31,243] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.97919614, 84.75903164, 1.0, 2.0, 0.2506197782916456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 411833.5596296086, 411833.559629608, 160970.6752920693]
[2019-04-27 22:31:31,244] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:31:31,247] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6859789e-09 9.9983799e-01 7.6830706e-14 1.6200502e-04 1.4028278e-18], sampled 0.40916438401691146
[2019-04-27 22:31:40,737] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:31:40,738] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.2, 76.66666666666667, 1.0, 2.0, 0.2872192405399432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463895.9245454303, 463895.9245454297, 164535.6812041345]
[2019-04-27 22:31:40,743] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:31:40,750] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6014743e-09 9.9981970e-01 1.0827974e-13 1.8030184e-04 2.2259327e-18], sampled 0.06906996140254873
[2019-04-27 22:31:58,976] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:31:58,976] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.981648595, 77.08965863, 1.0, 2.0, 0.5861132780452569, 0.0, 2.0, 0.0, 1.0, 2.0, 1.004451096434682, 6.911200000000001, 6.9112, 168.9129446202387, 1638729.346193714, 1638729.346193713, 355908.7306103905]
[2019-04-27 22:31:58,977] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:31:58,983] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2164578e-06 9.9736613e-01 6.1291272e-10 2.6326848e-03 2.5377113e-13], sampled 0.9731790644950414
[2019-04-27 22:32:13,539] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:32:13,541] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.45629945, 93.19609387, 1.0, 2.0, 0.6027620260255173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 842323.2165326334, 842323.2165326327, 201254.36468934]
[2019-04-27 22:32:13,544] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:32:13,552] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4123984e-08 9.9968565e-01 6.1078188e-13 3.1437891e-04 2.2678010e-17], sampled 0.6985560772444838
[2019-04-27 22:32:15,488] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:32:15,489] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.16588190333334, 92.38591598833335, 1.0, 2.0, 0.3510783431776218, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545155.3225807857, 545155.3225807857, 170359.347398372]
[2019-04-27 22:32:15,490] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:32:15,493] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9129917e-09 9.9983156e-01 8.3698788e-14 1.6836224e-04 1.5647007e-18], sampled 0.08450628656640513
[2019-04-27 22:32:35,100] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:32:35,101] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.66666666666667, 84.0, 1.0, 2.0, 0.5419766475752494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757349.0692028189, 757349.0692028189, 190433.6967397708]
[2019-04-27 22:32:35,104] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:32:35,108] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2480273e-09 9.9982113e-01 9.4031784e-14 1.7890817e-04 1.8061073e-18], sampled 0.9875259041738881
[2019-04-27 22:32:51,280] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:32:51,281] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.88333333333334, 53.83333333333334, 1.0, 2.0, 0.8480923075110894, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005984532069663, 6.9112, 168.9123160002881, 2082382.592161535, 2015139.437863396, 420057.9172240687]
[2019-04-27 22:32:51,282] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:32:51,285] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5133719e-05 9.8892552e-01 6.7258142e-08 1.1049276e-02 1.4458337e-10], sampled 0.9586705413024719
[2019-04-27 22:32:51,285] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2082382.592161535 W.
[2019-04-27 22:32:54,690] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:32:54,691] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.31666666666667, 94.0, 1.0, 2.0, 0.6209888196530418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867804.455958477, 867804.455958477, 204720.8448404808]
[2019-04-27 22:32:54,693] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:32:54,698] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7068858e-08 9.9965572e-01 8.1627467e-13 3.4428539e-04 3.3622361e-17], sampled 0.8979787124464171
[2019-04-27 22:33:13,583] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:33:13,584] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.5, 88.0, 1.0, 2.0, 0.4927024715282641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688471.8493035258, 688471.8493035252, 182459.5464854985]
[2019-04-27 22:33:13,585] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:33:13,589] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3837144e-08 9.9969089e-01 5.9271517e-13 3.0915430e-04 2.1911500e-17], sampled 0.7517024564406545
[2019-04-27 22:33:18,468] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.], dtype=float32), 0.07219835]
[2019-04-27 22:33:18,468] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.90328959666667, 73.32149520000002, 1.0, 2.0, 0.5460225843809017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788606.75139435, 788606.75139435, 194373.4846645781]
[2019-04-27 22:33:18,469] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:33:18,472] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7811975e-08 9.9957031e-01 1.7565298e-12 4.2966366e-04 9.5138330e-17], sampled 0.09506846260803903
[2019-04-27 22:33:31,417] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.1441 2779287230.0959 926.0000
[2019-04-27 22:33:31,763] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.0602 3163843264.4297 1765.0000
[2019-04-27 22:33:32,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7787 3007702694.4489 1759.0000
[2019-04-27 22:33:32,394] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8251.5400 2927433814.4136 1321.0000
[2019-04-27 22:33:32,564] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8494.3990 2842196534.9379 1122.0000
[2019-04-27 22:33:33,579] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 100000, evaluation results [100000.0, 7884.060236760846, 3163843264.4296546, 1765.0, 8251.540028014097, 2927433814.4136367, 1321.0, 8658.144077323279, 2779287230.095904, 926.0, 7996.77870748214, 3007702694.448904, 1759.0, 8494.39895077874, 2842196534.9378514, 1122.0]
[2019-04-27 22:33:34,076] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2811640e-05 9.9023497e-01 3.8355346e-08 9.7521283e-03 7.0947484e-11], sum to 1.0000
[2019-04-27 22:33:34,088] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3529
[2019-04-27 22:33:34,096] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2068001.674157803 W.
[2019-04-27 22:33:34,103] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.0, 70.0, 1.0, 2.0, 0.4930037956505889, 1.0, 1.0, 0.4930037956505889, 1.0, 2.0, 0.8356048282028957, 6.9112, 6.9112, 170.5573041426782, 2068001.674157803, 2068001.674157803, 406622.3382035792], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3747000.0000, 
sim time next is 3747600.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.8068611707829372, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.972979332840644, 6.9112, 168.9125298630915, 2024675.933573424, 1980847.659279282, 410661.0679703061], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.7, 1.0, 1.0, 0.7673026154011291, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00617793328406444, 0.0, 0.8294378501185456, 0.5624099815481733, 0.5502354609109117, 0.6129269671198598], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6038362], dtype=float32), -0.54982543]. 
=============================================
[2019-04-27 22:33:41,939] A3C_AGENT_WORKER-Thread-11 INFO:Local step 6500, global step 103556: loss 0.4192
[2019-04-27 22:33:41,941] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 6500, global step 103557: learning rate 0.0000
[2019-04-27 22:33:41,982] A3C_AGENT_WORKER-Thread-18 INFO:Local step 6500, global step 103569: loss 0.0846
[2019-04-27 22:33:41,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 6500, global step 103569: learning rate 0.0000
[2019-04-27 22:33:42,229] A3C_AGENT_WORKER-Thread-9 INFO:Local step 6500, global step 103677: loss 0.0232
[2019-04-27 22:33:42,232] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 6500, global step 103677: learning rate 0.0000
[2019-04-27 22:33:42,323] A3C_AGENT_WORKER-Thread-2 INFO:Local step 6500, global step 103717: loss 0.0311
[2019-04-27 22:33:42,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 6500, global step 103717: learning rate 0.0000
[2019-04-27 22:33:42,424] A3C_AGENT_WORKER-Thread-20 INFO:Local step 6500, global step 103756: loss 0.0754
[2019-04-27 22:33:42,427] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 6500, global step 103756: learning rate 0.0000
[2019-04-27 22:33:42,478] A3C_AGENT_WORKER-Thread-13 INFO:Local step 6500, global step 103780: loss 0.0830
[2019-04-27 22:33:42,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 6500, global step 103781: learning rate 0.0000
[2019-04-27 22:33:42,565] A3C_AGENT_WORKER-Thread-14 INFO:Local step 6500, global step 103821: loss 0.2595
[2019-04-27 22:33:42,567] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 6500, global step 103821: learning rate 0.0000
[2019-04-27 22:33:42,840] A3C_AGENT_WORKER-Thread-21 INFO:Local step 6500, global step 103927: loss 0.0228
[2019-04-27 22:33:42,842] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 6500, global step 103928: learning rate 0.0000
[2019-04-27 22:33:43,143] A3C_AGENT_WORKER-Thread-15 INFO:Local step 6500, global step 104062: loss 0.0165
[2019-04-27 22:33:43,147] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 6500, global step 104065: learning rate 0.0000
[2019-04-27 22:33:43,409] A3C_AGENT_WORKER-Thread-3 INFO:Local step 6500, global step 104172: loss 0.1308
[2019-04-27 22:33:43,413] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 6500, global step 104173: learning rate 0.0000
[2019-04-27 22:33:43,507] A3C_AGENT_WORKER-Thread-17 INFO:Local step 6500, global step 104218: loss 0.0200
[2019-04-27 22:33:43,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 6500, global step 104219: learning rate 0.0000
[2019-04-27 22:33:43,534] A3C_AGENT_WORKER-Thread-10 INFO:Local step 6500, global step 104229: loss 0.0065
[2019-04-27 22:33:43,536] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 6500, global step 104229: learning rate 0.0000
[2019-04-27 22:33:43,687] A3C_AGENT_WORKER-Thread-22 INFO:Local step 6500, global step 104290: loss 0.0905
[2019-04-27 22:33:43,688] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 6500, global step 104292: learning rate 0.0000
[2019-04-27 22:33:43,742] A3C_AGENT_WORKER-Thread-16 INFO:Local step 6500, global step 104312: loss 0.0140
[2019-04-27 22:33:43,748] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 6500, global step 104312: learning rate 0.0000
[2019-04-27 22:33:43,827] A3C_AGENT_WORKER-Thread-12 INFO:Local step 6500, global step 104351: loss 0.0103
[2019-04-27 22:33:43,828] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 6500, global step 104351: learning rate 0.0000
[2019-04-27 22:33:43,865] A3C_AGENT_WORKER-Thread-19 INFO:Local step 6500, global step 104368: loss 0.0077
[2019-04-27 22:33:43,867] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 6500, global step 104368: learning rate 0.0000
[2019-04-27 22:33:56,506] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.01810675e-08 2.17664257e-01 1.79538361e-12 7.82335758e-01
 3.30926432e-20], sum to 1.0000
[2019-04-27 22:33:56,514] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7508
[2019-04-27 22:33:56,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3337635.50795311 W.
[2019-04-27 22:33:56,526] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.16666666666667, 67.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.50841227150499, 6.9112, 170.5573041426782, 3337635.50795311, 2909828.064832082, 550394.1116065637], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4121400.0000, 
sim time next is 4122000.0000, 
raw observation next is [34.0, 67.0, 1.0, 2.0, 0.7639485006931455, 1.0, 2.0, 0.7025642898608353, 1.0, 1.0, 1.03, 7.00510277497764, 6.9112, 170.5573041426782, 2948109.947320574, 2880843.569883422, 542289.2956049516], 
processed observation next is [1.0, 0.7391304347826086, 0.8104265402843602, 0.67, 1.0, 1.0, 0.7156006032447536, 1.0, 1.0, 0.6416437227238979, 1.0, 0.5, 1.0365853658536586, 0.009390277497764022, 0.0, 0.8375144448122397, 0.8189194298112705, 0.8002343249676173, 0.8093870083655994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7567379], dtype=float32), 0.4212131]. 
=============================================
[2019-04-27 22:33:56,548] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[36.03316 ]
 [34.74191 ]
 [34.04968 ]
 [33.19819 ]
 [32.233925]], R is [[35.24362183]
 [34.89118576]
 [34.54227448]
 [34.19685364]
 [33.8548851 ]].
[2019-04-27 22:33:58,952] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.4912285e-22 9.9999881e-01 6.8764515e-31 1.1568624e-06 0.0000000e+00], sum to 1.0000
[2019-04-27 22:33:58,962] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4754
[2019-04-27 22:33:58,970] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5832246196768941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 815010.4062550204, 815010.4062550204, 197662.1721715487], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4142400.0000, 
sim time next is 4143000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5833665816437785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815208.863036261, 815208.8630362618, 197687.9080803803], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49803202607684155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2264469063989614, 0.2264469063989616, 0.2950565792244482], 
reward next is 0.7049, 
noisyNet noise sample is [array([0.71267164], dtype=float32), -0.47355527]. 
=============================================
[2019-04-27 22:33:59,005] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[84.399826]
 [84.517746]
 [84.330154]
 [84.38054 ]
 [84.31973 ]], R is [[84.41537476]
 [84.27619934]
 [84.13860321]
 [84.00244141]
 [83.86749268]].
[2019-04-27 22:34:01,108] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7000, global step 111660: loss -138.0088
[2019-04-27 22:34:01,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7000, global step 111660: learning rate 0.0000
[2019-04-27 22:34:01,249] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7000, global step 111720: loss -118.8156
[2019-04-27 22:34:01,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7000, global step 111721: learning rate 0.0000
[2019-04-27 22:34:01,317] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7000, global step 111745: loss -131.4731
[2019-04-27 22:34:01,320] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7000, global step 111745: learning rate 0.0000
[2019-04-27 22:34:01,431] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7000, global step 111801: loss -82.3616
[2019-04-27 22:34:01,433] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7000, global step 111801: learning rate 0.0000
[2019-04-27 22:34:01,444] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7000, global step 111805: loss -124.6934
[2019-04-27 22:34:01,448] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7000, global step 111805: learning rate 0.0000
[2019-04-27 22:34:01,455] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7000, global step 111806: loss -93.2542
[2019-04-27 22:34:01,457] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7000, global step 111806: learning rate 0.0000
[2019-04-27 22:34:01,529] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7000, global step 111836: loss -57.3480
[2019-04-27 22:34:01,533] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7000, global step 111836: learning rate 0.0000
[2019-04-27 22:34:01,948] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7000, global step 112001: loss -75.7316
[2019-04-27 22:34:01,953] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7000, global step 112002: learning rate 0.0000
[2019-04-27 22:34:02,115] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7000, global step 112072: loss -75.2225
[2019-04-27 22:34:02,121] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7000, global step 112073: loss -109.2385
[2019-04-27 22:34:02,123] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7000, global step 112073: learning rate 0.0000
[2019-04-27 22:34:02,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7000, global step 112074: learning rate 0.0000
[2019-04-27 22:34:02,223] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7000, global step 112116: loss -115.3686
[2019-04-27 22:34:02,225] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7000, global step 112116: learning rate 0.0000
[2019-04-27 22:34:02,419] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7000, global step 112203: loss -110.3420
[2019-04-27 22:34:02,422] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7000, global step 112203: learning rate 0.0000
[2019-04-27 22:34:02,466] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7000, global step 112218: loss -121.2680
[2019-04-27 22:34:02,468] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7000, global step 112218: learning rate 0.0000
[2019-04-27 22:34:02,551] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7000, global step 112254: loss -82.4381
[2019-04-27 22:34:02,552] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7000, global step 112254: learning rate 0.0000
[2019-04-27 22:34:02,635] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7000, global step 112291: loss -124.6230
[2019-04-27 22:34:02,637] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7000, global step 112291: learning rate 0.0000
[2019-04-27 22:34:02,643] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7000, global step 112294: loss -104.5126
[2019-04-27 22:34:02,645] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7000, global step 112294: learning rate 0.0000
[2019-04-27 22:34:07,138] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.0757803e-04 1.0819396e-01 1.6745733e-05 8.9118171e-01 2.3628161e-08], sum to 1.0000
[2019-04-27 22:34:07,142] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7203
[2019-04-27 22:34:07,148] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.33333333333334, 59.00000000000001, 1.0, 2.0, 0.9218611747532007, 1.0, 2.0, 0.7815206268908628, 1.0, 1.0, 1.03, 7.005115232228035, 6.9112, 170.5573041426782, 3279862.668631603, 3212587.367559265, 600570.2240982854], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4274400.0000, 
sim time next is 4275000.0000, 
raw observation next is [36.5, 58.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.87300841536288, 6.9112, 170.5573041426782, 3599114.847846163, 2910132.35605682, 548140.6482865362], 
processed observation next is [1.0, 0.4782608695652174, 0.9289099526066351, 0.585, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09618084153628796, 0.0, 0.8375144448122397, 0.999754124401712, 0.8083700989046723, 0.8181203705769198], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55217695], dtype=float32), 0.6933374]. 
=============================================
[2019-04-27 22:34:07,170] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[ 9.75917 ]
 [10.110066]
 [10.038756]
 [10.133244]
 [10.298431]], R is [[9.63591194]
 [9.53955269]
 [9.4441576 ]
 [9.34971619]
 [9.25621891]].
[2019-04-27 22:34:09,412] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.1591621e-14 9.9992418e-01 1.3502722e-20 7.5802476e-05 1.3754855e-29], sum to 1.0000
[2019-04-27 22:34:09,420] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0875
[2019-04-27 22:34:09,425] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6179569735819913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863565.8625013917, 863565.8625013924, 204139.2569754155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4326600.0000, 
sim time next is 4327200.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.6177047092313014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863213.1916565789, 863213.1916565789, 204090.9446946408], 
processed observation next is [1.0, 0.08695652173913043, 0.6208530805687204, 0.84, 1.0, 1.0, 0.539403264134098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23978144212682748, 0.23978144212682748, 0.3046133502905087], 
reward next is 0.6954, 
noisyNet noise sample is [array([-1.81474], dtype=float32), 0.63796645]. 
=============================================
[2019-04-27 22:34:12,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.4172791e-07 9.9844021e-01 4.4647375e-09 1.5589421e-03 1.9710271e-13], sum to 1.0000
[2019-04-27 22:34:12,762] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5983
[2019-04-27 22:34:12,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3250606.982489532 W.
[2019-04-27 22:34:12,772] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.5, 60.0, 1.0, 2.0, 0.9079372975806049, 1.0, 2.0, 0.7745586883045652, 1.0, 1.0, 1.03, 7.005114133549809, 6.9112, 170.5573041426782, 3250606.982489532, 3183332.468445098, 595055.6692015174], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4379400.0000, 
sim time next is 4380000.0000, 
raw observation next is [34.0, 61.0, 1.0, 2.0, 0.8233747413939883, 1.0, 2.0, 0.7322774102112567, 1.0, 2.0, 1.03, 7.00510746215682, 6.9112, 170.5573041426782, 3072945.931245842, 3005676.196191581, 563128.8357438683], 
processed observation next is [1.0, 0.6956521739130435, 0.8104265402843602, 0.61, 1.0, 1.0, 0.7871984836072148, 1.0, 1.0, 0.6774426629051286, 1.0, 1.0, 1.0365853658536586, 0.00939074621568201, 0.0, 0.8375144448122397, 0.8535960920127339, 0.8349100544976614, 0.8404907996177138], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.24281844], dtype=float32), 0.2873713]. 
=============================================
[2019-04-27 22:34:12,789] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[18.560562]
 [18.51717 ]
 [18.791437]
 [18.758556]
 [19.096909]], R is [[18.56775093]
 [18.38207436]
 [18.19825363]
 [18.01627159]
 [17.83610916]].
[2019-04-27 22:34:13,192] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.6561716e-05 9.9110085e-01 6.0989402e-07 8.8419756e-03 1.9034002e-10], sum to 1.0000
[2019-04-27 22:34:13,202] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9536
[2019-04-27 22:34:13,207] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3202546.448471245 W.
[2019-04-27 22:34:13,211] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 57.0, 1.0, 2.0, 0.8850627806188468, 1.0, 2.0, 0.7631214298236858, 1.0, 2.0, 1.03, 7.005112328723577, 6.9112, 170.5573041426782, 3202546.448471245, 3135273.227297257, 586156.7721396822], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4377600.0000, 
sim time next is 4378200.0000, 
raw observation next is [35.5, 58.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.9087355180649, 6.9112, 170.5573041426782, 3624737.44599463, 2910162.177254209, 547901.3978865029], 
processed observation next is [1.0, 0.6956521739130435, 0.8815165876777251, 0.58, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09975355180649004, 0.0, 0.8375144448122397, 1.0068715127762862, 0.8083783825706136, 0.8177632804276163], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5206573], dtype=float32), 0.586249]. 
=============================================
[2019-04-27 22:34:17,859] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3478058e-28 1.0000000e+00 0.0000000e+00 1.8393867e-15 0.0000000e+00], sum to 1.0000
[2019-04-27 22:34:17,872] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4085
[2019-04-27 22:34:17,876] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5616974823647708, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 784916.8146099654, 784916.8146099654, 193826.4778020406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4475400.0000, 
sim time next is 4476000.0000, 
raw observation next is [28.66666666666667, 80.66666666666667, 1.0, 2.0, 0.5586200852771847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780614.8755969654, 780614.8755969654, 193289.581452214], 
processed observation next is [0.0, 0.8260869565217391, 0.5576619273301741, 0.8066666666666668, 1.0, 1.0, 0.46821697021347547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2168374654436015, 0.2168374654436015, 0.2884919126152448], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.08015248], dtype=float32), -1.5724888]. 
=============================================
[2019-04-27 22:34:17,897] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[92.70603 ]
 [92.591835]
 [92.43631 ]
 [92.35974 ]
 [92.27677 ]], R is [[92.59327698]
 [92.37805176]
 [92.16431427]
 [91.95122528]
 [91.73903656]].
[2019-04-27 22:34:19,058] A3C_AGENT_WORKER-Thread-11 INFO:Local step 7500, global step 119632: loss 0.0012
[2019-04-27 22:34:19,059] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 7500, global step 119632: learning rate 0.0000
[2019-04-27 22:34:19,335] A3C_AGENT_WORKER-Thread-18 INFO:Local step 7500, global step 119747: loss 0.1011
[2019-04-27 22:34:19,342] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 7500, global step 119747: learning rate 0.0000
[2019-04-27 22:34:19,350] A3C_AGENT_WORKER-Thread-20 INFO:Local step 7500, global step 119752: loss 0.1175
[2019-04-27 22:34:19,352] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 7500, global step 119752: learning rate 0.0000
[2019-04-27 22:34:19,396] A3C_AGENT_WORKER-Thread-9 INFO:Local step 7500, global step 119773: loss 0.0626
[2019-04-27 22:34:19,402] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 7500, global step 119774: learning rate 0.0000
[2019-04-27 22:34:19,414] A3C_AGENT_WORKER-Thread-13 INFO:Local step 7500, global step 119779: loss 0.0256
[2019-04-27 22:34:19,417] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 7500, global step 119779: learning rate 0.0000
[2019-04-27 22:34:19,447] A3C_AGENT_WORKER-Thread-14 INFO:Local step 7500, global step 119795: loss 0.0809
[2019-04-27 22:34:19,452] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 7500, global step 119795: learning rate 0.0000
[2019-04-27 22:34:19,523] A3C_AGENT_WORKER-Thread-2 INFO:Local step 7500, global step 119825: loss 0.2222
[2019-04-27 22:34:19,524] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 7500, global step 119825: learning rate 0.0000
[2019-04-27 22:34:19,910] A3C_AGENT_WORKER-Thread-21 INFO:Local step 7500, global step 119994: loss 0.0327
[2019-04-27 22:34:19,914] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 7500, global step 119995: learning rate 0.0000
[2019-04-27 22:34:20,041] A3C_AGENT_WORKER-Thread-15 INFO:Local step 7500, global step 120049: loss 0.0222
[2019-04-27 22:34:20,043] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 7500, global step 120049: learning rate 0.0000
[2019-04-27 22:34:20,051] A3C_AGENT_WORKER-Thread-3 INFO:Local step 7500, global step 120053: loss 0.0456
[2019-04-27 22:34:20,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 7500, global step 120053: learning rate 0.0000
[2019-04-27 22:34:20,266] A3C_AGENT_WORKER-Thread-17 INFO:Local step 7500, global step 120145: loss 0.0712
[2019-04-27 22:34:20,272] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 7500, global step 120146: learning rate 0.0000
[2019-04-27 22:34:20,314] A3C_AGENT_WORKER-Thread-16 INFO:Local step 7500, global step 120163: loss 0.2789
[2019-04-27 22:34:20,319] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 7500, global step 120164: learning rate 0.0000
[2019-04-27 22:34:20,529] A3C_AGENT_WORKER-Thread-19 INFO:Local step 7500, global step 120252: loss 0.4181
[2019-04-27 22:34:20,530] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 7500, global step 120253: learning rate 0.0000
[2019-04-27 22:34:20,538] A3C_AGENT_WORKER-Thread-22 INFO:Local step 7500, global step 120255: loss 0.2123
[2019-04-27 22:34:20,539] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 7500, global step 120255: learning rate 0.0000
[2019-04-27 22:34:20,635] A3C_AGENT_WORKER-Thread-12 INFO:Local step 7500, global step 120298: loss 0.1105
[2019-04-27 22:34:20,638] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 7500, global step 120300: learning rate 0.0000
[2019-04-27 22:34:20,656] A3C_AGENT_WORKER-Thread-10 INFO:Local step 7500, global step 120311: loss 0.1578
[2019-04-27 22:34:20,660] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 7500, global step 120312: learning rate 0.0000
[2019-04-27 22:34:24,891] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0223697e-22 1.0000000e+00 1.3105512e-28 2.4496384e-12 0.0000000e+00], sum to 1.0000
[2019-04-27 22:34:24,901] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5768
[2019-04-27 22:34:24,907] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3535653.207972594 W.
[2019-04-27 22:34:24,911] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.0, 88.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 9.824263059908048, 6.9112, 170.0564677201846, 3535653.207972594, 1455035.30827331, 307142.8430445399], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4587000.0000, 
sim time next is 4587600.0000, 
raw observation next is [28.0, 87.33333333333334, 1.0, 2.0, 0.6081404954272743, 1.0, 1.0, 0.6081404954272743, 1.0, 1.0, 1.03, 6.940584511454581, 6.9112, 170.5573041426782, 2551482.90156435, 2530433.58072393, 490948.5622020435], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8733333333333334, 1.0, 1.0, 0.5278801149726197, 1.0, 0.5, 0.5278801149726197, 1.0, 0.5, 1.0365853658536586, 0.0029384511454581207, 0.0, 0.8375144448122397, 0.7087452504345416, 0.7028982168677583, 0.7327590480627515], 
reward next is 0.1203, 
noisyNet noise sample is [array([0.8069042], dtype=float32), 0.6279629]. 
=============================================
[2019-04-27 22:34:31,779] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-27 22:34:31,780] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:34:31,781] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:34:31,781] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:34:31,783] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:34:31,784] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:34:31,785] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:34:31,785] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:34:31,782] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:34:31,788] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:34:31,788] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:34:31,808] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run6
[2019-04-27 22:34:31,810] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run6
[2019-04-27 22:34:31,857] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run6
[2019-04-27 22:34:31,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run6
[2019-04-27 22:34:31,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run6
[2019-04-27 22:34:41,792] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:34:41,795] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.46666666666667, 57.66666666666667, 1.0, 2.0, 0.4221596705322463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689657.2633692822, 689657.2633692828, 182743.8796769991]
[2019-04-27 22:34:41,795] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:34:41,798] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.6820396e-19 1.0000000e+00 2.6265775e-25 3.1952857e-11 2.6412108e-37], sampled 0.3655341119229104
[2019-04-27 22:35:06,742] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:35:06,747] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.15, 91.5, 1.0, 2.0, 0.4884237110301806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682491.0543431197, 682491.0543431191, 181801.6483101896]
[2019-04-27 22:35:06,749] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:35:06,755] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.833295e-20 1.000000e+00 9.391212e-27 7.817148e-12 0.000000e+00], sampled 0.640501004843154
[2019-04-27 22:35:39,703] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:35:39,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 71.0, 1.0, 2.0, 0.9834182312224764, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005992502688075, 6.9112, 168.9122625391003, 2271793.048997042, 2204544.261373738, 458283.7498488391]
[2019-04-27 22:35:39,708] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:35:39,710] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1600781e-17 1.0000000e+00 3.2450027e-23 3.9015077e-10 2.6063143e-34], sampled 0.22555768779569185
[2019-04-27 22:35:39,711] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2271793.048997042 W.
[2019-04-27 22:36:06,627] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:36:06,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.73333333333333, 59.16666666666667, 1.0, 2.0, 0.5643201405193947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788583.0810784464, 788583.0810784464, 194286.3640795134]
[2019-04-27 22:36:06,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:36:06,630] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.01951891e-19 1.00000000e+00 2.54898153e-26 1.24274835e-11
 0.00000000e+00], sampled 0.7193624111789518
[2019-04-27 22:36:14,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:36:14,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.07677781666667, 86.33013391, 1.0, 2.0, 0.6047325868834644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845078.0508085006, 845078.0508085006, 201630.5033574021]
[2019-04-27 22:36:14,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:36:14,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.1715423e-20 1.0000000e+00 3.1645822e-27 5.0441084e-12 0.0000000e+00], sampled 0.41796333656771156
[2019-04-27 22:36:14,280] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07551221]
[2019-04-27 22:36:14,280] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.25409051, 90.80779371, 1.0, 2.0, 0.5231182068916913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730987.5333064273, 730987.5333064273, 187297.1668072361]
[2019-04-27 22:36:14,282] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:36:14,288] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.8796463e-20 1.0000000e+00 1.5055336e-26 9.7694136e-12 0.0000000e+00], sampled 0.6421649966254735
[2019-04-27 22:36:42,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:36:42,409] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8659.9878 2779190307.3385 933.0000
[2019-04-27 22:36:42,688] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:36:42,786] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:36:43,088] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:36:44,106] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 125000, evaluation results [125000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8659.987755389953, 2779190307.3384666, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:36:50,529] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8000, global step 127706: loss -275.6744
[2019-04-27 22:36:50,532] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8000, global step 127707: learning rate 0.0000
[2019-04-27 22:36:50,623] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8000, global step 127747: loss -335.4398
[2019-04-27 22:36:50,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8000, global step 127747: learning rate 0.0000
[2019-04-27 22:36:50,630] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8000, global step 127750: loss -233.7593
[2019-04-27 22:36:50,631] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8000, global step 127750: learning rate 0.0000
[2019-04-27 22:36:50,719] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8000, global step 127784: loss -131.4069
[2019-04-27 22:36:50,722] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8000, global step 127786: learning rate 0.0000
[2019-04-27 22:36:50,797] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8000, global step 127820: loss -264.5763
[2019-04-27 22:36:50,799] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8000, global step 127820: learning rate 0.0000
[2019-04-27 22:36:50,799] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8000, global step 127821: loss -123.5729
[2019-04-27 22:36:50,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8000, global step 127821: learning rate 0.0000
[2019-04-27 22:36:50,966] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8000, global step 127888: loss -143.7502
[2019-04-27 22:36:50,967] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8000, global step 127888: learning rate 0.0000
[2019-04-27 22:36:51,206] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8000, global step 127987: loss -193.2701
[2019-04-27 22:36:51,209] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8000, global step 127987: learning rate 0.0000
[2019-04-27 22:36:51,336] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8000, global step 128042: loss -217.4223
[2019-04-27 22:36:51,337] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8000, global step 128042: learning rate 0.0000
[2019-04-27 22:36:51,401] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8000, global step 128066: loss -146.9025
[2019-04-27 22:36:51,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8000, global step 128069: learning rate 0.0000
[2019-04-27 22:36:51,547] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8000, global step 128128: loss -97.8378
[2019-04-27 22:36:51,549] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8000, global step 128129: learning rate 0.0000
[2019-04-27 22:36:51,637] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8000, global step 128160: loss -138.6930
[2019-04-27 22:36:51,638] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8000, global step 128160: learning rate 0.0000
[2019-04-27 22:36:51,723] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8000, global step 128201: loss -179.5272
[2019-04-27 22:36:51,725] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8000, global step 128201: learning rate 0.0000
[2019-04-27 22:36:51,809] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8000, global step 128233: loss -132.2974
[2019-04-27 22:36:51,818] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8000, global step 128233: learning rate 0.0000
[2019-04-27 22:36:51,837] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8000, global step 128245: loss -117.1029
[2019-04-27 22:36:51,840] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8000, global step 128245: learning rate 0.0000
[2019-04-27 22:36:51,900] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8000, global step 128276: loss -97.1081
[2019-04-27 22:36:51,901] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8000, global step 128276: learning rate 0.0000
[2019-04-27 22:37:02,017] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.9056116e-11 9.9974555e-01 8.9904575e-16 2.5441410e-04 8.5134532e-24], sum to 1.0000
[2019-04-27 22:37:02,025] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4203
[2019-04-27 22:37:02,032] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1914874.385495777 W.
[2019-04-27 22:37:02,036] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 66.66666666666666, 1.0, 2.0, 0.4565314230553945, 1.0, 1.0, 0.4565314230553945, 1.0, 2.0, 0.7801346345810606, 6.9112, 6.9112, 170.5573041426782, 1914874.385495777, 1914874.385495777, 384299.0216107712], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4963800.0000, 
sim time next is 4964400.0000, 
raw observation next is [30.0, 66.0, 1.0, 2.0, 0.4521326246047712, 1.0, 2.0, 0.4521326246047712, 1.0, 2.0, 0.772075351513103, 6.9112, 6.9112, 170.5573041426782, 1896407.746416672, 1896407.746416672, 381487.2438455358], 
processed observation next is [1.0, 0.4782608695652174, 0.6208530805687204, 0.66, 1.0, 1.0, 0.33991882482502556, 1.0, 1.0, 0.33991882482502556, 1.0, 1.0, 0.722043111601345, 0.0, 0.0, 0.8375144448122397, 0.5267799295601866, 0.5267799295601866, 0.5693839460381132], 
reward next is 0.4306, 
noisyNet noise sample is [array([-0.6519674], dtype=float32), 0.70251197]. 
=============================================
[2019-04-27 22:37:06,848] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5304986e-24 1.0000000e+00 8.3439375e-32 9.4965153e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:37:06,854] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6179
[2019-04-27 22:37:06,861] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5438862664541704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760018.4932231669, 760018.4932231669, 190758.7568232735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5058000.0000, 
sim time next is 5058600.0000, 
raw observation next is [31.83333333333334, 63.0, 1.0, 2.0, 0.5419154270068041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757263.4901138083, 757263.4901138083, 190424.636971973], 
processed observation next is [0.0, 0.5652173913043478, 0.7077409162717223, 0.63, 1.0, 1.0, 0.44809087591181207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21035096947605786, 0.21035096947605786, 0.28421587607757165], 
reward next is 0.7158, 
noisyNet noise sample is [array([0.17277808], dtype=float32), 1.2355142]. 
=============================================
[2019-04-27 22:37:09,313] A3C_AGENT_WORKER-Thread-20 INFO:Local step 8500, global step 135673: loss 5.3214
[2019-04-27 22:37:09,315] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 8500, global step 135673: learning rate 0.0000
[2019-04-27 22:37:09,322] A3C_AGENT_WORKER-Thread-11 INFO:Local step 8500, global step 135675: loss 6.0992
[2019-04-27 22:37:09,323] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 8500, global step 135675: learning rate 0.0000
[2019-04-27 22:37:09,355] A3C_AGENT_WORKER-Thread-18 INFO:Local step 8500, global step 135688: loss 6.9501
[2019-04-27 22:37:09,360] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 8500, global step 135689: learning rate 0.0000
[2019-04-27 22:37:09,408] A3C_AGENT_WORKER-Thread-13 INFO:Local step 8500, global step 135708: loss 8.0418
[2019-04-27 22:37:09,409] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 8500, global step 135708: learning rate 0.0000
[2019-04-27 22:37:09,653] A3C_AGENT_WORKER-Thread-2 INFO:Local step 8500, global step 135815: loss 6.0051
[2019-04-27 22:37:09,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 8500, global step 135816: learning rate 0.0000
[2019-04-27 22:37:09,697] A3C_AGENT_WORKER-Thread-14 INFO:Local step 8500, global step 135833: loss 4.6112
[2019-04-27 22:37:09,700] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 8500, global step 135834: learning rate 0.0000
[2019-04-27 22:37:09,725] A3C_AGENT_WORKER-Thread-9 INFO:Local step 8500, global step 135841: loss 3.7969
[2019-04-27 22:37:09,729] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 8500, global step 135841: learning rate 0.0000
[2019-04-27 22:37:10,020] A3C_AGENT_WORKER-Thread-21 INFO:Local step 8500, global step 135965: loss 4.2766
[2019-04-27 22:37:10,030] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 8500, global step 135971: learning rate 0.0000
[2019-04-27 22:37:10,142] A3C_AGENT_WORKER-Thread-15 INFO:Local step 8500, global step 136014: loss 6.4096
[2019-04-27 22:37:10,143] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 8500, global step 136014: learning rate 0.0000
[2019-04-27 22:37:10,356] A3C_AGENT_WORKER-Thread-3 INFO:Local step 8500, global step 136106: loss 6.0580
[2019-04-27 22:37:10,359] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 8500, global step 136108: learning rate 0.0000
[2019-04-27 22:37:10,511] A3C_AGENT_WORKER-Thread-16 INFO:Local step 8500, global step 136173: loss 2.2449
[2019-04-27 22:37:10,515] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 8500, global step 136173: learning rate 0.0000
[2019-04-27 22:37:10,620] A3C_AGENT_WORKER-Thread-17 INFO:Local step 8500, global step 136221: loss 2.1192
[2019-04-27 22:37:10,622] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 8500, global step 136221: learning rate 0.0000
[2019-04-27 22:37:10,626] A3C_AGENT_WORKER-Thread-22 INFO:Local step 8500, global step 136224: loss 2.3357
[2019-04-27 22:37:10,628] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 8500, global step 136224: learning rate 0.0000
[2019-04-27 22:37:10,728] A3C_AGENT_WORKER-Thread-10 INFO:Local step 8500, global step 136263: loss 3.5208
[2019-04-27 22:37:10,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 8500, global step 136263: learning rate 0.0000
[2019-04-27 22:37:10,789] A3C_AGENT_WORKER-Thread-12 INFO:Local step 8500, global step 136287: loss 3.1981
[2019-04-27 22:37:10,792] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 8500, global step 136288: learning rate 0.0000
[2019-04-27 22:37:10,816] A3C_AGENT_WORKER-Thread-19 INFO:Local step 8500, global step 136300: loss 3.8423
[2019-04-27 22:37:10,819] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 8500, global step 136300: learning rate 0.0000
[2019-04-27 22:37:15,400] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0727659e-22 1.0000000e+00 2.0405574e-29 5.3078434e-15 0.0000000e+00], sum to 1.0000
[2019-04-27 22:37:15,413] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6353
[2019-04-27 22:37:15,420] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5217247307778852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729039.6691264116, 729039.669126411, 187069.6609698313], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5175600.0000, 
sim time next is 5176200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5233596320074735, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731325.008657745, 731325.0086577456, 187336.6337033267], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42573449639454636, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2031458357382625, 0.20314583573826264, 0.27960691597511445], 
reward next is 0.7204, 
noisyNet noise sample is [array([-0.9262935], dtype=float32), 1.7378391]. 
=============================================
[2019-04-27 22:37:20,578] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.2806661e-20 1.0000000e+00 1.7914632e-26 4.0760666e-11 1.2456793e-37], sum to 1.0000
[2019-04-27 22:37:20,588] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7942
[2019-04-27 22:37:20,592] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.53333333333334, 84.66666666666667, 1.0, 2.0, 0.5661469796991381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791136.8637080308, 791136.8637080308, 194608.7906780089], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5271600.0000, 
sim time next is 5272200.0000, 
raw observation next is [28.55, 85.0, 1.0, 2.0, 0.5678159465288047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793469.958131984, 793469.958131984, 194903.4707997384], 
processed observation next is [1.0, 0.0, 0.552132701421801, 0.85, 1.0, 1.0, 0.4792963211190418, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22040832170332889, 0.22040832170332889, 0.29090070268617674], 
reward next is 0.7091, 
noisyNet noise sample is [array([-0.8889093], dtype=float32), -0.7790121]. 
=============================================
[2019-04-27 22:37:23,454] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6535515e-07 3.9903444e-01 4.8865184e-10 6.0096538e-01 6.6298089e-17], sum to 1.0000
[2019-04-27 22:37:23,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5824
[2019-04-27 22:37:23,470] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.1, 53.0, 1.0, 2.0, 1.0397076453208, 1.0, 2.0, 1.0397076453208, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2908510.975633127, 2908510.975633127, 553448.9807919493], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5322000.0000, 
sim time next is 5322600.0000, 
raw observation next is [36.1, 53.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.102949163190041, 6.9112, 170.5573041426782, 3046847.46638167, 2909489.740931378, 552651.2682792378], 
processed observation next is [1.0, 0.6086956521739131, 0.9099526066350712, 0.53, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.019174916319004077, 0.0, 0.8375144448122397, 0.8463465184393528, 0.8081915947031606, 0.824852639222743], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.6048289], dtype=float32), -0.5438887]. 
=============================================
[2019-04-27 22:37:27,945] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3376881e-07 9.9505264e-01 1.4015890e-10 4.9472339e-03 2.7908645e-15], sum to 1.0000
[2019-04-27 22:37:27,955] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4360
[2019-04-27 22:37:27,964] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2599112.928915703 W.
[2019-04-27 22:37:27,974] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.35, 70.5, 1.0, 2.0, 0.9292218225455886, 1.0, 1.0, 0.9292218225455886, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2599112.928915703, 2599112.928915703, 487693.7344423991], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5387400.0000, 
sim time next is 5388000.0000, 
raw observation next is [32.53333333333333, 69.66666666666666, 1.0, 2.0, 1.004309314644848, 1.0, 2.0, 1.004309314644848, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2809375.214626117, 2809375.214626116, 531601.5603509038], 
processed observation next is [1.0, 0.34782608695652173, 0.7409162717219588, 0.6966666666666665, 1.0, 1.0, 1.0051919453552387, 1.0, 1.0, 1.0051919453552387, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7803820040628103, 0.7803820040628101, 0.7934351647028415], 
reward next is 0.2066, 
noisyNet noise sample is [array([-1.535836], dtype=float32), -0.37924343]. 
=============================================
[2019-04-27 22:37:27,993] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[23.357416]
 [24.442732]
 [25.179533]
 [27.056055]
 [26.220146]], R is [[22.55379677]
 [22.32825851]
 [22.10497665]
 [21.88392639]
 [21.66508675]].
[2019-04-27 22:37:28,234] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9000, global step 143668: loss -48.2696
[2019-04-27 22:37:28,236] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9000, global step 143668: learning rate 0.0000
[2019-04-27 22:37:28,338] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9000, global step 143718: loss -40.4520
[2019-04-27 22:37:28,341] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9000, global step 143719: learning rate 0.0000
[2019-04-27 22:37:28,398] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9000, global step 143741: loss -67.6802
[2019-04-27 22:37:28,400] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9000, global step 143741: learning rate 0.0000
[2019-04-27 22:37:28,444] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9000, global step 143758: loss 9.7932
[2019-04-27 22:37:28,445] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9000, global step 143758: learning rate 0.0000
[2019-04-27 22:37:28,553] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9000, global step 143808: loss -44.6226
[2019-04-27 22:37:28,555] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9000, global step 143809: learning rate 0.0000
[2019-04-27 22:37:28,698] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9000, global step 143867: loss 1.1229
[2019-04-27 22:37:28,699] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9000, global step 143867: learning rate 0.0000
[2019-04-27 22:37:28,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.7695365e-07 9.9997652e-01 1.5723547e-09 2.2833881e-05 1.8993403e-13], sum to 1.0000
[2019-04-27 22:37:28,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7253
[2019-04-27 22:37:28,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3410289.654036747 W.
[2019-04-27 22:37:28,789] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.93333333333333, 56.0, 1.0, 2.0, 0.9839322949891339, 1.0, 2.0, 0.8125561870088295, 1.0, 2.0, 1.03, 7.005120130641246, 6.9112, 170.5573041426782, 3410289.654036747, 3343010.844031771, 626028.2796489497], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5406000.0000, 
sim time next is 5406600.0000, 
raw observation next is [36.91666666666666, 55.0, 1.0, 2.0, 0.9175162811263684, 1.0, 2.0, 0.779348180077447, 1.0, 2.0, 1.03, 7.005114889384096, 6.9112, 170.5573041426782, 3270733.467974534, 3203458.412495255, 598841.8020657402], 
processed observation next is [1.0, 0.5652173913043478, 0.9486571879936805, 0.55, 1.0, 1.0, 0.9006220254534559, 1.0, 1.0, 0.7341544338282494, 1.0, 1.0, 1.0365853658536586, 0.009391488938409598, 0.0, 0.8375144448122397, 0.9085370744373705, 0.8898495590264597, 0.8937937344264779], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1684016], dtype=float32), 0.4311731]. 
=============================================
[2019-04-27 22:37:28,823] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9000, global step 143916: loss -34.5797
[2019-04-27 22:37:28,826] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9000, global step 143916: learning rate 0.0000
[2019-04-27 22:37:28,975] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9000, global step 143984: loss 4.4644
[2019-04-27 22:37:28,981] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9000, global step 143987: learning rate 0.0000
[2019-04-27 22:37:29,075] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9000, global step 144024: loss -6.3741
[2019-04-27 22:37:29,076] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9000, global step 144024: learning rate 0.0000
[2019-04-27 22:37:29,177] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9000, global step 144065: loss 24.5805
[2019-04-27 22:37:29,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9000, global step 144065: learning rate 0.0000
[2019-04-27 22:37:29,330] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9000, global step 144132: loss 8.4320
[2019-04-27 22:37:29,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9000, global step 144132: learning rate 0.0000
[2019-04-27 22:37:29,338] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9000, global step 144133: loss -23.2500
[2019-04-27 22:37:29,341] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9000, global step 144133: learning rate 0.0000
[2019-04-27 22:37:29,515] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9000, global step 144208: loss -56.9914
[2019-04-27 22:37:29,517] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9000, global step 144208: learning rate 0.0000
[2019-04-27 22:37:29,617] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9000, global step 144247: loss 54.9255
[2019-04-27 22:37:29,623] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9000, global step 144250: learning rate 0.0000
[2019-04-27 22:37:29,630] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9000, global step 144253: loss -8.5243
[2019-04-27 22:37:29,631] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9000, global step 144254: learning rate 0.0000
[2019-04-27 22:37:29,738] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9000, global step 144298: loss 7.1375
[2019-04-27 22:37:29,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9000, global step 144298: learning rate 0.0000
[2019-04-27 22:37:30,559] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.8655367e-11 1.0000000e+00 3.1940363e-15 1.2038580e-08 4.7288665e-20], sum to 1.0000
[2019-04-27 22:37:30,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7527
[2019-04-27 22:37:30,580] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2987302.602963164 W.
[2019-04-27 22:37:30,588] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.58333333333333, 68.0, 1.0, 2.0, 0.7826061983056457, 1.0, 2.0, 0.7118931386670855, 1.0, 2.0, 1.03, 7.005104246482117, 6.9112, 170.5573041426782, 2987302.602963164, 2920035.171427499, 548688.6531343579], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5417400.0000, 
sim time next is 5418000.0000, 
raw observation next is [30.9, 71.0, 1.0, 2.0, 1.01858671130861, 1.0, 2.0, 1.01858671130861, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2849359.220799937, 2849359.220799937, 540316.0463326238], 
processed observation next is [1.0, 0.7391304347826086, 0.6635071090047393, 0.71, 1.0, 1.0, 1.0223936280826627, 1.0, 1.0, 1.0223936280826627, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7914886724444269, 0.7914886724444269, 0.806441860197946], 
reward next is 0.1936, 
noisyNet noise sample is [array([-0.27356213], dtype=float32), 0.6038311]. 
=============================================
[2019-04-27 22:37:30,610] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[30.042675]
 [29.942232]
 [30.083834]
 [29.970953]
 [30.116255]], R is [[30.93195534]
 [30.6226368 ]
 [30.31641006]
 [30.01324654]
 [29.71311378]].
[2019-04-27 22:37:34,544] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.95729750e-06 1.44507706e-01 7.84886396e-08 8.55485260e-01
 1.03118625e-11], sum to 1.0000
[2019-04-27 22:37:34,552] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2071
[2019-04-27 22:37:34,559] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.23333333333333, 69.33333333333333, 1.0, 2.0, 1.002576210934348, 1.0, 2.0, 1.002576210934348, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2804521.72575315, 2804521.725753151, 530554.8964118611], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5478000.0000, 
sim time next is 5478600.0000, 
raw observation next is [33.46666666666667, 68.66666666666667, 1.0, 2.0, 1.004847000295542, 1.0, 2.0, 1.004847000295542, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2810880.986005743, 2810880.986005743, 531932.7359094227], 
processed observation next is [1.0, 0.391304347826087, 0.7851500789889416, 0.6866666666666668, 1.0, 1.0, 1.0058397593922195, 1.0, 1.0, 1.0058397593922195, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7808002738904842, 0.7808002738904842, 0.7939294565812279], 
reward next is 0.2061, 
noisyNet noise sample is [array([0.18938467], dtype=float32), -0.8368451]. 
=============================================
[2019-04-27 22:37:40,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1275211e-08 9.9985540e-01 1.4529822e-10 1.4462265e-04 2.9633143e-14], sum to 1.0000
[2019-04-27 22:37:40,443] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1117
[2019-04-27 22:37:40,452] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2433387.348114565 W.
[2019-04-27 22:37:40,455] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 57.83333333333333, 1.0, 2.0, 0.8700300814657235, 1.0, 2.0, 0.8700300814657235, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2433387.348114565, 2433387.348114565, 455402.7992704416], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5572200.0000, 
sim time next is 5572800.0000, 
raw observation next is [33.1, 57.0, 1.0, 2.0, 0.6034319721306493, 1.0, 2.0, 0.6034319721306493, 1.0, 1.0, 1.03, 6.93139187393586, 6.9112, 170.5573041426782, 2531708.05022951, 2517243.789576335, 489225.3347235431], 
processed observation next is [1.0, 0.5217391304347826, 0.7677725118483413, 0.57, 1.0, 1.0, 0.5222071953381316, 1.0, 1.0, 0.5222071953381316, 1.0, 0.5, 1.0365853658536586, 0.0020191873935860192, 0.0, 0.8375144448122397, 0.7032522361748639, 0.6992343859934264, 0.7301870667515569], 
reward next is 0.1689, 
noisyNet noise sample is [array([0.29597673], dtype=float32), 0.88906306]. 
=============================================
[2019-04-27 22:37:40,641] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9283783e-13 1.0000000e+00 6.6949871e-16 1.7208178e-10 2.7247828e-21], sum to 1.0000
[2019-04-27 22:37:40,651] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4740
[2019-04-27 22:37:40,655] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.46666666666667, 58.33333333333334, 1.0, 2.0, 0.4923990788460354, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688047.7699068038, 688047.7699068038, 182416.9687054068], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5592000.0000, 
sim time next is 5592600.0000, 
raw observation next is [32.2, 60.0, 1.0, 2.0, 0.487147862103459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 680707.6960670656, 680707.6960670656, 181610.642081165], 
processed observation next is [1.0, 0.7391304347826086, 0.7251184834123224, 0.6, 1.0, 1.0, 0.38210585795597474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18908547112974045, 0.18908547112974045, 0.27106065982263433], 
reward next is 0.7289, 
noisyNet noise sample is [array([0.29163012], dtype=float32), -0.59781325]. 
=============================================
[2019-04-27 22:37:43,171] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.4826395e-24 1.0000000e+00 2.3480401e-29 1.7238059e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:37:43,181] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2300
[2019-04-27 22:37:43,187] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4978311659426183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695640.7150467213, 695640.7150467207, 183256.7908125539], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5634000.0000, 
sim time next is 5634600.0000, 
raw observation next is [25.76666666666667, 90.16666666666667, 1.0, 2.0, 0.4987063692652957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 696864.0751059032, 696864.0751059025, 183393.5386969224], 
processed observation next is [0.0, 0.21739130434782608, 0.42022116903633505, 0.9016666666666667, 1.0, 1.0, 0.3960317701991515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19357335419608424, 0.19357335419608404, 0.27372169954764536], 
reward next is 0.7263, 
noisyNet noise sample is [array([1.1561157], dtype=float32), -1.0875735]. 
=============================================
[2019-04-27 22:37:43,223] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 22:37:43,225] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:37:43,226] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:37:43,227] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:37:43,228] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:37:43,229] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:37:43,232] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:37:43,233] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:37:43,231] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:37:43,236] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:37:43,236] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:37:43,250] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run7
[2019-04-27 22:37:43,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run7
[2019-04-27 22:37:43,251] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run7
[2019-04-27 22:37:43,275] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run7
[2019-04-27 22:37:43,330] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run7
[2019-04-27 22:37:45,687] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:37:45,692] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.72495860833333, 95.08303198833333, 1.0, 2.0, 0.3998635667592763, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 599545.6942296802, 599545.6942296802, 174575.4869296046]
[2019-04-27 22:37:45,692] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:37:45,696] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5633989e-28 1.0000000e+00 7.1632508e-37 2.4170162e-23 0.0000000e+00], sampled 0.8085337735333847
[2019-04-27 22:37:49,709] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:37:49,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.46666666666667, 65.33333333333333, 1.0, 2.0, 0.2264836921629681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 376482.9664923292, 376482.9664923292, 158357.9745134281]
[2019-04-27 22:37:49,712] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:37:49,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.4473473e-26 1.0000000e+00 4.1842112e-34 1.3036500e-21 0.0000000e+00], sampled 0.6576964848053838
[2019-04-27 22:38:24,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:38:24,068] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.3, 89.0, 1.0, 2.0, 0.7797536506866575, 1.0, 1.0, 0.7797536506866575, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2180668.235019135, 2180668.235019135, 410126.9778734616]
[2019-04-27 22:38:24,071] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:38:24,074] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6100799e-23 1.0000000e+00 1.8800710e-30 2.5796374e-19 0.0000000e+00], sampled 0.05770860906075259
[2019-04-27 22:38:24,075] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2180668.235019135 W.
[2019-04-27 22:38:44,602] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:38:44,605] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.21446580666667, 81.07123495333333, 1.0, 2.0, 0.3507247535352742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552003.2428080288, 552003.2428080288, 171070.8755984993]
[2019-04-27 22:38:44,608] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:38:44,611] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.6866469e-25 1.0000000e+00 1.7938922e-32 1.3789322e-20 0.0000000e+00], sampled 0.3571158302382116
[2019-04-27 22:38:58,261] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:38:58,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.59638648833333, 82.58046593166667, 1.0, 2.0, 0.6082740682197018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850029.0420576894, 850029.0420576894, 202296.6932674265]
[2019-04-27 22:38:58,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:38:58,271] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6212199e-27 1.0000000e+00 9.9617105e-36 1.2576543e-22 0.0000000e+00], sampled 0.8546291730482818
[2019-04-27 22:39:18,759] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.07766406]
[2019-04-27 22:39:18,760] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.69135217, 85.90524636, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.99845270164734, 6.9112, 168.9119954269845, 1560052.036537546, 1498152.319182627, 318401.7684433079]
[2019-04-27 22:39:18,761] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:39:18,765] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0836572e-22 1.0000000e+00 1.2342942e-29 8.3242814e-19 0.0000000e+00], sampled 0.518033827682261
[2019-04-27 22:39:53,381] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:39:53,646] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.3694 2779190338.1949 933.0000
[2019-04-27 22:39:53,740] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:39:54,101] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:39:54,245] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:39:55,261] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 150000, evaluation results [150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.369356548688, 2779190338.194891, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:39:55,705] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.5238388e-32 1.0000000e+00 0.0000000e+00 1.3621696e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 22:39:55,713] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8234
[2019-04-27 22:39:55,719] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 82.66666666666667, 1.0, 2.0, 0.5087170290158941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710857.0853025321, 710857.0853025321, 184973.2267337021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5640000.0000, 
sim time next is 5640600.0000, 
raw observation next is [27.35, 81.83333333333333, 1.0, 2.0, 0.5090096920599287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711266.1756944802, 711266.1756944802, 185019.8857423539], 
processed observation next is [0.0, 0.2608695652173913, 0.4952606635071091, 0.8183333333333332, 1.0, 1.0, 0.40844541212039603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19757393769291115, 0.19757393769291115, 0.2761490831975431], 
reward next is 0.7239, 
noisyNet noise sample is [array([0.12434897], dtype=float32), 0.20628202]. 
=============================================
[2019-04-27 22:39:56,524] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3554698e-33 1.0000000e+00 0.0000000e+00 6.9258834e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 22:39:56,528] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1200
[2019-04-27 22:39:56,538] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 91.0, 1.0, 2.0, 0.4978753062570742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695702.4143876395, 695702.4143876395, 183263.6760318325], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5633400.0000, 
sim time next is 5634000.0000, 
raw observation next is [25.6, 91.0, 1.0, 2.0, 0.4978311659426183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 695640.7150467213, 695640.7150467207, 183256.7908125539], 
processed observation next is [0.0, 0.21739130434782608, 0.4123222748815167, 0.91, 1.0, 1.0, 0.3949773083646004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19323353195742257, 0.1932335319574224, 0.2735175982276924], 
reward next is 0.7265, 
noisyNet noise sample is [array([-1.431807], dtype=float32), 0.051890597]. 
=============================================
[2019-04-27 22:39:56,572] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[85.755424]
 [85.72147 ]
 [85.94431 ]
 [85.82184 ]
 [85.672516]], R is [[85.74285889]
 [85.61190033]
 [85.48213959]
 [85.35345459]
 [85.22594452]].
[2019-04-27 22:39:59,072] A3C_AGENT_WORKER-Thread-13 INFO:Local step 9500, global step 151631: loss 0.0087
[2019-04-27 22:39:59,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 9500, global step 151631: learning rate 0.0000
[2019-04-27 22:39:59,092] A3C_AGENT_WORKER-Thread-20 INFO:Local step 9500, global step 151641: loss 0.0400
[2019-04-27 22:39:59,094] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 9500, global step 151641: learning rate 0.0000
[2019-04-27 22:39:59,220] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1967705e-37 1.0000000e+00 0.0000000e+00 2.8815330e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 22:39:59,228] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5992
[2019-04-27 22:39:59,233] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.13333333333333, 69.33333333333334, 1.0, 2.0, 0.5336942278659694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 745771.2920402092, 745771.2920402098, 189043.8162094113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5682000.0000, 
sim time next is 5682600.0000, 
raw observation next is [29.9, 70.5, 1.0, 2.0, 0.5326909357630535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744368.8249072004, 744368.8249072004, 188876.7617264902], 
processed observation next is [0.0, 0.782608695652174, 0.6161137440758293, 0.705, 1.0, 1.0, 0.43697703103982344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2067691180297779, 0.2067691180297779, 0.2819056145171495], 
reward next is 0.7181, 
noisyNet noise sample is [array([-0.41794595], dtype=float32), 0.4965185]. 
=============================================
[2019-04-27 22:39:59,304] A3C_AGENT_WORKER-Thread-18 INFO:Local step 9500, global step 151727: loss 0.0269
[2019-04-27 22:39:59,307] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 9500, global step 151728: learning rate 0.0000
[2019-04-27 22:39:59,335] A3C_AGENT_WORKER-Thread-11 INFO:Local step 9500, global step 151739: loss 0.1294
[2019-04-27 22:39:59,336] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 9500, global step 151740: learning rate 0.0000
[2019-04-27 22:39:59,418] A3C_AGENT_WORKER-Thread-14 INFO:Local step 9500, global step 151773: loss 0.5326
[2019-04-27 22:39:59,421] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 9500, global step 151773: learning rate 0.0000
[2019-04-27 22:39:59,452] A3C_AGENT_WORKER-Thread-2 INFO:Local step 9500, global step 151787: loss 0.4861
[2019-04-27 22:39:59,460] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 9500, global step 151787: learning rate 0.0000
[2019-04-27 22:39:59,717] A3C_AGENT_WORKER-Thread-9 INFO:Local step 9500, global step 151904: loss 0.0904
[2019-04-27 22:39:59,717] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 9500, global step 151904: learning rate 0.0000
[2019-04-27 22:39:59,907] A3C_AGENT_WORKER-Thread-21 INFO:Local step 9500, global step 151984: loss 0.0799
[2019-04-27 22:39:59,914] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 9500, global step 151984: learning rate 0.0000
[2019-04-27 22:39:59,953] A3C_AGENT_WORKER-Thread-15 INFO:Local step 9500, global step 152000: loss 0.0011
[2019-04-27 22:39:59,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 9500, global step 152001: learning rate 0.0000
[2019-04-27 22:40:00,307] A3C_AGENT_WORKER-Thread-3 INFO:Local step 9500, global step 152151: loss 0.0087
[2019-04-27 22:40:00,308] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 9500, global step 152151: learning rate 0.0000
[2019-04-27 22:40:00,316] A3C_AGENT_WORKER-Thread-17 INFO:Local step 9500, global step 152152: loss 0.0048
[2019-04-27 22:40:00,319] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 9500, global step 152152: learning rate 0.0000
[2019-04-27 22:40:00,368] A3C_AGENT_WORKER-Thread-16 INFO:Local step 9500, global step 152173: loss 0.0022
[2019-04-27 22:40:00,372] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 9500, global step 152173: learning rate 0.0000
[2019-04-27 22:40:00,424] A3C_AGENT_WORKER-Thread-12 INFO:Local step 9500, global step 152195: loss 0.0045
[2019-04-27 22:40:00,427] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 9500, global step 152195: learning rate 0.0000
[2019-04-27 22:40:00,534] A3C_AGENT_WORKER-Thread-10 INFO:Local step 9500, global step 152245: loss 0.0877
[2019-04-27 22:40:00,540] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 9500, global step 152245: learning rate 0.0000
[2019-04-27 22:40:00,671] A3C_AGENT_WORKER-Thread-22 INFO:Local step 9500, global step 152306: loss 0.1695
[2019-04-27 22:40:00,675] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 9500, global step 152307: learning rate 0.0000
[2019-04-27 22:40:00,905] A3C_AGENT_WORKER-Thread-19 INFO:Local step 9500, global step 152408: loss 0.0298
[2019-04-27 22:40:00,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 9500, global step 152408: learning rate 0.0000
[2019-04-27 22:40:09,060] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2212692e-14 1.0000000e+00 9.6374251e-18 3.8916841e-13 1.8223241e-23], sum to 1.0000
[2019-04-27 22:40:09,069] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8237
[2019-04-27 22:40:09,077] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2054174.616339742 W.
[2019-04-27 22:40:09,082] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.53333333333333, 63.33333333333333, 1.0, 2.0, 0.8279381211667594, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005983229894671, 6.9112, 168.9123930979185, 2054174.616339742, 1986932.355154095, 414844.0284715139], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5835000.0000, 
sim time next is 5835600.0000, 
raw observation next is [32.6, 63.0, 1.0, 2.0, 0.7927367504046985, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981074597912, 6.9112, 168.9123160248709, 2004908.270734213, 1937667.569266444, 406039.0770272419], 
processed observation next is [1.0, 0.5652173913043478, 0.7440758293838864, 0.63, 1.0, 1.0, 0.7502852414514439, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009478107459791208, 0.0, 0.8294368000749871, 0.5569189640928369, 0.5382409914629012, 0.6060284731749879], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.42331555], dtype=float32), -1.2663352]. 
=============================================
[2019-04-27 22:40:15,561] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4590581e-25 1.0000000e+00 1.0020643e-33 1.6855487e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:15,575] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8642
[2019-04-27 22:40:15,579] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 91.00000000000001, 1.0, 2.0, 0.5419584445019228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757323.6234659345, 757323.6234659345, 190431.1994464738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5962200.0000, 
sim time next is 5962800.0000, 
raw observation next is [26.83333333333333, 91.0, 1.0, 2.0, 0.5408372192221654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755756.2847229472, 755756.2847229472, 190241.7570227987], 
processed observation next is [1.0, 0.0, 0.470774091627172, 0.91, 1.0, 1.0, 0.4467918303881511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20993230131192978, 0.20993230131192978, 0.2839429209295503], 
reward next is 0.7161, 
noisyNet noise sample is [array([0.6640094], dtype=float32), 0.49986687]. 
=============================================
[2019-04-27 22:40:17,902] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10000, global step 159655: loss 144.4593
[2019-04-27 22:40:17,908] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10000, global step 159656: learning rate 0.0000
[2019-04-27 22:40:18,012] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10000, global step 159701: loss 129.8799
[2019-04-27 22:40:18,014] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10000, global step 159702: loss 134.6341
[2019-04-27 22:40:18,015] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10000, global step 159702: learning rate 0.0000
[2019-04-27 22:40:18,017] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10000, global step 159702: learning rate 0.0000
[2019-04-27 22:40:18,060] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10000, global step 159726: loss 22.9749
[2019-04-27 22:40:18,062] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10000, global step 159726: learning rate 0.0000
[2019-04-27 22:40:18,106] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10000, global step 159744: loss 166.6649
[2019-04-27 22:40:18,111] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10000, global step 159747: learning rate 0.0000
[2019-04-27 22:40:18,251] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10000, global step 159808: loss 172.4813
[2019-04-27 22:40:18,255] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10000, global step 159809: learning rate 0.0000
[2019-04-27 22:40:18,420] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3263745e-27 1.0000000e+00 1.0544968e-35 2.7260055e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:18,429] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2501
[2019-04-27 22:40:18,436] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2559270.528307627 W.
[2019-04-27 22:40:18,441] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 68.16666666666667, 1.0, 2.0, 0.6099947605688143, 1.0, 2.0, 0.6099947605688143, 1.0, 1.0, 1.03, 6.944204707014305, 6.9112, 170.5573041426782, 2559270.528307627, 2535627.914127636, 491631.3623608339], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6005400.0000, 
sim time next is 6006000.0000, 
raw observation next is [31.0, 69.33333333333334, 1.0, 2.0, 0.5004920029287092, 1.0, 2.0, 0.5004920029287092, 1.0, 2.0, 0.8691890938525783, 6.9112, 6.9112, 170.5573041426782, 2099443.216820053, 2099443.216820053, 415486.306188564], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.6933333333333335, 1.0, 1.0, 0.39818313605868577, 1.0, 1.0, 0.39818313605868577, 1.0, 1.0, 0.840474504698266, 0.0, 0.0, 0.8375144448122397, 0.5831786713389036, 0.5831786713389036, 0.620128815206812], 
reward next is 0.3799, 
noisyNet noise sample is [array([0.02250745], dtype=float32), 1.472042]. 
=============================================
[2019-04-27 22:40:18,460] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[62.543163]
 [61.70675 ]
 [61.34732 ]
 [62.31295 ]
 [61.20169 ]], R is [[64.96067047]
 [64.31106567]
 [63.97578812]
 [63.45027542]
 [62.88044739]].
[2019-04-27 22:40:18,518] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10000, global step 159917: loss 9.9938
[2019-04-27 22:40:18,520] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10000, global step 159917: learning rate 0.0000
[2019-04-27 22:40:18,570] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10000, global step 159940: loss 27.6945
[2019-04-27 22:40:18,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10000, global step 159942: learning rate 0.0000
[2019-04-27 22:40:18,672] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10000, global step 159986: loss 196.8311
[2019-04-27 22:40:18,677] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10000, global step 159987: learning rate 0.0000
[2019-04-27 22:40:18,986] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10000, global step 160114: loss 291.2781
[2019-04-27 22:40:18,989] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10000, global step 160114: learning rate 0.0000
[2019-04-27 22:40:19,023] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10000, global step 160132: loss 207.6154
[2019-04-27 22:40:19,026] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10000, global step 160134: learning rate 0.0000
[2019-04-27 22:40:19,105] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10000, global step 160164: loss 254.4431
[2019-04-27 22:40:19,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10000, global step 160165: learning rate 0.0000
[2019-04-27 22:40:19,261] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10000, global step 160234: loss 197.1957
[2019-04-27 22:40:19,262] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10000, global step 160234: learning rate 0.0000
[2019-04-27 22:40:19,282] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10000, global step 160244: loss 27.6782
[2019-04-27 22:40:19,284] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10000, global step 160245: learning rate 0.0000
[2019-04-27 22:40:19,328] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10000, global step 160264: loss 104.8524
[2019-04-27 22:40:19,329] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10000, global step 160264: learning rate 0.0000
[2019-04-27 22:40:19,400] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10000, global step 160298: loss 172.7838
[2019-04-27 22:40:19,403] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10000, global step 160299: learning rate 0.0000
[2019-04-27 22:40:19,722] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8541860e-29 1.0000000e+00 1.1813797e-38 1.0304308e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:19,730] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8797
[2019-04-27 22:40:19,733] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 74.0, 1.0, 2.0, 0.5351201013424776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747764.4741624974, 747764.4741624967, 189282.49013173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6027600.0000, 
sim time next is 6028200.0000, 
raw observation next is [29.45, 75.0, 1.0, 2.0, 0.533764301461443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 745869.2455511234, 745869.2455511227, 189056.2838746078], 
processed observation next is [1.0, 0.782608695652174, 0.5947867298578199, 0.75, 1.0, 1.0, 0.43827024272463017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20718590154197872, 0.20718590154197852, 0.28217355802180266], 
reward next is 0.7178, 
noisyNet noise sample is [array([0.07293302], dtype=float32), 0.96760106]. 
=============================================
[2019-04-27 22:40:36,627] A3C_AGENT_WORKER-Thread-13 INFO:Local step 10500, global step 167593: loss 0.1942
[2019-04-27 22:40:36,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 10500, global step 167595: learning rate 0.0000
[2019-04-27 22:40:36,707] A3C_AGENT_WORKER-Thread-18 INFO:Local step 10500, global step 167623: loss 0.5743
[2019-04-27 22:40:36,712] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 10500, global step 167624: learning rate 0.0000
[2019-04-27 22:40:36,827] A3C_AGENT_WORKER-Thread-11 INFO:Local step 10500, global step 167682: loss 0.6702
[2019-04-27 22:40:36,831] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 10500, global step 167682: learning rate 0.0000
[2019-04-27 22:40:36,862] A3C_AGENT_WORKER-Thread-20 INFO:Local step 10500, global step 167692: loss 0.2796
[2019-04-27 22:40:36,863] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 10500, global step 167692: learning rate 0.0000
[2019-04-27 22:40:36,938] A3C_AGENT_WORKER-Thread-2 INFO:Local step 10500, global step 167723: loss 0.0933
[2019-04-27 22:40:36,943] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 10500, global step 167723: learning rate 0.0000
[2019-04-27 22:40:36,998] A3C_AGENT_WORKER-Thread-14 INFO:Local step 10500, global step 167749: loss 0.0054
[2019-04-27 22:40:36,999] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 10500, global step 167749: learning rate 0.0000
[2019-04-27 22:40:37,325] A3C_AGENT_WORKER-Thread-21 INFO:Local step 10500, global step 167892: loss 0.0154
[2019-04-27 22:40:37,330] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 10500, global step 167893: learning rate 0.0000
[2019-04-27 22:40:37,365] A3C_AGENT_WORKER-Thread-9 INFO:Local step 10500, global step 167910: loss 0.0012
[2019-04-27 22:40:37,369] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 10500, global step 167911: learning rate 0.0000
[2019-04-27 22:40:37,597] A3C_AGENT_WORKER-Thread-15 INFO:Local step 10500, global step 168005: loss 0.0240
[2019-04-27 22:40:37,598] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 10500, global step 168005: learning rate 0.0000
[2019-04-27 22:40:37,972] A3C_AGENT_WORKER-Thread-3 INFO:Local step 10500, global step 168167: loss 0.0103
[2019-04-27 22:40:37,976] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 10500, global step 168167: learning rate 0.0000
[2019-04-27 22:40:38,005] A3C_AGENT_WORKER-Thread-17 INFO:Local step 10500, global step 168181: loss 0.0311
[2019-04-27 22:40:38,007] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 10500, global step 168182: learning rate 0.0000
[2019-04-27 22:40:38,075] A3C_AGENT_WORKER-Thread-16 INFO:Local step 10500, global step 168210: loss 0.1119
[2019-04-27 22:40:38,080] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 10500, global step 168211: learning rate 0.0000
[2019-04-27 22:40:38,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 10500, global step 168326: loss 0.0060
[2019-04-27 22:40:38,343] A3C_AGENT_WORKER-Thread-10 INFO:Local step 10500, global step 168327: loss 0.0030
[2019-04-27 22:40:38,344] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 10500, global step 168327: learning rate 0.0000
[2019-04-27 22:40:38,344] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 10500, global step 168327: learning rate 0.0000
[2019-04-27 22:40:38,416] A3C_AGENT_WORKER-Thread-22 INFO:Local step 10500, global step 168354: loss 0.0886
[2019-04-27 22:40:38,418] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 10500, global step 168354: learning rate 0.0000
[2019-04-27 22:40:38,459] A3C_AGENT_WORKER-Thread-12 INFO:Local step 10500, global step 168374: loss 0.1521
[2019-04-27 22:40:38,462] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 10500, global step 168374: learning rate 0.0000
[2019-04-27 22:40:40,013] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.5627033e-23 1.0000000e+00 3.5566863e-30 1.7598387e-18 1.9625115e-37], sum to 1.0000
[2019-04-27 22:40:40,021] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4662
[2019-04-27 22:40:40,027] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 87.66666666666667, 1.0, 2.0, 0.5242918556501751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732628.1155994362, 732628.1155994355, 187489.5026628953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6326400.0000, 
sim time next is 6327000.0000, 
raw observation next is [26.9, 87.0, 1.0, 2.0, 0.5242368870535852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732551.2778130334, 732551.2778130334, 187480.5095918376], 
processed observation next is [0.0, 0.21739130434782608, 0.4739336492890995, 0.87, 1.0, 1.0, 0.42679143018504234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20348646605917595, 0.20348646605917595, 0.27982165610722026], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.79018265], dtype=float32), -0.24257876]. 
=============================================
[2019-04-27 22:40:40,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[59.527737]
 [59.49422 ]
 [59.448376]
 [59.366035]
 [59.33268 ]], R is [[59.68305206]
 [59.80638885]
 [59.92853165]
 [60.04949188]
 [60.16923141]].
[2019-04-27 22:40:45,568] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0525967e-26 1.0000000e+00 4.5174354e-33 4.5193788e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:45,577] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9682
[2019-04-27 22:40:45,582] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.0, 1.0, 2.0, 0.7328113345609427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1024146.83015515, 1024146.83015515, 228085.5313411718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6415200.0000, 
sim time next is 6415800.0000, 
raw observation next is [27.08333333333333, 84.66666666666667, 1.0, 2.0, 0.8095661276354895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1131473.305538636, 1131473.305538636, 246271.0952812609], 
processed observation next is [1.0, 0.2608695652173913, 0.4826224328593995, 0.8466666666666667, 1.0, 1.0, 0.7705615995608307, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3142981404273989, 0.3142981404273989, 0.3675687989272551], 
reward next is 0.6324, 
noisyNet noise sample is [array([-0.398504], dtype=float32), 0.29831296]. 
=============================================
[2019-04-27 22:40:47,301] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1250971e-17 1.0000000e+00 1.1556959e-23 6.0314335e-13 3.6079548e-32], sum to 1.0000
[2019-04-27 22:40:47,307] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4675
[2019-04-27 22:40:47,311] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.76666666666667, 68.0, 1.0, 2.0, 0.4759341195083934, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564977731, 665033.4607242984, 665033.4607242984, 179913.5297994881], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6456000.0000, 
sim time next is 6456600.0000, 
raw observation next is [29.65, 68.5, 1.0, 2.0, 0.4700685638035283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104278, 656834.8538178012, 656834.8538178019, 179041.7086901276], 
processed observation next is [1.0, 0.7391304347826086, 0.6042654028436019, 0.685, 1.0, 1.0, 0.3615283901247329, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522869, 0.18245412606050035, 0.18245412606050054, 0.26722643088078746], 
reward next is 0.7328, 
noisyNet noise sample is [array([1.1497483], dtype=float32), -0.14668767]. 
=============================================
[2019-04-27 22:40:47,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2077543e-15 1.0000000e+00 3.1886334e-21 5.2153122e-13 1.9412185e-28], sum to 1.0000
[2019-04-27 22:40:47,389] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7967
[2019-04-27 22:40:47,399] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2244201.364526378 W.
[2019-04-27 22:40:47,409] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.0, 67.66666666666667, 1.0, 2.0, 0.8024494575596776, 1.0, 1.0, 0.8024494575596776, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2244201.364526378, 2244201.364526377, 421058.9738872497], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6448800.0000, 
sim time next is 6449400.0000, 
raw observation next is [30.0, 67.5, 1.0, 2.0, 0.5311267427841805, 1.0, 2.0, 0.5311267427841805, 1.0, 1.0, 0.9110234820497778, 6.911199999999999, 6.9112, 170.5573041426782, 2228079.916040577, 2228079.916040578, 435129.8694984224], 
processed observation next is [1.0, 0.6521739130434783, 0.6208530805687204, 0.675, 1.0, 1.0, 0.4350924611857596, 1.0, 1.0, 0.4350924611857596, 1.0, 0.5, 0.8914920512802168, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6189110877890491, 0.6189110877890495, 0.6494475664155559], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.09193067], dtype=float32), 1.5966161]. 
=============================================
[2019-04-27 22:40:48,373] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.0616030e-26 1.0000000e+00 1.8767872e-32 2.5249512e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:48,377] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6300
[2019-04-27 22:40:48,389] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 91.0, 1.0, 2.0, 0.5249302965196438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733520.5598054687, 733520.5598054692, 187594.3296114696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6487200.0000, 
sim time next is 6487800.0000, 
raw observation next is [26.38333333333333, 91.00000000000001, 1.0, 2.0, 1.002744314557542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1401643.011032975, 1401643.011032975, 299762.3359514407], 
processed observation next is [1.0, 0.08695652173913043, 0.44944707740916257, 0.9100000000000001, 1.0, 1.0, 1.003306403081376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3893452808424931, 0.3893452808424931, 0.44740647156931446], 
reward next is 0.5526, 
noisyNet noise sample is [array([-2.1672516], dtype=float32), 1.5070652]. 
=============================================
[2019-04-27 22:40:48,424] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.4685541e-28 1.0000000e+00 2.5469493e-34 1.1291677e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:40:48,435] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5055
[2019-04-27 22:40:48,440] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 71.5, 1.0, 2.0, 0.5065285289715604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 707797.9601494962, 707797.9601494956, 184625.4918272536], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6460200.0000, 
sim time next is 6460800.0000, 
raw observation next is [28.9, 72.0, 1.0, 2.0, 0.5039603006425784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704208.0551312147, 704208.0551312154, 184219.2377788639], 
processed observation next is [1.0, 0.782608695652174, 0.5687203791469194, 0.72, 1.0, 1.0, 0.4023618080031064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19561334864755964, 0.19561334864755983, 0.2749540862371103], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.6686136], dtype=float32), 1.2971351]. 
=============================================
[2019-04-27 22:40:54,045] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 22:40:54,047] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:40:54,047] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:40:54,048] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:40:54,048] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:40:54,050] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:40:54,049] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:40:54,052] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:40:54,055] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:40:54,054] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:40:54,057] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:40:54,074] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run8
[2019-04-27 22:40:54,074] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run8
[2019-04-27 22:40:54,099] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run8
[2019-04-27 22:40:54,101] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run8
[2019-04-27 22:40:54,162] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run8
[2019-04-27 22:41:31,567] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:41:31,567] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.53644257, 79.13593492, 1.0, 2.0, 0.5720982757917057, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799456.3668939915, 799456.3668939915, 195663.0408233718]
[2019-04-27 22:41:31,568] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:41:31,572] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.9042212e-26 1.0000000e+00 6.4156186e-35 3.4337353e-18 0.0000000e+00], sampled 0.6570417478190144
[2019-04-27 22:41:43,553] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:41:43,599] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 94.0, 1.0, 2.0, 0.4746813606329229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749892.0029466772, 749892.0029466777, 189683.0792734421]
[2019-04-27 22:41:43,601] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:41:43,603] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.6086668e-23 1.0000000e+00 3.2894834e-31 2.7418793e-16 0.0000000e+00], sampled 0.3991867312688515
[2019-04-27 22:41:51,356] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:41:51,357] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.17996338, 68.70456304, 1.0, 2.0, 0.9523543838281067, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005990810240634, 6.9112, 168.9123142148168, 2228315.125761221, 2161067.518240239, 449005.7962843012]
[2019-04-27 22:41:51,360] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:41:51,362] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6867769e-22 1.0000000e+00 5.5978869e-30 1.1764286e-15 0.0000000e+00], sampled 0.14222433908370702
[2019-04-27 22:41:51,363] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2228315.125761221 W.
[2019-04-27 22:41:58,116] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:41:58,118] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.66666666666667, 59.66666666666667, 1.0, 2.0, 0.5455580731291005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762355.4890822403, 762355.4890822397, 191042.6626692287]
[2019-04-27 22:41:58,119] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:41:58,125] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1762796e-24 1.0000000e+00 2.1824401e-33 2.0974885e-17 0.0000000e+00], sampled 0.6667695164089004
[2019-04-27 22:42:04,929] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:42:04,931] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.248976515, 39.75494855333334, 1.0, 2.0, 0.5945377246765035, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 830825.7613029445, 830825.7613029451, 199728.5953701225]
[2019-04-27 22:42:04,933] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:42:04,936] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.7819531e-25 1.0000000e+00 1.6865361e-33 1.8429316e-17 0.0000000e+00], sampled 0.44441358796945407
[2019-04-27 22:42:07,518] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:42:07,519] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 69.66666666666667, 1.0, 2.0, 0.6439446792931339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899897.8467283173, 899897.8467283173, 209218.5528079979]
[2019-04-27 22:42:07,522] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:42:07,526] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0105531e-23 1.0000000e+00 4.1296033e-32 9.4815756e-17 0.0000000e+00], sampled 0.3408236751659046
[2019-04-27 22:42:25,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:42:25,645] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.25, 84.0, 1.0, 2.0, 0.9526288836298518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1331547.379096038, 1331547.379096038, 284826.1462349914]
[2019-04-27 22:42:25,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:42:25,651] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4811054e-21 1.0000000e+00 3.7704081e-29 3.1326165e-15 1.1420710e-37], sampled 0.12143916977782054
[2019-04-27 22:42:55,254] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:42:55,255] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.72106565166667, 49.88163185166667, 1.0, 2.0, 0.7010629128714313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1069693.150189664, 1069693.150189664, 232501.1868947587]
[2019-04-27 22:42:55,256] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:42:55,265] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4453717e-22 1.0000000e+00 1.5715174e-30 6.0941808e-16 0.0000000e+00], sampled 0.193456923294397
[2019-04-27 22:42:57,223] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.08467102]
[2019-04-27 22:42:57,224] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.0, 89.0, 1.0, 2.0, 0.4040069532945039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595397.4831866066, 595397.4831866072, 173891.3984539635]
[2019-04-27 22:42:57,225] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:42:57,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.4563502e-25 1.0000000e+00 5.7942849e-34 1.0624001e-17 0.0000000e+00], sampled 0.7676417231281228
[2019-04-27 22:43:04,531] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:43:04,538] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:43:05,113] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:43:05,115] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:43:05,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:43:06,159] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 175000, evaluation results [175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:43:07,560] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11000, global step 175595: loss -236.9260
[2019-04-27 22:43:07,561] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11000, global step 175596: learning rate 0.0000
[2019-04-27 22:43:07,585] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11000, global step 175606: loss -220.8418
[2019-04-27 22:43:07,586] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11000, global step 175606: learning rate 0.0000
[2019-04-27 22:43:07,655] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11000, global step 175633: loss -196.7385
[2019-04-27 22:43:07,657] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11000, global step 175634: learning rate 0.0000
[2019-04-27 22:43:07,737] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11000, global step 175667: loss -242.5095
[2019-04-27 22:43:07,742] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11000, global step 175670: learning rate 0.0000
[2019-04-27 22:43:08,048] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11000, global step 175796: loss -350.2342
[2019-04-27 22:43:08,051] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11000, global step 175796: learning rate 0.0000
[2019-04-27 22:43:08,119] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11000, global step 175824: loss -388.3522
[2019-04-27 22:43:08,122] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11000, global step 175824: learning rate 0.0000
[2019-04-27 22:43:08,221] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11000, global step 175875: loss -252.4609
[2019-04-27 22:43:08,222] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11000, global step 175875: learning rate 0.0000
[2019-04-27 22:43:08,394] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11000, global step 175947: loss -172.9365
[2019-04-27 22:43:08,397] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11000, global step 175948: learning rate 0.0000
[2019-04-27 22:43:08,481] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11000, global step 175980: loss -408.8062
[2019-04-27 22:43:08,484] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11000, global step 175981: learning rate 0.0000
[2019-04-27 22:43:08,810] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11000, global step 176122: loss -348.3708
[2019-04-27 22:43:08,819] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11000, global step 176122: learning rate 0.0000
[2019-04-27 22:43:08,896] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11000, global step 176153: loss -340.6617
[2019-04-27 22:43:08,897] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11000, global step 176153: learning rate 0.0000
[2019-04-27 22:43:08,973] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11000, global step 176189: loss -166.4233
[2019-04-27 22:43:08,979] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11000, global step 176191: learning rate 0.0000
[2019-04-27 22:43:09,125] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11000, global step 176252: loss -167.2871
[2019-04-27 22:43:09,127] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11000, global step 176252: learning rate 0.0000
[2019-04-27 22:43:09,175] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11000, global step 176278: loss -192.2414
[2019-04-27 22:43:09,179] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11000, global step 176278: learning rate 0.0000
[2019-04-27 22:43:09,183] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11000, global step 176279: loss -174.9490
[2019-04-27 22:43:09,187] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11000, global step 176281: learning rate 0.0000
[2019-04-27 22:43:09,274] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11000, global step 176314: loss -233.2751
[2019-04-27 22:43:09,274] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11000, global step 176314: learning rate 0.0000
[2019-04-27 22:43:11,894] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.7874193e-21 1.0000000e+00 6.9079567e-31 6.1656443e-15 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:11,905] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7681
[2019-04-27 22:43:11,914] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 87.0, 1.0, 2.0, 0.5113883717854631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 714591.1487356533, 714591.1487356533, 185399.6397526673], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6645600.0000, 
sim time next is 6646200.0000, 
raw observation next is [26.56666666666667, 87.0, 1.0, 2.0, 0.5100081957000538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712661.9057015249, 712661.9057015249, 185179.027469198], 
processed observation next is [1.0, 0.9565217391304348, 0.45813586097946307, 0.87, 1.0, 1.0, 0.40964842855428174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1979616404726458, 0.1979616404726458, 0.27638660816298205], 
reward next is 0.7236, 
noisyNet noise sample is [array([-1.9596126], dtype=float32), -0.093057066]. 
=============================================
[2019-04-27 22:43:12,451] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4752417e-20 1.0000000e+00 8.5005629e-31 3.9037052e-13 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:12,462] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1082
[2019-04-27 22:43:12,465] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 87.0, 1.0, 2.0, 0.5043772921234793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704790.9307557036, 704790.9307557043, 184284.7388195243], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6649200.0000, 
sim time next is 6649800.0000, 
raw observation next is [26.33333333333333, 87.33333333333333, 1.0, 2.0, 0.5039062422202693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704132.4916524874, 704132.4916524874, 184210.3413430082], 
processed observation next is [1.0, 1.0, 0.44707740916271704, 0.8733333333333333, 1.0, 1.0, 0.40229667737381836, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19559235879235762, 0.19559235879235762, 0.2749408079746391], 
reward next is 0.7251, 
noisyNet noise sample is [array([0.59846705], dtype=float32), -0.38248146]. 
=============================================
[2019-04-27 22:43:18,155] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.6269386e-21 1.0000000e+00 2.6550205e-29 4.6420089e-14 1.7742264e-37], sum to 1.0000
[2019-04-27 22:43:18,166] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7797
[2019-04-27 22:43:18,172] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 85.66666666666666, 1.0, 2.0, 0.3176181740080889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506957.1601056605, 506957.1601056605, 167611.6768216007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6756000.0000, 
sim time next is 6756600.0000, 
raw observation next is [21.55, 85.83333333333334, 1.0, 2.0, 0.3146323385756924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502486.7575306022, 502486.7575306016, 167279.7886846644], 
processed observation next is [1.0, 0.17391304347826086, 0.22037914691943136, 0.8583333333333334, 1.0, 1.0, 0.17425582960926794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13957965486961174, 0.13957965486961155, 0.24967132639502151], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.9354793], dtype=float32), -0.53285235]. 
=============================================
[2019-04-27 22:43:22,713] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1976040e-26 1.0000000e+00 2.6403699e-33 7.4938798e-17 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:22,718] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7901
[2019-04-27 22:43:22,724] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.3365327607482251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526828.3924248168, 526828.3924248168, 168981.12464931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6827400.0000, 
sim time next is 6828000.0000, 
raw observation next is [23.73333333333333, 76.33333333333333, 1.0, 2.0, 0.3340880907468066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523223.6843403777, 523223.6843403777, 168701.4906607152], 
processed observation next is [0.0, 0.0, 0.3238546603475513, 0.7633333333333333, 1.0, 1.0, 0.19769649487567062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14533991231677157, 0.14533991231677157, 0.25179326964285853], 
reward next is 0.7482, 
noisyNet noise sample is [array([1.7506973], dtype=float32), 0.07871125]. 
=============================================
[2019-04-27 22:43:22,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.382385]
 [77.048134]
 [78.04389 ]
 [79.34014 ]
 [81.01063 ]], R is [[76.05921936]
 [76.04641724]
 [76.03371429]
 [76.02201843]
 [76.01074219]].
[2019-04-27 22:43:26,344] A3C_AGENT_WORKER-Thread-13 INFO:Local step 11500, global step 183570: loss 0.1672
[2019-04-27 22:43:26,346] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 11500, global step 183571: learning rate 0.0000
[2019-04-27 22:43:26,392] A3C_AGENT_WORKER-Thread-11 INFO:Local step 11500, global step 183587: loss 0.0297
[2019-04-27 22:43:26,395] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 11500, global step 183589: learning rate 0.0000
[2019-04-27 22:43:26,522] A3C_AGENT_WORKER-Thread-18 INFO:Local step 11500, global step 183643: loss 0.0089
[2019-04-27 22:43:26,527] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 11500, global step 183645: learning rate 0.0000
[2019-04-27 22:43:26,581] A3C_AGENT_WORKER-Thread-20 INFO:Local step 11500, global step 183665: loss 0.1809
[2019-04-27 22:43:26,584] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 11500, global step 183666: learning rate 0.0000
[2019-04-27 22:43:26,878] A3C_AGENT_WORKER-Thread-2 INFO:Local step 11500, global step 183788: loss 0.1133
[2019-04-27 22:43:26,881] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 11500, global step 183788: learning rate 0.0000
[2019-04-27 22:43:26,885] A3C_AGENT_WORKER-Thread-14 INFO:Local step 11500, global step 183789: loss 0.0431
[2019-04-27 22:43:26,890] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 11500, global step 183790: learning rate 0.0000
[2019-04-27 22:43:27,153] A3C_AGENT_WORKER-Thread-21 INFO:Local step 11500, global step 183913: loss 0.1328
[2019-04-27 22:43:27,154] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 11500, global step 183913: learning rate 0.0000
[2019-04-27 22:43:27,298] A3C_AGENT_WORKER-Thread-9 INFO:Local step 11500, global step 183976: loss 0.0324
[2019-04-27 22:43:27,300] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 11500, global step 183977: learning rate 0.0000
[2019-04-27 22:43:27,467] A3C_AGENT_WORKER-Thread-15 INFO:Local step 11500, global step 184045: loss 0.0547
[2019-04-27 22:43:27,472] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 11500, global step 184045: learning rate 0.0000
[2019-04-27 22:43:27,588] A3C_AGENT_WORKER-Thread-16 INFO:Local step 11500, global step 184096: loss 0.0017
[2019-04-27 22:43:27,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.4145384e-23 1.0000000e+00 5.5971926e-34 1.4436300e-15 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:27,595] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 11500, global step 184096: learning rate 0.0000
[2019-04-27 22:43:27,605] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9174
[2019-04-27 22:43:27,608] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 63.16666666666666, 1.0, 2.0, 0.3688237051575706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 562188.2769463515, 562188.2769463509, 171512.6995161288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6897000.0000, 
sim time next is 6897600.0000, 
raw observation next is [26.8, 64.0, 1.0, 2.0, 0.3693110611153672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562659.6548752574, 562659.6548752574, 171545.3113086715], 
processed observation next is [0.0, 0.8695652173913043, 0.4691943127962086, 0.64, 1.0, 1.0, 0.2401338085727316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15629434857646038, 0.15629434857646038, 0.256037778072644], 
reward next is 0.7440, 
noisyNet noise sample is [array([2.269368], dtype=float32), 1.093373]. 
=============================================
[2019-04-27 22:43:27,731] A3C_AGENT_WORKER-Thread-17 INFO:Local step 11500, global step 184157: loss 0.0368
[2019-04-27 22:43:27,741] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 11500, global step 184158: learning rate 0.0000
[2019-04-27 22:43:27,774] A3C_AGENT_WORKER-Thread-3 INFO:Local step 11500, global step 184172: loss 0.0804
[2019-04-27 22:43:27,782] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 11500, global step 184175: learning rate 0.0000
[2019-04-27 22:43:27,992] A3C_AGENT_WORKER-Thread-19 INFO:Local step 11500, global step 184258: loss 0.0077
[2019-04-27 22:43:27,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 11500, global step 184258: learning rate 0.0000
[2019-04-27 22:43:28,264] A3C_AGENT_WORKER-Thread-12 INFO:Local step 11500, global step 184385: loss 0.0404
[2019-04-27 22:43:28,266] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 11500, global step 184385: learning rate 0.0000
[2019-04-27 22:43:28,275] A3C_AGENT_WORKER-Thread-22 INFO:Local step 11500, global step 184388: loss 0.0475
[2019-04-27 22:43:28,278] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 11500, global step 184389: learning rate 0.0000
[2019-04-27 22:43:28,310] A3C_AGENT_WORKER-Thread-10 INFO:Local step 11500, global step 184402: loss 0.1269
[2019-04-27 22:43:28,314] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 11500, global step 184405: learning rate 0.0000
[2019-04-27 22:43:35,948] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5556286e-18 1.0000000e+00 1.4585358e-24 3.0904701e-10 1.5624962e-31], sum to 1.0000
[2019-04-27 22:43:35,956] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5365
[2019-04-27 22:43:35,965] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1864361.013330664 W.
[2019-04-27 22:43:35,972] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.76666666666667, 46.66666666666667, 1.0, 2.0, 0.6515082309804512, 1.0, 1.0, 0.6515082309804512, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1864361.013330664, 1864361.013330665, 359274.6562010525], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7050000.0000, 
sim time next is 7050600.0000, 
raw observation next is [31.55, 48.0, 1.0, 2.0, 0.6666457208192234, 1.0, 2.0, 0.6666457208192234, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1895471.155396265, 1895471.155396265, 364047.9309075054], 
processed observation next is [1.0, 0.6086956521739131, 0.6943127962085308, 0.48, 1.0, 1.0, 0.5983683383364137, 1.0, 1.0, 0.5983683383364137, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5265197653878514, 0.5265197653878514, 0.5433551207574707], 
reward next is 0.4566, 
noisyNet noise sample is [array([-1.2548542], dtype=float32), -0.8551518]. 
=============================================
[2019-04-27 22:43:36,172] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7415339e-16 1.0000000e+00 2.7122527e-23 2.4620350e-10 1.4478111e-29], sum to 1.0000
[2019-04-27 22:43:36,180] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8075
[2019-04-27 22:43:36,187] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1816663.736863228 W.
[2019-04-27 22:43:36,194] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.43333333333334, 54.83333333333333, 1.0, 2.0, 0.4331365365142261, 1.0, 2.0, 0.4331365365142261, 1.0, 2.0, 0.7234013434853405, 6.9112, 6.9112, 170.5573041426782, 1816663.736863228, 1816663.736863228, 367417.7869470406], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7037400.0000, 
sim time next is 7038000.0000, 
raw observation next is [30.6, 54.0, 1.0, 2.0, 0.7385651460702443, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.950240013415151, 6.9112, 168.9127235697651, 1942446.551923827, 1914750.260948383, 396583.6736482849], 
processed observation next is [1.0, 0.4782608695652174, 0.6492890995260664, 0.54, 1.0, 1.0, 0.6850182482774028, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.003904001341515073, 0.0, 0.8294388013069923, 0.5395684866455075, 0.5318750724856619, 0.5919159308183357], 
reward next is 0.2129, 
noisyNet noise sample is [array([-0.93307155], dtype=float32), 1.1017383]. 
=============================================
[2019-04-27 22:43:36,209] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[54.52482 ]
 [54.352985]
 [54.059765]
 [54.709248]
 [54.324993]], R is [[54.64918518]
 [54.55430984]
 [54.45759964]
 [54.34660339]
 [54.26711655]].
[2019-04-27 22:43:39,338] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5915224e-22 1.0000000e+00 2.0051556e-28 5.2096304e-15 1.7646474e-38], sum to 1.0000
[2019-04-27 22:43:39,352] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3224
[2019-04-27 22:43:39,359] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 88.0, 1.0, 2.0, 0.5245874467852158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747102.4655124333, 747102.4655124326, 189334.376211013], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7106400.0000, 
sim time next is 7107000.0000, 
raw observation next is [25.16666666666667, 86.83333333333334, 1.0, 2.0, 0.5273177890111737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750799.7312878647, 750799.7312878647, 189768.2273237901], 
processed observation next is [1.0, 0.2608695652173913, 0.39178515007898923, 0.8683333333333334, 1.0, 1.0, 0.43050336025442604, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20855548091329576, 0.20855548091329576, 0.28323616018476133], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.09866828], dtype=float32), -1.9783599]. 
=============================================
[2019-04-27 22:43:39,380] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.484962]
 [62.488605]
 [62.533504]
 [62.549763]
 [62.518368]], R is [[62.56568527]
 [62.65744019]
 [62.7484169 ]
 [62.83752823]
 [62.92512512]].
[2019-04-27 22:43:42,171] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4017055e-20 1.0000000e+00 3.1610969e-26 1.7003770e-14 3.5824575e-33], sum to 1.0000
[2019-04-27 22:43:42,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9469
[2019-04-27 22:43:42,182] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 72.66666666666667, 1.0, 2.0, 0.5202683054237087, 1.0, 2.0, 0.5202683054237087, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1454495.930743966, 1454495.930743966, 306657.3006198042], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7129200.0000, 
sim time next is 7129800.0000, 
raw observation next is [27.55, 73.5, 1.0, 2.0, 0.9746856396564278, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1362397.232043433, 1362397.232043433, 291303.5470092486], 
processed observation next is [1.0, 0.5217391304347826, 0.504739336492891, 0.735, 1.0, 1.0, 0.969500770670395, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37844367556762026, 0.37844367556762026, 0.4347814134466397], 
reward next is 0.5652, 
noisyNet noise sample is [array([0.5282956], dtype=float32), -1.9268442]. 
=============================================
[2019-04-27 22:43:43,449] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.0412255e-30 1.0000000e+00 6.8679672e-38 7.2103027e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:43,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3341
[2019-04-27 22:43:43,460] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.01666666666667, 83.83333333333333, 1.0, 2.0, 0.4758985212935705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665387.7399736088, 665387.7399736088, 179957.9294025557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7159800.0000, 
sim time next is 7160400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.47720091044263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666986.696240317, 666986.696240317, 180124.2261184937], 
processed observation next is [1.0, 0.9130434782608695, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3701215788465422, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18527408228897693, 0.18527408228897693, 0.26884212853506523], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.7151525], dtype=float32), -1.5998095]. 
=============================================
[2019-04-27 22:43:45,037] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12000, global step 191532: loss 0.0185
[2019-04-27 22:43:45,038] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12000, global step 191532: learning rate 0.0000
[2019-04-27 22:43:45,149] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12000, global step 191581: loss 0.0874
[2019-04-27 22:43:45,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12000, global step 191582: learning rate 0.0000
[2019-04-27 22:43:45,216] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12000, global step 191607: loss 0.1828
[2019-04-27 22:43:45,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12000, global step 191608: learning rate 0.0000
[2019-04-27 22:43:45,326] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12000, global step 191659: loss 0.0707
[2019-04-27 22:43:45,330] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12000, global step 191660: learning rate 0.0000
[2019-04-27 22:43:45,624] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12000, global step 191781: loss 0.0850
[2019-04-27 22:43:45,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12000, global step 191782: learning rate 0.0000
[2019-04-27 22:43:45,701] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12000, global step 191815: loss 0.0090
[2019-04-27 22:43:45,706] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12000, global step 191816: learning rate 0.0000
[2019-04-27 22:43:45,928] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12000, global step 191908: loss 0.1489
[2019-04-27 22:43:45,931] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12000, global step 191909: learning rate 0.0000
[2019-04-27 22:43:45,961] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12000, global step 191922: loss 0.1007
[2019-04-27 22:43:45,965] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12000, global step 191923: learning rate 0.0000
[2019-04-27 22:43:46,077] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12000, global step 191971: loss 0.0419
[2019-04-27 22:43:46,080] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12000, global step 191971: learning rate 0.0000
[2019-04-27 22:43:46,331] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12000, global step 192078: loss 0.0381
[2019-04-27 22:43:46,333] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12000, global step 192079: learning rate 0.0000
[2019-04-27 22:43:46,408] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12000, global step 192112: loss 0.0142
[2019-04-27 22:43:46,409] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12000, global step 192112: learning rate 0.0000
[2019-04-27 22:43:46,589] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12000, global step 192186: loss 0.0349
[2019-04-27 22:43:46,594] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12000, global step 192188: learning rate 0.0000
[2019-04-27 22:43:46,652] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12000, global step 192214: loss 0.0369
[2019-04-27 22:43:46,654] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12000, global step 192214: learning rate 0.0000
[2019-04-27 22:43:46,857] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12000, global step 192298: loss 0.0253
[2019-04-27 22:43:46,858] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12000, global step 192298: learning rate 0.0000
[2019-04-27 22:43:46,992] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12000, global step 192356: loss 0.0974
[2019-04-27 22:43:46,994] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12000, global step 192356: learning rate 0.0000
[2019-04-27 22:43:47,163] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12000, global step 192428: loss 0.0878
[2019-04-27 22:43:47,166] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12000, global step 192429: learning rate 0.0000
[2019-04-27 22:43:49,922] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2909418e-25 1.0000000e+00 1.5859178e-36 4.3981706e-17 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:49,930] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5499
[2019-04-27 22:43:49,935] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 91.0, 1.0, 2.0, 0.3532704015992723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 544990.5873430512, 544990.5873430519, 170251.2897799421], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7253400.0000, 
sim time next is 7254000.0000, 
raw observation next is [22.3, 91.0, 1.0, 2.0, 0.3529199585290324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 544707.1351822099, 544707.1351822099, 170234.988984636], 
processed observation next is [1.0, 1.0, 0.25592417061611383, 0.91, 1.0, 1.0, 0.2203854922036535, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15130753755061385, 0.15130753755061385, 0.254082073111397], 
reward next is 0.7459, 
noisyNet noise sample is [array([0.7177727], dtype=float32), 0.24840598]. 
=============================================
[2019-04-27 22:43:49,949] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[79.24852 ]
 [79.25431 ]
 [79.249916]
 [79.23382 ]
 [79.20788 ]], R is [[79.26148987]
 [79.21476746]
 [79.16850281]
 [79.12268066]
 [79.07728577]].
[2019-04-27 22:43:51,431] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.6733425e-26 1.0000000e+00 1.0068924e-33 8.4173990e-18 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:51,439] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9696
[2019-04-27 22:43:51,443] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 85.66666666666667, 1.0, 2.0, 0.3202498099343761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 504565.0852633117, 504565.0852633124, 167330.6141950369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7285200.0000, 
sim time next is 7285800.0000, 
raw observation next is [22.25, 85.33333333333334, 1.0, 2.0, 0.3194155092829265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503177.2750253567, 503177.2750253567, 167224.0212844154], 
processed observation next is [1.0, 0.30434782608695654, 0.2535545023696683, 0.8533333333333334, 1.0, 1.0, 0.18001868588304398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1397714652848213, 0.1397714652848213, 0.24958809146927668], 
reward next is 0.7504, 
noisyNet noise sample is [array([-0.51803595], dtype=float32), 0.17024945]. 
=============================================
[2019-04-27 22:43:56,118] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4908767e-25 1.0000000e+00 1.1792940e-32 4.8624824e-18 0.0000000e+00], sum to 1.0000
[2019-04-27 22:43:56,126] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9945
[2019-04-27 22:43:56,133] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 77.33333333333334, 1.0, 2.0, 0.3777967807384725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 571943.8332661344, 571943.8332661344, 172247.2842018072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7339200.0000, 
sim time next is 7339800.0000, 
raw observation next is [24.76666666666667, 77.66666666666666, 1.0, 2.0, 0.3771479680363486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571253.0646516632, 571253.0646516638, 172195.2143893151], 
processed observation next is [1.0, 0.9565217391304348, 0.3728278041074251, 0.7766666666666666, 1.0, 1.0, 0.24957586510403446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15868140684768423, 0.1586814068476844, 0.2570077826706196], 
reward next is 0.7430, 
noisyNet noise sample is [array([0.4667745], dtype=float32), 0.5937205]. 
=============================================
[2019-04-27 22:44:03,858] A3C_AGENT_WORKER-Thread-11 INFO:Local step 12500, global step 199514: loss 0.2635
[2019-04-27 22:44:03,863] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 12500, global step 199516: learning rate 0.0000
[2019-04-27 22:44:03,956] A3C_AGENT_WORKER-Thread-13 INFO:Local step 12500, global step 199555: loss 0.0041
[2019-04-27 22:44:03,959] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 12500, global step 199556: learning rate 0.0000
[2019-04-27 22:44:04,000] A3C_AGENT_WORKER-Thread-18 INFO:Local step 12500, global step 199575: loss 0.0003
[2019-04-27 22:44:04,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 12500, global step 199575: learning rate 0.0000
[2019-04-27 22:44:04,174] A3C_AGENT_WORKER-Thread-20 INFO:Local step 12500, global step 199649: loss 0.2848
[2019-04-27 22:44:04,178] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 12500, global step 199649: learning rate 0.0000
[2019-04-27 22:44:04,400] A3C_AGENT_WORKER-Thread-2 INFO:Local step 12500, global step 199745: loss 0.0253
[2019-04-27 22:44:04,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 12500, global step 199745: learning rate 0.0000
[2019-04-27 22:44:04,758] A3C_AGENT_WORKER-Thread-21 INFO:Local step 12500, global step 199896: loss 0.0003
[2019-04-27 22:44:04,759] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 12500, global step 199896: learning rate 0.0000
[2019-04-27 22:44:04,786] A3C_AGENT_WORKER-Thread-9 INFO:Local step 12500, global step 199908: loss 0.0291
[2019-04-27 22:44:04,788] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 12500, global step 199908: learning rate 0.0000
[2019-04-27 22:44:04,800] A3C_AGENT_WORKER-Thread-14 INFO:Local step 12500, global step 199914: loss 0.1192
[2019-04-27 22:44:04,802] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 12500, global step 199914: learning rate 0.0000
[2019-04-27 22:44:05,006] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-27 22:44:05,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:44:05,013] A3C_AGENT_WORKER-Thread-15 INFO:Local step 12500, global step 200000: loss 0.1736
[2019-04-27 22:44:05,013] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:44:05,014] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:44:05,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:44:05,016] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 12500, global step 200000: learning rate 0.0000
[2019-04-27 22:44:05,015] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:44:05,016] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:44:05,017] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:44:05,020] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:44:05,018] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:44:05,022] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:44:05,039] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run9
[2019-04-27 22:44:05,040] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run9
[2019-04-27 22:44:05,040] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run9
[2019-04-27 22:44:05,116] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run9
[2019-04-27 22:44:05,117] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run9
[2019-04-27 22:44:15,093] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:44:15,094] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.61007829333334, 49.42363283333334, 1.0, 2.0, 0.3290045434895869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533496.5815225496, 533496.581522549, 169573.7632979112]
[2019-04-27 22:44:15,096] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:44:15,101] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2262400e-26 1.0000000e+00 2.6783685e-35 7.7216726e-19 0.0000000e+00], sampled 0.7933961738638808
[2019-04-27 22:44:15,565] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:44:15,568] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.38333333333334, 81.5, 1.0, 2.0, 0.3158108272810242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501628.1467267951, 501628.1467267957, 167185.1406348292]
[2019-04-27 22:44:15,569] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:44:15,571] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1524474e-28 1.0000000e+00 8.5211485e-38 3.7828873e-20 0.0000000e+00], sampled 0.6306043508795142
[2019-04-27 22:44:39,280] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:44:39,281] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.92933744666666, 78.87376945, 1.0, 2.0, 0.4993378538545389, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697746.7657564771, 697746.7657564771, 183491.1598614833]
[2019-04-27 22:44:39,282] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:44:39,286] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0361545e-27 1.0000000e+00 4.2551739e-37 8.7936317e-20 0.0000000e+00], sampled 0.04397969489713538
[2019-04-27 22:44:49,526] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:44:49,527] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.33333333333333, 85.66666666666666, 1.0, 2.0, 0.5072853085785841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 708855.7994473958, 708855.7994473958, 184744.6282809509]
[2019-04-27 22:44:49,530] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:44:49,533] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.2056690e-27 1.0000000e+00 2.8233674e-36 2.3719451e-19 0.0000000e+00], sampled 0.25286877599782676
[2019-04-27 22:45:01,676] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:45:01,679] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.17202001, 57.40068078, 1.0, 2.0, 0.7204763455226805, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005976562031237, 6.9112, 168.9123160366112, 1903782.838287486, 1836545.338173014, 389145.3734017132]
[2019-04-27 22:45:01,679] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:45:01,685] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.9240902e-27 1.0000000e+00 5.5289395e-36 3.3777657e-19 0.0000000e+00], sampled 0.24072361264486342
[2019-04-27 22:45:01,688] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1903782.838287486 W.
[2019-04-27 22:45:09,316] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:45:09,320] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.9, 67.66666666666667, 1.0, 2.0, 0.560474094251915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783206.62014309, 783206.6201430893, 193612.6363552581]
[2019-04-27 22:45:09,322] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:45:09,327] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1584670e-29 1.0000000e+00 0.0000000e+00 5.6642663e-21 0.0000000e+00], sampled 0.8859516135664645
[2019-04-27 22:45:34,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:45:34,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.71666666666667, 62.16666666666666, 1.0, 2.0, 0.5693536612195281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795619.5761335377, 795619.5761335377, 195175.8119165002]
[2019-04-27 22:45:34,502] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:45:34,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.0577125e-31 1.0000000e+00 0.0000000e+00 6.0018103e-22 0.0000000e+00], sampled 0.9436281871562342
[2019-04-27 22:45:59,198] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:45:59,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.77352910666666, 69.68429942833333, 1.0, 2.0, 0.3645004309665989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 558009.3785300106, 558009.37853001, 171225.531157344]
[2019-04-27 22:45:59,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:45:59,206] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.3958696e-27 1.0000000e+00 8.3599775e-36 4.1912239e-19 0.0000000e+00], sampled 0.01995452556860078
[2019-04-27 22:46:06,346] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:46:06,349] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.31666666666666, 64.5, 1.0, 2.0, 0.4838834377651655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724033.3049543169, 724033.3049543169, 187037.5292465091]
[2019-04-27 22:46:06,350] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:46:06,355] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.8049722e-28 1.0000000e+00 2.9013779e-37 7.1925287e-20 0.0000000e+00], sampled 0.4243826363442136
[2019-04-27 22:46:11,491] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00163462], dtype=float32), 0.09377625]
[2019-04-27 22:46:11,492] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.33333333333334, 87.66666666666667, 1.0, 2.0, 0.7881605814381655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1109177.750850785, 1109177.750850785, 242093.9954172774]
[2019-04-27 22:46:11,492] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:46:11,495] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6172975e-27 1.0000000e+00 4.1721253e-36 2.9121827e-19 0.0000000e+00], sampled 0.8568219876044086
[2019-04-27 22:46:14,338] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:46:15,415] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.8604 2842435242.9853 1131.0000
[2019-04-27 22:46:15,476] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:46:15,522] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:46:15,686] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:46:16,702] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 200000, evaluation results [200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.860364057107, 2842435242.9852624, 1131.0]
[2019-04-27 22:46:16,950] A3C_AGENT_WORKER-Thread-16 INFO:Local step 12500, global step 200104: loss 0.0859
[2019-04-27 22:46:16,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 12500, global step 200106: learning rate 0.0000
[2019-04-27 22:46:17,088] A3C_AGENT_WORKER-Thread-3 INFO:Local step 12500, global step 200160: loss 0.0021
[2019-04-27 22:46:17,090] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 12500, global step 200160: learning rate 0.0000
[2019-04-27 22:46:17,344] A3C_AGENT_WORKER-Thread-17 INFO:Local step 12500, global step 200272: loss 0.1844
[2019-04-27 22:46:17,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 12500, global step 200273: learning rate 0.0000
[2019-04-27 22:46:17,373] A3C_AGENT_WORKER-Thread-19 INFO:Local step 12500, global step 200290: loss 0.1487
[2019-04-27 22:46:17,381] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 12500, global step 200291: learning rate 0.0000
[2019-04-27 22:46:17,491] A3C_AGENT_WORKER-Thread-22 INFO:Local step 12500, global step 200336: loss 0.0098
[2019-04-27 22:46:17,492] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 12500, global step 200336: learning rate 0.0000
[2019-04-27 22:46:17,626] A3C_AGENT_WORKER-Thread-12 INFO:Local step 12500, global step 200400: loss 0.0662
[2019-04-27 22:46:17,632] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 12500, global step 200400: learning rate 0.0000
[2019-04-27 22:46:17,818] A3C_AGENT_WORKER-Thread-10 INFO:Local step 12500, global step 200477: loss 0.0010
[2019-04-27 22:46:17,820] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 12500, global step 200479: learning rate 0.0000
[2019-04-27 22:46:18,374] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.0984439e-28 1.0000000e+00 1.0115343e-35 1.2610135e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:46:18,386] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5037
[2019-04-27 22:46:18,394] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.5, 1.0, 2.0, 0.4058138906486476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 596810.241652311, 596810.2416523115, 173983.2552016551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7515000.0000, 
sim time next is 7515600.0000, 
raw observation next is [23.6, 92.66666666666667, 1.0, 2.0, 0.4066744334745181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597597.7098401077, 597597.7098401077, 174041.3197998646], 
processed observation next is [0.0, 1.0, 0.3175355450236968, 0.9266666666666667, 1.0, 1.0, 0.285149919848817, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16599936384447436, 0.16599936384447436, 0.25976316388039494], 
reward next is 0.7402, 
noisyNet noise sample is [array([-0.5590579], dtype=float32), -1.8452574]. 
=============================================
[2019-04-27 22:46:21,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5232447e-27 1.0000000e+00 0.0000000e+00 4.0335443e-23 0.0000000e+00], sum to 1.0000
[2019-04-27 22:46:21,161] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2616
[2019-04-27 22:46:21,168] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 78.0, 1.0, 2.0, 0.4700621584989068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656825.9008090019, 656825.9008090025, 179038.4852045533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7585200.0000, 
sim time next is 7585800.0000, 
raw observation next is [26.86666666666667, 79.33333333333333, 1.0, 2.0, 0.4726763135735764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660479.8399926816, 660479.8399926816, 179425.9147896494], 
processed observation next is [0.0, 0.8260869565217391, 0.4723538704581361, 0.7933333333333333, 1.0, 1.0, 0.3646702573175619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18346662222018933, 0.18346662222018933, 0.2677998728203722], 
reward next is 0.7322, 
noisyNet noise sample is [array([-0.4914019], dtype=float32), -1.9727417]. 
=============================================
[2019-04-27 22:46:23,942] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.7401312e-23 1.0000000e+00 4.3834523e-30 1.3762752e-17 0.0000000e+00], sum to 1.0000
[2019-04-27 22:46:23,945] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4012
[2019-04-27 22:46:23,952] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 89.33333333333334, 1.0, 2.0, 0.5538310938006488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780730.8434152335, 780730.843415233, 193333.0884085688], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7632600.0000, 
sim time next is 7633200.0000, 
raw observation next is [25.33333333333334, 87.66666666666667, 1.0, 2.0, 0.7881605814381655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1109177.750850785, 1109177.750850785, 242093.9954172774], 
processed observation next is [1.0, 0.34782608695652173, 0.3996840442338076, 0.8766666666666667, 1.0, 1.0, 0.7447717848652596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3081049307918847, 0.3081049307918847, 0.36133432151832445], 
reward next is 0.6387, 
noisyNet noise sample is [array([-0.84902847], dtype=float32), 0.39277518]. 
=============================================
[2019-04-27 22:46:34,323] A3C_AGENT_WORKER-Thread-13 INFO:Local step 13000, global step 207503: loss 0.0532
[2019-04-27 22:46:34,325] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 13000, global step 207504: learning rate 0.0000
[2019-04-27 22:46:34,419] A3C_AGENT_WORKER-Thread-11 INFO:Local step 13000, global step 207543: loss 0.0798
[2019-04-27 22:46:34,421] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 13000, global step 207543: learning rate 0.0000
[2019-04-27 22:46:34,510] A3C_AGENT_WORKER-Thread-18 INFO:Local step 13000, global step 207581: loss 0.0487
[2019-04-27 22:46:34,512] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 13000, global step 207582: learning rate 0.0000
[2019-04-27 22:46:34,732] A3C_AGENT_WORKER-Thread-20 INFO:Local step 13000, global step 207679: loss 0.0917
[2019-04-27 22:46:34,735] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 13000, global step 207679: learning rate 0.0000
[2019-04-27 22:46:34,748] A3C_AGENT_WORKER-Thread-2 INFO:Local step 13000, global step 207685: loss 0.0361
[2019-04-27 22:46:34,752] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 13000, global step 207685: learning rate 0.0000
[2019-04-27 22:46:35,115] A3C_AGENT_WORKER-Thread-14 INFO:Local step 13000, global step 207843: loss 0.0701
[2019-04-27 22:46:35,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 13000, global step 207843: learning rate 0.0000
[2019-04-27 22:46:35,201] A3C_AGENT_WORKER-Thread-9 INFO:Local step 13000, global step 207878: loss 0.0919
[2019-04-27 22:46:35,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 13000, global step 207879: learning rate 0.0000
[2019-04-27 22:46:35,255] A3C_AGENT_WORKER-Thread-21 INFO:Local step 13000, global step 207901: loss 0.0654
[2019-04-27 22:46:35,260] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 13000, global step 207901: learning rate 0.0000
[2019-04-27 22:46:35,445] A3C_AGENT_WORKER-Thread-15 INFO:Local step 13000, global step 207979: loss 0.2153
[2019-04-27 22:46:35,446] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 13000, global step 207980: learning rate 0.0000
[2019-04-27 22:46:35,474] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.6563862e-16 1.0000000e+00 6.2104660e-24 1.3850056e-12 1.2925230e-29], sum to 1.0000
[2019-04-27 22:46:35,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4591
[2019-04-27 22:46:35,496] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 90.33333333333334, 1.0, 2.0, 0.5881361047409477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821876.4746654474, 821876.4746654474, 198549.443549744], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7792800.0000, 
sim time next is 7793400.0000, 
raw observation next is [25.45, 90.66666666666667, 1.0, 2.0, 0.5806038977963939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811346.7553858794, 811346.7553858794, 197181.5423690554], 
processed observation next is [1.0, 0.17391304347826086, 0.4052132701421801, 0.9066666666666667, 1.0, 1.0, 0.4947034913209565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22537409871829983, 0.22537409871829983, 0.2943008095060528], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.27632806], dtype=float32), 0.54343826]. 
=============================================
[2019-04-27 22:46:35,902] A3C_AGENT_WORKER-Thread-3 INFO:Local step 13000, global step 208168: loss 0.3193
[2019-04-27 22:46:35,903] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 13000, global step 208169: learning rate 0.0000
[2019-04-27 22:46:35,905] A3C_AGENT_WORKER-Thread-16 INFO:Local step 13000, global step 208171: loss 0.2937
[2019-04-27 22:46:35,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 13000, global step 208172: learning rate 0.0000
[2019-04-27 22:46:36,046] A3C_AGENT_WORKER-Thread-17 INFO:Local step 13000, global step 208226: loss 0.1306
[2019-04-27 22:46:36,049] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 13000, global step 208227: learning rate 0.0000
[2019-04-27 22:46:36,181] A3C_AGENT_WORKER-Thread-19 INFO:Local step 13000, global step 208282: loss 0.1338
[2019-04-27 22:46:36,185] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 13000, global step 208282: learning rate 0.0000
[2019-04-27 22:46:36,330] A3C_AGENT_WORKER-Thread-22 INFO:Local step 13000, global step 208344: loss 0.2121
[2019-04-27 22:46:36,332] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 13000, global step 208344: learning rate 0.0000
[2019-04-27 22:46:36,382] A3C_AGENT_WORKER-Thread-12 INFO:Local step 13000, global step 208365: loss 0.1216
[2019-04-27 22:46:36,385] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 13000, global step 208365: learning rate 0.0000
[2019-04-27 22:46:36,459] A3C_AGENT_WORKER-Thread-10 INFO:Local step 13000, global step 208395: loss 0.2629
[2019-04-27 22:46:36,460] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 13000, global step 208395: learning rate 0.0000
[2019-04-27 22:46:39,014] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9310819e-22 1.0000000e+00 1.0386620e-30 2.6752289e-16 1.4976841e-36], sum to 1.0000
[2019-04-27 22:46:39,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6217
[2019-04-27 22:46:39,027] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 87.66666666666667, 1.0, 2.0, 0.5048302653729568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705424.1024843471, 705424.1024843471, 184356.4518423816], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7860000.0000, 
sim time next is 7860600.0000, 
raw observation next is [26.35, 88.0, 1.0, 2.0, 0.5054856309470599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706340.1814283559, 706340.1814283559, 184460.1662099836], 
processed observation next is [1.0, 1.0, 0.4478672985781992, 0.88, 1.0, 1.0, 0.4041995553579035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19620560595232106, 0.19620560595232106, 0.2753136809104233], 
reward next is 0.7247, 
noisyNet noise sample is [array([0.31236094], dtype=float32), -0.19466418]. 
=============================================
[2019-04-27 22:46:44,807] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:44,807] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:44,809] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run2
[2019-04-27 22:46:44,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:44,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:44,860] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run2
[2019-04-27 22:46:45,008] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,009] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,011] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run2
[2019-04-27 22:46:45,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run2
[2019-04-27 22:46:45,198] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run2
[2019-04-27 22:46:45,339] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,340] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run2
[2019-04-27 22:46:45,370] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,370] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,372] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run2
[2019-04-27 22:46:45,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run2
[2019-04-27 22:46:45,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,484] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run2
[2019-04-27 22:46:45,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run2
[2019-04-27 22:46:45,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,731] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run2
[2019-04-27 22:46:45,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,760] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run2
[2019-04-27 22:46:45,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run2
[2019-04-27 22:46:45,812] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,812] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,814] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run2
[2019-04-27 22:46:45,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,842] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run2
[2019-04-27 22:46:45,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 22:46:45,918] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:46:45,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run2
[2019-04-27 22:46:47,264] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.2712031e-36 1.0000000e+00 0.0000000e+00 8.5350917e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 22:46:47,270] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-27 22:46:47,279] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.05, 84.0, 1.0, 2.0, 0.3218866218840605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511543.1756402256, 511543.1756402256, 167936.0829869875], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 25800.0000, 
sim time next is 26400.0000, 
raw observation next is [22.1, 84.0, 1.0, 2.0, 0.3151566439682977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500318.5019161051, 500318.5019161051, 167083.3718369162], 
processed observation next is [1.0, 0.30434782608695654, 0.24644549763033188, 0.84, 1.0, 1.0, 0.1748875228533707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13897736164336252, 0.13897736164336252, 0.24937816692077044], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.11887925], dtype=float32), 0.73999614]. 
=============================================
[2019-04-27 22:46:50,081] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6833668e-18 9.9999714e-01 8.9667687e-28 2.8283125e-06 4.6549480e-37], sum to 1.0000
[2019-04-27 22:46:50,092] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1940
[2019-04-27 22:46:50,099] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.45, 88.0, 1.0, 2.0, 0.3473432723090848, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 539420.6966323056, 539420.6966323056, 169890.0225572612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 81000.0000, 
sim time next is 81600.0000, 
raw observation next is [22.36666666666667, 88.33333333333333, 1.0, 2.0, 0.3456443531509074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537359.5098161114, 537359.5098161114, 169736.950705746], 
processed observation next is [1.0, 0.9565217391304348, 0.2590837282780413, 0.8833333333333333, 1.0, 1.0, 0.2116197025914547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1492665305044754, 0.1492665305044754, 0.25333873239663585], 
reward next is 0.7467, 
noisyNet noise sample is [array([0.2871435], dtype=float32), 0.6725794]. 
=============================================
[2019-04-27 22:47:00,268] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.0337996e-33 1.0000000e+00 0.0000000e+00 2.9921458e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 22:47:00,277] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8679
[2019-04-27 22:47:00,282] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 93.0, 1.0, 2.0, 0.2945893765243544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 471646.5396369338, 471646.5396369344, 165062.4401791976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 206400.0000, 
sim time next is 207000.0000, 
raw observation next is [20.6, 93.0, 1.0, 2.0, 0.2955235204301194, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 472817.2235251571, 472817.2235251571, 165141.3872876969], 
processed observation next is [0.0, 0.391304347826087, 0.17535545023696694, 0.93, 1.0, 1.0, 0.15123315714472219, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13133811764587697, 0.13133811764587697, 0.2464796825189506], 
reward next is 0.7535, 
noisyNet noise sample is [array([1.2607204], dtype=float32), -1.4541751]. 
=============================================
[2019-04-27 22:47:00,296] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[95.47934 ]
 [95.44234 ]
 [95.39618 ]
 [95.30394 ]
 [95.293465]], R is [[95.3127594 ]
 [95.11327362]
 [94.91591644]
 [94.72063446]
 [94.5273056 ]].
[2019-04-27 22:47:11,829] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.6772868e-31 1.0000000e+00 0.0000000e+00 4.0754233e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 22:47:11,841] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9893
[2019-04-27 22:47:11,846] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 81.83333333333334, 1.0, 2.0, 0.3521309759641594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 563761.5262129023, 563761.5262129023, 172090.8664762153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 407400.0000, 
sim time next is 408000.0000, 
raw observation next is [21.83333333333334, 81.66666666666667, 1.0, 2.0, 0.2945288600983056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 472578.3910940443, 472578.3910940449, 165135.3811838985], 
processed observation next is [1.0, 0.7391304347826086, 0.23380726698262277, 0.8166666666666668, 1.0, 1.0, 0.1500347712027778, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1312717753039012, 0.13127177530390136, 0.24647071818492317], 
reward next is 0.7535, 
noisyNet noise sample is [array([-0.53179204], dtype=float32), 1.6581815]. 
=============================================
[2019-04-27 22:47:11,887] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[86.78147 ]
 [86.044685]
 [85.89063 ]
 [85.82134 ]
 [85.68964 ]], R is [[87.11138916]
 [86.98342133]
 [86.79607391]
 [86.61095428]
 [86.42663574]].
[2019-04-27 22:47:17,030] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 22:47:17,033] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:47:17,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:47:17,033] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:47:17,039] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:47:17,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:47:17,045] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:47:17,036] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:47:17,046] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:47:17,048] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:47:17,048] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:47:17,053] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run10
[2019-04-27 22:47:17,077] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run10
[2019-04-27 22:47:17,078] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run10
[2019-04-27 22:47:17,129] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run10
[2019-04-27 22:47:17,152] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run10
[2019-04-27 22:47:28,786] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:47:28,788] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.46771193, 58.33404562666666, 1.0, 2.0, 0.1972734916953952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 329992.5885425273, 329992.5885425266, 122874.2691545789]
[2019-04-27 22:47:28,790] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:47:28,792] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7627257e-27 1.0000000e+00 1.4315602e-37 9.4226299e-23 0.0000000e+00], sampled 0.4833886114147953
[2019-04-27 22:47:37,518] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:47:37,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [18.06666666666667, 90.66666666666667, 1.0, 2.0, 0.2348014910155285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 390510.8021315899, 390510.8021315905, 159083.5625554454]
[2019-04-27 22:47:37,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:47:37,525] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3563603e-27 1.0000000e+00 1.1493523e-37 8.2574053e-23 0.0000000e+00], sampled 0.6467925392553865
[2019-04-27 22:48:04,841] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:48:04,842] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.28861721, 94.421440665, 1.0, 2.0, 0.4046264318324835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598926.2290780228, 598926.2290780228, 174297.3080808382]
[2019-04-27 22:48:04,843] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:48:04,850] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.4419884e-28 1.0000000e+00 2.7559001e-38 3.5157988e-23 0.0000000e+00], sampled 0.4456256074378051
[2019-04-27 22:48:09,521] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:48:09,522] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.4974510962164605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695109.4536510713, 695109.453651072, 183198.4501562706]
[2019-04-27 22:48:09,525] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:48:09,527] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2648981e-28 1.0000000e+00 0.0000000e+00 1.5941046e-23 0.0000000e+00], sampled 0.08352534980268755
[2019-04-27 22:48:35,749] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:48:35,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.9310957151666619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1301430.710723819, 1301430.71072382, 278631.0077431873]
[2019-04-27 22:48:35,750] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:48:35,752] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0096801e-27 1.0000000e+00 3.5364866e-38 4.0783358e-23 0.0000000e+00], sampled 0.8853810115399433
[2019-04-27 22:49:07,900] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.105293974]
[2019-04-27 22:49:07,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.789285215, 79.41520385499999, 1.0, 2.0, 0.4410485151030382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639240.2261027446, 639240.2261027452, 177813.5589575495]
[2019-04-27 22:49:07,904] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:49:07,909] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8197276e-29 1.0000000e+00 0.0000000e+00 5.3673347e-24 0.0000000e+00], sampled 0.20819938108032876
[2019-04-27 22:49:27,180] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:49:27,464] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:49:27,694] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:49:27,806] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:49:28,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:49:29,111] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 225000, evaluation results [225000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:49:36,659] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1061264e-23 1.0000000e+00 8.2540921e-32 9.4354538e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:36,670] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8109
[2019-04-27 22:49:36,676] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 81.0, 1.0, 2.0, 0.2292047561545382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380660.749794824, 380660.749794824, 158663.3649295302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 597600.0000, 
sim time next is 598200.0000, 
raw observation next is [19.26666666666667, 81.5, 1.0, 2.0, 0.2289954706381522, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380504.1474334769, 380504.1474334762, 158613.0745081647], 
processed observation next is [1.0, 0.9565217391304348, 0.1121642969984204, 0.815, 1.0, 1.0, 0.07107888028693035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10569559650929913, 0.10569559650929894, 0.23673593210173835], 
reward next is 0.7633, 
noisyNet noise sample is [array([0.90041685], dtype=float32), -2.5694911]. 
=============================================
[2019-04-27 22:49:39,194] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9870324e-26 1.0000000e+00 1.1536972e-36 1.6139749e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:39,200] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0756
[2019-04-27 22:49:39,207] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.66666666666667, 1.0, 2.0, 0.5807786436928657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954331.6182756666, 954331.618275666, 211545.4873722269], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 661200.0000, 
sim time next is 661800.0000, 
raw observation next is [24.7, 53.83333333333333, 1.0, 2.0, 0.5804288207173234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 953424.9344160293, 953424.9344160286, 211471.7032750262], 
processed observation next is [1.0, 0.6521739130434783, 0.3696682464454976, 0.5383333333333333, 1.0, 1.0, 0.4944925550811125, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2648402595600081, 0.26484025956000795, 0.31562940787317345], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.7382887], dtype=float32), -1.5189297]. 
=============================================
[2019-04-27 22:49:39,539] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.4896466e-30 1.0000000e+00 0.0000000e+00 2.3092521e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:39,549] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5871
[2019-04-27 22:49:39,559] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.48333333333333, 55.0, 1.0, 2.0, 0.3202940534713662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525968.4570514825, 525968.4570514825, 168698.9697377339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 666600.0000, 
sim time next is 667200.0000, 
raw observation next is [24.26666666666667, 56.00000000000001, 1.0, 2.0, 0.2417796091616441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397264.0672467303, 397264.0672467309, 160112.5659771853], 
processed observation next is [1.0, 0.7391304347826086, 0.34913112164297017, 0.56, 1.0, 1.0, 0.08648145682125793, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11035112979075842, 0.11035112979075859, 0.2389739790704258], 
reward next is 0.7610, 
noisyNet noise sample is [array([0.8948306], dtype=float32), -0.26985508]. 
=============================================
[2019-04-27 22:49:39,596] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3289410e-29 1.0000000e+00 0.0000000e+00 3.7686205e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:39,606] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7021
[2019-04-27 22:49:39,611] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.66666666666667, 60.66666666666666, 1.0, 2.0, 0.5897084698377402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 966447.4489664057, 966447.4489664051, 213397.3104552402], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 643200.0000, 
sim time next is 643800.0000, 
raw observation next is [23.83333333333333, 59.83333333333334, 1.0, 2.0, 0.5921751943221502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 970182.2452247476, 970182.2452247476, 213914.8060947911], 
processed observation next is [1.0, 0.43478260869565216, 0.32859399684044216, 0.5983333333333334, 1.0, 1.0, 0.5086448124363254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26949506811798546, 0.26949506811798546, 0.31927582999222553], 
reward next is 0.6807, 
noisyNet noise sample is [array([-0.33753315], dtype=float32), -0.89398324]. 
=============================================
[2019-04-27 22:49:45,416] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6979965e-27 1.0000000e+00 4.0088016e-38 3.0593586e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:45,425] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4711
[2019-04-27 22:49:45,439] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 71.66666666666667, 1.0, 2.0, 0.2502482864269294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 411660.5502184638, 411660.5502184638, 160928.9597454255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 760800.0000, 
sim time next is 761400.0000, 
raw observation next is [21.45, 73.5, 1.0, 2.0, 0.2509682782545777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412607.3434961428, 412607.3434961434, 161003.1870389815], 
processed observation next is [1.0, 0.8260869565217391, 0.2156398104265403, 0.735, 1.0, 1.0, 0.09755214247539479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11461315097115078, 0.11461315097115095, 0.24030326423728582], 
reward next is 0.7597, 
noisyNet noise sample is [array([0.6696141], dtype=float32), -0.067023344]. 
=============================================
[2019-04-27 22:49:45,696] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7757477e-27 1.0000000e+00 5.9935081e-36 2.7738104e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:45,707] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5289
[2019-04-27 22:49:45,711] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666666, 56.0, 1.0, 2.0, 0.5410158222457473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 879874.7795319294, 879874.7795319294, 203320.0198753377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 733200.0000, 
sim time next is 733800.0000, 
raw observation next is [25.13333333333333, 55.5, 1.0, 2.0, 0.5358969499269023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 871885.0325699777, 871885.0325699777, 202343.5722974583], 
processed observation next is [1.0, 0.4782608695652174, 0.3902053712480251, 0.555, 1.0, 1.0, 0.44083969870711115, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2421902868249938, 0.2421902868249938, 0.3020053317872512], 
reward next is 0.6980, 
noisyNet noise sample is [array([-0.19167176], dtype=float32), -1.2087036]. 
=============================================
[2019-04-27 22:49:48,057] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.9060456e-23 1.0000000e+00 2.6369677e-33 6.5441270e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:48,066] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5489
[2019-04-27 22:49:48,071] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.5, 89.33333333333334, 1.0, 2.0, 0.2540526982891675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417111.0264477726, 417111.0264477726, 161313.052710072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 778200.0000, 
sim time next is 778800.0000, 
raw observation next is [19.5, 89.66666666666667, 1.0, 2.0, 0.2547393510533554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418024.3709610438, 418024.3709610438, 161381.6543271054], 
processed observation next is [0.0, 0.0, 0.12322274881516594, 0.8966666666666667, 1.0, 1.0, 0.1020956036787414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11611788082251216, 0.11611788082251216, 0.2408681407867245], 
reward next is 0.7591, 
noisyNet noise sample is [array([0.44830683], dtype=float32), -2.8487926]. 
=============================================
[2019-04-27 22:49:57,539] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4355563e-24 1.0000000e+00 8.0189457e-36 1.6339836e-21 0.0000000e+00], sum to 1.0000
[2019-04-27 22:49:57,549] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5097
[2019-04-27 22:49:57,556] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3388869456484538, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 524994.6937871921, 524994.6937871921, 168689.7937031262], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 949800.0000, 
sim time next is 950400.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3406862791570941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 527781.4865893979, 527781.4865893973, 168912.608624645], 
processed observation next is [1.0, 0.0, 0.23222748815165886, 0.94, 1.0, 1.0, 0.20564611946637842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14660596849705496, 0.14660596849705482, 0.2521083710815597], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.14969094], dtype=float32), 1.2189485]. 
=============================================
[2019-04-27 22:50:06,655] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8563760e-21 1.0000000e+00 7.4041479e-31 3.4914544e-19 2.6401228e-37], sum to 1.0000
[2019-04-27 22:50:06,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5241
[2019-04-27 22:50:06,672] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.73333333333333, 67.0, 1.0, 2.0, 0.7365807830015667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138469.343710732, 1138469.343710732, 242778.3475346128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1095600.0000, 
sim time next is 1096200.0000, 
raw observation next is [25.75, 67.0, 1.0, 2.0, 0.7502723978122694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1159155.047152493, 1159155.047152493, 246202.704848598], 
processed observation next is [1.0, 0.6956521739130435, 0.41943127962085314, 0.67, 1.0, 1.0, 0.6991233708581559, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3219875130979147, 0.3219875130979147, 0.36746672365462385], 
reward next is 0.6325, 
noisyNet noise sample is [array([2.2294495], dtype=float32), 0.25092742]. 
=============================================
[2019-04-27 22:50:06,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6058781e-23 1.0000000e+00 3.8564369e-36 9.4131596e-20 0.0000000e+00], sum to 1.0000
[2019-04-27 22:50:06,891] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7319
[2019-04-27 22:50:06,898] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.6260929258838839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 963043.2271748716, 963043.2271748709, 216480.3294812276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [25.65, 68.5, 1.0, 2.0, 0.6503885145067244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1000715.377150935, 1000715.377150935, 221819.5131032153], 
processed observation next is [1.0, 0.6086956521739131, 0.41469194312796204, 0.685, 1.0, 1.0, 0.578781342779186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2779764936530375, 0.2779764936530375, 0.3310739001540527], 
reward next is 0.6689, 
noisyNet noise sample is [array([0.9247573], dtype=float32), 0.3414175]. 
=============================================
[2019-04-27 22:50:06,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.50352]
 [74.43651]
 [74.25008]
 [74.21912]
 [74.23076]], R is [[74.49768829]
 [74.42960358]
 [74.36290741]
 [74.2703476 ]
 [74.19052887]].
[2019-04-27 22:50:09,950] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.7514438e-28 1.0000000e+00 1.8426529e-37 1.1302250e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 22:50:09,962] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8153
[2019-04-27 22:50:09,968] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 88.0, 1.0, 2.0, 0.300321639261978, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479376.6590755205, 479376.6590755211, 165593.1023995398], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1119600.0000, 
sim time next is 1120200.0000, 
raw observation next is [21.25, 88.16666666666667, 1.0, 2.0, 0.2994230330085953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 478242.5696574055, 478242.5696574049, 165515.9688794796], 
processed observation next is [1.0, 1.0, 0.20616113744075834, 0.8816666666666667, 1.0, 1.0, 0.15593136507059674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1328451582381682, 0.13284515823816803, 0.24703875952161133], 
reward next is 0.7530, 
noisyNet noise sample is [array([0.44221598], dtype=float32), 0.26936382]. 
=============================================
[2019-04-27 22:50:10,227] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4060208e-21 1.0000000e+00 6.0267701e-30 3.4539797e-18 3.5840241e-36], sum to 1.0000
[2019-04-27 22:50:10,237] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6197
[2019-04-27 22:50:10,245] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666666, 87.66666666666666, 1.0, 2.0, 0.3188953490194287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511114.2906416905, 511114.2906416905, 167933.7240781819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1147800.0000, 
sim time next is 1148400.0000, 
raw observation next is [21.3, 87.0, 1.0, 2.0, 0.3343944523434787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535428.7799681944, 535428.7799681938, 169806.1478887032], 
processed observation next is [1.0, 0.30434782608695654, 0.2085308056872039, 0.87, 1.0, 1.0, 0.19806560523310687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1487302166578318, 0.14873021665783162, 0.2534420117741839], 
reward next is 0.7466, 
noisyNet noise sample is [array([0.5412766], dtype=float32), 0.112433575]. 
=============================================
[2019-04-27 22:50:23,962] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3231239e-26 1.0000000e+00 8.3811078e-38 8.3922774e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:50:23,973] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2353
[2019-04-27 22:50:23,979] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.91666666666667, 90.50000000000001, 1.0, 2.0, 0.5491828657643029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 856840.7394381252, 856840.7394381252, 202185.1770847914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1343400.0000, 
sim time next is 1344000.0000, 
raw observation next is [21.83333333333334, 90.0, 1.0, 2.0, 0.6095514060263363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954581.580616492, 954581.5806164913, 214705.5538799792], 
processed observation next is [1.0, 0.5652173913043478, 0.23380726698262277, 0.9, 1.0, 1.0, 0.5295800072606461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26516155017124776, 0.2651615501712476, 0.32045605056713317], 
reward next is 0.6795, 
noisyNet noise sample is [array([0.6810845], dtype=float32), 0.75697845]. 
=============================================
[2019-04-27 22:50:24,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.179985]
 [75.199745]
 [75.19558 ]
 [75.181015]
 [75.0797  ]], R is [[74.9353714 ]
 [74.88425446]
 [74.83015442]
 [74.78230286]
 [74.73131561]].
[2019-04-27 22:50:27,865] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 22:50:27,867] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:50:27,868] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:50:27,869] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:50:27,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:50:27,870] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:50:27,871] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:50:27,870] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:50:27,873] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:50:27,872] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:50:27,878] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:50:27,890] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run11
[2019-04-27 22:50:27,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run11
[2019-04-27 22:50:27,917] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run11
[2019-04-27 22:50:27,965] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run11
[2019-04-27 22:50:27,988] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run11
[2019-04-27 22:50:32,007] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:50:32,008] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.87891565833333, 94.77774938833333, 1.0, 2.0, 0.2963206018939999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477737.632590089, 477737.6325900896, 165499.3763063312]
[2019-04-27 22:50:32,010] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:50:32,014] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0602339e-25 1.0000000e+00 8.1172470e-36 8.9117787e-23 0.0000000e+00], sampled 0.526259314553745
[2019-04-27 22:51:01,874] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:51:01,877] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.77504282, 79.93531483, 1.0, 2.0, 0.5110948026217629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714180.7904668733, 714180.7904668739, 185352.9459248622]
[2019-04-27 22:51:01,877] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:51:01,879] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.13496338e-26 1.00000000e+00 3.49159565e-37 1.23247715e-23
 0.00000000e+00], sampled 0.6123183119035412
[2019-04-27 22:51:31,542] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:51:31,545] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.8, 51.0, 1.0, 2.0, 0.6077570388556609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 849306.2335182468, 849306.2335182462, 202198.9014716319]
[2019-04-27 22:51:31,547] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:51:31,550] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.5808735e-28 1.0000000e+00 0.0000000e+00 7.1852981e-25 0.0000000e+00], sampled 0.3653239902909936
[2019-04-27 22:51:41,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:51:41,044] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.86666666666667, 48.33333333333334, 1.0, 2.0, 0.7648003255824801, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005979329909211, 6.9112, 168.9123160187005, 1965811.262796897, 1898571.799069103, 399331.6119770745]
[2019-04-27 22:51:41,045] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:51:41,049] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.02243630e-26 1.00000000e+00 2.99035030e-37 1.13196436e-23
 0.00000000e+00], sampled 0.3208487466782497
[2019-04-27 22:51:41,049] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1965811.262796897 W.
[2019-04-27 22:51:56,734] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:51:56,736] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.63333333333334, 61.0, 1.0, 2.0, 0.8859448670367428, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986896612105, 6.9112, 168.9123159731043, 2135363.391750067, 2068118.559981321, 430183.0420037222]
[2019-04-27 22:51:56,736] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:51:56,742] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.8650869e-26 1.0000000e+00 1.9446199e-36 3.6634189e-23 0.0000000e+00], sampled 0.8641472381709336
[2019-04-27 22:51:56,742] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2135363.391750067 W.
[2019-04-27 22:52:22,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:52:22,732] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.642286825, 31.10357846, 1.0, 2.0, 0.2627643515248514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 429935.7443608632, 429935.7443608625, 162182.4275228721]
[2019-04-27 22:52:22,735] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:52:22,738] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0402469e-26 1.0000000e+00 3.6738905e-36 5.4152764e-23 0.0000000e+00], sampled 0.3243861898440089
[2019-04-27 22:52:27,268] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:52:27,268] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.16666666666666, 79.66666666666666, 1.0, 2.0, 0.5698536619690396, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796318.5432824278, 796318.5432824278, 195263.5846601297]
[2019-04-27 22:52:27,269] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:52:27,273] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0443102e-27 1.0000000e+00 1.1143730e-37 6.0128497e-24 0.0000000e+00], sampled 0.3237419436659523
[2019-04-27 22:52:33,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.1089668]
[2019-04-27 22:52:33,652] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.768822185, 97.669293045, 1.0, 2.0, 0.5624717112322732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785999.1233653859, 785999.1233653859, 193960.6396868755]
[2019-04-27 22:52:33,654] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:52:33,656] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.2888957e-28 1.0000000e+00 0.0000000e+00 1.0845865e-24 0.0000000e+00], sampled 0.5503173023705458
[2019-04-27 22:52:37,849] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:52:38,212] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:52:38,931] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:52:39,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:52:39,165] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:52:40,182] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 250000, evaluation results [250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:52:41,425] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7891925e-24 1.0000000e+00 5.2095823e-33 1.1664688e-21 0.0000000e+00], sum to 1.0000
[2019-04-27 22:52:41,440] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3642
[2019-04-27 22:52:41,446] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 72.33333333333334, 1.0, 2.0, 0.4279153722120074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 618066.4080205425, 618066.4080205419, 175668.1792200477], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1432200.0000, 
sim time next is 1432800.0000, 
raw observation next is [27.2, 71.0, 1.0, 2.0, 0.4270633068830873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617325.7241782949, 617325.7241782949, 175610.5870608765], 
processed observation next is [0.0, 0.6086956521739131, 0.4881516587677725, 0.71, 1.0, 1.0, 0.30971482756998475, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17147936782730414, 0.17147936782730414, 0.26210535382220373], 
reward next is 0.7379, 
noisyNet noise sample is [array([-0.25809923], dtype=float32), 0.4202527]. 
=============================================
[2019-04-27 22:52:48,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6365763e-25 1.0000000e+00 7.0606774e-36 5.9375661e-23 0.0000000e+00], sum to 1.0000
[2019-04-27 22:52:48,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7062
[2019-04-27 22:52:48,169] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 90.0, 1.0, 2.0, 0.3197523380591949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504207.7298161962, 504207.7298161956, 167312.8241175561], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1569600.0000, 
sim time next is 1570200.0000, 
raw observation next is [21.6, 89.83333333333333, 1.0, 2.0, 0.3108920714226966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490465.1565361891, 490465.1565361884, 166292.216691375], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.8983333333333333, 1.0, 1.0, 0.16974948364180312, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13624032126005253, 0.13624032126005234, 0.24819733834533583], 
reward next is 0.7518, 
noisyNet noise sample is [array([0.0503297], dtype=float32), 2.200106]. 
=============================================
[2019-04-27 22:52:48,486] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5412381e-25 1.0000000e+00 1.6017204e-35 2.5865310e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:52:48,494] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3392
[2019-04-27 22:52:48,504] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.93333333333334, 86.66666666666667, 1.0, 2.0, 0.3532044258487639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543859.5606279703, 543859.5606279703, 170128.1805058593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1545600.0000, 
sim time next is 1546200.0000, 
raw observation next is [22.85, 87.0, 1.0, 2.0, 0.3522002160636193, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542915.4779411874, 542915.4779411874, 170067.4820511354], 
processed observation next is [0.0, 0.9130434782608695, 0.28199052132701435, 0.87, 1.0, 1.0, 0.21951833260677023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15080985498366317, 0.15080985498366317, 0.25383206276288867], 
reward next is 0.7462, 
noisyNet noise sample is [array([-1.2131857], dtype=float32), 0.7948598]. 
=============================================
[2019-04-27 22:53:06,631] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.5523851e-21 1.0000000e+00 5.2956483e-30 3.5554601e-18 1.4374874e-36], sum to 1.0000
[2019-04-27 22:53:06,641] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0261
[2019-04-27 22:53:06,645] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.81666666666667, 97.66666666666667, 1.0, 2.0, 0.3535136926087159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540522.7813059741, 540522.7813059747, 169734.2354439703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1831800.0000, 
sim time next is 1832400.0000, 
raw observation next is [21.8, 98.0, 1.0, 2.0, 0.3550032613765913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542418.572054986, 542418.5720549854, 169879.2927229101], 
processed observation next is [1.0, 0.21739130434782608, 0.23222748815165886, 0.98, 1.0, 1.0, 0.2228954956344473, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15067182557082945, 0.15067182557082928, 0.25355118316852254], 
reward next is 0.7464, 
noisyNet noise sample is [array([-0.46041492], dtype=float32), 0.9219922]. 
=============================================
[2019-04-27 22:53:12,172] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.4240565e-18 1.0000000e+00 5.0030699e-27 7.2481028e-11 1.5982987e-32], sum to 1.0000
[2019-04-27 22:53:12,179] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8267
[2019-04-27 22:53:12,188] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1772912.49884446 W.
[2019-04-27 22:53:12,195] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.6, 78.0, 1.0, 2.0, 0.6340707100194273, 1.0, 1.0, 0.6340707100194273, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1772912.49884446, 1772912.49884446, 347163.2593045463], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1942200.0000, 
sim time next is 1942800.0000, 
raw observation next is [26.66666666666667, 77.66666666666666, 1.0, 2.0, 0.6257828619230003, 1.0, 2.0, 0.6257828619230003, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1749720.091868918, 1749720.091868919, 343956.4253537962], 
processed observation next is [1.0, 0.4782608695652174, 0.4628751974723541, 0.7766666666666666, 1.0, 1.0, 0.5491359782204822, 1.0, 1.0, 0.5491359782204822, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4860333588524772, 0.4860333588524775, 0.5133677990355168], 
reward next is 0.4866, 
noisyNet noise sample is [array([0.72304404], dtype=float32), -0.46101007]. 
=============================================
[2019-04-27 22:53:12,671] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0540242e-20 1.0000000e+00 1.7305646e-28 3.4830372e-17 1.0099886e-34], sum to 1.0000
[2019-04-27 22:53:12,679] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9414
[2019-04-27 22:53:12,687] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 87.0, 1.0, 2.0, 0.4274254435734332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617571.7663213384, 617571.7663213384, 175626.4153832308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1926000.0000, 
sim time next is 1926600.0000, 
raw observation next is [24.91666666666667, 86.33333333333333, 1.0, 2.0, 0.482797476369263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696694.9731691432, 696694.9731691432, 183750.4049189779], 
processed observation next is [1.0, 0.30434782608695654, 0.37993680884676173, 0.8633333333333333, 1.0, 1.0, 0.37686442936055786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1935263814358731, 0.1935263814358731, 0.27425433569996704], 
reward next is 0.7257, 
noisyNet noise sample is [array([1.4920288], dtype=float32), -0.505891]. 
=============================================
[2019-04-27 22:53:18,478] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1460343e-21 1.0000000e+00 3.8127703e-30 6.7330393e-18 2.3919519e-35], sum to 1.0000
[2019-04-27 22:53:18,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8566
[2019-04-27 22:53:18,494] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.5, 1.0, 2.0, 0.5047805089321843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705354.5522846185, 705354.5522846185, 184348.6271707236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [26.63333333333333, 85.66666666666666, 1.0, 2.0, 0.5032134804873083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703164.1418835776, 703164.1418835782, 184101.2471757254], 
processed observation next is [0.0, 0.6956521739130435, 0.46129541864139006, 0.8566666666666666, 1.0, 1.0, 0.40146202468350395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1953233727454382, 0.19532337274543837, 0.27477798085929167], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.10500257], dtype=float32), -0.696414]. 
=============================================
[2019-04-27 22:53:19,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.7939079e-23 1.0000000e+00 2.0724411e-34 5.6405136e-18 0.0000000e+00], sum to 1.0000
[2019-04-27 22:53:19,409] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1333
[2019-04-27 22:53:19,419] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 91.5, 1.0, 2.0, 0.4756561449921946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664644.9191639184, 664644.9191639189, 179869.5470382831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2064600.0000, 
sim time next is 2065200.0000, 
raw observation next is [24.96666666666667, 91.66666666666666, 1.0, 2.0, 0.4765954530357104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 665957.8466101834, 665957.8466101841, 180009.9954166866], 
processed observation next is [0.0, 0.9130434782608695, 0.3823064770932071, 0.9166666666666665, 1.0, 1.0, 0.3693921120912173, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18498829072505094, 0.18498829072505113, 0.2686716349502785], 
reward next is 0.7313, 
noisyNet noise sample is [array([-1.9834977], dtype=float32), -1.2696736]. 
=============================================
[2019-04-27 22:53:20,792] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.6130799e-22 1.0000000e+00 8.6497238e-31 1.2296732e-16 8.7625190e-36], sum to 1.0000
[2019-04-27 22:53:20,799] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-27 22:53:20,807] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 88.0, 1.0, 2.0, 0.4859981027887919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679100.5861074259, 679100.5861074264, 181431.3260307892], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2055600.0000, 
sim time next is 2056200.0000, 
raw observation next is [25.73333333333333, 88.16666666666667, 1.0, 2.0, 0.484744198171802, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677347.9067545087, 677347.9067545087, 181240.2720779809], 
processed observation next is [0.0, 0.8260869565217391, 0.41864139020537117, 0.8816666666666667, 1.0, 1.0, 0.3792098773154241, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18815219632069688, 0.18815219632069688, 0.2705078687731058], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.9522085], dtype=float32), 0.0063036857]. 
=============================================
[2019-04-27 22:53:25,797] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4336088e-21 1.0000000e+00 5.3323444e-28 9.2023413e-18 6.7685226e-34], sum to 1.0000
[2019-04-27 22:53:25,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5853
[2019-04-27 22:53:25,815] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 94.0, 1.0, 2.0, 0.5066144268144847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707918.0295500153, 707918.029550016, 184638.4407051234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2167200.0000, 
sim time next is 2167800.0000, 
raw observation next is [25.25, 94.16666666666667, 1.0, 2.0, 1.030350846668959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1440257.792198818, 1440257.792198818, 308307.3099489983], 
processed observation next is [1.0, 0.08695652173913043, 0.39573459715639814, 0.9416666666666668, 1.0, 1.0, 1.036567285143324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4000716089441161, 0.4000716089441161, 0.4601601641029826], 
reward next is 0.5398, 
noisyNet noise sample is [array([0.07937536], dtype=float32), -0.019580035]. 
=============================================
[2019-04-27 22:53:34,014] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.1495037e-16 1.0000000e+00 5.6144379e-23 8.2134386e-12 1.1082782e-27], sum to 1.0000
[2019-04-27 22:53:34,024] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7709
[2019-04-27 22:53:34,036] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2033723.164829981 W.
[2019-04-27 22:53:34,042] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.01666666666667, 62.83333333333333, 1.0, 2.0, 0.7272595168366665, 1.0, 2.0, 0.7272595168366665, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2033723.164829981, 2033723.164829981, 386014.2515442123], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2289000.0000, 
sim time next is 2289600.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.9782439246834848, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.998304663232206, 6.9112, 168.9124388168266, 2264550.82117962, 2202755.964564364, 456974.0066842018], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.63, 1.0, 1.0, 0.9737878610644395, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008710466323220612, 0.0, 0.8294374030397039, 0.6290418947721167, 0.6118766568234344, 0.6820507562450774], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.551434], dtype=float32), -0.75033396]. 
=============================================
[2019-04-27 22:53:38,074] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2365652e-15 1.0000000e+00 2.6850547e-23 1.4690620e-12 7.1077822e-28], sum to 1.0000
[2019-04-27 22:53:38,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7654
[2019-04-27 22:53:38,100] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2040750.837324067 W.
[2019-04-27 22:53:38,106] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.7, 68.0, 1.0, 2.0, 0.4865134810025037, 1.0, 2.0, 0.4865134810025037, 1.0, 1.0, 0.8400578725684659, 6.911200000000001, 6.9112, 170.5573041426782, 2040750.837324067, 2040750.837324067, 405094.8392424893], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2366400.0000, 
sim time next is 2367000.0000, 
raw observation next is [30.85, 67.5, 1.0, 2.0, 0.5045489638428539, 1.0, 2.0, 0.5045489638428539, 1.0, 2.0, 0.875059086987114, 6.9112, 6.9112, 170.5573041426782, 2116478.001135462, 2116478.001135462, 418091.7760912491], 
processed observation next is [1.0, 0.391304347826087, 0.661137440758294, 0.675, 1.0, 1.0, 0.4030710407745227, 1.0, 1.0, 0.4030710407745227, 1.0, 1.0, 0.8476330329111146, 0.0, 0.0, 0.8375144448122397, 0.5879105558709616, 0.5879105558709616, 0.6240175762555956], 
reward next is 0.3760, 
noisyNet noise sample is [array([-0.6436847], dtype=float32), 0.3772523]. 
=============================================
[2019-04-27 22:53:38,121] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[50.91044 ]
 [52.73288 ]
 [53.815926]
 [55.066376]
 [56.348278]], R is [[49.77371597]
 [49.67136002]
 [49.59938431]
 [49.55102921]
 [49.0679245 ]].
[2019-04-27 22:53:39,184] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 22:53:39,187] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:53:39,189] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:53:39,190] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:53:39,191] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:53:39,193] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:53:39,195] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:53:39,197] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:53:39,198] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:53:39,192] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:53:39,207] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:53:39,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run12
[2019-04-27 22:53:39,246] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run12
[2019-04-27 22:53:39,247] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run12
[2019-04-27 22:53:39,296] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run12
[2019-04-27 22:53:39,321] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run12
[2019-04-27 22:53:51,736] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:53:51,739] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.85, 74.0, 1.0, 2.0, 0.3727795936317874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587264.2082583966, 587264.2082583966, 174065.416617077]
[2019-04-27 22:53:51,741] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:53:51,744] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.5220938e-16 1.0000000e+00 9.8735456e-24 5.7883628e-12 2.1186987e-28], sampled 0.34584172225469223
[2019-04-27 22:53:55,824] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:53:55,825] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.7, 83.33333333333334, 1.0, 2.0, 0.2953547423128546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473647.7973442443, 473647.7973442437, 165208.9065330052]
[2019-04-27 22:53:55,826] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:53:55,828] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2817503e-16 1.0000000e+00 8.8781144e-24 5.4996519e-12 1.8639766e-28], sampled 0.41306797190050226
[2019-04-27 22:54:03,578] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:03,581] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.56666666666667, 78.66666666666667, 1.0, 2.0, 0.4147886821953669, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608421.2986899934, 608421.2986899934, 175021.3092074438]
[2019-04-27 22:54:03,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:54:03,585] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.1683477e-17 1.0000000e+00 4.0752678e-25 1.2160336e-12 4.6049775e-30], sampled 0.9305476068458587
[2019-04-27 22:54:08,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:08,916] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.28333333333333, 86.0, 1.0, 2.0, 0.821982847268113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1170335.615647305, 1170335.615647304, 252428.626704863]
[2019-04-27 22:54:08,919] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:54:08,924] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.3562091e-16 1.0000000e+00 1.8481851e-23 7.8569694e-12 4.4987884e-28], sampled 0.6536133514116855
[2019-04-27 22:54:19,066] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:19,069] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.61666666666667, 65.83333333333333, 1.0, 2.0, 0.6994755802042112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059751.117565753, 1059751.117565753, 231252.8974881658]
[2019-04-27 22:54:19,072] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:54:19,076] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9727871e-16 1.0000000e+00 7.6672641e-24 5.1103952e-12 1.5636495e-28], sampled 0.28678445386777085
[2019-04-27 22:54:19,805] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:19,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 94.0, 1.0, 2.0, 0.3700343603149981, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561406.0748820656, 561406.0748820656, 171365.3923998176]
[2019-04-27 22:54:19,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:54:19,811] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1342702e-16 1.0000000e+00 4.6712509e-24 4.0153133e-12 8.6166021e-29], sampled 0.8534116806987152
[2019-04-27 22:54:27,810] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:27,812] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.72162192, 91.6982066, 1.0, 2.0, 0.3138772377496244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500394.2290068078, 500394.2290068078, 167115.8972008181]
[2019-04-27 22:54:27,814] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:54:27,817] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.2244104e-17 1.0000000e+00 7.4168347e-25 1.6298543e-12 9.4557232e-30], sampled 0.9998566511958124
[2019-04-27 22:54:27,914] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:27,915] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.5, 72.0, 1.0, 2.0, 0.5047786781593409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705351.9932066245, 705351.9932066251, 184347.6148695407]
[2019-04-27 22:54:27,917] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:54:27,923] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.4534532e-17 1.0000000e+00 9.7130895e-25 1.8595282e-12 1.3073399e-29], sampled 0.8118665646547973
[2019-04-27 22:54:41,469] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:41,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.5, 63.5, 1.0, 2.0, 1.004620037061242, 1.0, 2.0, 0.8229000580448834, 1.0, 1.0, 1.03, 7.005121763465232, 6.9112, 170.5573041426782, 3453762.8139474, 3386482.834284184, 634832.751195673]
[2019-04-27 22:54:41,471] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:54:41,479] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.28511099e-11 1.00000000e+00 6.41743925e-17 1.25523325e-08
 3.24653042e-20], sampled 0.7807303431475697
[2019-04-27 22:54:41,481] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3453762.8139474 W.
[2019-04-27 22:54:55,091] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:54:55,097] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.201090615, 57.396333195, 1.0, 2.0, 0.5813615368233176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812405.8992412287, 812405.8992412287, 197325.5481754395]
[2019-04-27 22:54:55,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:54:55,104] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8427517e-17 1.0000000e+00 5.1010320e-25 1.3565136e-12 6.0322902e-30], sampled 0.5866474973060958
[2019-04-27 22:55:12,862] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:55:12,863] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.66666666666667, 82.33333333333334, 1.0, 2.0, 0.6368676089674078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 890003.6602019869, 890003.6602019863, 207815.6355489795]
[2019-04-27 22:55:12,863] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:55:12,867] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4715914e-16 1.0000000e+00 2.6845106e-24 3.0548873e-12 4.4324700e-29], sampled 0.1694046631980849
[2019-04-27 22:55:14,262] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:55:14,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.69414231, 87.41344083666667, 1.0, 2.0, 0.5278108774063409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 737547.1874819302, 737547.1874819297, 188067.4261013435]
[2019-04-27 22:55:14,267] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:55:14,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1602758e-17 1.0000000e+00 1.5269114e-25 7.5182764e-13 1.4163830e-30], sampled 0.3668696437427853
[2019-04-27 22:55:15,644] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:55:15,645] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.55, 62.16666666666667, 1.0, 2.0, 0.8877548980970035, 1.0, 2.0, 0.8877548980970035, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2483011.147169054, 2483011.147169055, 464855.742238818]
[2019-04-27 22:55:15,646] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:55:15,648] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4844430e-13 1.0000000e+00 8.2111802e-20 4.7779847e-10 1.0852493e-23], sampled 0.2596837722383952
[2019-04-27 22:55:15,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2483011.147169054 W.
[2019-04-27 22:55:30,874] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:55:30,875] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.11666666666667, 79.5, 1.0, 2.0, 0.5390430917658303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753248.3140870406, 753248.31408704, 189938.4198542496]
[2019-04-27 22:55:30,878] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:55:30,881] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1396117e-17 1.0000000e+00 2.6691821e-25 9.8825689e-13 2.7705843e-30], sampled 0.2095766494375626
[2019-04-27 22:55:45,208] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.10090149]
[2019-04-27 22:55:45,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.13569073, 71.51162885, 1.0, 2.0, 0.4687686846403498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655017.9504274298, 655017.9504274292, 178847.772105242]
[2019-04-27 22:55:45,209] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:55:45,211] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.4344843e-17 1.0000000e+00 9.6716792e-25 1.8567001e-12 1.3003963e-29], sampled 0.5591870624577918
[2019-04-27 22:55:48,767] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:55:49,972] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.1701 3164000012.1395 1778.0000
[2019-04-27 22:55:50,167] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:55:50,221] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:55:50,327] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7724 2842493333.3205 1131.0000
[2019-04-27 22:55:51,345] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 275000, evaluation results [275000.0, 7884.170123390672, 3164000012.13951, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8496.772351911874, 2842493333.320488, 1131.0]
[2019-04-27 22:55:54,028] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2136999e-17 1.0000000e+00 9.5065165e-27 4.1541295e-11 3.8519163e-32], sum to 1.0000
[2019-04-27 22:55:54,038] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3425
[2019-04-27 22:55:54,041] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.55, 79.0, 1.0, 2.0, 0.5718864987981999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799160.316084342, 799160.316084342, 195625.3288737926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2413800.0000, 
sim time next is 2414400.0000, 
raw observation next is [29.46666666666667, 79.33333333333333, 1.0, 2.0, 0.5719913985712006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799306.9593057355, 799306.9593057355, 195643.918306955], 
processed observation next is [1.0, 0.9565217391304348, 0.5955766192733019, 0.7933333333333333, 1.0, 1.0, 0.4843269862303622, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22202971091825988, 0.22202971091825988, 0.2920058482193358], 
reward next is 0.7080, 
noisyNet noise sample is [array([-1.025031], dtype=float32), -0.2643432]. 
=============================================
[2019-04-27 22:56:00,346] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.1354414e-22 1.0000000e+00 2.6001203e-32 9.3854628e-16 0.0000000e+00], sum to 1.0000
[2019-04-27 22:56:00,356] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8892
[2019-04-27 22:56:00,362] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.96666666666667, 93.0, 1.0, 2.0, 0.5563324514544964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777416.9696505311, 777416.9696505311, 192892.673262696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2492400.0000, 
sim time next is 2493000.0000, 
raw observation next is [26.95, 93.0, 1.0, 2.0, 0.5557684444988097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776628.5402283682, 776628.5402283682, 192794.9312219789], 
processed observation next is [1.0, 0.8695652173913043, 0.476303317535545, 0.93, 1.0, 1.0, 0.4647812584323008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21573015006343563, 0.21573015006343563, 0.28775362868952076], 
reward next is 0.7122, 
noisyNet noise sample is [array([-0.98638856], dtype=float32), -1.0066454]. 
=============================================
[2019-04-27 22:56:00,383] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[79.51505 ]
 [79.55272 ]
 [79.797554]
 [79.39374 ]
 [79.08527 ]], R is [[79.7958374 ]
 [79.7099762 ]
 [79.62525177]
 [79.54162598]
 [79.45896912]].
[2019-04-27 22:56:00,425] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.0445804e-19 1.0000000e+00 3.9025790e-29 2.9183134e-13 9.7846551e-37], sum to 1.0000
[2019-04-27 22:56:00,432] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9393
[2019-04-27 22:56:00,439] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 95.5, 1.0, 2.0, 0.9791897872384039, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564263171, 1368697.103232593, 1368697.103232592, 292650.1005221762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2514600.0000, 
sim time next is 2515200.0000, 
raw observation next is [26.33333333333333, 95.66666666666667, 1.0, 2.0, 0.8756640798677482, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.91295651041, 1223906.980113238, 1223906.980113237, 263331.8880248092], 
processed observation next is [1.0, 0.08695652173913043, 0.44707740916271704, 0.9566666666666667, 1.0, 1.0, 0.8501976865876484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451521995, 0.3399741611425661, 0.3399741611425658, 0.39303266869374504], 
reward next is 0.6070, 
noisyNet noise sample is [array([0.02862917], dtype=float32), -1.9851785]. 
=============================================
[2019-04-27 22:56:04,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.0587960e-18 1.0000000e+00 1.1943275e-27 2.1548511e-14 2.1087515e-33], sum to 1.0000
[2019-04-27 22:56:04,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7143
[2019-04-27 22:56:04,891] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 91.16666666666667, 1.0, 2.0, 0.5060612323874153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707144.765629216, 707144.7656292167, 184550.80795788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2589000.0000, 
sim time next is 2589600.0000, 
raw observation next is [25.6, 91.33333333333334, 1.0, 2.0, 0.5035242918282284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703598.5971951223, 703598.5971951229, 184149.6919935926], 
processed observation next is [1.0, 1.0, 0.4123222748815167, 0.9133333333333334, 1.0, 1.0, 0.40183649617858846, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19544405477642285, 0.19544405477642302, 0.2748502865576009], 
reward next is 0.7251, 
noisyNet noise sample is [array([1.945133], dtype=float32), -0.3817835]. 
=============================================
[2019-04-27 22:56:05,054] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.6918699e-18 1.0000000e+00 2.0891024e-26 1.1244585e-12 4.8420129e-32], sum to 1.0000
[2019-04-27 22:56:05,060] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9461
[2019-04-27 22:56:05,069] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.76666666666667, 78.5, 1.0, 2.0, 0.5164667385774078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721689.8410657181, 721689.8410657181, 186218.1996131859], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2569800.0000, 
sim time next is 2570400.0000, 
raw observation next is [28.7, 79.0, 1.0, 2.0, 0.522806852253863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730552.3073473206, 730552.3073473212, 187248.0963011619], 
processed observation next is [1.0, 0.782608695652174, 0.5592417061611374, 0.79, 1.0, 1.0, 0.42506849669140123, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20293119648536684, 0.202931196485367, 0.2794747705987491], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.8684409], dtype=float32), -0.91869354]. 
=============================================
[2019-04-27 22:56:06,140] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.4520266e-21 1.0000000e+00 5.6995425e-32 6.4152710e-14 1.4186002e-37], sum to 1.0000
[2019-04-27 22:56:06,147] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0740
[2019-04-27 22:56:06,153] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.51666666666667, 85.33333333333334, 1.0, 2.0, 0.5366543953401602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 749909.2179798448, 749909.2179798455, 189538.286373994], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2578200.0000, 
sim time next is 2578800.0000, 
raw observation next is [27.43333333333334, 85.66666666666667, 1.0, 2.0, 0.5364686463239524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749649.5647398575, 749649.5647398575, 189507.0929359912], 
processed observation next is [1.0, 0.8695652173913043, 0.49921011058451853, 0.8566666666666667, 1.0, 1.0, 0.4415284895469306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20823599020551595, 0.20823599020551595, 0.28284640736715105], 
reward next is 0.7172, 
noisyNet noise sample is [array([1.0851313], dtype=float32), 0.9284888]. 
=============================================
[2019-04-27 22:56:14,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.3903270e-26 1.0000000e+00 3.6035732e-36 9.8738720e-19 0.0000000e+00], sum to 1.0000
[2019-04-27 22:56:14,920] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5270
[2019-04-27 22:56:14,924] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 99.0, 1.0, 2.0, 0.3832775035394224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576025.5661270086, 576025.5661270086, 172481.3179557139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2749800.0000, 
sim time next is 2750400.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3812954273964836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573988.3278223346, 573988.3278223339, 172329.3308357447], 
processed observation next is [0.0, 0.8695652173913043, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2545728040921489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15944120217287072, 0.15944120217287053, 0.25720795647126077], 
reward next is 0.7428, 
noisyNet noise sample is [array([0.23943324], dtype=float32), 0.5317554]. 
=============================================
[2019-04-27 22:56:16,428] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5735761e-21 1.0000000e+00 3.8661240e-31 1.0329046e-16 2.6261894e-38], sum to 1.0000
[2019-04-27 22:56:16,438] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6936
[2019-04-27 22:56:16,447] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3900876994207442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582070.2072966028, 582070.2072966034, 172894.7167615541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2743200.0000, 
sim time next is 2743800.0000, 
raw observation next is [23.0, 94.00000000000001, 1.0, 2.0, 0.3919668394299962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584875.186179163, 584875.186179163, 173149.4248793958], 
processed observation next is [0.0, 0.782608695652174, 0.28909952606635075, 0.9400000000000002, 1.0, 1.0, 0.26742992702409185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16246532949421197, 0.16246532949421197, 0.258431977431934], 
reward next is 0.7416, 
noisyNet noise sample is [array([1.4218088], dtype=float32), 0.8669326]. 
=============================================
[2019-04-27 22:56:24,544] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2506169e-21 1.0000000e+00 1.0612357e-28 1.2684617e-14 4.0915405e-35], sum to 1.0000
[2019-04-27 22:56:24,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5333
[2019-04-27 22:56:24,562] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.6897703147367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062221.276120694, 1062221.276120694, 230933.8717244141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2909400.0000, 
sim time next is 2910000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6798689586545278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1046995.521519257, 1046995.521519256, 228620.2719340856], 
processed observation next is [1.0, 0.6956521739130435, 0.2417061611374408, 0.94, 1.0, 1.0, 0.614299950186178, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2908320893109047, 0.2908320893109045, 0.3412242864687845], 
reward next is 0.6588, 
noisyNet noise sample is [array([-1.2311566], dtype=float32), -0.44561023]. 
=============================================
[2019-04-27 22:56:24,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[68.455414]
 [68.44783 ]
 [68.36258 ]
 [68.32024 ]
 [68.31061 ]], R is [[68.49681091]
 [68.46716309]
 [68.44509125]
 [68.41724396]
 [68.3877182 ]].
[2019-04-27 22:56:29,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.8495685e-17 1.0000000e+00 3.6988719e-25 1.2877713e-14 6.8029050e-32], sum to 1.0000
[2019-04-27 22:56:29,053] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4070
[2019-04-27 22:56:29,061] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3099701987121949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490351.5515316635, 490351.5515316641, 166313.7923316537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2948400.0000, 
sim time next is 2949000.0000, 
raw observation next is [21.0, 94.00000000000001, 1.0, 2.0, 0.3110690049642909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 491912.914583405, 491912.914583405, 166424.9596614277], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.9400000000000002, 1.0, 1.0, 0.169962656583483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13664247627316806, 0.13664247627316806, 0.2483954621812354], 
reward next is 0.7516, 
noisyNet noise sample is [array([-1.0685588], dtype=float32), -0.22285087]. 
=============================================
[2019-04-27 22:56:29,087] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.660553]
 [61.6304  ]
 [61.78995 ]
 [61.826466]
 [61.91157 ]], R is [[61.74595642]
 [61.8802681 ]
 [62.01361847]
 [62.14546585]
 [62.2765274 ]].
[2019-04-27 22:56:29,562] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3825198e-18 1.0000000e+00 6.2135751e-30 3.2533481e-13 5.4659433e-36], sum to 1.0000
[2019-04-27 22:56:29,571] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4017
[2019-04-27 22:56:29,577] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3071500491668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485706.0995626082, 485706.0995626082, 165969.880051621], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2959200.0000, 
sim time next is 2959800.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3258918714676126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 514351.9279243697, 514351.9279243691, 168095.8772717536], 
processed observation next is [1.0, 0.2608695652173913, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18782153188868989, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14287553553454713, 0.142875535534547, 0.2508893690623188], 
reward next is 0.7491, 
noisyNet noise sample is [array([0.64023274], dtype=float32), -0.051416125]. 
=============================================
[2019-04-27 22:56:30,812] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.5977438e-22 1.0000000e+00 4.2722790e-32 2.3951707e-16 0.0000000e+00], sum to 1.0000
[2019-04-27 22:56:30,818] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6220
[2019-04-27 22:56:30,823] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 90.0, 1.0, 2.0, 0.5545332603137373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864787.5775435641, 864787.5775435634, 203176.4031333039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2972400.0000, 
sim time next is 2973000.0000, 
raw observation next is [22.0, 89.0, 1.0, 2.0, 0.5395591959381556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844189.1259799454, 844189.1259799454, 200583.1075371584], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.89, 1.0, 1.0, 0.4452520432989826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23449697943887374, 0.23449697943887374, 0.29937777244352], 
reward next is 0.7006, 
noisyNet noise sample is [array([-0.5225615], dtype=float32), 1.2563583]. 
=============================================
[2019-04-27 22:56:30,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.64488 ]
 [75.59278 ]
 [75.62902 ]
 [75.67032 ]
 [75.639915]], R is [[75.61579132]
 [75.55638885]
 [75.48731232]
 [75.42525482]
 [75.3729248 ]].
[2019-04-27 22:56:32,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2860761e-20 1.0000000e+00 1.3188593e-31 1.8158553e-17 6.0563895e-36], sum to 1.0000
[2019-04-27 22:56:32,095] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4147
[2019-04-27 22:56:32,105] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.6067918806907895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 959263.9464168317, 959263.9464168324, 214947.5562249226], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2993400.0000, 
sim time next is 2994000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5611963101398872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887170.2215449043, 887170.2215449043, 205581.7588431406], 
processed observation next is [1.0, 0.6521739130434783, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4713208555902256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2464361726513623, 0.2464361726513623, 0.3068384460345382], 
reward next is 0.6932, 
noisyNet noise sample is [array([0.10062031], dtype=float32), -0.9065395]. 
=============================================
[2019-04-27 22:56:32,127] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.53306]
 [68.56906]
 [68.5062 ]
 [68.63563]
 [68.5836 ]], R is [[68.59494019]
 [68.58817291]
 [68.59295654]
 [68.57791901]
 [68.58865356]].
[2019-04-27 22:56:43,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.2834980e-22 1.0000000e+00 5.6040347e-32 3.6110648e-18 2.1461426e-37], sum to 1.0000
[2019-04-27 22:56:43,646] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5306
[2019-04-27 22:56:43,653] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4551870254107773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 644494.1438965485, 644494.1438965485, 177965.0173941553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3214800.0000, 
sim time next is 3215400.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4557218816054515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645251.6411246454, 645251.6411246454, 178042.8757508932], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.34424323084994163, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17923656697906815, 0.17923656697906815, 0.26573563544909434], 
reward next is 0.7343, 
noisyNet noise sample is [array([0.18533976], dtype=float32), -0.4136448]. 
=============================================
[2019-04-27 22:56:50,204] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 22:56:50,206] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 22:56:50,207] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 22:56:50,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:56:50,209] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 22:56:50,210] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 22:56:50,208] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 22:56:50,212] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:56:50,213] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:56:50,210] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:56:50,213] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 22:56:50,239] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run13
[2019-04-27 22:56:50,239] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run13
[2019-04-27 22:56:50,240] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run13
[2019-04-27 22:56:50,323] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run13
[2019-04-27 22:56:50,346] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run13
[2019-04-27 22:57:00,433] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:00,434] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.26666666666667, 61.0, 1.0, 2.0, 0.2652640663338414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 436684.6447625969, 436684.6447625976, 162443.3665428396]
[2019-04-27 22:57:00,434] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 22:57:00,437] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8281401e-20 1.0000000e+00 8.7434842e-29 5.8704947e-16 1.6449109e-34], sampled 0.9760621559823139
[2019-04-27 22:57:02,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:02,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.45, 74.0, 1.0, 2.0, 0.3045696286662054, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484052.6319105866, 484052.6319105866, 165897.9352543763]
[2019-04-27 22:57:02,901] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:57:02,907] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.5864450e-20 1.0000000e+00 2.1018489e-29 2.7061546e-16 2.9639599e-35], sampled 0.961677829604381
[2019-04-27 22:57:07,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:07,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.31000696666667, 97.61696195666667, 1.0, 2.0, 0.3178569206549926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504557.3110176794, 504557.3110176788, 167399.7745753368]
[2019-04-27 22:57:07,658] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:57:07,660] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5316815e-20 1.0000000e+00 9.7439777e-30 1.7817740e-16 1.1753894e-35], sampled 0.3593838352197747
[2019-04-27 22:57:10,512] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:10,512] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.5, 75.5, 1.0, 2.0, 0.3516749710018386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 542032.8963588405, 542032.8963588398, 169992.5359783983]
[2019-04-27 22:57:10,512] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 22:57:10,520] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.31713142e-21 1.00000000e+00 3.97543651e-30 1.09476256e-16
 4.00003745e-36], sampled 0.834687842600309
[2019-04-27 22:57:11,817] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:11,819] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.94465016833333, 77.23582806499999, 1.0, 2.0, 0.617129464569581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 862408.9880115483, 862408.9880115477, 203972.7353692742]
[2019-04-27 22:57:11,821] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 22:57:11,826] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.60742503e-20 1.00000000e+00 1.04550386e-29 1.85159311e-16
 1.27973367e-35], sampled 0.47989938266550125
[2019-04-27 22:57:12,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:57:12,432] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.90431849, 86.56024425499999, 1.0, 2.0, 0.485531701657462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678448.6608768232, 678448.6608768232, 181360.0102194506]
[2019-04-27 22:57:12,433] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:57:12,435] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2780944e-20 1.0000000e+00 7.4653837e-30 1.5421439e-16 8.5380121e-36], sampled 0.4780546136392355
[2019-04-27 22:58:37,573] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:58:37,576] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.24494366333333, 90.38636771, 1.0, 2.0, 0.8222839597673192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1149257.739012124, 1149257.739012123, 249460.8585483844]
[2019-04-27 22:58:37,577] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 22:58:37,580] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6129408e-20 1.0000000e+00 1.0508539e-29 1.8563669e-16 1.2876470e-35], sampled 0.7278266619440372
[2019-04-27 22:58:52,743] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([0.00098259], dtype=float32), 0.09012548]
[2019-04-27 22:58:52,744] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.90000000000001, 64.5, 1.0, 2.0, 0.7322485511757826, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977297133454, 6.9112, 168.9123160308289, 1920256.893494224, 1853018.871877218, 391812.916094439]
[2019-04-27 22:58:52,746] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 22:58:52,749] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6040602e-15 1.0000000e+00 2.2469515e-22 1.9146774e-12 8.2083643e-27], sampled 0.7429053598397699
[2019-04-27 22:58:52,750] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1920256.893494224 W.
[2019-04-27 22:59:00,967] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 22:59:01,146] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 22:59:01,424] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 22:59:01,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 22:59:01,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 22:59:02,674] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 300000, evaluation results [300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 22:59:06,643] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7593315e-16 1.0000000e+00 2.0874386e-25 1.1945496e-14 6.7614877e-31], sum to 1.0000
[2019-04-27 22:59:06,656] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2381
[2019-04-27 22:59:06,661] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5231463325092446, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731026.848608408, 731026.848608408, 187301.7586755215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3358800.0000, 
sim time next is 3359400.0000, 
raw observation next is [27.83333333333334, 80.66666666666667, 1.0, 2.0, 0.5234511477541129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731452.9336891326, 731452.9336891333, 187351.8494333081], 
processed observation next is [0.0, 0.9130434782608695, 0.5181674565560824, 0.8066666666666668, 1.0, 1.0, 0.42584475633025654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20318137046920348, 0.20318137046920368, 0.2796296260198628], 
reward next is 0.7204, 
noisyNet noise sample is [array([0.35680404], dtype=float32), -0.09352373]. 
=============================================
[2019-04-27 22:59:09,232] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1965274e-11 6.4627699e-07 1.3137018e-16 9.9999940e-01 4.8206187e-22], sum to 1.0000
[2019-04-27 22:59:09,244] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9448
[2019-04-27 22:59:09,255] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.9365516379373143, 1.0, 2.0, 0.9365516379373143, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2619636.543044004, 2619636.543044005, 491830.4304397139], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3424200.0000, 
sim time next is 3424800.0000, 
raw observation next is [33.66666666666667, 61.00000000000001, 1.0, 2.0, 0.9683558406946393, 1.0, 2.0, 0.9683558406946393, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2708692.784673672, 2708692.784673672, 510160.3717383039], 
processed observation next is [1.0, 0.6521739130434783, 0.7946287519747238, 0.6100000000000001, 1.0, 1.0, 0.9618745068610112, 1.0, 1.0, 0.9618745068610112, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7524146624093534, 0.7524146624093534, 0.7614333906541849], 
reward next is 0.2386, 
noisyNet noise sample is [array([-1.7483675], dtype=float32), 0.001399168]. 
=============================================
[2019-04-27 22:59:11,880] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.7950498e-12 6.9222288e-08 1.1722387e-17 9.9999988e-01 7.8152735e-22], sum to 1.0000
[2019-04-27 22:59:11,887] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4573
[2019-04-27 22:59:11,891] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.33333333333334, 62.0, 1.0, 2.0, 0.9531070259476129, 1.0, 2.0, 0.9531070259476129, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2665993.16615082, 2665993.166150821, 501296.8639532014], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3426000.0000, 
sim time next is 3426600.0000, 
raw observation next is [33.16666666666666, 62.5, 1.0, 2.0, 0.9471126008234098, 1.0, 2.0, 0.9471126008234098, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2649208.02075383, 2649208.02075383, 497849.844173762], 
processed observation next is [1.0, 0.6521739130434783, 0.7709320695102682, 0.625, 1.0, 1.0, 0.9362802419559154, 1.0, 1.0, 0.9362802419559154, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7358911168760639, 0.7358911168760639, 0.7430594689160627], 
reward next is 0.2569, 
noisyNet noise sample is [array([-0.19574967], dtype=float32), -2.9826708]. 
=============================================
[2019-04-27 22:59:12,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.3793793e-16 1.0000000e+00 1.2685514e-25 6.8892107e-11 3.6028695e-30], sum to 1.0000
[2019-04-27 22:59:12,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6442
[2019-04-27 22:59:12,212] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5394196110894326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753774.6415356724, 753774.641535673, 190004.9813101035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3434400.0000, 
sim time next is 3435000.0000, 
raw observation next is [30.66666666666667, 70.66666666666667, 1.0, 2.0, 0.5429697960851061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758737.373915605, 758737.373915605, 190603.7892000398], 
processed observation next is [1.0, 0.782608695652174, 0.6524486571879939, 0.7066666666666667, 1.0, 1.0, 0.44936120010253744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2107603816432236, 0.2107603816432236, 0.284483267462746], 
reward next is 0.7155, 
noisyNet noise sample is [array([-0.11251368], dtype=float32), -2.3834455]. 
=============================================
[2019-04-27 22:59:12,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[57.97637 ]
 [57.69072 ]
 [57.197258]
 [56.480858]
 [55.051254]], R is [[58.16296387]
 [58.29774475]
 [58.43247986]
 [58.56700897]
 [58.70273972]].
[2019-04-27 22:59:13,360] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9623283e-14 9.9997449e-01 2.0371363e-23 2.5519963e-05 1.4555046e-29], sum to 1.0000
[2019-04-27 22:59:13,368] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3685
[2019-04-27 22:59:13,376] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5083189621411425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710300.659531922, 710300.6595319213, 184909.8645130012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3457800.0000, 
sim time next is 3458400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5081282849349131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 710034.1272554488, 710034.1272554483, 184879.5362175215], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4073834758251965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19723170201540247, 0.1972317020154023, 0.2759396062948082], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.37664092], dtype=float32), -0.09709357]. 
=============================================
[2019-04-27 22:59:16,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2850839e-15 1.0000000e+00 1.2448901e-23 6.3806626e-14 3.3711740e-27], sum to 1.0000
[2019-04-27 22:59:16,529] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2816
[2019-04-27 22:59:16,533] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5172682962910803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 722810.2866421308, 722810.2866421315, 186345.9626474338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3538200.0000, 
sim time next is 3538800.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5168245954510621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 722190.065849511, 722190.0658495104, 186274.2302118073], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.41786095837477355, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20060835162486415, 0.20060835162486398, 0.27802123912210047], 
reward next is 0.7220, 
noisyNet noise sample is [array([0.6225689], dtype=float32), 1.2126169]. 
=============================================
[2019-04-27 22:59:18,180] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3876909e-11 9.9999666e-01 1.2729470e-20 3.3428464e-06 1.3404612e-25], sum to 1.0000
[2019-04-27 22:59:18,192] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6240
[2019-04-27 22:59:18,202] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6083406038433897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850122.0590868837, 850122.0590868837, 202301.3287638593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3564000.0000, 
sim time next is 3564600.0000, 
raw observation next is [27.16666666666666, 79.00000000000001, 1.0, 2.0, 0.7646196907920382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1068623.241328951, 1068623.241328951, 235408.5105545231], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.7900000000000001, 1.0, 1.0, 0.7164092660145038, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29683978925804194, 0.29683978925804194, 0.3513559859022733], 
reward next is 0.6486, 
noisyNet noise sample is [array([-0.51455766], dtype=float32), 0.08143517]. 
=============================================
[2019-04-27 22:59:19,086] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.8112006e-09 2.5579301e-03 5.5219082e-15 9.9744213e-01 5.4807676e-18], sum to 1.0000
[2019-04-27 22:59:19,102] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1327
[2019-04-27 22:59:19,109] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 68.66666666666667, 1.0, 2.0, 0.793817284779101, 1.0, 2.0, 0.793817284779101, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2220038.422359318, 2220038.422359318, 416873.6553159138], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3580800.0000, 
sim time next is 3581400.0000, 
raw observation next is [31.0, 69.33333333333333, 1.0, 2.0, 0.7997649848601601, 1.0, 2.0, 0.7997649848601601, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2236687.011851042, 2236687.011851042, 419758.6698883608], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6933333333333332, 1.0, 1.0, 0.7587529938074218, 1.0, 1.0, 0.7587529938074218, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6213019477364006, 0.6213019477364006, 0.6265054774453146], 
reward next is 0.3735, 
noisyNet noise sample is [array([-0.26807097], dtype=float32), 1.807378]. 
=============================================
[2019-04-27 22:59:26,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0539167e-06 9.8372656e-01 1.0676187e-09 1.6269410e-02 7.2076897e-12], sum to 1.0000
[2019-04-27 22:59:26,025] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3606
[2019-04-27 22:59:26,031] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2276800.144049217 W.
[2019-04-27 22:59:26,036] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.33333333333334, 67.66666666666667, 1.0, 2.0, 0.5427300369332257, 1.0, 2.0, 0.5427300369332257, 1.0, 2.0, 0.9425425905871296, 6.9112, 6.9112, 170.5573041426782, 2276800.144049217, 2276800.144049217, 445952.4053789623], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3666000.0000, 
sim time next is 3666600.0000, 
raw observation next is [31.5, 66.5, 1.0, 2.0, 1.038522318789121, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.003635950987643, 6.9112, 168.912393116239, 2348922.192317982, 2283345.166014529, 475463.110856968], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.665, 1.0, 1.0, 1.0464124322760495, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009243595098764334, 0.0, 0.829437178628897, 0.652478386754995, 0.6342625461151469, 0.7096464341148776], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1948612], dtype=float32), -0.7886197]. 
=============================================
[2019-04-27 22:59:31,118] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5867513e-07 9.9999833e-01 7.6317896e-11 1.4848748e-06 3.0379503e-12], sum to 1.0000
[2019-04-27 22:59:31,127] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4956
[2019-04-27 22:59:31,136] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2796187.708145691 W.
[2019-04-27 22:59:31,142] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.33333333333334, 64.66666666666667, 1.0, 2.0, 0.9996002500419301, 1.0, 2.0, 0.9996002500419301, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2796187.708145691, 2796187.70814569, 528755.6398801985], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3763200.0000, 
sim time next is 3763800.0000, 
raw observation next is [34.5, 63.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.062311196041035, 6.9112, 170.5573041426782, 3017702.932939394, 2909455.836404748, 552868.9942330456], 
processed observation next is [1.0, 0.5652173913043478, 0.8341232227488152, 0.635, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.015111119604103518, 0.0, 0.8375144448122397, 0.8382508147053872, 0.8081821767790967, 0.8251776033329039], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25714296], dtype=float32), -1.7452924]. 
=============================================
[2019-04-27 22:59:31,179] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.8148395e-06 9.9875331e-01 7.5532185e-09 1.2418310e-03 6.0643733e-11], sum to 1.0000
[2019-04-27 22:59:31,189] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8614
[2019-04-27 22:59:31,194] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1942248.375110521 W.
[2019-04-27 22:59:31,204] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 67.66666666666667, 1.0, 2.0, 0.6945777519417182, 1.0, 1.0, 0.6945777519417182, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1942248.375110521, 1942248.375110521, 371805.2750897748], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3748800.0000, 
sim time next is 3749400.0000, 
raw observation next is [30.0, 66.5, 1.0, 2.0, 0.4517723307936564, 1.0, 2.0, 0.4517723307936564, 1.0, 1.0, 0.7694680061103015, 6.9112, 6.9112, 170.5573041426782, 1894895.207523942, 1894895.207523942, 380935.9013324768], 
processed observation next is [1.0, 0.391304347826087, 0.6208530805687204, 0.665, 1.0, 1.0, 0.33948473589597156, 1.0, 1.0, 0.33948473589597156, 1.0, 0.5, 0.7188634220857335, 0.0, 0.0, 0.8375144448122397, 0.5263597798677616, 0.5263597798677616, 0.5685610467648907], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.325181], dtype=float32), 0.8590769]. 
=============================================
[2019-04-27 22:59:32,507] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2677788e-17 1.0000000e+00 1.5815157e-26 1.2069206e-15 7.2554638e-29], sum to 1.0000
[2019-04-27 22:59:32,516] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5560
[2019-04-27 22:59:32,529] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 67.5, 1.0, 2.0, 0.5378671673391603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751604.5188493406, 751604.5188493399, 189742.479166377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3784200.0000, 
sim time next is 3784800.0000, 
raw observation next is [30.66666666666667, 69.0, 1.0, 2.0, 0.5395623729842705, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753974.2050768668, 753974.2050768662, 190027.7037936371], 
processed observation next is [1.0, 0.8260869565217391, 0.6524486571879939, 0.69, 1.0, 1.0, 0.44525587106538606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20943727918801855, 0.20943727918801838, 0.28362343849796584], 
reward next is 0.7164, 
noisyNet noise sample is [array([0.61805785], dtype=float32), 0.7498737]. 
=============================================
[2019-04-27 22:59:35,845] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.6623234e-26 1.0000000e+00 0.0000000e+00 7.8376666e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:59:35,854] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0368
[2019-04-27 22:59:35,863] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.33333333333334, 65.66666666666667, 1.0, 2.0, 0.6100629810045384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 852529.9510764782, 852529.9510764782, 202636.523260855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3838800.0000, 
sim time next is 3839400.0000, 
raw observation next is [33.5, 65.0, 1.0, 2.0, 0.6095542188354509, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 851818.6984207645, 851818.698420764, 202540.3727720526], 
processed observation next is [0.0, 0.43478260869565216, 0.7867298578199052, 0.65, 1.0, 1.0, 0.5295833961872902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23661630511687903, 0.23661630511687887, 0.30229906383888444], 
reward next is 0.6977, 
noisyNet noise sample is [array([0.16458246], dtype=float32), 0.59048504]. 
=============================================
[2019-04-27 22:59:39,851] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.1150148e-22 1.0000000e+00 2.4560228e-32 4.4031879e-18 5.3837940e-34], sum to 1.0000
[2019-04-27 22:59:39,864] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8068
[2019-04-27 22:59:39,871] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5821277182919646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813476.9861038653, 813476.9861038653, 197463.5189598995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3885000.0000, 
sim time next is 3885600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5829586620148277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814638.6088747102, 814638.6088747096, 197613.973473719], 
processed observation next is [0.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4975405566443707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22628850246519727, 0.2262885024651971, 0.29494622906525225], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.17124334], dtype=float32), -1.299023]. 
=============================================
[2019-04-27 22:59:39,981] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1232059e-20 1.0000000e+00 7.6886723e-32 9.6873322e-18 1.1673691e-34], sum to 1.0000
[2019-04-27 22:59:39,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8950
[2019-04-27 22:59:39,998] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5369604764834505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750337.0801989657, 750337.0801989657, 189589.4968473333], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3875400.0000, 
sim time next is 3876000.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5366007063392637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 749834.1676537087, 749834.1676537087, 189529.2308415456], 
processed observation next is [0.0, 0.8695652173913043, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4416875979991129, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20828726879269688, 0.20828726879269688, 0.2828794490172322], 
reward next is 0.7171, 
noisyNet noise sample is [array([0.4783453], dtype=float32), 0.056513086]. 
=============================================
[2019-04-27 22:59:40,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.33579 ]
 [61.351444]
 [61.35593 ]
 [61.36361 ]
 [61.325783]], R is [[61.40625   ]
 [61.50921631]
 [61.61086273]
 [61.71018219]
 [61.80899048]].
[2019-04-27 22:59:41,958] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0069407e-26 1.0000000e+00 0.0000000e+00 2.3696713e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 22:59:41,971] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1602
[2019-04-27 22:59:41,977] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 93.16666666666667, 1.0, 2.0, 0.5674161613056328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792911.0869564826, 792911.0869564819, 194831.853932362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3906600.0000, 
sim time next is 3907200.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.5635043300668936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 787442.641918431, 787442.6419184317, 194143.037041534], 
processed observation next is [0.0, 0.21739130434782608, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.47410160249023325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21873406719956417, 0.21873406719956437, 0.28976572692766267], 
reward next is 0.7102, 
noisyNet noise sample is [array([0.97773075], dtype=float32), -1.1766036]. 
=============================================
[2019-04-27 22:59:43,894] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4580343e-22 1.0000000e+00 5.3937843e-34 4.3704010e-20 1.0314097e-36], sum to 1.0000
[2019-04-27 22:59:43,904] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7351
[2019-04-27 22:59:43,908] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 61.0, 1.0, 2.0, 0.6050224737384846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845483.3117925103, 845483.3117925103, 201685.7757938854], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3951600.0000, 
sim time next is 3952200.0000, 
raw observation next is [34.0, 60.5, 1.0, 2.0, 0.6015151681422277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840580.1191057914, 840580.1191057914, 201028.9345735524], 
processed observation next is [0.0, 0.7391304347826086, 0.8104265402843602, 0.605, 1.0, 1.0, 0.519897792942443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2334944775293865, 0.2334944775293865, 0.3000431859306752], 
reward next is 0.7000, 
noisyNet noise sample is [array([-0.4876646], dtype=float32), -0.35246012]. 
=============================================
[2019-04-27 22:59:48,594] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0443753e-15 1.0000000e+00 6.3717005e-25 1.4509823e-10 8.5615617e-28], sum to 1.0000
[2019-04-27 22:59:48,605] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4062
[2019-04-27 22:59:48,614] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 79.0, 1.0, 2.0, 0.5629319470540625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786642.4959972055, 786642.4959972055, 194043.554331205], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4041600.0000, 
sim time next is 4042200.0000, 
raw observation next is [29.16666666666667, 79.0, 1.0, 2.0, 0.5587371736765008, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780778.5548874289, 780778.5548874289, 193310.5622956923], 
processed observation next is [1.0, 0.782608695652174, 0.581358609794629, 0.79, 1.0, 1.0, 0.4683580405740973, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21688293191317468, 0.21688293191317468, 0.2885232273070034], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.7585241], dtype=float32), 0.63603467]. 
=============================================
[2019-04-27 22:59:51,800] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5221592e-10 2.7126563e-01 4.9302348e-19 7.2873431e-01 4.6523596e-23], sum to 1.0000
[2019-04-27 22:59:51,805] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2305
[2019-04-27 22:59:51,818] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2787416.410930168 W.
[2019-04-27 22:59:51,822] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333333, 72.33333333333334, 1.0, 2.0, 0.9964681212859052, 1.0, 2.0, 0.9964681212859052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2787416.410930168, 2787416.410930168, 526860.1557291633], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4099800.0000, 
sim time next is 4100400.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 1.000045482953898, 1.0, 2.0, 1.000045482953898, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2797434.555310799, 2797434.555310799, 529017.6918732598], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.71, 1.0, 1.0, 1.0000547987396362, 1.0, 1.0, 1.0000547987396362, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7770651542529997, 0.7770651542529997, 0.789578644586955], 
reward next is 0.2104, 
noisyNet noise sample is [array([-0.01179092], dtype=float32), -1.1774285]. 
=============================================
[2019-04-27 22:59:53,203] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0892778e-26 1.0000000e+00 0.0000000e+00 3.3401553e-21 0.0000000e+00], sum to 1.0000
[2019-04-27 22:59:53,212] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5303
[2019-04-27 22:59:53,220] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.83333333333334, 1.0, 2.0, 0.7336170904601088, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1025273.464897259, 1025273.464897259, 228269.2253772327], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4083000.0000, 
sim time next is 4083600.0000, 
raw observation next is [27.0, 90.66666666666667, 1.0, 2.0, 0.8263011242058245, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1154875.345869785, 1154875.345869785, 250469.8526333685], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9066666666666667, 1.0, 1.0, 0.7907242460311138, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32079870718605136, 0.32079870718605136, 0.3738356009453261], 
reward next is 0.6262, 
noisyNet noise sample is [array([-0.3403807], dtype=float32), 1.5575073]. 
=============================================
[2019-04-27 22:59:54,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.6107448e-16 1.0000000e+00 2.4359334e-25 3.1189953e-10 8.0861541e-29], sum to 1.0000
[2019-04-27 22:59:54,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8502
[2019-04-27 22:59:54,493] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2650222.056782413 W.
[2019-04-27 22:59:54,500] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.5, 71.0, 1.0, 2.0, 0.6316498283617974, 1.0, 2.0, 0.6316498283617974, 1.0, 1.0, 1.03, 6.986484892935772, 6.9112, 170.5573041426782, 2650222.056782413, 2596292.425578872, 499740.9656452773], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4102200.0000, 
sim time next is 4102800.0000, 
raw observation next is [32.66666666666667, 71.0, 1.0, 2.0, 0.6006023029728425, 1.0, 2.0, 0.6006023029728425, 1.0, 2.0, 1.03, 6.925867468083757, 6.9112, 170.5573041426782, 2519824.159143894, 2509317.255121044, 488198.3815700536], 
processed observation next is [1.0, 0.4782608695652174, 0.7472353870458138, 0.71, 1.0, 1.0, 0.5187979553889668, 1.0, 1.0, 0.5187979553889668, 1.0, 1.0, 1.0365853658536586, 0.001466746808375685, 0.0, 0.8375144448122397, 0.6999511553177483, 0.6970325708669566, 0.7286543008508263], 
reward next is 0.1980, 
noisyNet noise sample is [array([-0.92848], dtype=float32), 0.8585171]. 
=============================================
[2019-04-27 22:59:55,129] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.9629181e-11 9.9970490e-01 9.4543197e-19 2.9510396e-04 1.6959182e-21], sum to 1.0000
[2019-04-27 22:59:55,134] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2931
[2019-04-27 22:59:55,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3458158.746168336 W.
[2019-04-27 22:59:55,146] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.5, 65.5, 1.0, 2.0, 1.006711906362734, 1.0, 2.0, 0.8239459926956297, 1.0, 2.0, 1.03, 7.005121928576787, 6.9112, 170.5573041426782, 3458158.746168336, 3390878.648228997, 635733.5092171367], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4116600.0000, 
sim time next is 4117200.0000, 
raw observation next is [35.33333333333334, 66.0, 1.0, 2.0, 0.987460023656851, 1.0, 2.0, 0.8143200513426881, 1.0, 2.0, 1.03, 7.00512040906669, 6.9112, 170.5573041426782, 3417702.705562092, 3350423.696109645, 627519.1176527604], 
processed observation next is [1.0, 0.6521739130434783, 0.8736176935229073, 0.66, 1.0, 1.0, 0.9848915947672904, 1.0, 1.0, 0.7762892184851664, 1.0, 1.0, 1.0365853658536586, 0.009392040906668963, 0.0, 0.8375144448122397, 0.9493618626561366, 0.9306732489193459, 0.9365956979891945], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.26764658], dtype=float32), -0.4067166]. 
=============================================
[2019-04-27 22:59:57,328] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.0469883e-22 1.0000000e+00 1.3891995e-30 1.9413226e-20 1.8562367e-34], sum to 1.0000
[2019-04-27 22:59:57,337] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8186
[2019-04-27 22:59:57,341] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5721029547719283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799462.9078126133, 799462.9078126133, 195663.9561112169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5729372617275736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800629.2168491923, 800629.2168491923, 195812.6602071171], 
processed observation next is [1.0, 0.043478260869565216, 0.5418641390205374, 0.8733333333333334, 1.0, 1.0, 0.485466580394667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2223970046803312, 0.2223970046803312, 0.2922577018016673], 
reward next is 0.7077, 
noisyNet noise sample is [array([0.02995614], dtype=float32), -0.42149004]. 
=============================================
[2019-04-27 22:59:57,361] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[52.08051 ]
 [52.192394]
 [52.229572]
 [52.458157]
 [52.665283]], R is [[51.91334534]
 [52.10217667]
 [52.28917313]
 [52.47386932]
 [52.65627289]].
[2019-04-27 22:59:57,732] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4411236e-17 1.0000000e+00 4.7781764e-25 7.0340808e-16 3.8774455e-27], sum to 1.0000
[2019-04-27 22:59:57,740] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1997
[2019-04-27 22:59:57,744] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.9481919089868157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1325341.678390726, 1325341.678390727, 283541.3807136527], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4158000.0000, 
sim time next is 4158600.0000, 
raw observation next is [28.83333333333334, 84.83333333333333, 1.0, 2.0, 0.9794442522347163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1369053.020016631, 1369053.02001663, 292728.6669476815], 
processed observation next is [1.0, 0.13043478260869565, 0.5655608214849924, 0.8483333333333333, 1.0, 1.0, 0.9752340388370077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3802925055601753, 0.38029250556017496, 0.4369084581308679], 
reward next is 0.5631, 
noisyNet noise sample is [array([-0.40768337], dtype=float32), 0.9997827]. 
=============================================
[2019-04-27 22:59:58,012] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2486423e-17 1.0000000e+00 2.9900981e-23 4.9184374e-15 1.9760455e-25], sum to 1.0000
[2019-04-27 22:59:58,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8524
[2019-04-27 22:59:58,027] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 88.16666666666667, 1.0, 2.0, 0.8064759847043237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1127152.139103602, 1127152.139103602, 245509.9468453412], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4169400.0000, 
sim time next is 4170000.0000, 
raw observation next is [28.66666666666667, 87.33333333333334, 1.0, 2.0, 0.7760527835367242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1084610.153922436, 1084610.153922437, 238121.4827969765], 
processed observation next is [1.0, 0.2608695652173913, 0.5576619273301741, 0.8733333333333334, 1.0, 1.0, 0.7301840765502702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3012805983117878, 0.30128059831178805, 0.3554051982044425], 
reward next is 0.6446, 
noisyNet noise sample is [array([-0.92479], dtype=float32), 1.1122196]. 
=============================================
[2019-04-27 22:59:58,044] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[40.81226 ]
 [40.530483]
 [40.66877 ]
 [40.699368]
 [40.465176]], R is [[41.20005798]
 [41.42162704]
 [41.61096573]
 [41.79972076]
 [41.97855759]].
[2019-04-27 22:59:59,661] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7888245e-14 1.0000000e+00 1.5436476e-20 1.1746494e-13 1.2643744e-22], sum to 1.0000
[2019-04-27 22:59:59,669] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1722
[2019-04-27 22:59:59,677] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3171704.461984676 W.
[2019-04-27 22:59:59,683] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [36.83333333333334, 51.16666666666667, 1.0, 2.0, 0.8703830055151338, 1.0, 2.0, 0.7557815422718295, 1.0, 2.0, 1.03, 7.005111170545224, 6.9112, 170.5573041426782, 3171704.461984676, 3104432.070460953, 580547.7665895473], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4200600.0000, 
sim time next is 4201200.0000, 
raw observation next is [37.0, 50.0, 1.0, 2.0, 0.8759409349014052, 1.0, 2.0, 0.7585605069649654, 1.0, 2.0, 1.03, 7.005111609037963, 6.9112, 170.5573041426782, 3183381.498349871, 3116108.792715963, 582661.5880901713], 
processed observation next is [1.0, 0.6521739130434783, 0.95260663507109, 0.5, 1.0, 1.0, 0.8505312468691629, 1.0, 1.0, 0.7091090445361028, 1.0, 1.0, 1.0365853658536586, 0.00939116090379626, 0.0, 0.8375144448122397, 0.8842726384305197, 0.8655857757544342, 0.8696441613286139], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.6475609], dtype=float32), 1.1606854]. 
=============================================
[2019-04-27 23:00:00,869] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.8747317e-19 1.0000000e+00 4.2333866e-27 6.0516616e-19 8.7453808e-30], sum to 1.0000
[2019-04-27 23:00:00,878] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3816
[2019-04-27 23:00:00,881] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 75.66666666666667, 1.0, 2.0, 0.598659206729781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 836587.5184940917, 836587.5184940911, 200496.4403018882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4237800.0000, 
sim time next is 4238400.0000, 
raw observation next is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5952278981837773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831790.6091750228, 831790.6091750228, 199860.3863433832], 
processed observation next is [1.0, 0.043478260869565216, 0.6524486571879939, 0.7633333333333334, 1.0, 1.0, 0.5123227688961172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2310529469930619, 0.2310529469930619, 0.2982990840946018], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.74907875], dtype=float32), -0.49936008]. 
=============================================
[2019-04-27 23:00:01,606] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 23:00:01,608] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:00:01,609] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:00:01,609] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:00:01,613] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:00:01,610] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:00:01,619] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:00:01,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:00:01,615] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:00:01,624] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:00:01,627] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:00:01,636] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run14
[2019-04-27 23:00:01,636] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run14
[2019-04-27 23:00:01,636] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run14
[2019-04-27 23:00:01,661] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run14
[2019-04-27 23:00:01,708] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run14
[2019-04-27 23:00:06,666] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:00:06,667] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.7, 66.33333333333333, 1.0, 2.0, 0.240326275419888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 398303.3230872205, 398303.3230872211, 159804.7491575889]
[2019-04-27 23:00:06,669] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:00:06,672] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2842673e-22 1.0000000e+00 4.0043681e-31 2.7133317e-21 1.0708097e-33], sampled 0.2826814843115919
[2019-04-27 23:00:11,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:00:11,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.96049216333333, 64.935067505, 1.0, 2.0, 0.3838868599406235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 592088.5482429644, 592088.5482429644, 174308.685040763]
[2019-04-27 23:00:11,679] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:00:11,681] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1419482e-21 1.0000000e+00 3.4445772e-30 1.1910787e-20 1.1045696e-32], sampled 0.8844129058165027
[2019-04-27 23:00:11,888] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:00:11,889] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 70.0, 1.0, 2.0, 0.2508041021938299, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412422.978333547, 412422.9783335476, 160985.8883752157]
[2019-04-27 23:00:11,890] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:00:11,892] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.1445715e-21 1.0000000e+00 3.4592989e-30 1.1890132e-20 1.1092143e-32], sampled 0.35035895125955574
[2019-04-27 23:00:24,151] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:00:24,153] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.8, 58.5, 1.0, 2.0, 0.2097894646684012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 350273.5317910273, 350273.5317910279, 156415.8035170966]
[2019-04-27 23:00:24,155] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:00:24,158] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.3312529e-21 1.0000000e+00 1.5185604e-30 6.6899153e-21 4.5375044e-33], sampled 0.20796314706487273
[2019-04-27 23:01:14,724] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:01:14,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.83333333333334, 59.16666666666667, 1.0, 2.0, 0.6212995386009679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 868238.8494488058, 868238.8494488064, 204781.6233637368]
[2019-04-27 23:01:14,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:01:14,729] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.1247210e-22 1.0000000e+00 3.8394340e-31 2.7462584e-21 1.0265792e-33], sampled 0.6235845684029255
[2019-04-27 23:01:40,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:01:40,856] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.46666666666667, 91.33333333333334, 1.0, 2.0, 0.6503151431811222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 908804.2321318315, 908804.2321318315, 210488.6914154046]
[2019-04-27 23:01:40,857] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:01:40,859] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.1424623e-21 1.0000000e+00 7.3597434e-30 2.1770912e-20 2.5397143e-32], sampled 0.43925526340039356
[2019-04-27 23:01:51,804] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:01:51,806] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.968159225, 90.345513555, 1.0, 2.0, 0.4188405772270263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610982.0040742004, 610982.0040741998, 175164.8872098323]
[2019-04-27 23:01:51,807] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:01:51,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7767443e-21 1.0000000e+00 1.9200355e-30 8.1239694e-21 5.8694137e-33], sampled 0.20525064956559913
[2019-04-27 23:02:00,625] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.0864977]
[2019-04-27 23:02:00,625] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.53628476833333, 77.75027704833333, 1.0, 2.0, 0.9143591134414403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1278023.195062328, 1278023.195062328, 273915.2763020049]
[2019-04-27 23:02:00,625] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:02:00,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7663122e-19 1.0000000e+00 1.3641054e-27 9.2323655e-19 7.5015814e-30], sampled 0.4839913499625216
[2019-04-27 23:02:11,662] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:02:12,338] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:02:12,492] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:02:12,540] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:02:12,634] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:02:13,648] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 325000, evaluation results [325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:02:14,840] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.4977561e-21 1.0000000e+00 2.2571580e-29 2.5436857e-20 4.0066428e-32], sum to 1.0000
[2019-04-27 23:02:14,849] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7734
[2019-04-27 23:02:14,855] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.9087680681200181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1270203.772790303, 1270203.772790304, 272362.4198184147], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4251000.0000, 
sim time next is 4251600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.8695328207646479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1215332.475601116, 1215332.475601116, 261695.6773657591], 
processed observation next is [1.0, 0.21739130434782608, 0.5734597156398105, 0.79, 1.0, 1.0, 0.8428106274272866, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3375923543336433, 0.3375923543336433, 0.39059056323247626], 
reward next is 0.6094, 
noisyNet noise sample is [array([1.049823], dtype=float32), -0.5214391]. 
=============================================
[2019-04-27 23:02:15,610] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.0173572e-08 1.0000000e+00 4.4001955e-12 3.8126821e-08 9.6301363e-13], sum to 1.0000
[2019-04-27 23:02:15,616] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0386
[2019-04-27 23:02:15,620] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.3890891700667728, 1.0, 1.0, 0.3890891700667728, 1.0, 1.0, 0.6757192146511931, 6.9112, 6.9112, 170.5573041426782, 1631779.36025254, 1631779.36025254, 346930.8958155593], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4245000.0000, 
sim time next is 4245600.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.062915979974281, 6.9112, 168.9122775675887, 1561460.742162373, 1453828.636934785, 311352.6353767895], 
processed observation next is [1.0, 0.13043478260869565, 0.6208530805687204, 0.75, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.015171597997428066, 0.0, 0.8294366112321255, 0.4337390950451036, 0.40384128803744024, 0.4647054259355067], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.1944563], dtype=float32), -0.7826697]. 
=============================================
[2019-04-27 23:02:18,145] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6762840e-21 1.0000000e+00 1.6365400e-30 1.3283224e-22 3.7859537e-32], sum to 1.0000
[2019-04-27 23:02:18,156] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8071
[2019-04-27 23:02:18,162] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.33333333333334, 49.33333333333333, 1.0, 2.0, 0.5536512424199139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 773668.8932776864, 773668.8932776871, 192431.9374194784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4297200.0000, 
sim time next is 4297800.0000, 
raw observation next is [36.16666666666666, 49.66666666666667, 1.0, 2.0, 0.5571315156630094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778533.9883165538, 778533.9883165538, 193033.9268778411], 
processed observation next is [1.0, 0.7391304347826086, 0.9131121642969979, 0.4966666666666667, 1.0, 1.0, 0.4664235128469993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21625944119904272, 0.21625944119904272, 0.28811033862364344], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.0313985], dtype=float32), 1.1890072]. 
=============================================
[2019-04-27 23:02:22,005] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.7938342e-09 1.0000000e+00 2.1833367e-13 1.7073237e-08 5.0031509e-14], sum to 1.0000
[2019-04-27 23:02:22,010] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8615
[2019-04-27 23:02:22,019] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3220440.034282866 W.
[2019-04-27 23:02:22,025] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666666, 76.5, 1.0, 2.0, 0.8935793750413031, 1.0, 2.0, 0.767379727034914, 1.0, 2.0, 1.03, 7.005113000676649, 6.9112, 170.5573041426782, 3220440.034282866, 3153166.331761559, 589448.115683227], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4351800.0000, 
sim time next is 4352400.0000, 
raw observation next is [33.0, 75.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 8.303782144831285, 6.9112, 170.5573041426782, 3908055.198050009, 2910491.961087468, 545205.4557161558], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.75, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.13925821448312847, 0.0, 0.8375144448122397, 1.0855708883472248, 0.8084699891909634, 0.8137394861435161], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7510961], dtype=float32), 0.45424178]. 
=============================================
[2019-04-27 23:02:25,778] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.827431e-29 1.000000e+00 0.000000e+00 6.914706e-31 0.000000e+00], sum to 1.0000
[2019-04-27 23:02:25,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2428
[2019-04-27 23:02:25,794] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 67.66666666666667, 1.0, 2.0, 0.6213718860321714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 868339.9931392365, 868339.9931392359, 204795.4328057515], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4452600.0000, 
sim time next is 4453200.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.6163653069300615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861340.6811531755, 861340.6811531755, 203834.7300675448], 
processed observation next is [0.0, 0.5652173913043478, 0.7630331753554502, 0.67, 1.0, 1.0, 0.5377895264217608, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23926130032032653, 0.23926130032032653, 0.3042309403993206], 
reward next is 0.6958, 
noisyNet noise sample is [array([1.2836285], dtype=float32), -0.66922224]. 
=============================================
[2019-04-27 23:02:32,195] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1300570e-32 1.0000000e+00 0.0000000e+00 2.7573806e-32 0.0000000e+00], sum to 1.0000
[2019-04-27 23:02:32,203] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1085
[2019-04-27 23:02:32,208] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 75.66666666666667, 1.0, 2.0, 0.5013865127069486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700610.3909894257, 700610.3909894257, 183813.1069169299], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4524600.0000, 
sim time next is 4525200.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.497565174219769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695268.9118359083, 695268.9118359083, 183214.9644337025], 
processed observation next is [0.0, 0.391304347826087, 0.5260663507109005, 0.74, 1.0, 1.0, 0.39465683640936017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1931302532877523, 0.1931302532877523, 0.2734551707965709], 
reward next is 0.7265, 
noisyNet noise sample is [array([0.80745804], dtype=float32), -0.8160683]. 
=============================================
[2019-04-27 23:02:32,459] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3353924e-30 1.0000000e+00 0.0000000e+00 1.2281709e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:02:32,465] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8034
[2019-04-27 23:02:32,471] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5107364156530574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713679.8284751853, 713679.8284751853, 185295.2089790481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4517400.0000, 
sim time next is 4518000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5118758042310636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715272.494265225, 715272.494265225, 185477.5739025362], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41189855931453445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1986868039625625, 0.1986868039625625, 0.27683219985453167], 
reward next is 0.7232, 
noisyNet noise sample is [array([-1.4246466], dtype=float32), -0.013514502]. 
=============================================
[2019-04-27 23:02:32,487] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.73693 ]
 [68.74869 ]
 [68.75683 ]
 [68.76614 ]
 [68.763306]], R is [[68.80427551]
 [68.8396759 ]
 [68.87498474]
 [68.91018677]
 [68.94530487]].
[2019-04-27 23:02:35,446] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8674864e-27 1.0000000e+00 0.0000000e+00 4.9352258e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:02:35,452] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2698
[2019-04-27 23:02:35,462] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5470604401761648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764455.6319718116, 764455.6319718122, 191297.8258501207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4573200.0000, 
sim time next is 4573800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5477374752111377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765402.0533916256, 765402.0533916249, 191413.3666113194], 
processed observation next is [0.0, 0.9565217391304348, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4551053918206478, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2126116814976738, 0.2126116814976736, 0.2856915919571931], 
reward next is 0.7143, 
noisyNet noise sample is [array([0.7526623], dtype=float32), -0.66124374]. 
=============================================
[2019-04-27 23:02:37,398] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1173302e-09 1.0000000e+00 8.9703479e-14 1.8286018e-08 1.5895022e-15], sum to 1.0000
[2019-04-27 23:02:37,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9169
[2019-04-27 23:02:37,415] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 94.0, 1.0, 2.0, 0.9336194138699042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304960.355947803, 1304960.355947803, 279355.7588205847], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4598400.0000, 
sim time next is 4599000.0000, 
raw observation next is [27.5, 94.0, 1.0, 2.0, 0.9477929068566072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1324783.622482717, 1324783.622482717, 283425.8497045267], 
processed observation next is [1.0, 0.21739130434782608, 0.5023696682464456, 0.94, 1.0, 1.0, 0.9370998877790448, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3679954506896436, 0.3679954506896436, 0.423023656275413], 
reward next is 0.5770, 
noisyNet noise sample is [array([-0.61423695], dtype=float32), -0.01800553]. 
=============================================
[2019-04-27 23:02:37,427] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[22.697731]
 [22.669743]
 [22.741508]
 [22.751444]
 [22.723095]], R is [[23.25911522]
 [23.60957527]
 [23.95163345]
 [24.30020142]
 [24.63563156]].
[2019-04-27 23:02:41,831] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.0908834e-20 1.0000000e+00 4.3160385e-29 3.5712634e-21 3.6534813e-31], sum to 1.0000
[2019-04-27 23:02:41,838] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2287
[2019-04-27 23:02:41,846] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 92.33333333333334, 1.0, 2.0, 0.8106625785716584, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1133006.555019287, 1133006.555019287, 246546.2275873722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4677600.0000, 
sim time next is 4678200.0000, 
raw observation next is [27.0, 91.5, 1.0, 2.0, 0.8133959602979925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1136828.857087731, 1136828.857087731, 247226.5909844394], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.915, 1.0, 1.0, 0.7751758557807139, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3157857936354808, 0.3157857936354808, 0.36899491191707373], 
reward next is 0.6310, 
noisyNet noise sample is [array([0.8135881], dtype=float32), -1.0185928]. 
=============================================
[2019-04-27 23:02:54,055] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.3834882e-21 1.0000000e+00 4.6301615e-32 1.5011609e-22 2.4396072e-32], sum to 1.0000
[2019-04-27 23:02:54,064] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-27 23:02:54,073] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2141802.214424876 W.
[2019-04-27 23:02:54,082] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.66666666666667, 71.33333333333333, 1.0, 2.0, 0.7658699922544562, 1.0, 1.0, 0.7658699922544562, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2141802.214424876, 2141802.214424876, 403594.0294351691], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4873200.0000, 
sim time next is 4873800.0000, 
raw observation next is [29.83333333333333, 70.66666666666667, 1.0, 2.0, 0.7617100210249301, 1.0, 2.0, 0.7617100210249301, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2130157.033420214, 2130157.033420215, 401657.5404792904], 
processed observation next is [1.0, 0.391304347826087, 0.6129541864139019, 0.7066666666666667, 1.0, 1.0, 0.7129036397890725, 1.0, 1.0, 0.7129036397890725, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5917102870611706, 0.5917102870611708, 0.5994888663870006], 
reward next is 0.4005, 
noisyNet noise sample is [array([-0.53505003], dtype=float32), -1.4833089]. 
=============================================
[2019-04-27 23:02:54,697] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.4354612e-21 1.0000000e+00 5.3636750e-30 6.4497944e-22 2.5681145e-31], sum to 1.0000
[2019-04-27 23:02:54,710] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9287
[2019-04-27 23:02:54,724] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2266327.805903738 W.
[2019-04-27 23:02:54,729] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5402359648508556, 1.0, 2.0, 0.5402359648508556, 1.0, 1.0, 0.9358409440745256, 6.911200000000001, 6.9112, 170.5573041426782, 2266327.805903738, 2266327.805903737, 443619.3872031649], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4882200.0000, 
sim time next is 4882800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.4431944958801251, 1.0, 2.0, 0.4431944958801251, 1.0, 2.0, 0.7671050607339148, 6.911200000000001, 6.9112, 170.5573041426782, 1858885.480632552, 1858885.480632551, 377682.515458121], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.32914999503629533, 1.0, 1.0, 0.32914999503629533, 1.0, 1.0, 0.7159817813828229, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5163570779534866, 0.5163570779534864, 0.5637052469524194], 
reward next is 0.4363, 
noisyNet noise sample is [array([0.7448058], dtype=float32), 0.8500048]. 
=============================================
[2019-04-27 23:03:02,364] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.927574e-27 1.000000e+00 0.000000e+00 1.376551e-28 0.000000e+00], sum to 1.0000
[2019-04-27 23:03:02,373] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7461
[2019-04-27 23:03:02,377] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4974896336573949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 695163.3212670541, 695163.3212670541, 183203.817596582], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5030400.0000, 
sim time next is 5031000.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.4984229207998456, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 696467.8704098178, 696467.8704098184, 183349.424544926], 
processed observation next is [0.0, 0.21739130434782608, 0.4312796208530806, 0.89, 1.0, 1.0, 0.3956902660239104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1934632973360605, 0.19346329733606066, 0.2736558575297403], 
reward next is 0.7263, 
noisyNet noise sample is [array([2.2032938], dtype=float32), -0.28218865]. 
=============================================
[2019-04-27 23:03:02,400] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[70.00141]
 [69.98377]
 [69.96193]
 [69.88629]
 [69.87493]], R is [[70.04521179]
 [70.07131958]
 [70.09741211]
 [70.12376404]
 [70.15107727]].
[2019-04-27 23:03:03,342] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.833579e-30 1.000000e+00 0.000000e+00 3.142931e-32 0.000000e+00], sum to 1.0000
[2019-04-27 23:03:03,351] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7769
[2019-04-27 23:03:03,357] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4757018060878663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664837.0839761337, 664837.0839761337, 179892.8770539988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5018400.0000, 
sim time next is 5019000.0000, 
raw observation next is [26.0, 84.83333333333333, 1.0, 2.0, 0.4778245022237005, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667675.7651618401, 667675.7651618401, 180194.3905076849], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8483333333333333, 1.0, 1.0, 0.3708728942454223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18546549032273338, 0.18546549032273338, 0.2689468515040073], 
reward next is 0.7311, 
noisyNet noise sample is [array([1.6055541], dtype=float32), -1.2180482]. 
=============================================
[2019-04-27 23:03:03,379] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[74.178154]
 [74.116516]
 [74.194435]
 [74.01362 ]
 [73.71767 ]], R is [[74.5290451 ]
 [74.51525879]
 [74.50154114]
 [74.48786926]
 [74.47422028]].
[2019-04-27 23:03:12,628] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 23:03:12,630] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:03:12,630] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:03:12,632] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:03:12,632] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:03:12,632] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:03:12,633] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:03:12,635] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:03:12,635] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:03:12,637] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:03:12,639] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:03:12,655] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run15
[2019-04-27 23:03:12,683] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run15
[2019-04-27 23:03:12,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run15
[2019-04-27 23:03:12,736] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run15
[2019-04-27 23:03:12,736] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run15
[2019-04-27 23:03:14,497] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:03:14,499] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.3, 67.0, 1.0, 2.0, 0.4655063909590794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 662065.4708943171, 662065.4708943171, 179859.7338607415]
[2019-04-27 23:03:14,500] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:03:14,505] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3272212e-29 1.0000000e+00 0.0000000e+00 1.4076845e-29 0.0000000e+00], sampled 0.20400731190371357
[2019-04-27 23:03:49,629] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:03:49,629] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 92.5, 1.0, 2.0, 0.39971745840467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596330.8494793294, 596330.8494793294, 174197.298255804]
[2019-04-27 23:03:49,632] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:03:49,638] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.9997810e-29 1.0000000e+00 0.0000000e+00 4.3054814e-29 0.0000000e+00], sampled 0.3738439542986287
[2019-04-27 23:03:57,741] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:03:57,743] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.71009274, 97.1375244, 1.0, 2.0, 0.5047647071207535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705332.4643081349, 705332.4643081349, 184345.1917002048]
[2019-04-27 23:03:57,744] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:03:57,749] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.9187501e-29 1.0000000e+00 0.0000000e+00 1.6641589e-29 0.0000000e+00], sampled 0.4641070035529521
[2019-04-27 23:04:12,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:04:12,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 75.66666666666666, 1.0, 2.0, 0.5036069638713233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703714.157063147, 703714.1570631478, 184162.8827910981]
[2019-04-27 23:04:12,322] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:04:12,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6692580e-29 1.0000000e+00 0.0000000e+00 6.9892554e-30 0.0000000e+00], sampled 0.10108887042289705
[2019-04-27 23:04:14,882] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:04:14,883] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.9, 71.5, 1.0, 2.0, 0.5455578882656952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762355.2306639103, 762355.2306639103, 191041.5105392155]
[2019-04-27 23:04:14,883] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:04:14,889] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2468350e-30 1.0000000e+00 0.0000000e+00 3.8412086e-30 0.0000000e+00], sampled 0.9102565724638909
[2019-04-27 23:04:16,130] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:04:16,130] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.5, 69.5, 1.0, 2.0, 0.5511518524934306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770174.9947139128, 770174.9947139123, 191998.3481314512]
[2019-04-27 23:04:16,132] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:04:16,135] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.857317e-30 1.000000e+00 0.000000e+00 3.676706e-30 0.000000e+00], sampled 0.3003302264696033
[2019-04-27 23:04:56,445] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:04:56,448] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.11666666666667, 76.0, 1.0, 2.0, 0.5806497095865053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811410.798091635, 811410.798091635, 197195.938762238]
[2019-04-27 23:04:56,449] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:04:56,455] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.16036166e-29 1.00000000e+00 0.00000000e+00 4.83688579e-30
 0.00000000e+00], sampled 0.1471852873796191
[2019-04-27 23:05:16,863] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00174361], dtype=float32), 0.08117713]
[2019-04-27 23:05:16,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.0, 68.0, 1.0, 2.0, 0.4123883998719468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611077.6542607344, 611077.6542607344, 175447.8841552928]
[2019-04-27 23:05:16,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:05:16,868] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.21939314e-29 1.00000000e+00 0.00000000e+00 5.08182985e-30
 0.00000000e+00], sampled 0.9402115456143492
[2019-04-27 23:05:22,030] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:05:22,535] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:05:23,188] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:05:23,220] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:05:23,256] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:05:24,270] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 350000, evaluation results [350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:05:30,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3992889e-15 1.0000000e+00 2.4868354e-22 6.8639580e-16 1.7399544e-24], sum to 1.0000
[2019-04-27 23:05:30,753] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7949
[2019-04-27 23:05:30,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 81.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.028660695154411, 6.9112, 168.9120261594646, 1537142.196916321, 1453811.996063272, 311356.4872345119], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5299200.0000, 
sim time next is 5299800.0000, 
raw observation next is [30.83333333333334, 79.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.312699151412536, 6.9112, 168.9105529125367, 1738783.274946582, 1453950.011578757, 311356.6477623066], 
processed observation next is [1.0, 0.34782608695652173, 0.6603475513428123, 0.7916666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.040149915141253614, 0.0, 0.8294281423862698, 0.48299535415182837, 0.4038750032163214, 0.4647114145706069], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.67424846], dtype=float32), -1.4360294]. 
=============================================
[2019-04-27 23:05:41,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8605983e-08 9.9271798e-01 3.1910156e-12 7.2819972e-03 4.6459608e-14], sum to 1.0000
[2019-04-27 23:05:41,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2411
[2019-04-27 23:05:41,209] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.65, 92.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.98660263004508, 6.9112, 168.9124035686867, 1507284.606958002, 1453791.561157479, 311353.3411929866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5459400.0000, 
sim time next is 5460000.0000, 
raw observation next is [27.63333333333333, 92.0, 1.0, 2.0, 0.9298339555523581, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128951144307, 1299666.018307709, 1299666.01830771, 278278.9615278119], 
processed observation next is [1.0, 0.17391304347826086, 0.5086887835703, 0.92, 1.0, 1.0, 0.9154625970510339, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.829439643669834, 0.36101833841880804, 0.36101833841880837, 0.41534173362359983], 
reward next is 0.5847, 
noisyNet noise sample is [array([1.5887332], dtype=float32), 0.8345143]. 
=============================================
[2019-04-27 23:05:41,225] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[20.941301]
 [21.428427]
 [22.125841]
 [22.821898]
 [23.228325]], R is [[21.41487694]
 [21.35900879]
 [21.688694  ]
 [22.03491211]
 [22.40287399]].
[2019-04-27 23:05:47,369] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.6161554e-13 2.4317910e-01 8.1529666e-22 7.5682086e-01 1.0375203e-26], sum to 1.0000
[2019-04-27 23:05:47,382] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9585
[2019-04-27 23:05:47,391] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.08333333333334, 69.16666666666667, 1.0, 2.0, 0.8765405106913963, 1.0, 2.0, 0.8765405106913963, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2451614.231901357, 2451614.231901357, 458854.7398820191], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5565000.0000, 
sim time next is 5565600.0000, 
raw observation next is [31.3, 68.0, 1.0, 2.0, 0.8442447664138758, 1.0, 2.0, 0.8442447664138758, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2361200.25843686, 2361200.25843686, 441987.2778454548], 
processed observation next is [1.0, 0.43478260869565216, 0.6824644549763034, 0.68, 1.0, 1.0, 0.8123430920649106, 1.0, 1.0, 0.8123430920649106, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6558889606769056, 0.6558889606769056, 0.6596825042469475], 
reward next is 0.3403, 
noisyNet noise sample is [array([0.8707328], dtype=float32), 1.134143]. 
=============================================
[2019-04-27 23:05:48,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.7590606e-14 5.2502185e-01 1.7938104e-21 4.7497815e-01 3.8765557e-25], sum to 1.0000
[2019-04-27 23:05:48,410] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3826
[2019-04-27 23:05:48,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1910489.171463397 W.
[2019-04-27 23:05:48,433] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.7, 51.33333333333334, 1.0, 2.0, 0.6832302918689799, 1.0, 2.0, 0.6832302918689799, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1910489.171463397, 1910489.171463397, 367025.3407278963], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5577000.0000, 
sim time next is 5577600.0000, 
raw observation next is [33.8, 50.66666666666667, 1.0, 2.0, 0.9602093380806233, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.983811723376244, 6.9112, 168.9125253158664, 2239309.01613182, 2187795.891967142, 452104.7229669802], 
processed observation next is [1.0, 0.5652173913043478, 0.800947867298578, 0.5066666666666667, 1.0, 1.0, 0.9520594434706304, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007261172337624355, 0.0, 0.8294378277895887, 0.6220302822588389, 0.6077210811019839, 0.6747831686074331], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.5088221], dtype=float32), -0.18881285]. 
=============================================
[2019-04-27 23:05:50,957] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.4366095e-25 1.0000000e+00 3.7958373e-36 7.4557981e-21 0.0000000e+00], sum to 1.0000
[2019-04-27 23:05:50,962] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4111
[2019-04-27 23:05:50,967] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 91.5, 1.0, 2.0, 0.5222979293180688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729840.9116384251, 729840.9116384251, 187163.156377542], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5617800.0000, 
sim time next is 5618400.0000, 
raw observation next is [26.1, 91.66666666666666, 1.0, 2.0, 0.522220160378864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729732.2027074888, 729732.2027074888, 187150.3899612127], 
processed observation next is [0.0, 0.0, 0.4360189573459717, 0.9166666666666665, 1.0, 1.0, 0.42436163901067947, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2027033896409691, 0.2027033896409691, 0.27932894024061594], 
reward next is 0.7207, 
noisyNet noise sample is [array([-1.1258526], dtype=float32), -0.93151766]. 
=============================================
[2019-04-27 23:05:51,240] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.1761819e-31 1.0000000e+00 0.0000000e+00 3.5752193e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:05:51,253] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5957
[2019-04-27 23:05:51,260] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 92.16666666666667, 1.0, 2.0, 0.5096127999432747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712109.2127325597, 712109.2127325597, 185115.6167547054], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5626200.0000, 
sim time next is 5626800.0000, 
raw observation next is [25.7, 92.0, 1.0, 2.0, 0.508023626854121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709887.8341999968, 709887.8341999968, 184862.5178650199], 
processed observation next is [0.0, 0.13043478260869565, 0.4170616113744076, 0.92, 1.0, 1.0, 0.4072573817519529, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19719106505555467, 0.19719106505555467, 0.2759142057686864], 
reward next is 0.7241, 
noisyNet noise sample is [array([-0.5098643], dtype=float32), 0.153692]. 
=============================================
[2019-04-27 23:06:02,875] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0836600e-17 1.0000000e+00 1.0664609e-24 1.6107430e-16 1.1303802e-24], sum to 1.0000
[2019-04-27 23:06:02,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5788
[2019-04-27 23:06:02,892] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.15, 93.16666666666667, 1.0, 2.0, 0.9362288958828693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104242, 1308609.990056106, 1308609.990056107, 280097.0301772735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5803800.0000, 
sim time next is 5804400.0000, 
raw observation next is [26.1, 93.33333333333334, 1.0, 2.0, 0.8847047732399508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1236550.425674917, 1236550.425674916, 265764.228701172], 
processed observation next is [1.0, 0.17391304347826086, 0.4360189573459717, 0.9333333333333335, 1.0, 1.0, 0.8610900882409045, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3434862293541436, 0.34348622935414336, 0.396663027912197], 
reward next is 0.6033, 
noisyNet noise sample is [array([1.3734251], dtype=float32), 0.07154659]. 
=============================================
[2019-04-27 23:06:03,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.0212644e-24 1.0000000e+00 5.1169728e-33 3.1356213e-20 5.4504629e-32], sum to 1.0000
[2019-04-27 23:06:03,469] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5767
[2019-04-27 23:06:03,471] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.5461227528156555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763144.8483131959, 763144.8483131966, 191137.5523005457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5781000.0000, 
sim time next is 5781600.0000, 
raw observation next is [27.5, 86.0, 1.0, 2.0, 0.5443056818944126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760604.7878911024, 760604.7878911024, 190828.5858248018], 
processed observation next is [0.0, 0.9565217391304348, 0.5023696682464456, 0.86, 1.0, 1.0, 0.45097070107760556, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21127910774752845, 0.21127910774752845, 0.284818784813137], 
reward next is 0.7152, 
noisyNet noise sample is [array([-0.5329493], dtype=float32), -0.68388635]. 
=============================================
[2019-04-27 23:06:07,254] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5355243e-21 1.0000000e+00 6.6022449e-29 1.0504595e-17 9.0549576e-29], sum to 1.0000
[2019-04-27 23:06:07,259] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2901
[2019-04-27 23:06:07,264] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666666, 94.83333333333333, 1.0, 2.0, 0.817234511469615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104292, 1142196.62750742, 1142196.62750742, 248184.5119213736], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5885400.0000, 
sim time next is 5886000.0000, 
raw observation next is [25.9, 95.0, 1.0, 2.0, 0.7774507243969447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1086564.913465274, 1086564.913465275, 238450.4391852209], 
processed observation next is [1.0, 0.13043478260869565, 0.42654028436018954, 0.95, 1.0, 1.0, 0.7318683426469214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3018235870736872, 0.3018235870736875, 0.3558961778883894], 
reward next is 0.6441, 
noisyNet noise sample is [array([0.74588335], dtype=float32), -0.65785414]. 
=============================================
[2019-04-27 23:06:07,291] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[45.943222]
 [45.786938]
 [45.311623]
 [44.961452]
 [45.44716 ]], R is [[46.40539932]
 [46.57091904]
 [46.71912384]
 [46.83174515]
 [46.72481155]].
[2019-04-27 23:06:15,748] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4132774e-18 1.0000000e+00 8.0254740e-26 2.1563971e-14 2.6023014e-26], sum to 1.0000
[2019-04-27 23:06:15,756] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0348
[2019-04-27 23:06:15,760] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.7212140076413015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1007931.193695608, 1007931.193695608, 225490.1060581056], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5976000.0000, 
sim time next is 5976600.0000, 
raw observation next is [26.06666666666667, 93.16666666666667, 1.0, 2.0, 0.7470131953901096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1044004.520233728, 1044004.520233727, 231319.4086159989], 
processed observation next is [1.0, 0.17391304347826086, 0.4344391785150081, 0.9316666666666668, 1.0, 1.0, 0.6951966209519392, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29000125562048, 0.2900012556204797, 0.34525284868059536], 
reward next is 0.6547, 
noisyNet noise sample is [array([-1.3228091], dtype=float32), -0.5505652]. 
=============================================
[2019-04-27 23:06:17,250] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.9607696e-12 9.9890840e-01 1.7866433e-18 1.0915535e-03 5.9390829e-21], sum to 1.0000
[2019-04-27 23:06:17,260] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1131
[2019-04-27 23:06:17,269] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2402454.270164791 W.
[2019-04-27 23:06:17,275] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.26666666666667, 72.33333333333333, 1.0, 2.0, 0.5726539541947083, 1.0, 2.0, 0.5726539541947083, 1.0, 1.0, 0.9945105388796669, 6.911200000000001, 6.9112, 170.5573041426782, 2402454.270164791, 2402454.27016479, 469023.5569800448], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6000000.0000, 
sim time next is 6000600.0000, 
raw observation next is [31.43333333333333, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.249597881702803, 6.9112, 168.9114123058816, 2523968.011497147, 2283899.102194243, 475356.1960739332], 
processed observation next is [1.0, 0.43478260869565216, 0.6887835703001578, 0.7166666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.033839788170280286, 0.0, 0.8294323624010252, 0.7011022254158742, 0.6344164172761786, 0.7094868598118406], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8110892], dtype=float32), -0.83355796]. 
=============================================
[2019-04-27 23:06:17,546] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1120365e-25 1.0000000e+00 2.1922555e-38 1.1854025e-22 0.0000000e+00], sum to 1.0000
[2019-04-27 23:06:17,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5916
[2019-04-27 23:06:17,560] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 87.66666666666667, 1.0, 2.0, 0.5404300852562047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755187.160740262, 755187.160740262, 190172.9857932953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6040200.0000, 
sim time next is 6040800.0000, 
raw observation next is [27.2, 88.0, 1.0, 2.0, 0.5409022988955433, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755847.258253799, 755847.2582537985, 190252.5810709409], 
processed observation next is [1.0, 0.9565217391304348, 0.4881516587677725, 0.88, 1.0, 1.0, 0.4468702396331847, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2099575717371664, 0.20995757173716625, 0.28395907622528493], 
reward next is 0.7160, 
noisyNet noise sample is [array([-0.02290342], dtype=float32), 0.48175532]. 
=============================================
[2019-04-27 23:06:21,334] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6084843e-10 9.9970955e-01 1.4324705e-16 2.9048137e-04 9.2138180e-19], sum to 1.0000
[2019-04-27 23:06:21,343] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0310
[2019-04-27 23:06:21,354] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2015872.107969355 W.
[2019-04-27 23:06:21,363] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.73333333333333, 66.33333333333334, 1.0, 2.0, 0.7208819782942417, 1.0, 2.0, 0.7208819782942417, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2015872.107969355, 2015872.107969355, 383190.2449258858], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 6103200.0000, 
sim time next is 6103800.0000, 
raw observation next is [30.7, 66.5, 1.0, 2.0, 0.8885701048523014, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.988447186087699, 6.9112, 168.9124969555905, 2139037.933893561, 2084236.270537122, 431553.5846849107], 
processed observation next is [1.0, 0.6521739130434783, 0.6540284360189573, 0.665, 1.0, 1.0, 0.8657471142798812, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007724718608769887, 0.0, 0.8294376885276508, 0.5941772038593225, 0.578954519593645, 0.6441098278879265], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9598893], dtype=float32), 1.318277]. 
=============================================
[2019-04-27 23:06:23,282] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 23:06:23,283] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:06:23,283] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:06:23,284] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:06:23,284] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:06:23,292] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:06:23,290] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:06:23,295] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:06:23,297] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:06:23,292] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:06:23,299] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:06:23,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run16
[2019-04-27 23:06:23,339] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run16
[2019-04-27 23:06:23,366] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run16
[2019-04-27 23:06:23,387] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run16
[2019-04-27 23:06:23,411] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run16
[2019-04-27 23:06:25,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.079551265]
[2019-04-27 23:06:25,885] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.68048067166667, 84.74116276666668, 1.0, 2.0, 0.3930400745540952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626940.6200937877, 626940.6200937877, 177536.7692355873]
[2019-04-27 23:06:25,887] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:06:25,891] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9663565e-21 1.0000000e+00 2.6086273e-30 1.1368399e-18 1.1377484e-30], sampled 0.9525655650715606
[2019-04-27 23:06:27,981] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.079551265]
[2019-04-27 23:06:27,982] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.9, 90.0, 1.0, 2.0, 0.2924557214353884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468358.5032334123, 468358.5032334116, 164834.0716436225]
[2019-04-27 23:06:27,983] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:06:27,989] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6583325e-23 1.0000000e+00 4.1208674e-33 1.1473997e-20 2.0064820e-33], sampled 0.6750143238061587
[2019-04-27 23:07:15,890] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.079551265]
[2019-04-27 23:07:15,891] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.08333333333333, 58.83333333333334, 1.0, 2.0, 0.5661409876245135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791128.4872320953, 791128.4872320953, 194607.3746356762]
[2019-04-27 23:07:15,891] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:07:15,896] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.76420995e-23 1.00000000e+00 1.90136634e-33 1.01100196e-20
 8.77048641e-34], sampled 0.07181538885639138
[2019-04-27 23:08:01,018] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.079551265]
[2019-04-27 23:08:01,019] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.53333333333333, 63.0, 1.0, 2.0, 0.6774822195405061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 946786.677029445, 946786.6770294455, 216070.2874853919]
[2019-04-27 23:08:01,020] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:08:01,025] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2182748e-23 1.0000000e+00 2.7812646e-33 1.1446935e-20 1.3179929e-33], sampled 0.2500690243294422
[2019-04-27 23:08:30,358] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.079551265]
[2019-04-27 23:08:30,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.39812337666666, 86.65987229, 1.0, 2.0, 0.3168555493041468, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504928.7850377075, 504928.7850377075, 167452.0525411033]
[2019-04-27 23:08:30,359] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:08:30,363] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0647112e-22 1.0000000e+00 1.2506731e-31 1.1339894e-19 6.1711914e-32], sampled 0.07916392599059141
[2019-04-27 23:08:33,620] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8073.9553 3145158846.9938 1289.0000
[2019-04-27 23:08:33,900] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8367.0968 2917516604.7685 1074.0000
[2019-04-27 23:08:34,412] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8581.2596 2833522922.9463 914.0000
[2019-04-27 23:08:34,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8168.5112 2991044638.6519 1347.0000
[2019-04-27 23:08:34,633] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8743.2495 2772275349.1423 749.0000
[2019-04-27 23:08:35,649] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 375000, evaluation results [375000.0, 8073.955323511598, 3145158846.993793, 1289.0, 8367.096760162543, 2917516604.768496, 1074.0, 8743.249523230887, 2772275349.1423492, 749.0, 8168.511199851255, 2991044638.651933, 1347.0, 8581.259579320087, 2833522922.946339, 914.0]
[2019-04-27 23:08:44,177] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.152532e-36 1.000000e+00 0.000000e+00 3.815298e-32 0.000000e+00], sum to 1.0000
[2019-04-27 23:08:44,187] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5078
[2019-04-27 23:08:44,197] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 88.0, 1.0, 2.0, 0.5356992737036375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748574.0814228959, 748574.0814228959, 189378.8006929159], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6244200.0000, 
sim time next is 6244800.0000, 
raw observation next is [27.33333333333334, 87.66666666666667, 1.0, 2.0, 0.535774913296884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 748679.8157333675, 748679.8157333675, 189391.5421148086], 
processed observation next is [0.0, 0.2608695652173913, 0.4944707740916275, 0.8766666666666667, 1.0, 1.0, 0.44069266662275175, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20796661548149095, 0.20796661548149095, 0.28267394345493824], 
reward next is 0.7173, 
noisyNet noise sample is [array([-0.69152415], dtype=float32), 0.0860218]. 
=============================================
[2019-04-27 23:08:44,988] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4249941e-27 1.0000000e+00 1.7978465e-38 9.8438255e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:08:44,999] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0040
[2019-04-27 23:08:45,004] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333334, 89.66666666666667, 1.0, 2.0, 0.5264028862061872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735579.0213199375, 735579.0213199375, 187836.5630195024], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6240000.0000, 
sim time next is 6240600.0000, 
raw observation next is [26.8, 89.5, 1.0, 2.0, 0.5272986350211235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736831.1469815219, 736831.1469815212, 187984.0942567497], 
processed observation next is [0.0, 0.21739130434782608, 0.4691943127962086, 0.895, 1.0, 1.0, 0.4304802831579801, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2046753186059783, 0.2046753186059781, 0.2805732750100742], 
reward next is 0.7194, 
noisyNet noise sample is [array([-1.6478219], dtype=float32), -0.60927975]. 
=============================================
[2019-04-27 23:08:51,458] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.3382123e-28 1.0000000e+00 0.0000000e+00 1.4840150e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:08:51,465] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4215
[2019-04-27 23:08:51,470] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.85, 66.66666666666666, 1.0, 2.0, 0.539469530378493, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753844.4225476262, 753844.4225476262, 190011.4529554805], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6346200.0000, 
sim time next is 6346800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5402909341979382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754992.6444811684, 754992.6444811678, 190149.8576792844], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.44613365566016644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20972017902254678, 0.2097201790225466, 0.2838057577302752], 
reward next is 0.7162, 
noisyNet noise sample is [array([0.914388], dtype=float32), -2.0030189]. 
=============================================
[2019-04-27 23:08:58,838] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.480170e-29 1.000000e+00 0.000000e+00 9.087716e-24 0.000000e+00], sum to 1.0000
[2019-04-27 23:08:58,842] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3774
[2019-04-27 23:08:58,848] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333333, 91.0, 1.0, 2.0, 0.7272886022556856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1016424.794027235, 1016424.794027235, 226844.8089140306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6489600.0000, 
sim time next is 6490200.0000, 
raw observation next is [26.31666666666667, 91.0, 1.0, 2.0, 0.6992266518543517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977188.671049151, 977188.671049151, 220677.4762813017], 
processed observation next is [1.0, 0.08695652173913043, 0.4462875197472356, 0.91, 1.0, 1.0, 0.6376224721136767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27144129751365303, 0.27144129751365303, 0.3293693675840324], 
reward next is 0.6706, 
noisyNet noise sample is [array([0.8426191], dtype=float32), 0.15797287]. 
=============================================
[2019-04-27 23:09:00,606] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4534335e-13 2.5263196e-03 6.8670733e-23 9.9747366e-01 4.3269194e-28], sum to 1.0000
[2019-04-27 23:09:00,616] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3327
[2019-04-27 23:09:00,625] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.1, 57.0, 1.0, 2.0, 0.7054051163103569, 1.0, 1.0, 0.7054051163103569, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1972552.815740993, 1972552.815740993, 376436.2051370308], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6519600.0000, 
sim time next is 6520200.0000, 
raw observation next is [31.15, 56.83333333333333, 1.0, 2.0, 0.710337427842434, 1.0, 2.0, 0.710337427842434, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1986358.023911553, 1986358.023911553, 378570.3633002929], 
processed observation next is [1.0, 0.4782608695652174, 0.6753554502369667, 0.5683333333333332, 1.0, 1.0, 0.6510089492077519, 1.0, 1.0, 0.6510089492077519, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5517661177532092, 0.5517661177532092, 0.5650303929855118], 
reward next is 0.4350, 
noisyNet noise sample is [array([-0.11523798], dtype=float32), -0.78616357]. 
=============================================
[2019-04-27 23:09:08,679] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5064095e-30 1.0000000e+00 0.0000000e+00 1.9478667e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:09:08,687] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2180
[2019-04-27 23:09:08,693] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333333, 87.33333333333333, 1.0, 2.0, 0.5039062422202693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704132.4916524874, 704132.4916524874, 184210.3413430082], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6649800.0000, 
sim time next is 6650400.0000, 
raw observation next is [26.26666666666667, 87.66666666666667, 1.0, 2.0, 0.5033134755043966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703303.9159347689, 703303.9159347682, 184116.8241436439], 
processed observation next is [1.0, 1.0, 0.44391785150079005, 0.8766666666666667, 1.0, 1.0, 0.4015825006077068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19536219887076914, 0.19536219887076894, 0.2748012300651401], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.7772792], dtype=float32), 0.23351288]. 
=============================================
[2019-04-27 23:09:34,156] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3020773e-22 1.0000000e+00 1.7821352e-30 1.5422004e-19 2.6520129e-30], sum to 1.0000
[2019-04-27 23:09:34,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3917
[2019-04-27 23:09:34,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2154350.727032921 W.
[2019-04-27 23:09:34,181] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.7, 53.5, 1.0, 2.0, 0.8845947750223521, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.948807873864834, 6.9112, 168.9127331365267, 2154350.727032921, 2127670.442232677, 434454.3130179688], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7053000.0000, 
sim time next is 7053600.0000, 
raw observation next is [30.5, 55.0, 1.0, 2.0, 0.7741671735783425, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.948986736985689, 6.9112, 168.9126998954268, 1996648.663686429, 1969841.492675567, 405553.3494785323], 
processed observation next is [1.0, 0.6521739130434783, 0.6445497630331753, 0.55, 1.0, 1.0, 0.7279122573233042, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0037786736985689017, 0.0, 0.8294386850551533, 0.5546246288017858, 0.5471781924098797, 0.6053035066843766], 
reward next is 0.2058, 
noisyNet noise sample is [array([2.0152779], dtype=float32), -0.6827148]. 
=============================================
[2019-04-27 23:09:34,816] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-27 23:09:34,817] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:09:34,818] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:09:34,819] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:09:34,819] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:09:34,819] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:09:34,820] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:09:34,819] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:09:34,821] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:09:34,824] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:09:34,824] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:09:34,844] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run17
[2019-04-27 23:09:34,844] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run17
[2019-04-27 23:09:34,845] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run17
[2019-04-27 23:09:34,918] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run17
[2019-04-27 23:09:34,938] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run17
[2019-04-27 23:09:37,937] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:09:37,938] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.45, 40.0, 1.0, 2.0, 0.4308048168822926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715197.9878604923, 715197.987860493, 183868.8906255657]
[2019-04-27 23:09:37,939] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:09:37,946] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.9865888e-27 1.0000000e+00 3.1402370e-38 3.7770379e-25 2.1540893e-38], sampled 0.46748095285209423
[2019-04-27 23:10:04,167] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:04,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.15952222, 90.04611708333334, 1.0, 2.0, 0.3738320160691304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 567326.9482163612, 567326.9482163605, 171884.7151921743]
[2019-04-27 23:10:04,169] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:10:04,173] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0259719e-28 1.0000000e+00 0.0000000e+00 4.4204033e-26 0.0000000e+00], sampled 0.710038700664218
[2019-04-27 23:10:16,101] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:16,104] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.82398238166667, 86.37884890833334, 1.0, 2.0, 0.4192284371611933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632154.9117762223, 632154.9117762223, 177705.017797715]
[2019-04-27 23:10:16,107] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:10:16,110] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0044019e-27 1.0000000e+00 0.0000000e+00 7.1252727e-26 0.0000000e+00], sampled 0.06743037098325255
[2019-04-27 23:10:18,389] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:18,390] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.9, 77.5, 1.0, 2.0, 0.4997083423811424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104271, 698264.6358355493, 698264.6358355493, 183553.2797640573]
[2019-04-27 23:10:18,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:10:18,394] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.014011e-27 1.000000e+00 0.000000e+00 7.198985e-26 0.000000e+00], sampled 0.24468661263547886
[2019-04-27 23:10:37,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:37,952] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.287129145, 65.434703595, 1.0, 2.0, 0.5566858858416739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777911.038447152, 777911.038447152, 192955.9612876953]
[2019-04-27 23:10:37,953] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:10:37,955] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.0891598e-30 1.0000000e+00 0.0000000e+00 3.2049263e-28 0.0000000e+00], sampled 0.9575559935352924
[2019-04-27 23:10:38,017] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:38,018] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 74.0, 1.0, 2.0, 0.8747429575698875, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1222618.795375424, 1222618.795375424, 263081.4571804818]
[2019-04-27 23:10:38,021] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:10:38,023] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.8410651e-27 1.0000000e+00 3.0307154e-38 3.6918377e-25 2.0783430e-38], sampled 0.8463464089462346
[2019-04-27 23:10:40,403] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:40,406] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.66666666666667, 73.0, 1.0, 2.0, 0.5404450279479359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755208.0488084322, 755208.0488084329, 190175.6901563578]
[2019-04-27 23:10:40,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:10:40,412] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.0019900e-29 1.0000000e+00 0.0000000e+00 3.5084096e-27 0.0000000e+00], sampled 0.6765744716538336
[2019-04-27 23:10:43,231] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:10:43,233] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.46666666666667, 65.0, 1.0, 2.0, 0.9903961049324337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104219, 1384371.342886472, 1384371.342886473, 296010.0362952484]
[2019-04-27 23:10:43,234] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:10:43,237] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.3213042e-28 1.0000000e+00 0.0000000e+00 5.3057287e-26 0.0000000e+00], sampled 0.4323326582512984
[2019-04-27 23:11:15,633] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:11:15,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.16666666666667, 63.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.937785078680928, 6.9112, 168.9123770576083, 1472628.150558249, 1453767.845454361, 311352.2933329373]
[2019-04-27 23:11:15,636] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:11:15,642] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8935568e-25 1.0000000e+00 4.4197067e-36 9.5385700e-24 3.0921449e-36], sampled 0.4383140498207606
[2019-04-27 23:11:30,620] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.073753834]
[2019-04-27 23:11:30,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.51699143, 75.98697813, 1.0, 2.0, 0.4156205673243012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618563.768279244, 618563.768279244, 176224.9915421789]
[2019-04-27 23:11:30,622] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:11:30,624] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3429876e-27 1.0000000e+00 0.0000000e+00 9.3511424e-26 0.0000000e+00], sampled 0.8991579043693145
[2019-04-27 23:11:46,065] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:11:46,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:11:46,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:11:46,169] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:11:46,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:11:47,662] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 400000, evaluation results [400000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:11:50,757] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.1902298e-27 1.0000000e+00 8.6212646e-38 9.0991982e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 23:11:50,763] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3133
[2019-04-27 23:11:50,770] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.4, 88.0, 1.0, 2.0, 0.4750357433376128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664685.6372309809, 664685.6372309809, 179894.1851140492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7081200.0000, 
sim time next is 7081800.0000, 
raw observation next is [25.36666666666667, 88.16666666666667, 1.0, 2.0, 0.4745974721251046, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664380.7659595414, 664380.7659595414, 179868.5737645611], 
processed observation next is [1.0, 1.0, 0.40126382306477115, 0.8816666666666667, 1.0, 1.0, 0.3669849061748248, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1845502127665393, 0.1845502127665393, 0.26846055785755385], 
reward next is 0.7315, 
noisyNet noise sample is [array([-1.117762], dtype=float32), 3.1673486]. 
=============================================
[2019-04-27 23:11:52,042] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4196313e-26 1.0000000e+00 1.6832034e-37 2.3598801e-25 1.4092154e-37], sum to 1.0000
[2019-04-27 23:11:52,054] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9020
[2019-04-27 23:11:52,061] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 92.16666666666667, 1.0, 2.0, 0.772374062045634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1084727.429762709, 1084727.429762708, 237972.5326901757], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7092600.0000, 
sim time next is 7093200.0000, 
raw observation next is [24.7, 92.33333333333334, 1.0, 2.0, 0.6809797864940342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 957410.1870051591, 957410.1870051591, 217556.9984728628], 
processed observation next is [1.0, 0.08695652173913043, 0.3696682464454976, 0.9233333333333335, 1.0, 1.0, 0.6156382969807641, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26594727416809977, 0.26594727416809977, 0.3247119380191982], 
reward next is 0.6753, 
noisyNet noise sample is [array([-0.64946747], dtype=float32), 0.7487395]. 
=============================================
[2019-04-27 23:11:52,921] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0387188e-28 1.0000000e+00 0.0000000e+00 8.5067108e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:11:52,931] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8440
[2019-04-27 23:11:52,938] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 80.16666666666667, 1.0, 2.0, 0.50004453226819, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711263.0992687366, 711263.0992687372, 185205.4327044014], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7110600.0000, 
sim time next is 7111200.0000, 
raw observation next is [26.33333333333334, 79.33333333333334, 1.0, 2.0, 0.4756000108491052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675383.9255396517, 675383.9255396517, 181250.010066206], 
processed observation next is [1.0, 0.30434782608695654, 0.44707740916271754, 0.7933333333333334, 1.0, 1.0, 0.3681927841555484, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18760664598323656, 0.18760664598323656, 0.2705224030838896], 
reward next is 0.7295, 
noisyNet noise sample is [array([0.9460592], dtype=float32), 2.4238079]. 
=============================================
[2019-04-27 23:11:56,185] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.3541209e-29 1.0000000e+00 0.0000000e+00 6.6278106e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:11:56,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8828
[2019-04-27 23:11:56,199] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.47720091044263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666986.696240317, 666986.696240317, 180124.2261184937], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7160400.0000, 
sim time next is 7161000.0000, 
raw observation next is [25.96666666666667, 84.0, 1.0, 2.0, 0.4793260540578593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 670566.7363261781, 670566.7363261781, 180522.1158581421], 
processed observation next is [1.0, 0.9130434782608695, 0.42969984202211703, 0.84, 1.0, 1.0, 0.37268199284079434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1862685378683828, 0.1862685378683828, 0.2694359938181225], 
reward next is 0.7306, 
noisyNet noise sample is [array([0.8571249], dtype=float32), -1.6718668]. 
=============================================
[2019-04-27 23:11:56,220] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[72.67156 ]
 [72.313965]
 [72.6542  ]
 [72.505295]
 [72.42127 ]], R is [[72.60714722]
 [72.61222839]
 [72.61751556]
 [72.62272644]
 [72.62781525]].
[2019-04-27 23:11:57,061] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3330876e-24 1.0000000e+00 3.3491792e-34 3.9010986e-25 1.7949035e-34], sum to 1.0000
[2019-04-27 23:11:57,070] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7074
[2019-04-27 23:11:57,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 83.5, 1.0, 2.0, 0.8715517392595451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1303050.687787422, 1303050.687787421, 274202.9493283784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7230600.0000, 
sim time next is 7231200.0000, 
raw observation next is [24.33333333333334, 82.66666666666667, 1.0, 2.0, 0.8681887734307624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1301619.070462619, 1301619.070462619, 273715.6453515848], 
processed observation next is [1.0, 0.6956521739130435, 0.35229067930489766, 0.8266666666666667, 1.0, 1.0, 0.8411912932900751, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36156085290628304, 0.36156085290628304, 0.4085308139575893], 
reward next is 0.5915, 
noisyNet noise sample is [array([0.2107145], dtype=float32), -0.2007951]. 
=============================================
[2019-04-27 23:12:07,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.75979610e-28 1.00000000e+00 0.00000000e+00 1.18435774e-23
 0.00000000e+00], sum to 1.0000
[2019-04-27 23:12:07,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5727
[2019-04-27 23:12:07,745] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.91666666666667, 72.83333333333333, 1.0, 2.0, 0.5686544204305177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876473.0143512196, 876473.0143512203, 204890.7498148936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7351800.0000, 
sim time next is 7352400.0000, 
raw observation next is [24.83333333333334, 73.66666666666667, 1.0, 2.0, 0.4606569003450017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 709042.0398696957, 709042.0398696951, 185532.1371341849], 
processed observation next is [1.0, 0.08695652173913043, 0.3759873617693526, 0.7366666666666667, 1.0, 1.0, 0.350189036560243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19695612218602657, 0.1969561221860264, 0.2769136375137088], 
reward next is 0.7231, 
noisyNet noise sample is [array([1.5667042], dtype=float32), -0.652642]. 
=============================================
[2019-04-27 23:12:07,868] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8995230e-27 1.0000000e+00 0.0000000e+00 1.2979676e-22 7.1058483e-38], sum to 1.0000
[2019-04-27 23:12:07,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3699
[2019-04-27 23:12:07,880] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 88.5, 1.0, 2.0, 0.3238150250920718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524529.2638232014, 524529.263823202, 168898.2585364995], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7405800.0000, 
sim time next is 7406400.0000, 
raw observation next is [20.53333333333333, 88.0, 1.0, 2.0, 0.2823665952888118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 457425.8603242738, 457425.8603242744, 164084.863050205], 
processed observation next is [1.0, 0.7391304347826086, 0.17219589257503945, 0.88, 1.0, 1.0, 0.13538144010700218, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12706273897896495, 0.1270627389789651, 0.24490278067194776], 
reward next is 0.7551, 
noisyNet noise sample is [array([0.8850516], dtype=float32), 0.2439522]. 
=============================================
[2019-04-27 23:12:15,402] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.8817602e-32 1.0000000e+00 0.0000000e+00 3.6822362e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:12:15,413] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3256
[2019-04-27 23:12:15,419] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 88.0, 1.0, 2.0, 0.4038394038054958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595894.2537914377, 595894.2537914377, 173960.5580025301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7507200.0000, 
sim time next is 7507800.0000, 
raw observation next is [24.05, 88.5, 1.0, 2.0, 0.4036417151594379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595228.1077983001, 595228.1077982995, 173887.0774089706], 
processed observation next is [0.0, 0.9130434782608695, 0.3388625592417062, 0.885, 1.0, 1.0, 0.2814960423607686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16534114105508335, 0.1653411410550832, 0.25953295135667254], 
reward next is 0.7405, 
noisyNet noise sample is [array([0.8144488], dtype=float32), -1.4422483]. 
=============================================
[2019-04-27 23:12:18,136] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.805313e-28 1.000000e+00 0.000000e+00 5.773392e-26 0.000000e+00], sum to 1.0000
[2019-04-27 23:12:18,146] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3125
[2019-04-27 23:12:18,149] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.9, 90.0, 1.0, 2.0, 0.4051850920970963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 596411.0850560442, 596411.0850560435, 173962.7340567657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7509600.0000, 
sim time next is 7510200.0000, 
raw observation next is [23.85, 90.33333333333333, 1.0, 2.0, 0.4051719434541921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596370.3290199444, 596370.3290199444, 173958.2841457388], 
processed observation next is [0.0, 0.9565217391304348, 0.3293838862559243, 0.9033333333333333, 1.0, 1.0, 0.2833396909086652, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16565842472776235, 0.16565842472776235, 0.2596392300682669], 
reward next is 0.7404, 
noisyNet noise sample is [array([-0.74985194], dtype=float32), 1.3684868]. 
=============================================
[2019-04-27 23:12:18,493] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1434727e-32 1.0000000e+00 0.0000000e+00 2.2776473e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:12:18,501] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8810
[2019-04-27 23:12:18,509] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4046505640864498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595742.8224814701, 595742.8224814708, 173904.6966147587], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7522800.0000, 
sim time next is 7523400.0000, 
raw observation next is [23.5, 93.0, 1.0, 2.0, 0.4078350447546752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 600431.5050611332, 600431.5050611338, 174339.1243004749], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 1.0, 1.0, 0.28654824669237977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1667865291836481, 0.16678652918364828, 0.260207648209664], 
reward next is 0.7398, 
noisyNet noise sample is [array([-0.88042545], dtype=float32), 0.45703462]. 
=============================================
[2019-04-27 23:12:18,692] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9080071e-35 1.0000000e+00 0.0000000e+00 1.4263363e-32 0.0000000e+00], sum to 1.0000
[2019-04-27 23:12:18,706] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8052
[2019-04-27 23:12:18,713] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.51666666666667, 93.0, 1.0, 2.0, 0.4072154426557484, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 599021.1350886654, 599021.1350886661, 174192.8168834619], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7519800.0000, 
sim time next is 7520400.0000, 
raw observation next is [23.5, 93.0, 1.0, 2.0, 0.4061989509719723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 597897.7361913783, 597897.7361913783, 174100.098180348], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 1.0, 1.0, 0.28457704936382205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1660827044976051, 0.1660827044976051, 0.25985089280648954], 
reward next is 0.7401, 
noisyNet noise sample is [array([1.0725919], dtype=float32), 2.7784653]. 
=============================================
[2019-04-27 23:12:20,249] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.3383942e-30 1.0000000e+00 0.0000000e+00 4.1138592e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:12:20,260] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5652
[2019-04-27 23:12:20,263] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.58333333333333, 72.16666666666667, 1.0, 2.0, 0.4528783460931059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640800.7608636237, 640800.7608636237, 177575.6038553252], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7582200.0000, 
sim time next is 7582800.0000, 
raw observation next is [27.46666666666667, 73.33333333333334, 1.0, 2.0, 0.4558510037865461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 642940.3347757935, 642940.3347757929, 177741.299175611], 
processed observation next is [0.0, 0.782608695652174, 0.500789889415482, 0.7333333333333334, 1.0, 1.0, 0.34439879974282656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1785945374377204, 0.17859453743772025, 0.2652855211576284], 
reward next is 0.7347, 
noisyNet noise sample is [array([-0.12319795], dtype=float32), 0.08309412]. 
=============================================
[2019-04-27 23:12:24,706] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.0199593e-22 1.0000000e+00 4.8353839e-33 1.9423448e-20 1.6095391e-31], sum to 1.0000
[2019-04-27 23:12:24,715] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0409
[2019-04-27 23:12:24,724] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2104647.832920549 W.
[2019-04-27 23:12:24,730] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.33333333333333, 63.0, 1.0, 2.0, 0.8640001207512346, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.970296466225721, 6.9112, 168.9125574851674, 2104647.832920549, 2062722.864876755, 425461.4467986762], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7656600.0000, 
sim time next is 7657200.0000, 
raw observation next is [30.2, 64.0, 1.0, 2.0, 0.6254621180076846, 1.0, 1.0, 0.6254621180076846, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1748822.544071713, 1748822.544071713, 343840.1493204739], 
processed observation next is [1.0, 0.6521739130434783, 0.6303317535545023, 0.64, 1.0, 1.0, 0.5487495397682947, 1.0, 0.5, 0.5487495397682947, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48578404001992026, 0.48578404001992026, 0.5131942527171253], 
reward next is 0.4868, 
noisyNet noise sample is [array([0.25896764], dtype=float32), -0.5607282]. 
=============================================
[2019-04-27 23:12:27,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8796268e-27 1.0000000e+00 1.2382452e-38 1.3565972e-26 2.3047498e-38], sum to 1.0000
[2019-04-27 23:12:27,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9628
[2019-04-27 23:12:27,866] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.05, 91.0, 1.0, 2.0, 0.4763874576163864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665667.1187408757, 665667.1187408757, 179978.8588216557], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7691400.0000, 
sim time next is 7692000.0000, 
raw observation next is [25.0, 91.33333333333333, 1.0, 2.0, 0.4773952216361586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667075.7326485447, 667075.7326485447, 180129.786353574], 
processed observation next is [1.0, 0.0, 0.38388625592417064, 0.9133333333333333, 1.0, 1.0, 0.3703556887182634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18529881462459574, 0.18529881462459574, 0.268850427393394], 
reward next is 0.7311, 
noisyNet noise sample is [array([-0.15832728], dtype=float32), -1.699752]. 
=============================================
[2019-04-27 23:12:27,890] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[64.16789 ]
 [63.963474]
 [64.00857 ]
 [64.52868 ]
 [65.39342 ]], R is [[64.3687439 ]
 [64.45642853]
 [64.54336548]
 [64.62924957]
 [64.71395874]].
[2019-04-27 23:12:31,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.7678653e-30 1.0000000e+00 0.0000000e+00 3.3801238e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:12:31,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6742
[2019-04-27 23:12:31,021] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.46666666666667, 68.83333333333333, 1.0, 2.0, 0.4776343206228006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 667409.9363188114, 667409.9363188114, 180167.8236839646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7753800.0000, 
sim time next is 7754400.0000, 
raw observation next is [29.3, 70.0, 1.0, 2.0, 0.4886406812334785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682794.3316196293, 682794.3316196299, 181836.7736899109], 
processed observation next is [1.0, 0.782608695652174, 0.5876777251184835, 0.7, 1.0, 1.0, 0.3839044352210584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18966509211656368, 0.18966509211656385, 0.27139816968643415], 
reward next is 0.7286, 
noisyNet noise sample is [array([-1.2719235], dtype=float32), 1.6743201]. 
=============================================
[2019-04-27 23:12:41,119] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.4169516e-25 1.0000000e+00 9.5940635e-34 1.3407087e-21 1.0908220e-33], sum to 1.0000
[2019-04-27 23:12:41,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7092
[2019-04-27 23:12:41,134] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.63333333333333, 63.0, 1.0, 2.0, 0.343073954179076, 1.0, 2.0, 0.343073954179076, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 958898.338311645, 958898.338311645, 258735.4685910108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7924200.0000, 
sim time next is 7924800.0000, 
raw observation next is [30.46666666666667, 64.0, 1.0, 2.0, 0.4704702557259771, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104282, 657396.3184934863, 657396.3184934863, 179101.0412728067], 
processed observation next is [1.0, 0.7391304347826086, 0.6429699842022119, 0.64, 1.0, 1.0, 0.3620123562963579, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522889, 0.18261008847041285, 0.18261008847041285, 0.26731498697433836], 
reward next is 0.7327, 
noisyNet noise sample is [array([-0.38233206], dtype=float32), 0.87039554]. 
=============================================
[2019-04-27 23:12:41,154] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4469255e-23 1.0000000e+00 4.5630702e-34 3.3245815e-22 5.7918348e-33], sum to 1.0000
[2019-04-27 23:12:41,160] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7129
[2019-04-27 23:12:41,169] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1898452.156432743 W.
[2019-04-27 23:12:41,177] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.63333333333333, 72.0, 1.0, 2.0, 0.4526196117870718, 1.0, 2.0, 0.4526196117870718, 1.0, 1.0, 0.7796179804004941, 6.9112, 6.9112, 170.5573041426782, 1898452.156432743, 1898452.156432743, 382888.6963935549], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7902600.0000, 
sim time next is 7903200.0000, 
raw observation next is [29.66666666666667, 72.0, 1.0, 2.0, 0.8810112237102886, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.98894016710179, 6.9112, 168.9124942882909, 2128457.805090228, 2073306.40585771, 429474.3616026841], 
processed observation next is [1.0, 0.4782608695652174, 0.6050552922590839, 0.72, 1.0, 1.0, 0.8566400285666128, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007774016710178966, 0.0, 0.8294376754299886, 0.59123827919173, 0.575918446071586, 0.6410065098547524], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.49348423], dtype=float32), 0.88262194]. 
=============================================
[2019-04-27 23:12:43,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:43,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:43,129] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run3
[2019-04-27 23:12:43,573] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:43,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:43,593] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run3
[2019-04-27 23:12:43,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:43,933] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:43,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:43,940] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:43,956] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run3
[2019-04-27 23:12:43,972] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run3
[2019-04-27 23:12:44,107] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:44,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:44,116] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run3
[2019-04-27 23:12:44,134] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:44,135] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:44,138] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run3
[2019-04-27 23:12:44,166] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:44,166] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:44,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run3
[2019-04-27 23:12:44,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:44,241] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:44,243] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run3
[2019-04-27 23:12:44,515] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:44,515] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:44,517] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run3
[2019-04-27 23:12:45,079] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run3
[2019-04-27 23:12:45,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,114] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,115] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,117] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,119] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,123] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run3
[2019-04-27 23:12:45,151] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,153] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run3
[2019-04-27 23:12:45,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,194] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run3
[2019-04-27 23:12:45,218] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run3
[2019-04-27 23:12:45,250] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:12:45,255] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:45,255] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run3
[2019-04-27 23:12:45,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run3
[2019-04-27 23:12:46,792] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 23:12:46,794] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:12:46,795] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:12:46,796] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:12:46,795] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:46,798] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:46,800] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:46,796] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:12:46,797] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:12:46,803] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:46,807] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:12:46,822] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run18
[2019-04-27 23:12:46,823] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run18
[2019-04-27 23:12:46,845] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run18
[2019-04-27 23:12:46,907] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run18
[2019-04-27 23:12:46,908] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run18
[2019-04-27 23:12:49,391] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.0640706]
[2019-04-27 23:12:49,395] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.8, 96.0, 1.0, 2.0, 0.6470694877448426, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 963756.13857955, 963756.13857955, 217418.7816625081]
[2019-04-27 23:12:49,396] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:12:49,399] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.1185397e-31 1.0000000e+00 0.0000000e+00 3.1084312e-28 0.0000000e+00], sampled 0.9093371024728111
[2019-04-27 23:14:22,610] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.0640706]
[2019-04-27 23:14:22,611] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.01166311, 49.31993996, 1.0, 2.0, 0.5641543040559405, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788351.254583219, 788351.254583219, 194254.8468621274]
[2019-04-27 23:14:22,612] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:14:22,615] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1385710e-34 1.0000000e+00 0.0000000e+00 1.4144439e-31 0.0000000e+00], sampled 0.14869373785795748
[2019-04-27 23:14:32,942] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.0640706]
[2019-04-27 23:14:32,943] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.1, 83.0, 1.0, 2.0, 0.5122285982526144, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715765.6390373865, 715765.6390373865, 185533.9812585267]
[2019-04-27 23:14:32,945] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:14:32,946] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8577119e-32 1.0000000e+00 0.0000000e+00 1.5464990e-29 0.0000000e+00], sampled 0.15303176261692653
[2019-04-27 23:14:56,851] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:14:57,670] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:14:57,771] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:14:57,884] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:14:57,925] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:14:58,941] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 425000, evaluation results [425000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:15:01,777] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8013115e-23 1.0000000e+00 2.5947190e-33 4.6732183e-21 8.1522590e-34], sum to 1.0000
[2019-04-27 23:15:01,790] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7185
[2019-04-27 23:15:01,804] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 62.66666666666667, 1.0, 2.0, 0.8106494063434205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1215766.281177766, 1215766.281177767, 257815.2429750311], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 48000.0000, 
sim time next is 48600.0000, 
raw observation next is [27.7, 62.5, 1.0, 2.0, 0.883599305018736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1322282.517607203, 1322282.517607203, 277843.0599871189], 
processed observation next is [1.0, 0.5652173913043478, 0.5118483412322274, 0.625, 1.0, 1.0, 0.8597581988177542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.36730069933533416, 0.36730069933533416, 0.4146911343091326], 
reward next is 0.5853, 
noisyNet noise sample is [array([-0.02693344], dtype=float32), 0.43262148]. 
=============================================
[2019-04-27 23:15:08,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9790339e-35 1.0000000e+00 0.0000000e+00 1.8079086e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:08,984] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6965
[2019-04-27 23:15:08,994] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.43333333333333, 93.0, 1.0, 2.0, 0.2930741634207988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 470432.0629111052, 470432.0629111046, 164986.5679789401], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 202800.0000, 
sim time next is 203400.0000, 
raw observation next is [20.45, 93.0, 1.0, 2.0, 0.2934348620499913, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 470853.9445166664, 470853.9445166664, 165015.1063738504], 
processed observation next is [0.0, 0.34782608695652173, 0.16824644549763035, 0.93, 1.0, 1.0, 0.14871670126504977, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13079276236574067, 0.13079276236574067, 0.2462912035430603], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.4470161], dtype=float32), 0.09835126]. 
=============================================
[2019-04-27 23:15:09,124] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4810087e-29 1.0000000e+00 0.0000000e+00 4.1587534e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:09,133] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0068
[2019-04-27 23:15:09,139] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.38333333333333, 87.0, 1.0, 2.0, 0.2617277092614976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 425882.882816951, 425882.8828169516, 162003.8217120457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 349800.0000, 
sim time next is 350400.0000, 
raw observation next is [20.36666666666667, 87.0, 1.0, 2.0, 0.2611485861593384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425052.7378845677, 425052.7378845677, 161949.4078572044], 
processed observation next is [1.0, 0.043478260869565216, 0.1642969984202214, 0.87, 1.0, 1.0, 0.10981757368594985, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11807020496793547, 0.11807020496793547, 0.24171553411523047], 
reward next is 0.7583, 
noisyNet noise sample is [array([0.18496965], dtype=float32), 0.57468134]. 
=============================================
[2019-04-27 23:15:22,526] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6889828e-33 1.0000000e+00 0.0000000e+00 1.0015991e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:22,532] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5140
[2019-04-27 23:15:22,539] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.43333333333333, 72.16666666666667, 1.0, 2.0, 0.4887070843447773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794751.4259406975, 794751.4259406975, 193622.5392612843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 385800.0000, 
sim time next is 386400.0000, 
raw observation next is [22.46666666666667, 72.33333333333334, 1.0, 2.0, 0.4855222838000752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788872.5015319347, 788872.5015319347, 193032.9468206623], 
processed observation next is [1.0, 0.4782608695652174, 0.2638230647709322, 0.7233333333333334, 1.0, 1.0, 0.3801473298796087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2191312504255374, 0.2191312504255374, 0.2881088758517348], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.24232423], dtype=float32), 1.903325]. 
=============================================
[2019-04-27 23:15:26,108] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.707652e-31 1.000000e+00 0.000000e+00 1.241492e-27 0.000000e+00], sum to 1.0000
[2019-04-27 23:15:26,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1589
[2019-04-27 23:15:26,125] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.0, 1.0, 2.0, 0.2376285661083507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 392091.3341572003, 392091.3341572003, 159676.3655643652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 438600.0000, 
sim time next is 439200.0000, 
raw observation next is [19.6, 85.0, 1.0, 2.0, 0.2375883467431448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392025.2315532363, 392025.2315532357, 159672.5643642883], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.85, 1.0, 1.0, 0.08143174306402987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10889589765367676, 0.10889589765367658, 0.2383172602452064], 
reward next is 0.7617, 
noisyNet noise sample is [array([0.5481445], dtype=float32), 0.20651397]. 
=============================================
[2019-04-27 23:15:27,312] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.1433596e-29 1.0000000e+00 0.0000000e+00 4.8636128e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:27,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6568
[2019-04-27 23:15:27,328] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.86666666666666, 82.0, 1.0, 2.0, 0.229338830578283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 378904.9235685278, 378904.9235685278, 158880.38700392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 453000.0000, 
sim time next is 453600.0000, 
raw observation next is [19.9, 82.0, 1.0, 2.0, 0.2303264801330011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380402.9795722726, 380402.9795722726, 158978.7952441557], 
processed observation next is [1.0, 0.2608695652173913, 0.14218009478672985, 0.82, 1.0, 1.0, 0.07268250618433868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10566749432563129, 0.10566749432563129, 0.23728178394650107], 
reward next is 0.7627, 
noisyNet noise sample is [array([-1.3229833], dtype=float32), 1.0874584]. 
=============================================
[2019-04-27 23:15:46,444] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.1650455e-31 1.0000000e+00 0.0000000e+00 2.6245518e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:46,454] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7605
[2019-04-27 23:15:46,458] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.28333333333334, 72.0, 1.0, 2.0, 0.2879159252798174, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461782.0591718462, 461782.0591718456, 164385.9963950096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 810600.0000, 
sim time next is 811200.0000, 
raw observation next is [23.46666666666667, 71.0, 1.0, 2.0, 0.2884861582285086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462428.4785197963, 462428.4785197969, 164428.0509224132], 
processed observation next is [0.0, 0.391304347826087, 0.31121642969984215, 0.71, 1.0, 1.0, 0.14275440750422722, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12845235514438785, 0.12845235514438802, 0.24541500137673614], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.28200093], dtype=float32), -0.49928528]. 
=============================================
[2019-04-27 23:15:48,190] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6955156e-33 1.0000000e+00 0.0000000e+00 1.2863843e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:48,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6713
[2019-04-27 23:15:48,205] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 89.66666666666667, 1.0, 2.0, 0.266010431261114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433403.7071040743, 433403.7071040743, 162465.6624087682], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 798000.0000, 
sim time next is 798600.0000, 
raw observation next is [20.15, 88.83333333333334, 1.0, 2.0, 0.2669562477402215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 434605.4772025349, 434605.4772025342, 162550.7975552343], 
processed observation next is [0.0, 0.21739130434782608, 0.15402843601895733, 0.8883333333333334, 1.0, 1.0, 0.11681475631351984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1207237436673708, 0.12072374366737061, 0.2426131306794542], 
reward next is 0.7574, 
noisyNet noise sample is [array([-0.3693127], dtype=float32), 1.3636626]. 
=============================================
[2019-04-27 23:15:48,402] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.7106641e-37 1.0000000e+00 0.0000000e+00 3.4809404e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:48,413] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3157
[2019-04-27 23:15:48,424] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.33333333333334, 1.0, 2.0, 0.2604042190692589, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426021.8213545502, 426021.8213545502, 161939.8462004638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 789600.0000, 
sim time next is 790200.0000, 
raw observation next is [19.4, 92.5, 1.0, 2.0, 0.2604571569572372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 425989.4158092547, 425989.4158092541, 161942.9033441317], 
processed observation next is [0.0, 0.13043478260869565, 0.11848341232227487, 0.925, 1.0, 1.0, 0.10898452645450264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11833039328034853, 0.11833039328034836, 0.2417058258867637], 
reward next is 0.7583, 
noisyNet noise sample is [array([0.84405977], dtype=float32), -0.69257826]. 
=============================================
[2019-04-27 23:15:51,537] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.3276925e-30 1.0000000e+00 0.0000000e+00 4.8267922e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:51,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3952
[2019-04-27 23:15:51,549] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2939414835487605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469210.9502239273, 469210.9502239273, 164875.3876123086], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 895800.0000, 
sim time next is 896400.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2941999846469605, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469623.3579305353, 469623.3579305353, 164904.2378586496], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14963853571922953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13045093275848202, 0.13045093275848202, 0.2461257281472382], 
reward next is 0.7539, 
noisyNet noise sample is [array([-1.28269], dtype=float32), -0.053256813]. 
=============================================
[2019-04-27 23:15:54,775] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.4454519e-33 1.0000000e+00 0.0000000e+00 2.1887381e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:15:54,785] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6401
[2019-04-27 23:15:54,792] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.76666666666667, 71.5, 1.0, 2.0, 0.4523465566367458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703886.8113209782, 703886.8113209788, 184994.2462644377], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1084200.0000, 
sim time next is 1084800.0000, 
raw observation next is [24.93333333333334, 71.0, 1.0, 2.0, 0.597826819227571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 928211.6839156456, 928211.6839156456, 211449.8592102245], 
processed observation next is [1.0, 0.5652173913043478, 0.3807266982622437, 0.71, 1.0, 1.0, 0.5154539990693626, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25783657886545713, 0.25783657886545713, 0.31559680479137986], 
reward next is 0.6844, 
noisyNet noise sample is [array([0.7573925], dtype=float32), 0.14865069]. 
=============================================
[2019-04-27 23:15:57,339] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-27 23:15:57,341] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:15:57,342] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:15:57,345] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:15:57,346] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:15:57,346] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:15:57,348] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:15:57,348] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:15:57,349] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:15:57,348] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:15:57,351] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:15:57,371] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run19
[2019-04-27 23:15:57,399] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run19
[2019-04-27 23:15:57,400] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run19
[2019-04-27 23:15:57,400] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run19
[2019-04-27 23:15:57,477] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run19
[2019-04-27 23:16:11,378] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:16:11,382] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.37763004333333, 87.56118795, 1.0, 2.0, 0.258574139859548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 422833.6284873747, 422833.6284873747, 161750.7155639327]
[2019-04-27 23:16:11,383] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:16:11,385] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.5431727e-27 1.0000000e+00 1.2961220e-37 5.4056743e-25 1.2227792e-37], sampled 0.9092880221141841
[2019-04-27 23:16:29,230] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:16:29,231] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.5, 94.33333333333334, 1.0, 2.0, 0.4259573929352983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622513.4018254412, 622513.4018254412, 176304.953978988]
[2019-04-27 23:16:29,232] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:16:29,234] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.789873e-28 1.000000e+00 0.000000e+00 7.270325e-26 0.000000e+00], sampled 0.281208173309646
[2019-04-27 23:16:36,358] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:16:36,358] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.34755462, 68.06012245, 1.0, 2.0, 0.9318433167626576, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989710989637, 6.9112, 168.9123159582803, 2199608.038354004, 2132361.209982446, 443055.8659890342]
[2019-04-27 23:16:36,359] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:16:36,361] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4728307e-26 1.0000000e+00 3.3372244e-37 1.0101787e-24 3.1484069e-37], sampled 0.0883131032605805
[2019-04-27 23:16:36,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2199608.038354004 W.
[2019-04-27 23:17:18,178] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:17:18,183] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.08353572, 73.75304424, 1.0, 2.0, 0.4683949376553338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666838.2187785715, 666838.2187785715, 180376.6810166879]
[2019-04-27 23:17:18,184] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:17:18,189] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2804664e-28 1.0000000e+00 0.0000000e+00 2.0639368e-26 0.0000000e+00], sampled 0.20114036634587595
[2019-04-27 23:17:18,295] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:17:18,296] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.46666666666666, 47.5, 1.0, 2.0, 0.5626010425760903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786179.9181340468, 786179.9181340468, 193984.6252335144]
[2019-04-27 23:17:18,299] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:17:18,301] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.3702907e-29 1.0000000e+00 0.0000000e+00 5.3530537e-27 0.0000000e+00], sampled 0.06317757325186957
[2019-04-27 23:18:05,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.057171386]
[2019-04-27 23:18:05,363] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.18708660166667, 81.04913722500001, 1.0, 2.0, 0.6093226063425841, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851494.9028825172, 851494.9028825172, 202495.617101772]
[2019-04-27 23:18:05,365] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:18:05,369] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6315188e-28 1.0000000e+00 0.0000000e+00 3.1860114e-26 0.0000000e+00], sampled 0.9743774562600529
[2019-04-27 23:18:07,515] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:18:07,623] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:18:08,321] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:18:08,334] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:18:08,553] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:18:09,570] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 450000, evaluation results [450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:18:19,516] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7082833e-30 1.0000000e+00 0.0000000e+00 5.0928632e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:19,528] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8451
[2019-04-27 23:18:19,537] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.63333333333334, 68.66666666666667, 1.0, 2.0, 0.6260929258838839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 963043.2271748716, 963043.2271748709, 216480.3294812276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1088400.0000, 
sim time next is 1089000.0000, 
raw observation next is [25.65, 68.5, 1.0, 2.0, 0.6503885145067244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1000715.377150935, 1000715.377150935, 221819.5131032153], 
processed observation next is [1.0, 0.6086956521739131, 0.41469194312796204, 0.685, 1.0, 1.0, 0.578781342779186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2779764936530375, 0.2779764936530375, 0.3310739001540527], 
reward next is 0.6689, 
noisyNet noise sample is [array([0.08654428], dtype=float32), 2.042527]. 
=============================================
[2019-04-27 23:18:19,559] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[71.24884]
 [71.18246]
 [70.94778]
 [70.91801]
 [70.89048]], R is [[71.22425842]
 [71.18891144]
 [71.15461731]
 [71.0941391 ]
 [71.04608917]].
[2019-04-27 23:18:20,061] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9346725e-29 1.0000000e+00 0.0000000e+00 3.4543460e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:20,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4699
[2019-04-27 23:18:20,078] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.13333333333333, 69.5, 1.0, 2.0, 0.332286620568118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515907.662497577, 515907.6624975776, 168005.839660841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1101000.0000, 
sim time next is 1101600.0000, 
raw observation next is [25.0, 70.0, 1.0, 2.0, 0.3344799801869318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 519988.7483065563, 519988.7483065563, 168346.3379643153], 
processed observation next is [1.0, 0.782608695652174, 0.38388625592417064, 0.7, 1.0, 1.0, 0.1981686508276287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14444131897404341, 0.14444131897404341, 0.25126319099151534], 
reward next is 0.7487, 
noisyNet noise sample is [array([0.5715024], dtype=float32), 0.035919495]. 
=============================================
[2019-04-27 23:18:20,861] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5193151e-28 1.0000000e+00 1.3295643e-37 8.2631637e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:20,866] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7115
[2019-04-27 23:18:20,870] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.3, 70.33333333333334, 1.0, 2.0, 0.800817054196954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1234726.663193558, 1234726.663193559, 259279.9750308123], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1161600.0000, 
sim time next is 1162200.0000, 
raw observation next is [25.5, 69.66666666666666, 1.0, 2.0, 0.8224270581344343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1264903.845157319, 1264903.845157319, 264872.7545283222], 
processed observation next is [1.0, 0.43478260869565216, 0.40758293838862564, 0.6966666666666665, 1.0, 1.0, 0.7860566965475112, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3513621792103664, 0.3513621792103664, 0.395332469445257], 
reward next is 0.6047, 
noisyNet noise sample is [array([1.3759295], dtype=float32), -0.18037657]. 
=============================================
[2019-04-27 23:18:21,141] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.2284789e-29 1.0000000e+00 0.0000000e+00 8.0683104e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:21,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0859
[2019-04-27 23:18:21,159] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 71.0, 1.0, 2.0, 0.7515355220082233, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1161592.620794898, 1161592.620794897, 246580.2009313029], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1161000.0000, 
sim time next is 1161600.0000, 
raw observation next is [25.3, 70.33333333333334, 1.0, 2.0, 0.800817054196954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1234726.663193558, 1234726.663193559, 259279.9750308123], 
processed observation next is [1.0, 0.43478260869565216, 0.39810426540284366, 0.7033333333333335, 1.0, 1.0, 0.7600205472252458, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34297962866487725, 0.3429796286648775, 0.3869850373594213], 
reward next is 0.6130, 
noisyNet noise sample is [array([-0.99679273], dtype=float32), 1.483776]. 
=============================================
[2019-04-27 23:18:31,212] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5558090e-26 1.0000000e+00 7.8249358e-38 1.0156152e-24 6.5552388e-37], sum to 1.0000
[2019-04-27 23:18:31,221] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4621
[2019-04-27 23:18:31,227] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.91666666666666, 95.0, 1.0, 2.0, 0.7253751878885295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1080365.046095802, 1080365.046095803, 235179.6522792216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1335000.0000, 
sim time next is 1335600.0000, 
raw observation next is [22.9, 95.0, 1.0, 2.0, 0.7969583734124992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1187707.119992839, 1187707.119992838, 253212.0967466673], 
processed observation next is [1.0, 0.4782608695652174, 0.2843601895734597, 0.95, 1.0, 1.0, 0.7553715342319267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3299186444424553, 0.329918644442455, 0.3779285026069661], 
reward next is 0.6221, 
noisyNet noise sample is [array([-1.162065], dtype=float32), -0.3587972]. 
=============================================
[2019-04-27 23:18:44,316] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3206056e-32 1.0000000e+00 0.0000000e+00 5.1139169e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:44,323] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5671
[2019-04-27 23:18:44,329] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 96.83333333333334, 1.0, 2.0, 0.3302525080932052, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 516865.1005311398, 516865.1005311398, 168194.9085333298], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1479000.0000, 
sim time next is 1479600.0000, 
raw observation next is [21.0, 97.0, 1.0, 2.0, 0.3277733108839336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 513609.3363222133, 513609.3363222133, 167957.7200052232], 
processed observation next is [0.0, 0.13043478260869565, 0.19431279620853087, 0.97, 1.0, 1.0, 0.19008832636618503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1426692600895037, 0.1426692600895037, 0.2506831641869003], 
reward next is 0.7493, 
noisyNet noise sample is [array([-0.17636429], dtype=float32), 0.6840397]. 
=============================================
[2019-04-27 23:18:49,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.3976929e-31 1.0000000e+00 0.0000000e+00 1.9489733e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:49,334] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4021
[2019-04-27 23:18:49,340] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 93.33333333333334, 1.0, 2.0, 0.7747718840818051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1148471.143816504, 1148471.143816504, 246683.8978784679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1614000.0000, 
sim time next is 1614600.0000, 
raw observation next is [23.2, 94.0, 1.0, 2.0, 0.7547092638807567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1118050.769086616, 1118050.769086617, 241571.1496205619], 
processed observation next is [1.0, 0.6956521739130435, 0.29857819905213273, 0.94, 1.0, 1.0, 0.7044689926274177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31056965807961556, 0.31056965807961584, 0.3605539546575551], 
reward next is 0.6394, 
noisyNet noise sample is [array([-1.0682912], dtype=float32), -1.4719336]. 
=============================================
[2019-04-27 23:18:53,177] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.2762697e-32 1.0000000e+00 0.0000000e+00 2.6003524e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 23:18:53,185] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5396
[2019-04-27 23:18:53,195] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 95.33333333333334, 1.0, 2.0, 0.4118738087639186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606635.0646343129, 606635.0646343129, 174926.319172735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1628400.0000, 
sim time next is 1629000.0000, 
raw observation next is [23.2, 95.5, 1.0, 2.0, 0.4137812188467885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608971.9607899502, 608971.9607899502, 175132.4409361442], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.955, 1.0, 1.0, 0.2937123118636006, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16915887799720838, 0.16915887799720838, 0.2613917028897675], 
reward next is 0.7386, 
noisyNet noise sample is [array([0.11583037], dtype=float32), -0.69379103]. 
=============================================
[2019-04-27 23:18:53,213] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.17741 ]
 [75.08449 ]
 [75.13815 ]
 [75.1208  ]
 [75.145035]], R is [[75.19734955]
 [75.18428802]
 [75.17147827]
 [75.15878296]
 [75.14601898]].
[2019-04-27 23:18:53,519] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5661308e-23 1.0000000e+00 1.4546569e-32 9.7902238e-21 2.1465401e-33], sum to 1.0000
[2019-04-27 23:18:53,528] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1768
[2019-04-27 23:18:53,538] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 88.0, 1.0, 2.0, 0.9123370336778922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1275195.184950111, 1275195.184950111, 273350.8277725301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1681200.0000, 
sim time next is 1681800.0000, 
raw observation next is [26.0, 87.66666666666667, 1.0, 2.0, 0.8621616290912381, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1205024.029461767, 1205024.029461767, 259738.3870755889], 
processed observation next is [1.0, 0.4782608695652174, 0.4312796208530806, 0.8766666666666667, 1.0, 1.0, 0.8339296736039012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3347288970727131, 0.3347288970727131, 0.38766923444117746], 
reward next is 0.6123, 
noisyNet noise sample is [array([-0.03253222], dtype=float32), 0.48572814]. 
=============================================
[2019-04-27 23:18:56,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0512043e-26 1.0000000e+00 5.9229075e-38 8.3901120e-27 5.9812853e-37], sum to 1.0000
[2019-04-27 23:18:56,399] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4104
[2019-04-27 23:18:56,407] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.15, 94.0, 1.0, 2.0, 0.4930067856161018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688897.216802289, 688897.216802289, 182507.62037615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1729800.0000, 
sim time next is 1730400.0000, 
raw observation next is [25.1, 94.0, 1.0, 2.0, 0.4911164248082961, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686254.890200783, 686254.8902007823, 182215.8789517085], 
processed observation next is [1.0, 0.0, 0.38862559241706174, 0.94, 1.0, 1.0, 0.386887258805176, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1906263583891064, 0.19062635838910622, 0.2719639984353858], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.61120886], dtype=float32), 0.089754246]. 
=============================================
[2019-04-27 23:19:01,105] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.1260211e-32 1.0000000e+00 0.0000000e+00 2.3512285e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:19:01,114] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3809
[2019-04-27 23:19:01,118] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 94.0, 1.0, 2.0, 0.4629610546274237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656232.777746873, 656232.7777468737, 179198.3239041596], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1742400.0000, 
sim time next is 1743000.0000, 
raw observation next is [24.28333333333333, 94.00000000000001, 1.0, 2.0, 0.5416862005381788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768554.3567444338, 768554.3567444343, 191871.5166570789], 
processed observation next is [1.0, 0.17391304347826086, 0.34992101105845175, 0.9400000000000002, 1.0, 1.0, 0.4478146994435889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21348732131789827, 0.21348732131789844, 0.28637539799564016], 
reward next is 0.7136, 
noisyNet noise sample is [array([-0.6625165], dtype=float32), 0.7402097]. 
=============================================
[2019-04-27 23:19:01,149] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[77.817085]
 [78.16975 ]
 [78.193535]
 [78.28486 ]
 [78.45367 ]], R is [[77.36856079]
 [77.32741547]
 [77.289711  ]
 [77.2518692 ]
 [77.21307373]].
[2019-04-27 23:19:04,381] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5586289e-29 1.0000000e+00 0.0000000e+00 3.1090139e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:19:04,394] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3574
[2019-04-27 23:19:04,400] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.45, 94.83333333333333, 1.0, 2.0, 0.471409588423889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660603.3525919177, 660603.3525919183, 179482.4667023973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1997400.0000, 
sim time next is 1998000.0000, 
raw observation next is [24.4, 95.0, 1.0, 2.0, 0.4695653630414371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658875.7304304363, 658875.7304304363, 179319.5416612722], 
processed observation next is [0.0, 0.13043478260869565, 0.3554502369668246, 0.95, 1.0, 1.0, 0.3609221241463098, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18302103623067675, 0.18302103623067675, 0.2676411069571227], 
reward next is 0.7324, 
noisyNet noise sample is [array([-1.0402546], dtype=float32), -0.4758006]. 
=============================================
[2019-04-27 23:19:04,421] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[71.53068 ]
 [71.591805]
 [71.68374 ]
 [72.09164 ]
 [72.300865]], R is [[71.83226013]
 [71.84605408]
 [71.85956573]
 [71.87294006]
 [71.88613892]].
[2019-04-27 23:19:07,586] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2899708e-26 1.0000000e+00 7.2480206e-37 3.5325475e-24 3.3926756e-37], sum to 1.0000
[2019-04-27 23:19:07,594] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6947
[2019-04-27 23:19:07,604] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1909406.89148773 W.
[2019-04-27 23:19:07,610] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.4, 74.5, 1.0, 2.0, 0.4552290605535048, 1.0, 1.0, 0.4552290605535048, 1.0, 1.0, 0.7651448982907022, 6.911199999999999, 6.9112, 170.5573041426782, 1909406.89148773, 1909406.89148773, 381327.460313509], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [27.46666666666667, 74.33333333333333, 1.0, 2.0, 0.6419739448925276, 1.0, 2.0, 0.6419739448925276, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1795029.086012104, 1795029.086012105, 350260.3097429968], 
processed observation next is [1.0, 0.5652173913043478, 0.500789889415482, 0.7433333333333333, 1.0, 1.0, 0.5686433070994308, 1.0, 1.0, 0.5686433070994308, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4986191905589178, 0.49861919055891807, 0.5227765817059654], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14628589], dtype=float32), 0.40413067]. 
=============================================
[2019-04-27 23:19:07,640] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.530323]
 [61.823936]
 [62.807796]
 [62.7934  ]
 [63.32839 ]], R is [[60.97130966]
 [60.79245377]
 [60.18452835]
 [60.15819931]
 [60.16711807]].
[2019-04-27 23:19:08,619] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 23:19:08,619] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:19:08,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:19:08,620] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:19:08,622] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:19:08,620] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:19:08,622] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:19:08,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:19:08,623] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:19:08,623] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:19:08,626] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:19:08,648] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run20
[2019-04-27 23:19:08,650] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run20
[2019-04-27 23:19:08,650] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run20
[2019-04-27 23:19:08,651] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run20
[2019-04-27 23:19:08,651] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run20
[2019-04-27 23:19:09,706] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:09,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.4, 64.5, 1.0, 2.0, 0.479648042394997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692896.4615053739, 692896.4615053739, 183350.6115317455]
[2019-04-27 23:19:09,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:19:09,710] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7590915e-29 1.0000000e+00 0.0000000e+00 2.5362548e-27 0.0000000e+00], sampled 0.7052765977571487
[2019-04-27 23:19:10,078] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:10,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.58333333333334, 66.16666666666667, 1.0, 2.0, 0.9631632426345235, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9128148338188, 1346281.230932234, 1346281.230932234, 287900.3905837151]
[2019-04-27 23:19:10,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:19:10,084] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0027629e-28 1.0000000e+00 0.0000000e+00 1.2199952e-26 0.0000000e+00], sampled 0.8244559333181044
[2019-04-27 23:19:21,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:21,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.56980072333334, 53.73763392666666, 1.0, 2.0, 0.2906592776360186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473633.4964234911, 473633.4964234911, 165128.7465596409]
[2019-04-27 23:19:21,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:19:21,786] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7825239e-28 1.0000000e+00 0.0000000e+00 1.6607433e-26 0.0000000e+00], sampled 0.5760632067481818
[2019-04-27 23:19:39,593] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:39,594] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.2, 86.5, 1.0, 2.0, 0.37398026248421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 572001.5514209801, 572001.5514209796, 172418.7252591502]
[2019-04-27 23:19:39,596] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:19:39,599] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7867228e-29 1.0000000e+00 0.0000000e+00 3.1820136e-27 0.0000000e+00], sampled 0.24772469369481187
[2019-04-27 23:19:40,023] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:40,024] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.56666666666667, 84.0, 1.0, 2.0, 0.5952889141730185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 909360.8490708873, 909360.8490708879, 209347.7690139222]
[2019-04-27 23:19:40,025] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:19:40,031] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.041472e-28 1.000000e+00 0.000000e+00 6.602100e-27 0.000000e+00], sampled 0.5830275621623477
[2019-04-27 23:19:42,162] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:42,163] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.5, 88.33333333333334, 1.0, 2.0, 0.5408388472486584, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129564443689, 781992.6985421347, 781992.6985421347, 193570.8571005182]
[2019-04-27 23:19:42,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:19:42,170] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6230442e-28 1.0000000e+00 0.0000000e+00 1.0018577e-26 0.0000000e+00], sampled 0.9544729367634542
[2019-04-27 23:19:47,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:47,295] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.26666666666667, 75.33333333333333, 1.0, 2.0, 0.6982862673080539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 975873.8536989326, 975873.8536989326, 220473.8462968949]
[2019-04-27 23:19:47,299] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:19:47,301] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7872839e-28 1.0000000e+00 0.0000000e+00 1.0960900e-26 0.0000000e+00], sampled 0.13667253669303336
[2019-04-27 23:19:52,578] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:19:52,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.23333333333333, 90.66666666666667, 1.0, 2.0, 0.518927788324336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725129.9867546567, 725129.9867546561, 186614.6635048635]
[2019-04-27 23:19:52,580] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:19:52,582] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4716323e-29 1.0000000e+00 0.0000000e+00 1.0516128e-27 0.0000000e+00], sampled 0.7374618810160395
[2019-04-27 23:20:02,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:20:02,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.48402849, 58.78629924, 1.0, 2.0, 0.341611306274296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 535552.0142012541, 535552.0142012541, 169693.874160142]
[2019-04-27 23:20:02,079] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:20:02,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8586717e-29 1.0000000e+00 0.0000000e+00 1.3093276e-27 0.0000000e+00], sampled 0.10756303780136156
[2019-04-27 23:20:46,108] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:20:46,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.08333333333334, 94.16666666666667, 1.0, 2.0, 0.5084645254834032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710504.1308242374, 710504.1308242374, 184931.8795463392]
[2019-04-27 23:20:46,110] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:20:46,115] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9532323e-30 1.0000000e+00 0.0000000e+00 7.2882283e-28 0.0000000e+00], sampled 0.8356649297321725
[2019-04-27 23:21:03,129] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06309729]
[2019-04-27 23:21:03,130] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.575943685, 51.473391295, 1.0, 2.0, 0.3701435878660004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563276.3671229712, 563276.3671229718, 171579.6641386974]
[2019-04-27 23:21:03,131] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:21:03,136] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.3319867e-30 1.0000000e+00 0.0000000e+00 6.1657443e-28 0.0000000e+00], sampled 0.8575011494236225
[2019-04-27 23:21:19,209] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:21:19,774] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:21:19,787] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:21:19,822] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:21:20,174] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:21:21,191] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 475000, evaluation results [475000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:21:22,505] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1145843e-24 1.0000000e+00 2.5638721e-34 4.0021721e-22 7.7920674e-34], sum to 1.0000
[2019-04-27 23:21:22,512] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0238
[2019-04-27 23:21:22,524] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1808198.075227051 W.
[2019-04-27 23:21:22,531] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.53333333333333, 74.16666666666667, 1.0, 2.0, 0.6521687640146447, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.959856139159344, 6.9112, 168.9126735388948, 1808198.075227051, 1773679.793319212, 376057.3405222574], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1950600.0000, 
sim time next is 1951200.0000, 
raw observation next is [27.6, 74.0, 1.0, 2.0, 0.6601228771025149, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.958545770303629, 6.9112, 168.9126348370209, 1819328.051413024, 1785739.396409452, 377703.7467006461], 
processed observation next is [1.0, 0.6086956521739131, 0.5071090047393366, 0.74, 1.0, 1.0, 0.5905094904849577, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.004734577030362885, 0.0, 0.8294383655885964, 0.5053689031702845, 0.49603872122484777, 0.5637369353740986], 
reward next is 0.1995, 
noisyNet noise sample is [array([0.00135054], dtype=float32), 0.928849]. 
=============================================
[2019-04-27 23:21:23,752] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.2801606e-31 1.0000000e+00 0.0000000e+00 4.5076135e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:23,761] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6121
[2019-04-27 23:21:23,766] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333334, 83.83333333333334, 1.0, 2.0, 1.008442683325105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9126810343946, 1443100.531123237, 1443100.531123237, 306757.5958559882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1959000.0000, 
sim time next is 1959600.0000, 
raw observation next is [25.26666666666667, 84.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.098293308255682, 6.9112, 168.9119481209484, 1624688.738012027, 1491959.098265013, 317423.7145905334], 
processed observation next is [1.0, 0.6956521739130435, 0.3965244865718801, 0.8466666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.018709330825568228, 0.0, 0.8294349934983004, 0.4513024272255631, 0.41443308285139246, 0.47376673819482595], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.34087974], dtype=float32), 1.214422]. 
=============================================
[2019-04-27 23:21:24,408] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.5209823e-35 1.0000000e+00 0.0000000e+00 2.0925736e-32 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:24,418] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5259
[2019-04-27 23:21:24,424] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 94.0, 1.0, 2.0, 0.5037625457317685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703931.6310894974, 703931.6310894967, 184188.0953561375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2016000.0000, 
sim time next is 2016600.0000, 
raw observation next is [25.6, 94.00000000000001, 1.0, 2.0, 0.5044118205129783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704839.1949613115, 704839.1949613115, 184290.5711192366], 
processed observation next is [0.0, 0.34782608695652173, 0.4123222748815167, 0.9400000000000002, 1.0, 1.0, 0.4029058078469618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19578866526703098, 0.19578866526703098, 0.2750605539093084], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.07928444], dtype=float32), 2.019258]. 
=============================================
[2019-04-27 23:21:24,767] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0525273e-28 1.0000000e+00 0.0000000e+00 2.5122326e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:24,774] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3650
[2019-04-27 23:21:24,779] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 95.66666666666667, 1.0, 2.0, 0.4016734466723614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 597582.7548619639, 597582.7548619632, 174263.4205400444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1975200.0000, 
sim time next is 1975800.0000, 
raw observation next is [22.95, 95.83333333333333, 1.0, 2.0, 0.4047787068330637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600682.9922490107, 600682.9922490107, 174504.5394089108], 
processed observation next is [1.0, 0.8695652173913043, 0.28672985781990523, 0.9583333333333333, 1.0, 1.0, 0.2828659118470647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1668563867358363, 0.1668563867358363, 0.26045453643121014], 
reward next is 0.7395, 
noisyNet noise sample is [array([-0.18635614], dtype=float32), 0.41866514]. 
=============================================
[2019-04-27 23:21:25,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.113911e-30 1.000000e+00 0.000000e+00 3.257086e-28 0.000000e+00], sum to 1.0000
[2019-04-27 23:21:25,188] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7747
[2019-04-27 23:21:25,191] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.8, 87.0, 1.0, 2.0, 0.4274254435734332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617571.7663213384, 617571.7663213384, 175626.4153832308], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1926000.0000, 
sim time next is 1926600.0000, 
raw observation next is [24.91666666666667, 86.33333333333333, 1.0, 2.0, 0.482797476369263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696694.9731691432, 696694.9731691432, 183750.4049189779], 
processed observation next is [1.0, 0.30434782608695654, 0.37993680884676173, 0.8633333333333333, 1.0, 1.0, 0.37686442936055786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1935263814358731, 0.1935263814358731, 0.27425433569996704], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.5661967], dtype=float32), 0.35185087]. 
=============================================
[2019-04-27 23:21:37,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4187973e-28 1.0000000e+00 0.0000000e+00 5.2951282e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:37,720] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4818
[2019-04-27 23:21:37,723] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.2, 79.5, 1.0, 2.0, 0.5599434419713749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782464.8134040869, 782464.8134040869, 193520.9843603424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2140200.0000, 
sim time next is 2140800.0000, 
raw observation next is [29.03333333333333, 80.33333333333333, 1.0, 2.0, 0.5604960811354082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783237.3559466929, 783237.3559466923, 193617.3095072526], 
processed observation next is [0.0, 0.782608695652174, 0.5750394944707741, 0.8033333333333332, 1.0, 1.0, 0.4704772061872387, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2175659322074147, 0.21756593220741452, 0.28898105896604864], 
reward next is 0.7110, 
noisyNet noise sample is [array([-0.62919736], dtype=float32), -0.31921494]. 
=============================================
[2019-04-27 23:21:38,667] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.9924701e-28 1.0000000e+00 2.6353734e-37 6.9718585e-25 1.1019833e-36], sum to 1.0000
[2019-04-27 23:21:38,678] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2476
[2019-04-27 23:21:38,683] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.5246303416972946, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733101.2680343774, 733101.2680343774, 187545.0784434577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2156400.0000, 
sim time next is 2157000.0000, 
raw observation next is [26.05, 93.0, 1.0, 2.0, 0.5228074776523001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 730553.1815580953, 730553.1815580946, 187246.6412671545], 
processed observation next is [0.0, 1.0, 0.43364928909952616, 0.93, 1.0, 1.0, 0.425069250183494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20293143932169314, 0.20293143932169294, 0.27947259890620074], 
reward next is 0.7205, 
noisyNet noise sample is [array([-0.9095552], dtype=float32), -0.590723]. 
=============================================
[2019-04-27 23:21:38,697] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[62.70313 ]
 [62.731247]
 [62.78288 ]
 [62.831646]
 [62.88726 ]], R is [[62.73147583]
 [62.82424164]
 [62.91568375]
 [63.00585175]
 [63.09482574]].
[2019-04-27 23:21:45,868] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.1668578e-30 1.0000000e+00 0.0000000e+00 1.7247018e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:45,881] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8132
[2019-04-27 23:21:45,891] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.93333333333333, 66.66666666666666, 1.0, 2.0, 0.5393925598794129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753736.8272809098, 753736.8272809092, 190001.1652270584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2310000.0000, 
sim time next is 2310600.0000, 
raw observation next is [31.81666666666667, 67.33333333333334, 1.0, 2.0, 0.5449708193366625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761534.5743908766, 761534.574390876, 190944.5480114457], 
processed observation next is [1.0, 0.7391304347826086, 0.7069510268562403, 0.6733333333333335, 1.0, 1.0, 0.45177207148995474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2115373817752435, 0.21153738177524334, 0.2849918627036503], 
reward next is 0.7150, 
noisyNet noise sample is [array([-1.6405232], dtype=float32), 0.34748825]. 
=============================================
[2019-04-27 23:21:49,051] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4696417e-20 1.0000000e+00 7.3102734e-30 9.7999106e-19 5.7035125e-28], sum to 1.0000
[2019-04-27 23:21:49,066] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.7341
[2019-04-27 23:21:49,073] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1934650.123138147 W.
[2019-04-27 23:21:49,079] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.1, 70.0, 1.0, 2.0, 0.6918629518588528, 1.0, 1.0, 0.6918629518588528, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1934650.123138147, 1934650.123138147, 370659.4852901665], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2364000.0000, 
sim time next is 2364600.0000, 
raw observation next is [30.25, 69.5, 1.0, 2.0, 0.4613591109923623, 1.0, 2.0, 0.4613591109923623, 1.0, 1.0, 0.7965080815840713, 6.9112, 6.9112, 170.5573041426782, 1935141.916111085, 1935141.916111085, 388675.4165307505], 
processed observation next is [1.0, 0.34782608695652173, 0.6327014218009479, 0.695, 1.0, 1.0, 0.3510350734847739, 1.0, 1.0, 0.3510350734847739, 1.0, 0.5, 0.7518391238830138, 0.0, 0.0, 0.8375144448122397, 0.5375394211419681, 0.5375394211419681, 0.5801125619861948], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.42751095], dtype=float32), -0.9123659]. 
=============================================
[2019-04-27 23:21:51,340] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8378826e-26 1.0000000e+00 5.8479721e-36 1.5874581e-25 2.1473874e-37], sum to 1.0000
[2019-04-27 23:21:51,351] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9724
[2019-04-27 23:21:51,356] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.63333333333334, 78.66666666666667, 1.0, 2.0, 0.5723458124047743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 799802.4074891193, 799802.4074891187, 195707.1499520162], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2413200.0000, 
sim time next is 2413800.0000, 
raw observation next is [29.55, 79.0, 1.0, 2.0, 0.5718864987981999, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799160.316084342, 799160.316084342, 195625.3288737926], 
processed observation next is [1.0, 0.9565217391304348, 0.5995260663507109, 0.79, 1.0, 1.0, 0.48420060096168654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22198897669009499, 0.22198897669009499, 0.2919781027967054], 
reward next is 0.7080, 
noisyNet noise sample is [array([-1.4399691], dtype=float32), -0.12911516]. 
=============================================
[2019-04-27 23:21:51,428] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3437082e-30 1.0000000e+00 0.0000000e+00 3.8629643e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:21:51,436] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7110
[2019-04-27 23:21:51,443] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.4, 65.0, 1.0, 2.0, 0.55414718820447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774362.1775313123, 774362.1775313123, 192516.7319889667], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2397600.0000, 
sim time next is 2398200.0000, 
raw observation next is [32.28333333333333, 65.66666666666667, 1.0, 2.0, 0.5635218040779693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787467.0692054107, 787467.0692054107, 194148.3641803628], 
processed observation next is [1.0, 0.782608695652174, 0.7290679304897314, 0.6566666666666667, 1.0, 1.0, 0.4741226555156256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21874085255705852, 0.21874085255705852, 0.2897736778811385], 
reward next is 0.7102, 
noisyNet noise sample is [array([-1.3546348], dtype=float32), -1.1475054]. 
=============================================
[2019-04-27 23:22:10,359] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.1117378e-28 1.0000000e+00 0.0000000e+00 9.1233797e-26 1.3831658e-38], sum to 1.0000
[2019-04-27 23:22:10,371] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9095
[2019-04-27 23:22:10,378] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 100.0, 1.0, 2.0, 0.4591589106539988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 650969.4285451074, 650969.4285451068, 178653.963590918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2709000.0000, 
sim time next is 2709600.0000, 
raw observation next is [23.33333333333333, 100.0, 1.0, 2.0, 0.4503384064312532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 643133.9493691779, 643133.9493691786, 177966.7730345486], 
processed observation next is [0.0, 0.34782608695652173, 0.30489731437598716, 1.0, 1.0, 1.0, 0.33775711618223275, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1786483192692161, 0.1786483192692163, 0.2656220493052964], 
reward next is 0.7344, 
noisyNet noise sample is [array([1.0294131], dtype=float32), 1.7223926]. 
=============================================
[2019-04-27 23:22:11,063] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9838377e-26 1.0000000e+00 2.3901434e-37 1.8463167e-24 2.0146624e-36], sum to 1.0000
[2019-04-27 23:22:11,077] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6099
[2019-04-27 23:22:11,081] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4657907796082175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657914.0540058315, 657914.0540058315, 179319.7866438999], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2656800.0000, 
sim time next is 2657400.0000, 
raw observation next is [24.83333333333334, 89.0, 1.0, 2.0, 0.4570686967275243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650237.9931508697, 650237.9931508697, 178632.8789596416], 
processed observation next is [0.0, 0.782608695652174, 0.3759873617693526, 0.89, 1.0, 1.0, 0.3458658996717161, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18062166476413047, 0.18062166476413047, 0.2666162372531964], 
reward next is 0.7334, 
noisyNet noise sample is [array([0.5717936], dtype=float32), 0.51287603]. 
=============================================
[2019-04-27 23:22:13,862] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5595697e-27 1.0000000e+00 4.3555504e-37 2.5405496e-23 2.2445173e-38], sum to 1.0000
[2019-04-27 23:22:13,874] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4317
[2019-04-27 23:22:13,878] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.6698743651496316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1031683.842355097, 1031683.842355097, 226323.6684308749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2912400.0000, 
sim time next is 2913000.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.4163884446079314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 641823.9082245174, 641823.9082245168, 178850.6932124772], 
processed observation next is [1.0, 0.7391304347826086, 0.23380726698262277, 0.95, 1.0, 1.0, 0.2968535477203993, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17828441895125482, 0.17828441895125466, 0.26694133315295104], 
reward next is 0.7331, 
noisyNet noise sample is [array([-0.00613383], dtype=float32), 0.5074578]. 
=============================================
[2019-04-27 23:22:13,898] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[65.97241 ]
 [66.17717 ]
 [66.06337 ]
 [65.754616]
 [65.82915 ]], R is [[66.63844299]
 [66.63426208]
 [66.6578064 ]
 [66.67351532]
 [66.65880585]].
[2019-04-27 23:22:19,479] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-27 23:22:19,481] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:22:19,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:22:19,482] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:22:19,484] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:22:19,485] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:22:19,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:22:19,486] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:22:19,487] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:22:19,487] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:22:19,487] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:22:19,493] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run21
[2019-04-27 23:22:19,493] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run21
[2019-04-27 23:22:19,540] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run21
[2019-04-27 23:22:19,541] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run21
[2019-04-27 23:22:19,581] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run21
[2019-04-27 23:23:20,214] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.063240334]
[2019-04-27 23:23:20,217] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.33067319666667, 63.63731583666667, 1.0, 2.0, 0.7121676949459808, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.003318204468163, 6.9112, 168.9123381981561, 1892155.844806582, 1826804.259315782, 387406.6568954029]
[2019-04-27 23:23:20,218] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:23:20,220] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.9734803e-27 1.0000000e+00 6.4427584e-38 2.0941792e-24 5.7422117e-38], sampled 0.43241759063618934
[2019-04-27 23:23:20,223] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1892155.844806582 W.
[2019-04-27 23:23:29,995] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.063240334]
[2019-04-27 23:23:29,998] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.93760559, 81.03986507, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.651155041662584, 6.9112, 168.9089171179996, 1979051.522474332, 1454114.502300761, 311366.1911242582]
[2019-04-27 23:23:30,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:23:30,007] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.16730915e-30 1.00000000e+00 0.00000000e+00 1.10058933e-27
 0.00000000e+00], sampled 0.6263958276392194
[2019-04-27 23:23:30,010] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1979051.522474332 W.
[2019-04-27 23:23:33,106] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.063240334]
[2019-04-27 23:23:33,109] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.36666666666667, 58.66666666666667, 1.0, 2.0, 0.5691254416884088, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9883826795531417, 6.9112, 6.9112, 168.9126087327072, 1591197.011451009, 1591197.011451009, 348209.5952189552]
[2019-04-27 23:23:33,113] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:23:33,117] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.34982757e-29 1.00000000e+00 0.00000000e+00 1.00426446e-26
 0.00000000e+00], sampled 0.3319042391633745
[2019-04-27 23:24:30,730] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:24:31,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:24:31,446] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:24:31,643] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:24:31,652] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:24:32,671] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 500000, evaluation results [500000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:24:43,801] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4379433e-31 1.0000000e+00 0.0000000e+00 1.2189200e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:24:43,811] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0230
[2019-04-27 23:24:43,819] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3379539573354989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 524904.8674610177, 524904.8674610184, 168721.9697361065], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3049200.0000, 
sim time next is 3049800.0000, 
raw observation next is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3446697329484064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534811.0765066483, 534811.0765066483, 169503.1329633374], 
processed observation next is [1.0, 0.30434782608695654, 0.2022116903633494, 0.9900000000000001, 1.0, 1.0, 0.2104454613836222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14855863236295788, 0.14855863236295788, 0.2529897506915484], 
reward next is 0.7470, 
noisyNet noise sample is [array([-0.48089823], dtype=float32), -0.9111576]. 
=============================================
[2019-04-27 23:24:50,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.0227275e-30 1.0000000e+00 0.0000000e+00 4.5054348e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:24:50,141] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8608
[2019-04-27 23:24:50,149] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.8859886559846346, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1287727.92825234, 1287727.928252339, 273280.8439283373], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3074400.0000, 
sim time next is 3075000.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.8404968436560365, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1219732.275116041, 1219732.275116041, 260476.2360193947], 
processed observation next is [1.0, 0.6086956521739131, 0.28909952606635075, 1.0, 1.0, 1.0, 0.8078275224771524, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33881452086556696, 0.33881452086556696, 0.3887705015214846], 
reward next is 0.6112, 
noisyNet noise sample is [array([-0.5211228], dtype=float32), 2.6067321]. 
=============================================
[2019-04-27 23:24:50,173] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.07725 ]
 [70.291794]
 [70.58418 ]
 [70.76195 ]
 [71.16658 ]], R is [[70.06187439]
 [69.95337677]
 [69.85108185]
 [69.75709534]
 [69.669487  ]].
[2019-04-27 23:24:58,795] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8105911e-25 1.0000000e+00 1.2003591e-38 4.1248969e-24 1.2105412e-36], sum to 1.0000
[2019-04-27 23:24:58,806] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8163
[2019-04-27 23:24:58,812] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 76.5, 1.0, 2.0, 0.5067806721666409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708150.4100429899, 708150.4100429906, 184666.289243801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3231000.0000, 
sim time next is 3231600.0000, 
raw observation next is [28.66666666666666, 77.33333333333333, 1.0, 2.0, 0.5156798263094226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720589.8685582536, 720589.8685582536, 186090.4566880261], 
processed observation next is [0.0, 0.391304347826087, 0.5576619273301735, 0.7733333333333333, 1.0, 1.0, 0.41648171844508736, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20016385237729267, 0.20016385237729267, 0.277746950280636], 
reward next is 0.7223, 
noisyNet noise sample is [array([-0.257008], dtype=float32), 0.23250113]. 
=============================================
[2019-04-27 23:25:01,760] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.3508901e-27 1.0000000e+00 0.0000000e+00 1.0761797e-23 1.4140665e-38], sum to 1.0000
[2019-04-27 23:25:01,768] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5579
[2019-04-27 23:25:01,775] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.0, 1.0, 2.0, 0.5350461803463259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 747661.1423160979, 747661.1423160972, 189270.173455255], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3321000.0000, 
sim time next is 3321600.0000, 
raw observation next is [30.33333333333333, 71.33333333333333, 1.0, 2.0, 0.5411709263616752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756222.7671212461, 756222.7671212461, 190299.097676928], 
processed observation next is [0.0, 0.43478260869565216, 0.6366508688783569, 0.7133333333333333, 1.0, 1.0, 0.44719388718274117, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21006187975590168, 0.21006187975590168, 0.28402850399541496], 
reward next is 0.7160, 
noisyNet noise sample is [array([0.5732], dtype=float32), -0.4018747]. 
=============================================
[2019-04-27 23:25:17,628] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.8130999e-25 1.0000000e+00 6.2365495e-34 6.6055867e-24 3.2549255e-35], sum to 1.0000
[2019-04-27 23:25:17,634] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8491
[2019-04-27 23:25:17,638] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.5512261777908358, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770278.8939225607, 770278.8939225607, 192011.4572317818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3531000.0000, 
sim time next is 3531600.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.5516599039562212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770885.1992047711, 770885.1992047705, 192085.9950600215], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.79, 1.0, 1.0, 0.4598312095858087, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21413477755688085, 0.2141347775568807, 0.28669551501495744], 
reward next is 0.7133, 
noisyNet noise sample is [array([-1.0426229], dtype=float32), -0.5909744]. 
=============================================
[2019-04-27 23:25:20,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2320923e-22 1.0000000e+00 1.7472771e-32 3.2767381e-21 1.4423020e-32], sum to 1.0000
[2019-04-27 23:25:20,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5641
[2019-04-27 23:25:20,661] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5219144311735413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729304.8407424163, 729304.8407424163, 187100.5978990221], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3621600.0000, 
sim time next is 3622200.0000, 
raw observation next is [28.0, 79.00000000000001, 1.0, 2.0, 0.5201811884542626, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726882.0397544084, 726882.0397544084, 186818.3332392653], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.7900000000000001, 1.0, 1.0, 0.4219050463304369, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2019116777095579, 0.2019116777095579, 0.2788333331929333], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.3812023], dtype=float32), 0.29307783]. 
=============================================
[2019-04-27 23:25:20,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4162346e-26 1.0000000e+00 6.7370369e-36 1.6394192e-24 1.9612927e-36], sum to 1.0000
[2019-04-27 23:25:20,855] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4377
[2019-04-27 23:25:20,860] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5243107355649014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732654.5068667321, 732654.5068667321, 187492.39677599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5219144311735413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729304.8407424163, 729304.8407424163, 187100.5978990221], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42399329057053164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20258467798400454, 0.20258467798400454, 0.27925462372988374], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.71039397], dtype=float32), -1.2298521]. 
=============================================
[2019-04-27 23:25:24,162] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9953727e-20 1.0000000e+00 1.6122223e-26 2.3381230e-18 9.5240125e-27], sum to 1.0000
[2019-04-27 23:25:24,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7923
[2019-04-27 23:25:24,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1873052.638136503 W.
[2019-04-27 23:25:24,192] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666667, 63.0, 1.0, 2.0, 0.6698539084402734, 1.0, 2.0, 0.6698539084402734, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1873052.638136503, 1873052.638136503, 361493.1558985763], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3674400.0000, 
sim time next is 3675000.0000, 
raw observation next is [32.83333333333333, 63.0, 1.0, 2.0, 0.4212101578753186, 1.0, 2.0, 0.4212101578753186, 1.0, 1.0, 0.7315027479016463, 6.9112, 6.9112, 170.5573041426782, 1766600.816493043, 1766600.816493043, 364940.9122634113], 
processed observation next is [1.0, 0.5217391304347826, 0.7551342812006318, 0.63, 1.0, 1.0, 0.3026628408136369, 1.0, 1.0, 0.3026628408136369, 1.0, 0.5, 0.6725643267093248, 0.0, 0.0, 0.8375144448122397, 0.4907224490258453, 0.4907224490258453, 0.5446879287513601], 
reward next is 0.4553, 
noisyNet noise sample is [array([-0.80843556], dtype=float32), -1.1061376]. 
=============================================
[2019-04-27 23:25:24,214] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[46.33921 ]
 [44.1799  ]
 [43.45952 ]
 [43.559723]
 [44.683533]], R is [[47.95740128]
 [47.93828583]
 [47.81056595]
 [47.68800354]
 [47.21112442]].
[2019-04-27 23:25:24,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.1964275e-27 1.0000000e+00 1.6872423e-35 1.5914373e-23 5.5993833e-37], sum to 1.0000
[2019-04-27 23:25:24,479] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6912
[2019-04-27 23:25:24,484] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 85.66666666666667, 1.0, 2.0, 0.5411973793647823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756259.7452430632, 756259.7452430632, 190302.7103807035], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3825600.0000, 
sim time next is 3826200.0000, 
raw observation next is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.5423248753624772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757835.8504527296, 757835.8504527302, 190493.3553043394], 
processed observation next is [0.0, 0.2608695652173913, 0.5181674565560824, 0.8483333333333333, 1.0, 1.0, 0.44858418718370746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21050995845909157, 0.21050995845909173, 0.28431844075274537], 
reward next is 0.7157, 
noisyNet noise sample is [array([0.35595253], dtype=float32), -0.46008655]. 
=============================================
[2019-04-27 23:25:28,725] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6995817e-26 1.0000000e+00 8.9823985e-37 3.4751795e-25 1.6204772e-35], sum to 1.0000
[2019-04-27 23:25:28,736] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1637
[2019-04-27 23:25:28,741] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5250013921539649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733619.94084384, 733619.9408438393, 187605.9176718834], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3701400.0000, 
sim time next is 3702000.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5240767536253047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732327.4355252552, 732327.4355252557, 187454.3107003784], 
processed observation next is [1.0, 0.8695652173913043, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4265984983437406, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2034242876459042, 0.20342428764590437, 0.27978255328414686], 
reward next is 0.7202, 
noisyNet noise sample is [array([0.33114812], dtype=float32), -0.72959864]. 
=============================================
[2019-04-27 23:25:28,756] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[60.56719 ]
 [60.586163]
 [60.49741 ]
 [60.73953 ]
 [61.089695]], R is [[60.78318024]
 [60.89533997]
 [61.00614548]
 [61.11465454]
 [61.22078323]].
[2019-04-27 23:25:31,833] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 23:25:31,836] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:25:31,840] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:25:31,841] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:25:31,841] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:25:31,843] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:25:31,842] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:25:31,845] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:25:31,845] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:25:31,845] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:25:31,848] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:25:31,859] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run22
[2019-04-27 23:25:31,884] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run22
[2019-04-27 23:25:31,913] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run22
[2019-04-27 23:25:31,939] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run22
[2019-04-27 23:25:31,940] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run22
[2019-04-27 23:25:58,598] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:25:58,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.63049350666667, 77.61556223666666, 1.0, 2.0, 0.3120521159734507, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498440.9602658827, 498440.9602658833, 166980.3824621177]
[2019-04-27 23:25:58,599] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:25:58,602] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.7423189e-30 1.0000000e+00 0.0000000e+00 1.4984281e-27 0.0000000e+00], sampled 0.2991186251828797
[2019-04-27 23:26:11,431] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:11,432] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.25763972, 56.43099049, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.912221129561908, 6.9112, 168.9128281362853, 1454479.847890215, 1453755.423984994, 311351.5428205568]
[2019-04-27 23:26:11,435] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:26:11,438] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.3684147e-28 1.0000000e+00 0.0000000e+00 1.9968918e-25 0.0000000e+00], sampled 0.09918392000449994
[2019-04-27 23:26:12,006] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:12,006] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.49698004333333, 59.29981044333334, 1.0, 2.0, 0.6606760559406504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 923289.7233008571, 923289.7233008564, 212596.9041369688]
[2019-04-27 23:26:12,007] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:26:12,009] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.7331172e-31 1.0000000e+00 0.0000000e+00 1.8045411e-28 0.0000000e+00], sampled 0.9637925578625345
[2019-04-27 23:26:13,972] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:13,972] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.124309645, 96.11929431666667, 1.0, 2.0, 0.821502748991709, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1148165.296516657, 1148165.296516657, 249265.9619591414]
[2019-04-27 23:26:13,974] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:26:13,979] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6564391e-28 1.0000000e+00 0.0000000e+00 4.0131256e-26 0.0000000e+00], sampled 0.8775697280282749
[2019-04-27 23:26:19,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:19,600] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.95967915333333, 90.45884952, 1.0, 2.0, 0.3840948180771001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581220.1340122984, 581220.1340122984, 173063.0933794077]
[2019-04-27 23:26:19,601] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:26:19,605] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8427915e-30 1.0000000e+00 0.0000000e+00 1.2357748e-27 0.0000000e+00], sampled 0.9102347503407445
[2019-04-27 23:26:31,525] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:31,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.19709817, 71.09333791333333, 1.0, 2.0, 0.5317277529877558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743022.4269502746, 743022.4269502746, 188717.7213974983]
[2019-04-27 23:26:31,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:26:31,534] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8237571e-30 1.0000000e+00 0.0000000e+00 6.2437334e-28 0.0000000e+00], sampled 0.757696558087222
[2019-04-27 23:26:50,315] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:26:50,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.05, 59.0, 1.0, 2.0, 0.9588268925116623, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.9112, 6.9112, 168.9129563891453, 1340216.181746225, 1340216.181746225, 286638.8423299899]
[2019-04-27 23:26:50,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:26:50,326] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6737449e-27 1.0000000e+00 3.3540051e-38 5.1866614e-25 4.9166350e-38], sampled 0.6620734697367548
[2019-04-27 23:27:32,524] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.06763945]
[2019-04-27 23:27:32,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.08333333333334, 92.0, 1.0, 2.0, 0.6326778376668059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 884146.141562499, 884146.141562499, 206986.5064490082]
[2019-04-27 23:27:32,525] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:27:32,528] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0185149e-30 1.0000000e+00 0.0000000e+00 6.8949881e-28 0.0000000e+00], sampled 0.6445270919047613
[2019-04-27 23:27:41,365] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:27:41,905] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:27:41,913] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:27:42,216] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:27:42,386] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:27:43,401] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 525000, evaluation results [525000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:27:49,347] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8881269e-31 1.0000000e+00 0.0000000e+00 4.5131342e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:27:49,355] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9048
[2019-04-27 23:27:49,360] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 92.33333333333334, 1.0, 2.0, 0.5751620359736044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 803739.3186089606, 803739.3186089612, 196209.1342637642], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3904800.0000, 
sim time next is 3905400.0000, 
raw observation next is [27.16666666666666, 93.16666666666667, 1.0, 2.0, 0.5728608688741702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800522.4243562263, 800522.4243562256, 195798.149770918], 
processed observation next is [0.0, 0.17391304347826086, 0.4865718799368086, 0.9316666666666668, 1.0, 1.0, 0.48537454081225323, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22236734009895176, 0.22236734009895157, 0.292236044434206], 
reward next is 0.7078, 
noisyNet noise sample is [array([0.5919296], dtype=float32), 0.2950553]. 
=============================================
[2019-04-27 23:27:50,070] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.7527911e-28 1.0000000e+00 0.0000000e+00 3.1868758e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:27:50,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4530
[2019-04-27 23:27:50,089] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 55.66666666666667, 1.0, 2.0, 0.5987374763543966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836696.9383127566, 836696.9383127566, 200511.287819026], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3856800.0000, 
sim time next is 3857400.0000, 
raw observation next is [35.0, 55.5, 1.0, 2.0, 0.5924232812887424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827869.8199108832, 827869.8199108832, 199343.5388423999], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.555, 1.0, 1.0, 0.5089437123960752, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2299638388641342, 0.2299638388641342, 0.2975276699140297], 
reward next is 0.7025, 
noisyNet noise sample is [array([-0.49264953], dtype=float32), -0.28759947]. 
=============================================
[2019-04-27 23:27:56,669] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.1704852e-26 1.0000000e+00 1.2055049e-35 1.4082320e-22 5.1140794e-35], sum to 1.0000
[2019-04-27 23:27:56,683] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6382
[2019-04-27 23:27:56,687] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.66666666666667, 1.0, 2.0, 0.6163266092105485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861286.581036685, 861286.581036685, 203827.0705384729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3959400.0000, 
sim time next is 3960000.0000, 
raw observation next is [32.0, 71.0, 1.0, 2.0, 0.6126502051477495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 856146.9137832858, 856146.9137832851, 203126.3088945808], 
processed observation next is [0.0, 0.8695652173913043, 0.7156398104265403, 0.71, 1.0, 1.0, 0.5333135001780114, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23781858716202384, 0.23781858716202364, 0.303173595365046], 
reward next is 0.6968, 
noisyNet noise sample is [array([-0.22900586], dtype=float32), 3.9702396]. 
=============================================
[2019-04-27 23:27:56,703] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[59.776806]
 [59.843918]
 [59.904594]
 [59.957363]
 [59.97711 ]], R is [[59.81518936]
 [59.91281891]
 [60.00866318]
 [60.10284042]
 [60.19336319]].
[2019-04-27 23:28:02,921] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.2624306e-17 1.0000000e+00 1.8033061e-24 9.9184001e-15 2.2107300e-24], sum to 1.0000
[2019-04-27 23:28:02,930] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7013
[2019-04-27 23:28:02,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3225451.397602714 W.
[2019-04-27 23:28:02,947] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 68.33333333333333, 1.0, 2.0, 0.8959645513800635, 1.0, 2.0, 0.7685723152042945, 1.0, 2.0, 1.03, 7.005113188868767, 6.9112, 170.5573041426782, 3225451.397602714, 3158177.560271737, 590374.3588988202], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4110000.0000, 
sim time next is 4110600.0000, 
raw observation next is [34.0, 67.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.914276921233292, 6.9112, 170.5573041426782, 3628711.603924672, 2910166.802684863, 547868.887832837], 
processed observation next is [1.0, 0.5652173913043478, 0.8104265402843602, 0.6766666666666667, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.10030769212332924, 0.0, 0.8375144448122397, 1.0079754455346313, 0.8083796674124619, 0.8177147579594581], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.04183399], dtype=float32), 1.0633534]. 
=============================================
[2019-04-27 23:28:04,721] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0096162e-24 1.0000000e+00 3.9569964e-36 8.4184419e-22 3.1030396e-36], sum to 1.0000
[2019-04-27 23:28:04,732] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2826
[2019-04-27 23:28:04,735] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5799068443955615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810372.3083294906, 810372.3083294906, 197062.4012206825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4138200.0000, 
sim time next is 4138800.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5804033018767291, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811066.331867296, 811066.331867296, 197151.9409428816], 
processed observation next is [1.0, 0.9130434782608695, 0.5734597156398105, 0.84, 1.0, 1.0, 0.494461809490035, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22529620329647113, 0.22529620329647113, 0.2942566282729576], 
reward next is 0.7057, 
noisyNet noise sample is [array([0.67906696], dtype=float32), -0.043298837]. 
=============================================
[2019-04-27 23:28:04,946] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.9551728e-15 1.0000000e+00 6.2212373e-22 9.5602120e-12 1.9704257e-22], sum to 1.0000
[2019-04-27 23:28:04,952] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8971
[2019-04-27 23:28:04,959] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2732155.717309657 W.
[2019-04-27 23:28:04,965] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.83333333333334, 79.0, 1.0, 2.0, 0.9767346612034115, 1.0, 2.0, 0.9767346612034115, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2732155.717309657, 2732155.717309657, 515092.105532017], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4096200.0000, 
sim time next is 4096800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.936236887309107, 1.0, 2.0, 0.936236887309107, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2618755.22873068, 2618755.22873068, 491656.530055562], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.79, 1.0, 1.0, 0.9231769726615747, 1.0, 1.0, 0.9231769726615747, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7274320079807445, 0.7274320079807445, 0.7338157165008388], 
reward next is 0.2662, 
noisyNet noise sample is [array([-1.1931156], dtype=float32), -0.9739475]. 
=============================================
[2019-04-27 23:28:12,542] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.0487112e-26 1.0000000e+00 6.8786948e-37 6.6475775e-23 5.6337741e-36], sum to 1.0000
[2019-04-27 23:28:12,554] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6418
[2019-04-27 23:28:12,558] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [36.5, 49.0, 1.0, 2.0, 0.5515995785758565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104266, 770800.8704077734, 770800.8704077734, 192078.8101527573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4296600.0000, 
sim time next is 4297200.0000, 
raw observation next is [36.33333333333334, 49.33333333333333, 1.0, 2.0, 0.5536512424200338, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773668.8932778541, 773668.8932778541, 192431.9374119439], 
processed observation next is [1.0, 0.7391304347826086, 0.9210110584518172, 0.4933333333333333, 1.0, 1.0, 0.4622304125542576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21490802591051503, 0.21490802591051503, 0.2872118468834983], 
reward next is 0.7128, 
noisyNet noise sample is [array([1.8117969], dtype=float32), 0.35636723]. 
=============================================
[2019-04-27 23:28:15,604] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.1468996e-26 1.0000000e+00 4.2830400e-36 1.9603662e-20 3.3915664e-35], sum to 1.0000
[2019-04-27 23:28:15,615] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7560
[2019-04-27 23:28:15,619] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333333, 66.83333333333333, 1.0, 2.0, 0.5300821452160126, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740722.0960106722, 740722.0960106716, 188443.2998642867], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4461000.0000, 
sim time next is 4461600.0000, 
raw observation next is [30.66666666666667, 67.66666666666667, 1.0, 2.0, 0.5409105871556351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 755858.8442419033, 755858.844241904, 190254.3010965207], 
processed observation next is [0.0, 0.6521739130434783, 0.6524486571879939, 0.6766666666666667, 1.0, 1.0, 0.446880225488717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20996079006719537, 0.20996079006719556, 0.28396164342764285], 
reward next is 0.7160, 
noisyNet noise sample is [array([-2.2124765], dtype=float32), -0.32943916]. 
=============================================
[2019-04-27 23:28:17,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3946266e-12 1.0000000e+00 4.5449236e-16 1.8940119e-10 3.5579603e-16], sum to 1.0000
[2019-04-27 23:28:17,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9309
[2019-04-27 23:28:17,375] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3099211.082165366 W.
[2019-04-27 23:28:17,381] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 45.0, 1.0, 2.0, 0.8358771042430947, 1.0, 2.0, 0.73852859163581, 1.0, 2.0, 1.03, 7.005108448386133, 6.9112, 170.5573041426782, 3099211.082165366, 3031940.640634931, 567680.9781816776], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4291200.0000, 
sim time next is 4291800.0000, 
raw observation next is [37.83333333333334, 45.5, 1.0, 2.0, 0.8269739513687909, 1.0, 2.0, 0.7340770151986581, 1.0, 2.0, 1.03, 7.005107746070613, 6.9112, 170.5573041426782, 3080507.148789928, 3013237.210356669, 564433.2995196204], 
processed observation next is [1.0, 0.6956521739130435, 0.9921011058451821, 0.455, 1.0, 1.0, 0.7915348811672179, 1.0, 1.0, 0.6796108616851302, 1.0, 1.0, 1.0365853658536586, 0.009390774607061303, 0.0, 0.8375144448122397, 0.8556964302194244, 0.8370103362101858, 0.8424377604770453], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17762397], dtype=float32), -1.3806669]. 
=============================================
[2019-04-27 23:28:36,507] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.9680027e-18 1.0000000e+00 1.2024759e-24 9.7285546e-17 7.5784541e-25], sum to 1.0000
[2019-04-27 23:28:36,518] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6309
[2019-04-27 23:28:36,522] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.5780849467102173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807825.3874027558, 807825.3874027558, 196733.765324302], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4583400.0000, 
sim time next is 4584000.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.5748120178281863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803250.0135053252, 803250.0135053252, 196147.0614414631], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4877253226845617, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22312500375147923, 0.22312500375147923, 0.2927568081215867], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.6312123], dtype=float32), 0.43737432]. 
=============================================
[2019-04-27 23:28:36,541] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[38.862144]
 [39.48914 ]
 [40.780853]
 [42.080673]
 [43.503674]], R is [[38.25428009]
 [38.57810593]
 [38.89952469]
 [39.21887589]
 [39.53633118]].
[2019-04-27 23:28:38,149] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4428535e-20 1.0000000e+00 1.0724685e-27 1.1818746e-17 4.6545944e-27], sum to 1.0000
[2019-04-27 23:28:38,158] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4698
[2019-04-27 23:28:38,165] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.83333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.05293647549628, 6.9112, 169.1975589848846, 1554544.265664462, 1453822.109837478, 311402.6336429201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4673400.0000, 
sim time next is 4674000.0000, 
raw observation next is [27.0, 90.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.95280388078335, 6.9112, 168.9121469915918, 1483290.226422459, 1453775.142952008, 311350.9824840707], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.9066666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0041603880783349645, 0.0, 0.8294359700442029, 0.41202506289512747, 0.40382642859778, 0.4647029589314488], 
reward next is 0.3273, 
noisyNet noise sample is [array([0.7672831], dtype=float32), -0.71985185]. 
=============================================
[2019-04-27 23:28:38,192] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[45.474014]
 [47.609833]
 [47.173695]
 [47.07505 ]
 [47.22235 ]], R is [[45.38256454]
 [44.92873764]
 [45.198452  ]
 [45.46646118]
 [45.73274994]].
[2019-04-27 23:28:42,226] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 23:28:42,229] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:28:42,230] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:28:42,230] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:28:42,232] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:28:42,233] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:28:42,235] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:28:42,236] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:28:42,236] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:28:42,236] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:28:42,237] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:28:42,261] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run23
[2019-04-27 23:28:42,262] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run23
[2019-04-27 23:28:42,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run23
[2019-04-27 23:28:42,345] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run23
[2019-04-27 23:28:42,370] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run23
[2019-04-27 23:29:08,486] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:29:08,490] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.38007136, 99.46185712, 1.0, 2.0, 0.4392765771347736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 632684.3725695634, 632684.3725695641, 177056.5066564986]
[2019-04-27 23:29:08,490] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:29:08,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5219664e-27 1.0000000e+00 2.0746675e-38 2.9695431e-25 3.4977393e-38], sampled 0.09623725413682105
[2019-04-27 23:29:24,268] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:29:24,270] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.16240443833334, 99.28313175000001, 1.0, 2.0, 0.5560110910433834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776967.737524685, 776967.7375246844, 192834.3283747859]
[2019-04-27 23:29:24,271] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:29:24,273] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4973932e-28 1.0000000e+00 0.0000000e+00 2.1780399e-26 0.0000000e+00], sampled 0.06378090082321486
[2019-04-27 23:29:49,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:29:49,557] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.391397623153789, 6.9112, 170.5573041426782, 3253715.508796373, 2909730.418140879, 551063.7088860873]
[2019-04-27 23:29:49,558] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:29:49,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.5629352e-25 1.0000000e+00 5.5752193e-35 5.1094306e-23 8.9681650e-35], sampled 0.2076058716368605
[2019-04-27 23:29:49,567] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 3253715.508796373 W.
[2019-04-27 23:29:58,326] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:29:58,328] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.95, 61.83333333333334, 1.0, 2.0, 0.5143478352324645, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718727.9691041455, 718727.969104145, 185874.6199871733]
[2019-04-27 23:29:58,330] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:29:58,334] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7684707e-28 1.0000000e+00 0.0000000e+00 2.5394024e-26 0.0000000e+00], sampled 0.011040619187668299
[2019-04-27 23:30:08,772] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:30:08,774] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.75, 72.66666666666666, 1.0, 2.0, 0.643470155319587, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899234.4296796802, 899234.4296796802, 209122.282085348]
[2019-04-27 23:30:08,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:30:08,779] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4720885e-27 1.0000000e+00 3.2849950e-38 3.9922700e-25 5.5164620e-38], sampled 0.4507742876161601
[2019-04-27 23:30:15,158] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:30:15,160] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.9, 58.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.50144383441943, 6.9112, 168.9097001488421, 2708812.268615192, 2290081.050451939, 475101.7000146341]
[2019-04-27 23:30:15,161] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:30:15,165] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.27714695e-23 1.00000000e+00 3.33468609e-33 8.09121277e-22
 5.34701707e-33], sampled 0.5771870085526334
[2019-04-27 23:30:15,166] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2708812.268615192 W.
[2019-04-27 23:30:16,284] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:30:16,285] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.45632153, 79.99090641333333, 1.0, 2.0, 0.5502240152144526, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 768877.9733923207, 768877.9733923207, 191838.4564929492]
[2019-04-27 23:30:16,286] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:30:16,289] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1589037e-29 1.0000000e+00 0.0000000e+00 6.6570292e-27 0.0000000e+00], sampled 0.7671238624226916
[2019-04-27 23:30:33,341] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:30:33,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.46666666666667, 67.0, 1.0, 2.0, 0.4170665138822869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611146.1835130262, 611146.1835130256, 175261.3542101542]
[2019-04-27 23:30:33,342] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:30:33,344] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.02081812e-29 1.00000000e+00 0.00000000e+00 1.08059615e-26
 0.00000000e+00], sampled 0.7973942709193362
[2019-04-27 23:30:39,720] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07101124]
[2019-04-27 23:30:39,723] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.07939801666667, 94.43835929666668, 1.0, 2.0, 0.3645282253507143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559895.0257235384, 559895.0257235384, 171437.8227322131]
[2019-04-27 23:30:39,723] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:30:39,727] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.4881629e-27 1.0000000e+00 4.6971808e-38 5.0606403e-25 7.8787306e-38], sampled 0.9073403424897636
[2019-04-27 23:30:49,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:30:49,056] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:30:49,071] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:30:49,214] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:30:49,245] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:30:50,260] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 550000, evaluation results [550000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:30:50,815] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.5851554e-23 1.0000000e+00 4.2038620e-32 1.3459869e-21 1.2849495e-33], sum to 1.0000
[2019-04-27 23:30:50,825] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8805
[2019-04-27 23:30:50,833] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2467897.807073378 W.
[2019-04-27 23:30:50,836] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 72.33333333333334, 1.0, 2.0, 0.5882378216956244, 1.0, 2.0, 0.5882378216956244, 1.0, 1.0, 1.021574563065023, 6.9112, 6.9112, 170.5573041426782, 2467897.807073378, 2467897.807073378, 481525.3447982669], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4720800.0000, 
sim time next is 4721400.0000, 
raw observation next is [31.0, 71.0, 1.0, 2.0, 0.5863563404484017, 1.0, 2.0, 0.5863563404484017, 1.0, 2.0, 1.018307052353954, 6.9112, 6.9112, 170.5573041426782, 2459996.455079755, 2459996.455079755, 479998.2523767434], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.71, 1.0, 1.0, 0.5016341451185562, 1.0, 1.0, 0.5016341451185562, 1.0, 1.0, 1.022325673602383, 0.0, 0.0, 0.8375144448122397, 0.6833323486332652, 0.6833323486332652, 0.7164153020548409], 
reward next is 0.2836, 
noisyNet noise sample is [array([0.4108062], dtype=float32), -0.53223217]. 
=============================================
[2019-04-27 23:30:56,252] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6063805e-23 1.0000000e+00 2.7757412e-34 2.5372663e-23 1.0426853e-34], sum to 1.0000
[2019-04-27 23:30:56,262] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1336
[2019-04-27 23:30:56,267] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5091015903215216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711394.6329976249, 711394.6329976243, 185034.4587508654], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5005800.0000, 
sim time next is 5006400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5104633257084197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713298.0968898302, 713298.0968898295, 185251.679123224], 
processed observation next is [1.0, 0.9565217391304348, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4101967779619514, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19813836024717504, 0.19813836024717485, 0.2764950434674985], 
reward next is 0.7235, 
noisyNet noise sample is [array([1.3711025], dtype=float32), -0.9948661]. 
=============================================
[2019-04-27 23:30:57,406] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.1009285e-25 1.0000000e+00 5.8267981e-35 1.4570635e-22 3.2245236e-35], sum to 1.0000
[2019-04-27 23:30:57,407] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5103
[2019-04-27 23:30:57,421] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.75, 75.5, 1.0, 2.0, 0.493124438284351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689061.6706909187, 689061.670690918, 182525.5695888306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4833000.0000, 
sim time next is 4833600.0000, 
raw observation next is [27.66666666666666, 76.0, 1.0, 2.0, 0.4923956981189029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 688043.0443596254, 688043.0443596254, 182412.9919730041], 
processed observation next is [1.0, 0.9565217391304348, 0.5102685624012636, 0.76, 1.0, 1.0, 0.3884285519504854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19112306787767372, 0.19112306787767372, 0.272258196974633], 
reward next is 0.7277, 
noisyNet noise sample is [array([0.45070815], dtype=float32), -1.3809892]. 
=============================================
[2019-04-27 23:31:48,849] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.9690337e-32 1.0000000e+00 0.0000000e+00 7.6804467e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 23:31:48,855] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9326
[2019-04-27 23:31:48,911] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.26666666666667, 77.0, 1.0, 2.0, 0.5474570426683874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765010.0390632303, 765010.0390632297, 191365.900452364], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5251200.0000, 
sim time next is 5251800.0000, 
raw observation next is [29.08333333333333, 77.5, 1.0, 2.0, 0.5459009997419011, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 762834.8621751659, 762834.8621751665, 191100.422018578], 
processed observation next is [1.0, 0.782608695652174, 0.5774091627172194, 0.775, 1.0, 1.0, 0.4528927707733748, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21189857282643498, 0.21189857282643515, 0.2852245104754896], 
reward next is 0.7148, 
noisyNet noise sample is [array([1.6878401], dtype=float32), 1.1512514]. 
=============================================
[2019-04-27 23:32:04,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.59751996e-30 1.00000000e+00 0.00000000e+00 1.16323355e-29
 0.00000000e+00], sum to 1.0000
[2019-04-27 23:32:04,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0799
[2019-04-27 23:32:04,639] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5509749634307706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769927.7218306224, 769927.7218306231, 191968.559560129], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5149800.0000, 
sim time next is 5150400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5510132275605634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 769981.2111797953, 769981.2111797959, 191975.1303179837], 
processed observation next is [0.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4590520813982691, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21388366977216536, 0.21388366977216552, 0.28653004525072195], 
reward next is 0.7135, 
noisyNet noise sample is [array([-1.2458088], dtype=float32), 0.3101149]. 
=============================================
[2019-04-27 23:33:01,344] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1487326e-13 9.9999988e-01 1.2695730e-17 6.7029227e-08 8.9309946e-18], sum to 1.0000
[2019-04-27 23:33:01,352] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1163
[2019-04-27 23:33:01,355] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.23333333333333, 81.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.13542675899951, 6.9112, 168.9114202113907, 1612936.604697984, 1453863.871387565, 311355.9506040112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5380800.0000, 
sim time next is 5381400.0000, 
raw observation next is [30.41666666666667, 80.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.231930301401244, 6.9112, 168.9107479819956, 1681444.903759142, 1453910.764296407, 311355.9275346305], 
processed observation next is [1.0, 0.2608695652173913, 0.6406003159557664, 0.8016666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03207303014012437, 0.0, 0.8294291002666162, 0.4670680288219839, 0.4038641011934464, 0.46471033960392616], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.14077906], dtype=float32), -0.40102354]. 
=============================================
[2019-04-27 23:33:07,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.8760705e-19 1.0000000e+00 1.5522700e-26 5.4139948e-16 3.7952342e-26], sum to 1.0000
[2019-04-27 23:33:07,656] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2560
[2019-04-27 23:33:07,668] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 86.0, 1.0, 2.0, 0.8365889707277824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1169262.016483936, 1169262.016483935, 253091.0094335143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5554800.0000, 
sim time next is 5555400.0000, 
raw observation next is [27.88333333333333, 85.33333333333334, 1.0, 2.0, 0.8302677065363072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1160422.247817472, 1160422.247817473, 251477.0291355819], 
processed observation next is [1.0, 0.30434782608695654, 0.5205371248025275, 0.8533333333333334, 1.0, 1.0, 0.7955032608871171, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32233951328263116, 0.3223395132826314, 0.37533884945609236], 
reward next is 0.6247, 
noisyNet noise sample is [array([0.47129217], dtype=float32), -1.1454821]. 
=============================================
[2019-04-27 23:33:14,789] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-27 23:33:14,793] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:33:14,794] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:33:14,794] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:33:14,796] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:33:14,796] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:33:14,799] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:33:14,797] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:33:14,800] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:33:14,800] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:33:14,803] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:33:14,822] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run24
[2019-04-27 23:33:14,823] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run24
[2019-04-27 23:33:14,881] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run24
[2019-04-27 23:33:14,882] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run24
[2019-04-27 23:33:14,939] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run24
[2019-04-27 23:33:27,683] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:33:27,684] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.66810649666667, 89.01612164000001, 1.0, 2.0, 0.3279889129519058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530369.9365499058, 530369.9365499058, 169364.4508720447]
[2019-04-27 23:33:27,685] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:33:27,687] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.0607048e-30 1.0000000e+00 0.0000000e+00 5.5533973e-28 0.0000000e+00], sampled 0.8452484956478321
[2019-04-27 23:33:37,003] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:33:37,004] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.0, 94.0, 1.0, 2.0, 0.4089266886049329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610154.2443231365, 610154.2443231365, 175475.3522996919]
[2019-04-27 23:33:37,005] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:33:37,009] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.4883579e-30 1.0000000e+00 0.0000000e+00 5.9889844e-28 0.0000000e+00], sampled 0.6119410859380787
[2019-04-27 23:33:42,064] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:33:42,065] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.73333333333333, 63.0, 1.0, 2.0, 0.9739818997071773, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991967378664, 6.9112, 168.9123159407793, 2258585.50507649, 2191337.075957936, 455390.4341250354]
[2019-04-27 23:33:42,067] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:33:42,069] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.0356857e-24 1.0000000e+00 2.8674067e-33 2.1987558e-22 2.2680978e-33], sampled 0.6149505931621972
[2019-04-27 23:33:42,071] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2258585.50507649 W.
[2019-04-27 23:33:45,950] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:33:45,951] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.7, 82.0, 1.0, 2.0, 0.5699891740009368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796507.9799948317, 796507.9799948317, 195287.3608418972]
[2019-04-27 23:33:45,952] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:33:45,955] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6905646e-30 1.0000000e+00 0.0000000e+00 1.9961119e-28 0.0000000e+00], sampled 0.8282677313083172
[2019-04-27 23:33:51,098] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:33:51,099] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.60964138833333, 68.20370212833333, 1.0, 2.0, 0.6099393400589264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 852357.1002268008, 852357.1002268002, 202612.5806667071]
[2019-04-27 23:33:51,100] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:33:51,103] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.3952125e-31 1.0000000e+00 0.0000000e+00 6.8753647e-29 0.0000000e+00], sampled 0.5823220737582803
[2019-04-27 23:34:10,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:34:10,474] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5229393191414531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730737.4756872482, 730737.4756872487, 187267.831458377]
[2019-04-27 23:34:10,475] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:34:10,477] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3534619e-30 1.0000000e+00 0.0000000e+00 1.6219116e-28 0.0000000e+00], sampled 0.86339742769768
[2019-04-27 23:34:16,804] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:34:16,806] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.3, 94.0, 1.0, 2.0, 0.6179085955666362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 863498.2290137887, 863498.2290137893, 204129.4158307629]
[2019-04-27 23:34:16,807] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:34:16,809] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8563174e-31 1.0000000e+00 0.0000000e+00 2.5401930e-29 0.0000000e+00], sampled 0.1343214289792719
[2019-04-27 23:34:28,121] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:34:28,121] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.97869327, 59.99563443, 1.0, 2.0, 0.523275179980549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787475.963744469, 787475.963744469, 194242.7741963407]
[2019-04-27 23:34:28,122] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:34:28,124] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.3337223e-30 1.0000000e+00 0.0000000e+00 5.8322677e-28 0.0000000e+00], sampled 0.26061704635059535
[2019-04-27 23:34:29,773] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07385316]
[2019-04-27 23:34:29,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.63333333333334, 71.33333333333334, 1.0, 2.0, 0.5435706640655803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759577.3175048416, 759577.3175048416, 190703.2967786542]
[2019-04-27 23:34:29,777] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:34:29,780] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5756561e-31 1.0000000e+00 0.0000000e+00 7.0890135e-29 0.0000000e+00], sampled 0.78390831146881
[2019-04-27 23:34:40,418] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:34:40,563] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:34:40,650] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:34:40,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:34:40,777] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:34:41,793] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 575000, evaluation results [575000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:34:42,150] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.714310e-27 1.000000e+00 0.000000e+00 6.374569e-25 1.196245e-38], sum to 1.0000
[2019-04-27 23:34:42,156] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4167
[2019-04-27 23:34:42,160] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 85.0, 1.0, 2.0, 0.5373561199602739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750890.1393398107, 750890.1393398113, 189655.7173870411], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5691600.0000, 
sim time next is 5692200.0000, 
raw observation next is [27.41666666666667, 85.16666666666667, 1.0, 2.0, 0.5371257793989258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750568.1525656576, 750568.1525656576, 189616.9457800235], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690366, 0.8516666666666667, 1.0, 1.0, 0.4423202161432841, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20849115349046046, 0.20849115349046046, 0.28301036683585595], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.5529471], dtype=float32), 1.4387616]. 
=============================================
[2019-04-27 23:34:42,664] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4419308e-26 1.0000000e+00 2.9761836e-36 8.7461038e-25 9.3728490e-35], sum to 1.0000
[2019-04-27 23:34:42,670] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8994
[2019-04-27 23:34:42,676] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.53333333333333, 87.0, 1.0, 2.0, 0.514571793229529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719041.0244809346, 719041.0244809346, 185910.3212599573], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5701200.0000, 
sim time next is 5701800.0000, 
raw observation next is [26.51666666666667, 87.0, 1.0, 2.0, 0.5142109877555255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718536.6795371415, 718536.6795371415, 185852.2360275718], 
processed observation next is [0.0, 1.0, 0.45576619273301755, 0.87, 1.0, 1.0, 0.4147120334403922, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19959352209365042, 0.19959352209365042, 0.27739139705607735], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.7019562], dtype=float32), -1.4843231]. 
=============================================
[2019-04-27 23:34:43,597] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.1441177e-33 1.0000000e+00 0.0000000e+00 5.9570302e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:34:43,609] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6593
[2019-04-27 23:34:43,620] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.5, 70.0, 1.0, 2.0, 0.5451095698350268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 761728.5319104239, 761728.5319104233, 190966.1246960469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5652000.0000, 
sim time next is 5652600.0000, 
raw observation next is [30.66666666666667, 69.5, 1.0, 2.0, 0.5489081798708574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 767038.5732999502, 767038.5732999496, 191614.1336950899], 
processed observation next is [0.0, 0.43478260869565216, 0.6524486571879939, 0.695, 1.0, 1.0, 0.4565158793624787, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2130662703610973, 0.21306627036109713, 0.2859912443210297], 
reward next is 0.7140, 
noisyNet noise sample is [array([0.2604576], dtype=float32), 0.7530246]. 
=============================================
[2019-04-27 23:34:49,063] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0936361e-30 1.0000000e+00 0.0000000e+00 8.3722590e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:34:49,069] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7223
[2019-04-27 23:34:49,074] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.2, 53.0, 1.0, 2.0, 0.5235475983510683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731587.7569039543, 731587.7569039549, 187367.89772706], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5751600.0000, 
sim time next is 5752200.0000, 
raw observation next is [33.0, 53.0, 1.0, 2.0, 0.5180462406040486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723897.725706286, 723897.7257062853, 186471.9257191476], 
processed observation next is [0.0, 0.5652173913043478, 0.7630331753554502, 0.53, 1.0, 1.0, 0.41933282000487776, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20108270158507946, 0.20108270158507927, 0.2783163070435039], 
reward next is 0.7217, 
noisyNet noise sample is [array([0.8416358], dtype=float32), 0.5264224]. 
=============================================
[2019-04-27 23:34:49,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4406629e-25 1.0000000e+00 2.3165911e-35 7.0216162e-23 7.1176298e-36], sum to 1.0000
[2019-04-27 23:34:49,590] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6092
[2019-04-27 23:34:49,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.48333333333333, 71.0, 1.0, 2.0, 0.5716510713747606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798831.2035700041, 798831.2035700041, 195582.3402300803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5767800.0000, 
sim time next is 5768400.0000, 
raw observation next is [30.26666666666667, 72.0, 1.0, 2.0, 0.5582309294339238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780070.8701151449, 780070.8701151442, 193222.2889117747], 
processed observation next is [0.0, 0.782608695652174, 0.6334913112164299, 0.72, 1.0, 1.0, 0.46774810775171544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21668635280976248, 0.21668635280976228, 0.2883914759877234], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.32619166], dtype=float32), 0.21414047]. 
=============================================
[2019-04-27 23:34:49,880] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2920711e-19 1.0000000e+00 4.2642650e-26 4.9395100e-17 1.5956648e-27], sum to 1.0000
[2019-04-27 23:34:49,889] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5748
[2019-04-27 23:34:49,901] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1670954.684893925 W.
[2019-04-27 23:34:49,912] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.65, 90.0, 1.0, 2.0, 0.5976300314431864, 0.0, 2.0, 0.0, 1.0, 2.0, 1.017783058218837, 6.911199999999999, 6.9112, 168.9128981374187, 1670954.684893925, 1670954.684893925, 361735.2529659927], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 5797800.0000, 
sim time next is 5798400.0000, 
raw observation next is [26.6, 90.33333333333334, 1.0, 2.0, 0.5636152676475302, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9596902065757086, 6.911199999999999, 6.9112, 168.9129564958551, 1575779.878896874, 1575779.878896874, 340797.5570505221], 
processed observation next is [1.0, 0.08695652173913043, 0.4597156398104266, 0.9033333333333334, 1.0, 1.0, 0.47423526222594, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.9508417153362299, -8.881784197001253e-17, 0.0, 0.8294399450807284, 0.43771663302690944, 0.43771663302690944, 0.5086530702246599], 
reward next is 0.4913, 
noisyNet noise sample is [array([-0.37054294], dtype=float32), -1.5657579]. 
=============================================
[2019-04-27 23:34:50,349] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.0223102e-24 1.0000000e+00 3.3730417e-33 9.5878609e-23 1.6605805e-31], sum to 1.0000
[2019-04-27 23:34:50,358] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1938
[2019-04-27 23:34:50,365] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.26666666666667, 72.0, 1.0, 2.0, 0.5582309294339238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780070.8701151449, 780070.8701151442, 193222.2889117747], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5768400.0000, 
sim time next is 5769000.0000, 
raw observation next is [30.05, 73.0, 1.0, 2.0, 0.5556177029862147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776417.8176715361, 776417.8176715361, 192768.8500099145], 
processed observation next is [0.0, 0.782608695652174, 0.6232227488151659, 0.73, 1.0, 1.0, 0.4645996421520659, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21567161601987114, 0.21567161601987114, 0.2877147015073351], 
reward next is 0.7123, 
noisyNet noise sample is [array([0.8684008], dtype=float32), -0.71102136]. 
=============================================
[2019-04-27 23:34:50,394] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[52.097973]
 [52.24541 ]
 [52.421947]
 [52.493618]
 [52.59212 ]], R is [[52.15306473]
 [52.34314346]
 [52.52780151]
 [52.71417999]
 [52.89853287]].
[2019-04-27 23:34:59,085] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0163844e-28 1.0000000e+00 0.0000000e+00 8.7208419e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:34:59,098] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1766
[2019-04-27 23:34:59,102] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.45, 81.0, 1.0, 2.0, 0.5688148956551572, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794866.4189164727, 794866.4189164727, 195080.8698226971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5941800.0000, 
sim time next is 5942400.0000, 
raw observation next is [29.33333333333334, 81.33333333333334, 1.0, 2.0, 0.5666369186222883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791821.7622108033, 791821.7622108033, 194695.9060357982], 
processed observation next is [1.0, 0.782608695652174, 0.5892575039494474, 0.8133333333333335, 1.0, 1.0, 0.477875805569022, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21995048950300092, 0.21995048950300092, 0.2905909045310421], 
reward next is 0.7094, 
noisyNet noise sample is [array([-0.8968774], dtype=float32), -0.7149429]. 
=============================================
[2019-04-27 23:35:00,627] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6526762e-22 1.0000000e+00 1.8212048e-31 2.5182849e-19 3.9522110e-30], sum to 1.0000
[2019-04-27 23:35:00,632] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1255
[2019-04-27 23:35:00,636] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.2, 93.0, 1.0, 2.0, 0.9038350009678966, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1263304.621457548, 1263304.621457549, 270994.1707909416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6057000.0000, 
sim time next is 6057600.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.8073139519324175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1128323.926483824, 1128323.926483823, 245713.4375419818], 
processed observation next is [1.0, 0.08695652173913043, 0.44075829383886256, 0.93, 1.0, 1.0, 0.7678481348583344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3134233129121733, 0.31342331291217307, 0.3667364739432564], 
reward next is 0.6333, 
noisyNet noise sample is [array([0.28222027], dtype=float32), -2.00016]. 
=============================================
[2019-04-27 23:35:12,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.56831055e-31 1.00000000e+00 0.00000000e+00 1.50423545e-27
 0.00000000e+00], sum to 1.0000
[2019-04-27 23:35:12,984] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8896
[2019-04-27 23:35:12,988] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.5124905265649867, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 716131.769476908, 716131.769476908, 185575.9632581578], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6394800.0000, 
sim time next is 6395400.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.5123525211006227, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715938.8617114736, 715938.8617114729, 185553.8411422499], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.412472916988702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19887190603096488, 0.1988719060309647, 0.2769460315555969], 
reward next is 0.7231, 
noisyNet noise sample is [array([-1.8701822], dtype=float32), -0.2497765]. 
=============================================
[2019-04-27 23:35:21,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.7022933e-32 1.0000000e+00 0.0000000e+00 1.3632211e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:35:21,591] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2135
[2019-04-27 23:35:21,598] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5402909341979382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 754992.6444811684, 754992.6444811678, 190149.8576792844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6346800.0000, 
sim time next is 6347400.0000, 
raw observation next is [31.16666666666667, 65.5, 1.0, 2.0, 0.5468106131215998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764106.4011059444, 764106.4011059437, 191255.2296954122], 
processed observation next is [0.0, 0.4782608695652174, 0.6761453396524489, 0.655, 1.0, 1.0, 0.4539886905079516, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21225177808498455, 0.21225177808498435, 0.28545556670957045], 
reward next is 0.7145, 
noisyNet noise sample is [array([0.0928243], dtype=float32), 0.6056013]. 
=============================================
[2019-04-27 23:35:26,624] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1042119e-21 1.0000000e+00 1.0750967e-30 4.5345431e-19 8.4733677e-31], sum to 1.0000
[2019-04-27 23:35:26,634] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7776
[2019-04-27 23:35:26,640] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.9, 69.0, 1.0, 2.0, 0.382754964371837, 1.0, 2.0, 0.382754964371837, 1.0, 1.0, 0.6560262869994657, 6.911199999999999, 6.9112, 170.5573041426782, 1605194.794037141, 1605194.794037141, 342388.1831120629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6439800.0000, 
sim time next is 6440400.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 8.394464445664822, 6.9112, 168.9045172538305, 2506671.269918887, 1454444.996268781, 310698.7200802906], 
processed observation next is [1.0, 0.5652173913043478, 0.6161137440758293, 0.69, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.14832644456648217, 0.0, 0.8293985045388859, 0.6962975749774686, 0.40401249896355024, 0.46372943295565755], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.70417506], dtype=float32), -0.4437069]. 
=============================================
[2019-04-27 23:35:28,774] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2223058e-26 1.0000000e+00 3.4019065e-35 1.0676825e-23 2.0469531e-34], sum to 1.0000
[2019-04-27 23:35:28,780] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9172
[2019-04-27 23:35:28,792] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 90.33333333333334, 1.0, 2.0, 0.5292804803370087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739601.4817388266, 739601.4817388272, 188310.8657901357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6484800.0000, 
sim time next is 6485400.0000, 
raw observation next is [26.55, 90.5, 1.0, 2.0, 0.5283061626593757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738239.5248319198, 738239.5248319198, 188149.887730802], 
processed observation next is [1.0, 0.043478260869565216, 0.4573459715639811, 0.905, 1.0, 1.0, 0.43169417187876585, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2050665346755333, 0.2050665346755333, 0.2808207279564209], 
reward next is 0.7192, 
noisyNet noise sample is [array([0.66326725], dtype=float32), 0.24848571]. 
=============================================
[2019-04-27 23:35:32,755] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-27 23:35:32,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:35:32,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:35:32,758] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:35:32,758] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:35:32,759] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:35:32,759] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:35:32,759] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:35:32,760] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:35:32,759] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:35:32,760] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:35:32,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run25
[2019-04-27 23:35:32,804] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run25
[2019-04-27 23:35:32,804] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run25
[2019-04-27 23:35:32,805] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run25
[2019-04-27 23:35:32,888] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run25
[2019-04-27 23:36:39,553] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.078434564]
[2019-04-27 23:36:39,554] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.64782013166667, 82.88882654166667, 1.0, 2.0, 0.7072489442648764, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129564911616, 988405.2648604381, 988405.2648604386, 222425.1210517937]
[2019-04-27 23:36:39,554] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:36:39,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.8431852e-26 1.0000000e+00 1.4369501e-36 1.4516424e-23 2.3077258e-36], sampled 0.8062256627542884
[2019-04-27 23:36:40,575] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.078434564]
[2019-04-27 23:36:40,576] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 74.33333333333333, 1.0, 2.0, 0.5242594921444128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732582.8763112919, 732582.8763112926, 187484.5307922745]
[2019-04-27 23:36:40,577] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:36:40,580] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2496558e-27 1.0000000e+00 1.3933017e-38 7.5420580e-25 2.2935761e-38], sampled 0.16571661921784786
[2019-04-27 23:36:50,589] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.078434564]
[2019-04-27 23:36:50,590] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.95, 74.5, 1.0, 2.0, 0.7029694826912712, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597546887581, 6.9112, 168.9123160472588, 1879284.16192247, 1812047.437322903, 385291.9816520192]
[2019-04-27 23:36:50,591] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:36:50,592] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4573111e-21 1.0000000e+00 5.3460257e-30 2.3592752e-19 7.9849040e-30], sampled 0.621214153351751
[2019-04-27 23:36:50,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1879284.16192247 W.
[2019-04-27 23:37:02,522] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:37:02,764] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:37:02,823] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:37:02,846] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:37:03,173] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:37:04,189] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 600000, evaluation results [600000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:37:05,066] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0115140e-24 1.0000000e+00 9.6726728e-33 2.3221541e-22 4.3030425e-33], sum to 1.0000
[2019-04-27 23:37:05,076] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0494
[2019-04-27 23:37:05,084] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2023898.025238406 W.
[2019-04-27 23:37:05,088] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.18333333333334, 78.16666666666667, 1.0, 2.0, 0.723749362104678, 1.0, 1.0, 0.723749362104678, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2023898.025238406, 2023898.025238406, 384452.1635244768], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6599400.0000, 
sim time next is 6600000.0000, 
raw observation next is [28.36666666666667, 77.33333333333334, 1.0, 2.0, 0.436708388131121, 1.0, 2.0, 0.436708388131121, 1.0, 1.0, 0.7483206968527096, 6.911200000000001, 6.9112, 170.5573041426782, 1831657.622017822, 1831657.622017822, 372565.9343035401], 
processed observation next is [1.0, 0.391304347826087, 0.543443917851501, 0.7733333333333334, 1.0, 1.0, 0.32133540738689276, 1.0, 1.0, 0.32133540738689276, 1.0, 0.5, 0.6930740205520849, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5087937838938394, 0.5087937838938394, 0.5560685586620001], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.03766189], dtype=float32), -1.5617965]. 
=============================================
[2019-04-27 23:37:05,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[54.845493]
 [55.40207 ]
 [55.536312]
 [55.74643 ]
 [56.541367]], R is [[53.99464035]
 [53.45469284]
 [53.00150681]
 [52.90845108]
 [52.81849289]].
[2019-04-27 23:37:08,522] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5389066e-28 1.0000000e+00 0.0000000e+00 1.1053160e-24 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:08,523] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7843
[2019-04-27 23:37:08,528] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 56.0, 1.0, 2.0, 0.3208807839871262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506548.4462990309, 506548.4462990315, 167501.6409878361], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6810000.0000, 
sim time next is 6810600.0000, 
raw observation next is [26.65, 56.5, 1.0, 2.0, 0.3188316578683428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503857.9567020479, 503857.9567020485, 167308.9110961396], 
processed observation next is [1.0, 0.8260869565217391, 0.462085308056872, 0.565, 1.0, 1.0, 0.17931525044378646, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13996054352834664, 0.1399605435283468, 0.24971479268080538], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.44017652], dtype=float32), 0.6789647]. 
=============================================
[2019-04-27 23:37:18,703] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5676773e-30 1.0000000e+00 0.0000000e+00 4.6071807e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:18,712] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7433
[2019-04-27 23:37:18,724] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.41666666666667, 82.33333333333334, 1.0, 2.0, 0.3960332664974838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627250.7252275207, 627250.7252275207, 177594.4617805309], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6761400.0000, 
sim time next is 6762000.0000, 
raw observation next is [22.53333333333333, 81.66666666666667, 1.0, 2.0, 0.3487074423974256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 551949.6496468761, 551949.6496468768, 171103.700153481], 
processed observation next is [1.0, 0.2608695652173913, 0.26698262243285936, 0.8166666666666668, 1.0, 1.0, 0.2153101715631634, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15331934712413225, 0.15331934712413245, 0.25537865694549405], 
reward next is 0.7446, 
noisyNet noise sample is [array([0.12032278], dtype=float32), 0.9054262]. 
=============================================
[2019-04-27 23:37:18,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[74.85669 ]
 [75.06527 ]
 [75.37781 ]
 [75.40138 ]
 [75.398964]], R is [[74.97467804]
 [74.95986176]
 [74.95025635]
 [74.9480896 ]
 [74.94630432]].
[2019-04-27 23:37:19,073] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8206431e-30 1.0000000e+00 0.0000000e+00 9.5242886e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:19,081] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7381
[2019-04-27 23:37:19,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 50.0, 1.0, 2.0, 0.9962828163074118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1536412.822916233, 1536412.822916233, 318786.9920947404], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6800400.0000, 
sim time next is 6801000.0000, 
raw observation next is [28.95, 50.33333333333334, 1.0, 2.0, 0.4634626496994402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715458.3699750584, 715458.369975059, 186202.897116919], 
processed observation next is [1.0, 0.7391304347826086, 0.5710900473933649, 0.5033333333333334, 1.0, 1.0, 0.353569457469205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19873843610418288, 0.19873843610418304, 0.277914771816297], 
reward next is 0.7221, 
noisyNet noise sample is [array([0.29804164], dtype=float32), -0.16483656]. 
=============================================
[2019-04-27 23:37:19,102] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[74.34212]
 [74.23702]
 [74.06089]
 [74.05042]
 [74.04322]], R is [[75.11214447]
 [74.88522339]
 [74.66059875]
 [74.44443512]
 [74.25461578]].
[2019-04-27 23:37:24,140] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.8446094e-30 1.0000000e+00 0.0000000e+00 1.6849539e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:24,152] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2446
[2019-04-27 23:37:24,155] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 81.83333333333334, 1.0, 2.0, 0.3367879131771817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525679.7773040036, 525679.7773040036, 168852.8277994767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6839400.0000, 
sim time next is 6840000.0000, 
raw observation next is [23.1, 82.0, 1.0, 2.0, 0.3373104225798131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526188.2841999789, 526188.2841999796, 168885.4356389139], 
processed observation next is [0.0, 0.17391304347826086, 0.2938388625592418, 0.82, 1.0, 1.0, 0.20157882238531696, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14616341227777194, 0.1461634122777721, 0.25206781438643866], 
reward next is 0.7479, 
noisyNet noise sample is [array([-0.39037693], dtype=float32), 0.24602097]. 
=============================================
[2019-04-27 23:37:24,194] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[74.63994]
 [74.64773]
 [74.77068]
 [74.80484]
 [74.82956]], R is [[74.69096375]
 [74.69203186]
 [74.69315338]
 [74.6943512 ]
 [74.69561768]].
[2019-04-27 23:37:29,231] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3474310e-33 1.0000000e+00 0.0000000e+00 3.1307289e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:29,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9099
[2019-04-27 23:37:29,277] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.55, 86.33333333333334, 1.0, 2.0, 0.4229799826563206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618794.1464536366, 618794.1464536366, 175963.571477473], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6919800.0000, 
sim time next is 6920400.0000, 
raw observation next is [24.5, 86.66666666666667, 1.0, 2.0, 0.4226153594569511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 618355.9671132707, 618355.9671132707, 175924.0946681097], 
processed observation next is [0.0, 0.08695652173913043, 0.3601895734597157, 0.8666666666666667, 1.0, 1.0, 0.304355854767411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17176554642035297, 0.17176554642035297, 0.2625732756240443], 
reward next is 0.7374, 
noisyNet noise sample is [array([-0.5327225], dtype=float32), 0.41658214]. 
=============================================
[2019-04-27 23:37:32,664] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.87802538e-23 1.00000000e+00 3.05555393e-33 7.87354047e-21
 1.19324975e-32], sum to 1.0000
[2019-04-27 23:37:32,672] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1162
[2019-04-27 23:37:32,692] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.35, 78.0, 1.0, 2.0, 0.5977991114801654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 854855.0022633283, 854855.0022633277, 202843.5922504146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7021800.0000, 
sim time next is 7022400.0000, 
raw observation next is [26.5, 77.0, 1.0, 2.0, 0.5928815583752316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 847976.1421943277, 847976.1421943277, 201930.7281508207], 
processed observation next is [1.0, 0.2608695652173913, 0.4549763033175356, 0.77, 1.0, 1.0, 0.5094958534641344, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23554892838731323, 0.23554892838731323, 0.30138914649376225], 
reward next is 0.6986, 
noisyNet noise sample is [array([0.94097674], dtype=float32), -1.4106879]. 
=============================================
[2019-04-27 23:37:34,176] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0358763e-23 1.0000000e+00 2.4072490e-34 3.7883546e-23 2.4870575e-34], sum to 1.0000
[2019-04-27 23:37:34,186] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7841
[2019-04-27 23:37:34,199] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 75.0, 1.0, 2.0, 0.6364402611088918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 910828.0310405577, 910828.031040557, 210518.0316832197], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7023600.0000, 
sim time next is 7024200.0000, 
raw observation next is [26.95, 74.0, 1.0, 2.0, 0.7689866624988516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1100967.114886573, 1100967.114886574, 240056.2035162981], 
processed observation next is [1.0, 0.30434782608695654, 0.476303317535545, 0.74, 1.0, 1.0, 0.7216706777094598, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.30582419857960363, 0.30582419857960386, 0.35829284106910164], 
reward next is 0.6417, 
noisyNet noise sample is [array([-1.2814721], dtype=float32), -0.66890806]. 
=============================================
[2019-04-27 23:37:35,341] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.9206060e-26 1.0000000e+00 1.1861624e-37 6.9127230e-25 1.7434710e-37], sum to 1.0000
[2019-04-27 23:37:35,347] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6283
[2019-04-27 23:37:35,353] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 86.66666666666667, 1.0, 2.0, 0.483674153488301, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675852.2249214473, 675852.2249214473, 181077.1368164156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7076400.0000, 
sim time next is 7077000.0000, 
raw observation next is [25.65, 86.83333333333333, 1.0, 2.0, 0.4821435089750622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 673712.7317680084, 673712.7317680084, 180845.0936523137], 
processed observation next is [1.0, 0.9130434782608695, 0.41469194312796204, 0.8683333333333333, 1.0, 1.0, 0.3760765168374243, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18714242549111346, 0.18714242549111346, 0.2699180502273339], 
reward next is 0.7301, 
noisyNet noise sample is [array([0.65647304], dtype=float32), 0.81851315]. 
=============================================
[2019-04-27 23:37:35,409] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[63.943638]
 [64.19459 ]
 [64.139786]
 [64.503334]
 [64.53323 ]], R is [[64.02236176]
 [64.11186981]
 [64.20016479]
 [64.28739166]
 [64.37387085]].
[2019-04-27 23:37:37,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4375179e-29 1.0000000e+00 0.0000000e+00 3.6114789e-25 6.4919518e-38], sum to 1.0000
[2019-04-27 23:37:37,343] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3466
[2019-04-27 23:37:37,355] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1880915.119064021 W.
[2019-04-27 23:37:37,361] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.61666666666667, 69.33333333333334, 1.0, 2.0, 0.4484421845131166, 1.0, 1.0, 0.4484421845131166, 1.0, 2.0, 0.7569808690189218, 6.9112, 6.9112, 170.5573041426782, 1880915.119064021, 1880915.119064021, 377766.7231378272], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7059000.0000, 
sim time next is 7059600.0000, 
raw observation next is [28.4, 71.0, 1.0, 2.0, 0.4517907824210887, 1.0, 2.0, 0.4517907824210887, 1.0, 2.0, 0.7642320409274818, 6.9112, 6.9112, 170.5573041426782, 1894972.668708602, 1894972.668708602, 380065.2543045841], 
processed observation next is [1.0, 0.7391304347826086, 0.5450236966824644, 0.71, 1.0, 1.0, 0.339506966772396, 1.0, 1.0, 0.339506966772396, 1.0, 1.0, 0.7124780986920508, 0.0, 0.0, 0.8375144448122397, 0.5263812968635005, 0.5263812968635005, 0.5672615735889315], 
reward next is 0.4327, 
noisyNet noise sample is [array([-0.48855048], dtype=float32), -0.2024918]. 
=============================================
[2019-04-27 23:37:41,065] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5399069e-25 1.0000000e+00 2.8605646e-35 2.1867608e-22 1.7836218e-33], sum to 1.0000
[2019-04-27 23:37:41,067] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5394
[2019-04-27 23:37:41,069] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 83.16666666666667, 1.0, 2.0, 0.5868321658513949, 0.0, 1.0, 0.0, 1.0, 1.0, 0.9830443567304923, 6.9112, 6.9112, 168.912956510431, 1640740.856834015, 1640740.856834015, 351257.1598391959], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7139400.0000, 
sim time next is 7140000.0000, 
raw observation next is [26.33333333333334, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.936410452597335, 6.9112, 168.9072548346165, 2181548.19734758, 1454253.168008504, 311347.076142977], 
processed observation next is [1.0, 0.6521739130434783, 0.44707740916271754, 0.8333333333333335, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.10252104525973352, 0.0, 0.8294119473138151, 0.6059856103743277, 0.40395921333569557, 0.46469712857160744], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.33712697], dtype=float32), 0.3976588]. 
=============================================
[2019-04-27 23:37:41,110] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[60.5269  ]
 [59.765694]
 [59.40948 ]
 [58.800262]
 [57.73073 ]], R is [[59.06320572]
 [58.47257233]
 [57.8878479 ]
 [57.75447845]
 [57.64385605]].
[2019-04-27 23:37:49,057] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.2608084e-25 1.0000000e+00 6.4780558e-36 1.8823032e-22 1.8895046e-37], sum to 1.0000
[2019-04-27 23:37:49,064] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7468
[2019-04-27 23:37:49,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1918508.185636886 W.
[2019-04-27 23:37:49,072] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [29.33333333333334, 75.0, 1.0, 2.0, 0.7309989504157384, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.988925580598369, 6.9112, 168.9124299314508, 1918508.185636886, 1863367.155552732, 392150.5285207072], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7212000.0000, 
sim time next is 7212600.0000, 
raw observation next is [29.25, 76.0, 1.0, 2.0, 0.7728777176528279, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.990412275786387, 6.9112, 168.9124222426618, 1977115.418403501, 1920919.681430885, 401833.9630274118], 
processed observation next is [1.0, 0.4782608695652174, 0.5853080568720379, 0.76, 1.0, 1.0, 0.7263586959672624, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007921227578638668, 0.0, 0.829437321652967, 0.5491987273343059, 0.533588800397468, 0.5997521836230026], 
reward next is 0.0042, 
noisyNet noise sample is [array([-0.36601788], dtype=float32), 0.0588942]. 
=============================================
[2019-04-27 23:37:49,386] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0858718e-28 1.0000000e+00 0.0000000e+00 1.7518685e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:49,394] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5382
[2019-04-27 23:37:49,402] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1975785.012421775 W.
[2019-04-27 23:37:49,409] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.6, 84.0, 1.0, 2.0, 0.7719270756281535, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.000943807722077, 6.9112, 168.9118965795342, 1975785.012421775, 1912118.065993634, 401214.5003743005], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7201800.0000, 
sim time next is 7202400.0000, 
raw observation next is [28.73333333333333, 84.0, 1.0, 2.0, 0.4515849567167842, 1.0, 1.0, 0.4515849567167842, 1.0, 2.0, 0.784253728389797, 6.9112, 6.9112, 170.5573041426782, 1894108.598745406, 1894108.598745406, 383281.3658375561], 
processed observation next is [1.0, 0.34782608695652173, 0.560821484992101, 0.84, 1.0, 1.0, 0.33925898399612553, 1.0, 0.5, 0.33925898399612553, 1.0, 1.0, 0.7368947907192647, 0.0, 0.0, 0.8375144448122397, 0.5261412774292794, 0.5261412774292794, 0.5720617400560538], 
reward next is 0.4279, 
noisyNet noise sample is [array([0.07415976], dtype=float32), -2.091833]. 
=============================================
[2019-04-27 23:37:51,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8529667e-29 1.0000000e+00 0.0000000e+00 1.0156026e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:51,649] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7779
[2019-04-27 23:37:51,653] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 89.33333333333333, 1.0, 2.0, 0.3221267845688506, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508478.1178813502, 508478.1178813502, 167647.5761161949], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7278600.0000, 
sim time next is 7279200.0000, 
raw observation next is [21.7, 89.0, 1.0, 2.0, 0.3204884988383386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 505796.1216798144, 505796.1216798138, 167441.9681416478], 
processed observation next is [1.0, 0.2608695652173913, 0.2274881516587678, 0.89, 1.0, 1.0, 0.18131144438354044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14049892268883735, 0.14049892268883715, 0.2499133852860415], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.6507106], dtype=float32), 0.18192255]. 
=============================================
[2019-04-27 23:37:54,064] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4429132e-24 1.0000000e+00 3.4063440e-37 4.2953401e-23 2.6760373e-37], sum to 1.0000
[2019-04-27 23:37:54,083] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7046
[2019-04-27 23:37:54,090] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 83.0, 1.0, 2.0, 0.3911180894396441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581906.0985251336, 581906.0985251341, 172825.8553227844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7476000.0000, 
sim time next is 7476600.0000, 
raw observation next is [24.68333333333333, 82.5, 1.0, 2.0, 0.3929669905831121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 583741.7616279752, 583741.7616279759, 172963.5841207946], 
processed observation next is [0.0, 0.5217391304347826, 0.3688783570300157, 0.825, 1.0, 1.0, 0.26863492841338804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1621504893411042, 0.1621504893411044, 0.2581546031653651], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.06894624], dtype=float32), -0.11074681]. 
=============================================
[2019-04-27 23:37:55,475] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.3756328e-27 1.0000000e+00 2.0463798e-37 3.3497218e-23 5.9121623e-38], sum to 1.0000
[2019-04-27 23:37:55,482] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3571
[2019-04-27 23:37:55,487] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 91.16666666666667, 1.0, 2.0, 0.5776088857498417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 924981.165246668, 924981.165246668, 209820.0580557807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7397400.0000, 
sim time next is 7398000.0000, 
raw observation next is [20.7, 91.0, 1.0, 2.0, 0.5077998920721613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814044.519756606, 814044.519756606, 196386.7965715397], 
processed observation next is [1.0, 0.6521739130434783, 0.18009478672985785, 0.91, 1.0, 1.0, 0.40698782177368825, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22612347771016833, 0.22612347771016833, 0.2931146217485667], 
reward next is 0.7069, 
noisyNet noise sample is [array([-1.6431895], dtype=float32), 0.42829183]. 
=============================================
[2019-04-27 23:37:55,526] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.05893]
 [67.04716]
 [67.14893]
 [67.07488]
 [66.84493]], R is [[67.27507019]
 [67.28915405]
 [67.30945587]
 [67.34942627]
 [67.38724518]].
[2019-04-27 23:37:58,099] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3322906e-31 1.0000000e+00 0.0000000e+00 8.0513201e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:37:58,104] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2267
[2019-04-27 23:37:58,109] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 73.33333333333334, 1.0, 2.0, 0.3574353477227357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549117.5728930659, 549117.5728930659, 170529.9301087906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7348800.0000, 
sim time next is 7349400.0000, 
raw observation next is [24.95, 73.0, 1.0, 2.0, 0.3558436563160816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 547202.70411356, 547202.70411356, 170385.2903284315], 
processed observation next is [1.0, 0.043478260869565216, 0.3815165876777251, 0.73, 1.0, 1.0, 0.22390801965792964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15200075114265557, 0.15200075114265557, 0.2543064034752709], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.375745], dtype=float32), 0.9664104]. 
=============================================
[2019-04-27 23:38:06,410] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.658597e-31 1.000000e+00 0.000000e+00 6.615931e-30 0.000000e+00], sum to 1.0000
[2019-04-27 23:38:06,417] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6055
[2019-04-27 23:38:06,425] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 87.0, 1.0, 2.0, 0.3709798392562047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561317.5032886798, 561317.5032886798, 171309.6733639968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7470000.0000, 
sim time next is 7470600.0000, 
raw observation next is [23.6, 86.66666666666667, 1.0, 2.0, 0.3723569981784813, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 562537.1515428284, 562537.1515428284, 171387.7869494212], 
processed observation next is [0.0, 0.4782608695652174, 0.3175355450236968, 0.8666666666666667, 1.0, 1.0, 0.24380361226323047, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1562603198730079, 0.1562603198730079, 0.2558026670886884], 
reward next is 0.7442, 
noisyNet noise sample is [array([2.001831], dtype=float32), -0.4734342]. 
=============================================
[2019-04-27 23:38:06,825] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.44225371e-29 1.00000000e+00 0.00000000e+00 1.00657006e-25
 0.00000000e+00], sum to 1.0000
[2019-04-27 23:38:06,831] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6726
[2019-04-27 23:38:06,837] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 85.0, 1.0, 2.0, 0.3838705267360344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 574791.6780117674, 574791.6780117681, 172304.2200867445], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7473600.0000, 
sim time next is 7474200.0000, 
raw observation next is [24.21666666666667, 84.50000000000001, 1.0, 2.0, 0.386239480210638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 577380.7410647481, 577380.7410647476, 172505.4138475132], 
processed observation next is [0.0, 0.5217391304347826, 0.34676145339652464, 0.8450000000000002, 1.0, 1.0, 0.2605294942296843, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16038353918465226, 0.1603835391846521, 0.2574707669365869], 
reward next is 0.7425, 
noisyNet noise sample is [array([1.5532228], dtype=float32), -1.2959547]. 
=============================================
[2019-04-27 23:38:08,271] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-27 23:38:08,272] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:38:08,273] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:38:08,274] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:38:08,275] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:38:08,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:38:08,277] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:38:08,277] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:38:08,277] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:38:08,278] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:38:08,279] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:38:08,311] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run26
[2019-04-27 23:38:08,311] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run26
[2019-04-27 23:38:08,368] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run26
[2019-04-27 23:38:08,368] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run26
[2019-04-27 23:38:08,434] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run26
[2019-04-27 23:38:27,787] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:38:27,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [17.23333333333333, 92.66666666666667, 1.0, 2.0, 0.2148920048182471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 358747.9412532523, 358747.9412532523, 156871.5953370844]
[2019-04-27 23:38:27,789] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:38:27,792] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.682701e-28 1.000000e+00 0.000000e+00 2.786012e-25 0.000000e+00], sampled 0.5887379608079052
[2019-04-27 23:38:28,142] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:38:28,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.96666666666667, 48.33333333333333, 1.0, 2.0, 0.2890604091673847, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 467718.0694704193, 467718.0694704193, 164790.5992554921]
[2019-04-27 23:38:28,143] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:38:28,146] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.23304608e-28 1.00000000e+00 0.00000000e+00 1.02396685e-25
 0.00000000e+00], sampled 0.08828945711165526
[2019-04-27 23:38:29,945] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:38:29,946] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.33333333333334, 85.0, 1.0, 2.0, 0.3538494131237457, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542387.8529091681, 542387.8529091687, 169931.9467074938]
[2019-04-27 23:38:29,946] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:38:29,950] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.7983265e-28 1.0000000e+00 0.0000000e+00 1.7453848e-25 0.0000000e+00], sampled 0.05895281201182223
[2019-04-27 23:38:41,542] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:38:41,543] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.561048545, 99.22385886333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.374157035989995, 6.9112, 168.9102055485127, 1782412.183909212, 1453979.877850571, 311353.1035293365]
[2019-04-27 23:38:41,544] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:38:41,547] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.4707297e-25 1.0000000e+00 3.2421865e-35 1.2002772e-22 6.5019003e-35], sampled 0.1984053171855661
[2019-04-27 23:38:41,549] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1782412.183909212 W.
[2019-04-27 23:39:10,477] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:39:10,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666666, 71.66666666666667, 1.0, 2.0, 0.8924628140925601, 1.0, 2.0, 0.8924628140925601, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2496192.132536153, 2496192.132536154, 467396.2926540825]
[2019-04-27 23:39:10,478] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:39:10,480] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.5790286e-26 1.0000000e+00 1.2127597e-36 1.4839881e-23 2.5062361e-36], sampled 0.5906113912484623
[2019-04-27 23:39:10,481] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2496192.132536153 W.
[2019-04-27 23:39:15,393] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:39:15,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.77499849, 84.01913462, 1.0, 2.0, 0.5199918918042812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726617.433115448, 726617.4331154487, 186786.514473905]
[2019-04-27 23:39:15,394] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:39:15,398] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7672972e-28 1.0000000e+00 0.0000000e+00 5.9031614e-26 0.0000000e+00], sampled 0.7989788966184362
[2019-04-27 23:39:36,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.084246]
[2019-04-27 23:39:36,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.20660214833333, 62.72465077666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.168324586392751, 6.9112, 168.9114347721997, 1636291.243063282, 1453879.855291488, 311347.7184337689]
[2019-04-27 23:39:36,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:39:36,923] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2972227e-27 1.0000000e+00 0.0000000e+00 6.1251273e-25 1.7550474e-38], sampled 0.7682864764890658
[2019-04-27 23:39:51,642] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:39:52,604] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:39:52,641] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:39:52,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:39:52,747] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:39:53,758] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 625000, evaluation results [625000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:39:56,877] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.5756821e-21 1.0000000e+00 3.3769075e-31 6.7125096e-22 4.8923497e-31], sum to 1.0000
[2019-04-27 23:39:56,885] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3897
[2019-04-27 23:39:56,895] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2167746.008535244 W.
[2019-04-27 23:39:56,914] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.55, 62.66666666666666, 1.0, 2.0, 0.5167584328484387, 1.0, 2.0, 0.5167584328484387, 1.0, 1.0, 0.8937476623876908, 6.9112, 6.9112, 170.5573041426782, 2167746.008535244, 2167746.008535244, 426237.3352998784], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7733400.0000, 
sim time next is 7734000.0000, 
raw observation next is [31.6, 62.33333333333334, 1.0, 2.0, 0.7025183799419177, 1.0, 2.0, 0.7025183799419177, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1964473.117367787, 1964473.117367788, 375203.5579252609], 
processed observation next is [1.0, 0.5217391304347826, 0.6966824644549764, 0.6233333333333334, 1.0, 1.0, 0.6415884095685755, 1.0, 1.0, 0.6415884095685755, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5456869770466075, 0.5456869770466077, 0.5600053103362104], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5165901], dtype=float32), -0.10997237]. 
=============================================
[2019-04-27 23:39:56,928] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[54.134987]
 [54.915615]
 [53.99943 ]
 [54.211216]
 [54.04353 ]], R is [[54.578022  ]
 [54.03224182]
 [53.49192047]
 [52.95700073]
 [52.78799438]].
[2019-04-27 23:40:11,913] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:11,915] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:11,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run4
[2019-04-27 23:40:13,322] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.6512897e-23 1.0000000e+00 1.0973423e-31 9.7045636e-19 5.2899897e-32], sum to 1.0000
[2019-04-27 23:40:13,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2876
[2019-04-27 23:40:13,341] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333333, 84.5, 1.0, 2.0, 0.624603450830829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 872857.8190646672, 872857.8190646672, 205411.0861903034], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7887000.0000, 
sim time next is 7887600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.6229492341773959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870545.1714443369, 870545.1714443369, 205091.3508350434], 
processed observation next is [1.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.5457219688884287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24181810317898247, 0.24181810317898247, 0.3061064937836469], 
reward next is 0.6939, 
noisyNet noise sample is [array([0.88312024], dtype=float32), 0.69632655]. 
=============================================
[2019-04-27 23:40:15,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.0776391e-23 1.0000000e+00 4.8478320e-33 1.8041346e-21 9.7189479e-32], sum to 1.0000
[2019-04-27 23:40:15,915] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7891
[2019-04-27 23:40:15,919] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2139240.010929631 W.
[2019-04-27 23:40:15,924] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.65, 69.0, 1.0, 2.0, 0.8887144758834483, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.996128116218053, 6.9112, 168.9124511153206, 2139240.010929631, 2078989.262557839, 431316.9563817183], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7828200.0000, 
sim time next is 7828800.0000, 
raw observation next is [30.76666666666667, 69.33333333333333, 1.0, 2.0, 0.7823238437937511, 1.0, 1.0, 0.7823238437937511, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2187863.408146088, 2187863.408146088, 411358.5336718255], 
processed observation next is [1.0, 0.6086956521739131, 0.6571879936808849, 0.6933333333333332, 1.0, 1.0, 0.7377395708358446, 1.0, 0.5, 0.7377395708358446, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6077398355961355, 0.6077398355961355, 0.6139679607042171], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.33574662], dtype=float32), -0.43407646]. 
=============================================
[2019-04-27 23:40:18,136] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.6199361e-26 1.0000000e+00 8.9603681e-35 2.2820735e-23 2.9370378e-35], sum to 1.0000
[2019-04-27 23:40:18,143] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6090
[2019-04-27 23:40:18,147] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 87.0, 1.0, 2.0, 0.3513727655910265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 543999.6587933301, 543999.6587933307, 170222.2389485166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 79200.0000, 
sim time next is 79800.0000, 
raw observation next is [22.61666666666667, 87.33333333333333, 1.0, 2.0, 0.3503533535017277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 542928.806202112, 542928.8062021126, 170147.3838959649], 
processed observation next is [1.0, 0.9565217391304348, 0.2709320695102688, 0.8733333333333333, 1.0, 1.0, 0.21729319699003335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15081355727836446, 0.1508135572783646, 0.2539513192477088], 
reward next is 0.7460, 
noisyNet noise sample is [array([-0.1253698], dtype=float32), -2.443522]. 
=============================================
[2019-04-27 23:40:18,740] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:18,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:18,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run4
[2019-04-27 23:40:19,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:19,696] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:19,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run4
[2019-04-27 23:40:19,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:19,992] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,001] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run4
[2019-04-27 23:40:20,202] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:20,202] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,204] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run4
[2019-04-27 23:40:20,422] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:20,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,431] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run4
[2019-04-27 23:40:20,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:20,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,659] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run4
[2019-04-27 23:40:20,785] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:20,786] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run4
[2019-04-27 23:40:20,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:20,859] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:20,862] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run4
[2019-04-27 23:40:23,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:23,075] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:23,082] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run4
[2019-04-27 23:40:23,359] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:23,388] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:23,390] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run4
[2019-04-27 23:40:23,580] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:23,581] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:23,588] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run4
[2019-04-27 23:40:23,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:23,595] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:23,656] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run4
[2019-04-27 23:40:24,244] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:24,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:24,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run4
[2019-04-27 23:40:24,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:24,461] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:24,486] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run4
[2019-04-27 23:40:24,561] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-27 23:40:24,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:24,571] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run4
[2019-04-27 23:40:30,308] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.3876921e-29 1.0000000e+00 0.0000000e+00 7.7436544e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:40:30,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1040
[2019-04-27 23:40:30,331] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.58333333333334, 96.0, 1.0, 2.0, 0.8156846133887776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1223659.216140579, 1223659.216140578, 259213.1658876538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 144600.0000, 
sim time next is 145200.0000, 
raw observation next is [22.56666666666667, 96.0, 1.0, 2.0, 0.7684594695932436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1153431.100435433, 1153431.100435433, 246902.4024887146], 
processed observation next is [1.0, 0.6956521739130435, 0.26856240126382325, 0.96, 1.0, 1.0, 0.7210355055340284, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3203975278987314, 0.3203975278987314, 0.36851104849061883], 
reward next is 0.6315, 
noisyNet noise sample is [array([-0.6933418], dtype=float32), 0.8890368]. 
=============================================
[2019-04-27 23:40:34,160] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1675632e-29 1.0000000e+00 0.0000000e+00 3.0535724e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:40:34,176] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0458
[2019-04-27 23:40:34,181] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.33333333333334, 1.0, 2.0, 0.9424317766578285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1407771.218817024, 1407771.218817024, 295150.583107924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 127200.0000, 
sim time next is 127800.0000, 
raw observation next is [22.8, 95.5, 1.0, 2.0, 0.9552732516788897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1425884.400256732, 1425884.400256732, 298995.2809642474], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.955, 1.0, 1.0, 0.946112351420349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.39607900007131447, 0.39607900007131447, 0.44626161337947373], 
reward next is 0.5537, 
noisyNet noise sample is [array([0.70644313], dtype=float32), 0.14932984]. 
=============================================
[2019-04-27 23:40:34,368] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0676027e-28 1.0000000e+00 0.0000000e+00 2.9100083e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:40:34,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9219
[2019-04-27 23:40:34,391] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.3819982060870853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574951.4300596942, 574951.4300596935, 172412.015864021], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 150000.0000, 
sim time next is 150600.0000, 
raw observation next is [22.5, 96.0, 1.0, 2.0, 0.3810339500190703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573499.0209116923, 573499.0209116916, 172282.963680865], 
processed observation next is [1.0, 0.7391304347826086, 0.2654028436018958, 0.96, 1.0, 1.0, 0.2542577711073136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1593052835865812, 0.159305283586581, 0.2571387517624851], 
reward next is 0.7429, 
noisyNet noise sample is [array([-2.0777974], dtype=float32), -0.6631258]. 
=============================================
[2019-04-27 23:40:41,165] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9950676e-28 1.0000000e+00 1.6389699e-38 4.3217128e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:40:41,171] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1477
[2019-04-27 23:40:41,174] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.31666666666667, 88.83333333333334, 1.0, 2.0, 0.3006929564937792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478882.1058054178, 478882.1058054178, 165542.645426857], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 240600.0000, 
sim time next is 241200.0000, 
raw observation next is [21.3, 89.0, 1.0, 2.0, 0.3006138163358041, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 478712.2646796132, 478712.2646796139, 165529.8364474007], 
processed observation next is [0.0, 0.8260869565217391, 0.2085308056872039, 0.89, 1.0, 1.0, 0.1573660437780772, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13297562907767033, 0.13297562907767052, 0.24705945738418017], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.02280275], dtype=float32), 1.0258292]. 
=============================================
[2019-04-27 23:40:52,872] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0420308e-31 1.0000000e+00 0.0000000e+00 3.6762692e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:40:52,880] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8386
[2019-04-27 23:40:52,885] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 76.66666666666667, 1.0, 2.0, 0.3979314826951001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654638.7772868029, 654638.7772868029, 179169.1155806829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 463200.0000, 
sim time next is 463800.0000, 
raw observation next is [21.06666666666667, 75.83333333333333, 1.0, 2.0, 0.4052513659093974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666519.4414477205, 666519.4414477205, 180246.454953312], 
processed observation next is [1.0, 0.34782608695652173, 0.19747235387045833, 0.7583333333333333, 1.0, 1.0, 0.28343538061373186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18514428929103346, 0.18514428929103346, 0.26902455963180893], 
reward next is 0.7310, 
noisyNet noise sample is [array([-0.92994684], dtype=float32), 1.377182]. 
=============================================
[2019-04-27 23:40:53,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.4185150e-25 1.0000000e+00 5.3744549e-35 7.0591656e-22 3.4329589e-35], sum to 1.0000
[2019-04-27 23:40:53,096] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7777
[2019-04-27 23:40:53,108] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.23333333333333, 89.16666666666667, 1.0, 2.0, 0.2364949611716479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 393337.1472689391, 393337.1472689398, 159239.9564376285], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 529800.0000, 
sim time next is 530400.0000, 
raw observation next is [18.16666666666667, 89.33333333333334, 1.0, 2.0, 0.219604316795121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 365366.1159042977, 365366.1159042984, 157679.6430990646], 
processed observation next is [1.0, 0.13043478260869565, 0.06003159557661956, 0.8933333333333334, 1.0, 1.0, 0.0597642371025554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1014905877511938, 0.10149058775119399, 0.23534275089412626], 
reward next is 0.7647, 
noisyNet noise sample is [array([0.8391573], dtype=float32), -0.20986825]. 
=============================================
[2019-04-27 23:40:54,274] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 23:40:54,275] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:40:54,276] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:40:54,277] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:40:54,278] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:40:54,277] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:54,279] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:54,280] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:54,278] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:54,280] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:40:54,284] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:40:54,290] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run27
[2019-04-27 23:40:54,290] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run27
[2019-04-27 23:40:54,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run27
[2019-04-27 23:40:54,357] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run27
[2019-04-27 23:40:54,357] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run27
[2019-04-27 23:41:14,035] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:14,038] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.55, 83.0, 1.0, 2.0, 0.4482939238261501, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 666977.9242965096, 666977.9242965089, 180973.5311710705]
[2019-04-27 23:41:14,039] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:41:14,042] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.376089e-28 1.000000e+00 0.000000e+00 5.459511e-26 0.000000e+00], sampled 0.5559816732113249
[2019-04-27 23:41:17,316] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:17,317] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.8, 93.0, 1.0, 2.0, 0.4558420282297143, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687818.8750510194, 687818.8750510187, 183242.8854862213]
[2019-04-27 23:41:17,319] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:41:17,322] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9688038e-28 1.0000000e+00 0.0000000e+00 1.4312741e-25 0.0000000e+00], sampled 0.9067917138401248
[2019-04-27 23:41:27,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:27,322] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [33.06666666666667, 59.66666666666667, 1.0, 2.0, 0.5704919447144311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 797210.8199547116, 797210.8199547116, 195377.2356514107]
[2019-04-27 23:41:27,325] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:41:27,328] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1237176e-29 1.0000000e+00 0.0000000e+00 5.5809020e-27 0.0000000e+00], sampled 0.7547453818488377
[2019-04-27 23:41:39,122] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:39,123] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.11666666666667, 53.66666666666667, 1.0, 2.0, 0.6016808039380093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 840811.6764691822, 840811.6764691828, 201058.7711771761]
[2019-04-27 23:41:39,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:41:39,126] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.8963662e-30 1.0000000e+00 0.0000000e+00 2.6198404e-27 0.0000000e+00], sampled 0.8548830116231229
[2019-04-27 23:41:41,986] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:41,986] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.1, 63.0, 1.0, 2.0, 0.5623351199231236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785808.1797852091, 785808.1797852091, 193938.659184007]
[2019-04-27 23:41:41,986] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:41:41,988] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.4851974e-29 1.0000000e+00 0.0000000e+00 1.9665695e-26 0.0000000e+00], sampled 0.6307359537050923
[2019-04-27 23:41:58,325] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:41:58,325] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.58333333333334, 87.0, 1.0, 2.0, 0.5174033452941711, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722999.0630069259, 722999.0630069259, 186367.320172311]
[2019-04-27 23:41:58,326] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:41:58,328] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.9629933e-29 1.0000000e+00 0.0000000e+00 2.5488356e-26 0.0000000e+00], sampled 0.07498895622479917
[2019-04-27 23:42:04,689] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:42:04,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.22108798, 74.15728711, 1.0, 2.0, 0.4086418744648561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 614300.4387820023, 614300.4387820023, 175977.0095586453]
[2019-04-27 23:42:04,690] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:42:04,692] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.9101539e-29 1.0000000e+00 0.0000000e+00 2.1358639e-26 0.0000000e+00], sampled 0.9466731627931221
[2019-04-27 23:42:16,564] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:42:16,564] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.360647955, 82.89793711499999, 1.0, 2.0, 0.3581861463467029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553761.2223714319, 553761.2223714312, 171014.5504509862]
[2019-04-27 23:42:16,565] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:42:16,568] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.0848908e-28 1.0000000e+00 0.0000000e+00 3.0411355e-25 0.0000000e+00], sampled 0.20471516782457067
[2019-04-27 23:42:17,382] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09399129]
[2019-04-27 23:42:17,383] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.546986845, 98.38568632, 1.0, 2.0, 0.4495721631022399, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642780.486967819, 642780.486967819, 177949.7535550868]
[2019-04-27 23:42:17,384] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:42:17,385] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.082202e-29 1.000000e+00 0.000000e+00 2.982768e-26 0.000000e+00], sampled 0.9055924155720884
[2019-04-27 23:42:21,942] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:42:22,126] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:42:22,182] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:42:22,355] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:42:22,366] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:42:23,382] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 650000, evaluation results [650000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:42:29,593] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1812246e-26 1.0000000e+00 2.2006678e-38 8.1084457e-24 1.6343444e-37], sum to 1.0000
[2019-04-27 23:42:29,600] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8380
[2019-04-27 23:42:29,605] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.73333333333333, 83.5, 1.0, 2.0, 0.2227071482667095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 370819.1397679032, 370819.1397679038, 157885.6006832933], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 600600.0000, 
sim time next is 601200.0000, 
raw observation next is [18.6, 84.0, 1.0, 2.0, 0.22190195719751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 369653.2720077017, 369653.2720077011, 157767.6579558848], 
processed observation next is [1.0, 1.0, 0.08056872037914704, 0.84, 1.0, 1.0, 0.06253247855121684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1026814644465838, 0.10268146444658365, 0.23547411635206686], 
reward next is 0.7645, 
noisyNet noise sample is [array([0.86825544], dtype=float32), -0.5132359]. 
=============================================
[2019-04-27 23:42:32,325] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.8744808e-27 1.0000000e+00 0.0000000e+00 6.8223390e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:32,330] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1121
[2019-04-27 23:42:32,336] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.16666666666667, 1.0, 2.0, 0.2156493060691995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 157168.2631591955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [17.6, 92.0, 1.0, 2.0, 0.2149129967709744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073988, 157091.67273694], 
processed observation next is [1.0, 0.17391304347826086, 0.0331753554502371, 0.92, 1.0, 1.0, 0.05411204430237877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09950699402983283, 0.099506994029833, 0.2344651831894627], 
reward next is 0.7655, 
noisyNet noise sample is [array([-0.15630727], dtype=float32), 1.3114685]. 
=============================================
[2019-04-27 23:42:32,390] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.7480658e-30 1.0000000e+00 0.0000000e+00 2.0555755e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:32,398] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4939
[2019-04-27 23:42:32,404] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 55.0, 1.0, 2.0, 0.3502085986538186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 575125.4332449789, 575125.4332449782, 172544.3181289358], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 651600.0000, 
sim time next is 652200.0000, 
raw observation next is [24.53333333333333, 54.66666666666667, 1.0, 2.0, 0.4284192119878631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703771.0454103132, 703771.0454103132, 183774.9448439348], 
processed observation next is [1.0, 0.5652173913043478, 0.36176935229067925, 0.5466666666666667, 1.0, 1.0, 0.31134844817814833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19549195705842032, 0.19549195705842032, 0.274290962453634], 
reward next is 0.7257, 
noisyNet noise sample is [array([0.6333058], dtype=float32), 0.25965497]. 
=============================================
[2019-04-27 23:42:32,463] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0702391e-31 1.0000000e+00 0.0000000e+00 3.6479664e-31 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:32,471] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5350
[2019-04-27 23:42:32,477] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 93.0, 1.0, 2.0, 0.2214840591204185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 368918.9041889188, 368918.9041889194, 157741.2396162729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 702000.0000, 
sim time next is 702600.0000, 
raw observation next is [17.6, 92.83333333333333, 1.0, 2.0, 0.220778831403438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367787.6481783985, 367787.6481783985, 157666.9052754362], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9283333333333332, 1.0, 1.0, 0.06117931494390118, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10216323560511069, 0.10216323560511069, 0.23532373921706895], 
reward next is 0.7647, 
noisyNet noise sample is [array([-1.0042329], dtype=float32), 0.53387433]. 
=============================================
[2019-04-27 23:42:33,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [8.0074256e-35 1.0000000e+00 0.0000000e+00 1.7830464e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:33,287] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2596
[2019-04-27 23:42:33,292] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.7, 82.66666666666667, 1.0, 2.0, 0.2393789772093673, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395749.9689925067, 395749.9689925067, 159798.8369622887], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 682800.0000, 
sim time next is 683400.0000, 
raw observation next is [19.6, 83.33333333333333, 1.0, 2.0, 0.2397787823615639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 396477.27191647, 396477.2719164694, 159832.2482227832], 
processed observation next is [1.0, 0.9130434782608695, 0.127962085308057, 0.8333333333333333, 1.0, 1.0, 0.08407082212236615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11013257553235278, 0.1101325755323526, 0.238555594362363], 
reward next is 0.7614, 
noisyNet noise sample is [array([1.1323221], dtype=float32), -0.06034965]. 
=============================================
[2019-04-27 23:42:37,571] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2107349e-27 1.0000000e+00 0.0000000e+00 2.0559294e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:37,581] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2420
[2019-04-27 23:42:37,584] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.91666666666667, 48.16666666666666, 1.0, 2.0, 0.6114461933550479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1003030.436615798, 1003030.436615798, 218083.1726137713], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 742200.0000, 
sim time next is 742800.0000, 
raw observation next is [25.83333333333334, 48.33333333333333, 1.0, 2.0, 0.5711676738858369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937474.5379057351, 937474.5379057344, 209554.2178707939], 
processed observation next is [1.0, 0.6086956521739131, 0.42338072669826254, 0.4833333333333333, 1.0, 1.0, 0.4833345468504059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2604095938627042, 0.260409593862704, 0.31276748935939386], 
reward next is 0.6872, 
noisyNet noise sample is [array([1.8257295], dtype=float32), 1.6647929]. 
=============================================
[2019-04-27 23:42:47,736] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9797689e-29 1.0000000e+00 0.0000000e+00 4.7083860e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:47,743] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1207
[2019-04-27 23:42:47,749] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.03333333333334, 82.83333333333334, 1.0, 2.0, 0.3303007847433387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 514730.1321471648, 514730.1321471654, 167969.7924274406], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 933000.0000, 
sim time next is 933600.0000, 
raw observation next is [22.96666666666667, 83.66666666666667, 1.0, 2.0, 0.3323981950555167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 517377.7231322267, 517377.7231322261, 168159.1274346377], 
processed observation next is [0.0, 0.8260869565217391, 0.2875197472353872, 0.8366666666666667, 1.0, 1.0, 0.19566047597050207, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1437160342033963, 0.14371603420339613, 0.250983772290504], 
reward next is 0.7490, 
noisyNet noise sample is [array([1.7495619], dtype=float32), 0.9480782]. 
=============================================
[2019-04-27 23:42:55,274] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.9983798e-27 1.0000000e+00 0.0000000e+00 4.5158836e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:42:55,286] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4746
[2019-04-27 23:42:55,292] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 95.0, 1.0, 2.0, 0.3073458732453746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 487114.345083642, 487114.3450836426, 166095.502004618], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1064400.0000, 
sim time next is 1065000.0000, 
raw observation next is [20.85, 95.0, 1.0, 2.0, 0.3083842137190738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 488194.5129736612, 488194.5129736612, 166163.0152901783], 
processed observation next is [1.0, 0.30434782608695654, 0.18720379146919444, 0.95, 1.0, 1.0, 0.16672796833623352, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1356095869371281, 0.1356095869371281, 0.24800450043310193], 
reward next is 0.7520, 
noisyNet noise sample is [array([1.1182356], dtype=float32), 0.49247172]. 
=============================================
[2019-04-27 23:42:55,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[73.853935]
 [73.862   ]
 [73.88985 ]
 [73.90586 ]
 [73.98387 ]], R is [[73.84417725]
 [73.85783386]
 [73.87125397]
 [73.88507843]
 [73.89666748]].
[2019-04-27 23:43:05,658] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2332754e-25 1.0000000e+00 2.5293144e-37 2.7107012e-23 4.7644780e-37], sum to 1.0000
[2019-04-27 23:43:05,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1393
[2019-04-27 23:43:05,674] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 93.66666666666667, 1.0, 2.0, 0.536796362988044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 779058.4267126544, 779058.4267126544, 193226.5869690716], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1320000.0000, 
sim time next is 1320600.0000, 
raw observation next is [23.68333333333333, 93.83333333333334, 1.0, 2.0, 0.5231184542164877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761068.9233658643, 761068.923365865, 191096.5628245871], 
processed observation next is [1.0, 0.2608695652173913, 0.32148499210110576, 0.9383333333333335, 1.0, 1.0, 0.4254439207427563, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21140803426829566, 0.21140803426829585, 0.2852187504844584], 
reward next is 0.7148, 
noisyNet noise sample is [array([-1.0685134], dtype=float32), -1.2505115]. 
=============================================
[2019-04-27 23:43:08,504] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.957729e-32 1.000000e+00 0.000000e+00 8.412405e-30 0.000000e+00], sum to 1.0000
[2019-04-27 23:43:08,513] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1445
[2019-04-27 23:43:08,517] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 94.0, 1.0, 2.0, 0.3239144034887438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 509831.6226230711, 509831.6226230717, 167720.4217062879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1368000.0000, 
sim time next is 1368600.0000, 
raw observation next is [21.16666666666667, 94.16666666666667, 1.0, 2.0, 0.3237531098649404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 509624.6295686291, 509624.6295686285, 167705.5916745497], 
processed observation next is [1.0, 0.8695652173913043, 0.2022116903633494, 0.9416666666666668, 1.0, 1.0, 0.18524471068065104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14156239710239696, 0.14156239710239682, 0.2503068532455966], 
reward next is 0.7497, 
noisyNet noise sample is [array([0.14170992], dtype=float32), 0.4024776]. 
=============================================
[2019-04-27 23:43:08,755] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.7997864e-29 1.0000000e+00 0.0000000e+00 2.2290283e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:43:08,760] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2760
[2019-04-27 23:43:08,769] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.15, 94.0, 1.0, 2.0, 0.3223590488058941, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508003.9912409307, 508003.9912409307, 167594.3001819901], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1366200.0000, 
sim time next is 1366800.0000, 
raw observation next is [21.16666666666667, 94.0, 1.0, 2.0, 0.3231308022277213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509013.5723045908, 509013.5723045908, 167666.8986250857], 
processed observation next is [1.0, 0.8260869565217391, 0.2022116903633494, 0.94, 1.0, 1.0, 0.18449494244303768, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14139265897349743, 0.14139265897349743, 0.2502491024255011], 
reward next is 0.7498, 
noisyNet noise sample is [array([-0.18804151], dtype=float32), 0.56893647]. 
=============================================
[2019-04-27 23:43:13,011] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.0346999e-30 1.0000000e+00 1.9735993e-38 6.2553771e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:43:13,020] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5584
[2019-04-27 23:43:13,036] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.9, 92.66666666666667, 1.0, 2.0, 0.3832052040680715, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576963.7118294627, 576963.711829462, 172597.2331045948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1455600.0000, 
sim time next is 1456200.0000, 
raw observation next is [22.85, 93.0, 1.0, 2.0, 0.3826004951318505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 576206.3941894145, 576206.3941894138, 172534.4013985867], 
processed observation next is [0.0, 0.8695652173913043, 0.28199052132701435, 0.93, 1.0, 1.0, 0.2561451748576512, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1600573317192818, 0.1600573317192816, 0.2575140319381891], 
reward next is 0.7425, 
noisyNet noise sample is [array([-0.9238523], dtype=float32), 0.90942615]. 
=============================================
[2019-04-27 23:43:14,359] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 23:43:14,361] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:43:14,362] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:43:14,363] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:43:14,364] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:43:14,365] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:43:14,365] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:43:14,366] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:43:14,366] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:43:14,367] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:43:14,367] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:43:14,392] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run28
[2019-04-27 23:43:14,393] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run28
[2019-04-27 23:43:14,448] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run28
[2019-04-27 23:43:14,449] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run28
[2019-04-27 23:43:14,512] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run28
[2019-04-27 23:43:18,268] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:18,269] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.75858919, 94.33933063, 1.0, 2.0, 0.2363879149368935, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 392463.4967534872, 392463.4967534872, 159346.6966858807]
[2019-04-27 23:43:18,271] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:43:18,272] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4490313e-31 1.0000000e+00 0.0000000e+00 4.6554797e-28 0.0000000e+00], sampled 0.32847736813943196
[2019-04-27 23:43:24,039] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:24,042] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.0, 62.0, 1.0, 2.0, 0.2895444917947173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463480.6529531262, 463480.6529531256, 164493.9486541766]
[2019-04-27 23:43:24,042] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:43:24,045] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.3696044e-31 1.0000000e+00 0.0000000e+00 6.8662994e-28 0.0000000e+00], sampled 0.4537610478941245
[2019-04-27 23:43:43,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:43,860] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.27145276666667, 82.56940720000001, 1.0, 2.0, 0.4060826009072258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 608417.2311849818, 608417.2311849812, 175379.4654895141]
[2019-04-27 23:43:43,862] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:43:43,863] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3555746e-31 1.0000000e+00 0.0000000e+00 1.3192689e-28 0.0000000e+00], sampled 0.8119258587389784
[2019-04-27 23:43:45,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:45,102] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.91126535333333, 82.60998995666667, 1.0, 2.0, 0.4210073077929422, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622521.0608243099, 622521.0608243092, 176499.1958116686]
[2019-04-27 23:43:45,103] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:43:45,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1032906e-30 1.0000000e+00 0.0000000e+00 8.8185549e-28 0.0000000e+00], sampled 0.23176771708420763
[2019-04-27 23:43:45,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:45,154] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.130584835, 96.15884889, 1.0, 2.0, 0.2848578930204107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456832.9578334519, 456832.9578334519, 164047.8427424437]
[2019-04-27 23:43:45,154] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:43:45,157] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.57419335e-30 1.00000000e+00 0.00000000e+00 1.21726582e-27
 0.00000000e+00], sampled 0.9810508534774185
[2019-04-27 23:43:53,999] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:43:54,000] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.93333333333334, 73.33333333333334, 1.0, 2.0, 0.745138930019449, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994790665354731, 6.9112, 168.9123912936355, 1938296.015765392, 1878994.119230838, 395168.522970612]
[2019-04-27 23:43:54,002] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:43:54,004] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4062923e-28 1.0000000e+00 0.0000000e+00 1.1590633e-25 0.0000000e+00], sampled 0.6374888066572039
[2019-04-27 23:43:54,007] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1938296.015765392 W.
[2019-04-27 23:44:16,997] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:44:16,998] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.08333333333333, 93.00000000000001, 1.0, 2.0, 0.7646290267128402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1068636.295664344, 1068636.295664344, 235412.9370440474]
[2019-04-27 23:44:16,998] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:44:17,000] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0863135e-31 1.0000000e+00 0.0000000e+00 1.0796185e-28 0.0000000e+00], sampled 0.041524231685235
[2019-04-27 23:44:21,320] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:44:21,321] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.25, 86.16666666666667, 1.0, 2.0, 0.5296273506821226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740086.3573434277, 740086.3573434271, 188368.3708849643]
[2019-04-27 23:44:21,323] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:44:21,327] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.6243373e-32 1.0000000e+00 0.0000000e+00 5.9444604e-29 0.0000000e+00], sampled 0.8109983514455765
[2019-04-27 23:44:27,274] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:44:27,275] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.73333333333333, 84.16666666666667, 1.0, 2.0, 0.5147999463730263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719359.9440573108, 719359.9440573108, 185946.5675804545]
[2019-04-27 23:44:27,276] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:44:27,279] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1715609e-32 1.0000000e+00 0.0000000e+00 4.5344681e-29 0.0000000e+00], sampled 0.10600942654991774
[2019-04-27 23:44:40,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.103182286]
[2019-04-27 23:44:40,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.13333333333333, 62.66666666666667, 1.0, 2.0, 0.3510543729503853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 559219.9686554418, 559219.9686554418, 171721.2714185643]
[2019-04-27 23:44:40,109] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:44:40,111] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5356436e-31 1.0000000e+00 0.0000000e+00 4.7217269e-28 0.0000000e+00], sampled 0.5279030143610337
[2019-04-27 23:44:42,064] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:44:42,343] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:44:42,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:44:42,564] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:44:42,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:44:43,619] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 675000, evaluation results [675000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:44:44,195] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.0672718e-28 1.0000000e+00 0.0000000e+00 4.8041635e-26 1.9711916e-38], sum to 1.0000
[2019-04-27 23:44:44,201] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-27 23:44:44,206] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 69.33333333333334, 1.0, 2.0, 0.4339430502299369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622305.1670256971, 622305.1670256971, 175954.0529619521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1437600.0000, 
sim time next is 1438200.0000, 
raw observation next is [27.8, 69.5, 1.0, 2.0, 0.4380729333670599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625911.7252979735, 625911.7252979735, 176244.3485751881], 
processed observation next is [0.0, 0.6521739130434783, 0.5165876777251186, 0.695, 1.0, 1.0, 0.3229794377916384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17386436813832598, 0.17386436813832598, 0.2630512665301315], 
reward next is 0.7369, 
noisyNet noise sample is [array([-0.7428581], dtype=float32), -1.3618956]. 
=============================================
[2019-04-27 23:44:47,280] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.3830777e-28 1.0000000e+00 0.0000000e+00 1.6306515e-22 2.0678725e-38], sum to 1.0000
[2019-04-27 23:44:47,287] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2454
[2019-04-27 23:44:47,294] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 96.66666666666667, 1.0, 2.0, 0.7759857961140745, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102290.8804797, 1102290.8804797, 240568.0929563844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1671600.0000, 
sim time next is 1672200.0000, 
raw observation next is [24.05, 96.0, 1.0, 2.0, 0.8629951862666918, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1223988.652060425, 1223988.652060425, 262545.9603633804], 
processed observation next is [1.0, 0.34782608695652173, 0.3388625592417062, 0.96, 1.0, 1.0, 0.83493395935746, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33999684779456246, 0.33999684779456246, 0.39185964233340354], 
reward next is 0.6081, 
noisyNet noise sample is [array([0.36864632], dtype=float32), 0.12265079]. 
=============================================
[2019-04-27 23:44:47,534] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5842558e-32 1.0000000e+00 0.0000000e+00 1.9452658e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:44:47,539] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6049
[2019-04-27 23:44:47,570] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333333, 96.16666666666666, 1.0, 2.0, 0.3359527023222311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523185.4357217235, 523185.4357217235, 168624.0252234735], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1476600.0000, 
sim time next is 1477200.0000, 
raw observation next is [21.26666666666667, 96.33333333333333, 1.0, 2.0, 0.3344182530572016, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521447.604229324, 521447.6042293247, 168504.2985355485], 
processed observation next is [0.0, 0.08695652173913043, 0.2069510268562403, 0.9633333333333333, 1.0, 1.0, 0.19809428079180913, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14484655673036778, 0.14484655673036798, 0.2514989530381321], 
reward next is 0.7485, 
noisyNet noise sample is [array([0.8229591], dtype=float32), 0.23420939]. 
=============================================
[2019-04-27 23:44:48,740] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [8.2014072e-29 1.0000000e+00 1.9624283e-38 2.4870390e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:44:48,745] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6347
[2019-04-27 23:44:48,751] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.73333333333333, 57.66666666666667, 1.0, 2.0, 0.3526441832612439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540634.1755224256, 540634.1755224256, 169789.5666963984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1506000.0000, 
sim time next is 1506600.0000, 
raw observation next is [28.0, 56.0, 1.0, 2.0, 0.3499784286171283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 537403.8140550866, 537403.8140550873, 169550.3510660284], 
processed observation next is [0.0, 0.43478260869565216, 0.5260663507109005, 0.56, 1.0, 1.0, 0.21684148026160036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14927883723752405, 0.14927883723752425, 0.25306022547168416], 
reward next is 0.7469, 
noisyNet noise sample is [array([1.2000134], dtype=float32), 1.0620041]. 
=============================================
[2019-04-27 23:44:52,344] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1131203e-29 1.0000000e+00 0.0000000e+00 1.4722337e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:44:52,367] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1276
[2019-04-27 23:44:52,385] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 66.0, 1.0, 2.0, 0.3449968494480293, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533414.5341833519, 533414.5341833519, 169336.5385145323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1533600.0000, 
sim time next is 1534200.0000, 
raw observation next is [25.71666666666667, 67.33333333333333, 1.0, 2.0, 0.3510396991243185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542337.4698520551, 542337.4698520551, 170054.130770924], 
processed observation next is [0.0, 0.782608695652174, 0.41785150078988953, 0.6733333333333333, 1.0, 1.0, 0.21812011942688975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1506492971811264, 0.1506492971811264, 0.25381213547899106], 
reward next is 0.7462, 
noisyNet noise sample is [array([-0.41005582], dtype=float32), 1.5099301]. 
=============================================
[2019-04-27 23:44:58,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3632180e-30 1.0000000e+00 0.0000000e+00 2.1252727e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:44:58,755] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6108
[2019-04-27 23:44:58,759] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 0.5082964139928445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710269.1412918553, 710269.141291856, 184906.389975793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [26.93333333333333, 84.66666666666666, 1.0, 2.0, 0.5090827636249359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711368.3166479467, 711368.3166479474, 185031.53148485], 
processed observation next is [1.0, 0.8260869565217391, 0.4755134281200631, 0.8466666666666666, 1.0, 1.0, 0.4085334501505251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976023101799852, 0.1976023101799854, 0.2761664649027612], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.847316], dtype=float32), 0.4232687]. 
=============================================
[2019-04-27 23:45:03,116] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3595072e-28 1.0000000e+00 0.0000000e+00 8.6731766e-25 5.5115400e-38], sum to 1.0000
[2019-04-27 23:45:03,121] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6117
[2019-04-27 23:45:03,132] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.5137183490508516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721897.8675752864, 721897.8675752857, 186292.2935799577], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1738800.0000, 
sim time next is 1739400.0000, 
raw observation next is [24.46666666666667, 94.00000000000001, 1.0, 2.0, 0.5072102117221868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713654.1920733452, 713654.1920733452, 185362.3277467391], 
processed observation next is [1.0, 0.13043478260869565, 0.3586097946287521, 0.9400000000000002, 1.0, 1.0, 0.406277363520707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1982372755759292, 0.1982372755759292, 0.2766601906667748], 
reward next is 0.7233, 
noisyNet noise sample is [array([-0.71235126], dtype=float32), 0.23926964]. 
=============================================
[2019-04-27 23:45:04,772] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3276032e-29 1.0000000e+00 0.0000000e+00 3.0477158e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:45:04,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2036
[2019-04-27 23:45:04,797] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 92.0, 1.0, 2.0, 0.575435989696104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811331.1134664734, 811331.1134664734, 197185.1258892216], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1751400.0000, 
sim time next is 1752000.0000, 
raw observation next is [24.76666666666667, 91.66666666666667, 1.0, 2.0, 0.6116227370694627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 861469.2550439711, 861469.2550439718, 203804.8178290089], 
processed observation next is [1.0, 0.2608695652173913, 0.3728278041074251, 0.9166666666666667, 1.0, 1.0, 0.5320755868306779, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23929701528999198, 0.23929701528999217, 0.3041862952671775], 
reward next is 0.6958, 
noisyNet noise sample is [array([-1.1301951], dtype=float32), -1.1888095]. 
=============================================
[2019-04-27 23:45:04,819] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[73.65616 ]
 [73.95678 ]
 [73.872635]
 [74.00138 ]
 [73.962654]], R is [[73.44656372]
 [73.41779327]
 [73.403862  ]
 [73.38217926]
 [73.37572479]].
[2019-04-27 23:45:14,777] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.953555e-29 1.000000e+00 0.000000e+00 9.054237e-28 0.000000e+00], sum to 1.0000
[2019-04-27 23:45:14,783] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2886
[2019-04-27 23:45:14,788] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 6.931184113241208, 6.9112, 168.9127597464964, 1497087.636675818, 1482910.23545745, 316008.8620621869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1958400.0000, 
sim time next is 1959000.0000, 
raw observation next is [25.43333333333334, 83.83333333333334, 1.0, 2.0, 0.9800699596118326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129402385257, 1403079.134111359, 1403079.134111359, 298025.0620835507], 
processed observation next is [1.0, 0.6956521739130435, 0.40442338072669864, 0.8383333333333334, 1.0, 1.0, 0.9759879031467863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294398652497983, 0.38974420391982195, 0.38974420391982195, 0.4448135254978369], 
reward next is 0.5552, 
noisyNet noise sample is [array([0.83571297], dtype=float32), 1.2915593]. 
=============================================
[2019-04-27 23:45:14,828] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.97855 ]
 [71.76311 ]
 [71.046104]
 [70.47025 ]
 [71.11146 ]], R is [[72.52797699]
 [72.23111725]
 [71.98391724]
 [71.26407623]
 [70.55143738]].
[2019-04-27 23:45:18,478] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1236528e-30 1.0000000e+00 0.0000000e+00 1.5225111e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 23:45:18,484] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6100
[2019-04-27 23:45:18,489] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.95, 97.66666666666667, 1.0, 2.0, 0.4643511869416615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655091.546806363, 655091.546806363, 179005.5196459925], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2004600.0000, 
sim time next is 2005200.0000, 
raw observation next is [23.9, 98.0, 1.0, 2.0, 0.4633728702384586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653961.8516538395, 653961.8516538395, 178893.4476831057], 
processed observation next is [0.0, 0.21739130434782608, 0.33175355450236965, 0.98, 1.0, 1.0, 0.353461289443926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1816560699038443, 0.1816560699038443, 0.2670051457956802], 
reward next is 0.7330, 
noisyNet noise sample is [array([1.433547], dtype=float32), 1.499895]. 
=============================================
[2019-04-27 23:45:24,497] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5749622e-30 1.0000000e+00 0.0000000e+00 1.3060182e-30 0.0000000e+00], sum to 1.0000
[2019-04-27 23:45:24,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0989
[2019-04-27 23:45:24,534] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.8, 79.0, 1.0, 2.0, 0.5377633480352927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751459.3925727669, 751459.3925727676, 189724.9930378456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2109600.0000, 
sim time next is 2110200.0000, 
raw observation next is [29.0, 78.16666666666667, 1.0, 2.0, 0.5395752796280108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753992.2469766083, 753992.2469766083, 190029.6961108312], 
processed observation next is [0.0, 0.43478260869565216, 0.5734597156398105, 0.7816666666666667, 1.0, 1.0, 0.44527142123856717, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20944229082683563, 0.20944229082683563, 0.2836264121057182], 
reward next is 0.7164, 
noisyNet noise sample is [array([-0.4629449], dtype=float32), -1.0526707]. 
=============================================
[2019-04-27 23:45:27,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6391482e-34 1.0000000e+00 0.0000000e+00 8.0966676e-32 0.0000000e+00], sum to 1.0000
[2019-04-27 23:45:27,518] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7236
[2019-04-27 23:45:27,524] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.25, 75.0, 1.0, 2.0, 0.5683418105705752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794205.0789347552, 794205.0789347552, 194996.6378245307], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2136600.0000, 
sim time next is 2137200.0000, 
raw observation next is [30.06666666666666, 75.66666666666666, 1.0, 2.0, 0.5661459106881557, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 791135.3693099535, 791135.3693099535, 194608.8102520434], 
processed observation next is [0.0, 0.7391304347826086, 0.6240126382306473, 0.7566666666666666, 1.0, 1.0, 0.47728422974476586, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21975982480832043, 0.21975982480832043, 0.2904609108239454], 
reward next is 0.7095, 
noisyNet noise sample is [array([-0.5014568], dtype=float32), -0.5329014]. 
=============================================
[2019-04-27 23:45:28,152] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.350766e-23 1.000000e+00 2.695932e-32 4.070320e-21 5.472338e-32], sum to 1.0000
[2019-04-27 23:45:28,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6091
[2019-04-27 23:45:28,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2125942.050392915 W.
[2019-04-27 23:45:28,159] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.3, 64.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.858077893562901, 6.9112, 168.9077254551632, 2125942.050392915, 1454215.086993875, 311350.9387501937], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2283600.0000, 
sim time next is 2284200.0000, 
raw observation next is [31.5, 63.5, 1.0, 2.0, 0.438572670547498, 1.0, 1.0, 0.438572670547498, 1.0, 1.0, 0.7574984714118668, 6.9112, 6.9112, 170.5573041426782, 1839483.572563337, 1839483.572563337, 374621.917583522], 
processed observation next is [1.0, 0.43478260869565216, 0.6919431279620853, 0.635, 1.0, 1.0, 0.3235815307801181, 1.0, 0.5, 0.3235815307801181, 1.0, 0.5, 0.7042664285510569, 0.0, 0.0, 0.8375144448122397, 0.5109676590453713, 0.5109676590453713, 0.5591371904231671], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5644878], dtype=float32), 0.2160603]. 
=============================================
[2019-04-27 23:45:33,470] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.7821643e-26 1.0000000e+00 3.3372643e-36 1.6223179e-23 1.1038260e-36], sum to 1.0000
[2019-04-27 23:45:33,478] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8144
[2019-04-27 23:45:33,482] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.13333333333333, 70.66666666666667, 1.0, 2.0, 0.5390484969802368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 753255.8699064497, 753255.8699064504, 189943.1877000185], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2223600.0000, 
sim time next is 2224200.0000, 
raw observation next is [30.96666666666667, 71.33333333333333, 1.0, 2.0, 0.548377951945325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766297.3710551489, 766297.3710551496, 191525.0280133002], 
processed observation next is [1.0, 0.7391304347826086, 0.6666666666666667, 0.7133333333333333, 1.0, 1.0, 0.45587705053653615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21286038084865247, 0.21286038084865266, 0.2858582507661197], 
reward next is 0.7141, 
noisyNet noise sample is [array([-0.8559075], dtype=float32), 0.76074123]. 
=============================================
[2019-04-27 23:45:33,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.2222914e-20 1.0000000e+00 1.0984382e-26 2.5474379e-17 5.6278042e-28], sum to 1.0000
[2019-04-27 23:45:33,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9261
[2019-04-27 23:45:33,701] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.93333333333333, 95.33333333333334, 1.0, 2.0, 0.6431525871204777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898790.4477961449, 898790.4477961449, 209050.4975111737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2172000.0000, 
sim time next is 2172600.0000, 
raw observation next is [24.9, 95.5, 1.0, 2.0, 0.6251284712388214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873591.8153861965, 873591.8153861965, 205511.5342816726], 
processed observation next is [1.0, 0.13043478260869565, 0.3791469194312796, 0.955, 1.0, 1.0, 0.5483475557094234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24266439316283236, 0.24266439316283236, 0.30673363325622777], 
reward next is 0.6933, 
noisyNet noise sample is [array([0.5130748], dtype=float32), 0.346117]. 
=============================================
[2019-04-27 23:45:34,647] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.0181201e-24 1.0000000e+00 2.4802547e-34 2.5957336e-22 6.8473378e-35], sum to 1.0000
[2019-04-27 23:45:34,656] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9915
[2019-04-27 23:45:34,661] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.35, 74.0, 1.0, 2.0, 0.5545794842767985, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 774966.4860106757, 774966.4860106757, 192590.4977421927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2226600.0000, 
sim time next is 2227200.0000, 
raw observation next is [30.2, 74.66666666666667, 1.0, 2.0, 0.554832280312203, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 775319.8708003875, 775319.8708003875, 192634.0995559795], 
processed observation next is [1.0, 0.782608695652174, 0.6303317535545023, 0.7466666666666667, 1.0, 1.0, 0.4636533497737385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2153666307778854, 0.2153666307778854, 0.2875135814268351], 
reward next is 0.7125, 
noisyNet noise sample is [array([-0.32407817], dtype=float32), 1.4310026]. 
=============================================
[2019-04-27 23:45:36,607] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4717956e-25 1.0000000e+00 7.1734302e-35 4.9639778e-22 1.8329878e-34], sum to 1.0000
[2019-04-27 23:45:36,633] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3320
[2019-04-27 23:45:36,638] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.05, 75.33333333333333, 1.0, 2.0, 0.5553783215004101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776083.1847961639, 776083.1847961632, 192728.4417279382], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2227800.0000, 
sim time next is 2228400.0000, 
raw observation next is [29.9, 76.0, 1.0, 2.0, 0.5557608008668448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776617.8551437154, 776617.8551437147, 192794.5471801717], 
processed observation next is [1.0, 0.8260869565217391, 0.6161137440758293, 0.76, 1.0, 1.0, 0.4647720492371623, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21572718198436538, 0.2157271819843652, 0.2877530554927936], 
reward next is 0.7122, 
noisyNet noise sample is [array([-1.3918009], dtype=float32), -2.011397]. 
=============================================
[2019-04-27 23:45:38,537] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.8906347e-17 1.0000000e+00 8.9221825e-24 3.0779015e-16 2.0557433e-24], sum to 1.0000
[2019-04-27 23:45:38,542] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-27 23:45:38,561] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.1, 64.5, 1.0, 2.0, 0.5722235126479306, 1.0, 1.0, 0.5722235126479306, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1599853.695616729, 1599853.695616729, 324218.877425603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2283000.0000, 
sim time next is 2283600.0000, 
raw observation next is [31.3, 64.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.850735781399794, 6.9112, 168.9077708792758, 2120730.074643993, 1454211.517754251, 311350.9363792589], 
processed observation next is [1.0, 0.43478260869565216, 0.6824644549763034, 0.64, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0939535781399794, 0.0, 0.8294144813293385, 0.5890916874011091, 0.40394764382062526, 0.4647028901182969], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4927883], dtype=float32), -0.5352331]. 
=============================================
[2019-04-27 23:45:46,826] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 23:45:46,838] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:45:46,838] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:45:46,840] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run29
[2019-04-27 23:45:46,892] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:45:46,893] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:45:46,899] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:45:46,902] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:45:46,902] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:45:46,902] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:45:46,903] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:45:46,904] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:45:46,905] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run29
[2019-04-27 23:45:46,952] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run29
[2019-04-27 23:45:46,952] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run29
[2019-04-27 23:45:46,996] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run29
[2019-04-27 23:46:15,312] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.10228227]
[2019-04-27 23:46:15,313] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.39556082666667, 76.2006929, 1.0, 2.0, 0.5783509589192164, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 808197.258651962, 808197.2586519613, 196782.5886029218]
[2019-04-27 23:46:15,314] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:46:15,318] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5218566e-27 1.0000000e+00 1.2290123e-38 9.2905243e-25 2.1067762e-38], sampled 0.529227213902059
[2019-04-27 23:46:16,677] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.10228227]
[2019-04-27 23:46:16,678] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.2, 86.0, 1.0, 2.0, 0.3641748797366817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558343.733877123, 558343.733877123, 171277.561066999]
[2019-04-27 23:46:16,679] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:46:16,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2646178e-26 1.0000000e+00 2.4655068e-37 6.2360575e-24 4.1548908e-37], sampled 0.2836137154654036
[2019-04-27 23:47:00,131] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.10228227]
[2019-04-27 23:47:00,132] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.715379675, 80.29465205833333, 1.0, 2.0, 0.5623216155825135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 785789.3018133574, 785789.3018133574, 193935.589495881]
[2019-04-27 23:47:00,133] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:47:00,136] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.61239375e-27 1.00000000e+00 1.33358156e-38 9.78598420e-25
 2.28568035e-38], sampled 0.8684371605308874
[2019-04-27 23:47:01,647] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.10228227]
[2019-04-27 23:47:01,648] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.73419325666667, 80.91701184166668, 1.0, 2.0, 0.5806970562404644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 811476.9864681754, 811476.9864681761, 197205.6183045121]
[2019-04-27 23:47:01,650] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:47:01,653] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9658034e-27 1.0000000e+00 1.7682458e-38 1.1693361e-24 3.0210196e-38], sampled 0.8922403137308104
[2019-04-27 23:47:06,795] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.10228227]
[2019-04-27 23:47:06,796] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.293150335, 91.21611054, 1.0, 2.0, 0.5862308006393409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819212.9270797564, 819212.9270797564, 198209.4213074565]
[2019-04-27 23:47:06,797] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:47:06,799] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7085190e-27 1.0000000e+00 1.4476002e-38 1.0310187e-24 2.4804011e-38], sampled 0.07957423684576759
[2019-04-27 23:47:28,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:47:28,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:47:28,969] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:47:29,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:47:29,525] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:47:30,537] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 700000, evaluation results [700000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:47:40,209] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.9502491e-31 1.0000000e+00 0.0000000e+00 4.7470292e-28 0.0000000e+00], sum to 1.0000
[2019-04-27 23:47:40,218] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8629
[2019-04-27 23:47:40,225] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2135508.95312254 W.
[2019-04-27 23:47:40,231] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.35, 74.5, 1.0, 2.0, 0.886048863420509, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991183317842113, 6.9112, 168.912480014579, 2135508.95312254, 2078766.194701217, 430767.4776222305], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2565000.0000, 
sim time next is 2565600.0000, 
raw observation next is [29.26666666666667, 75.0, 1.0, 2.0, 0.4930286319111655, 1.0, 1.0, 0.4930286319111655, 1.0, 2.0, 0.8514458972788329, 6.911199999999999, 6.9112, 170.5573041426782, 2068105.955315597, 2068105.955315598, 409508.5230782694], 
processed observation next is [1.0, 0.6956521739130435, 0.5860979462875199, 0.75, 1.0, 1.0, 0.38919112278453677, 1.0, 0.5, 0.38919112278453677, 1.0, 1.0, 0.8188364600961375, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5744738764765547, 0.574473876476555, 0.6112067508630886], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4893363], dtype=float32), -0.16892725]. 
=============================================
[2019-04-27 23:47:44,129] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3903221e-32 1.0000000e+00 0.0000000e+00 1.8860143e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:47:44,134] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7740
[2019-04-27 23:47:44,144] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 86.5, 1.0, 2.0, 0.4660689571230514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 655977.7309307858, 655977.7309307863, 179061.9022285829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2619000.0000, 
sim time next is 2619600.0000, 
raw observation next is [25.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4682794274503636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657664.9178185721, 657664.9178185728, 179205.7604229623], 
processed observation next is [0.0, 0.30434782608695654, 0.4154818325434437, 0.8566666666666667, 1.0, 1.0, 0.35937280415706463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18268469939404783, 0.18268469939404802, 0.26747128421337657], 
reward next is 0.7325, 
noisyNet noise sample is [array([0.5326081], dtype=float32), -1.0562811]. 
=============================================
[2019-04-27 23:47:45,013] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.9659037e-34 1.0000000e+00 0.0000000e+00 1.3685864e-29 0.0000000e+00], sum to 1.0000
[2019-04-27 23:47:45,019] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5918
[2019-04-27 23:47:45,023] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4770567381362009, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666730.5403852839, 666730.5403852839, 180095.5526592168], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2629800.0000, 
sim time next is 2630400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.47695265418527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666584.9123931888, 666584.9123931894, 180079.941457943], 
processed observation next is [0.0, 0.43478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.36982247492201203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18516247566477467, 0.18516247566477484, 0.2687760320267806], 
reward next is 0.7312, 
noisyNet noise sample is [array([0.06097808], dtype=float32), 0.23795463]. 
=============================================
[2019-04-27 23:48:00,150] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1161873e-26 1.0000000e+00 2.3143099e-37 7.2985932e-24 9.7708225e-36], sum to 1.0000
[2019-04-27 23:48:00,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4899
[2019-04-27 23:48:00,161] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.754685], dtype=float32), -1.6515105]. 
=============================================
[2019-04-27 23:48:06,181] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.7823775e-30 1.0000000e+00 0.0000000e+00 3.1744740e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:48:06,187] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-27 23:48:06,197] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.526962387468613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787175.6370659193, 787175.6370659193, 194221.3347785668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2898000.0000, 
sim time next is 2898600.0000, 
raw observation next is [22.83333333333334, 94.00000000000001, 1.0, 2.0, 0.5686072264897417, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 851744.2705237466, 851744.2705237466, 202139.1769588858], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115327, 0.9400000000000002, 1.0, 1.0, 0.4802496704695683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23659563070104073, 0.23659563070104073, 0.30170026411774], 
reward next is 0.6983, 
noisyNet noise sample is [array([-0.040736], dtype=float32), -0.7026151]. 
=============================================
[2019-04-27 23:48:09,162] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7076664e-28 1.0000000e+00 0.0000000e+00 8.7009749e-26 0.0000000e+00], sum to 1.0000
[2019-04-27 23:48:09,170] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8643
[2019-04-27 23:48:09,185] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.309942844654496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 490120.9203563386, 490120.9203563379, 166292.8501386649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2951400.0000, 
sim time next is 2952000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.309755161853096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 489824.2734502368, 489824.2734502368, 166271.065492433], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16837971307601926, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13606229818062135, 0.13606229818062135, 0.24816576939169105], 
reward next is 0.7518, 
noisyNet noise sample is [array([1.3320299], dtype=float32), -0.18362384]. 
=============================================
[2019-04-27 23:48:09,207] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[70.70048]
 [70.74924]
 [70.82997]
 [70.85753]
 [70.88472]], R is [[70.81256866]
 [70.85624695]
 [70.89941406]
 [70.94216156]
 [70.98486328]].
[2019-04-27 23:48:09,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8609846e-29 1.0000000e+00 0.0000000e+00 2.0709864e-27 0.0000000e+00], sum to 1.0000
[2019-04-27 23:48:09,803] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4945
[2019-04-27 23:48:09,815] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.590560166453826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 917938.325498177, 917938.3254981777, 210056.5198575559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2971800.0000, 
sim time next is 2972400.0000, 
raw observation next is [22.0, 90.0, 1.0, 2.0, 0.5545332603137373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864787.5775435641, 864787.5775435634, 203176.4031333039], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9, 1.0, 1.0, 0.4632930847153461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24021877153987892, 0.24021877153987872, 0.3032483628855282], 
reward next is 0.6968, 
noisyNet noise sample is [array([-0.7134703], dtype=float32), -0.71642435]. 
=============================================
[2019-04-27 23:48:33,718] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-27 23:48:33,720] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:48:33,721] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:48:33,721] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:48:33,722] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:48:33,723] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:48:33,724] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:48:33,724] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:48:33,723] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:48:33,727] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:48:33,728] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:48:33,739] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run30
[2019-04-27 23:48:33,740] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run30
[2019-04-27 23:48:33,771] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run30
[2019-04-27 23:48:33,795] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run30
[2019-04-27 23:48:33,817] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run30
[2019-04-27 23:48:38,795] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:48:38,796] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.66666666666667, 62.33333333333334, 1.0, 2.0, 0.2211788888449744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 368333.2155881139, 368333.2155881139, 157735.0063189171]
[2019-04-27 23:48:38,797] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:48:38,800] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.709795e-28 1.000000e+00 0.000000e+00 2.439886e-25 0.000000e+00], sampled 0.4247326670106366
[2019-04-27 23:48:57,248] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:48:57,249] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.20582957, 84.62835789833333, 1.0, 2.0, 0.5650422404444068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789592.5212772185, 789592.5212772185, 194413.4188540021]
[2019-04-27 23:48:57,250] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:48:57,253] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.0008604e-29 1.0000000e+00 0.0000000e+00 4.4004831e-26 0.0000000e+00], sampled 0.38365191162108647
[2019-04-27 23:49:09,873] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:49:09,875] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.41170766333333, 99.926676695, 1.0, 2.0, 0.2784319343826574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 450242.5443505978, 450242.5443505985, 163612.665737035]
[2019-04-27 23:49:09,878] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:49:09,881] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2246475e-28 1.0000000e+00 0.0000000e+00 1.1980431e-25 0.0000000e+00], sampled 0.4958230253329645
[2019-04-27 23:49:27,124] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:49:27,125] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 89.0, 1.0, 2.0, 0.6540673159982022, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914050.085566055, 914050.085566055, 211243.3200539501]
[2019-04-27 23:49:27,126] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:49:27,127] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0299042e-28 1.0000000e+00 0.0000000e+00 1.0261136e-25 0.0000000e+00], sampled 0.8000938781359229
[2019-04-27 23:49:54,279] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:49:54,280] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.43333333333334, 74.33333333333334, 1.0, 2.0, 0.6705100861589945, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.00597344219727, 6.9112, 168.9123160624007, 1833862.77154405, 1766627.484728513, 378376.007383268]
[2019-04-27 23:49:54,281] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:49:54,283] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3353684e-23 1.0000000e+00 4.0637945e-33 3.8889969e-21 7.1179394e-33], sampled 0.5499756114137675
[2019-04-27 23:49:54,285] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1833862.77154405 W.
[2019-04-27 23:49:59,785] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09540573]
[2019-04-27 23:49:59,788] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.48333333333333, 61.83333333333334, 1.0, 2.0, 0.3188662728867219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505697.8240916816, 505697.8240916822, 167478.863352754]
[2019-04-27 23:49:59,790] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:49:59,793] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0746303e-28 1.0000000e+00 0.0000000e+00 1.0655047e-25 0.0000000e+00], sampled 0.15111808724826714
[2019-04-27 23:50:01,353] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:50:01,367] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:50:01,472] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:50:01,514] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:50:01,529] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:50:02,544] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 725000, evaluation results [725000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:50:03,317] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8768876e-22 1.0000000e+00 2.7869998e-33 5.0395009e-20 9.2201263e-32], sum to 1.0000
[2019-04-27 23:50:03,325] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7690
[2019-04-27 23:50:03,334] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2621555.204485107 W.
[2019-04-27 23:50:03,338] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.9372368636372554, 1.0, 1.0, 0.9372368636372554, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2621555.204485107, 2621555.204485108, 492217.2478609543], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3509400.0000, 
sim time next is 3510000.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.8476794240340066, 1.0, 2.0, 0.8476794240340066, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2370815.488261508, 2370815.488261507, 443756.4945628137], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.63, 1.0, 1.0, 0.8164812337759116, 1.0, 1.0, 0.8164812337759116, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6585598578504188, 0.6585598578504186, 0.6623231262131548], 
reward next is 0.3377, 
noisyNet noise sample is [array([-0.20979661], dtype=float32), 1.869303]. 
=============================================
[2019-04-27 23:50:03,353] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[59.87503 ]
 [59.89481 ]
 [59.585323]
 [60.299767]
 [59.147232]], R is [[60.48504639]
 [59.88019562]
 [59.28139496]
 [58.97317505]
 [58.67311096]].
[2019-04-27 23:50:03,562] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5591703e-28 1.0000000e+00 0.0000000e+00 4.2412308e-25 0.0000000e+00], sum to 1.0000
[2019-04-27 23:50:03,575] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6089
[2019-04-27 23:50:03,579] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4209428935094727, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616340.1499663205, 616340.1499663205, 175742.8214674003], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3304200.0000, 
sim time next is 3304800.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.421087259009552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616551.6015760098, 616551.6015760098, 175763.073442786], 
processed observation next is [0.0, 0.2608695652173913, 0.38388625592417064, 0.83, 1.0, 1.0, 0.3025147698910265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17126433377111383, 0.17126433377111383, 0.26233294543699404], 
reward next is 0.7377, 
noisyNet noise sample is [array([0.11571214], dtype=float32), 0.3414411]. 
=============================================
[2019-04-27 23:50:07,902] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.8223767e-17 1.0000000e+00 1.0272660e-23 4.5142492e-14 3.6281909e-23], sum to 1.0000
[2019-04-27 23:50:07,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8129
[2019-04-27 23:50:07,917] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2260621.124047761 W.
[2019-04-27 23:50:07,924] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 70.16666666666667, 1.0, 2.0, 0.8083152953777315, 1.0, 2.0, 0.8083152953777315, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2260621.124047761, 2260621.124047761, 423943.5110628877], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3406200.0000, 
sim time next is 3406800.0000, 
raw observation next is [31.33333333333334, 70.33333333333334, 1.0, 2.0, 0.8539239655450241, 1.0, 2.0, 0.8539239655450241, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2388297.100519547, 2388297.100519548, 446981.1454669838], 
processed observation next is [1.0, 0.43478260869565216, 0.6840442338072673, 0.7033333333333335, 1.0, 1.0, 0.8240047777650893, 1.0, 1.0, 0.8240047777650893, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6634158612554296, 0.6634158612554301, 0.6671360380104235], 
reward next is 0.3329, 
noisyNet noise sample is [array([-1.0166179], dtype=float32), -1.857972]. 
=============================================
[2019-04-27 23:50:08,522] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7359141e-23 1.0000000e+00 2.7358898e-33 1.4474147e-20 5.4965837e-33], sum to 1.0000
[2019-04-27 23:50:08,528] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2756
[2019-04-27 23:50:08,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2199226.222133082 W.
[2019-04-27 23:50:08,542] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.7863827278090938, 1.0, 2.0, 0.7863827278090938, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2199226.222133082, 2199226.222133082, 413308.580983547], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3415800.0000, 
sim time next is 3416400.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.4954007714203101, 1.0, 2.0, 0.4954007714203101, 1.0, 1.0, 0.8603473084184773, 6.911200000000001, 6.9112, 170.5573041426782, 2078066.014146583, 2078066.014146582, 411981.5273094097], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.67, 1.0, 1.0, 0.3920491221931447, 1.0, 1.0, 0.3920491221931447, 1.0, 0.5, 0.8296918395347282, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.577240559485162, 0.5772405594851616, 0.6148978019543428], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.25265244], dtype=float32), 1.5261438]. 
=============================================
[2019-04-27 23:50:10,613] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.4603083e-21 1.0000000e+00 1.7519643e-30 1.7497367e-19 7.6159944e-30], sum to 1.0000
[2019-04-27 23:50:10,618] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4004
[2019-04-27 23:50:10,622] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 71.33333333333334, 1.0, 2.0, 0.5430648807823452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758870.2912250569, 758870.2912250574, 190619.3539496338], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3435600.0000, 
sim time next is 3436200.0000, 
raw observation next is [30.0, 72.0, 1.0, 2.0, 0.538849562144199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752977.7837121148, 752977.7837121154, 189907.4777205655], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.72, 1.0, 1.0, 0.4443970628243361, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20916049547558743, 0.2091604954755876, 0.28344399659785896], 
reward next is 0.7166, 
noisyNet noise sample is [array([1.7310226], dtype=float32), -1.393091]. 
=============================================
[2019-04-27 23:50:11,630] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.7717719e-22 1.0000000e+00 6.4832886e-31 7.1967588e-19 8.3255967e-31], sum to 1.0000
[2019-04-27 23:50:11,636] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1544
[2019-04-27 23:50:11,640] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.6487600454747424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 906630.0822340903, 906630.0822340903, 210170.5499065353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3470400.0000, 
sim time next is 3471000.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.6908483189325079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 965474.3956512511, 965474.3956512511, 218878.8560032139], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.6275280950994071, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2681873321253475, 0.2681873321253475, 0.3266848597062894], 
reward next is 0.6733, 
noisyNet noise sample is [array([-0.34166873], dtype=float32), -1.6199328]. 
=============================================
[2019-04-27 23:50:11,658] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[54.800858]
 [54.909084]
 [55.38618 ]
 [55.692547]
 [55.63575 ]], R is [[54.78325653]
 [54.92173767]
 [55.03832626]
 [55.15676117]
 [55.28876495]].
[2019-04-27 23:50:14,470] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [8.5110350e-18 1.0000000e+00 4.0511190e-25 4.2177286e-15 6.6929390e-25], sum to 1.0000
[2019-04-27 23:50:14,475] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3850
[2019-04-27 23:50:14,480] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2284706.62178305 W.
[2019-04-27 23:50:14,483] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 0.9926444350295525, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.002159392509757, 6.9112, 168.9124155673479, 2284706.62178305, 2220177.104812132, 461190.3460280257], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.9746260078459726, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.002703836651698, 6.9112, 168.9123385846409, 2259487.025036152, 2194571.291475269, 455715.3961767012], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.9694289251156297, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.00915038366516976, 0.0, 0.8294369108537857, 0.6276352847322645, 0.609603136520908, 0.680172233099554], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1291298], dtype=float32), -0.9089638]. 
=============================================
[2019-04-27 23:50:14,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[44.09102 ]
 [43.661423]
 [43.550297]
 [43.510834]
 [43.614044]], R is [[43.90468979]
 [43.46564484]
 [43.0309906 ]
 [42.6006813 ]
 [42.54782486]].
[2019-04-27 23:50:16,252] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0965606e-21 1.0000000e+00 1.2033636e-28 5.5309282e-19 1.1608714e-28], sum to 1.0000
[2019-04-27 23:50:16,262] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9870
[2019-04-27 23:50:16,270] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2227043.422883333 W.
[2019-04-27 23:50:16,274] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.5308798848590811, 1.0, 2.0, 0.5308798848590811, 1.0, 1.0, 0.9219627953395158, 6.9112, 6.9112, 170.5573041426782, 2227043.422883333, 2227043.422883333, 437157.5717819545], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3590400.0000, 
sim time next is 3591000.0000, 
raw observation next is [32.5, 65.0, 1.0, 2.0, 0.8602310492617768, 1.0, 2.0, 0.8602310492617768, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2405954.048113847, 2405954.048113846, 450263.8160728523], 
processed observation next is [1.0, 0.5652173913043478, 0.7393364928909952, 0.65, 1.0, 1.0, 0.8316036738093697, 1.0, 1.0, 0.8316036738093697, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6683205689205131, 0.6683205689205128, 0.6720355463773915], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8547721], dtype=float32), 0.78791296]. 
=============================================
[2019-04-27 23:50:16,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[49.69194 ]
 [50.131126]
 [49.264004]
 [48.793564]
 [48.301064]], R is [[49.28358078]
 [49.13827133]
 [49.1022377 ]
 [48.6112175 ]
 [48.12510681]].
[2019-04-27 23:50:26,745] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.7144593e-21 1.0000000e+00 3.2671656e-28 6.1487266e-17 1.1689508e-27], sum to 1.0000
[2019-04-27 23:50:26,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0244
[2019-04-27 23:50:26,773] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 0.4525832250070201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646969.9019658825, 646969.9019658825, 178374.034024248], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3720600.0000, 
sim time next is 3721200.0000, 
raw observation next is [27.0, 74.0, 1.0, 2.0, 0.4515345951764086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645471.6100356221, 645471.6100356228, 178220.9800824272], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.74, 1.0, 1.0, 0.3391983074414561, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17929766945433948, 0.17929766945433967, 0.26600146280959286], 
reward next is 0.7340, 
noisyNet noise sample is [array([-2.3723037], dtype=float32), 1.9374186]. 
=============================================
[2019-04-27 23:50:28,074] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.759259e-27 1.000000e+00 8.491620e-38 9.922626e-25 3.772880e-38], sum to 1.0000
[2019-04-27 23:50:28,089] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2784
[2019-04-27 23:50:28,099] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5348235321367516, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747349.9094147426, 747349.9094147426, 189232.2036326701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3813600.0000, 
sim time next is 3814200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5348095010040107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747330.2957404146, 747330.2957404152, 189229.8614114463], 
processed observation next is [0.0, 0.13043478260869565, 0.4786729857819906, 0.89, 1.0, 1.0, 0.43952951928194056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20759174881678186, 0.207591748816782, 0.2824326289723079], 
reward next is 0.7176, 
noisyNet noise sample is [array([0.11309855], dtype=float32), 0.74801487]. 
=============================================
[2019-04-27 23:50:29,592] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.1214203e-27 1.0000000e+00 5.5394898e-38 2.6879243e-23 1.8274704e-36], sum to 1.0000
[2019-04-27 23:50:29,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0576
[2019-04-27 23:50:29,611] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.0, 1.0, 2.0, 0.6164534225815279, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861463.8684486238, 861463.8684486238, 203852.0129842114], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3843600.0000, 
sim time next is 3844200.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.616474343870628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861493.1167962622, 861493.1167962622, 203856.0112491574], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5379208962296723, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23930364355451728, 0.23930364355451728, 0.3042627033569513], 
reward next is 0.6957, 
noisyNet noise sample is [array([1.395044], dtype=float32), -0.83029133]. 
=============================================
[2019-04-27 23:50:32,665] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.920582e-30 1.000000e+00 0.000000e+00 2.313821e-25 0.000000e+00], sum to 1.0000
[2019-04-27 23:50:32,675] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7698
[2019-04-27 23:50:32,680] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.5794535150899367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 809738.5762904576, 809738.5762904576, 196980.0352166448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3891600.0000, 
sim time next is 3892200.0000, 
raw observation next is [27.91666666666667, 89.5, 1.0, 2.0, 0.5808031180806256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811625.2559050809, 811625.2559050809, 197223.4057942195], 
processed observation next is [0.0, 0.043478260869565216, 0.5221169036334916, 0.895, 1.0, 1.0, 0.49494351575978984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22545145997363358, 0.22545145997363358, 0.2943632922301783], 
reward next is 0.7056, 
noisyNet noise sample is [array([0.37403905], dtype=float32), 1.7616781]. 
=============================================
[2019-04-27 23:50:32,820] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.1769317e-28 1.0000000e+00 9.8854266e-37 9.8835367e-23 0.0000000e+00], sum to 1.0000
[2019-04-27 23:50:32,829] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5279
[2019-04-27 23:50:32,835] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666666, 90.66666666666666, 1.0, 2.0, 0.5728501985777337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 800507.5079413905, 800507.5079413912, 195796.6540469634], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3901200.0000, 
sim time next is 3901800.0000, 
raw observation next is [27.83333333333334, 89.83333333333333, 1.0, 2.0, 0.574327543093026, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802572.7462452691, 802572.7462452691, 196060.3594296371], 
processed observation next is [0.0, 0.13043478260869565, 0.5181674565560824, 0.8983333333333333, 1.0, 1.0, 0.4871416181843687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2229368739570192, 0.2229368739570192, 0.2926274021337867], 
reward next is 0.7074, 
noisyNet noise sample is [array([1.2076465], dtype=float32), -2.5515177]. 
=============================================
[2019-04-27 23:50:41,360] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6164865e-18 1.0000000e+00 6.6292652e-24 7.8985331e-15 3.2262783e-23], sum to 1.0000
[2019-04-27 23:50:41,365] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5731
[2019-04-27 23:50:41,373] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2576152.436824378 W.
[2019-04-27 23:50:41,378] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666667, 67.33333333333333, 1.0, 2.0, 0.6140143709695762, 1.0, 2.0, 0.6140143709695762, 1.0, 2.0, 1.03, 6.949713288347835, 6.9112, 170.5573041426782, 2576152.436824378, 2548563.801760417, 493214.3036372775], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5901100270836144, 1.0, 2.0, 0.5901100270836144, 1.0, 2.0, 1.023781212750239, 6.911199999999999, 6.9112, 170.5573041426782, 2475760.254338252, 2475760.254338253, 482823.2433064916], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5061566591368848, 1.0, 1.0, 0.5061566591368848, 1.0, 1.0, 1.029001478963706, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6877111817606255, 0.6877111817606257, 0.7206317064275993], 
reward next is 0.2794, 
noisyNet noise sample is [array([-2.724938], dtype=float32), -1.3924102]. 
=============================================
[2019-04-27 23:50:41,397] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[41.93423 ]
 [41.634754]
 [41.78843 ]
 [41.525143]
 [41.15698 ]], R is [[42.31632996]
 [41.96445847]
 [41.70682526]
 [41.55347443]
 [41.13793945]].
[2019-04-27 23:50:44,346] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.3681871e-22 1.0000000e+00 3.0138691e-32 1.4253722e-19 8.4304141e-33], sum to 1.0000
[2019-04-27 23:50:44,353] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7972
[2019-04-27 23:50:44,360] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3167785.24801278 W.
[2019-04-27 23:50:44,364] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.83333333333334, 67.0, 1.0, 2.0, 0.8685175621062006, 1.0, 2.0, 0.754848820567363, 1.0, 2.0, 1.03, 7.005111023372939, 6.9112, 170.5573041426782, 3167785.24801278, 3100512.961914552, 579842.4557350329], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4119000.0000, 
sim time next is 4119600.0000, 
raw observation next is [34.66666666666667, 67.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.889350739777005, 6.9112, 170.5573041426782, 3610835.160351059, 2910145.996820624, 548034.8397224655], 
processed observation next is [1.0, 0.6956521739130435, 0.8420221169036337, 0.67, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.09781507397770053, 0.0, 0.8375144448122397, 1.003009766764183, 0.8083738880057288, 0.8179624473469634], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.765215], dtype=float32), 0.99972636]. 
=============================================
[2019-04-27 23:50:54,141] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 23:50:54,143] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:50:54,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:50:54,145] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:50:54,148] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:50:54,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:50:54,147] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:50:54,153] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:50:54,154] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:50:54,154] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:50:54,154] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:50:54,174] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run31
[2019-04-27 23:50:54,195] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run31
[2019-04-27 23:50:54,196] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run31
[2019-04-27 23:50:54,241] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run31
[2019-04-27 23:50:54,242] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run31
[2019-04-27 23:51:01,284] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:01,285] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.76146642833334, 42.86672614166667, 1.0, 2.0, 0.3261823170313695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 530481.7413186298, 530481.7413186298, 169296.3663629906]
[2019-04-27 23:51:01,285] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:51:01,287] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8106501e-21 1.0000000e+00 6.9761734e-31 2.5163670e-17 1.6742378e-30], sampled 0.5295951113045664
[2019-04-27 23:51:04,171] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:04,172] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.37848423, 72.496716765, 1.0, 2.0, 0.2841886225433156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 461146.1247342205, 461146.1247342199, 164324.21900436]
[2019-04-27 23:51:04,173] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:51:04,176] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.1095674e-22 1.0000000e+00 2.1645005e-31 1.3183255e-17 5.2691873e-31], sampled 0.7350258207153598
[2019-04-27 23:51:38,257] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:38,258] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.10288782, 85.82006594666667, 1.0, 2.0, 0.5719112524022001, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799194.9200490254, 799194.9200490254, 195628.7434544888]
[2019-04-27 23:51:38,259] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:51:38,262] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.8389501e-19 1.0000000e+00 3.0217625e-27 3.2561509e-15 6.6165223e-27], sampled 0.34175734246110334
[2019-04-27 23:51:44,148] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:44,148] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.4, 42.83333333333334, 1.0, 2.0, 0.9683984086547283, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005991668392607, 6.9112, 168.9123159346584, 2250770.648930017, 2183522.431924094, 453731.6205869241]
[2019-04-27 23:51:44,149] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:51:44,152] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.7034652e-18 1.0000000e+00 1.3567687e-25 1.9872294e-14 2.7836429e-25], sampled 0.2561053274485544
[2019-04-27 23:51:44,154] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2250770.648930017 W.
[2019-04-27 23:51:49,776] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:49,777] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.3, 79.0, 1.0, 2.0, 0.6106729901932588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 853382.7483190882, 853382.7483190882, 202750.5025294446]
[2019-04-27 23:51:49,778] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:51:49,781] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.9731860e-22 1.0000000e+00 2.5045318e-31 1.4645456e-17 6.0976926e-31], sampled 0.8484904262476465
[2019-04-27 23:51:51,830] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:51,831] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.9, 74.66666666666667, 1.0, 2.0, 0.5694032283946452, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795688.8676632694, 795688.8676632694, 195183.4805911155]
[2019-04-27 23:51:51,832] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:51:51,834] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7331547e-20 1.0000000e+00 1.8512276e-29 1.6786944e-16 4.2830392e-29], sampled 0.7777615829986334
[2019-04-27 23:51:53,744] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:51:53,745] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.51163205333334, 85.31290671000001, 1.0, 2.0, 0.5476853295559255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 765329.1594012314, 765329.1594012314, 191403.7026162841]
[2019-04-27 23:51:53,746] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:51:53,751] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2706407e-20 1.0000000e+00 2.7363546e-29 2.1074991e-16 6.3017325e-29], sampled 0.7732712150730332
[2019-04-27 23:52:03,932] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:52:03,933] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.33333333333333, 85.0, 1.0, 2.0, 0.5317478955359676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743050.5834638949, 743050.5834638949, 188719.5245679506]
[2019-04-27 23:52:03,934] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:52:03,939] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5338098e-20 1.0000000e+00 1.5480927e-29 1.5270291e-16 3.5889211e-29], sampled 0.6767969853722532
[2019-04-27 23:52:21,053] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.09373453]
[2019-04-27 23:52:21,054] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.50792493833333, 59.725255555, 1.0, 2.0, 0.7715287684061815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1078284.18726737, 1078284.187267371, 237038.925626219]
[2019-04-27 23:52:21,054] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:52:21,058] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1927004e-20 1.0000000e+00 4.5164800e-29 2.6910835e-16 1.0325162e-28], sampled 0.4118816220518362
[2019-04-27 23:52:31,487] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:52:31,973] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:52:32,520] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:52:32,598] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:52:32,969] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:52:33,985] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 750000, evaluation results [750000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:53:27,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.5876087e-27 1.0000000e+00 0.0000000e+00 5.1516316e-21 2.9070598e-38], sum to 1.0000
[2019-04-27 23:53:27,924] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1980
[2019-04-27 23:53:27,961] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.00000000000001, 1.0, 2.0, 0.6144071240269208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 858603.1092581487, 858603.1092581487, 203461.6278172622], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4396200.0000, 
sim time next is 4396800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6172120494033209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 862524.4432381731, 862524.4432381738, 203997.1329587804], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5388096980762902, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23959012312171477, 0.23959012312171496, 0.3044733327742991], 
reward next is 0.6955, 
noisyNet noise sample is [array([0.25665787], dtype=float32), 0.018335495]. 
=============================================
[2019-04-27 23:53:45,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.52884704e-26 1.00000000e+00 2.91389898e-38 2.66165315e-21
 1.31746315e-36], sum to 1.0000
[2019-04-27 23:53:45,423] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2325
[2019-04-27 23:53:45,446] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 94.0, 1.0, 2.0, 0.4558572356147115, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650958.3237914713, 650958.3237914713, 178766.5752197482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4656000.0000, 
sim time next is 4656600.0000, 
raw observation next is [24.25, 94.0, 1.0, 2.0, 0.4596692198115073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 654075.6781767484, 654075.6781767484, 179033.3958349618], 
processed observation next is [1.0, 0.9130434782608695, 0.3483412322274882, 0.94, 1.0, 1.0, 0.3489990600138642, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18168768838243013, 0.18168768838243013, 0.2672140236342713], 
reward next is 0.7328, 
noisyNet noise sample is [array([-0.5260928], dtype=float32), -1.077073]. 
=============================================
[2019-04-27 23:54:07,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.2553447e-14 1.0000000e+00 2.6066114e-20 7.7391024e-11 1.1040035e-20], sum to 1.0000
[2019-04-27 23:54:07,253] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3968
[2019-04-27 23:54:07,338] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.117261241434012, 6.9112, 168.9112810767593, 1600040.53305901, 1453855.046321287, 311356.4020030585], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4608000.0000, 
sim time next is 4608600.0000, 
raw observation next is [30.33333333333333, 82.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.078333960503885, 6.9112, 168.9062133674497, 2282285.200171171, 1454313.253573855, 311334.3435555479], 
processed observation next is [1.0, 0.34782608695652173, 0.6366508688783569, 0.825, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.11671339605038851, 0.0, 0.8294068332332403, 0.6339681111586587, 0.40397590377051523, 0.46467812470977293], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.31663764], dtype=float32), 2.724777]. 
=============================================
[2019-04-27 23:54:25,022] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8705825e-23 1.0000000e+00 2.5950358e-33 6.6229599e-20 2.2653060e-32], sum to 1.0000
[2019-04-27 23:54:25,024] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1956
[2019-04-27 23:54:25,033] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4857725200252103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 678785.2714381652, 678785.2714381646, 181396.6560870234], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4756800.0000, 
sim time next is 4757400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.485108267694878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677856.794554996, 677856.794554996, 181295.4846495466], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.37964851529503374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18829355404305442, 0.18829355404305442, 0.2705902755963382], 
reward next is 0.7294, 
noisyNet noise sample is [array([-1.5804626], dtype=float32), 0.05449209]. 
=============================================
[2019-04-27 23:54:25,759] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.5739549e-19 1.0000000e+00 5.7342786e-26 2.2031714e-15 1.4814810e-25], sum to 1.0000
[2019-04-27 23:54:25,768] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4972
[2019-04-27 23:54:25,769] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2201503.064271207 W.
[2019-04-27 23:54:25,774] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.787196028047067, 1.0, 2.0, 0.787196028047067, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2201503.064271207, 2201503.064271207, 413683.989043854], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4709400.0000, 
sim time next is 4710000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5972852855578793, 1.0, 2.0, 0.5972852855578793, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1669977.36036106, 1669977.36036106, 333252.3150782505], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.5148015488649148, 1.0, 1.0, 0.5148015488649148, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4638826001002944, 0.4638826001002944, 0.4973915150421649], 
reward next is 0.5026, 
noisyNet noise sample is [array([1.3064654], dtype=float32), -1.5306633]. 
=============================================
[2019-04-27 23:54:25,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[48.8901  ]
 [47.90915 ]
 [47.272274]
 [46.719276]
 [46.964058]], R is [[49.32463455]
 [49.21395111]
 [49.13785553]
 [49.06192398]
 [48.9809494 ]].
[2019-04-27 23:54:26,736] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.5504748e-21 1.0000000e+00 9.4469668e-29 3.1964131e-15 7.8446678e-29], sum to 1.0000
[2019-04-27 23:54:26,741] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4435
[2019-04-27 23:54:26,746] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 79.83333333333334, 1.0, 2.0, 0.5919076776101134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827149.0195213107, 827149.0195213107, 199241.0098672827], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4774200.0000, 
sim time next is 4774800.0000, 
raw observation next is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5713417938296217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798398.853453197, 798398.853453197, 195523.5098540154], 
processed observation next is [1.0, 0.2608695652173913, 0.4944707740916275, 0.8066666666666668, 1.0, 1.0, 0.48354432991520685, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22177745929255474, 0.22177745929255474, 0.29182613411047076], 
reward next is 0.7082, 
noisyNet noise sample is [array([1.2537472], dtype=float32), -0.89501244]. 
=============================================
[2019-04-27 23:54:36,990] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6289597e-19 1.0000000e+00 5.6608551e-28 8.6470846e-16 3.3739900e-28], sum to 1.0000
[2019-04-27 23:54:36,999] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5419
[2019-04-27 23:54:37,003] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5091368395051649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 711443.9050444425, 711443.9050444419, 185040.0746899869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4927800.0000, 
sim time next is 4928400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5099145363569004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712530.9865658786, 712530.9865658792, 185164.0716666581], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.40953558597216916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1979252740460774, 0.19792527404607754, 0.276364286069639], 
reward next is 0.7236, 
noisyNet noise sample is [array([0.13223502], dtype=float32), 0.3130354]. 
=============================================
[2019-04-27 23:54:38,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1868477e-19 1.0000000e+00 2.2816870e-26 1.7934426e-15 4.1597033e-26], sum to 1.0000
[2019-04-27 23:54:38,108] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3640
[2019-04-27 23:54:38,121] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.66666666666667, 1.0, 2.0, 0.6381011400471721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 891728.2070959712, 891728.2070959718, 208050.6939751662], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.6465221921074618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903501.3935972198, 903501.3935972198, 209724.8204450519], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.865, 1.0, 1.0, 0.5741231230210383, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25097260933256105, 0.25097260933256105, 0.31302212006724167], 
reward next is 0.6870, 
noisyNet noise sample is [array([1.3016214], dtype=float32), -1.3838955]. 
=============================================
[2019-04-27 23:54:39,618] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9461530e-24 1.0000000e+00 9.7117479e-34 1.8706811e-19 5.8471500e-35], sum to 1.0000
[2019-04-27 23:54:39,625] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4713
[2019-04-27 23:54:39,639] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5223618816257681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729930.3070669887, 729930.3070669881, 187173.6109945841], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5085000.0000, 
sim time next is 5085600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5224365732243104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 730034.7143716051, 730034.7143716051, 187185.8046662234], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42462237737868713, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2027874206587792, 0.2027874206587792, 0.2793817980092887], 
reward next is 0.7206, 
noisyNet noise sample is [array([-1.8799298], dtype=float32), 0.023917448]. 
=============================================
[2019-04-27 23:54:43,975] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.7246202e-26 1.0000000e+00 1.3584086e-35 7.4403455e-20 7.9541744e-36], sum to 1.0000
[2019-04-27 23:54:43,983] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7246
[2019-04-27 23:54:43,987] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2111063.298728571 W.
[2019-04-27 23:54:43,995] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.83333333333334, 63.0, 1.0, 2.0, 0.8685837062918939, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.980989373935348, 6.9112, 168.9125403864016, 2111063.298728571, 2061552.436866308, 426381.3832551236], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4979400.0000, 
sim time next is 4980000.0000, 
raw observation next is [30.86666666666667, 63.00000000000001, 1.0, 2.0, 0.8458485915513011, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.978538970210844, 6.9112, 168.9124987986066, 2079242.228436756, 2031469.774792435, 420448.6670926048], 
processed observation next is [1.0, 0.6521739130434783, 0.6619273301737759, 0.6300000000000001, 1.0, 1.0, 0.8142754115075916, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.0067338970210843565, 0.0, 0.8294376975777035, 0.5775672856768767, 0.5642971596645653, 0.6275353240188132], 
reward next is 0.0358, 
noisyNet noise sample is [array([0.5801679], dtype=float32), 0.9163011]. 
=============================================
[2019-04-27 23:54:44,011] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[65.003105]
 [64.36032 ]
 [64.0548  ]
 [63.157734]
 [63.12682 ]], R is [[65.13561249]
 [64.49892426]
 [64.20604706]
 [63.95664978]
 [63.70349503]].
[2019-04-27 23:54:44,815] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6773280e-19 1.0000000e+00 1.1452922e-27 2.7438582e-15 1.5104397e-27], sum to 1.0000
[2019-04-27 23:54:44,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5525
[2019-04-27 23:54:44,825] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2009636.943381746 W.
[2019-04-27 23:54:44,832] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 68.0, 1.0, 2.0, 0.4791029026083671, 1.0, 2.0, 0.4791029026083671, 1.0, 1.0, 0.8219942802416724, 6.9112, 6.9112, 170.5573041426782, 2009636.943381746, 2009636.943381746, 399257.0006066743], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4962600.0000, 
sim time next is 4963200.0000, 
raw observation next is [30.0, 67.33333333333334, 1.0, 2.0, 0.6909193238906065, 1.0, 2.0, 0.6909193238906065, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1932009.085643575, 1932009.085643575, 370256.4227798901], 
processed observation next is [1.0, 0.43478260869565216, 0.6208530805687204, 0.6733333333333335, 1.0, 1.0, 0.6276136432416946, 1.0, 1.0, 0.6276136432416946, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5366691904565486, 0.5366691904565486, 0.5526215265371495], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7139191], dtype=float32), -1.4573267]. 
=============================================
[2019-04-27 23:54:45,607] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3602384e-25 1.0000000e+00 2.1077199e-35 2.0370114e-21 1.9117501e-35], sum to 1.0000
[2019-04-27 23:54:45,627] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2489
[2019-04-27 23:54:45,633] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5234912123965862, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 731508.9379428031, 731508.9379428037, 187358.1546805001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5173800.0000, 
sim time next is 5174400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5225737880560548, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 730226.5195145251, 730226.5195145257, 187208.2098061929], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42478769645307807, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20284069986514586, 0.20284069986514602, 0.2794152385167058], 
reward next is 0.7206, 
noisyNet noise sample is [array([0.4221977], dtype=float32), -1.3124979]. 
=============================================
[2019-04-27 23:54:48,370] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.8091758e-25 1.0000000e+00 2.2837752e-35 4.2523083e-19 1.5846600e-36], sum to 1.0000
[2019-04-27 23:54:48,387] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4646
[2019-04-27 23:54:48,393] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5011830809076951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 700326.0326932248, 700326.0326932248, 183781.6750144393], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5033400.0000, 
sim time next is 5034000.0000, 
raw observation next is [26.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5031349410749865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703054.358707691, 703054.3587076915, 184088.7353093144], 
processed observation next is [0.0, 0.2608695652173913, 0.44707740916271754, 0.8733333333333334, 1.0, 1.0, 0.4013673988855258, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19529287741880305, 0.19529287741880322, 0.2747593064318125], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.11510349], dtype=float32), 0.27866665]. 
=============================================
[2019-04-27 23:54:48,419] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.815636]
 [72.84812 ]
 [72.8399  ]
 [72.85747 ]
 [72.85548 ]], R is [[72.7943573 ]
 [72.79211426]
 [72.79022217]
 [72.78837585]
 [72.78664398]].
[2019-04-27 23:54:51,689] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.1993549e-26 1.0000000e+00 8.1757868e-38 1.8834122e-19 5.8123494e-34], sum to 1.0000
[2019-04-27 23:54:51,695] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9310
[2019-04-27 23:54:51,699] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 77.33333333333333, 1.0, 2.0, 0.5201964524523757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726903.3764013159, 726903.3764013159, 186820.9851021372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5125200.0000, 
sim time next is 5125800.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5220508323514502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729495.5083918261, 729495.5083918266, 187123.1724870216], 
processed observation next is [0.0, 0.30434782608695654, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.4241576293390966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20263764121995168, 0.20263764121995184, 0.27928831714480834], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.4199414], dtype=float32), -0.79025006]. 
=============================================
[2019-04-27 23:54:54,029] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8512498e-28 1.0000000e+00 0.0000000e+00 2.7269741e-23 1.6614808e-38], sum to 1.0000
[2019-04-27 23:54:54,034] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4857
[2019-04-27 23:54:54,039] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.55082529293436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769718.4977741354, 769718.4977741349, 191942.8620324664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5154600.0000, 
sim time next is 5155200.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5516185973084917, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770827.45667694, 770827.4566769394, 192079.1430813605], 
processed observation next is [0.0, 0.6956521739130435, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45978144254035147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21411873796581665, 0.21411873796581649, 0.2866852881811351], 
reward next is 0.7133, 
noisyNet noise sample is [array([-0.04024587], dtype=float32), -0.1407792]. 
=============================================
[2019-04-27 23:54:57,808] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-27 23:54:57,810] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:54:57,810] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:54:57,811] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:54:57,811] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:54:57,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:54:57,812] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:54:57,812] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:54:57,813] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:54:57,813] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:54:57,814] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:54:57,829] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run32
[2019-04-27 23:54:57,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run32
[2019-04-27 23:54:57,887] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run32
[2019-04-27 23:54:57,913] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run32
[2019-04-27 23:54:57,942] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run32
[2019-04-27 23:55:08,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:55:08,057] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.28333333333333, 74.83333333333334, 1.0, 2.0, 0.2638325361547078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 433725.1113663855, 433725.1113663848, 162303.5312655763]
[2019-04-27 23:55:08,058] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:55:08,062] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1856189e-23 1.0000000e+00 5.8127039e-34 3.4742490e-19 2.4599690e-33], sampled 0.7035202207477568
[2019-04-27 23:55:14,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:55:14,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.8, 77.0, 1.0, 2.0, 0.2516217638547551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 414365.1978151435, 414365.1978151435, 161055.9001920587]
[2019-04-27 23:55:14,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:55:14,118] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.7509959e-22 1.0000000e+00 2.9063369e-32 3.0709436e-18 1.1429200e-31], sampled 0.33220917837214226
[2019-04-27 23:55:26,948] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:55:26,949] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.94348471, 59.71357087, 1.0, 2.0, 0.8708062303904693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1217113.320524872, 1217113.320524872, 262037.986382054]
[2019-04-27 23:55:26,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:55:26,953] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7304940e-22 1.0000000e+00 5.5469921e-32 4.3927695e-18 2.1548286e-31], sampled 0.29963684285282144
[2019-04-27 23:55:33,836] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:55:33,837] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.3, 73.0, 1.0, 2.0, 0.6942150675371561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 970181.6435739237, 970181.6435739244, 219601.16089574]
[2019-04-27 23:55:33,837] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:55:33,838] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1528030e-23 1.0000000e+00 5.5799670e-34 3.3955164e-19 2.3634199e-33], sampled 0.41190943109212785
[2019-04-27 23:55:55,174] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:55:55,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.8, 75.0, 1.0, 2.0, 0.9711614106048481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564997352, 1357467.981560711, 1357467.98156071, 290261.3609000359]
[2019-04-27 23:55:55,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:55:55,178] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0654505e-20 1.0000000e+00 1.1378227e-29 8.5100977e-17 3.9990295e-29], sampled 0.05252329280310697
[2019-04-27 23:56:12,832] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:56:12,834] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.3, 88.0, 1.0, 2.0, 0.544985603880378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761555.2415108259, 761555.2415108259, 190944.2960489046]
[2019-04-27 23:56:12,835] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:56:12,837] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6358663e-22 1.0000000e+00 2.6316732e-32 2.9089976e-18 1.0368998e-31], sampled 0.4016574271603127
[2019-04-27 23:56:19,483] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:56:19,505] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.35003265, 77.99924298, 1.0, 2.0, 0.8917182408743876, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005987257278986, 6.9112, 168.9123159693509, 2143444.397879078, 2076199.310243298, 431800.2871424747]
[2019-04-27 23:56:19,507] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-27 23:56:19,509] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.9763678e-18 1.0000000e+00 1.7105947e-25 1.8007649e-14 5.0210859e-25], sampled 0.08858790273467332
[2019-04-27 23:56:19,510] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2143444.397879078 W.
[2019-04-27 23:56:21,420] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:56:21,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.46511674333333, 81.60705068, 1.0, 2.0, 0.3783506355246449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 582350.2190923807, 582350.2190923813, 173416.875158777]
[2019-04-27 23:56:21,421] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:56:21,423] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9755134e-22 1.0000000e+00 9.5663717e-32 5.9780128e-18 3.6764656e-31], sampled 0.7661885565946629
[2019-04-27 23:56:24,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08982473]
[2019-04-27 23:56:24,150] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.3, 93.66666666666666, 1.0, 2.0, 0.4268336968904757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 630904.3337613522, 630904.3337613528, 177301.5789666324]
[2019-04-27 23:56:24,151] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:56:24,153] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4521410e-23 1.0000000e+00 7.8077777e-34 4.0969465e-19 3.2854171e-33], sampled 0.716380205150528
[2019-04-27 23:56:36,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-27 23:56:36,527] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-27 23:56:36,594] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-27 23:56:36,677] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-27 23:56:36,704] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-27 23:56:37,718] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 775000, evaluation results [775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-27 23:56:38,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.8926276e-15 1.0000000e+00 3.2230184e-23 3.0369211e-14 1.8607295e-21], sum to 1.0000
[2019-04-27 23:56:38,747] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2221
[2019-04-27 23:56:38,754] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2136603.006379012 W.
[2019-04-27 23:56:38,758] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5093418007826446, 1.0, 2.0, 0.5093418007826446, 1.0, 1.0, 0.8845582660519915, 6.9112, 6.9112, 170.5573041426782, 2136603.006379012, 2136603.006379012, 421666.6482527347], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5230200.0000, 
sim time next is 5230800.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5173502098678929, 1.0, 2.0, 0.5173502098678929, 1.0, 2.0, 0.8984662242117132, 6.9112, 6.9112, 170.5573041426782, 2170230.964256934, 2170230.964256934, 427351.9221209935], 
processed observation next is [1.0, 0.5652173913043478, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4184942287564975, 1.0, 1.0, 0.4184942287564975, 1.0, 1.0, 0.8761783222094063, 0.0, 0.0, 0.8375144448122397, 0.6028419345158149, 0.6028419345158149, 0.6378386897328261], 
reward next is 0.3622, 
noisyNet noise sample is [array([-1.2034898], dtype=float32), 1.8609133]. 
=============================================
[2019-04-27 23:56:45,518] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.2221615e-18 1.0000000e+00 2.6648962e-27 2.7570971e-13 3.5163432e-26], sum to 1.0000
[2019-04-27 23:56:45,524] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2634
[2019-04-27 23:56:45,530] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.9, 58.66666666666667, 1.0, 2.0, 0.5801915235340569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810770.275935575, 810770.275935575, 197116.7241273046], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5334000.0000, 
sim time next is 5334600.0000, 
raw observation next is [34.65, 59.83333333333334, 1.0, 2.0, 0.5850136715667648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817511.4272832045, 817511.4272832045, 197989.9596420763], 
processed observation next is [1.0, 0.7391304347826086, 0.8412322274881516, 0.5983333333333334, 1.0, 1.0, 0.5000164717671864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2270865075786679, 0.2270865075786679, 0.29550740245086016], 
reward next is 0.7045, 
noisyNet noise sample is [array([0.19920024], dtype=float32), 0.3697456]. 
=============================================
[2019-04-27 23:56:51,146] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.7940876e-12 9.9999952e-01 1.3659029e-17 4.6817607e-07 8.8852524e-18], sum to 1.0000
[2019-04-27 23:56:51,177] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9780
[2019-04-27 23:56:51,183] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 85.66666666666667, 1.0, 2.0, 0.5906428527188955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825380.8296423217, 825380.8296423217, 199015.6882775268], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5443800.0000, 
sim time next is 5444400.0000, 
raw observation next is [28.9, 86.33333333333334, 1.0, 2.0, 0.5921734180530719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827520.5176142306, 827520.5176142306, 199296.8405240625], 
processed observation next is [1.0, 0.0, 0.5687203791469194, 0.8633333333333334, 1.0, 1.0, 0.5086426723530987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2298668104483974, 0.2298668104483974, 0.2974579709314366], 
reward next is 0.7025, 
noisyNet noise sample is [array([0.18597145], dtype=float32), 1.4524828]. 
=============================================
[2019-04-27 23:56:56,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4633652e-17 9.9657345e-01 1.7931835e-23 3.4265739e-03 1.0068259e-22], sum to 1.0000
[2019-04-27 23:56:56,518] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4208
[2019-04-27 23:56:56,524] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 85.16666666666667, 1.0, 2.0, 0.5062960044907171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707472.9337201746, 707472.9337201746, 184588.4830706276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5638200.0000, 
sim time next is 5638800.0000, 
raw observation next is [26.90000000000001, 84.33333333333334, 1.0, 2.0, 0.5078105802265798, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709590.0336350915, 709590.0336350915, 184828.9231312728], 
processed observation next is [0.0, 0.2608695652173913, 0.4739336492891, 0.8433333333333334, 1.0, 1.0, 0.4070006990681684, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971083426764143, 0.1971083426764143, 0.275864064375034], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.62696177], dtype=float32), 0.7817663]. 
=============================================
[2019-04-27 23:57:03,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.0019920e-13 1.0266276e-03 2.3192349e-18 9.9897337e-01 9.3890955e-18], sum to 1.0000
[2019-04-27 23:57:03,142] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8561
[2019-04-27 23:57:03,148] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.6, 95.0, 1.0, 2.0, 0.3267056966171575, 1.0, 2.0, 0.3267056966171575, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 913129.2639370791, 913129.2639370791, 255238.5940400155], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5547600.0000, 
sim time next is 5548200.0000, 
raw observation next is [25.78333333333333, 94.16666666666667, 1.0, 2.0, 0.3606718081355594, 1.0, 2.0, 0.3606718081355594, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1008107.800780558, 1008107.800780558, 262671.4345162096], 
processed observation next is [1.0, 0.21739130434782608, 0.4210110584518167, 0.9416666666666668, 1.0, 1.0, 0.22972507004284262, 1.0, 1.0, 0.22972507004284262, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.28002994466126613, 0.28002994466126613, 0.39204691718837253], 
reward next is 0.6080, 
noisyNet noise sample is [array([-0.749328], dtype=float32), -0.2801313]. 
=============================================
[2019-04-27 23:57:04,707] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.5417851e-14 3.0992719e-04 5.5873491e-20 9.9969006e-01 2.2849057e-20], sum to 1.0000
[2019-04-27 23:57:04,734] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5512
[2019-04-27 23:57:04,742] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.6, 61.16666666666667, 1.0, 2.0, 0.9331727519773599, 1.0, 2.0, 0.9331727519773599, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2610175.560068244, 2610175.560068244, 489911.158914321], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5569800.0000, 
sim time next is 5570400.0000, 
raw observation next is [32.7, 60.33333333333334, 1.0, 2.0, 0.8472920334845337, 1.0, 2.0, 0.8472920334845337, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2369730.995339381, 2369730.995339381, 443551.8147222218], 
processed observation next is [1.0, 0.4782608695652174, 0.7488151658767774, 0.6033333333333334, 1.0, 1.0, 0.8160144981741371, 1.0, 1.0, 0.8160144981741371, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6582586098164948, 0.6582586098164948, 0.6620176339137639], 
reward next is 0.3380, 
noisyNet noise sample is [array([1.1832145], dtype=float32), -0.45260504]. 
=============================================
[2019-04-27 23:57:09,771] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5096540e-15 1.2133662e-02 2.7369465e-22 9.8786628e-01 3.0756726e-21], sum to 1.0000
[2019-04-27 23:57:09,790] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3420
[2019-04-27 23:57:09,810] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.56666666666667, 87.0, 1.0, 2.0, 0.2583459961748788, 1.0, 2.0, 0.2583459961748788, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 722002.3429705057, 722002.3429705057, 242333.4481540119], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5700000.0000, 
sim time next is 5700600.0000, 
raw observation next is [26.55, 87.0, 1.0, 2.0, 0.2578794744161901, 1.0, 2.0, 0.2578794744161901, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 720698.1115547904, 720698.1115547904, 242254.6916391487], 
processed observation next is [0.0, 1.0, 0.4573459715639811, 0.87, 1.0, 1.0, 0.10587888483878322, 1.0, 1.0, 0.10587888483878322, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20019391987633067, 0.20019391987633067, 0.36157416662559505], 
reward next is 0.6384, 
noisyNet noise sample is [array([-1.0815977], dtype=float32), -0.7323124]. 
=============================================
[2019-04-27 23:57:17,688] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.6214510e-20 9.9999917e-01 1.6080979e-28 8.6616762e-07 4.3933298e-28], sum to 1.0000
[2019-04-27 23:57:17,705] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1954
[2019-04-27 23:57:17,709] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.08333333333334, 57.16666666666667, 1.0, 2.0, 0.5421806642376003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757634.2603337981, 757634.2603337975, 190469.5307139758], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5759400.0000, 
sim time next is 5760000.0000, 
raw observation next is [32.9, 58.0, 1.0, 2.0, 0.543803572676141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 759902.8968467354, 759902.8968467347, 190744.3764636886], 
processed observation next is [0.0, 0.6956521739130435, 0.7582938388625592, 0.58, 1.0, 1.0, 0.45036575021221803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21108413801298206, 0.21108413801298187, 0.2846930991995352], 
reward next is 0.7153, 
noisyNet noise sample is [array([-2.4640079], dtype=float32), -0.9955524]. 
=============================================
[2019-04-27 23:57:17,722] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[74.56381 ]
 [74.5173  ]
 [74.487526]
 [74.455894]
 [74.35634 ]], R is [[74.5869751 ]
 [74.5568161 ]
 [74.5273056 ]
 [74.49822998]
 [74.46949005]].
[2019-04-27 23:57:25,698] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3157959e-13 9.9804139e-01 1.5023085e-20 1.9586065e-03 3.6114907e-19], sum to 1.0000
[2019-04-27 23:57:25,704] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4112
[2019-04-27 23:57:25,707] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 87.5, 1.0, 2.0, 0.5588127744463366, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780884.2381319392, 780884.2381319398, 193323.712609451], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5952600.0000, 
sim time next is 5953200.0000, 
raw observation next is [27.76666666666667, 88.0, 1.0, 2.0, 0.5579892147050463, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779732.9744231346, 779732.9744231352, 193180.4580219668], 
processed observation next is [1.0, 0.9130434782608695, 0.515007898894155, 0.88, 1.0, 1.0, 0.4674568851868027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21659249289531515, 0.21659249289531532, 0.28832904182383107], 
reward next is 0.7117, 
noisyNet noise sample is [array([1.7877601], dtype=float32), 0.6694129]. 
=============================================
[2019-04-27 23:57:29,346] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2143626e-14 5.9595722e-01 1.5549941e-20 4.0404281e-01 2.1857547e-18], sum to 1.0000
[2019-04-27 23:57:29,357] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1502
[2019-04-27 23:57:29,366] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 74.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.191134068605495, 6.9112, 168.9117085510252, 1652484.331874416, 1453890.936273281, 311345.2669254686], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6008400.0000, 
sim time next is 6009000.0000, 
raw observation next is [26.76666666666667, 76.0, 1.0, 2.0, 0.9914882182065553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9127285760856, 1411705.775627123, 1411705.775627122, 300359.5054749814], 
processed observation next is [1.0, 0.5652173913043478, 0.46761453396524505, 0.76, 1.0, 1.0, 0.9897448412127172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.829438825890318, 0.39214049322975636, 0.39214049322975614, 0.44829776936564386], 
reward next is 0.5517, 
noisyNet noise sample is [array([1.1182245], dtype=float32), -0.56567824]. 
=============================================
[2019-04-27 23:57:29,377] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[53.970474]
 [52.983868]
 [53.11567 ]
 [52.904465]
 [51.986282]], R is [[54.55477524]
 [54.00922775]
 [53.46913528]
 [53.06960678]
 [52.53890991]].
[2019-04-27 23:57:29,807] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2267264e-12 3.1866673e-01 4.7125472e-17 6.8133330e-01 7.8905566e-17], sum to 1.0000
[2019-04-27 23:57:29,818] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3810
[2019-04-27 23:57:29,820] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.3607063059233532, 1.0, 2.0, 0.3607063059233532, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1008204.270306168, 1008204.270306168, 262680.5588899463], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5976000.0000, 
sim time next is 5976600.0000, 
raw observation next is [26.06666666666667, 93.16666666666667, 1.0, 2.0, 0.3736383159754177, 1.0, 2.0, 0.3736383159754177, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1044367.921599569, 1044367.921599569, 265688.5657375446], 
processed observation next is [1.0, 0.17391304347826086, 0.4344391785150081, 0.9316666666666668, 1.0, 1.0, 0.24534736864508155, 1.0, 1.0, 0.24534736864508155, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2901022004443247, 0.2901022004443247, 0.3965500981157382], 
reward next is 0.6034, 
noisyNet noise sample is [array([0.04805149], dtype=float32), 1.0148163]. 
=============================================
[2019-04-27 23:57:39,351] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-27 23:57:39,353] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-27 23:57:39,353] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-27 23:57:39,354] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:57:39,354] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:57:39,355] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-27 23:57:39,356] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:57:39,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run33
[2019-04-27 23:57:39,408] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-27 23:57:39,410] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:57:39,410] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-27 23:57:39,412] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-27 23:57:39,412] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run33
[2019-04-27 23:57:39,454] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run33
[2019-04-27 23:57:39,487] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run33
[2019-04-27 23:57:39,514] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run33
[2019-04-27 23:57:46,167] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08239424]
[2019-04-27 23:57:46,167] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.06666666666666, 78.66666666666666, 1.0, 2.0, 0.2460573142883501, 1.0, 2.0, 0.2460573142883501, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 701300.1212319976, 701300.1212319982, 242069.532861345]
[2019-04-27 23:57:46,167] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-27 23:57:46,169] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.7674963e-14 4.8662618e-02 7.7584113e-21 9.5133740e-01 7.3498364e-20], sampled 0.6989390360681412
[2019-04-27 23:58:35,467] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08239424]
[2019-04-27 23:58:35,468] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.76666666666667, 60.66666666666667, 1.0, 2.0, 0.5931125562949977, 1.0, 2.0, 0.5931125562949977, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1658284.499668977, 1658284.499668977, 332225.1611814852]
[2019-04-27 23:58:35,469] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-27 23:58:35,470] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0340788e-14 4.8843995e-02 8.8687704e-21 9.5115596e-01 8.2889320e-20], sampled 0.8939776282742484
[2019-04-27 23:58:55,106] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08239424]
[2019-04-27 23:58:55,106] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.24707952333334, 63.30346842333334, 1.0, 2.0, 0.4299673668107526, 1.0, 2.0, 0.4299673668107526, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1201894.243844428, 1201894.243844428, 280417.5958094664]
[2019-04-27 23:58:55,107] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-27 23:58:55,112] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0166547e-13 5.4880057e-02 5.4479543e-20 9.4511998e-01 4.6999808e-19], sampled 0.8456554107884536
[2019-04-27 23:58:56,316] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.08239424]
[2019-04-27 23:58:56,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.9, 84.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 526490.5874569297, 526490.5874569297, 238243.6256241212]
[2019-04-27 23:58:56,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-27 23:58:56,321] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4069338e-14 4.5665216e-02 2.8197529e-21 9.5433474e-01 2.7960928e-20], sampled 0.6128172656923795
[2019-04-27 23:59:07,782] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6552.8389 3487314277.5913 107.0000
[2019-04-27 23:59:08,067] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6564.2942 3364552256.4402 67.0000
[2019-04-27 23:59:08,081] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6599.4435 3447781158.1900 72.0000
[2019-04-27 23:59:08,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6452.9650 3401071694.1401 104.0000
[2019-04-27 23:59:08,325] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6272.2691 3660118585.0046 332.0000
[2019-04-27 23:59:09,341] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 800000, evaluation results [800000.0, 6272.269075624779, 3660118585.0046487, 332.0, 6599.443516126816, 3447781158.1899605, 72.0, 6564.294235453905, 3364552256.4402275, 67.0, 6552.838902536637, 3487314277.591317, 107.0, 6452.9649501687645, 3401071694.1400576, 104.0]
[2019-04-27 23:59:12,358] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6679275e-15 7.6898010e-03 1.0261249e-21 9.9231017e-01 1.1762926e-20], sum to 1.0000
[2019-04-27 23:59:12,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-27 23:59:12,375] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.76666666666667, 88.33333333333334, 1.0, 2.0, 0.2609717175565268, 1.0, 2.0, 0.2609717175565268, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 729342.969750874, 729342.969750874, 242781.2322117871], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6218400.0000, 
sim time next is 6219000.0000, 
raw observation next is [26.75, 88.5, 1.0, 2.0, 0.2610152461153594, 1.0, 2.0, 0.2610152461153594, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 729464.661267132, 729464.6612671315, 242788.6938888478], 
processed observation next is [1.0, 1.0, 0.4668246445497631, 0.885, 1.0, 1.0, 0.10965692303055351, 1.0, 1.0, 0.10965692303055351, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20262907257420334, 0.20262907257420318, 0.36237118490872805], 
reward next is 0.6376, 
noisyNet noise sample is [array([-1.4357096], dtype=float32), -1.1922977]. 
=============================================
[2019-04-27 23:59:12,393] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[65.30732 ]
 [65.34552 ]
 [65.29217 ]
 [65.25836 ]
 [65.087746]], R is [[65.2985611 ]
 [65.28321838]
 [65.26807404]
 [65.25309753]
 [65.23820496]].
[2019-04-27 23:59:18,683] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3302828e-14 7.9835523e-03 1.1384764e-23 9.9201643e-01 3.2061837e-20], sum to 1.0000
[2019-04-27 23:59:18,692] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4824
[2019-04-27 23:59:18,697] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.76666666666667, 65.0, 1.0, 2.0, 0.2636861845175025, 1.0, 2.0, 0.2636861845175025, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 736931.7518977537, 736931.7518977537, 243246.5213115234], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6266400.0000, 
sim time next is 6267000.0000, 
raw observation next is [30.78333333333333, 64.5, 1.0, 2.0, 0.2625699692604601, 1.0, 2.0, 0.2625699692604601, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 733811.164253727, 733811.164253727, 243054.5506853212], 
processed observation next is [0.0, 0.5217391304347826, 0.6579778830963664, 0.645, 1.0, 1.0, 0.11153008344633747, 1.0, 1.0, 0.11153008344633747, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20383643451492414, 0.20383643451492414, 0.36276798609749433], 
reward next is 0.6372, 
noisyNet noise sample is [array([-1.0078666], dtype=float32), 1.0866697]. 
=============================================
[2019-04-27 23:59:18,723] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.753895]
 [63.809074]
 [63.698635]
 [63.65325 ]
 [63.765644]], R is [[63.83092499]
 [63.82955933]
 [63.82793808]
 [63.82607651]
 [63.82394791]].
[2019-04-27 23:59:19,199] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.9056553e-14 2.2693351e-01 1.2034905e-21 7.7306652e-01 2.4466839e-20], sum to 1.0000
[2019-04-27 23:59:19,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8194
[2019-04-27 23:59:19,217] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.1, 65.0, 1.0, 2.0, 0.504841939148684, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705440.4202404728, 705440.4202404734, 184358.3245940627], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6283200.0000, 
sim time next is 6283800.0000, 
raw observation next is [29.95, 66.0, 1.0, 2.0, 0.2543970741411126, 1.0, 1.0, 0.2543970741411126, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 710962.5903648804, 710962.5903648811, 241671.6222874402], 
processed observation next is [0.0, 0.7391304347826086, 0.6184834123222749, 0.66, 1.0, 1.0, 0.10168322185676218, 1.0, 0.5, 0.10168322185676218, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.197489608434689, 0.19748960843468918, 0.360703913861851], 
reward next is 0.6393, 
noisyNet noise sample is [array([1.7569697], dtype=float32), -1.0621526]. 
=============================================
[2019-04-27 23:59:22,001] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [9.2785632e-15 9.0891868e-02 7.7974278e-24 9.0910810e-01 2.8401848e-22], sum to 1.0000
[2019-04-27 23:59:22,015] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4504
[2019-04-27 23:59:22,019] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.16666666666667, 82.33333333333334, 1.0, 2.0, 0.2562504080951171, 1.0, 2.0, 0.2562504080951171, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 716143.8252173144, 716143.8252173144, 241980.5872596625], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6391200.0000, 
sim time next is 6391800.0000, 
raw observation next is [27.15, 82.5, 1.0, 2.0, 0.2562702807366654, 1.0, 2.0, 0.2562702807366654, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 716199.3818935043, 716199.3818935043, 241983.9671258424], 
processed observation next is [0.0, 1.0, 0.485781990521327, 0.825, 1.0, 1.0, 0.10394009727309085, 1.0, 1.0, 0.10394009727309085, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19894427274819565, 0.19894427274819565, 0.36117010018782447], 
reward next is 0.6388, 
noisyNet noise sample is [array([-0.71736586], dtype=float32), 0.4902406]. 
=============================================
[2019-04-27 23:59:22,804] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.6694540e-14 9.6253969e-02 2.1010829e-20 9.0374607e-01 6.9714032e-20], sum to 1.0000
[2019-04-27 23:59:22,810] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8538
[2019-04-27 23:59:22,812] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.1, 79.0, 1.0, 2.0, 0.6536401164357271, 1.0, 2.0, 0.6536401164357271, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1827676.792623047, 1827676.792623047, 354905.2504225185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6423600.0000, 
sim time next is 6424200.0000, 
raw observation next is [28.2, 78.5, 1.0, 2.0, 0.7268636440117002, 1.0, 2.0, 0.7268636440117002, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2032615.087868333, 2032615.087868333, 385831.9876256387], 
processed observation next is [1.0, 0.34782608695652173, 0.5355450236966824, 0.785, 1.0, 1.0, 0.6709200530261449, 1.0, 1.0, 0.6709200530261449, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.564615302185648, 0.564615302185648, 0.575868638247222], 
reward next is 0.4241, 
noisyNet noise sample is [array([-1.2274804], dtype=float32), 1.6122476]. 
=============================================
[2019-04-27 23:59:32,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6634451e-16 4.4998865e-06 3.1599283e-24 9.9999547e-01 2.2968162e-21], sum to 1.0000
[2019-04-27 23:59:32,980] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4941
[2019-04-27 23:59:32,983] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.1, 55.0, 1.0, 2.0, 0.7858145944550856, 1.0, 2.0, 0.7858145944550856, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2197635.730006236, 2197635.730006237, 413015.0181512865], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6530400.0000, 
sim time next is 6531000.0000, 
raw observation next is [31.98333333333333, 55.5, 1.0, 2.0, 0.796750546059744, 1.0, 2.0, 0.796750546059744, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2228249.074782087, 2228249.074782087, 418282.8041400764], 
processed observation next is [1.0, 0.6086956521739131, 0.7148499210110584, 0.555, 1.0, 1.0, 0.7551211398310169, 1.0, 1.0, 0.7551211398310169, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6189580763283575, 0.6189580763283575, 0.6243026927463827], 
reward next is 0.3757, 
noisyNet noise sample is [array([0.57279325], dtype=float32), 0.25590813]. 
=============================================
[2019-04-27 23:59:33,007] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[54.73417 ]
 [54.69428 ]
 [54.58711 ]
 [54.545277]
 [54.648556]], R is [[54.56637573]
 [54.40427399]
 [54.28342438]
 [54.16133499]
 [54.03095245]].
[2019-04-27 23:59:33,139] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1568057e-16 5.0235471e-06 4.8224159e-23 9.9999499e-01 2.2537218e-22], sum to 1.0000
[2019-04-27 23:59:33,144] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4277
[2019-04-27 23:59:33,149] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.4938471137874578, 1.0, 2.0, 0.4938471137874578, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1380583.559825241, 1380583.559825241, 298354.0500593479], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6612600.0000, 
sim time next is 6613200.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5652943300087284, 1.0, 2.0, 0.5652943300087284, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1580466.424380691, 1580466.424380691, 321787.7313106439], 
processed observation next is [1.0, 0.5652173913043478, 0.6682464454976303, 0.66, 1.0, 1.0, 0.4762582289261788, 1.0, 1.0, 0.4762582289261788, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4390184512168586, 0.4390184512168586, 0.48028019598603566], 
reward next is 0.5197, 
noisyNet noise sample is [array([0.16394013], dtype=float32), -1.1051443]. 
=============================================
[2019-04-27 23:59:35,470] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.9394710e-17 1.0956277e-06 5.0371814e-23 9.9999893e-01 6.9433033e-22], sum to 1.0000
[2019-04-27 23:59:35,477] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7310
[2019-04-27 23:59:35,485] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.33333333333334, 83.0, 1.0, 2.0, 0.5809557458389901, 1.0, 2.0, 0.5809557458389901, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1624286.250398471, 1624286.250398471, 327319.7269485466], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6596400.0000, 
sim time next is 6597000.0000, 
raw observation next is [27.5, 82.0, 1.0, 2.0, 0.6477672290397322, 1.0, 2.0, 0.6477672290397322, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1811241.430388389, 1811241.43038839, 352560.4021837609], 
processed observation next is [1.0, 0.34782608695652173, 0.5023696682464456, 0.82, 1.0, 1.0, 0.5756231675177496, 1.0, 1.0, 0.5756231675177496, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5031226195523303, 0.5031226195523306, 0.5262095554981506], 
reward next is 0.4738, 
noisyNet noise sample is [array([-0.2305493], dtype=float32), 0.3437155]. 
=============================================
[2019-04-27 23:59:35,503] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[55.40444]
 [56.46352]
 [56.65743]
 [56.61612]
 [56.58367]], R is [[54.8560524 ]
 [54.81895447]
 [54.88012314]
 [54.95473099]
 [55.02827454]].
[2019-04-27 23:59:38,267] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.76380387e-16 5.01388377e-06 1.99296478e-23 9.99994993e-01
 1.02905775e-22], sum to 1.0000
[2019-04-27 23:59:38,271] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4008
[2019-04-27 23:59:38,277] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.86666666666667, 64.33333333333333, 1.0, 2.0, 0.7246234631797253, 1.0, 2.0, 0.7246234631797253, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2026344.679633956, 2026344.679633956, 384833.9099589467], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6709200.0000, 
sim time next is 6709800.0000, 
raw observation next is [29.83333333333334, 64.66666666666667, 1.0, 2.0, 0.7260836682427543, 1.0, 2.0, 0.7260836682427543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2030431.882055602, 2030431.882055602, 385480.9382350776], 
processed observation next is [1.0, 0.6521739130434783, 0.6129541864139023, 0.6466666666666667, 1.0, 1.0, 0.6699803231840413, 1.0, 1.0, 0.6699803231840413, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5640088561265562, 0.5640088561265562, 0.5753446839329517], 
reward next is 0.4247, 
noisyNet noise sample is [array([-0.47251424], dtype=float32), 2.0889988]. 
=============================================
[2019-04-27 23:59:45,830] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.4860240e-14 2.9608046e-04 8.2325554e-19 9.9970394e-01 3.5484737e-18], sum to 1.0000
[2019-04-27 23:59:45,838] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9757
[2019-04-27 23:59:45,843] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.33333333333334, 65.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 513481.8033859233, 513481.8033859239, 236262.1029676686], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6817200.0000, 
sim time next is 6817800.0000, 
raw observation next is [25.21666666666667, 66.16666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 515080.8430042375, 515080.8430042375, 236569.7875388337], 
processed observation next is [1.0, 0.9130434782608695, 0.3941548183254346, 0.6616666666666667, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14307801194562153, 0.14307801194562153, 0.3530892351325876], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.88813734], dtype=float32), 1.1269588]. 
=============================================
[2019-04-27 23:59:47,468] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3832112e-18 1.3803096e-07 2.2607127e-25 9.9999988e-01 3.0349350e-25], sum to 1.0000
[2019-04-27 23:59:47,476] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3103
[2019-04-27 23:59:47,484] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.4, 82.66666666666667, 1.0, 2.0, 0.1768920011341444, 1.0, 2.0, 0.1768920011341444, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 541821.3586169227, 541821.3586169227, 238848.8065521692], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6846000.0000, 
sim time next is 6846600.0000, 
raw observation next is [23.5, 82.33333333333334, 1.0, 2.0, 0.177616807169398, 1.0, 2.0, 0.177616807169398, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 543200.7474872294, 543200.7474872294, 238818.3653325031], 
processed observation next is [0.0, 0.21739130434782608, 0.31279620853080575, 0.8233333333333335, 1.0, 1.0, 0.009176876107708433, 1.0, 1.0, 0.009176876107708433, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15088909652423038, 0.15088909652423038, 0.3564453213917957], 
reward next is 0.6436, 
noisyNet noise sample is [array([1.0997835], dtype=float32), 0.41597083]. 
=============================================
[2019-04-27 23:59:49,742] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.4101571e-16 1.1524885e-06 5.5746783e-21 9.9999881e-01 8.2341946e-20], sum to 1.0000
[2019-04-27 23:59:49,753] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4650
[2019-04-27 23:59:49,759] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.83333333333333, 74.83333333333334, 1.0, 2.0, 0.2037337863542579, 1.0, 2.0, 0.2037337863542579, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 599172.7738102619, 599172.7738102612, 238947.1482551035], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6905400.0000, 
sim time next is 6906000.0000, 
raw observation next is [25.76666666666667, 75.66666666666667, 1.0, 2.0, 0.2048210984141403, 1.0, 2.0, 0.2048210984141403, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 601043.2319420204, 601043.2319420204, 238894.2630050369], 
processed observation next is [0.0, 0.9565217391304348, 0.42022116903633505, 0.7566666666666667, 1.0, 1.0, 0.041953130619446145, 1.0, 1.0, 0.041953130619446145, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1669564533172279, 0.1669564533172279, 0.3565586015000551], 
reward next is 0.6434, 
noisyNet noise sample is [array([0.28164843], dtype=float32), -0.2806888]. 
=============================================
[2019-04-27 23:59:49,780] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[48.51572 ]
 [48.624935]
 [48.660793]
 [48.739155]
 [48.789516]], R is [[48.57113647]
 [48.72879028]
 [48.88482285]
 [49.03932571]
 [49.19234085]].
[2019-04-27 23:59:53,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.3094397e-14 2.0148233e-03 2.2576959e-20 9.9798512e-01 2.1925372e-20], sum to 1.0000
[2019-04-27 23:59:53,352] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9522
[2019-04-27 23:59:53,356] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.7, 76.5, 1.0, 2.0, 0.2059455504875123, 1.0, 2.0, 0.2059455504875123, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 603014.6076301388, 603014.6076301388, 238846.6630665266], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6906600.0000, 
sim time next is 6907200.0000, 
raw observation next is [25.63333333333333, 77.33333333333333, 1.0, 2.0, 0.2071536285149867, 1.0, 2.0, 0.2071536285149867, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 605224.6705439392, 605224.6705439392, 238811.5836966699], 
processed observation next is [0.0, 0.9565217391304348, 0.4139020537124801, 0.7733333333333333, 1.0, 1.0, 0.04476340784938155, 1.0, 1.0, 0.04476340784938155, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1681179640399831, 0.1681179640399831, 0.3564351995472685], 
reward next is 0.6436, 
noisyNet noise sample is [array([0.5888016], dtype=float32), 0.32225004]. 
=============================================
[2019-04-27 23:59:54,603] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.8598202e-15 7.2885277e-06 1.1732876e-22 9.9999273e-01 1.5332054e-23], sum to 1.0000
[2019-04-27 23:59:54,610] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1232
[2019-04-27 23:59:54,614] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.75, 91.0, 1.0, 2.0, 0.2105970525653622, 1.0, 2.0, 0.2105970525653622, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 613975.4904493558, 613975.4904493558, 239108.4820964582], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6928200.0000, 
sim time next is 6928800.0000, 
raw observation next is [23.7, 91.33333333333333, 1.0, 2.0, 0.2102238630887658, 1.0, 2.0, 0.2102238630887658, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 613045.0460414697, 613045.0460414704, 239078.6017684906], 
processed observation next is [0.0, 0.17391304347826086, 0.3222748815165877, 0.9133333333333333, 1.0, 1.0, 0.04846248564911542, 1.0, 1.0, 0.04846248564911542, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1702902905670749, 0.1702902905670751, 0.3568337339828218], 
reward next is 0.6432, 
noisyNet noise sample is [array([-1.2498285], dtype=float32), 0.02791602]. 
=============================================
[2019-04-27 23:59:55,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8141174e-17 2.4200331e-08 6.9774456e-25 1.0000000e+00 7.9269876e-25], sum to 1.0000
[2019-04-27 23:59:55,727] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4336
[2019-04-27 23:59:55,735] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.7, 55.0, 1.0, 2.0, 0.1918698847981242, 1.0, 2.0, 0.1918698847981242, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 575981.4721936123, 575981.4721936116, 239158.7348759916], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6984000.0000, 
sim time next is 6984600.0000, 
raw observation next is [28.58333333333334, 56.16666666666667, 1.0, 2.0, 0.1913193499063105, 1.0, 2.0, 0.1913193499063105, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 573280.2559507323, 573280.2559507323, 238918.971315566], 
processed observation next is [0.0, 0.8695652173913043, 0.5537124802527649, 0.5616666666666668, 1.0, 1.0, 0.025685963742542756, 1.0, 1.0, 0.025685963742542756, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15924451554187008, 0.15924451554187008, 0.35659547957547166], 
reward next is 0.6434, 
noisyNet noise sample is [array([0.43308115], dtype=float32), 1.4131097]. 
=============================================
[2019-04-28 00:00:00,923] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-28 00:00:00,924] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:00:00,924] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:00:00,924] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:00:00,925] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:00:00,925] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:00:00,925] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:00:00,926] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:00:00,926] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:00:00,926] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:00:00,926] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:00:00,939] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run34
[2019-04-28 00:00:00,972] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run34
[2019-04-28 00:00:01,000] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run34
[2019-04-28 00:00:01,001] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run34
[2019-04-28 00:00:01,028] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run34
[2019-04-28 00:00:19,344] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07830782]
[2019-04-28 00:00:19,345] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.09726348666667, 95.97786004333334, 1.0, 2.0, 0.2438811150930895, 1.0, 2.0, 0.2438811150930895, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 171.5212843490159, 681563.1232036651, 681563.1232036658, 240142.5896942489]
[2019-04-28 00:00:19,346] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:00:19,349] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5763794e-19 2.5343943e-06 1.7107518e-27 9.9999750e-01 2.1450934e-26], sampled 0.0914665521976068
[2019-04-28 00:00:22,417] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07830782]
[2019-04-28 00:00:22,418] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.75, 82.16666666666667, 1.0, 2.0, 0.4896329814906385, 1.0, 2.0, 0.4896329814906385, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1370802.250027657, 1370802.250027657, 297292.8721574591]
[2019-04-28 00:00:22,420] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:00:22,424] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.2148587e-19 3.4046971e-06 6.9547472e-27 9.9999654e-01 8.2360986e-26], sampled 0.2844438127279775
[2019-04-28 00:00:33,326] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07830782]
[2019-04-28 00:00:33,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.3, 83.0, 1.0, 2.0, 0.3022171618344092, 1.0, 2.0, 0.3022171618344092, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 844660.8267949348, 844660.8267949348, 250013.0868802367]
[2019-04-28 00:00:33,327] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:00:33,329] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.7005034e-20 2.1897436e-06 8.5516538e-28 9.9999785e-01 1.1031557e-26], sampled 0.6299520400740684
[2019-04-28 00:00:55,375] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07830782]
[2019-04-28 00:00:55,376] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.9330744, 67.7197615, 1.0, 2.0, 0.27403956496271, 1.0, 2.0, 0.27403956496271, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 765873.341631717, 765873.341631717, 245540.2144260906]
[2019-04-28 00:00:55,376] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:00:55,378] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.3074361e-20 1.8275751e-06 3.6169956e-28 9.9999821e-01 4.8334721e-27], sampled 0.200324640855001
[2019-04-28 00:01:30,946] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.8096 3429641291.0945 33.0000
[2019-04-28 00:01:30,950] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 00:01:31,041] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 00:01:31,270] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 00:01:31,398] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 00:01:32,410] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 825000, evaluation results [825000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.809555340946, 3429641291.0944977, 33.0]
[2019-04-28 00:01:38,573] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2635756e-17 2.0018751e-03 8.4129935e-25 9.9799806e-01 1.3595770e-22], sum to 1.0000
[2019-04-28 00:01:38,578] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6683
[2019-04-28 00:01:38,582] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.05, 83.5, 1.0, 2.0, 0.2383769018470709, 1.0, 2.0, 0.2383769018470709, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 666177.1691844146, 666177.1691844139, 239075.450270842], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7158600.0000, 
sim time next is 7159200.0000, 
raw observation next is [26.03333333333333, 83.66666666666667, 1.0, 2.0, 0.2382381877025607, 1.0, 2.0, 0.2382381877025607, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 665789.393044519, 665789.3930445183, 239053.7288402924], 
processed observation next is [1.0, 0.8695652173913043, 0.4328593996840442, 0.8366666666666667, 1.0, 1.0, 0.08221468397898878, 1.0, 1.0, 0.08221468397898878, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.18494149806792196, 0.18494149806792176, 0.35679661020939163], 
reward next is 0.6432, 
noisyNet noise sample is [array([-0.5221979], dtype=float32), -2.1186395]. 
=============================================
[2019-04-28 00:01:40,173] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.4376915e-19 3.7744925e-08 7.0252797e-25 1.0000000e+00 1.1879822e-23], sum to 1.0000
[2019-04-28 00:01:40,189] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3322
[2019-04-28 00:01:40,196] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [24.6, 92.66666666666666, 1.0, 2.0, 0.580772761191949, 1.0, 2.0, 0.580772761191949, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1623774.258539109, 1623774.258539108, 327245.6494923554], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7220400.0000, 
sim time next is 7221000.0000, 
raw observation next is [24.75, 90.83333333333334, 1.0, 2.0, 0.5776541421337751, 1.0, 2.0, 0.5776541421337751, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1615048.386099234, 1615048.386099234, 326131.2953893406], 
processed observation next is [1.0, 0.5652173913043478, 0.3720379146919432, 0.9083333333333334, 1.0, 1.0, 0.4911495688358736, 1.0, 1.0, 0.4911495688358736, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.44862455169423165, 0.44862455169423165, 0.486763127446777], 
reward next is 0.5132, 
noisyNet noise sample is [array([0.0559212], dtype=float32), 0.670851]. 
=============================================
[2019-04-28 00:01:40,210] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.042027]
 [62.117825]
 [62.318813]
 [62.56302 ]
 [62.440952]], R is [[61.93278122]
 [61.82502747]
 [61.71657181]
 [61.62485886]
 [61.58744812]].
[2019-04-28 00:01:41,232] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.1726230e-16 3.1388168e-05 1.8489391e-22 9.9996865e-01 1.2316508e-20], sum to 1.0000
[2019-04-28 00:01:41,243] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7131
[2019-04-28 00:01:41,274] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.08333333333333, 78.0, 1.0, 2.0, 0.6918898647011575, 1.0, 2.0, 0.6918898647011575, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1934725.44722572, 1934725.44722572, 370673.1563104201], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7213800.0000, 
sim time next is 7214400.0000, 
raw observation next is [29.0, 79.0, 1.0, 2.0, 0.7073743897568431, 1.0, 2.0, 0.7073743897568431, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1978064.658306246, 1978064.658306245, 377298.9793876709], 
processed observation next is [1.0, 0.5217391304347826, 0.5734597156398105, 0.79, 1.0, 1.0, 0.6474390238034253, 1.0, 1.0, 0.6474390238034253, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5494624050850683, 0.549462405085068, 0.5631328050562252], 
reward next is 0.4369, 
noisyNet noise sample is [array([-0.4211148], dtype=float32), 1.8000495]. 
=============================================
[2019-04-28 00:01:46,896] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2249725e-19 2.2213874e-07 6.6817574e-27 9.9999976e-01 4.7007574e-26], sum to 1.0000
[2019-04-28 00:01:46,916] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7226
[2019-04-28 00:01:46,919] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.1, 89.5, 1.0, 2.0, 0.1706578991439219, 1.0, 2.0, 0.1706578991439219, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 528192.6884678473, 528192.6884678473, 238854.013717851], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7259400.0000, 
sim time next is 7260000.0000, 
raw observation next is [22.06666666666667, 89.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 524055.48325638, 524055.48325638, 238351.8574797522], 
processed observation next is [1.0, 0.0, 0.2448657187993683, 0.8933333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14557096757121668, 0.14557096757121668, 0.3557490410145555], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2878395], dtype=float32), -0.8389132]. 
=============================================
[2019-04-28 00:01:46,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.75442 ]
 [65.28071 ]
 [66.21421 ]
 [67.813446]
 [69.46291 ]], R is [[63.54266357]
 [63.55073929]
 [63.55854797]
 [63.566082  ]
 [63.57352066]].
[2019-04-28 00:01:49,759] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.7788322e-16 3.8879614e-05 3.0155959e-22 9.9996114e-01 6.7726655e-22], sum to 1.0000
[2019-04-28 00:01:49,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6031
[2019-04-28 00:01:49,783] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.4, 60.5, 1.0, 2.0, 0.2786650932353686, 1.0, 2.0, 0.2786650932353686, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 839605.2651672694, 839605.2651672694, 253994.4304160242], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7303800.0000, 
sim time next is 7304400.0000, 
raw observation next is [27.5, 60.0, 1.0, 2.0, 0.266582717770421, 1.0, 2.0, 0.266582717770421, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 803474.4231713752, 803474.4231713752, 251722.6390908347], 
processed observation next is [1.0, 0.5652173913043478, 0.5023696682464456, 0.6, 1.0, 1.0, 0.11636472020532647, 1.0, 1.0, 0.11636472020532647, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22318733976982644, 0.22318733976982644, 0.37570543147885777], 
reward next is 0.6243, 
noisyNet noise sample is [array([0.48184353], dtype=float32), -0.91136616]. 
=============================================
[2019-04-28 00:01:50,388] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.6481672e-18 1.5292602e-05 1.5891417e-26 9.9998474e-01 3.7022619e-24], sum to 1.0000
[2019-04-28 00:01:50,398] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7497
[2019-04-28 00:01:50,404] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.63333333333333, 92.33333333333334, 1.0, 2.0, 0.2940992720913304, 1.0, 2.0, 0.2940992720913304, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 932066.8836879914, 932066.8836879914, 261725.9802261616], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7377600.0000, 
sim time next is 7378200.0000, 
raw observation next is [20.7, 92.5, 1.0, 2.0, 0.3030780597111354, 1.0, 2.0, 0.3030780597111354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 958455.0080766829, 958455.0080766829, 263501.0377948256], 
processed observation next is [1.0, 0.391304347826087, 0.18009478672985785, 0.925, 1.0, 1.0, 0.1603350117001631, 1.0, 1.0, 0.1603350117001631, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.26623750224352305, 0.26623750224352305, 0.39328513103705315], 
reward next is 0.6067, 
noisyNet noise sample is [array([0.980895], dtype=float32), -0.03324589]. 
=============================================
[2019-04-28 00:01:54,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.5017334e-13 1.3196368e-05 2.3930817e-18 9.9998677e-01 5.2783665e-18], sum to 1.0000
[2019-04-28 00:01:54,173] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9371
[2019-04-28 00:01:54,179] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.2, 90.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 490385.3388835548, 490385.3388835548, 231640.7440310844], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7423200.0000, 
sim time next is 7423800.0000, 
raw observation next is [21.16666666666667, 90.83333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 491523.0147521871, 491523.0147521871, 231894.342591703], 
processed observation next is [1.0, 0.9565217391304348, 0.2022116903633494, 0.9083333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.13653417076449642, 0.13653417076449642, 0.34611095909209405], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8274242], dtype=float32), -1.1759528]. 
=============================================
[2019-04-28 00:02:02,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.4540093e-21 9.9602812e-07 1.6923124e-29 9.9999905e-01 3.5831640e-26], sum to 1.0000
[2019-04-28 00:02:02,729] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7511
[2019-04-28 00:02:02,735] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.43333333333333, 92.66666666666667, 1.0, 2.0, 0.2045359294368974, 1.0, 2.0, 0.2045359294368974, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 598501.824734332, 598501.824734332, 238581.8277065347], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7525200.0000, 
sim time next is 7525800.0000, 
raw observation next is [23.4, 92.5, 1.0, 2.0, 0.202898727821628, 1.0, 2.0, 0.202898727821628, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 594932.0353804355, 594932.0353804355, 238542.9844057721], 
processed observation next is [0.0, 0.08695652173913043, 0.30805687203791465, 0.925, 1.0, 1.0, 0.039637021471840936, 1.0, 1.0, 0.039637021471840936, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16525889871678764, 0.16525889871678764, 0.3560343050832419], 
reward next is 0.6440, 
noisyNet noise sample is [array([-1.9035487], dtype=float32), 0.114870496]. 
=============================================
[2019-04-28 00:02:04,008] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9096040e-17 4.9389678e-07 3.1887730e-24 9.9999952e-01 1.9290423e-23], sum to 1.0000
[2019-04-28 00:02:04,038] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3639
[2019-04-28 00:02:04,044] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 87.0, 1.0, 2.0, 0.2448253616358596, 1.0, 2.0, 0.2448253616358596, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 684204.0280767009, 684204.0280767016, 240102.6530325233], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7590600.0000, 
sim time next is 7591200.0000, 
raw observation next is [25.93333333333333, 87.33333333333334, 1.0, 2.0, 0.2444870596713943, 1.0, 2.0, 0.2444870596713943, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 683258.2874604741, 683258.2874604741, 240048.1832772433], 
processed observation next is [0.0, 0.8695652173913043, 0.42812006319115314, 0.8733333333333334, 1.0, 1.0, 0.08974344538722205, 1.0, 1.0, 0.08974344538722205, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18979396873902057, 0.18979396873902057, 0.3582808705630497], 
reward next is 0.6417, 
noisyNet noise sample is [array([-2.047259], dtype=float32), 1.9623822]. 
=============================================
[2019-04-28 00:02:05,020] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.4395761e-19 4.1031285e-07 1.9955793e-27 9.9999964e-01 1.0217614e-26], sum to 1.0000
[2019-04-28 00:02:05,026] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3297
[2019-04-28 00:02:05,031] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.205070435398584, 1.0, 2.0, 0.205070435398584, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 597918.6018946007, 597918.6018946007, 238315.2158951031], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7522200.0000, 
sim time next is 7522800.0000, 
raw observation next is [23.5, 93.0, 1.0, 2.0, 0.2044278599213262, 1.0, 2.0, 0.2044278599213262, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 596050.9438928734, 596050.9438928734, 238224.100104101], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.93, 1.0, 1.0, 0.041479349302802636, 1.0, 1.0, 0.041479349302802636, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16556970663690926, 0.16556970663690926, 0.35555835836432986], 
reward next is 0.6444, 
noisyNet noise sample is [array([-0.17207354], dtype=float32), 0.0075205844]. 
=============================================
[2019-04-28 00:02:20,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:20,439] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:20,442] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run5
[2019-04-28 00:02:27,975] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.0284841e-18 1.1177596e-07 4.1871652e-26 9.9999988e-01 2.8300158e-24], sum to 1.0000
[2019-04-28 00:02:27,984] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6172
[2019-04-28 00:02:27,990] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 93.0, 1.0, 2.0, 0.2585183978798687, 1.0, 2.0, 0.2585183978798687, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 722484.3181470279, 722484.3181470279, 242364.4708826182], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7869600.0000, 
sim time next is 7870200.0000, 
raw observation next is [26.01666666666667, 92.66666666666667, 1.0, 2.0, 0.4893361186264364, 1.0, 2.0, 0.4893361186264364, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1367964.716030066, 1367964.716030066, 296970.1312450783], 
processed observation next is [1.0, 0.08695652173913043, 0.43206951026856255, 0.9266666666666667, 1.0, 1.0, 0.38474231159811617, 1.0, 1.0, 0.38474231159811617, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3799901988972405, 0.3799901988972405, 0.4432390018583258], 
reward next is 0.5568, 
noisyNet noise sample is [array([0.3921086], dtype=float32), 0.20646802]. 
=============================================
[2019-04-28 00:02:29,110] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:29,111] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:29,122] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run5
[2019-04-28 00:02:29,425] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:29,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:29,450] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run5
[2019-04-28 00:02:29,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:29,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:29,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run5
[2019-04-28 00:02:30,076] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:30,077] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:30,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run5
[2019-04-28 00:02:30,168] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:30,169] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:30,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run5
[2019-04-28 00:02:30,394] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:30,395] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:30,399] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run5
[2019-04-28 00:02:31,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:31,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:31,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run5
[2019-04-28 00:02:31,521] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:31,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:31,530] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run5
[2019-04-28 00:02:32,228] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:32,228] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:32,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run5
[2019-04-28 00:02:32,446] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8020060e-17 6.6058390e-09 2.3362180e-23 1.0000000e+00 4.2252662e-22], sum to 1.0000
[2019-04-28 00:02:32,447] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7579
[2019-04-28 00:02:32,450] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.25, 84.0, 1.0, 2.0, 0.1717537780006231, 1.0, 2.0, 0.1717537780006231, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 539532.7604073766, 539532.7604073773, 240064.7580592579], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 28200.0000, 
sim time next is 28800.0000, 
raw observation next is [22.3, 84.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 520976.4938254154, 520976.4938254154, 237483.4073209603], 
processed observation next is [1.0, 0.34782608695652173, 0.25592417061611383, 0.84, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14471569272928206, 0.14471569272928206, 0.35445284674770194], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2633295], dtype=float32), 0.77516264]. 
=============================================
[2019-04-28 00:02:32,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:32,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:32,652] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run5
[2019-04-28 00:02:33,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:33,060] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:33,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run5
[2019-04-28 00:02:33,095] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:33,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:33,121] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run5
[2019-04-28 00:02:33,290] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:33,291] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:33,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run5
[2019-04-28 00:02:34,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:34,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:34,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run5
[2019-04-28 00:02:34,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:02:34,208] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:34,210] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run5
[2019-04-28 00:02:37,207] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-28 00:02:37,209] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:02:37,210] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:37,211] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:02:37,212] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:02:37,219] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:37,220] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:37,222] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:02:37,222] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:37,226] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run35
[2019-04-28 00:02:37,226] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run35
[2019-04-28 00:02:37,227] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:02:37,256] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:02:37,287] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run35
[2019-04-28 00:02:37,313] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run35
[2019-04-28 00:02:37,344] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run35
[2019-04-28 00:02:54,116] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07532836]
[2019-04-28 00:02:54,117] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.55, 88.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 386212.557526308, 386212.557526308, 209745.1819675092]
[2019-04-28 00:02:54,117] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:02:54,120] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0163198e-18 1.9957811e-09 9.8826547e-26 1.0000000e+00 6.0513200e-25], sampled 0.11492616842461079
[2019-04-28 00:03:11,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07532836]
[2019-04-28 00:03:11,502] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.61666666666667, 78.50000000000001, 1.0, 2.0, 0.2891892723467856, 1.0, 2.0, 0.2891892723467856, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 808228.7965786466, 808228.7965786466, 248313.4560746499]
[2019-04-28 00:03:11,503] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:03:11,506] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9706951e-20 3.5823627e-10 7.1963143e-28 1.0000000e+00 5.1476086e-27], sampled 0.6451243348102307
[2019-04-28 00:03:31,710] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07532836]
[2019-04-28 00:03:31,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.497437255, 74.78690733, 1.0, 2.0, 0.6520266250801027, 1.0, 2.0, 0.6520266250801027, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 1823140.705550896, 1823140.705550897, 354742.5896835218]
[2019-04-28 00:03:31,712] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:03:31,714] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.7933968e-21 1.8601934e-10 1.1111901e-28 1.0000000e+00 8.4248409e-28], sampled 0.559980164709023
[2019-04-28 00:03:45,865] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07532836]
[2019-04-28 00:03:45,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.88333333333334, 53.83333333333334, 1.0, 2.0, 0.7209463072742109, 1.0, 2.0, 0.7209463072742109, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2016069.190514553, 2016069.190514553, 382940.3850530253]
[2019-04-28 00:03:45,870] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:03:45,872] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6944195e-22 2.8851810e-11 5.3354899e-31 1.0000000e+00 4.7885649e-30], sampled 0.035440371659659076
[2019-04-28 00:04:09,152] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00222432], dtype=float32), 0.07532836]
[2019-04-28 00:04:09,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.34865374, 64.47600353, 1.0, 2.0, 0.2667379513069476, 1.0, 2.0, 0.2667379513069476, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 810895.1646128362, 810895.1646128362, 252791.9984846792]
[2019-04-28 00:04:09,153] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:04:09,156] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5844526e-20 3.3395625e-10 5.9076832e-28 1.0000000e+00 4.2512693e-27], sampled 0.7985029503796496
[2019-04-28 00:04:16,646] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 00:04:16,873] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 00:04:16,898] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 00:04:16,942] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 00:04:17,027] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 00:04:18,040] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 850000, evaluation results [850000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 00:04:25,581] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.1281956e-12 4.7143429e-05 2.9389452e-16 9.9995291e-01 2.3219367e-16], sum to 1.0000
[2019-04-28 00:04:25,591] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0282
[2019-04-28 00:04:25,601] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.9, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 520480.7423455451, 520480.7423455451, 237483.5559131885], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 167400.0000, 
sim time next is 168000.0000, 
raw observation next is [20.8, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 515573.4381945661, 515573.4381945655, 236514.913666204], 
processed observation next is [1.0, 0.9565217391304348, 0.1848341232227489, 0.96, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.14321484394293504, 0.14321484394293485, 0.3530073338301552], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.9774618], dtype=float32), 0.62950504]. 
=============================================
[2019-04-28 00:04:25,619] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[35.809044]
 [35.851612]
 [35.88168 ]
 [35.935368]
 [35.994793]], R is [[35.41758728]
 [35.06341171]
 [34.71277618]
 [34.36565018]
 [34.66524124]].
[2019-04-28 00:04:37,118] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3329287e-18 9.9999225e-01 1.7035694e-31 7.7089344e-06 2.8649318e-28], sum to 1.0000
[2019-04-28 00:04:37,125] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2217
[2019-04-28 00:04:37,130] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 81.0, 1.0, 2.0, 0.2780057178120681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 450883.3626340893, 450883.3626340887, 163641.1343474644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 411600.0000, 
sim time next is 412200.0000, 
raw observation next is [21.25, 81.0, 1.0, 2.0, 0.2741856425400488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 445290.5144321945, 445290.5144321945, 163263.2274444941], 
processed observation next is [1.0, 0.782608695652174, 0.20616113744075834, 0.81, 1.0, 1.0, 0.12552487053017924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12369180956449848, 0.12369180956449848, 0.24367645887237926], 
reward next is 0.7563, 
noisyNet noise sample is [array([0.5950102], dtype=float32), 0.60142106]. 
=============================================
[2019-04-28 00:04:40,497] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.08020072e-18 9.99989271e-01 1.21036436e-32 1.07276564e-05
 4.75143528e-28], sum to 1.0000
[2019-04-28 00:04:40,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3066
[2019-04-28 00:04:40,514] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 73.0, 1.0, 2.0, 0.4791762917907562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773024.1119324766, 773024.1119324766, 191641.6318986931], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 394200.0000, 
sim time next is 394800.0000, 
raw observation next is [22.83333333333333, 73.0, 1.0, 2.0, 0.5187384732909831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 836339.7480974811, 836339.7480974811, 198703.6201582337], 
processed observation next is [1.0, 0.5652173913043478, 0.2812006319115322, 0.73, 1.0, 1.0, 0.42016683529034105, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23231659669374474, 0.23231659669374474, 0.2965725674003488], 
reward next is 0.7034, 
noisyNet noise sample is [array([-0.3064973], dtype=float32), -0.13878885]. 
=============================================
[2019-04-28 00:04:41,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.2656993e-19 9.9999702e-01 2.5246489e-31 2.9817656e-06 1.0254086e-28], sum to 1.0000
[2019-04-28 00:04:41,452] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5698
[2019-04-28 00:04:41,455] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.25, 79.50000000000001, 1.0, 2.0, 0.6505882583214113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1042132.917662223, 1042132.917662223, 225708.4408434813], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 403800.0000, 
sim time next is 404400.0000, 
raw observation next is [22.2, 80.0, 1.0, 2.0, 0.5961380829508031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 954474.4791067817, 954474.479106781, 213665.3531353726], 
processed observation next is [1.0, 0.6956521739130435, 0.2511848341232228, 0.8, 1.0, 1.0, 0.5134193770491603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2651317997518838, 0.2651317997518836, 0.3189035121423472], 
reward next is 0.6811, 
noisyNet noise sample is [array([-0.16350171], dtype=float32), -0.6058345]. 
=============================================
[2019-04-28 00:04:46,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.0577757e-19 9.9450052e-01 5.4133746e-34 5.4995273e-03 3.5980554e-29], sum to 1.0000
[2019-04-28 00:04:46,720] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3788
[2019-04-28 00:04:46,735] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 53.0, 1.0, 2.0, 0.580819256691232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 951152.6045233355, 951152.6045233348, 211520.7097591924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 486000.0000, 
sim time next is 486600.0000, 
raw observation next is [25.06666666666667, 53.0, 1.0, 2.0, 0.5029257488295957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 823978.8810772432, 823978.8810772427, 196343.2298963897], 
processed observation next is [1.0, 0.6521739130434783, 0.38704581358609813, 0.53, 1.0, 1.0, 0.40111536003565745, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22888302252145645, 0.2288830225214563, 0.29304959686028315], 
reward next is 0.7070, 
noisyNet noise sample is [array([1.2979525], dtype=float32), 0.523834]. 
=============================================
[2019-04-28 00:04:49,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.3305548e-20 1.0000000e+00 1.0033276e-31 1.0696311e-08 1.6017358e-31], sum to 1.0000
[2019-04-28 00:04:49,864] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8120
[2019-04-28 00:04:49,884] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.76666666666667, 75.0, 1.0, 2.0, 0.2397413199632716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 396109.0358317908, 396109.0358317908, 159848.4993827649], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 502800.0000, 
sim time next is 503400.0000, 
raw observation next is [20.53333333333333, 76.5, 1.0, 2.0, 0.2398221583705169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 396352.3710722703, 396352.3710722709, 159849.4220526468], 
processed observation next is [1.0, 0.8260869565217391, 0.17219589257503945, 0.765, 1.0, 1.0, 0.08412308237411674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11009788085340842, 0.11009788085340859, 0.23858122694424896], 
reward next is 0.7614, 
noisyNet noise sample is [array([-0.6755887], dtype=float32), 1.4670303]. 
=============================================
[2019-04-28 00:04:52,183] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.2305291e-20 1.0000000e+00 7.8605619e-32 2.1738907e-10 1.6415866e-29], sum to 1.0000
[2019-04-28 00:04:52,192] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2994
[2019-04-28 00:04:52,199] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.13333333333333, 91.66666666666666, 1.0, 2.0, 0.206440420760213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 344979.0289928328, 344979.0289928322, 155960.3284788288], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 614400.0000, 
sim time next is 615000.0000, 
raw observation next is [17.11666666666667, 91.83333333333333, 1.0, 2.0, 0.206211874541012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 344595.6360731431, 344595.6360731431, 155942.4043292709], 
processed observation next is [1.0, 0.08695652173913043, 0.010268562401264081, 0.9183333333333333, 1.0, 1.0, 0.043628764507243366, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09572101002031753, 0.09572101002031753, 0.23274985720786703], 
reward next is 0.7673, 
noisyNet noise sample is [array([-0.6853632], dtype=float32), 0.3499684]. 
=============================================
[2019-04-28 00:04:52,215] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[79.69557 ]
 [79.58524 ]
 [79.75651 ]
 [80.002625]
 [79.88921 ]], R is [[79.51467896]
 [79.486763  ]
 [79.45883179]
 [79.42996979]
 [79.39764404]].
[2019-04-28 00:04:52,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.2919801e-18 9.9998546e-01 5.3904795e-33 1.4593705e-05 3.2970016e-29], sum to 1.0000
[2019-04-28 00:04:52,757] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9087
[2019-04-28 00:04:52,762] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.06666666666667, 58.0, 1.0, 2.0, 0.6372104790311179, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1044985.718161123, 1044985.718161123, 223828.8376568352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 567600.0000, 
sim time next is 568200.0000, 
raw observation next is [23.98333333333333, 58.5, 1.0, 2.0, 0.6424025593383613, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1053425.796239427, 1053425.796239427, 225013.9144379167], 
processed observation next is [1.0, 0.5652173913043478, 0.3357030015797787, 0.585, 1.0, 1.0, 0.5691597100462185, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2926182767331742, 0.2926182767331742, 0.3358416633401742], 
reward next is 0.6642, 
noisyNet noise sample is [array([0.20775267], dtype=float32), 0.05917534]. 
=============================================
[2019-04-28 00:04:55,616] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0309196e-20 9.9999988e-01 2.5958501e-31 1.0016953e-07 9.7714471e-30], sum to 1.0000
[2019-04-28 00:04:55,625] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4839
[2019-04-28 00:04:55,632] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.0, 1.0, 2.0, 0.5758176870902904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 947255.4411877558, 947255.4411877552, 210515.0070299353], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 655200.0000, 
sim time next is 655800.0000, 
raw observation next is [24.7, 53.0, 1.0, 2.0, 0.5379671656988837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 885088.8569347081, 885088.8569347075, 202939.2965670972], 
processed observation next is [1.0, 0.6086956521739131, 0.3696682464454976, 0.53, 1.0, 1.0, 0.4433339345769683, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2458580158151967, 0.24585801581519653, 0.3028944724882048], 
reward next is 0.6971, 
noisyNet noise sample is [array([0.36470434], dtype=float32), 0.119126834]. 
=============================================
[2019-04-28 00:05:00,317] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9849807e-18 1.0000000e+00 1.4252623e-28 5.1305243e-08 1.0003759e-26], sum to 1.0000
[2019-04-28 00:05:00,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2689
[2019-04-28 00:05:00,327] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.23333333333333, 49.66666666666666, 1.0, 2.0, 0.6109809502398384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1006422.373508957, 1006422.373508958, 217980.0518838072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 747600.0000, 
sim time next is 748200.0000, 
raw observation next is [25.16666666666667, 49.83333333333334, 1.0, 2.0, 0.6262301618591973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1031855.432564975, 1031855.432564975, 221349.0490915797], 
processed observation next is [1.0, 0.6521739130434783, 0.39178515007898923, 0.4983333333333334, 1.0, 1.0, 0.5496748938062618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.28662650904582637, 0.28662650904582637, 0.33037171506205926], 
reward next is 0.6696, 
noisyNet noise sample is [array([0.21974275], dtype=float32), -1.0100155]. 
=============================================
[2019-04-28 00:05:13,895] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.6858938e-17 1.0000000e+00 8.8309726e-29 6.5381500e-09 4.5597281e-27], sum to 1.0000
[2019-04-28 00:05:13,903] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2658
[2019-04-28 00:05:13,913] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.9, 93.0, 1.0, 2.0, 0.3803033263627877, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589522.8986981047, 589522.8986981041, 174136.7082311656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 976200.0000, 
sim time next is 976800.0000, 
raw observation next is [21.9, 93.0, 1.0, 2.0, 0.3449124497226351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 534645.4057453935, 534645.4057453941, 169474.8919889455], 
processed observation next is [1.0, 0.30434782608695654, 0.23696682464454974, 0.93, 1.0, 1.0, 0.21073789123209044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14851261270705374, 0.1485126127070539, 0.25294759998350075], 
reward next is 0.7471, 
noisyNet noise sample is [array([2.1741989], dtype=float32), 0.10148148]. 
=============================================
[2019-04-28 00:05:14,371] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-28 00:05:14,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:05:14,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:05:14,376] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:05:14,377] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:05:14,378] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:05:14,377] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:05:14,380] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:05:14,378] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:05:14,382] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:05:14,384] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:05:14,389] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run36
[2019-04-28 00:05:14,425] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run36
[2019-04-28 00:05:14,452] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run36
[2019-04-28 00:05:14,453] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run36
[2019-04-28 00:05:14,517] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run36
[2019-04-28 00:05:20,039] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:05:20,042] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [20.50298993, 76.58616857166666, 1.0, 2.0, 0.2545803065270565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 420117.6473921646, 420117.6473921652, 161321.4690810231]
[2019-04-28 00:05:20,043] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:05:20,046] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0446486e-18 1.0000000e+00 4.3515304e-30 2.2765608e-08 4.0820451e-28], sampled 0.003893363848252007
[2019-04-28 00:05:24,533] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:05:24,535] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.66666666666667, 67.33333333333334, 1.0, 2.0, 0.490256842976646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685540.5853380639, 685540.5853380639, 182145.4100899468]
[2019-04-28 00:05:24,537] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:05:24,541] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4363761e-19 1.0000000e+00 1.6833113e-31 9.7587387e-09 1.9629689e-29], sampled 0.7734213648695767
[2019-04-28 00:05:50,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:05:50,903] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.741696595, 96.17084313000001, 1.0, 2.0, 0.4986071964880549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696725.4512302433, 696725.4512302433, 183379.9497807233]
[2019-04-28 00:05:50,904] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:05:50,907] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6275404e-19 1.0000000e+00 2.0662948e-31 1.0297811e-08 2.3763887e-29], sampled 0.3300517294968094
[2019-04-28 00:05:53,760] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:05:53,762] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.71608857, 78.74639636, 1.0, 2.0, 0.5158855596107621, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720877.4493954248, 720877.4493954248, 186121.8658280396]
[2019-04-28 00:05:53,763] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:05:53,765] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.4153142e-20 1.0000000e+00 8.4300642e-32 8.1469222e-09 1.0291221e-29], sampled 0.7657322179392162
[2019-04-28 00:06:03,364] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:06:03,365] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [37.33665949, 58.44307114, 1.0, 2.0, 0.9470343236791219, 1.0, 2.0, 0.9470343236791219, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 2648973.053544916, 2648973.053544916, 498021.311544637]
[2019-04-28 00:06:03,366] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:06:03,369] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.09780655e-19 1.00000000e+00 3.11777237e-31 1.15039125e-08
 3.50142670e-29], sampled 0.45815248231307193
[2019-04-28 00:06:03,371] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2648973.053544916 W.
[2019-04-28 00:06:10,728] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06646924]
[2019-04-28 00:06:10,729] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 79.33333333333333, 1.0, 2.0, 0.5847457862090114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817136.9341858455, 817136.9341858455, 197938.400881651]
[2019-04-28 00:06:10,729] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:06:10,734] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2527203e-20 1.0000000e+00 3.2408682e-32 6.3491998e-09 4.2181631e-30], sampled 0.04180658624676392
[2019-04-28 00:06:39,744] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:06:40,366] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:06:40,374] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:06:40,417] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:06:40,484] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:06:41,499] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 875000, evaluation results [875000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:06:45,631] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0338232e-18 1.0000000e+00 7.5138141e-29 1.7333496e-09 1.0908046e-27], sum to 1.0000
[2019-04-28 00:06:45,640] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8000
[2019-04-28 00:06:45,644] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.65, 86.0, 1.0, 2.0, 0.3038511470227199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484043.942535425, 484043.942535425, 165914.981291909], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1117800.0000, 
sim time next is 1118400.0000, 
raw observation next is [21.53333333333333, 86.66666666666667, 1.0, 2.0, 0.3026724876742927, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 482481.6111709088, 482481.6111709088, 165806.939938598], 
processed observation next is [1.0, 0.9565217391304348, 0.21958925750394942, 0.8666666666666667, 1.0, 1.0, 0.15984637069191893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13402266976969687, 0.13402266976969687, 0.24747304468447462], 
reward next is 0.7525, 
noisyNet noise sample is [array([0.34493753], dtype=float32), -1.053918]. 
=============================================
[2019-04-28 00:06:47,325] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9298007e-16 1.0000000e+00 2.2113603e-26 5.5411458e-09 1.0589883e-26], sum to 1.0000
[2019-04-28 00:06:47,335] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4466
[2019-04-28 00:06:47,339] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 85.66666666666667, 1.0, 2.0, 0.3170925447827462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 506436.7575140374, 506436.7575140367, 167575.0214745703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1149600.0000, 
sim time next is 1150200.0000, 
raw observation next is [21.75, 85.0, 1.0, 2.0, 0.3143736639134423, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501437.1411009541, 501437.1411009541, 167196.2687153479], 
processed observation next is [1.0, 0.30434782608695654, 0.2298578199052133, 0.85, 1.0, 1.0, 0.1739441733896895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13928809475026502, 0.13928809475026502, 0.24954666972439984], 
reward next is 0.7505, 
noisyNet noise sample is [array([1.3187268], dtype=float32), -0.09511259]. 
=============================================
[2019-04-28 00:07:02,167] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5618340e-21 1.0000000e+00 5.9552128e-31 2.9503450e-12 2.1566748e-27], sum to 1.0000
[2019-04-28 00:07:02,175] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3475
[2019-04-28 00:07:02,179] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333333, 88.83333333333334, 1.0, 2.0, 0.4991752223904971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794473.4551694791, 794473.4551694791, 194404.6104285996], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1349400.0000, 
sim time next is 1350000.0000, 
raw observation next is [21.3, 89.0, 1.0, 2.0, 0.5133076278684294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 817199.4908224535, 817199.4908224535, 196970.023819039], 
processed observation next is [1.0, 0.6521739130434783, 0.2085308056872039, 0.89, 1.0, 1.0, 0.4136236480342523, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22699985856179264, 0.22699985856179264, 0.29398511017767015], 
reward next is 0.7060, 
noisyNet noise sample is [array([0.24296395], dtype=float32), -0.02868832]. 
=============================================
[2019-04-28 00:07:02,197] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[72.29645 ]
 [72.274765]
 [72.13139 ]
 [72.01301 ]
 [72.0371  ]], R is [[72.28343964]
 [72.27045441]
 [72.2537384 ]
 [72.22433472]
 [72.19718933]].
[2019-04-28 00:07:02,381] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.1817324e-21 1.0000000e+00 3.9499292e-33 5.1493296e-11 2.9568034e-33], sum to 1.0000
[2019-04-28 00:07:02,387] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3288
[2019-04-28 00:07:02,396] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.06666666666667, 94.66666666666666, 1.0, 2.0, 0.3253210251452163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 512586.8352017128, 512586.8352017128, 167943.1068103118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1370400.0000, 
sim time next is 1371000.0000, 
raw observation next is [21.03333333333333, 94.83333333333333, 1.0, 2.0, 0.3246202013418588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511649.2636062397, 511649.2636062391, 167874.5646775391], 
processed observation next is [1.0, 0.8695652173913043, 0.19589257503949445, 0.9483333333333333, 1.0, 1.0, 0.1862893992070588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1421247954461777, 0.14212479544617754, 0.2505590517575211], 
reward next is 0.7494, 
noisyNet noise sample is [array([-0.29186916], dtype=float32), -0.033357576]. 
=============================================
[2019-04-28 00:07:02,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[76.80563 ]
 [76.82967 ]
 [76.872574]
 [76.841064]
 [76.88479 ]], R is [[76.71783447]
 [76.69998932]
 [76.68254852]
 [76.66544342]
 [76.64848328]].
[2019-04-28 00:07:05,604] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9762273e-22 1.0000000e+00 1.2945478e-34 2.1397324e-12 7.1168535e-33], sum to 1.0000
[2019-04-28 00:07:05,613] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4583
[2019-04-28 00:07:05,619] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.28333333333333, 95.66666666666667, 1.0, 2.0, 0.3255316276889377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 509261.1217800908, 509261.1217800908, 167600.9652240079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1489800.0000, 
sim time next is 1490400.0000, 
raw observation next is [21.5, 95.0, 1.0, 2.0, 0.3303689384645563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 514964.5162376359, 514964.5162376359, 167991.731065452], 
processed observation next is [0.0, 0.2608695652173913, 0.21800947867298584, 0.95, 1.0, 1.0, 0.19321558851151358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14304569895489888, 0.14304569895489888, 0.2507339269633612], 
reward next is 0.7493, 
noisyNet noise sample is [array([-1.7461764], dtype=float32), 0.9092844]. 
=============================================
[2019-04-28 00:07:06,771] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.5401341e-22 1.0000000e+00 2.4262009e-36 1.0568273e-11 2.8794891e-31], sum to 1.0000
[2019-04-28 00:07:06,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7677
[2019-04-28 00:07:06,791] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 87.5, 1.0, 2.0, 0.3711971764614211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561635.2414356708, 561635.2414356708, 171336.9893165899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1416600.0000, 
sim time next is 1417200.0000, 
raw observation next is [23.66666666666667, 88.0, 1.0, 2.0, 0.379955787005837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 570553.6670177527, 570553.6670177534, 171980.2106871535], 
processed observation next is [0.0, 0.391304347826087, 0.3206951026856243, 0.88, 1.0, 1.0, 0.2529587795251048, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1584871297271535, 0.1584871297271537, 0.25668688162261716], 
reward next is 0.7433, 
noisyNet noise sample is [array([0.09084333], dtype=float32), 0.83596444]. 
=============================================
[2019-04-28 00:07:10,183] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2433177e-24 1.0000000e+00 2.6380920e-35 2.4293734e-13 1.9397403e-33], sum to 1.0000
[2019-04-28 00:07:10,189] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2309
[2019-04-28 00:07:10,193] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.36666666666667, 51.0, 1.0, 2.0, 0.3545495643181173, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 540696.9328101261, 540696.9328101267, 169702.3073243424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1514400.0000, 
sim time next is 1515000.0000, 
raw observation next is [29.43333333333333, 51.0, 1.0, 2.0, 0.356860421157582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 543278.6023775876, 543278.6023775876, 169886.0060121602], 
processed observation next is [0.0, 0.5217391304347826, 0.5939968404423379, 0.51, 1.0, 1.0, 0.2251330375392554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15091072288266322, 0.15091072288266322, 0.25356120300322416], 
reward next is 0.7464, 
noisyNet noise sample is [array([0.8251093], dtype=float32), -0.88514966]. 
=============================================
[2019-04-28 00:07:10,217] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[80.75554 ]
 [80.70711 ]
 [80.67012 ]
 [80.62894 ]
 [80.598526]], R is [[80.73416901]
 [80.67353821]
 [80.61371613]
 [80.55458832]
 [80.49612427]].
[2019-04-28 00:07:12,845] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9858936e-16 1.0000000e+00 5.5073647e-28 6.2146699e-10 7.3353735e-26], sum to 1.0000
[2019-04-28 00:07:12,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9389
[2019-04-28 00:07:12,860] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.26666666666667, 93.33333333333334, 1.0, 2.0, 0.7747718840818051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1148471.143816504, 1148471.143816504, 246683.8978784679], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1614000.0000, 
sim time next is 1614600.0000, 
raw observation next is [23.2, 94.0, 1.0, 2.0, 0.7547092638807567, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1118050.769086616, 1118050.769086617, 241571.1496205619], 
processed observation next is [1.0, 0.6956521739130435, 0.29857819905213273, 0.94, 1.0, 1.0, 0.7044689926274177, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31056965807961556, 0.31056965807961584, 0.3605539546575551], 
reward next is 0.6394, 
noisyNet noise sample is [array([1.1814893], dtype=float32), -0.2523612]. 
=============================================
[2019-04-28 00:07:13,637] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.2085130e-18 1.0000000e+00 2.9287241e-28 3.3092726e-10 8.1948299e-24], sum to 1.0000
[2019-04-28 00:07:13,645] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5368
[2019-04-28 00:07:13,649] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 84.0, 1.0, 2.0, 0.6905797084371784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1088208.295310962, 1088208.295310963, 233649.5504868588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1789200.0000, 
sim time next is 1789800.0000, 
raw observation next is [22.33333333333334, 84.5, 1.0, 2.0, 0.3927133440211353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 618773.9956941775, 618773.9956941782, 176834.4018436307], 
processed observation next is [1.0, 0.7391304347826086, 0.2575039494470777, 0.845, 1.0, 1.0, 0.2683293301459461, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17188166547060485, 0.17188166547060504, 0.2639319430501951], 
reward next is 0.7361, 
noisyNet noise sample is [array([1.4817241], dtype=float32), 0.78910816]. 
=============================================
[2019-04-28 00:07:13,836] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.7786136e-19 1.0000000e+00 2.3505470e-31 2.0736794e-11 1.4658852e-27], sum to 1.0000
[2019-04-28 00:07:13,844] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2090
[2019-04-28 00:07:13,851] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.3, 87.66666666666667, 1.0, 2.0, 0.3527663989668288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551096.573161713, 551096.573161713, 170923.8904187363], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1577400.0000, 
sim time next is 1578000.0000, 
raw observation next is [22.4, 87.33333333333334, 1.0, 2.0, 0.3288193058236363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512919.0675145621, 512919.0675145627, 167843.0005517647], 
processed observation next is [1.0, 0.2608695652173913, 0.2606635071090047, 0.8733333333333334, 1.0, 1.0, 0.19134856123329672, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14247751875404502, 0.14247751875404518, 0.2505119411220369], 
reward next is 0.7495, 
noisyNet noise sample is [array([0.00135393], dtype=float32), -0.07622617]. 
=============================================
[2019-04-28 00:07:13,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[69.189735]
 [69.2432  ]
 [69.278114]
 [69.2732  ]
 [69.28097 ]], R is [[69.27030182]
 [69.32248688]
 [69.37985992]
 [69.43691254]
 [69.49369812]].
[2019-04-28 00:07:17,410] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.9172325e-18 1.0000000e+00 3.7753754e-28 4.2370385e-08 1.1984396e-27], sum to 1.0000
[2019-04-28 00:07:17,420] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6061
[2019-04-28 00:07:17,423] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.55, 98.5, 1.0, 2.0, 0.4780335080428666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683266.0980582122, 683266.0980582122, 182182.5091285533], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665000.0000, 
sim time next is 1665600.0000, 
raw observation next is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
processed observation next is [1.0, 0.2608695652173913, 0.31595576619273325, 0.9833333333333334, 1.0, 1.0, 0.37126553384831656, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1898749121687438, 0.1898749121687438, 0.27196302475689255], 
reward next is 0.7280, 
noisyNet noise sample is [array([-0.00896351], dtype=float32), -0.5188261]. 
=============================================
[2019-04-28 00:07:20,974] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.0171316e-20 1.0000000e+00 1.7741178e-30 2.3931846e-12 3.2992278e-28], sum to 1.0000
[2019-04-28 00:07:20,984] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5844
[2019-04-28 00:07:20,992] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.2, 78.0, 1.0, 2.0, 0.5045633154050175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705050.9564022244, 705050.956402225, 184315.1357678146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1706400.0000, 
sim time next is 1707000.0000, 
raw observation next is [28.06666666666667, 78.66666666666667, 1.0, 2.0, 0.5064215132278067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707648.3718230554, 707648.371823056, 184609.1211600637], 
processed observation next is [1.0, 0.782608695652174, 0.529225908372828, 0.7866666666666667, 1.0, 1.0, 0.40532712437085144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19656899217307094, 0.1965689921730711, 0.27553600173143833], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.23567949], dtype=float32), 0.38377866]. 
=============================================
[2019-04-28 00:07:21,014] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.62374 ]
 [71.560524]
 [71.869446]
 [71.53384 ]
 [71.944626]], R is [[69.86946869]
 [69.89567566]
 [69.92323303]
 [69.95267487]
 [69.98388672]].
[2019-04-28 00:07:24,048] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5842285e-18 1.0000000e+00 8.2955152e-30 2.2844031e-12 3.6377323e-27], sum to 1.0000
[2019-04-28 00:07:24,055] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1714
[2019-04-28 00:07:24,062] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.96666666666667, 84.0, 1.0, 2.0, 0.7937308504470925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1195037.300128591, 1195037.300128591, 253907.9875041888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1766400.0000, 
sim time next is 1767000.0000, 
raw observation next is [23.88333333333333, 83.0, 1.0, 2.0, 0.7726479137351342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1171463.705653942, 1171463.705653941, 249418.0159064272], 
processed observation next is [1.0, 0.43478260869565216, 0.3309636650868877, 0.83, 1.0, 1.0, 0.7260818237772702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32540658490387275, 0.32540658490387253, 0.3722656953827272], 
reward next is 0.6277, 
noisyNet noise sample is [array([0.7351554], dtype=float32), -0.88387877]. 
=============================================
[2019-04-28 00:07:24,078] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[64.069176]
 [63.920628]
 [63.775703]
 [63.642002]
 [63.51563 ]], R is [[64.15510559]
 [64.13459015]
 [64.10713196]
 [64.07775879]
 [64.04701233]].
[2019-04-28 00:07:24,670] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5904100e-22 1.0000000e+00 3.8282528e-35 4.5618199e-14 9.9275129e-32], sum to 1.0000
[2019-04-28 00:07:24,675] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0124
[2019-04-28 00:07:24,682] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.93333333333333, 94.33333333333333, 1.0, 2.0, 0.3691017803113059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569129.1276547808, 569129.1276547813, 172286.3898561079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1824000.0000, 
sim time next is 1824600.0000, 
raw observation next is [21.91666666666666, 94.66666666666667, 1.0, 2.0, 0.3638138941523408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560595.237147645, 560595.237147645, 171545.0667072892], 
processed observation next is [1.0, 0.08695652173913043, 0.23775671406003138, 0.9466666666666668, 1.0, 1.0, 0.2335107158461937, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15572089920767915, 0.15572089920767915, 0.256037412995954], 
reward next is 0.7440, 
noisyNet noise sample is [array([-0.13928781], dtype=float32), -0.7483589]. 
=============================================
[2019-04-28 00:07:25,970] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4428176e-19 1.0000000e+00 2.9969931e-32 1.1158834e-14 3.1957670e-30], sum to 1.0000
[2019-04-28 00:07:25,978] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4165
[2019-04-28 00:07:25,984] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.98333333333333, 93.33333333333333, 1.0, 2.0, 0.4864381206033092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751619.0296594744, 751619.0296594744, 190077.4003858643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1822200.0000, 
sim time next is 1822800.0000, 
raw observation next is [21.96666666666667, 93.66666666666667, 1.0, 2.0, 0.4097134244071788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632575.5668060763, 632575.5668060763, 177993.3514750969], 
processed observation next is [1.0, 0.08695652173913043, 0.24012638230647723, 0.9366666666666668, 1.0, 1.0, 0.2888113547074443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1757154352239101, 0.1757154352239101, 0.26566171861954757], 
reward next is 0.7343, 
noisyNet noise sample is [array([-0.60798067], dtype=float32), 0.80827117]. 
=============================================
[2019-04-28 00:07:30,339] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.5579228e-24 1.0000000e+00 4.6265351e-35 6.6467106e-17 4.9171216e-31], sum to 1.0000
[2019-04-28 00:07:30,346] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1305
[2019-04-28 00:07:30,348] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.4583683932920895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648304.8621784303, 648304.8621784303, 178340.272852537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2089200.0000, 
sim time next is 2089800.0000, 
raw observation next is [23.9, 97.5, 1.0, 2.0, 0.4569873957359317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646711.1113267598, 646711.1113267604, 178184.7499755667], 
processed observation next is [0.0, 0.17391304347826086, 0.33175355450236965, 0.975, 1.0, 1.0, 0.3457679466697972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1796419753685444, 0.17964197536854454, 0.26594738802323387], 
reward next is 0.7341, 
noisyNet noise sample is [array([1.0692183], dtype=float32), 0.2745009]. 
=============================================
[2019-04-28 00:07:33,156] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-28 00:07:33,157] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:07:33,157] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:07:33,158] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:07:33,158] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:07:33,159] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:07:33,159] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:07:33,160] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:07:33,160] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:07:33,160] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:07:33,164] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:07:33,188] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run37
[2019-04-28 00:07:33,218] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run37
[2019-04-28 00:07:33,248] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run37
[2019-04-28 00:07:33,249] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run37
[2019-04-28 00:07:33,286] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run37
[2019-04-28 00:07:37,604] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:07:37,604] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.395070425, 95.56224435, 1.0, 2.0, 0.2417229115793518, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 399134.7954602707, 399134.79546027, 160051.3991709269]
[2019-04-28 00:07:37,607] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:07:37,610] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.5613856e-22 1.0000000e+00 1.5994994e-33 4.5114669e-15 2.1703046e-31], sampled 0.14909247485872612
[2019-04-28 00:07:54,744] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:07:54,746] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.68333333333333, 89.16666666666667, 1.0, 2.0, 0.7695749587760202, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1075552.171237476, 1075552.171237476, 236576.5791124694]
[2019-04-28 00:07:54,747] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:07:54,750] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1131499e-21 1.0000000e+00 4.6804688e-33 7.2147234e-15 5.9244873e-31], sampled 0.9760771301242914
[2019-04-28 00:07:56,080] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:07:56,081] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.08074664166666, 82.56164571999999, 1.0, 2.0, 0.4955900778299515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692508.1263168863, 692508.1263168863, 182908.5910636774]
[2019-04-28 00:07:56,083] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:07:56,085] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5486735e-22 1.0000000e+00 1.1718984e-33 3.9356467e-15 1.6226827e-31], sampled 0.42625916177053935
[2019-04-28 00:08:22,265] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:08:22,266] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [38.18912691333333, 52.524124275, 1.0, 2.0, 0.8839280464826743, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005986770625706, 6.9112, 168.9123159663, 2132540.457369674, 2065295.714982385, 429675.8162164388]
[2019-04-28 00:08:22,266] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:08:22,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6992806e-22 1.0000000e+00 2.5557867e-34 2.0162755e-15 3.9028558e-32], sampled 0.9003208773668109
[2019-04-28 00:08:22,269] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2132540.457369674 W.
[2019-04-28 00:08:28,775] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:08:28,776] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.0, 82.0, 1.0, 2.0, 0.6149954735722993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859425.630854698, 859425.630854698, 203572.4325710443]
[2019-04-28 00:08:28,778] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:08:28,781] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2582685e-22 1.0000000e+00 3.9643432e-34 2.4488591e-15 5.8872442e-32], sampled 0.9302446315299826
[2019-04-28 00:08:31,249] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06404402]
[2019-04-28 00:08:31,250] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.3, 84.0, 1.0, 2.0, 0.6010395577193197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839915.2202623155, 839915.2202623155, 200939.2161243957]
[2019-04-28 00:08:31,250] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:08:31,252] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4890470e-22 1.0000000e+00 2.0814097e-34 1.8470650e-15 3.2234095e-32], sampled 0.36366626050246054
[2019-04-28 00:10:06,907] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:10:08,229] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:10:08,691] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:10:09,414] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:10:10,161] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:10:11,177] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 900000, evaluation results [900000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:10:39,025] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3876777e-18 1.0000000e+00 2.6050248e-32 6.8467162e-11 5.6317532e-29], sum to 1.0000
[2019-04-28 00:10:39,026] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0189
[2019-04-28 00:10:39,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.48333333333333, 94.16666666666667, 1.0, 2.0, 0.4673698672905682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656852.309046034, 656852.309046034, 179131.1315331012], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2074200.0000, 
sim time next is 2074800.0000, 
raw observation next is [24.46666666666667, 94.33333333333334, 1.0, 2.0, 0.4670434045005027, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656258.9195113995, 656258.9195113995, 179065.532972666], 
processed observation next is [0.0, 0.0, 0.3586097946287521, 0.9433333333333335, 1.0, 1.0, 0.35788361988012374, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18229414430872207, 0.18229414430872207, 0.2672619895114418], 
reward next is 0.7327, 
noisyNet noise sample is [array([-0.47640526], dtype=float32), -0.06387681]. 
=============================================
[2019-04-28 00:10:51,666] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6832535e-16 1.0000000e+00 1.1824960e-26 7.1963768e-11 1.5235036e-23], sum to 1.0000
[2019-04-28 00:10:51,674] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2796
[2019-04-28 00:10:51,697] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 86.0, 1.0, 2.0, 0.5047112053019159, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705257.678776894, 705257.6787768946, 184337.4951966176], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2253000.0000, 
sim time next is 2253600.0000, 
raw observation next is [26.5, 86.0, 1.0, 2.0, 0.502926540079341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 702763.0537670986, 702763.0537670986, 184055.8480308753], 
processed observation next is [1.0, 0.08695652173913043, 0.4549763033175356, 0.86, 1.0, 1.0, 0.4011163133486036, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1952119593797496, 0.1952119593797496, 0.2747102209416049], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.26844898], dtype=float32), 0.707942]. 
=============================================
[2019-04-28 00:10:54,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.7590984e-15 1.0000000e+00 1.6057679e-23 5.0074927e-11 2.2712782e-23], sum to 1.0000
[2019-04-28 00:10:54,921] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9841
[2019-04-28 00:10:54,927] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 87.16666666666667, 1.0, 2.0, 0.6251125631082837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 873569.57527318, 873569.5752731793, 205508.0825212715], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2265000.0000, 
sim time next is 2265600.0000, 
raw observation next is [26.03333333333333, 86.33333333333334, 1.0, 2.0, 0.5980096843371455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835679.4958897944, 835679.4958897944, 200367.9193378125], 
processed observation next is [1.0, 0.21739130434782608, 0.4328593996840442, 0.8633333333333334, 1.0, 1.0, 0.5156743184784885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23213319330272067, 0.23213319330272067, 0.2990565960265858], 
reward next is 0.7009, 
noisyNet noise sample is [array([0.43460265], dtype=float32), -1.0580754]. 
=============================================
[2019-04-28 00:10:55,257] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1902266e-17 1.0000000e+00 3.9651439e-27 1.8795538e-12 1.7861885e-26], sum to 1.0000
[2019-04-28 00:10:55,273] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0792
[2019-04-28 00:10:55,280] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 85.0, 1.0, 2.0, 0.5176051644195403, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723281.1731181142, 723281.1731181135, 186400.2497881117], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2246400.0000, 
sim time next is 2247000.0000, 
raw observation next is [26.96666666666667, 85.16666666666667, 1.0, 2.0, 0.5171796552728332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722686.3810497116, 722686.3810497116, 186331.3946597172], 
processed observation next is [1.0, 0.0, 0.47709320695102697, 0.8516666666666667, 1.0, 1.0, 0.41828874129257004, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20074621695825323, 0.20074621695825323, 0.27810655919360777], 
reward next is 0.7219, 
noisyNet noise sample is [array([-0.07163161], dtype=float32), -0.9781662]. 
=============================================
[2019-04-28 00:10:55,315] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[59.849792]
 [61.67594 ]
 [61.72839 ]
 [61.75497 ]
 [61.810825]], R is [[58.48088837]
 [58.61787033]
 [58.75345993]
 [58.88754654]
 [59.01982117]].
[2019-04-28 00:11:01,415] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2613034e-14 1.0000000e+00 1.1676084e-21 4.4868624e-08 1.2328082e-20], sum to 1.0000
[2019-04-28 00:11:01,427] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5864
[2019-04-28 00:11:01,428] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3002828.585659872 W.
[2019-04-28 00:11:01,436] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.922921163035818, 6.9112, 168.9062027394669, 3002828.585659872, 2285107.470165816, 473773.0158055605], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2300400.0000, 
sim time next is 2301000.0000, 
raw observation next is [32.03333333333333, 64.83333333333334, 1.0, 2.0, 0.9374600383836638, 1.0, 1.0, 0.9374600383836638, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2622180.104013055, 2622180.104013055, 492340.0674185636], 
processed observation next is [1.0, 0.6521739130434783, 0.7172195892575038, 0.6483333333333334, 1.0, 1.0, 0.9246506486550166, 1.0, 0.5, 0.9246506486550166, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7283833622258487, 0.7283833622258487, 0.7348359215202441], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17528808], dtype=float32), -1.1131976]. 
=============================================
[2019-04-28 00:11:01,466] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[49.47    ]
 [49.288902]
 [49.219067]
 [48.996788]
 [49.13694 ]], R is [[49.30220795]
 [48.80918503]
 [48.66671371]
 [48.18004608]
 [47.698246  ]].
[2019-04-28 00:11:04,832] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.9180271e-17 1.0000000e+00 2.8303479e-26 5.6369553e-10 9.6102605e-25], sum to 1.0000
[2019-04-28 00:11:04,845] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8928
[2019-04-28 00:11:04,859] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2090806.794908013 W.
[2019-04-28 00:11:04,865] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.43333333333334, 63.83333333333334, 1.0, 2.0, 0.7476527250035099, 1.0, 2.0, 0.7476527250035099, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2090806.794908013, 2090806.794908013, 395196.8088689749], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2376600.0000, 
sim time next is 2377200.0000, 
raw observation next is [32.46666666666667, 63.66666666666667, 1.0, 2.0, 0.637807897567214, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999325662954689, 6.9112, 168.9124289061297, 1783385.122153511, 1720865.939083225, 371747.8729274618], 
processed observation next is [1.0, 0.5217391304347826, 0.7377567140600317, 0.6366666666666667, 1.0, 1.0, 0.563623972972547, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008812566295468916, 0.0, 0.829437354373645, 0.49538475615375305, 0.478018316412007, 0.554847571533525], 
reward next is 0.0045, 
noisyNet noise sample is [array([-0.4628835], dtype=float32), -1.6767046]. 
=============================================
[2019-04-28 00:11:05,750] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3316756e-15 1.0000000e+00 7.3684215e-23 5.5084416e-11 3.9859306e-22], sum to 1.0000
[2019-04-28 00:11:05,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8661
[2019-04-28 00:11:05,751] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1869494.13669565 W.
[2019-04-28 00:11:05,757] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.4, 88.0, 1.0, 2.0, 0.6685824020770216, 1.0, 2.0, 0.6685824020770216, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1869494.13669565, 1869494.136695649, 360958.2725571522], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2455200.0000, 
sim time next is 2455800.0000, 
raw observation next is [26.28333333333333, 88.16666666666667, 1.0, 2.0, 0.7024489065325809, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.97388044950269, 6.9112, 168.9125830449452, 1878555.686388639, 1834088.116534586, 386280.2997324673], 
processed observation next is [1.0, 0.43478260869565216, 0.4447077409162717, 0.8816666666666667, 1.0, 1.0, 0.6415047066657601, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0062680449502689585, 0.0, 0.8294381112657941, 0.5218210239968442, 0.5094689212596072, 0.5765377607947273], 
reward next is 0.1101, 
noisyNet noise sample is [array([0.13831027], dtype=float32), -0.5149703]. 
=============================================
[2019-04-28 00:11:07,584] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.0304002e-15 1.0000000e+00 3.7541875e-23 7.4617972e-11 2.3960785e-21], sum to 1.0000
[2019-04-28 00:11:07,588] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4982
[2019-04-28 00:11:07,592] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2678226.471999402 W.
[2019-04-28 00:11:07,595] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.06666666666667, 61.16666666666666, 1.0, 2.0, 0.9574758164806263, 1.0, 2.0, 0.9574758164806263, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2678226.471999402, 2678226.471999403, 503819.1570266919], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2389800.0000, 
sim time next is 2390400.0000, 
raw observation next is [33.1, 61.0, 1.0, 2.0, 0.6503495712218929, 1.0, 2.0, 0.6457648251252089, 1.0, 1.0, 1.03, 7.005093817592483, 6.9112, 170.5573041426782, 2709508.766594341, 2642248.80569654, 506104.0497598184], 
processed observation next is [1.0, 0.6956521739130435, 0.7677725118483413, 0.61, 1.0, 1.0, 0.578734423158907, 1.0, 1.0, 0.5732106326809746, 1.0, 0.5, 1.0365853658536586, 0.009389381759248305, 0.0, 0.8375144448122397, 0.7526413240539837, 0.7339580015823722, 0.7553791787459976], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.07478574], dtype=float32), 1.0963036]. 
=============================================
[2019-04-28 00:11:09,651] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.0511436e-18 1.0000000e+00 1.8154434e-28 6.0838047e-14 1.1683947e-25], sum to 1.0000
[2019-04-28 00:11:09,655] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0723
[2019-04-28 00:11:09,660] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 84.0, 1.0, 2.0, 0.9602712200657973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1342236.290751315, 1342236.290751314, 287054.3852909019], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2439000.0000, 
sim time next is 2439600.0000, 
raw observation next is [27.6, 84.0, 1.0, 2.0, 0.8848942044591367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1236815.347597194, 1236815.347597193, 265815.9295412017], 
processed observation next is [1.0, 0.21739130434782608, 0.5071090047393366, 0.84, 1.0, 1.0, 0.8613183186254659, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3435598187769983, 0.34355981877699804, 0.39674019334507715], 
reward next is 0.6033, 
noisyNet noise sample is [array([2.0235581], dtype=float32), 0.26272523]. 
=============================================
[2019-04-28 00:11:14,177] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7030375e-17 1.0000000e+00 2.1770585e-27 1.7999668e-12 4.0938408e-25], sum to 1.0000
[2019-04-28 00:11:14,183] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6856
[2019-04-28 00:11:14,197] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 95.33333333333333, 1.0, 2.0, 0.7050951351341217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985393.8444012653, 985393.8444012653, 221949.5710385862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2528400.0000, 
sim time next is 2529000.0000, 
raw observation next is [26.35, 95.0, 1.0, 2.0, 0.7298930664276032, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1020066.420432293, 1020066.420432294, 227430.54580256], 
processed observation next is [1.0, 0.2608695652173913, 0.4478672985781992, 0.95, 1.0, 1.0, 0.6745699595513291, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2833517834534147, 0.283351783453415, 0.33944857582471644], 
reward next is 0.6606, 
noisyNet noise sample is [array([0.04093632], dtype=float32), -0.6400099]. 
=============================================
[2019-04-28 00:11:14,229] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[59.800373]
 [59.757122]
 [59.813152]
 [59.877995]
 [59.69376 ]], R is [[59.90929031]
 [59.97893143]
 [60.04573441]
 [60.12244797]
 [60.17795181]].
[2019-04-28 00:11:15,846] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.8611934e-21 1.0000000e+00 2.1886714e-30 2.2891788e-16 1.2216798e-28], sum to 1.0000
[2019-04-28 00:11:15,851] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6270
[2019-04-28 00:11:15,857] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 89.0, 1.0, 2.0, 0.521605518401556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728873.0288200259, 728873.0288200252, 187049.960397948], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2584800.0000, 
sim time next is 2585400.0000, 
raw observation next is [26.3, 89.33333333333334, 1.0, 2.0, 0.5197959559566341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 726343.5460024896, 726343.546002489, 186755.3305043713], 
processed observation next is [1.0, 0.9565217391304348, 0.4454976303317536, 0.8933333333333334, 1.0, 1.0, 0.4214409107911254, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20176209611180268, 0.2017620961118025, 0.27873929926025565], 
reward next is 0.7213, 
noisyNet noise sample is [array([0.27984977], dtype=float32), 1.09319]. 
=============================================
[2019-04-28 00:11:19,397] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.9624713e-21 1.0000000e+00 2.8365558e-31 3.0380043e-14 2.6289579e-29], sum to 1.0000
[2019-04-28 00:11:19,399] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7848
[2019-04-28 00:11:19,427] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 92.0, 1.0, 2.0, 0.4307682945254191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626735.3034365972, 626735.3034365979, 176640.1461557788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2608800.0000, 
sim time next is 2609400.0000, 
raw observation next is [23.91666666666667, 92.0, 1.0, 2.0, 0.4300341013217571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626073.5303040537, 626073.5303040537, 176586.4412476636], 
processed observation next is [0.0, 0.17391304347826086, 0.3325434439178518, 0.92, 1.0, 1.0, 0.3132940979780206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17390931397334825, 0.17390931397334825, 0.26356185260845316], 
reward next is 0.7364, 
noisyNet noise sample is [array([-1.6246892], dtype=float32), 0.67682564]. 
=============================================
[2019-04-28 00:11:26,204] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5617597e-25 1.0000000e+00 1.5907704e-37 2.1534447e-19 1.5363821e-33], sum to 1.0000
[2019-04-28 00:11:26,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4162
[2019-04-28 00:11:26,212] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 100.0, 1.0, 2.0, 0.4157842069799606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610144.6145788797, 610144.6145788797, 175191.8476672653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2715600.0000, 
sim time next is 2716200.0000, 
raw observation next is [22.5, 100.0, 1.0, 2.0, 0.409181697380103, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 604254.4559888901, 604254.4559888901, 174749.847841923], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 1.0, 1.0, 1.0, 0.28817071973506386, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16784845999691392, 0.16784845999691392, 0.2608206684207806], 
reward next is 0.7392, 
noisyNet noise sample is [array([0.4409448], dtype=float32), -0.39994314]. 
=============================================
[2019-04-28 00:11:33,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.0206790e-23 1.0000000e+00 3.4327383e-34 1.6511499e-16 1.8410346e-32], sum to 1.0000
[2019-04-28 00:11:33,665] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0921
[2019-04-28 00:11:33,672] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.463536188504447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714057.5529147636, 714057.552914763, 186056.7008064638], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2859000.0000, 
sim time next is 2859600.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3977644377628135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 612691.2463478635, 612691.2463478629, 176141.2239813128], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2744149852564018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17019201287440652, 0.17019201287440636, 0.26289734922584], 
reward next is 0.7371, 
noisyNet noise sample is [array([1.305132], dtype=float32), 0.5081041]. 
=============================================
[2019-04-28 00:11:36,580] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-28 00:11:36,581] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:11:36,583] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:11:36,584] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:11:36,590] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:11:36,591] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:11:36,595] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:11:36,595] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:11:36,598] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:11:36,598] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:11:36,598] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:11:36,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run38
[2019-04-28 00:11:36,642] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run38
[2019-04-28 00:11:36,671] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run38
[2019-04-28 00:11:36,701] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run38
[2019-04-28 00:11:36,744] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run38
[2019-04-28 00:11:56,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:11:56,587] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.26666666666667, 85.0, 1.0, 2.0, 0.3571619915817976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 548519.0069484515, 548519.0069484508, 170474.6321809765]
[2019-04-28 00:11:56,588] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:11:56,590] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5053389e-23 1.0000000e+00 1.0032228e-34 6.7984433e-17 2.0334753e-32], sampled 0.5739858600070341
[2019-04-28 00:12:04,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:12:04,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.34014752333334, 83.66200757, 1.0, 2.0, 0.4358171411409837, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 641805.7530317566, 641805.7530317573, 178313.5023917481]
[2019-04-28 00:12:04,147] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:12:04,149] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4944159e-23 1.0000000e+00 2.2988089e-35 3.3723230e-17 5.1473178e-33], sampled 0.8860105037024157
[2019-04-28 00:12:09,354] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:12:09,355] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.5, 94.0, 1.0, 2.0, 0.4782334536441718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672195.1266441379, 672195.1266441385, 180764.6756520905]
[2019-04-28 00:12:09,355] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:12:09,357] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8266300e-23 1.0000000e+00 2.7864230e-35 3.6953328e-17 6.1592172e-33], sampled 0.8675605300635376
[2019-04-28 00:12:41,126] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:12:41,127] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.3, 55.66666666666667, 1.0, 2.0, 1.00555614006808, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.005993615227275, 6.9112, 168.9123924670287, 2302778.882118462, 2235529.253496183, 465023.9162586437]
[2019-04-28 00:12:41,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:12:41,131] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0612349e-22 1.0000000e+00 2.1294202e-34 9.7124616e-17 4.0982794e-32], sampled 0.3911655076850985
[2019-04-28 00:12:41,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2302778.882118462 W.
[2019-04-28 00:12:42,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:12:42,784] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.1, 79.5, 1.0, 2.0, 0.7620872689960962, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1065082.184280994, 1065082.184280994, 234819.3286671198]
[2019-04-28 00:12:42,785] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:12:42,787] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1924519e-23 1.0000000e+00 3.3591192e-35 4.0391302e-17 7.3313467e-33], sampled 0.5669830825683932
[2019-04-28 00:12:46,672] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:12:46,673] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.492021655, 82.37646465166667, 1.0, 2.0, 0.5630733944471339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 786840.2281450898, 786840.2281450898, 194067.5635085106]
[2019-04-28 00:12:46,674] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:12:46,677] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3337773e-23 1.0000000e+00 2.0760720e-35 3.2108058e-17 4.6812545e-33], sampled 0.9310328699305458
[2019-04-28 00:13:11,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06969979]
[2019-04-28 00:13:11,902] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.13333333333333, 64.16666666666667, 1.0, 2.0, 0.3529007719515778, 0.0, 2.0, 0.0, 1.0, 2.0, 0.5946563912622823, 6.911200000000001, 6.9112, 168.9129082451097, 986381.5326992698, 986381.5326992692, 238194.3239347101]
[2019-04-28 00:13:11,903] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:13:11,905] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.0375946e-24 1.0000000e+00 3.2922997e-36 1.3347919e-17 8.4068693e-34], sampled 0.73483086059978
[2019-04-28 00:13:13,159] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:13:13,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:13:14,055] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:13:14,055] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:13:14,212] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:13:15,225] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 925000, evaluation results [925000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:13:15,924] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.0158165e-22 1.0000000e+00 2.5341569e-34 1.8690006e-16 2.9758589e-33], sum to 1.0000
[2019-04-28 00:13:15,935] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7542
[2019-04-28 00:13:15,940] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 89.0, 1.0, 2.0, 0.4098329272822472, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 603852.4830929274, 603852.4830929274, 174672.3354776844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2836200.0000, 
sim time next is 2836800.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.4113506259202543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606088.3237429457, 606088.3237429451, 174881.5591191078], 
processed observation next is [1.0, 0.8695652173913043, 0.3364928909952607, 0.89, 1.0, 1.0, 0.29078388665090876, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1683578677063738, 0.16835786770637365, 0.2610172524165788], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.98773676], dtype=float32), -0.62584156]. 
=============================================
[2019-04-28 00:13:16,571] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1609142e-23 1.0000000e+00 3.6564977e-36 4.1230712e-17 6.4272640e-32], sum to 1.0000
[2019-04-28 00:13:16,595] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4763
[2019-04-28 00:13:16,600] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333333, 95.0, 1.0, 2.0, 0.3073100318811158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486690.1820198278, 486690.1820198271, 166057.2253637769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2947800.0000, 
sim time next is 2948400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3099701987121949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 490351.5515316635, 490351.5515316641, 166313.7923316537], 
processed observation next is [1.0, 0.13043478260869565, 0.19431279620853087, 0.94, 1.0, 1.0, 0.16863879362915046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13620876431435097, 0.13620876431435114, 0.248229540793513], 
reward next is 0.7518, 
noisyNet noise sample is [array([-0.47231895], dtype=float32), -0.17629677]. 
=============================================
[2019-04-28 00:13:23,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.06519396e-21 1.00000000e+00 1.21311384e-32 4.13086212e-14
 1.01944315e-30], sum to 1.0000
[2019-04-28 00:13:23,891] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7175
[2019-04-28 00:13:23,898] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3044939415586104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484889.0046768906, 484889.0046768906, 165973.4332609786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3046781921487323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485182.2398720158, 485182.2398720158, 165994.6212026615], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1622628821069064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13477284440889328, 0.13477284440889328, 0.24775316597412167], 
reward next is 0.7522, 
noisyNet noise sample is [array([0.07618792], dtype=float32), 1.0351787]. 
=============================================
[2019-04-28 00:13:28,383] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.5805488e-24 1.0000000e+00 9.6886161e-35 4.9312589e-19 3.9554953e-32], sum to 1.0000
[2019-04-28 00:13:28,403] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6125
[2019-04-28 00:13:28,406] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.5, 97.0, 1.0, 2.0, 0.3374536420638731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 168445.3945084656], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3051000.0000, 
sim time next is 3051600.0000, 
raw observation next is [21.66666666666667, 96.0, 1.0, 2.0, 0.3408426909329262, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 526683.8322111608, 526683.8322111614, 168784.6009892069], 
processed observation next is [1.0, 0.30434782608695654, 0.22590837282780438, 0.96, 1.0, 1.0, 0.2058345673890677, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1463010645031002, 0.14630106450310038, 0.251917314909264], 
reward next is 0.7481, 
noisyNet noise sample is [array([-0.2695984], dtype=float32), -0.39804062]. 
=============================================
[2019-04-28 00:13:33,780] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2395232e-20 1.0000000e+00 6.7554546e-31 3.3824872e-14 2.2395852e-28], sum to 1.0000
[2019-04-28 00:13:33,788] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9731
[2019-04-28 00:13:33,797] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.83333333333334, 84.0, 1.0, 2.0, 0.6496299206387757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 952143.2767183082, 952143.2767183089, 216070.7810334703], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3149400.0000, 
sim time next is 3150000.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.7332911583064474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073819.927410601, 1073819.927410601, 234747.6309682543], 
processed observation next is [1.0, 0.4782608695652174, 0.38388625592417064, 0.83, 1.0, 1.0, 0.6786640461523463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2982833131696114, 0.2982833131696114, 0.35036959846008103], 
reward next is 0.6496, 
noisyNet noise sample is [array([-1.7412951], dtype=float32), 1.0746739]. 
=============================================
[2019-04-28 00:13:33,812] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[66.04711]
 [65.76239]
 [65.31746]
 [65.42278]
 [65.61454]], R is [[65.88141632]
 [65.90010834]
 [65.91043854]
 [65.88259888]
 [65.86167908]].
[2019-04-28 00:13:38,207] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5148074e-21 1.0000000e+00 6.8461135e-34 4.8279384e-14 1.3498776e-31], sum to 1.0000
[2019-04-28 00:13:38,228] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6235
[2019-04-28 00:13:38,235] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.83333333333334, 1.0, 2.0, 0.5266959904319644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735988.7379402305, 735988.7379402305, 187884.2469297457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3271800.0000, 
sim time next is 3272400.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5223146229001158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729864.2466818137, 729864.2466818132, 187165.8974737502], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4244754492772479, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20274006852272602, 0.20274006852272589, 0.2793520857817167], 
reward next is 0.7206, 
noisyNet noise sample is [array([-0.47968563], dtype=float32), -0.88889784]. 
=============================================
[2019-04-28 00:13:38,967] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1536071e-22 1.0000000e+00 1.1749524e-31 1.7887896e-15 6.8043881e-31], sum to 1.0000
[2019-04-28 00:13:38,974] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1551
[2019-04-28 00:13:38,979] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5206777220900716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727576.1151952763, 727576.1151952758, 186899.1046222705], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3273000.0000, 
sim time next is 3273600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5199735503351353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726591.7946593971, 726591.7946593978, 186784.5782920096], 
processed observation next is [0.0, 0.9130434782608695, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4216548799218497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20183105407205476, 0.20183105407205496, 0.2787829526746412], 
reward next is 0.7212, 
noisyNet noise sample is [array([-1.2738103], dtype=float32), 1.1270784]. 
=============================================
[2019-04-28 00:13:49,526] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.1023526e-17 1.0000000e+00 4.3661929e-24 6.4582194e-13 1.7197655e-20], sum to 1.0000
[2019-04-28 00:13:49,534] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3080
[2019-04-28 00:13:49,538] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 89.0, 1.0, 2.0, 0.8402982449068611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1174449.165960978, 1174449.165960978, 254045.2440379111], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3394200.0000, 
sim time next is 3394800.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.8601459056015435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1202205.102822362, 1202205.102822362, 259212.7197674778], 
processed observation next is [1.0, 0.30434782608695654, 0.5260663507109005, 0.89, 1.0, 1.0, 0.831501091086197, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.33394586189510056, 0.33394586189510056, 0.38688465636936986], 
reward next is 0.6131, 
noisyNet noise sample is [array([-0.49039662], dtype=float32), -0.12427887]. 
=============================================
[2019-04-28 00:13:49,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.87415345e-18 1.00000000e+00 3.58843196e-27 1.36873265e-14
 2.31792772e-26], sum to 1.0000
[2019-04-28 00:13:49,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8388
[2019-04-28 00:13:49,843] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5023997800195541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 702026.7437834226, 702026.7437834232, 183972.8808807829], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3462600.0000, 
sim time next is 3463200.0000, 
raw observation next is [26.0, 89.0, 1.0, 2.0, 0.5016900063401247, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701034.6165113848, 701034.6165113841, 183861.1716263127], 
processed observation next is [1.0, 0.08695652173913043, 0.4312796208530806, 0.89, 1.0, 1.0, 0.39962651366280083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19473183791982912, 0.19473183791982893, 0.27441965914375027], 
reward next is 0.7256, 
noisyNet noise sample is [array([0.49016464], dtype=float32), -2.151993]. 
=============================================
[2019-04-28 00:13:53,337] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1988687e-13 1.0000000e+00 8.9160385e-21 5.7882471e-10 3.0624255e-19], sum to 1.0000
[2019-04-28 00:13:53,344] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4651
[2019-04-28 00:13:53,351] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2255454.005625474 W.
[2019-04-28 00:13:53,356] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.66666666666666, 66.0, 1.0, 2.0, 0.9717445389107514, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.98593948056551, 6.9112, 168.9125119144342, 2255454.005625474, 2202431.385339173, 455466.2731613651], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3494400.0000, 
sim time next is 3495000.0000, 
raw observation next is [30.83333333333334, 66.0, 1.0, 2.0, 1.007338462872557, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.987023913158579, 6.9112, 168.9124450637482, 2305273.58441525, 2251481.653638399, 466271.3733169692], 
processed observation next is [1.0, 0.43478260869565216, 0.6603475513428123, 0.66, 1.0, 1.0, 1.0088415215332012, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007582391315857873, 0.0, 0.8294374337149488, 0.6403537734486806, 0.6254115704551109, 0.6959274228611481], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21280244], dtype=float32), -2.389948]. 
=============================================
[2019-04-28 00:13:53,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[43.046844]
 [42.724808]
 [42.95363 ]
 [42.908348]
 [42.81941 ]], R is [[42.78956223]
 [42.36166763]
 [41.93805313]
 [41.51867294]
 [41.10348511]].
[2019-04-28 00:14:09,890] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.3932789e-20 1.0000000e+00 7.4450059e-34 5.9204752e-13 2.0441703e-31], sum to 1.0000
[2019-04-28 00:14:09,898] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9755
[2019-04-28 00:14:09,902] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.6032944792888155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843067.5828412719, 843067.5828412719, 201362.446010146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [33.16666666666666, 66.33333333333334, 1.0, 2.0, 0.6152069348073272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 859721.2570272072, 859721.2570272079, 203613.6697779665], 
processed observation next is [0.0, 0.43478260869565216, 0.7709320695102682, 0.6633333333333334, 1.0, 1.0, 0.5363938973582256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23881146028533531, 0.2388114602853355, 0.30390099966860673], 
reward next is 0.6961, 
noisyNet noise sample is [array([-0.8480532], dtype=float32), -0.55566984]. 
=============================================
[2019-04-28 00:14:12,133] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-28 00:14:12,139] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:14:12,142] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:14:12,143] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:14:12,144] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:14:12,144] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:14:12,145] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:14:12,146] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:14:12,147] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:14:12,147] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:14:12,148] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:14:12,165] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run39
[2019-04-28 00:14:12,200] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run39
[2019-04-28 00:14:12,201] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run39
[2019-04-28 00:14:12,267] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run39
[2019-04-28 00:14:12,301] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run39
[2019-04-28 00:14:28,098] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:14:28,101] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.3, 46.0, 1.0, 2.0, 0.4044598323938651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664054.7482684219, 664054.7482684219, 180128.1395707644]
[2019-04-28 00:14:28,102] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:14:28,104] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.4490536e-19 1.0000000e+00 4.4166862e-30 7.6385842e-12 9.0726997e-28], sampled 0.12240973753174567
[2019-04-28 00:14:46,410] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:14:46,482] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.51315369, 97.709525275, 1.0, 2.0, 0.2760637204887401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 447587.0162431282, 447587.0162431282, 163425.6861414627]
[2019-04-28 00:14:46,483] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:14:46,485] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2679323e-19 1.0000000e+00 2.2628900e-30 5.9268163e-12 4.8994165e-28], sampled 0.1364767269109145
[2019-04-28 00:14:56,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:14:56,063] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [36.21095120333334, 66.78367416, 1.0, 2.0, 0.7506416713192541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049078.080462183, 1049078.080462183, 232170.2271717057]
[2019-04-28 00:14:56,066] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:14:56,067] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8890444e-20 1.0000000e+00 5.0814590e-31 3.3634733e-12 1.2377604e-28], sampled 0.9821424179994384
[2019-04-28 00:15:01,465] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:15:01,466] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.5, 59.5, 1.0, 2.0, 0.616317412087149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861273.723291116, 861273.723291116, 203825.4037536614]
[2019-04-28 00:15:01,467] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:15:01,469] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.8911771e-20 1.0000000e+00 3.3893228e-31 2.8810660e-12 8.5215455e-29], sampled 0.5632185329213931
[2019-04-28 00:15:22,545] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:15:22,546] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.99117610666667, 83.88719737500001, 1.0, 2.0, 0.5798449630940059, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810285.8012957992, 810285.8012957992, 197051.1815162917]
[2019-04-28 00:15:22,547] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:15:22,548] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.8798595e-20 1.0000000e+00 5.0737496e-31 3.3612928e-12 1.2357977e-28], sampled 0.3666042708170658
[2019-04-28 00:15:26,950] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:15:26,953] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.03333333333333, 84.0, 1.0, 2.0, 0.3307689663698232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525896.6823120937, 525896.6823120937, 169043.9082156328]
[2019-04-28 00:15:26,954] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:15:26,959] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3873167e-19 1.0000000e+00 1.0335318e-30 4.4026388e-12 2.3803004e-28], sampled 0.17696646820385098
[2019-04-28 00:15:28,672] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.07405541]
[2019-04-28 00:15:28,673] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5147564680063219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 719299.1686335619, 719299.1686335619, 185939.4667979602]
[2019-04-28 00:15:28,674] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:15:28,675] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.9243625e-20 1.0000000e+00 2.6609191e-31 2.6318305e-12 6.8190505e-29], sampled 0.0984312381545237
[2019-04-28 00:15:38,535] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:15:38,799] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:15:38,860] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:15:38,900] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:15:39,024] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:15:40,034] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 950000, evaluation results [950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:15:42,175] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3450847e-20 1.0000000e+00 9.2603903e-30 1.2835463e-10 3.7734175e-28], sum to 1.0000
[2019-04-28 00:15:42,187] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3046
[2019-04-28 00:15:42,192] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 63.00000000000001, 1.0, 2.0, 0.6183279808014628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 864084.53857606, 864084.5385760607, 204210.7737611511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3842400.0000, 
sim time next is 3843000.0000, 
raw observation next is [34.0, 63.0, 1.0, 2.0, 0.6156361652512266, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 860321.3282730926, 860321.3282730932, 203695.9286405581], 
processed observation next is [0.0, 0.4782608695652174, 0.8104265402843602, 0.63, 1.0, 1.0, 0.5369110424713572, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23897814674252574, 0.2389781467425259, 0.3040237740903852], 
reward next is 0.6960, 
noisyNet noise sample is [array([0.70135945], dtype=float32), -1.3336545]. 
=============================================
[2019-04-28 00:15:42,238] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[65.53951 ]
 [65.50367 ]
 [65.499344]
 [65.45867 ]
 [65.43242 ]], R is [[65.61785889]
 [65.65688324]
 [65.69387054]
 [65.73352814]
 [65.77310181]].
[2019-04-28 00:15:48,421] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.5777930e-19 1.0000000e+00 9.4629087e-29 4.8802160e-12 1.9288342e-26], sum to 1.0000
[2019-04-28 00:15:48,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4800
[2019-04-28 00:15:48,431] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 56.0, 1.0, 2.0, 0.6044566303036362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844692.26445254, 844692.26445254, 201579.2819213372], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3935400.0000, 
sim time next is 3936000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5973702957298898, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834785.6409243108, 834785.6409243108, 200257.6478620151], 
processed observation next is [0.0, 0.5652173913043478, 0.8578199052132701, 0.56, 1.0, 1.0, 0.5149039707589033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23188490025675298, 0.23188490025675298, 0.29889201173435087], 
reward next is 0.7011, 
noisyNet noise sample is [array([1.3178233], dtype=float32), 1.1567636]. 
=============================================
[2019-04-28 00:15:48,445] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[66.63201 ]
 [66.68271 ]
 [66.67953 ]
 [66.687645]
 [66.68093 ]], R is [[66.66858673]
 [66.70103455]
 [66.73583221]
 [66.77051544]
 [66.80503845]].
[2019-04-28 00:15:48,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.9169014e-17 1.0000000e+00 1.3044824e-26 1.5370516e-09 7.4728139e-24], sum to 1.0000
[2019-04-28 00:15:48,620] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0917
[2019-04-28 00:15:48,629] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 74.33333333333333, 1.0, 2.0, 0.672243369778756, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939462.105483694, 939462.1054836946, 214975.247298513], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3957000.0000, 
sim time next is 3957600.0000, 
raw observation next is [32.0, 73.66666666666667, 1.0, 2.0, 0.6309860994818122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 881781.0125658162, 881781.0125658168, 206660.805679261], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.7366666666666667, 1.0, 1.0, 0.5554049391347134, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24493917015717115, 0.24493917015717132, 0.30844896370038954], 
reward next is 0.6916, 
noisyNet noise sample is [array([0.02798018], dtype=float32), -0.0224505]. 
=============================================
[2019-04-28 00:15:49,597] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6979498e-16 1.0000000e+00 3.1211020e-25 1.1121953e-09 2.5674502e-22], sum to 1.0000
[2019-04-28 00:15:49,604] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1888
[2019-04-28 00:15:49,609] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2992056.205917703 W.
[2019-04-28 00:15:49,620] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.0, 60.0, 1.0, 2.0, 0.784869115293377, 1.0, 2.0, 0.7130245971609511, 1.0, 2.0, 1.03, 7.005104424961243, 6.9112, 170.5573041426782, 2992056.205917703, 2924788.646530178, 549474.6496742095], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4028400.0000, 
sim time next is 4029000.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.9911047071088034, 1.0, 2.0, 0.9911047071088034, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2772396.705312191, 2772396.705312191, 523636.822046319], 
processed observation next is [1.0, 0.6521739130434783, 0.8104265402843602, 0.6, 1.0, 1.0, 0.9892827796491607, 1.0, 1.0, 0.9892827796491607, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.770110195920053, 0.770110195920053, 0.7815474955915209], 
reward next is 0.2185, 
noisyNet noise sample is [array([0.47088653], dtype=float32), 0.32179436]. 
=============================================
[2019-04-28 00:15:49,638] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[55.76886 ]
 [54.538776]
 [54.054237]
 [53.74819 ]
 [53.08419 ]], R is [[55.66069794]
 [55.10409164]
 [54.55305099]
 [54.00752258]
 [53.46744919]].
[2019-04-28 00:15:53,512] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [9.4700947e-14 1.0000000e+00 3.0036596e-21 1.5158948e-08 4.2429265e-19], sum to 1.0000
[2019-04-28 00:15:53,520] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5555
[2019-04-28 00:15:53,526] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2761606.99753603 W.
[2019-04-28 00:15:53,531] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.66666666666666, 73.66666666666666, 1.0, 2.0, 0.6751555887106293, 1.0, 2.0, 0.6581678338695772, 1.0, 2.0, 1.03, 7.005095773279052, 6.9112, 170.5573041426782, 2761606.99753603, 2694345.635700387, 513597.167112457], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4099200.0000, 
sim time next is 4099800.0000, 
raw observation next is [31.83333333333333, 72.33333333333334, 1.0, 2.0, 0.7193724530912041, 1.0, 2.0, 0.6802762660598646, 1.0, 2.0, 1.03, 7.005099259705597, 6.9112, 170.5573041426782, 2854477.772909737, 2787213.913604942, 527518.2525220084], 
processed observation next is [1.0, 0.43478260869565216, 0.7077409162717218, 0.7233333333333334, 1.0, 1.0, 0.6618945217966314, 1.0, 1.0, 0.6147906819998368, 1.0, 1.0, 1.0365853658536586, 0.009389925970559699, 0.0, 0.8375144448122397, 0.792910492474927, 0.774226087112484, 0.7873406754059826], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94296694], dtype=float32), -0.021021187]. 
=============================================
[2019-04-28 00:16:05,399] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8271066e-15 1.0000000e+00 5.0000872e-22 4.9793729e-09 3.9796748e-20], sum to 1.0000
[2019-04-28 00:16:05,406] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0257
[2019-04-28 00:16:05,410] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 0.6077134616791527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 849245.3125225074, 849245.3125225074, 202192.033350096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4226400.0000, 
sim time next is 4227000.0000, 
raw observation next is [31.83333333333334, 71.66666666666667, 1.0, 2.0, 0.6070549292664559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 848324.6830787209, 848324.6830787209, 202067.8425434175], 
processed observation next is [1.0, 0.9565217391304348, 0.7077409162717223, 0.7166666666666667, 1.0, 1.0, 0.526572203935489, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23564574529964472, 0.23564574529964472, 0.30159379484092164], 
reward next is 0.6984, 
noisyNet noise sample is [array([-0.70155966], dtype=float32), -0.7121704]. 
=============================================
[2019-04-28 00:16:05,424] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[46.070618]
 [46.245377]
 [46.262733]
 [46.61713 ]
 [47.64511 ]], R is [[46.17202377]
 [46.40852356]
 [46.64175797]
 [46.87171936]
 [47.09841919]].
[2019-04-28 00:16:05,614] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.39770183e-11 9.99999881e-01 1.80057049e-16 1.05001796e-07
 6.10610060e-14], sum to 1.0000
[2019-04-28 00:16:05,622] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4037
[2019-04-28 00:16:05,641] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3448819.560622787 W.
[2019-04-28 00:16:05,645] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 52.0, 1.0, 2.0, 1.00226770812773, 1.0, 2.0, 0.8217238935781276, 1.0, 2.0, 1.03, 7.00512157779695, 6.9112, 170.5573041426782, 3448819.560622787, 3381539.713961313, 633824.2240929308], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4284000.0000, 
sim time next is 4284600.0000, 
raw observation next is [38.0, 51.83333333333334, 1.0, 2.0, 0.9688782843194411, 1.0, 2.0, 0.8050291816739832, 1.0, 2.0, 1.03, 7.005118942542898, 6.9112, 170.5573041426782, 3378656.152039642, 3311378.193117841, 619721.5714928573], 
processed observation next is [1.0, 0.6086956521739131, 1.0, 0.5183333333333334, 1.0, 1.0, 0.9625039570113748, 1.0, 1.0, 0.7650953996072086, 1.0, 1.0, 1.0365853658536586, 0.009391894254289834, 0.0, 0.8375144448122397, 0.9385155977887895, 0.9198272758660669, 0.9249575693923243], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.57527137], dtype=float32), -0.07692771]. 
=============================================
[2019-04-28 00:16:06,060] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [7.6279287e-19 1.0000000e+00 9.5362585e-28 3.6050454e-13 1.4909156e-24], sum to 1.0000
[2019-04-28 00:16:06,068] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3780
[2019-04-28 00:16:06,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([-1.0864928], dtype=float32), 2.3339255]. 
=============================================
[2019-04-28 00:16:11,799] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7422419e-10 9.9997914e-01 7.9486558e-17 2.0859297e-05 4.7618180e-14], sum to 1.0000
[2019-04-28 00:16:11,811] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2756
[2019-04-28 00:16:11,817] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2644694.643342066 W.
[2019-04-28 00:16:11,820] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 65.5, 1.0, 2.0, 0.9455007390894771, 1.0, 2.0, 0.9455007390894771, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2644694.643342066, 2644694.643342066, 496929.525050916], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4372200.0000, 
sim time next is 4372800.0000, 
raw observation next is [33.33333333333333, 62.33333333333333, 1.0, 2.0, 0.9897381369826543, 1.0, 2.0, 0.9897381369826543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2768569.791056545, 2768569.791056545, 522817.7214127405], 
processed observation next is [1.0, 0.6086956521739131, 0.7788309636650866, 0.6233333333333333, 1.0, 1.0, 0.9876363096176558, 1.0, 1.0, 0.9876363096176558, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7690471641823736, 0.7690471641823736, 0.7803249573324484], 
reward next is 0.2197, 
noisyNet noise sample is [array([-0.60908276], dtype=float32), 1.4194645]. 
=============================================
[2019-04-28 00:16:15,934] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8460181e-16 9.9999893e-01 5.3932013e-27 1.0163656e-06 8.3833501e-20], sum to 1.0000
[2019-04-28 00:16:15,941] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1642
[2019-04-28 00:16:15,947] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4890695399877618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683393.7833797967, 683393.7833797967, 181900.7411829323], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4508400.0000, 
sim time next is 4509000.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4916380989483277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686984.0802107382, 686984.0802107382, 182295.9628613867], 
processed observation next is [0.0, 0.17391304347826086, 0.4312796208530806, 0.865, 1.0, 1.0, 0.3875157818654551, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1908289111696495, 0.1908289111696495, 0.27208352665878616], 
reward next is 0.7279, 
noisyNet noise sample is [array([-1.0297406], dtype=float32), 0.30755138]. 
=============================================
[2019-04-28 00:16:15,964] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[69.075966]
 [69.20596 ]
 [69.235535]
 [69.16853 ]
 [69.05883 ]], R is [[69.06337738]
 [69.10124969]
 [69.13921356]
 [69.17687225]
 [69.21317291]].
[2019-04-28 00:16:17,297] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.7589879e-16 9.9999225e-01 6.5865638e-24 7.7451614e-06 1.1888678e-20], sum to 1.0000
[2019-04-28 00:16:17,302] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7761
[2019-04-28 00:16:17,306] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 70.16666666666667, 1.0, 2.0, 0.6040661626796666, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844146.3922874386, 844146.3922874386, 201505.179232095], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4457400.0000, 
sim time next is 4458000.0000, 
raw observation next is [31.33333333333334, 69.33333333333334, 1.0, 2.0, 0.5876119191998253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 821143.6811967702, 821143.6811967709, 198459.8766766262], 
processed observation next is [0.0, 0.6086956521739131, 0.6840442338072673, 0.6933333333333335, 1.0, 1.0, 0.5031468906021991, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22809546699910282, 0.22809546699910302, 0.2962087711591436], 
reward next is 0.7038, 
noisyNet noise sample is [array([-0.16868956], dtype=float32), -0.273303]. 
=============================================
[2019-04-28 00:16:17,326] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[63.24565 ]
 [63.25212 ]
 [63.220043]
 [63.212887]
 [63.184093]], R is [[63.33112717]
 [63.39706039]
 [63.46087646]
 [63.5237236 ]
 [63.58615494]].
[2019-04-28 00:16:24,323] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.2592848e-11 9.9999940e-01 7.5013688e-17 5.4484764e-07 3.9354062e-14], sum to 1.0000
[2019-04-28 00:16:24,331] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1717
[2019-04-28 00:16:24,338] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 74.0, 1.0, 2.0, 0.4745648029814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669080.7530330656, 669080.753033065, 180474.7162654271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4651800.0000, 
sim time next is 4652400.0000, 
raw observation next is [26.66666666666667, 78.0, 1.0, 2.0, 0.4718766926789524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665992.9614055092, 665992.9614055092, 180161.2066178872], 
processed observation next is [1.0, 0.8695652173913043, 0.4628751974723541, 0.78, 1.0, 1.0, 0.3637068586493402, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18499804483486365, 0.18499804483486365, 0.2688973233102794], 
reward next is 0.7311, 
noisyNet noise sample is [array([0.54690427], dtype=float32), -0.43994033]. 
=============================================
[2019-04-28 00:16:26,816] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.3920385e-16 9.9999988e-01 5.8919829e-25 1.4656538e-07 9.1034126e-21], sum to 1.0000
[2019-04-28 00:16:26,826] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4250
[2019-04-28 00:16:26,829] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.5589505377687268, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 781076.8191990427, 781076.8191990422, 193348.4007264184], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4645200.0000, 
sim time next is 4645800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5595439798008855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781906.399618737, 781906.399618737, 193451.758029752], 
processed observation next is [1.0, 0.782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4693300961456452, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21719622211631584, 0.21719622211631584, 0.2887339672085851], 
reward next is 0.7113, 
noisyNet noise sample is [array([-0.44371033], dtype=float32), -0.34866688]. 
=============================================
[2019-04-28 00:16:31,861] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 00:16:31,862] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:16:31,863] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:16:31,863] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:16:31,864] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:16:31,864] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:16:31,865] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:16:31,864] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:16:31,866] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:16:31,872] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:16:31,873] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:16:31,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run40
[2019-04-28 00:16:31,916] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run40
[2019-04-28 00:16:31,945] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run40
[2019-04-28 00:16:31,971] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run40
[2019-04-28 00:16:31,998] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run40
[2019-04-28 00:16:38,441] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.070586145]
[2019-04-28 00:16:38,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [18.55, 87.33333333333333, 1.0, 2.0, 0.2453003539403731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 407695.6163674646, 407695.6163674646, 160130.3803430362]
[2019-04-28 00:16:38,442] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:16:38,445] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8375837e-13 9.9189466e-01 5.0429300e-22 8.1053339e-03 1.3321751e-17], sampled 0.1739936262442464
[2019-04-28 00:16:46,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.070586145]
[2019-04-28 00:16:46,426] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.9, 41.0, 1.0, 2.0, 0.3184922318531092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 525636.508197284, 525636.5081972833, 168443.4953526074]
[2019-04-28 00:16:46,427] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:16:46,430] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9648785e-13 9.9185377e-01 5.4425071e-22 8.1462329e-03 1.4153108e-17], sampled 0.5181663094826439
[2019-04-28 00:17:00,337] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.070586145]
[2019-04-28 00:17:00,339] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.90498991, 84.781252225, 1.0, 2.0, 0.5566283454666936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777830.6022745828, 777830.6022745835, 192943.3117907465]
[2019-04-28 00:17:00,340] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:17:00,343] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.2250521e-13 9.9127978e-01 9.9049809e-22 8.7201837e-03 2.2792705e-17], sampled 0.2686711270534262
[2019-04-28 00:17:28,380] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.070586145]
[2019-04-28 00:17:28,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.1, 43.0, 1.0, 2.0, 0.9435153599820273, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.000380912186138, 6.9112, 168.9123523109868, 2215944.032982879, 2152676.250816759, 446621.3989014798]
[2019-04-28 00:17:28,381] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:17:28,383] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.8523131e-13 9.9046600e-01 2.8794018e-21 9.5340423e-03 5.2996387e-17], sampled 0.6090001775898881
[2019-04-28 00:17:28,383] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2215944.032982879 W.
[2019-04-28 00:18:04,999] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.070586145]
[2019-04-28 00:18:05,001] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.2, 88.66666666666667, 1.0, 2.0, 0.5376331844118226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 751277.4402921482, 751277.4402921476, 189702.8160649922]
[2019-04-28 00:18:05,001] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:18:05,003] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2085423e-13 9.9071383e-01 1.9087494e-21 9.2861047e-03 3.8347087e-17], sampled 0.7244590835276584
[2019-04-28 00:18:05,263] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8168.1125 2931992093.5834 1313.0000
[2019-04-28 00:18:05,616] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7925.0279 3011364501.8498 1729.0000
[2019-04-28 00:18:05,748] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8385.8354 2848540929.5074 1115.0000
[2019-04-28 00:18:05,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7806.1250 3167804504.4806 1756.0000
[2019-04-28 00:18:05,988] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8568.4618 2783993072.1433 913.0000
[2019-04-28 00:18:07,004] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 975000, evaluation results [975000.0, 7806.124988445821, 3167804504.480573, 1756.0, 8168.112472454803, 2931992093.583417, 1313.0, 8568.461801816004, 2783993072.1432877, 913.0, 7925.027877015094, 3011364501.849797, 1729.0, 8385.835375957826, 2848540929.507418, 1115.0]
[2019-04-28 00:18:09,878] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7886800e-11 9.5023972e-01 1.2697416e-18 4.9760278e-02 2.6265299e-15], sum to 1.0000
[2019-04-28 00:18:09,879] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9950
[2019-04-28 00:18:09,892] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4869074997046324, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 680371.7222408339, 680371.7222408332, 181569.8228298847], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4838400.0000, 
sim time next is 4839000.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.2436105577165527, 1.0, 1.0, 0.2436105577165527, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 680807.9848500375, 680807.9848500375, 239906.8058266123], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.08868741893560567, 1.0, 0.5, 0.08868741893560567, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18911332912501042, 0.18911332912501042, 0.3580698594427049], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.06786824], dtype=float32), -2.7452652]. 
=============================================
[2019-04-28 00:18:09,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[49.488323]
 [50.97665 ]
 [51.81107 ]
 [51.66919 ]
 [51.91573 ]], R is [[47.72403717]
 [47.24679565]
 [47.41603851]
 [47.67040634]
 [47.92202759]].
[2019-04-28 00:18:13,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6872395e-08 7.8925043e-01 1.4965070e-14 2.1074957e-01 3.3143932e-11], sum to 1.0000
[2019-04-28 00:18:13,328] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9242
[2019-04-28 00:18:13,335] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.6424179511760737, 1.0, 2.0, 0.6424179511760737, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1796271.616554048, 1796271.616554048, 350444.3244846634], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4870200.0000, 
sim time next is 4870800.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.6456467086087625, 1.0, 2.0, 0.6456467086087625, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1805307.184545545, 1805307.184545545, 351720.5318158112], 
processed observation next is [1.0, 0.391304347826087, 0.5734597156398105, 0.74, 1.0, 1.0, 0.573068323625015, 1.0, 1.0, 0.573068323625015, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.501474217929318, 0.501474217929318, 0.5249560176355391], 
reward next is 0.4750, 
noisyNet noise sample is [array([-0.62174773], dtype=float32), -0.53667575]. 
=============================================
[2019-04-28 00:18:50,940] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.2407596e-07 9.8317116e-01 2.6683142e-11 1.6828505e-02 2.8801695e-08], sum to 1.0000
[2019-04-28 00:18:50,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8441
[2019-04-28 00:18:50,953] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3281562.967058653 W.
[2019-04-28 00:18:50,958] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.78333333333333, 65.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.430226992242127, 6.9112, 170.5573041426782, 3281562.967058653, 2909762.819847434, 550852.9089226453], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5482200.0000, 
sim time next is 5482800.0000, 
raw observation next is [35.0, 65.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 8.062622219092885, 6.9112, 170.5573041426782, 3735101.204948216, 2910290.632550673, 546884.5493140612], 
processed observation next is [1.0, 0.4782608695652174, 0.8578199052132701, 0.65, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.11514222190928854, 0.0, 0.8375144448122397, 1.0375281124856157, 0.8084140645974092, 0.8162455959911361], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48913768], dtype=float32), -0.9088811]. 
=============================================
[2019-04-28 00:18:52,154] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5278815e-07 9.4868249e-01 6.9946965e-11 5.1316805e-02 1.1299339e-07], sum to 1.0000
[2019-04-28 00:18:52,165] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1107
[2019-04-28 00:18:52,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 4095824.36670003 W.
[2019-04-28 00:18:52,184] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.53333333333333, 71.33333333333333, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 9.46342065626942, 6.9112, 168.8981941629773, 4095824.36670003, 2285349.434912744, 468734.7542572299], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5476200.0000, 
sim time next is 5476800.0000, 
raw observation next is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.8389878400305231, 1.0, 1.0, 0.7400839595295241, 1.0, 2.0, 1.03, 7.005108693778093, 6.9112, 170.5573041426782, 3105746.248262539, 3038475.630947861, 568823.4135846412], 
processed observation next is [1.0, 0.391304347826087, 0.7519747235387049, 0.7066666666666667, 1.0, 1.0, 0.8060094458199073, 1.0, 0.5, 0.6868481440114748, 1.0, 1.0, 1.0365853658536586, 0.009390869377809263, 0.0, 0.8375144448122397, 0.8627072911840387, 0.844021008596628, 0.8489901695293152], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52021307], dtype=float32), -1.4343817]. 
=============================================
[2019-04-28 00:18:59,514] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7308401e-12 2.6091034e-04 2.3367627e-19 9.9973911e-01 1.1436396e-13], sum to 1.0000
[2019-04-28 00:18:59,523] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1581
[2019-04-28 00:18:59,528] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.3, 95.0, 1.0, 2.0, 0.2719971891045617, 1.0, 2.0, 0.2719971891045617, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 760166.9986023525, 760166.9986023525, 244697.1739996861], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5536800.0000, 
sim time next is 5537400.0000, 
raw observation next is [26.26666666666667, 95.0, 1.0, 2.0, 0.5903575758837173, 1.0, 2.0, 0.5903575758837173, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1650592.941996197, 1650592.941996196, 330717.5629641655], 
processed observation next is [1.0, 0.08695652173913043, 0.44391785150079005, 0.95, 1.0, 1.0, 0.5064549107032739, 1.0, 1.0, 0.5064549107032739, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4584980394433881, 0.4584980394433878, 0.49360830293159036], 
reward next is 0.5064, 
noisyNet noise sample is [array([0.3806505], dtype=float32), -0.76527685]. 
=============================================
[2019-04-28 00:18:59,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.0394285e-12 6.0974583e-03 6.1044952e-19 9.9390256e-01 1.1375339e-16], sum to 1.0000
[2019-04-28 00:18:59,856] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.4287
[2019-04-28 00:18:59,874] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.26666666666667, 71.66666666666667, 1.0, 2.0, 0.2739499241078455, 1.0, 2.0, 0.2739499241078455, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 765626.3730992076, 765626.3730992076, 245046.0677085196], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5596800.0000, 
sim time next is 5597400.0000, 
raw observation next is [29.98333333333333, 73.33333333333333, 1.0, 2.0, 0.2742032889137582, 1.0, 2.0, 0.2742032889137582, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 766334.7218175835, 766334.7218175842, 245091.2273184185], 
processed observation next is [1.0, 0.782608695652174, 0.6200631911532385, 0.7333333333333333, 1.0, 1.0, 0.12554613122139544, 1.0, 1.0, 0.12554613122139544, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.21287075606043987, 0.21287075606044006, 0.36580780196778884], 
reward next is 0.6342, 
noisyNet noise sample is [array([0.27905086], dtype=float32), 0.54228944]. 
=============================================
[2019-04-28 00:19:01,547] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.5613853e-13 2.8860811e-04 1.5916678e-18 9.9971133e-01 2.3004667e-15], sum to 1.0000
[2019-04-28 00:19:01,555] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7841
[2019-04-28 00:19:01,561] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.26666666666667, 89.5, 1.0, 2.0, 0.2750375920941911, 1.0, 2.0, 0.2750375920941911, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 768667.2416173766, 768667.2416173766, 245238.702737535], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5529000.0000, 
sim time next is 5529600.0000, 
raw observation next is [27.2, 90.0, 1.0, 2.0, 0.274899522728642, 1.0, 2.0, 0.274899522728642, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 768281.2310774713, 768281.2310774713, 245214.0944210374], 
processed observation next is [1.0, 0.0, 0.4881516587677725, 0.9, 1.0, 1.0, 0.12638496714294215, 1.0, 1.0, 0.12638496714294215, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21341145307707535, 0.21341145307707535, 0.3659911857030409], 
reward next is 0.6340, 
noisyNet noise sample is [array([-2.253202], dtype=float32), 0.65808314]. 
=============================================
[2019-04-28 00:19:04,851] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.0767489e-17 1.4851922e-04 2.6413987e-25 9.9985147e-01 1.1693610e-20], sum to 1.0000
[2019-04-28 00:19:04,865] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9938
[2019-04-28 00:19:04,869] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.5, 1.0, 2.0, 0.2782813413307666, 1.0, 2.0, 0.2782813413307666, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 777736.0666104946, 777736.0666104946, 245823.2203413885], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5661000.0000, 
sim time next is 5661600.0000, 
raw observation next is [32.06666666666667, 63.0, 1.0, 2.0, 0.2772470477042456, 1.0, 2.0, 0.2772470477042456, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 774844.3958254545, 774844.3958254545, 245636.7161286669], 
processed observation next is [0.0, 0.5217391304347826, 0.7187993680884678, 0.63, 1.0, 1.0, 0.12921331048704285, 1.0, 1.0, 0.12921331048704285, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2152345543959596, 0.2152345543959596, 0.36662196437114464], 
reward next is 0.6334, 
noisyNet noise sample is [array([0.1544334], dtype=float32), -1.3926637]. 
=============================================
[2019-04-28 00:19:08,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.0270099e-13 6.8025817e-03 2.4362697e-21 9.9319744e-01 1.5715497e-17], sum to 1.0000
[2019-04-28 00:19:08,169] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6074
[2019-04-28 00:19:08,175] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 65.0, 1.0, 2.0, 0.2792670064660308, 1.0, 2.0, 0.2792670064660308, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 780491.7894596434, 780491.7894596434, 246001.9806453716], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5659200.0000, 
sim time next is 5659800.0000, 
raw observation next is [31.86666666666667, 64.5, 1.0, 2.0, 0.2800472549488784, 1.0, 2.0, 0.2800472549488784, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 782673.2133977687, 782673.2133977687, 246143.1554306601], 
processed observation next is [0.0, 0.5217391304347826, 0.7093206951026858, 0.645, 1.0, 1.0, 0.13258705415527516, 1.0, 1.0, 0.13258705415527516, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21740922594382464, 0.21740922594382464, 0.36737784392635836], 
reward next is 0.6326, 
noisyNet noise sample is [array([-0.6577765], dtype=float32), -0.14366536]. 
=============================================
[2019-04-28 00:19:09,438] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-28 00:19:09,439] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:19:09,440] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:19:09,442] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:19:09,442] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:19:09,444] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:19:09,445] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:19:09,451] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:19:09,452] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:19:09,452] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:19:09,454] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:19:09,467] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run41
[2019-04-28 00:19:09,509] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run41
[2019-04-28 00:19:09,539] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run41
[2019-04-28 00:19:09,566] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run41
[2019-04-28 00:19:09,615] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run41
[2019-04-28 00:19:16,929] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:16,929] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.05, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 402822.9469911921, 402822.9469911914, 213313.1843652443]
[2019-04-28 00:19:16,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:19:16,932] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.6880603e-14 1.2066829e-04 2.3543582e-21 9.9987936e-01 8.1119512e-17], sampled 0.5541479608113798
[2019-04-28 00:19:18,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:18,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.23333333333333, 64.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 394464.6350597366, 394464.6350597366, 211702.7459593645]
[2019-04-28 00:19:18,436] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:19:18,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.0901710e-14 1.1751689e-04 2.0301629e-21 9.9988246e-01 7.2232303e-17], sampled 0.8297050560085112
[2019-04-28 00:19:19,072] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:19,077] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [17.032479285, 83.10064188, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 347919.2687458896, 347919.2687458896, 185783.3906215562]
[2019-04-28 00:19:19,078] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:19:19,081] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1521222e-13 1.4198180e-04 5.5326060e-21 9.9985802e-01 1.5796954e-16], sampled 0.5674312259489068
[2019-04-28 00:19:26,620] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:26,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.99067025833333, 94.61586212166668, 1.0, 2.0, 0.2431370091188568, 1.0, 2.0, 0.2431370091188568, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 679482.950155637, 679482.950155637, 240034.2885681659]
[2019-04-28 00:19:26,622] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:19:26,624] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.2903129e-14 8.7545232e-05 4.3666316e-22 9.9991250e-01 2.1753682e-17], sampled 0.3315295925538405
[2019-04-28 00:19:48,189] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:48,190] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.76859540333333, 87.86110603666667, 1.0, 2.0, 0.2131006795096922, 1.0, 2.0, 0.2131006795096922, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 693707.7027610493, 693707.7027610493, 248308.394079538]
[2019-04-28 00:19:48,191] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:19:48,195] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.32769055e-14 1.05828636e-04 1.18776704e-21 9.99894142e-01
 4.75211523e-17], sampled 0.9179538597145741
[2019-04-28 00:19:52,490] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:19:52,490] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.886445625, 81.47917433333333, 1.0, 2.0, 0.1946343208134912, 1.0, 2.0, 0.1946343208134912, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 590674.3706953498, 590674.3706953498, 240992.1530129652]
[2019-04-28 00:19:52,491] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:19:52,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.8377668e-15 6.8098831e-05 1.1559000e-22 9.9993193e-01 7.7060559e-18], sampled 0.43085204074053696
[2019-04-28 00:20:02,134] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:02,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [38.54315990666667, 62.89725973666667, 1.0, 2.0, 0.7453304685516561, 1.0, 2.0, 0.7453304685516561, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2084279.260997767, 2084279.260997767, 394664.9806616938]
[2019-04-28 00:20:02,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:20:02,138] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9197842e-14 9.4540301e-05 6.3829066e-22 9.9990547e-01 2.9279944e-17], sampled 0.9712176912847847
[2019-04-28 00:20:19,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:19,324] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.2, 85.33333333333333, 1.0, 2.0, 0.4292557693980443, 1.0, 2.0, 0.4292557693980443, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1199918.986269042, 1199918.986269042, 279472.2450592743]
[2019-04-28 00:20:19,334] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:20:19,337] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.4164735e-14 1.1355227e-04 1.6878545e-21 9.9988639e-01 6.2528310e-17], sampled 0.7653340693811592
[2019-04-28 00:20:27,649] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:27,650] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.83333333333334, 90.33333333333333, 1.0, 2.0, 0.3557051665498929, 1.0, 2.0, 0.3557051665498929, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 994219.1875132108, 994219.1875132108, 261546.2333970412]
[2019-04-28 00:20:27,651] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:20:27,652] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.7386574e-14 1.0903988e-04 1.3680879e-21 9.9989092e-01 5.3067326e-17], sampled 0.8356943974740313
[2019-04-28 00:20:29,905] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:29,906] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.0, 90.0, 1.0, 2.0, 0.3983000468133097, 1.0, 2.0, 0.3983000468133097, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1113341.656992201, 1113341.656992201, 271406.355746945]
[2019-04-28 00:20:29,906] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:20:29,909] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1133153e-14 1.0442770e-04 1.0955687e-21 9.9989557e-01 4.4618222e-17], sampled 0.9984914607895573
[2019-04-28 00:20:38,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:38,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.26755945666667, 82.52397642, 1.0, 2.0, 0.4976274898787564, 1.0, 2.0, 0.4976274898787564, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1391153.645429867, 1391153.645429867, 299708.0110295001]
[2019-04-28 00:20:38,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:20:38,301] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.7211014e-14 1.2114617e-04 2.3689970e-21 9.9987888e-01 8.1508134e-17], sampled 0.5076852558786599
[2019-04-28 00:20:39,938] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.064858906]
[2019-04-28 00:20:39,938] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.83970811, 80.10605283, 1.0, 2.0, 0.2285968172347469, 1.0, 2.0, 0.2285968172347469, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 651577.6462603414, 651577.6462603414, 240064.4119074969]
[2019-04-28 00:20:39,938] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:20:39,940] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7472619e-14 8.0876664e-05 2.8519957e-22 9.9991918e-01 1.5599989e-17], sampled 0.794820875570497
[2019-04-28 00:20:48,350] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6699.5714 3429693654.7828 34.0000
[2019-04-28 00:20:48,745] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.9301 3393558219.8605 9.0000
[2019-04-28 00:20:48,795] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.2302 3511176873.5561 0.0000
[2019-04-28 00:20:49,007] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6501.8992 3684445102.8175 229.0000
[2019-04-28 00:20:49,320] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 00:20:50,332] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1000000, evaluation results [1000000.0, 6501.899204051614, 3684445102.8175116, 229.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.930144583607, 3393558219.8605165, 9.0, 6798.230193198986, 3511176873.556055, 0.0, 6699.571419347967, 3429693654.782802, 34.0]
[2019-04-28 00:21:04,804] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2126205e-11 1.0672048e-02 1.3809885e-18 9.8932797e-01 3.4640595e-14], sum to 1.0000
[2019-04-28 00:21:04,808] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8462
[2019-04-28 00:21:04,812] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.6, 72.0, 1.0, 2.0, 0.8283264945169192, 1.0, 2.0, 0.8283264945169192, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2316638.458722325, 2316638.458722325, 433905.6345204205], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5913000.0000, 
sim time next is 5913600.0000, 
raw observation next is [31.76666666666667, 71.33333333333333, 1.0, 2.0, 0.850342822501703, 1.0, 2.0, 0.850342822501703, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2378271.65016695, 2378271.650166949, 445132.3281221675], 
processed observation next is [1.0, 0.43478260869565216, 0.7045813586097948, 0.7133333333333333, 1.0, 1.0, 0.8196901475924132, 1.0, 1.0, 0.8196901475924132, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6606310139352639, 0.6606310139352636, 0.6643766091375635], 
reward next is 0.3356, 
noisyNet noise sample is [array([0.14776643], dtype=float32), 0.45749182]. 
=============================================
[2019-04-28 00:21:09,053] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2777528e-14 1.0255889e-04 7.8472430e-22 9.9989748e-01 1.9895189e-16], sum to 1.0000
[2019-04-28 00:21:09,062] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4198
[2019-04-28 00:21:09,066] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.8, 84.66666666666667, 1.0, 2.0, 0.2692747648721017, 1.0, 2.0, 0.2692747648721017, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 752555.8056662309, 752555.8056662302, 244218.5545069347], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6034800.0000, 
sim time next is 6035400.0000, 
raw observation next is [27.75, 85.0, 1.0, 2.0, 0.2691625586148843, 1.0, 2.0, 0.2691625586148843, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 752242.1071959262, 752242.1071959262, 244198.9094418437], 
processed observation next is [1.0, 0.8695652173913043, 0.514218009478673, 0.85, 1.0, 1.0, 0.11947296218660759, 1.0, 1.0, 0.11947296218660759, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2089561408877573, 0.2089561408877573, 0.36447598424155775], 
reward next is 0.6355, 
noisyNet noise sample is [array([0.00330002], dtype=float32), 0.57497984]. 
=============================================
[2019-04-28 00:21:14,697] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7745170e-14 1.7669936e-06 1.7678407e-20 9.9999821e-01 1.4702919e-17], sum to 1.0000
[2019-04-28 00:21:14,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5379
[2019-04-28 00:21:14,712] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.35, 68.66666666666666, 1.0, 2.0, 0.827435686832493, 1.0, 2.0, 0.827435686832493, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2314144.768042214, 2314144.768042214, 433445.1944734747], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6108600.0000, 
sim time next is 6109200.0000, 
raw observation next is [30.3, 69.0, 1.0, 2.0, 0.7819279562791757, 1.0, 2.0, 0.7819279562791757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2186755.130603894, 2186755.130603894, 411166.428499267], 
processed observation next is [1.0, 0.7391304347826086, 0.6350710900473934, 0.69, 1.0, 1.0, 0.7372625979267177, 1.0, 1.0, 0.7372625979267177, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.607431980723304, 0.607431980723304, 0.6136812365660701], 
reward next is 0.3863, 
noisyNet noise sample is [array([2.2369561], dtype=float32), 0.04802159]. 
=============================================
[2019-04-28 00:21:17,858] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.2994515e-16 8.5538358e-07 3.3141086e-21 9.9999917e-01 4.7328371e-18], sum to 1.0000
[2019-04-28 00:21:17,870] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3577
[2019-04-28 00:21:17,874] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 89.5, 1.0, 2.0, 0.2683962715854292, 1.0, 2.0, 0.2683962715854292, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 750099.7775282925, 750099.7775282925, 244064.2508278026], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6136200.0000, 
sim time next is 6136800.0000, 
raw observation next is [26.96666666666667, 89.66666666666667, 1.0, 2.0, 0.2682330708696362, 1.0, 2.0, 0.2682330708696362, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 749643.5133632413, 749643.5133632413, 244035.6930305456], 
processed observation next is [1.0, 0.0, 0.47709320695102697, 0.8966666666666667, 1.0, 1.0, 0.11835309743329664, 1.0, 1.0, 0.11835309743329664, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20823430926756703, 0.20823430926756703, 0.36423237765753075], 
reward next is 0.6358, 
noisyNet noise sample is [array([-1.0565966], dtype=float32), 0.09444692]. 
=============================================
[2019-04-28 00:21:25,332] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.49463726e-13 1.18425254e-04 3.17869838e-21 9.99881506e-01
 4.34928456e-17], sum to 1.0000
[2019-04-28 00:21:25,341] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7334
[2019-04-28 00:21:25,345] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.2667886812537342, 1.0, 2.0, 0.2667886812537342, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 745605.4070463944, 745605.4070463944, 243782.8364187522], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6311400.0000, 
sim time next is 6312000.0000, 
raw observation next is [27.3, 86.0, 1.0, 2.0, 0.2665777401583893, 1.0, 2.0, 0.2665777401583893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 745015.6764622277, 745015.6764622277, 243746.2199978475], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 1.0, 1.0, 0.11635872308239675, 1.0, 1.0, 0.11635872308239675, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20694879901728547, 0.20694879901728547, 0.36380032835499626], 
reward next is 0.6362, 
noisyNet noise sample is [array([-0.8148728], dtype=float32), 0.57463545]. 
=============================================
[2019-04-28 00:21:25,366] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[61.42231 ]
 [61.575348]
 [61.44312 ]
 [61.309155]
 [61.25549 ]], R is [[61.45078659]
 [61.47242737]
 [61.49385452]
 [61.51522064]
 [61.53650665]].
[2019-04-28 00:21:26,138] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9359188e-16 4.7118838e-06 4.8063820e-24 9.9999523e-01 4.4032003e-18], sum to 1.0000
[2019-04-28 00:21:26,144] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3381
[2019-04-28 00:21:26,148] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333334, 63.0, 1.0, 2.0, 0.2808719107073975, 1.0, 2.0, 0.2808719107073975, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 784978.7963176374, 784978.7963176374, 246290.7400769369], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6351000.0000, 
sim time next is 6351600.0000, 
raw observation next is [31.66666666666667, 63.00000000000001, 1.0, 2.0, 0.2724039865534008, 1.0, 2.0, 0.2724039865534008, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 761304.3035306432, 761304.3035306432, 244770.1943563545], 
processed observation next is [0.0, 0.5217391304347826, 0.6998420221169038, 0.6300000000000001, 1.0, 1.0, 0.12337829705229013, 1.0, 1.0, 0.12337829705229013, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2114734176474009, 0.2114734176474009, 0.36532864829306644], 
reward next is 0.6347, 
noisyNet noise sample is [array([0.05188158], dtype=float32), -0.6626636]. 
=============================================
[2019-04-28 00:21:28,350] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.1713633e-15 4.6376911e-07 2.2786415e-26 9.9999952e-01 3.1196054e-22], sum to 1.0000
[2019-04-28 00:21:28,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5516
[2019-04-28 00:21:28,367] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.3, 86.0, 1.0, 2.0, 0.2665777401583893, 1.0, 2.0, 0.2665777401583893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 745015.6764622277, 745015.6764622277, 243746.2199978475], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6312000.0000, 
sim time next is 6312600.0000, 
raw observation next is [27.3, 86.0, 1.0, 2.0, 0.2666841437153968, 1.0, 2.0, 0.2666841437153968, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 745313.1501393212, 745313.1501393212, 243764.6871630608], 
processed observation next is [0.0, 0.043478260869565216, 0.4928909952606636, 0.86, 1.0, 1.0, 0.11648692013903225, 1.0, 1.0, 0.11648692013903225, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20703143059425588, 0.20703143059425588, 0.36382789128815046], 
reward next is 0.6362, 
noisyNet noise sample is [array([-0.14603421], dtype=float32), -1.1630973]. 
=============================================
[2019-04-28 00:21:33,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.86250406e-12 1.20628705e-04 1.71837208e-19 9.99879360e-01
 9.08132483e-15], sum to 1.0000
[2019-04-28 00:21:33,255] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2620
[2019-04-28 00:21:33,266] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.75, 89.33333333333333, 1.0, 2.0, 0.3603749679250703, 1.0, 2.0, 0.3603749679250703, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1007277.717933299, 1007277.717933299, 262605.7911306543], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6504600.0000, 
sim time next is 6505200.0000, 
raw observation next is [26.8, 89.0, 1.0, 2.0, 0.3610283937389589, 1.0, 2.0, 0.3610283937389589, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1009104.95626925, 1009104.95626925, 262755.31671173], 
processed observation next is [1.0, 0.30434782608695654, 0.4691943127962086, 0.89, 1.0, 1.0, 0.2301546912517577, 1.0, 1.0, 0.2301546912517577, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.28030693229701387, 0.28030693229701387, 0.39217211449511935], 
reward next is 0.6078, 
noisyNet noise sample is [array([0.15961449], dtype=float32), -0.9672792]. 
=============================================
[2019-04-28 00:21:35,050] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.3984206e-13 1.7814114e-04 7.1072496e-20 9.9982184e-01 5.3672570e-17], sum to 1.0000
[2019-04-28 00:21:35,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1068
[2019-04-28 00:21:35,066] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.25, 63.0, 1.0, 2.0, 0.6867850091605372, 1.0, 2.0, 0.6867850091605372, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1920437.991776498, 1920437.991776498, 368509.9292383704], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6539400.0000, 
sim time next is 6540000.0000, 
raw observation next is [30.13333333333333, 63.66666666666666, 1.0, 2.0, 0.6787257984288904, 1.0, 2.0, 0.6787257984288904, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1897882.283084588, 1897882.283084588, 365141.6401631382], 
processed observation next is [1.0, 0.6956521739130435, 0.6271721958925749, 0.6366666666666666, 1.0, 1.0, 0.6129226487095064, 1.0, 1.0, 0.6129226487095064, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5271895230790522, 0.5271895230790522, 0.5449875226315496], 
reward next is 0.4550, 
noisyNet noise sample is [array([0.09602858], dtype=float32), -1.6022682]. 
=============================================
[2019-04-28 00:21:35,077] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.94428 ]
 [58.772434]
 [58.512646]
 [58.211018]
 [58.050465]], R is [[58.93240356]
 [58.79306412]
 [58.65731812]
 [58.5200386 ]
 [58.34885788]].
[2019-04-28 00:21:35,222] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.8829742e-12 5.3916086e-05 8.1190563e-17 9.9994612e-01 5.1068184e-15], sum to 1.0000
[2019-04-28 00:21:35,228] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6032
[2019-04-28 00:21:35,233] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 60.0, 1.0, 2.0, 0.6957288522885704, 1.0, 2.0, 0.6957288522885704, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1945470.120498581, 1945470.12049858, 372291.6457996175], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6518400.0000, 
sim time next is 6519000.0000, 
raw observation next is [30.88333333333333, 58.5, 1.0, 2.0, 0.6991951151800707, 1.0, 2.0, 0.6991951151800707, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1955171.693318386, 1955171.693318386, 373769.8320018773], 
processed observation next is [1.0, 0.43478260869565216, 0.6627172195892573, 0.585, 1.0, 1.0, 0.6375844761205671, 1.0, 1.0, 0.6375844761205671, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5431032481439961, 0.5431032481439961, 0.5578654208983244], 
reward next is 0.4421, 
noisyNet noise sample is [array([1.0382066], dtype=float32), -0.13592471]. 
=============================================
[2019-04-28 00:21:35,255] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[50.08627 ]
 [49.986347]
 [49.855537]
 [49.71992 ]
 [49.605804]], R is [[50.17494965]
 [50.11754227]
 [50.06803513]
 [50.02213669]
 [49.97632217]].
[2019-04-28 00:21:35,460] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.7304760e-14 7.4815830e-06 1.9596999e-21 9.9999249e-01 1.2525497e-16], sum to 1.0000
[2019-04-28 00:21:35,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7966
[2019-04-28 00:21:35,477] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.7, 65.66666666666667, 1.0, 2.0, 0.231458537858452, 1.0, 2.0, 0.231458537858452, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 646837.0161602845, 646837.0161602852, 238007.8380450032], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6542400.0000, 
sim time next is 6543000.0000, 
raw observation next is [29.6, 66.0, 1.0, 2.0, 0.2292750631758497, 1.0, 2.0, 0.2292750631758497, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 640733.2272126571, 640733.2272126571, 237675.0606350699], 
processed observation next is [1.0, 0.7391304347826086, 0.6018957345971565, 0.66, 1.0, 1.0, 0.07141573876608397, 1.0, 1.0, 0.07141573876608397, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.17798145200351587, 0.17798145200351587, 0.3547388964702536], 
reward next is 0.6453, 
noisyNet noise sample is [array([0.98303884], dtype=float32), -0.14513482]. 
=============================================
[2019-04-28 00:21:35,488] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[62.008564]
 [61.914898]
 [61.015446]
 [60.822144]
 [60.678265]], R is [[61.78671646]
 [61.81361389]
 [61.81663895]
 [61.66100693]
 [61.50105667]].
[2019-04-28 00:21:39,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.1078069e-14 1.0290201e-05 4.2575820e-22 9.9998975e-01 2.6510019e-17], sum to 1.0000
[2019-04-28 00:21:39,508] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0984
[2019-04-28 00:21:39,515] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.13333333333333, 88.33333333333333, 1.0, 2.0, 0.2505832817719414, 1.0, 2.0, 0.2505832817719414, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 700300.7191109152, 700300.7191109152, 241039.9911823695], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6651600.0000, 
sim time next is 6652200.0000, 
raw observation next is [26.06666666666667, 88.66666666666667, 1.0, 2.0, 0.2500317403564766, 1.0, 2.0, 0.2500317403564766, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 698758.8338846102, 698758.8338846102, 240949.4288730999], 
processed observation next is [1.0, 1.0, 0.4344391785150081, 0.8866666666666667, 1.0, 1.0, 0.09642378356201996, 1.0, 1.0, 0.09642378356201996, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1940996760790584, 0.1940996760790584, 0.3596260132434327], 
reward next is 0.6404, 
noisyNet noise sample is [array([-2.308346], dtype=float32), -0.9682284]. 
=============================================
[2019-04-28 00:21:42,080] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-28 00:21:42,082] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:21:42,083] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:21:42,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:21:42,087] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:21:42,089] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:21:42,091] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:21:42,093] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:21:42,094] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:21:42,094] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:21:42,095] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:21:42,105] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run42
[2019-04-28 00:21:42,127] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run42
[2019-04-28 00:21:42,127] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run42
[2019-04-28 00:21:42,182] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run42
[2019-04-28 00:21:42,215] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run42
[2019-04-28 00:21:50,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:21:50,217] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 87.00000000000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 472841.8058569767, 472841.8058569767, 227908.55320681]
[2019-04-28 00:21:50,218] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:21:50,220] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.6871397e-15 2.3280390e-06 2.7762068e-22 9.9999762e-01 9.0622781e-18], sampled 0.01804297752311157
[2019-04-28 00:21:52,725] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:21:52,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.95, 91.66666666666667, 1.0, 2.0, 0.1723240653977274, 1.0, 2.0, 0.1723240653977274, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 531768.0556732061, 531768.0556732068, 238554.0259953252]
[2019-04-28 00:21:52,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:21:52,730] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.0313600e-15 1.7874627e-06 1.0090610e-22 9.9999821e-01 4.0688129e-18], sampled 0.07430632564649697
[2019-04-28 00:21:58,272] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:21:58,273] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.23333333333333, 51.00000000000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 446862.6141315363, 446862.6141315356, 222578.6126023999]
[2019-04-28 00:21:58,274] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:21:58,275] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.5355112e-15 2.2111121e-06 2.2847608e-22 9.9999774e-01 7.7666808e-18], sampled 0.8828039457131142
[2019-04-28 00:22:28,278] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:22:28,280] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.2970518, 87.74639979, 1.0, 2.0, 0.4229924191417413, 1.0, 2.0, 0.4229924191417413, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1182386.460102895, 1182386.460102895, 278613.0529915025]
[2019-04-28 00:22:28,280] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:22:28,282] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4805942e-15 1.0906872e-06 1.5258342e-23 9.9999893e-01 9.1260831e-19], sampled 0.9513748032616917
[2019-04-28 00:22:30,576] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:22:30,577] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.36666666666667, 71.5, 1.0, 2.0, 0.8586767098783609, 1.0, 2.0, 0.8586767098783609, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 2401571.813294478, 2401571.813294479, 449925.7837919783]
[2019-04-28 00:22:30,578] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:22:30,581] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.00337104e-13 5.99066607e-06 1.02695984e-20 9.99994040e-01
 1.57874156e-16], sampled 0.8155314492628178
[2019-04-28 00:22:58,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:22:58,813] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.25, 74.66666666666666, 1.0, 2.0, 0.2894507123579729, 1.0, 2.0, 0.2894507123579729, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 808959.7433748727, 808959.7433748727, 248362.260129092]
[2019-04-28 00:22:58,814] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:22:58,816] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.9213749e-15 1.4343590e-06 4.3595148e-23 9.9999857e-01 2.0942147e-18], sampled 0.5713796188644834
[2019-04-28 00:22:59,391] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:22:59,392] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.11204640333334, 85.95588814333334, 1.0, 2.0, 0.2915471135926088, 1.0, 2.0, 0.2915471135926088, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 814823.3702867572, 814823.3702867572, 248456.3470352122]
[2019-04-28 00:22:59,394] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:22:59,397] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.0701440e-15 2.2664826e-06 2.5079887e-22 9.9999774e-01 8.3623319e-18], sampled 0.8501325908044044
[2019-04-28 00:23:05,850] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058547925]
[2019-04-28 00:23:05,851] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.06666666666667, 59.66666666666667, 1.0, 2.0, 0.2104152942366367, 1.0, 2.0, 0.2104152942366367, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 659252.0239249605, 659252.0239249605, 245930.6837327268]
[2019-04-28 00:23:05,851] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:23:05,852] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2153322e-15 2.2801082e-06 2.5724835e-22 9.9999774e-01 8.5301925e-18], sampled 0.3756110697099896
[2019-04-28 00:23:06,049] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 00:23:06,192] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 00:23:06,361] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 00:23:06,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 00:23:06,708] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 00:23:07,723] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1025000, evaluation results [1025000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 00:23:09,609] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.6815901e-15 5.9546283e-06 2.0934669e-21 9.9999404e-01 2.6668857e-18], sum to 1.0000
[2019-04-28 00:23:09,617] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1220
[2019-04-28 00:23:09,620] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.76666666666667, 86.16666666666667, 1.0, 2.0, 0.2556655056833047, 1.0, 2.0, 0.2556655056833047, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 714508.6521475512, 714508.6521475506, 241883.907768849], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6642600.0000, 
sim time next is 6643200.0000, 
raw observation next is [26.73333333333334, 86.33333333333334, 1.0, 2.0, 0.255942134412659, 1.0, 2.0, 0.255942134412659, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 715282.0045183134, 715282.0045183134, 241930.0947093741], 
processed observation next is [1.0, 0.9130434782608695, 0.4660347551342816, 0.8633333333333334, 1.0, 1.0, 0.10354474025621566, 1.0, 1.0, 0.10354474025621566, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1986894456995315, 0.1986894456995315, 0.36108969359608073], 
reward next is 0.6389, 
noisyNet noise sample is [array([-0.5877592], dtype=float32), 0.3325701]. 
=============================================
[2019-04-28 00:23:26,373] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.4341827e-15 6.4291089e-05 9.6145028e-21 9.9993575e-01 5.7446395e-18], sum to 1.0000
[2019-04-28 00:23:26,382] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7809
[2019-04-28 00:23:26,387] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.4, 61.0, 1.0, 2.0, 0.2272705976586773, 1.0, 2.0, 0.2272705976586773, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 640728.4103038368, 640728.4103038368, 238260.8771340474], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6949200.0000, 
sim time next is 6949800.0000, 
raw observation next is [29.55, 60.5, 1.0, 2.0, 0.2287400802741814, 1.0, 2.0, 0.2287400802741814, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 643571.2813364214, 643571.281336422, 238278.6383273673], 
processed observation next is [0.0, 0.43478260869565216, 0.5995260663507109, 0.605, 1.0, 1.0, 0.0707711810532306, 1.0, 1.0, 0.0707711810532306, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.17876980037122817, 0.17876980037122833, 0.35563975869756315], 
reward next is 0.6444, 
noisyNet noise sample is [array([-0.10146134], dtype=float32), 0.8131974]. 
=============================================
[2019-04-28 00:23:26,825] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [7.4935317e-17 6.9027345e-07 5.9835997e-25 9.9999928e-01 1.6566407e-20], sum to 1.0000
[2019-04-28 00:23:26,841] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9985
[2019-04-28 00:23:26,845] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 61.0, 1.0, 2.0, 0.2394648959139302, 1.0, 2.0, 0.2394648959139302, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 669218.6673342537, 669218.6673342531, 239247.374668424], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6970800.0000, 
sim time next is 6971400.0000, 
raw observation next is [30.0, 60.5, 1.0, 2.0, 0.2383862561614141, 1.0, 2.0, 0.2383862561614141, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 666203.3192257095, 666203.3192257095, 239077.1539152219], 
processed observation next is [0.0, 0.6956521739130435, 0.6208530805687204, 0.605, 1.0, 1.0, 0.0823930797125471, 1.0, 1.0, 0.0823930797125471, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18505647756269708, 0.18505647756269708, 0.3568315730077939], 
reward next is 0.6432, 
noisyNet noise sample is [array([1.006042], dtype=float32), -1.2630843]. 
=============================================
[2019-04-28 00:23:27,245] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.2893747e-16 1.2954224e-07 1.5866586e-25 9.9999988e-01 3.1746271e-21], sum to 1.0000
[2019-04-28 00:23:27,254] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5808
[2019-04-28 00:23:27,265] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.81666666666667, 52.0, 1.0, 2.0, 0.2335168685184465, 1.0, 2.0, 0.2335168685184465, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 652591.0033603156, 652591.0033603156, 238319.612217303], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6965400.0000, 
sim time next is 6966000.0000, 
raw observation next is [32.0, 52.0, 1.0, 2.0, 0.235740591424781, 1.0, 2.0, 0.235740591424781, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 658807.3731509831, 658807.3731509825, 238664.7566636979], 
processed observation next is [0.0, 0.6521739130434783, 0.7156398104265403, 0.52, 1.0, 1.0, 0.07920553183708552, 1.0, 1.0, 0.07920553183708552, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.18300204809749532, 0.18300204809749515, 0.3562160547219371], 
reward next is 0.6438, 
noisyNet noise sample is [array([-0.31645668], dtype=float32), -0.7557765]. 
=============================================
[2019-04-28 00:23:27,279] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[66.97223 ]
 [66.96959 ]
 [66.95795 ]
 [66.967415]
 [66.966484]], R is [[66.97163391]
 [66.9462204 ]
 [66.92153931]
 [66.89718628]
 [66.87321472]].
[2019-04-28 00:23:27,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.0962312e-15 8.7631510e-05 1.3954681e-21 9.9991238e-01 5.7475520e-18], sum to 1.0000
[2019-04-28 00:23:27,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7298
[2019-04-28 00:23:27,790] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.93333333333333, 86.0, 1.0, 2.0, 0.2422734276630175, 1.0, 2.0, 0.2422734276630175, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 677069.9854177856, 677069.9854177856, 239693.0604903416], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7073400.0000, 
sim time next is 7074000.0000, 
raw observation next is [25.9, 86.0, 1.0, 2.0, 0.2422277546278819, 1.0, 2.0, 0.2422277546278819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 676942.3048893916, 676942.3048893916, 239685.5808412303], 
processed observation next is [1.0, 0.9130434782608695, 0.42654028436018954, 0.86, 1.0, 1.0, 0.08702139111792999, 1.0, 1.0, 0.08702139111792999, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18803952913594213, 0.18803952913594213, 0.35773967289735864], 
reward next is 0.6423, 
noisyNet noise sample is [array([-0.3375922], dtype=float32), -1.0107173]. 
=============================================
[2019-04-28 00:23:27,801] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[58.149082]
 [58.172207]
 [58.15749 ]
 [58.239788]
 [58.21607 ]], R is [[58.21850586]
 [58.27857208]
 [58.33792114]
 [58.39655304]
 [58.45449448]].
[2019-04-28 00:23:36,843] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7162804e-14 1.0395920e-05 6.5102430e-21 9.9998963e-01 4.7002854e-16], sum to 1.0000
[2019-04-28 00:23:36,854] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5661
[2019-04-28 00:23:36,859] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.76666666666667, 86.0, 1.0, 2.0, 0.2375249938486538, 1.0, 2.0, 0.2375249938486538, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 663795.6579886505, 663795.6579886505, 238942.4013001205], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7173600.0000, 
sim time next is 7174200.0000, 
raw observation next is [25.78333333333333, 86.0, 1.0, 2.0, 0.2378878572580189, 1.0, 2.0, 0.2378878572580189, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 664810.0430194754, 664810.0430194754, 238999.3182260467], 
processed observation next is [1.0, 0.0, 0.4210110584518167, 0.86, 1.0, 1.0, 0.08179259910604685, 1.0, 1.0, 0.08179259910604685, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18466945639429871, 0.18466945639429871, 0.35671540033738314], 
reward next is 0.6433, 
noisyNet noise sample is [array([1.198162], dtype=float32), 0.7689463]. 
=============================================
[2019-04-28 00:23:40,778] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.6998266e-15 1.8780868e-06 2.7351412e-20 9.9999809e-01 5.3828212e-18], sum to 1.0000
[2019-04-28 00:23:40,785] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4494
[2019-04-28 00:23:40,790] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.5, 89.5, 1.0, 2.0, 0.4975890579079212, 1.0, 2.0, 0.4975890579079212, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1391051.209456126, 1391051.209456126, 299502.6113005537], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7216200.0000, 
sim time next is 7216800.0000, 
raw observation next is [25.66666666666667, 93.0, 1.0, 2.0, 0.4948855030451053, 1.0, 2.0, 0.4948855030451053, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1383488.321691113, 1383488.321691113, 298666.2302612578], 
processed observation next is [1.0, 0.5217391304347826, 0.4154818325434442, 0.93, 1.0, 1.0, 0.3914283169218137, 1.0, 1.0, 0.3914283169218137, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.38430231158086475, 0.38430231158086475, 0.44577049292725046], 
reward next is 0.5542, 
noisyNet noise sample is [array([-1.87357], dtype=float32), -0.00033027944]. 
=============================================
[2019-04-28 00:23:43,354] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9911304e-13 9.6424372e-08 4.3151666e-21 9.9999988e-01 2.5476028e-17], sum to 1.0000
[2019-04-28 00:23:43,362] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1684
[2019-04-28 00:23:43,366] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.96666666666667, 88.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 515399.9041475793, 515399.90414758, 236649.9169766691], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7262400.0000, 
sim time next is 7263000.0000, 
raw observation next is [21.95, 88.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 514052.5155222308, 514052.5155222308, 236379.0684453282], 
processed observation next is [1.0, 0.043478260869565216, 0.2393364928909953, 0.885, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1427923654228419, 0.1427923654228419, 0.35280457976914653], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2356635], dtype=float32), 0.38591924]. 
=============================================
[2019-04-28 00:23:43,383] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[53.730293]
 [53.834553]
 [53.861156]
 [54.20212 ]
 [54.291725]], R is [[52.92951965]
 [52.40022659]
 [51.87622452]
 [51.35746384]
 [50.84389114]].
[2019-04-28 00:23:46,715] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.5458603e-15 6.3148423e-07 2.9704896e-20 9.9999940e-01 3.2668564e-18], sum to 1.0000
[2019-04-28 00:23:46,724] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8396
[2019-04-28 00:23:46,733] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.9, 67.0, 1.0, 2.0, 0.1918960788935679, 1.0, 2.0, 0.1918960788935679, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 568721.3830240689, 568721.3830240689, 237999.9155523334], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7322400.0000, 
sim time next is 7323000.0000, 
raw observation next is [26.8, 67.5, 1.0, 2.0, 0.1941275935384752, 1.0, 2.0, 0.1941275935384752, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 575465.2548699105, 575465.2548699105, 238329.3398899344], 
processed observation next is [1.0, 0.782608695652174, 0.4691943127962086, 0.675, 1.0, 1.0, 0.02906938980539179, 1.0, 1.0, 0.02906938980539179, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15985145968608627, 0.15985145968608627, 0.3557154326715439], 
reward next is 0.6443, 
noisyNet noise sample is [array([-0.28578672], dtype=float32), 1.1778495]. 
=============================================
[2019-04-28 00:23:46,751] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[63.407234]
 [63.625504]
 [63.982414]
 [64.34888 ]
 [64.64886 ]], R is [[63.20359802]
 [63.21633911]
 [63.22931671]
 [63.24241257]
 [63.25606918]].
[2019-04-28 00:23:51,726] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.0523773e-11 4.0527386e-05 3.3666578e-18 9.9995947e-01 1.7536268e-13], sum to 1.0000
[2019-04-28 00:23:51,731] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6631
[2019-04-28 00:23:51,738] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.1, 81.0, 1.0, 2.0, 0.2029980902503584, 1.0, 2.0, 0.2029980902503584, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 592998.9236636126, 592998.9236636126, 238200.4997725539], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7479000.0000, 
sim time next is 7479600.0000, 
raw observation next is [25.2, 80.66666666666666, 1.0, 2.0, 0.2039551354550369, 1.0, 2.0, 0.2039551354550369, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 594675.6315628985, 594675.6315628985, 238157.0758735221], 
processed observation next is [0.0, 0.5652173913043478, 0.3933649289099526, 0.8066666666666665, 1.0, 1.0, 0.0409098017530565, 1.0, 1.0, 0.0409098017530565, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16518767543413848, 0.16518767543413848, 0.3554583221992867], 
reward next is 0.6445, 
noisyNet noise sample is [array([-0.7581783], dtype=float32), -0.574733]. 
=============================================
[2019-04-28 00:23:59,029] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-28 00:23:59,033] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:23:59,034] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:23:59,034] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:23:59,035] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:23:59,036] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:23:59,039] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:23:59,039] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:23:59,040] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:23:59,040] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:23:59,044] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:23:59,055] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run43
[2019-04-28 00:23:59,080] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run43
[2019-04-28 00:23:59,103] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run43
[2019-04-28 00:23:59,140] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run43
[2019-04-28 00:23:59,165] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run43
[2019-04-28 00:24:47,903] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.059737906]
[2019-04-28 00:24:47,905] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.64148916833333, 88.25154840833333, 1.0, 2.0, 0.5479089272200819, 1.0, 2.0, 0.5479089272200819, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1573887.335082996, 1573887.335082996, 321298.5923637957]
[2019-04-28 00:24:47,907] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:24:47,909] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.6103705e-14 3.0989431e-06 2.1907460e-21 9.9999690e-01 5.9713877e-17], sampled 0.079839416641307
[2019-04-28 00:24:49,580] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.059737906]
[2019-04-28 00:24:49,582] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [35.23911997833333, 47.43677556166666, 1.0, 2.0, 0.4701788847029822, 1.0, 2.0, 0.4701788847029822, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1314366.089249085, 1314366.089249085, 291737.9880866718]
[2019-04-28 00:24:49,583] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:24:49,606] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.0945172e-14 3.9737829e-06 5.1990321e-21 9.9999607e-01 1.1796004e-16], sampled 0.8076997381145268
[2019-04-28 00:24:52,226] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.059737906]
[2019-04-28 00:24:52,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.579850595, 73.81309948833334, 1.0, 2.0, 0.2693684294847714, 1.0, 2.0, 0.2693684294847714, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 752816.1819370929, 752816.1819370929, 244430.5158030638]
[2019-04-28 00:24:52,229] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:24:52,231] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.38932661e-13 4.92248091e-06 1.21506975e-20 9.99995112e-01
 2.28643600e-16], sampled 0.9604335266767049
[2019-04-28 00:25:16,835] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.059737906]
[2019-04-28 00:25:16,837] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.874315765, 66.92765035, 1.0, 2.0, 0.2955214293349952, 1.0, 2.0, 0.2955214293349952, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 825932.6896368912, 825932.6896368912, 249502.8326501406]
[2019-04-28 00:25:16,838] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:25:16,848] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.2077670e-13 9.2326054e-06 1.2404111e-19 9.9999082e-01 1.4203522e-15], sampled 0.15666330152787145
[2019-04-28 00:26:10,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.059737906]
[2019-04-28 00:26:10,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.630372495, 72.32292705, 1.0, 2.0, 0.235853045215162, 1.0, 2.0, 0.235853045215162, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 659119.0359253027, 659119.0359253027, 239165.2833432206]
[2019-04-28 00:26:10,673] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:26:10,701] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2462418e-13 7.8628982e-06 6.8873908e-20 9.9999213e-01 8.9346496e-16], sampled 0.36053222950231456
[2019-04-28 00:26:30,257] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 00:26:30,346] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 00:26:31,464] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 00:26:31,733] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.8121 3429643798.5602 33.0000
[2019-04-28 00:26:32,877] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 00:26:33,894] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1050000, evaluation results [1050000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.812113532402, 3429643798.560197, 33.0]
[2019-04-28 00:26:54,669] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.5005475e-13 2.0409416e-05 8.3794438e-21 9.9997962e-01 1.4407886e-16], sum to 1.0000
[2019-04-28 00:26:54,699] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2485
[2019-04-28 00:26:54,729] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.53333333333333, 94.33333333333334, 1.0, 2.0, 0.210767930159021, 1.0, 2.0, 0.210767930159021, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 609963.3064388874, 609963.3064388874, 238417.0282948815], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7623600.0000, 
sim time next is 7624200.0000, 
raw observation next is [23.61666666666667, 94.16666666666667, 1.0, 2.0, 0.2126430896720148, 1.0, 2.0, 0.2126430896720148, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 613866.3274392757, 613866.3274392757, 238448.6039409256], 
processed observation next is [1.0, 0.21739130434782608, 0.31832543443917877, 0.9416666666666668, 1.0, 1.0, 0.05137721647230699, 1.0, 1.0, 0.05137721647230699, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1705184242886877, 0.1705184242886877, 0.3558934387177994], 
reward next is 0.6441, 
noisyNet noise sample is [array([-0.48447022], dtype=float32), 0.7221185]. 
=============================================
[2019-04-28 00:27:11,013] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.1053419e-14 1.5381455e-06 7.7697896e-20 9.9999845e-01 3.7896324e-17], sum to 1.0000
[2019-04-28 00:27:11,016] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3868
[2019-04-28 00:27:11,053] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.01666666666667, 92.66666666666667, 1.0, 2.0, 0.4893361186264364, 1.0, 2.0, 0.4893361186264364, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1367964.716030066, 1367964.716030066, 296970.1312450783], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7870200.0000, 
sim time next is 7870800.0000, 
raw observation next is [26.03333333333333, 92.33333333333334, 1.0, 2.0, 0.4424727427334095, 1.0, 2.0, 0.4424727427334095, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1236880.118743036, 1236880.118743036, 283351.894974221], 
processed observation next is [1.0, 0.08695652173913043, 0.4328593996840442, 0.9233333333333335, 1.0, 1.0, 0.3282804129318187, 1.0, 1.0, 0.3282804129318187, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3435778107619544, 0.3435778107619544, 0.4229132760809269], 
reward next is 0.5771, 
noisyNet noise sample is [array([0.80106497], dtype=float32), -0.21616392]. 
=============================================
[2019-04-28 00:27:17,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:17,775] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:17,796] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run6
[2019-04-28 00:27:18,843] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.4302690e-15 1.8595183e-07 1.9809464e-22 9.9999976e-01 3.2691779e-17], sum to 1.0000
[2019-04-28 00:27:18,844] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5602888e-13 1.4121117e-07 6.4010689e-21 9.9999988e-01 2.3365749e-15], sum to 1.0000
[2019-04-28 00:27:18,847] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8506
[2019-04-28 00:27:18,849] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0397
[2019-04-28 00:27:18,850] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.2, 88.0, 1.0, 2.0, 0.3131979508294067, 1.0, 2.0, 0.3131979508294067, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 875360.2503494652, 875360.2503494652, 252468.6608959061], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7797600.0000, 
sim time next is 7798200.0000, 
raw observation next is [26.31666666666666, 87.50000000000001, 1.0, 2.0, 0.3276191563784212, 1.0, 2.0, 0.3276191563784212, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 915683.4371722679, 915683.4371722679, 255427.3838095558], 
processed observation next is [1.0, 0.2608695652173913, 0.4462875197472351, 0.8750000000000001, 1.0, 1.0, 0.18990259804629062, 1.0, 1.0, 0.18990259804629062, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.25435651032563, 0.25435651032563, 0.38123490120829223], 
reward next is 0.6188, 
noisyNet noise sample is [array([0.35064194], dtype=float32), 0.21903205]. 
=============================================
[2019-04-28 00:27:18,856] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.1, 88.33333333333334, 1.0, 2.0, 0.2675110367902331, 1.0, 2.0, 0.2675110367902331, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 747624.9076561594, 747624.90765616, 243909.4791458823], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7850400.0000, 
sim time next is 7851000.0000, 
raw observation next is [27.05, 88.16666666666667, 1.0, 2.0, 0.2663191531289684, 1.0, 2.0, 0.2663191531289684, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 744292.7420034283, 744292.7420034283, 243701.8816126723], 
processed observation next is [1.0, 0.8695652173913043, 0.4810426540284361, 0.8816666666666667, 1.0, 1.0, 0.11604717244454026, 1.0, 1.0, 0.11604717244454026, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2067479838898412, 0.2067479838898412, 0.36373415166070494], 
reward next is 0.6363, 
noisyNet noise sample is [array([-0.56324965], dtype=float32), -1.3452548]. 
=============================================
[2019-04-28 00:27:18,882] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[54.53229 ]
 [54.48719 ]
 [54.601845]
 [54.667645]
 [54.611282]], R is [[54.50803757]
 [54.5989151 ]
 [54.68870926]
 [54.7774086 ]
 [54.86491394]].
[2019-04-28 00:27:23,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:23,376] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:23,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run6
[2019-04-28 00:27:25,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:25,091] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:25,108] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run6
[2019-04-28 00:27:26,003] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.7580235e-14 1.4042514e-07 3.8936874e-21 9.9999988e-01 2.8717334e-16], sum to 1.0000
[2019-04-28 00:27:26,019] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3979
[2019-04-28 00:27:26,024] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.43333333333333, 66.5, 1.0, 2.0, 0.7649336243962841, 1.0, 2.0, 0.7649336243962841, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2139180.989775942, 2139180.989775942, 403156.4345933041], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7917000.0000, 
sim time next is 7917600.0000, 
raw observation next is [30.46666666666667, 66.0, 1.0, 2.0, 0.7526144686471166, 1.0, 2.0, 0.7526144686471166, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2104695.915203586, 2104695.915203586, 397457.2033426099], 
processed observation next is [1.0, 0.6521739130434783, 0.6429699842022119, 0.66, 1.0, 1.0, 0.7019451429483332, 1.0, 1.0, 0.7019451429483332, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5846377542232184, 0.5846377542232184, 0.5932197064815073], 
reward next is 0.4068, 
noisyNet noise sample is [array([-2.0106366], dtype=float32), -0.4481702]. 
=============================================
[2019-04-28 00:27:26,094] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:26,096] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:26,112] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run6
[2019-04-28 00:27:26,210] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.9401955e-13 1.2585579e-06 4.2238329e-19 9.9999869e-01 1.1447904e-16], sum to 1.0000
[2019-04-28 00:27:26,211] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9702
[2019-04-28 00:27:26,216] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.9, 92.0, 1.0, 2.0, 0.3727778679876951, 1.0, 2.0, 0.3727778679876951, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1108477.0726679, 1108477.0726679, 273496.6425992915], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 118800.0000, 
sim time next is 119400.0000, 
raw observation next is [22.9, 92.33333333333334, 1.0, 2.0, 0.3772767732895409, 1.0, 2.0, 0.3772767732895409, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1120127.572200953, 1120127.572200953, 274438.238839441], 
processed observation next is [1.0, 0.391304347826087, 0.2843601895734597, 0.9233333333333335, 1.0, 1.0, 0.24973105215607339, 1.0, 1.0, 0.24973105215607339, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3111465478335981, 0.3111465478335981, 0.4096093117006582], 
reward next is 0.5904, 
noisyNet noise sample is [array([0.35901937], dtype=float32), 1.2865037]. 
=============================================
[2019-04-28 00:27:26,423] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1403689e-16 3.2454977e-08 4.3731269e-24 1.0000000e+00 3.2690258e-19], sum to 1.0000
[2019-04-28 00:27:26,429] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6761
[2019-04-28 00:27:26,438] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.25, 88.83333333333334, 1.0, 2.0, 0.2700876431752207, 1.0, 2.0, 0.2700876431752207, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 754828.3971775874, 754828.3971775874, 244361.5617100169], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7848600.0000, 
sim time next is 7849200.0000, 
raw observation next is [27.2, 88.66666666666667, 1.0, 2.0, 0.268927300696267, 1.0, 2.0, 0.268927300696267, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 751584.3897244856, 751584.3897244856, 244157.6792282587], 
processed observation next is [1.0, 0.8695652173913043, 0.4881516587677725, 0.8866666666666667, 1.0, 1.0, 0.11918951891116508, 1.0, 1.0, 0.11918951891116508, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2087734415901349, 0.2087734415901349, 0.36441444660934136], 
reward next is 0.6356, 
noisyNet noise sample is [array([0.49545723], dtype=float32), 0.3706952]. 
=============================================
[2019-04-28 00:27:26,485] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:26,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:26,495] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run6
[2019-04-28 00:27:26,769] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:26,770] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:26,784] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run6
[2019-04-28 00:27:26,881] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:26,882] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:26,911] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run6
[2019-04-28 00:27:28,054] A3C_AGENT_WORKER-Thread-18 INFO:Local step 66500, global step 1060096: loss 0.0362
[2019-04-28 00:27:28,055] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 66500, global step 1060096: learning rate 0.0000
[2019-04-28 00:27:28,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:28,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:28,062] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run6
[2019-04-28 00:27:28,744] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:28,744] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:28,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run6
[2019-04-28 00:27:28,794] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:28,795] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:28,802] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run6
[2019-04-28 00:27:28,858] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.9469966e-14 7.0992223e-07 2.0314605e-22 9.9999928e-01 1.0121527e-17], sum to 1.0000
[2019-04-28 00:27:28,868] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.9377
[2019-04-28 00:27:28,883] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.2, 84.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 519806.4718312849, 519806.4718312849, 237192.5974966218], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 27600.0000, 
sim time next is 28200.0000, 
raw observation next is [22.25, 84.0, 1.0, 2.0, 0.1717537780006231, 1.0, 2.0, 0.1717537780006231, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 539532.7604073766, 539532.7604073773, 240064.7580592579], 
processed observation next is [1.0, 0.30434782608695654, 0.2535545023696683, 0.84, 1.0, 1.0, 0.0021129855429193934, 1.0, 1.0, 0.0021129855429193934, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.14987021122427127, 0.14987021122427147, 0.35830560904366854], 
reward next is 0.6417, 
noisyNet noise sample is [array([-0.3707552], dtype=float32), 1.4545741]. 
=============================================
[2019-04-28 00:27:29,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:29,061] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:29,064] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run6
[2019-04-28 00:27:29,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:29,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:29,171] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run6
[2019-04-28 00:27:29,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:29,272] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:29,280] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run6
[2019-04-28 00:27:29,324] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:29,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:29,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run6
[2019-04-28 00:27:29,635] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.6012241e-14 6.9302871e-08 5.5329263e-21 9.9999988e-01 3.9266446e-17], sum to 1.0000
[2019-04-28 00:27:29,638] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0398
[2019-04-28 00:27:29,674] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.51666666666667, 83.0, 1.0, 2.0, 0.1798913901574962, 1.0, 2.0, 0.1798913901574962, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 563312.0374239121, 563312.0374239121, 240937.3153523546], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 29400.0000, 
sim time next is 30000.0000, 
raw observation next is [22.73333333333333, 82.0, 1.0, 2.0, 0.2740398432799551, 1.0, 2.0, 0.2740398432799551, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 854707.0618148168, 854707.0618148168, 256344.9987018108], 
processed observation next is [1.0, 0.34782608695652173, 0.27646129541864134, 0.82, 1.0, 1.0, 0.12534920877103026, 1.0, 1.0, 0.12534920877103026, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23741862828189356, 0.23741862828189356, 0.38260447567434447], 
reward next is 0.6174, 
noisyNet noise sample is [array([0.10293666], dtype=float32), 0.05910014]. 
=============================================
[2019-04-28 00:27:29,698] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[52.203564]
 [52.184917]
 [52.047413]
 [52.030285]
 [51.983288]], R is [[52.11823654]
 [52.23744583]
 [51.71507263]
 [51.83961487]
 [51.3212204 ]].
[2019-04-28 00:27:30,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:30,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:30,458] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run6
[2019-04-28 00:27:33,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:27:33,399] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:27:33,447] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run6
[2019-04-28 00:27:34,159] A3C_AGENT_WORKER-Thread-2 INFO:Local step 66500, global step 1062344: loss 0.0032
[2019-04-28 00:27:34,159] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 66500, global step 1062344: learning rate 0.0000
[2019-04-28 00:27:35,526] A3C_AGENT_WORKER-Thread-20 INFO:Local step 66500, global step 1062911: loss 0.0708
[2019-04-28 00:27:35,528] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 66500, global step 1062911: learning rate 0.0000
[2019-04-28 00:27:36,469] A3C_AGENT_WORKER-Thread-10 INFO:Local step 66500, global step 1063344: loss 0.0490
[2019-04-28 00:27:36,470] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 66500, global step 1063344: learning rate 0.0000
[2019-04-28 00:27:36,894] A3C_AGENT_WORKER-Thread-9 INFO:Local step 66500, global step 1063529: loss 0.0257
[2019-04-28 00:27:36,898] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 66500, global step 1063529: learning rate 0.0000
[2019-04-28 00:27:36,925] A3C_AGENT_WORKER-Thread-13 INFO:Local step 66500, global step 1063546: loss 0.0418
[2019-04-28 00:27:36,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 66500, global step 1063546: learning rate 0.0000
[2019-04-28 00:27:37,111] A3C_AGENT_WORKER-Thread-21 INFO:Local step 66500, global step 1063622: loss 0.0016
[2019-04-28 00:27:37,115] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 66500, global step 1063624: learning rate 0.0000
[2019-04-28 00:27:37,280] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.1231790e-18 6.9536606e-11 2.7328644e-26 1.0000000e+00 7.1045237e-22], sum to 1.0000
[2019-04-28 00:27:37,281] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8432
[2019-04-28 00:27:37,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.1872385100014587, 1.0, 2.0, 0.1872385100014587, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 567315.2081962931, 567315.2081962938, 239332.3095940255], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 160800.0000, 
sim time next is 161400.0000, 
raw observation next is [21.9, 96.0, 1.0, 2.0, 0.1851178016836685, 1.0, 2.0, 0.1851178016836685, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 562666.6377250128, 562666.6377250123, 239312.6570945864], 
processed observation next is [1.0, 0.8695652173913043, 0.23696682464454974, 0.96, 1.0, 1.0, 0.01821421889598614, 1.0, 1.0, 0.01821421889598614, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.15629628825694802, 0.15629628825694786, 0.35718307029042745], 
reward next is 0.6428, 
noisyNet noise sample is [array([-1.388588], dtype=float32), 1.3535249]. 
=============================================
[2019-04-28 00:27:38,734] A3C_AGENT_WORKER-Thread-14 INFO:Local step 66500, global step 1064273: loss 0.2504
[2019-04-28 00:27:38,746] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 66500, global step 1064273: learning rate 0.0000
[2019-04-28 00:27:39,415] A3C_AGENT_WORKER-Thread-16 INFO:Local step 66500, global step 1064539: loss 0.6456
[2019-04-28 00:27:39,420] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 66500, global step 1064541: learning rate 0.0000
[2019-04-28 00:27:39,427] A3C_AGENT_WORKER-Thread-12 INFO:Local step 66500, global step 1064544: loss 0.7124
[2019-04-28 00:27:39,428] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 66500, global step 1064544: learning rate 0.0000
[2019-04-28 00:27:39,792] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.3264393e-14 2.0898514e-07 3.4987048e-20 9.9999976e-01 8.6893087e-16], sum to 1.0000
[2019-04-28 00:27:39,808] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4269
[2019-04-28 00:27:39,812] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.63333333333333, 96.0, 1.0, 2.0, 0.3723638901406652, 1.0, 2.0, 0.3723638901406652, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1098207.350675969, 1098207.350675969, 272362.000077486], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 142800.0000, 
sim time next is 143400.0000, 
raw observation next is [22.61666666666667, 96.0, 1.0, 2.0, 0.3753001038705613, 1.0, 2.0, 0.3753001038705613, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1107336.835974751, 1107336.835974751, 273149.3875512084], 
processed observation next is [1.0, 0.6521739130434783, 0.2709320695102688, 0.96, 1.0, 1.0, 0.247349522735616, 1.0, 1.0, 0.247349522735616, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.30759356554854195, 0.30759356554854195, 0.40768565306150506], 
reward next is 0.5923, 
noisyNet noise sample is [array([-1.5689491], dtype=float32), -0.12153707]. 
=============================================
[2019-04-28 00:27:40,202] A3C_AGENT_WORKER-Thread-19 INFO:Local step 66500, global step 1064839: loss 0.5143
[2019-04-28 00:27:40,204] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 66500, global step 1064839: learning rate 0.0000
[2019-04-28 00:27:40,212] A3C_AGENT_WORKER-Thread-3 INFO:Local step 66500, global step 1064841: loss 0.4958
[2019-04-28 00:27:40,216] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 66500, global step 1064842: learning rate 0.0000
[2019-04-28 00:27:40,576] A3C_AGENT_WORKER-Thread-11 INFO:Local step 66500, global step 1064988: loss 0.2983
[2019-04-28 00:27:40,577] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 66500, global step 1064988: learning rate 0.0000
[2019-04-28 00:27:40,721] A3C_AGENT_WORKER-Thread-22 INFO:Local step 66500, global step 1065046: loss 0.3627
[2019-04-28 00:27:40,723] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 66500, global step 1065047: learning rate 0.0000
[2019-04-28 00:27:41,631] A3C_AGENT_WORKER-Thread-17 INFO:Local step 66500, global step 1065403: loss 0.7208
[2019-04-28 00:27:41,634] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 66500, global step 1065405: learning rate 0.0000
[2019-04-28 00:27:41,821] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.4597920e-08 5.5413514e-02 5.6852859e-11 9.4458640e-01 1.5001043e-09], sum to 1.0000
[2019-04-28 00:27:41,832] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3890
[2019-04-28 00:27:41,847] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [20.7, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 509941.1643249152, 509941.1643249152, 235411.5441078946], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 168600.0000, 
sim time next is 169200.0000, 
raw observation next is [20.6, 96.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 504117.3357859528, 504117.3357859528, 234271.4620034345], 
processed observation next is [1.0, 1.0, 0.17535545023696694, 0.96, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14003259327387577, 0.14003259327387577, 0.3496588985125888], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.88643545], dtype=float32), -1.0818048]. 
=============================================
[2019-04-28 00:27:44,954] A3C_AGENT_WORKER-Thread-15 INFO:Local step 66500, global step 1066752: loss 2.3601
[2019-04-28 00:27:44,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 66500, global step 1066754: learning rate 0.0000
[2019-04-28 00:27:45,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.8168186e-24 9.9999607e-01 0.0000000e+00 3.8776266e-06 2.1076494e-33], sum to 1.0000
[2019-04-28 00:27:45,066] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1462
[2019-04-28 00:27:45,070] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3819850059386324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576836.0541452381, 576836.0541452381, 172637.409321775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3803188587064071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574320.0518680543, 574320.0518680537, 172414.0207110183], 
processed observation next is [1.0, 0.8695652173913043, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2533962153089242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595333477411262, 0.15953334774112604, 0.25733435927017656], 
reward next is 0.7427, 
noisyNet noise sample is [array([-1.2068225], dtype=float32), 0.8128689]. 
=============================================
[2019-04-28 00:27:46,283] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67000, global step 1067303: loss 0.6658
[2019-04-28 00:27:46,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67000, global step 1067304: learning rate 0.0000
[2019-04-28 00:27:46,489] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6800639e-19 9.9995494e-01 5.4524580e-34 4.5083703e-05 1.4969511e-25], sum to 1.0000
[2019-04-28 00:27:46,496] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9652
[2019-04-28 00:27:46,502] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 92.0, 1.0, 2.0, 0.2852606891197435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458312.3129156519, 458312.3129156519, 164154.6301250039], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 262800.0000, 
sim time next is 263400.0000, 
raw observation next is [20.5, 92.0, 1.0, 2.0, 0.2852929365555176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 458364.2539521999, 458364.2539521999, 164158.1624837981], 
processed observation next is [0.0, 0.043478260869565216, 0.1706161137440759, 0.92, 1.0, 1.0, 0.13890715247652718, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12732340387561109, 0.12732340387561109, 0.24501218281163897], 
reward next is 0.7550, 
noisyNet noise sample is [array([-0.37488663], dtype=float32), -0.09369654]. 
=============================================
[2019-04-28 00:27:53,411] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67000, global step 1070295: loss 0.0041
[2019-04-28 00:27:53,414] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67000, global step 1070295: learning rate 0.0000
[2019-04-28 00:27:55,254] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67000, global step 1071050: loss 0.1945
[2019-04-28 00:27:55,255] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67000, global step 1071050: learning rate 0.0000
[2019-04-28 00:27:55,893] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67000, global step 1071304: loss 0.1399
[2019-04-28 00:27:55,896] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67000, global step 1071304: learning rate 0.0000
[2019-04-28 00:27:56,080] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67000, global step 1071364: loss 0.2479
[2019-04-28 00:27:56,085] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67000, global step 1071364: learning rate 0.0000
[2019-04-28 00:27:56,263] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4359563e-19 9.9999988e-01 3.1485948e-32 1.3762440e-07 3.6355991e-25], sum to 1.0000
[2019-04-28 00:27:56,272] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4084
[2019-04-28 00:27:56,276] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.91666666666667, 80.83333333333333, 1.0, 2.0, 0.2640749210196531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 431159.913625452, 431159.9136254514, 162295.4495051271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 414600.0000, 
sim time next is 415200.0000, 
raw observation next is [20.83333333333334, 80.66666666666667, 1.0, 2.0, 0.2606400843770779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426190.2617188799, 426190.2617188799, 161959.4254899148], 
processed observation next is [1.0, 0.8260869565217391, 0.1864139020537128, 0.8066666666666668, 1.0, 1.0, 0.10920492093623844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11838618381079997, 0.11838618381079997, 0.241730485805843], 
reward next is 0.7583, 
noisyNet noise sample is [array([0.57540244], dtype=float32), 0.7230578]. 
=============================================
[2019-04-28 00:27:56,320] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67000, global step 1071463: loss 0.0060
[2019-04-28 00:27:56,321] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67000, global step 1071463: learning rate 0.0000
[2019-04-28 00:27:57,193] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67000, global step 1071802: loss 0.0496
[2019-04-28 00:27:57,198] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67000, global step 1071803: learning rate 0.0000
[2019-04-28 00:27:57,874] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67000, global step 1072072: loss 0.0311
[2019-04-28 00:27:57,884] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67000, global step 1072077: learning rate 0.0000
[2019-04-28 00:27:58,883] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67000, global step 1072474: loss 0.0002
[2019-04-28 00:27:58,886] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67000, global step 1072474: learning rate 0.0000
[2019-04-28 00:27:59,024] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67000, global step 1072539: loss 0.0165
[2019-04-28 00:27:59,030] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67000, global step 1072540: learning rate 0.0000
[2019-04-28 00:27:59,641] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67000, global step 1072778: loss 0.0068
[2019-04-28 00:27:59,642] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67000, global step 1072778: learning rate 0.0000
[2019-04-28 00:27:59,697] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67000, global step 1072796: loss 0.0547
[2019-04-28 00:27:59,700] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67000, global step 1072797: learning rate 0.0000
[2019-04-28 00:28:00,501] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.4299944e-17 9.9998260e-01 2.1047029e-27 1.7416061e-05 4.5285973e-22], sum to 1.0000
[2019-04-28 00:28:00,502] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67000, global step 1073129: loss 0.1300
[2019-04-28 00:28:00,505] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67000, global step 1073129: learning rate 0.0000
[2019-04-28 00:28:00,508] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8790
[2019-04-28 00:28:00,515] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.76666666666667, 87.0, 1.0, 2.0, 0.2281675200590061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 378658.161109756, 378658.1611097553, 158610.1881972825], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 519600.0000, 
sim time next is 520200.0000, 
raw observation next is [18.75, 87.0, 1.0, 2.0, 0.2275384810893793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 377668.3390990801, 377668.3390990808, 158545.4468832665], 
processed observation next is [1.0, 0.0, 0.08767772511848347, 0.87, 1.0, 1.0, 0.06932347119202324, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1049078719719667, 0.1049078719719669, 0.23663499534815896], 
reward next is 0.7634, 
noisyNet noise sample is [array([1.2727823], dtype=float32), -0.6191145]. 
=============================================
[2019-04-28 00:28:00,707] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67000, global step 1073211: loss 0.0416
[2019-04-28 00:28:00,708] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67000, global step 1073211: learning rate 0.0000
[2019-04-28 00:28:01,459] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67000, global step 1073522: loss 0.0791
[2019-04-28 00:28:01,460] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67000, global step 1073522: learning rate 0.0000
[2019-04-28 00:28:04,888] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67000, global step 1074874: loss 0.0162
[2019-04-28 00:28:04,891] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67000, global step 1074875: learning rate 0.0000
[2019-04-28 00:28:05,182] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-28 00:28:05,183] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:28:05,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:28:05,187] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:28:05,187] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:28:05,190] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:28:05,197] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:28:05,197] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:28:05,198] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:28:05,199] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:28:05,199] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:28:05,226] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run44
[2019-04-28 00:28:05,260] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run44
[2019-04-28 00:28:05,261] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run44
[2019-04-28 00:28:05,341] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run44
[2019-04-28 00:28:05,378] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run44
[2019-04-28 00:29:08,521] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.062009152]
[2019-04-28 00:29:08,522] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.0, 89.0, 1.0, 2.0, 0.5064369021369601, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707669.8826862844, 707669.882686285, 184610.3440527181]
[2019-04-28 00:29:08,523] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:29:08,528] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.4177495e-21 9.9999964e-01 1.6273546e-33 3.8237704e-07 7.7208136e-27], sampled 0.21324171274903359
[2019-04-28 00:29:09,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.062009152]
[2019-04-28 00:29:09,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 78.0, 1.0, 2.0, 0.5702163290496458, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 796825.5275609083, 796825.527560909, 195328.0523428456]
[2019-04-28 00:29:09,215] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:29:09,218] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4090464e-21 9.9999964e-01 6.0884069e-34 3.1641002e-07 3.5342798e-27], sampled 0.027142944732784002
[2019-04-28 00:29:31,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.062009152]
[2019-04-28 00:29:31,596] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.44595968833334, 61.83251409833333, 1.0, 2.0, 0.9141543061018863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104137, 1331938.775192811, 1331938.775192811, 281837.7669457538]
[2019-04-28 00:29:31,597] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:29:31,598] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8876228e-20 9.9999905e-01 1.4160298e-31 9.1056836e-07 2.6439664e-25], sampled 0.024340764595659814
[2019-04-28 00:29:33,516] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.062009152]
[2019-04-28 00:29:33,517] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.76666666666667, 67.66666666666667, 1.0, 2.0, 0.6751225012505185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 943487.4875156204, 943487.4875156204, 215570.5333552676]
[2019-04-28 00:29:33,517] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:29:33,520] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.2354913e-22 9.9999976e-01 6.9132137e-35 2.0361243e-07 6.1965140e-28], sampled 0.5833687830718294
[2019-04-28 00:29:41,465] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:29:41,910] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:29:42,015] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:29:42,018] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:29:42,616] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:29:43,628] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1075000, evaluation results [1075000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:29:44,096] A3C_AGENT_WORKER-Thread-18 INFO:Local step 67500, global step 1075199: loss 0.1541
[2019-04-28 00:29:44,096] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 67500, global step 1075199: learning rate 0.0000
[2019-04-28 00:29:49,556] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3571208e-16 9.9991167e-01 1.4845666e-28 8.8281442e-05 2.3256557e-21], sum to 1.0000
[2019-04-28 00:29:49,572] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4868
[2019-04-28 00:29:49,579] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 70.5, 1.0, 2.0, 0.4084047101630429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671773.8923314483, 671773.8923314483, 180715.5563780493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 552600.0000, 
sim time next is 553200.0000, 
raw observation next is [22.0, 69.0, 1.0, 2.0, 0.4610482251419944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758572.5729980959, 758572.5729980959, 189053.0230014907], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.69, 1.0, 1.0, 0.3506605122192703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2107146036105822, 0.2107146036105822, 0.28216869104700104], 
reward next is 0.7178, 
noisyNet noise sample is [array([-0.6012306], dtype=float32), 0.8410094]. 
=============================================
[2019-04-28 00:29:51,551] A3C_AGENT_WORKER-Thread-2 INFO:Local step 67500, global step 1078229: loss 0.0152
[2019-04-28 00:29:51,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 67500, global step 1078229: learning rate 0.0000
[2019-04-28 00:29:53,832] A3C_AGENT_WORKER-Thread-20 INFO:Local step 67500, global step 1079165: loss 0.0015
[2019-04-28 00:29:53,834] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 67500, global step 1079165: learning rate 0.0000
[2019-04-28 00:29:54,136] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.7412304e-18 9.9998748e-01 3.4729478e-26 1.2555905e-05 1.1333691e-20], sum to 1.0000
[2019-04-28 00:29:54,154] A3C_AGENT_WORKER-Thread-10 INFO:Local step 67500, global step 1079287: loss 0.2056
[2019-04-28 00:29:54,156] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4874
[2019-04-28 00:29:54,158] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 67500, global step 1079287: learning rate 0.0000
[2019-04-28 00:29:54,163] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.83333333333333, 1.0, 2.0, 0.220778831403438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 367787.6481783985, 367787.6481783985, 157666.9052754362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 702600.0000, 
sim time next is 703200.0000, 
raw observation next is [17.6, 92.66666666666667, 1.0, 2.0, 0.2168974310259757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 361364.6682528363, 361364.6682528363, 157314.7848902274], 
processed observation next is [1.0, 0.13043478260869565, 0.0331753554502371, 0.9266666666666667, 1.0, 1.0, 0.05650292894695866, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10037907451467674, 0.10037907451467674, 0.23479818640332448], 
reward next is 0.7652, 
noisyNet noise sample is [array([-0.66044164], dtype=float32), -0.7999517]. 
=============================================
[2019-04-28 00:29:54,248] A3C_AGENT_WORKER-Thread-9 INFO:Local step 67500, global step 1079327: loss 0.1088
[2019-04-28 00:29:54,255] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 67500, global step 1079329: learning rate 0.0000
[2019-04-28 00:29:54,765] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [9.1930570e-17 9.9997807e-01 1.1292903e-25 2.1981186e-05 4.5798983e-21], sum to 1.0000
[2019-04-28 00:29:54,771] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4942
[2019-04-28 00:29:54,775] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 50.0, 1.0, 2.0, 0.6286001612381256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1027981.185747482, 1027981.185747482, 221830.122651141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 739800.0000, 
sim time next is 740400.0000, 
raw observation next is [25.86666666666667, 49.33333333333334, 1.0, 2.0, 0.6256760036857524, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1024025.362968306, 1024025.362968305, 221193.4819674156], 
processed observation next is [1.0, 0.5652173913043478, 0.42496050552922615, 0.4933333333333334, 1.0, 1.0, 0.5490072333563282, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2844514897134183, 0.28445148971341805, 0.3301395253245009], 
reward next is 0.6699, 
noisyNet noise sample is [array([-0.2011835], dtype=float32), -1.0479425]. 
=============================================
[2019-04-28 00:29:54,978] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.6892298e-17 9.9916756e-01 9.1220381e-28 8.3251845e-04 1.8322620e-23], sum to 1.0000
[2019-04-28 00:29:54,985] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3587
[2019-04-28 00:29:55,006] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.6, 92.16666666666667, 1.0, 2.0, 0.2156493060691995, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 359411.5057573406, 359411.5057573401, 157168.2631591955], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 705000.0000, 
sim time next is 705600.0000, 
raw observation next is [17.6, 92.0, 1.0, 2.0, 0.2149129967709744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 358225.1785073982, 358225.1785073988, 157091.67273694], 
processed observation next is [1.0, 0.17391304347826086, 0.0331753554502371, 0.92, 1.0, 1.0, 0.05411204430237877, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09950699402983283, 0.099506994029833, 0.2344651831894627], 
reward next is 0.7655, 
noisyNet noise sample is [array([1.5606545], dtype=float32), -0.94735634]. 
=============================================
[2019-04-28 00:29:55,107] A3C_AGENT_WORKER-Thread-21 INFO:Local step 67500, global step 1079671: loss 0.0015
[2019-04-28 00:29:55,109] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 67500, global step 1079671: learning rate 0.0000
[2019-04-28 00:29:55,907] A3C_AGENT_WORKER-Thread-14 INFO:Local step 67500, global step 1079968: loss 0.2321
[2019-04-28 00:29:55,910] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 67500, global step 1079968: learning rate 0.0000
[2019-04-28 00:29:56,149] A3C_AGENT_WORKER-Thread-13 INFO:Local step 67500, global step 1080068: loss 0.0238
[2019-04-28 00:29:56,154] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 67500, global step 1080068: learning rate 0.0000
[2019-04-28 00:29:56,210] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.64117713e-19 9.99981999e-01 1.20346525e-29 1.79519375e-05
 3.69026828e-23], sum to 1.0000
[2019-04-28 00:29:56,234] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8040
[2019-04-28 00:29:56,243] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3380691997389883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523681.2813667222, 523681.2813667222, 168583.7487958096], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 946800.0000, 
sim time next is 947400.0000, 
raw observation next is [21.8, 94.00000000000001, 1.0, 2.0, 0.3376778345863402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 523118.7175607817, 523118.7175607817, 168540.317590579], 
processed observation next is [0.0, 1.0, 0.23222748815165886, 0.9400000000000002, 1.0, 1.0, 0.20202148745342194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1453107548779949, 0.1453107548779949, 0.2515527128217597], 
reward next is 0.7484, 
noisyNet noise sample is [array([-1.2877593], dtype=float32), -0.15928543]. 
=============================================
[2019-04-28 00:29:57,091] A3C_AGENT_WORKER-Thread-16 INFO:Local step 67500, global step 1080449: loss 0.0116
[2019-04-28 00:29:57,094] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 67500, global step 1080449: learning rate 0.0000
[2019-04-28 00:29:57,607] A3C_AGENT_WORKER-Thread-3 INFO:Local step 67500, global step 1080670: loss 0.1095
[2019-04-28 00:29:57,614] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 67500, global step 1080670: learning rate 0.0000
[2019-04-28 00:29:57,661] A3C_AGENT_WORKER-Thread-12 INFO:Local step 67500, global step 1080692: loss 0.0247
[2019-04-28 00:29:57,662] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 67500, global step 1080692: learning rate 0.0000
[2019-04-28 00:29:58,329] A3C_AGENT_WORKER-Thread-22 INFO:Local step 67500, global step 1080967: loss 0.1042
[2019-04-28 00:29:58,334] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 67500, global step 1080969: learning rate 0.0000
[2019-04-28 00:29:58,386] A3C_AGENT_WORKER-Thread-11 INFO:Local step 67500, global step 1080995: loss 0.2078
[2019-04-28 00:29:58,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 67500, global step 1080996: learning rate 0.0000
[2019-04-28 00:29:58,511] A3C_AGENT_WORKER-Thread-19 INFO:Local step 67500, global step 1081045: loss 0.0300
[2019-04-28 00:29:58,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 67500, global step 1081045: learning rate 0.0000
[2019-04-28 00:29:59,541] A3C_AGENT_WORKER-Thread-17 INFO:Local step 67500, global step 1081467: loss 0.0941
[2019-04-28 00:29:59,543] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 67500, global step 1081467: learning rate 0.0000
[2019-04-28 00:29:59,628] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1503943e-15 9.9997282e-01 1.2817364e-27 2.7170452e-05 5.1884504e-21], sum to 1.0000
[2019-04-28 00:29:59,636] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8380
[2019-04-28 00:29:59,640] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.4, 92.0, 1.0, 2.0, 0.2599698969719779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425513.1070046628, 425513.1070046628, 161899.20165011], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 784800.0000, 
sim time next is 785400.0000, 
raw observation next is [19.4, 92.0, 1.0, 2.0, 0.2598266316685817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425273.1627359597, 425273.1627359597, 161884.5354224691], 
processed observation next is [0.0, 0.08695652173913043, 0.11848341232227487, 0.92, 1.0, 1.0, 0.10822485743202616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11813143409332214, 0.11813143409332214, 0.2416187095857748], 
reward next is 0.7584, 
noisyNet noise sample is [array([0.5780535], dtype=float32), -1.4307349]. 
=============================================
[2019-04-28 00:30:00,877] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0455955e-19 9.9970895e-01 7.9066411e-28 2.9099497e-04 1.3889856e-23], sum to 1.0000
[2019-04-28 00:30:00,884] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7084
[2019-04-28 00:30:00,890] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2936542726538034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 468752.1887341646, 468752.1887341653, 164843.3139199838], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 897600.0000, 
sim time next is 898200.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2936097048136517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 468681.2068542926, 468681.2068542932, 164838.3568301626], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14892735519717068, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13018922412619238, 0.13018922412619255, 0.24602739825397404], 
reward next is 0.7540, 
noisyNet noise sample is [array([-1.3273892], dtype=float32), -0.99023753]. 
=============================================
[2019-04-28 00:30:02,896] A3C_AGENT_WORKER-Thread-15 INFO:Local step 67500, global step 1082811: loss 0.0593
[2019-04-28 00:30:02,914] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 67500, global step 1082812: learning rate 0.0000
[2019-04-28 00:30:02,941] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5443080e-18 9.9999952e-01 1.6474506e-30 4.8010060e-07 5.3770690e-23], sum to 1.0000
[2019-04-28 00:30:02,956] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4688
[2019-04-28 00:30:02,959] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.9, 62.5, 1.0, 2.0, 0.2886547487374136, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 462167.6222549282, 462167.6222549276, 164404.7663079722], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 822600.0000, 
sim time next is 823200.0000, 
raw observation next is [24.86666666666667, 62.66666666666667, 1.0, 2.0, 0.288554224655352, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 462045.0355241917, 462045.0355241924, 164396.7581199094], 
processed observation next is [0.0, 0.5217391304347826, 0.3775671406003162, 0.6266666666666667, 1.0, 1.0, 0.14283641524741206, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12834584320116435, 0.12834584320116454, 0.24536829570135732], 
reward next is 0.7546, 
noisyNet noise sample is [array([1.3841914], dtype=float32), 0.17451397]. 
=============================================
[2019-04-28 00:30:03,533] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68000, global step 1083074: loss 0.1756
[2019-04-28 00:30:03,536] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68000, global step 1083075: learning rate 0.0000
[2019-04-28 00:30:04,576] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.8203341e-16 9.9967790e-01 4.7510504e-25 3.2210560e-04 9.2959602e-19], sum to 1.0000
[2019-04-28 00:30:04,596] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5136
[2019-04-28 00:30:04,599] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.2, 82.33333333333334, 1.0, 2.0, 0.5205601970345691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827988.8692706674, 827988.8692706674, 198243.4293223979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1074000.0000, 
sim time next is 1074600.0000, 
raw observation next is [22.3, 81.5, 1.0, 2.0, 0.54169015138309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 861854.4169165186, 861854.416916518, 202247.3206357979], 
processed observation next is [1.0, 0.43478260869565216, 0.25592417061611383, 0.815, 1.0, 1.0, 0.4478194594976987, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23940400469903295, 0.23940400469903278, 0.30186167259074315], 
reward next is 0.6981, 
noisyNet noise sample is [array([0.37903756], dtype=float32), -2.2624063]. 
=============================================
[2019-04-28 00:30:07,931] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3652769e-19 1.0000000e+00 2.1756544e-28 3.9170732e-08 2.6995681e-22], sum to 1.0000
[2019-04-28 00:30:07,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2823
[2019-04-28 00:30:07,945] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2944205443343038, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469975.2778719852, 469975.2778719852, 164928.8755524824], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 903600.0000, 
sim time next is 904200.0000, 
raw observation next is [22.63333333333333, 78.16666666666667, 1.0, 2.0, 0.2951472756626367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471027.3742975031, 471027.3742975031, 165001.1317824985], 
processed observation next is [0.0, 0.4782608695652174, 0.27172195892575024, 0.7816666666666667, 1.0, 1.0, 0.15077985019594783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13084093730486196, 0.13084093730486196, 0.2462703459440276], 
reward next is 0.7537, 
noisyNet noise sample is [array([-0.542831], dtype=float32), 0.62124]. 
=============================================
[2019-04-28 00:30:09,594] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68000, global step 1086215: loss 0.0753
[2019-04-28 00:30:09,596] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68000, global step 1086215: learning rate 0.0000
[2019-04-28 00:30:11,500] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68000, global step 1087211: loss 0.0531
[2019-04-28 00:30:11,503] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68000, global step 1087211: learning rate 0.0000
[2019-04-28 00:30:11,638] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68000, global step 1087281: loss 0.0595
[2019-04-28 00:30:11,640] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68000, global step 1087282: learning rate 0.0000
[2019-04-28 00:30:11,997] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68000, global step 1087470: loss 0.0008
[2019-04-28 00:30:11,999] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68000, global step 1087471: learning rate 0.0000
[2019-04-28 00:30:12,432] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68000, global step 1087696: loss 0.0393
[2019-04-28 00:30:12,434] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68000, global step 1087696: learning rate 0.0000
[2019-04-28 00:30:12,842] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68000, global step 1087906: loss 0.0424
[2019-04-28 00:30:12,844] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68000, global step 1087908: learning rate 0.0000
[2019-04-28 00:30:12,997] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68000, global step 1087987: loss 0.0518
[2019-04-28 00:30:12,998] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68000, global step 1087987: learning rate 0.0000
[2019-04-28 00:30:13,654] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68000, global step 1088331: loss 0.0361
[2019-04-28 00:30:13,657] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68000, global step 1088331: learning rate 0.0000
[2019-04-28 00:30:14,412] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68000, global step 1088718: loss 0.1207
[2019-04-28 00:30:14,416] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68000, global step 1088720: learning rate 0.0000
[2019-04-28 00:30:14,500] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68000, global step 1088768: loss 0.1265
[2019-04-28 00:30:14,502] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68000, global step 1088769: learning rate 0.0000
[2019-04-28 00:30:14,662] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68000, global step 1088855: loss 0.0184
[2019-04-28 00:30:14,663] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68000, global step 1088855: learning rate 0.0000
[2019-04-28 00:30:14,926] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68000, global step 1088984: loss 0.0180
[2019-04-28 00:30:14,930] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68000, global step 1088985: learning rate 0.0000
[2019-04-28 00:30:15,212] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68000, global step 1089135: loss 0.0160
[2019-04-28 00:30:15,213] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68000, global step 1089135: learning rate 0.0000
[2019-04-28 00:30:15,721] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68000, global step 1089400: loss 0.1309
[2019-04-28 00:30:15,724] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68000, global step 1089400: learning rate 0.0000
[2019-04-28 00:30:18,397] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68000, global step 1090787: loss 0.0017
[2019-04-28 00:30:18,398] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68000, global step 1090787: learning rate 0.0000
[2019-04-28 00:30:19,118] A3C_AGENT_WORKER-Thread-18 INFO:Local step 68500, global step 1091161: loss 0.0526
[2019-04-28 00:30:19,121] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 68500, global step 1091161: learning rate 0.0000
[2019-04-28 00:30:23,714] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8405089e-21 1.0000000e+00 5.7395195e-30 7.0213546e-10 4.8112291e-24], sum to 1.0000
[2019-04-28 00:30:23,724] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3871
[2019-04-28 00:30:23,729] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.35, 91.66666666666667, 1.0, 2.0, 0.3541920629782502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 545006.4198378765, 545006.4198378765, 170212.5421992257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1230600.0000, 
sim time next is 1231200.0000, 
raw observation next is [22.5, 91.0, 1.0, 2.0, 0.3613032816597336, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 554878.4009200283, 554878.4009200289, 171009.83021727], 
processed observation next is [1.0, 0.2608695652173913, 0.2654028436018958, 0.91, 1.0, 1.0, 0.2304858815177513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1541328891444523, 0.15413288914445247, 0.2552385525630896], 
reward next is 0.7448, 
noisyNet noise sample is [array([-1.7973267], dtype=float32), 0.0804475]. 
=============================================
[2019-04-28 00:30:25,449] A3C_AGENT_WORKER-Thread-2 INFO:Local step 68500, global step 1094281: loss 0.0274
[2019-04-28 00:30:25,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 68500, global step 1094281: learning rate 0.0000
[2019-04-28 00:30:26,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2115766e-20 1.0000000e+00 4.0974121e-28 8.9674018e-11 6.0888781e-25], sum to 1.0000
[2019-04-28 00:30:26,950] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0631
[2019-04-28 00:30:26,958] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.01666666666667, 93.16666666666667, 1.0, 2.0, 0.5090087975043996, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733309.3756783827, 733309.3756783834, 187819.6956918664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1318200.0000, 
sim time next is 1318800.0000, 
raw observation next is [23.93333333333334, 93.33333333333334, 1.0, 2.0, 0.5404809540497971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 780564.6154391714, 780564.6154391721, 193394.8741250193], 
processed observation next is [1.0, 0.2608695652173913, 0.3333333333333337, 0.9333333333333335, 1.0, 1.0, 0.4463625952407194, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21682350428865874, 0.21682350428865893, 0.28864906585823774], 
reward next is 0.7114, 
noisyNet noise sample is [array([-0.7207738], dtype=float32), 1.1054159]. 
=============================================
[2019-04-28 00:30:27,215] A3C_AGENT_WORKER-Thread-20 INFO:Local step 68500, global step 1095127: loss 0.0588
[2019-04-28 00:30:27,219] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 68500, global step 1095128: learning rate 0.0000
[2019-04-28 00:30:27,737] A3C_AGENT_WORKER-Thread-9 INFO:Local step 68500, global step 1095381: loss 0.0615
[2019-04-28 00:30:27,740] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 68500, global step 1095382: learning rate 0.0000
[2019-04-28 00:30:27,750] A3C_AGENT_WORKER-Thread-10 INFO:Local step 68500, global step 1095383: loss 0.0313
[2019-04-28 00:30:27,750] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 68500, global step 1095384: learning rate 0.0000
[2019-04-28 00:30:28,392] A3C_AGENT_WORKER-Thread-21 INFO:Local step 68500, global step 1095698: loss 0.1406
[2019-04-28 00:30:28,397] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 68500, global step 1095699: learning rate 0.0000
[2019-04-28 00:30:28,881] A3C_AGENT_WORKER-Thread-14 INFO:Local step 68500, global step 1095933: loss 0.0081
[2019-04-28 00:30:28,882] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 68500, global step 1095933: learning rate 0.0000
[2019-04-28 00:30:29,102] A3C_AGENT_WORKER-Thread-13 INFO:Local step 68500, global step 1096035: loss 0.0452
[2019-04-28 00:30:29,106] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 68500, global step 1096036: learning rate 0.0000
[2019-04-28 00:30:29,754] A3C_AGENT_WORKER-Thread-16 INFO:Local step 68500, global step 1096348: loss 0.0038
[2019-04-28 00:30:29,756] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 68500, global step 1096348: learning rate 0.0000
[2019-04-28 00:30:30,456] A3C_AGENT_WORKER-Thread-12 INFO:Local step 68500, global step 1096686: loss 0.0178
[2019-04-28 00:30:30,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 68500, global step 1096687: learning rate 0.0000
[2019-04-28 00:30:30,635] A3C_AGENT_WORKER-Thread-11 INFO:Local step 68500, global step 1096768: loss 0.0156
[2019-04-28 00:30:30,636] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 68500, global step 1096770: learning rate 0.0000
[2019-04-28 00:30:30,791] A3C_AGENT_WORKER-Thread-3 INFO:Local step 68500, global step 1096847: loss 0.0490
[2019-04-28 00:30:30,797] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 68500, global step 1096847: learning rate 0.0000
[2019-04-28 00:30:31,323] A3C_AGENT_WORKER-Thread-19 INFO:Local step 68500, global step 1097099: loss 0.0232
[2019-04-28 00:30:31,327] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 68500, global step 1097101: learning rate 0.0000
[2019-04-28 00:30:31,359] A3C_AGENT_WORKER-Thread-22 INFO:Local step 68500, global step 1097120: loss 0.0121
[2019-04-28 00:30:31,363] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 68500, global step 1097120: learning rate 0.0000
[2019-04-28 00:30:31,729] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.94481577e-18 1.00000000e+00 2.67102785e-31 3.60908248e-10
 1.19116595e-26], sum to 1.0000
[2019-04-28 00:30:31,740] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2604
[2019-04-28 00:30:31,744] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 87.0, 1.0, 2.0, 0.36253528235101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 552512.9618878377, 552512.9618878377, 170682.8029150509], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1416000.0000, 
sim time next is 1416600.0000, 
raw observation next is [23.5, 87.5, 1.0, 2.0, 0.3711971764614211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 561635.2414356708, 561635.2414356708, 171336.9893165899], 
processed observation next is [0.0, 0.391304347826087, 0.31279620853080575, 0.875, 1.0, 1.0, 0.24240623670050732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15600978928768633, 0.15600978928768633, 0.2557268497262536], 
reward next is 0.7443, 
noisyNet noise sample is [array([0.6101209], dtype=float32), 0.24381906]. 
=============================================
[2019-04-28 00:30:31,934] A3C_AGENT_WORKER-Thread-17 INFO:Local step 68500, global step 1097392: loss 0.0620
[2019-04-28 00:30:31,937] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 68500, global step 1097393: learning rate 0.0000
[2019-04-28 00:30:33,841] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.5191445e-25 1.0000000e+00 8.7604552e-34 9.5822785e-13 3.4386726e-28], sum to 1.0000
[2019-04-28 00:30:33,857] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9994
[2019-04-28 00:30:33,862] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 95.0, 1.0, 2.0, 0.4111974519779803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606916.7836883408, 606916.7836883408, 174989.8254714457], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1626600.0000, 
sim time next is 1627200.0000, 
raw observation next is [23.2, 95.0, 1.0, 2.0, 0.4105674953619807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 605614.1123385016, 605614.1123385016, 174857.0122912386], 
processed observation next is [1.0, 0.8695652173913043, 0.29857819905213273, 0.95, 1.0, 1.0, 0.2898403558578081, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16822614231625044, 0.16822614231625044, 0.2609806153600576], 
reward next is 0.7390, 
noisyNet noise sample is [array([-0.68018055], dtype=float32), 1.2341683]. 
=============================================
[2019-04-28 00:30:34,743] A3C_AGENT_WORKER-Thread-15 INFO:Local step 68500, global step 1098749: loss 0.2416
[2019-04-28 00:30:34,749] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 68500, global step 1098752: learning rate 0.0000
[2019-04-28 00:30:35,202] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1037038e-19 1.0000000e+00 7.3450708e-31 1.2608757e-10 9.9428216e-25], sum to 1.0000
[2019-04-28 00:30:35,208] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4566
[2019-04-28 00:30:35,215] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 69.0, 1.0, 2.0, 0.4277090929814758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617149.4771918209, 617149.4771918209, 175561.0670608429], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1436400.0000, 
sim time next is 1437000.0000, 
raw observation next is [27.66666666666667, 69.16666666666667, 1.0, 2.0, 0.4303329792422638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619344.9523717558, 619344.9523717551, 175728.4968300429], 
processed observation next is [0.0, 0.6521739130434783, 0.5102685624012641, 0.6916666666666668, 1.0, 1.0, 0.31365419185814924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17204026454770996, 0.17204026454770976, 0.26228133855230285], 
reward next is 0.7377, 
noisyNet noise sample is [array([1.2642546], dtype=float32), -0.10565964]. 
=============================================
[2019-04-28 00:30:35,236] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[71.841225]
 [71.82306 ]
 [71.82407 ]
 [71.82412 ]
 [71.823204]], R is [[71.84333801]
 [71.86287689]
 [71.88222504]
 [71.90136719]
 [71.92028046]].
[2019-04-28 00:30:35,435] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69000, global step 1099088: loss 0.0131
[2019-04-28 00:30:35,437] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69000, global step 1099088: learning rate 0.0000
[2019-04-28 00:30:37,331] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-28 00:30:37,333] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:30:37,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:30:37,337] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:30:37,336] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:30:37,341] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:30:37,339] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:30:37,338] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:30:37,339] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:30:37,343] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:30:37,344] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:30:37,364] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run45
[2019-04-28 00:30:37,398] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run45
[2019-04-28 00:30:37,435] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run45
[2019-04-28 00:30:37,468] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run45
[2019-04-28 00:30:37,471] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run45
[2019-04-28 00:30:49,236] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:30:49,239] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [21.126290425, 82.27084844000001, 1.0, 2.0, 0.2807118672292829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 454881.2268023198, 454881.2268023191, 163912.3639105176]
[2019-04-28 00:30:49,240] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:30:49,244] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.4247168e-21 1.0000000e+00 1.6599092e-31 5.3397365e-12 4.0054326e-26], sampled 0.2989767246717786
[2019-04-28 00:31:04,902] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:04,903] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [34.58957172166667, 77.89821830333334, 1.0, 2.0, 0.7462966924200225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1043002.664180447, 1043002.664180448, 231171.6544888583]
[2019-04-28 00:31:04,904] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:31:04,906] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5809640e-22 1.0000000e+00 1.6047553e-33 9.6866363e-13 8.6913126e-28], sampled 0.8310549143828508
[2019-04-28 00:31:17,380] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:17,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 67.33333333333334, 1.0, 2.0, 0.9107992662885668, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993939841452468, 6.9112, 168.9123996003449, 2170152.59480324, 2111454.297121629, 437523.8420812555]
[2019-04-28 00:31:17,383] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:31:17,386] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.6801925e-19 1.0000000e+00 1.9238060e-28 7.1377730e-11 1.3558594e-23], sampled 0.351565281943111
[2019-04-28 00:31:17,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2170152.59480324 W.
[2019-04-28 00:31:27,574] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:27,575] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.96510868666667, 82.74142607333333, 1.0, 2.0, 0.4681214631033261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664389.5892039635, 664389.5892039628, 180073.7062406469]
[2019-04-28 00:31:27,576] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:31:27,578] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0578000e-21 1.0000000e+00 2.8214351e-32 2.7791096e-12 9.2686742e-27], sampled 0.5780949101908541
[2019-04-28 00:31:32,701] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:32,703] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.35255387833334, 63.53263192333333, 1.0, 2.0, 0.4221273304617215, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628501.2984366826, 628501.2984366833, 177178.5163400053]
[2019-04-28 00:31:32,704] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:31:32,706] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2020235e-22 1.0000000e+00 7.0117569e-33 1.6684226e-12 2.9368032e-27], sampled 0.3269923854261463
[2019-04-28 00:31:40,005] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:40,006] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.94427685666667, 86.12044809, 1.0, 2.0, 0.5268122153305957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 736151.2033068084, 736151.2033068084, 187903.1090633588]
[2019-04-28 00:31:40,007] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:31:40,010] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.1117332e-22 1.0000000e+00 1.8905114e-32 2.3987188e-12 6.6591632e-27], sampled 0.44240005683819295
[2019-04-28 00:31:58,109] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:58,110] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.81131207666667, 93.82496761333334, 1.0, 2.0, 0.4368286331049781, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 631528.7739400055, 631528.773940006, 177004.7540852604]
[2019-04-28 00:31:58,112] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:31:58,115] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3601656e-21 1.0000000e+00 4.1223632e-32 3.1987295e-12 1.2680718e-26], sampled 0.2090688227677262
[2019-04-28 00:31:58,287] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:31:58,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.31666666666667, 92.5, 1.0, 2.0, 0.3513846605857371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 539524.8411805467, 539524.8411805461, 169723.7818709143]
[2019-04-28 00:31:58,291] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:31:58,292] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0435959e-21 1.0000000e+00 7.6184853e-32 4.0100705e-12 2.1057284e-26], sampled 0.0741229301037094
[2019-04-28 00:32:00,997] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061224785]
[2019-04-28 00:32:01,001] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.536647965, 93.49733792500001, 1.0, 2.0, 0.5811797537402887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 812151.7749135258, 812151.7749135258, 197291.9366878011]
[2019-04-28 00:32:01,002] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:32:01,004] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.7409372e-22 1.0000000e+00 5.8818758e-33 1.5609564e-12 2.5396917e-27], sampled 0.3776587589656494
[2019-04-28 00:32:02,166] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:32:02,409] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:32:02,604] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:32:02,653] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:32:02,837] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:32:03,852] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1100000, evaluation results [1100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:32:08,374] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69000, global step 1102191: loss 0.0040
[2019-04-28 00:32:08,376] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69000, global step 1102192: learning rate 0.0000
[2019-04-28 00:32:09,093] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6843495e-22 1.0000000e+00 9.2154544e-31 3.9101774e-11 2.7474460e-26], sum to 1.0000
[2019-04-28 00:32:09,101] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8596
[2019-04-28 00:32:09,109] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.65, 98.0, 1.0, 2.0, 0.4341939420624599, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 619786.8178648998, 619786.817864899, 175622.3809237987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1668600.0000, 
sim time next is 1669200.0000, 
raw observation next is [23.66666666666667, 98.0, 1.0, 2.0, 0.4493481692295083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640961.0280242809, 640961.0280242809, 177726.5250660207], 
processed observation next is [1.0, 0.30434782608695654, 0.3206951026856243, 0.98, 1.0, 1.0, 0.33656405931266065, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1780447300067447, 0.1780447300067447, 0.26526347024779207], 
reward next is 0.7347, 
noisyNet noise sample is [array([0.13322306], dtype=float32), -0.55844325]. 
=============================================
[2019-04-28 00:32:10,097] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69000, global step 1103045: loss 0.0187
[2019-04-28 00:32:10,099] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69000, global step 1103045: learning rate 0.0000
[2019-04-28 00:32:10,673] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69000, global step 1103333: loss 0.0632
[2019-04-28 00:32:10,677] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69000, global step 1103335: learning rate 0.0000
[2019-04-28 00:32:10,712] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69000, global step 1103351: loss 0.0621
[2019-04-28 00:32:10,714] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69000, global step 1103351: learning rate 0.0000
[2019-04-28 00:32:11,491] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69000, global step 1103730: loss 0.0436
[2019-04-28 00:32:11,497] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69000, global step 1103730: learning rate 0.0000
[2019-04-28 00:32:11,710] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69000, global step 1103839: loss 0.0065
[2019-04-28 00:32:11,713] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69000, global step 1103839: learning rate 0.0000
[2019-04-28 00:32:11,840] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69000, global step 1103903: loss 0.0127
[2019-04-28 00:32:11,842] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69000, global step 1103904: learning rate 0.0000
[2019-04-28 00:32:12,746] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69000, global step 1104350: loss 0.1093
[2019-04-28 00:32:12,749] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69000, global step 1104350: learning rate 0.0000
[2019-04-28 00:32:13,358] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.8562564e-18 1.0000000e+00 3.8317621e-26 2.1810123e-11 7.9071536e-23], sum to 1.0000
[2019-04-28 00:32:13,362] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1769
[2019-04-28 00:32:13,367] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.33333333333334, 1.0, 2.0, 0.3163993274495812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497034.0225844915, 497034.0225844921, 166728.1809391894], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1575600.0000, 
sim time next is 1576200.0000, 
raw observation next is [22.1, 88.16666666666667, 1.0, 2.0, 0.3189601329738624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 500083.7246922507, 500083.7246922507, 166931.8206902927], 
processed observation next is [1.0, 0.21739130434782608, 0.24644549763033188, 0.8816666666666667, 1.0, 1.0, 0.17947003972754505, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1389121457478474, 0.1389121457478474, 0.24915197117954133], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.4213842], dtype=float32), 0.21528481]. 
=============================================
[2019-04-28 00:32:13,593] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69000, global step 1104752: loss 0.5622
[2019-04-28 00:32:13,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69000, global step 1104752: learning rate 0.0000
[2019-04-28 00:32:13,801] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69000, global step 1104850: loss 0.3287
[2019-04-28 00:32:13,802] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69000, global step 1104850: learning rate 0.0000
[2019-04-28 00:32:13,864] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69000, global step 1104878: loss 0.2511
[2019-04-28 00:32:13,866] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69000, global step 1104878: learning rate 0.0000
[2019-04-28 00:32:13,993] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3720120e-19 1.0000000e+00 4.4500555e-29 8.0556867e-11 4.1068588e-23], sum to 1.0000
[2019-04-28 00:32:13,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0535
[2019-04-28 00:32:14,010] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.28333333333333, 99.0, 1.0, 2.0, 0.4686413268067078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675854.8916661543, 675854.8916661543, 181507.2084592294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1651800.0000, 
sim time next is 1652400.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4658711685958647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 671398.7925332873, 671398.7925332873, 181028.0595106483], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.35647128746489726, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1864996645925798, 0.1864996645925798, 0.2701911335979825], 
reward next is 0.7298, 
noisyNet noise sample is [array([-0.04842675], dtype=float32), 0.4015547]. 
=============================================
[2019-04-28 00:32:14,312] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69000, global step 1105039: loss 0.1701
[2019-04-28 00:32:14,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69000, global step 1105040: learning rate 0.0000
[2019-04-28 00:32:14,346] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69000, global step 1105058: loss 0.2047
[2019-04-28 00:32:14,349] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69000, global step 1105058: learning rate 0.0000
[2019-04-28 00:32:15,136] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69000, global step 1105437: loss 0.1003
[2019-04-28 00:32:15,138] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69000, global step 1105438: learning rate 0.0000
[2019-04-28 00:32:15,904] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4659822e-18 1.0000000e+00 2.0182464e-29 1.8727078e-09 9.3635287e-26], sum to 1.0000
[2019-04-28 00:32:15,912] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6469
[2019-04-28 00:32:15,917] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 82.0, 1.0, 2.0, 0.5063752165239351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707583.6575528891, 707583.6575528898, 184601.4270496964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1710000.0000, 
sim time next is 1710600.0000, 
raw observation next is [27.28333333333333, 82.66666666666667, 1.0, 2.0, 0.5062232049111284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707371.1733267285, 707371.1733267285, 184577.3008305534], 
processed observation next is [1.0, 0.8260869565217391, 0.49210110584518163, 0.8266666666666667, 1.0, 1.0, 0.40508819868810647, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19649199259075792, 0.19649199259075792, 0.2754885087023185], 
reward next is 0.7245, 
noisyNet noise sample is [array([-0.4645264], dtype=float32), 3.5978723]. 
=============================================
[2019-04-28 00:32:16,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.7553274e-17 1.0000000e+00 2.8700076e-28 1.9845072e-09 1.5276645e-23], sum to 1.0000
[2019-04-28 00:32:16,353] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4115
[2019-04-28 00:32:16,360] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1937985.866821257 W.
[2019-04-28 00:32:16,365] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.63333333333334, 76.0, 1.0, 2.0, 0.7449173069719028, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.983002466108744, 6.9112, 168.9125287258777, 1937985.866821257, 1887046.855120693, 395519.6297866976], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1700400.0000, 
sim time next is 1701000.0000, 
raw observation next is [28.7, 75.5, 1.0, 2.0, 0.7015865436725944, 1.0, 1.0, 0.7015865436725944, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1961865.009984924, 1961865.009984923, 374800.3723486231], 
processed observation next is [1.0, 0.6956521739130435, 0.5592417061611374, 0.755, 1.0, 1.0, 0.6404657152681861, 1.0, 0.5, 0.6404657152681861, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.54496250277359, 0.5449625027735897, 0.5594035408188405], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.74084175], dtype=float32), -0.43060508]. 
=============================================
[2019-04-28 00:32:16,381] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[68.19264]
 [67.91857]
 [66.82546]
 [65.53945]
 [64.91659]], R is [[68.83761597]
 [68.19989777]
 [67.94036102]
 [67.68818665]
 [67.01130676]].
[2019-04-28 00:32:16,422] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2275126e-20 1.0000000e+00 2.0721765e-30 6.2980413e-11 1.2875166e-24], sum to 1.0000
[2019-04-28 00:32:16,431] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0091
[2019-04-28 00:32:16,434] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.95, 91.50000000000001, 1.0, 2.0, 0.50972278043252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712262.9458737795, 712262.9458737788, 185133.5208410282], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1721400.0000, 
sim time next is 1722000.0000, 
raw observation next is [25.9, 92.0, 1.0, 2.0, 0.5097685494312868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712326.9227911418, 712326.9227911412, 185140.8630280854], 
processed observation next is [1.0, 0.9565217391304348, 0.42654028436018954, 0.92, 1.0, 1.0, 0.40935969810998407, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19786858966420606, 0.1978685896642059, 0.2763296463105752], 
reward next is 0.7237, 
noisyNet noise sample is [array([-0.8732958], dtype=float32), -0.05109698]. 
=============================================
[2019-04-28 00:32:16,458] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[72.76667 ]
 [72.75091 ]
 [72.641426]
 [72.71368 ]
 [72.62788 ]], R is [[72.77925873]
 [72.77514648]
 [72.77108002]
 [72.76693726]
 [72.76265717]].
[2019-04-28 00:32:17,635] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6321437e-18 1.0000000e+00 1.3802804e-26 7.0381284e-10 6.8352478e-22], sum to 1.0000
[2019-04-28 00:32:17,644] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5452
[2019-04-28 00:32:17,651] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.96666666666667, 93.66666666666667, 1.0, 2.0, 0.4097134244071788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632575.5668060763, 632575.5668060763, 177993.3514750969], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1822800.0000, 
sim time next is 1823400.0000, 
raw observation next is [21.95, 94.0, 1.0, 2.0, 0.3731417406895722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575732.5297998098, 575732.5297998104, 172868.0394615163], 
processed observation next is [1.0, 0.08695652173913043, 0.2393364928909953, 0.94, 1.0, 1.0, 0.24474908516815924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15992570272216938, 0.15992570272216955, 0.258011999196293], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.76183546], dtype=float32), 0.5012897]. 
=============================================
[2019-04-28 00:32:17,956] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69000, global step 1106790: loss 0.3764
[2019-04-28 00:32:17,957] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69000, global step 1106790: learning rate 0.0000
[2019-04-28 00:32:18,810] A3C_AGENT_WORKER-Thread-18 INFO:Local step 69500, global step 1107199: loss 126.5588
[2019-04-28 00:32:18,810] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 69500, global step 1107199: learning rate 0.0000
[2019-04-28 00:32:25,252] A3C_AGENT_WORKER-Thread-2 INFO:Local step 69500, global step 1110312: loss -9.2371
[2019-04-28 00:32:25,253] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 69500, global step 1110312: learning rate 0.0000
[2019-04-28 00:32:26,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1275541e-18 1.0000000e+00 5.9106724e-31 8.6022422e-12 1.6019553e-24], sum to 1.0000
[2019-04-28 00:32:26,048] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7445
[2019-04-28 00:32:26,058] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1858226.539756437 W.
[2019-04-28 00:32:26,063] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.95, 87.0, 1.0, 2.0, 0.6879212856070943, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.979947096307901, 6.9112, 168.9117615821765, 1858226.539756437, 1809455.328564123, 382980.286004358], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1873800.0000, 
sim time next is 1874400.0000, 
raw observation next is [26.93333333333333, 87.33333333333334, 1.0, 2.0, 0.6036754008834979, 1.0, 1.0, 0.6036754008834979, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1687857.847331919, 1687857.847331919, 335610.7568701986], 
processed observation next is [1.0, 0.6956521739130435, 0.4755134281200631, 0.8733333333333334, 1.0, 1.0, 0.5225004829921661, 1.0, 0.5, 0.5225004829921661, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.46884940203664416, 0.46884940203664416, 0.5009115774182068], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.1628109], dtype=float32), 1.3679607]. 
=============================================
[2019-04-28 00:32:26,937] A3C_AGENT_WORKER-Thread-20 INFO:Local step 69500, global step 1111133: loss -133.9475
[2019-04-28 00:32:26,939] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 69500, global step 1111133: learning rate 0.0000
[2019-04-28 00:32:27,453] A3C_AGENT_WORKER-Thread-9 INFO:Local step 69500, global step 1111375: loss -1.0703
[2019-04-28 00:32:27,457] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 69500, global step 1111375: learning rate 0.0000
[2019-04-28 00:32:27,651] A3C_AGENT_WORKER-Thread-10 INFO:Local step 69500, global step 1111458: loss -86.6246
[2019-04-28 00:32:27,654] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 69500, global step 1111458: learning rate 0.0000
[2019-04-28 00:32:28,207] A3C_AGENT_WORKER-Thread-21 INFO:Local step 69500, global step 1111739: loss -95.4738
[2019-04-28 00:32:28,210] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 69500, global step 1111740: learning rate 0.0000
[2019-04-28 00:32:28,230] A3C_AGENT_WORKER-Thread-14 INFO:Local step 69500, global step 1111750: loss 40.7853
[2019-04-28 00:32:28,231] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 69500, global step 1111750: learning rate 0.0000
[2019-04-28 00:32:28,648] A3C_AGENT_WORKER-Thread-13 INFO:Local step 69500, global step 1111950: loss -67.6871
[2019-04-28 00:32:28,653] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 69500, global step 1111951: learning rate 0.0000
[2019-04-28 00:32:29,482] A3C_AGENT_WORKER-Thread-16 INFO:Local step 69500, global step 1112359: loss -56.7011
[2019-04-28 00:32:29,484] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 69500, global step 1112362: learning rate 0.0000
[2019-04-28 00:32:29,857] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7052964e-19 1.0000000e+00 4.3900646e-29 1.3303690e-10 1.3271878e-22], sum to 1.0000
[2019-04-28 00:32:29,865] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9988
[2019-04-28 00:32:29,869] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.15, 83.33333333333334, 1.0, 2.0, 0.5089385623157816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711166.7490680341, 711166.7490680347, 185008.6039761551], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2041800.0000, 
sim time next is 2042400.0000, 
raw observation next is [27.1, 83.66666666666667, 1.0, 2.0, 0.508118286132569, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710020.1507385301, 710020.1507385301, 184878.0600177137], 
processed observation next is [0.0, 0.6521739130434783, 0.4834123222748816, 0.8366666666666667, 1.0, 1.0, 0.4073714290753844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19722781964959168, 0.19722781964959168, 0.275937403011513], 
reward next is 0.7241, 
noisyNet noise sample is [array([1.1047598], dtype=float32), 1.216156]. 
=============================================
[2019-04-28 00:32:30,089] A3C_AGENT_WORKER-Thread-12 INFO:Local step 69500, global step 1112656: loss -100.8758
[2019-04-28 00:32:30,093] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 69500, global step 1112657: learning rate 0.0000
[2019-04-28 00:32:30,427] A3C_AGENT_WORKER-Thread-11 INFO:Local step 69500, global step 1112824: loss -38.0509
[2019-04-28 00:32:30,429] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 69500, global step 1112824: learning rate 0.0000
[2019-04-28 00:32:30,655] A3C_AGENT_WORKER-Thread-3 INFO:Local step 69500, global step 1112933: loss -117.0969
[2019-04-28 00:32:30,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 69500, global step 1112933: learning rate 0.0000
[2019-04-28 00:32:30,856] A3C_AGENT_WORKER-Thread-22 INFO:Local step 69500, global step 1113036: loss -3.8658
[2019-04-28 00:32:30,857] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 69500, global step 1113037: learning rate 0.0000
[2019-04-28 00:32:30,871] A3C_AGENT_WORKER-Thread-19 INFO:Local step 69500, global step 1113039: loss -61.6007
[2019-04-28 00:32:30,875] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 69500, global step 1113041: learning rate 0.0000
[2019-04-28 00:32:31,785] A3C_AGENT_WORKER-Thread-17 INFO:Local step 69500, global step 1113485: loss -41.1930
[2019-04-28 00:32:31,787] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 69500, global step 1113485: learning rate 0.0000
[2019-04-28 00:32:34,395] A3C_AGENT_WORKER-Thread-15 INFO:Local step 69500, global step 1114767: loss -37.1280
[2019-04-28 00:32:34,396] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 69500, global step 1114767: learning rate 0.0000
[2019-04-28 00:32:34,986] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.8763382e-20 1.0000000e+00 6.5730875e-31 1.6095546e-12 4.4060120e-25], sum to 1.0000
[2019-04-28 00:32:34,993] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1958
[2019-04-28 00:32:34,998] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 94.0, 1.0, 2.0, 0.4677171844808998, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657284.2766999168, 657284.2766999175, 179175.2532192061], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2073600.0000, 
sim time next is 2074200.0000, 
raw observation next is [24.48333333333333, 94.16666666666667, 1.0, 2.0, 0.4673698672905682, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 656852.309046034, 656852.309046034, 179131.1315331012], 
processed observation next is [0.0, 0.0, 0.3593996840442337, 0.9416666666666668, 1.0, 1.0, 0.3582769485428532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18245897473500944, 0.18245897473500944, 0.2673598978105988], 
reward next is 0.7326, 
noisyNet noise sample is [array([-1.3259168], dtype=float32), 1.0456438]. 
=============================================
[2019-04-28 00:32:35,717] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70000, global step 1115396: loss 5.7206
[2019-04-28 00:32:35,720] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70000, global step 1115396: learning rate 0.0000
[2019-04-28 00:32:40,274] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.785977e-13 1.000000e+00 4.182059e-18 5.283963e-08 9.328001e-15], sum to 1.0000
[2019-04-28 00:32:40,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2913
[2019-04-28 00:32:40,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2002619.325039119 W.
[2019-04-28 00:32:40,287] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.48333333333333, 76.5, 1.0, 2.0, 0.4774314455901182, 1.0, 2.0, 0.4774314455901182, 1.0, 2.0, 0.8291405319982935, 6.9112, 6.9112, 170.5573041426782, 2002619.325039119, 2002619.325039119, 399889.7857356322], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2195400.0000, 
sim time next is 2196000.0000, 
raw observation next is [29.6, 76.0, 1.0, 2.0, 0.4763601819038065, 1.0, 2.0, 0.4763601819038065, 1.0, 2.0, 0.8272801012474847, 6.911199999999999, 6.9112, 170.5573041426782, 1998121.644745997, 1998121.644745998, 399183.1258341284], 
processed observation next is [1.0, 0.43478260869565216, 0.6018957345971565, 0.76, 1.0, 1.0, 0.36910865289615247, 1.0, 1.0, 0.36910865289615247, 1.0, 1.0, 0.7893659771310788, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5550337902072213, 0.5550337902072217, 0.5957957102001916], 
reward next is 0.4042, 
noisyNet noise sample is [array([1.113893], dtype=float32), 0.09155694]. 
=============================================
[2019-04-28 00:32:40,312] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[44.164856]
 [44.312653]
 [43.507713]
 [43.11521 ]
 [42.77077 ]], R is [[44.66193771]
 [44.61846924]
 [44.57372665]
 [44.51422119]
 [44.06908035]].
[2019-04-28 00:32:41,670] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70000, global step 1118292: loss 0.7676
[2019-04-28 00:32:41,672] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70000, global step 1118292: learning rate 0.0000
[2019-04-28 00:32:43,233] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70000, global step 1119044: loss 0.4976
[2019-04-28 00:32:43,234] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70000, global step 1119044: learning rate 0.0000
[2019-04-28 00:32:43,736] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70000, global step 1119286: loss 0.3115
[2019-04-28 00:32:43,741] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70000, global step 1119286: learning rate 0.0000
[2019-04-28 00:32:44,214] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70000, global step 1119510: loss 0.5134
[2019-04-28 00:32:44,217] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70000, global step 1119510: learning rate 0.0000
[2019-04-28 00:32:44,553] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70000, global step 1119678: loss 0.3219
[2019-04-28 00:32:44,554] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70000, global step 1119678: learning rate 0.0000
[2019-04-28 00:32:44,582] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70000, global step 1119688: loss 0.3756
[2019-04-28 00:32:44,587] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70000, global step 1119688: learning rate 0.0000
[2019-04-28 00:32:44,938] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70000, global step 1119847: loss 0.3783
[2019-04-28 00:32:44,940] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70000, global step 1119848: learning rate 0.0000
[2019-04-28 00:32:46,058] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70000, global step 1120393: loss 0.5501
[2019-04-28 00:32:46,060] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70000, global step 1120393: learning rate 0.0000
[2019-04-28 00:32:46,418] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70000, global step 1120570: loss 0.2194
[2019-04-28 00:32:46,422] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70000, global step 1120571: learning rate 0.0000
[2019-04-28 00:32:46,961] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70000, global step 1120834: loss 0.2202
[2019-04-28 00:32:46,963] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70000, global step 1120836: learning rate 0.0000
[2019-04-28 00:32:47,137] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70000, global step 1120921: loss 0.1691
[2019-04-28 00:32:47,139] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70000, global step 1120921: learning rate 0.0000
[2019-04-28 00:32:47,156] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70000, global step 1120931: loss 0.1526
[2019-04-28 00:32:47,158] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70000, global step 1120931: learning rate 0.0000
[2019-04-28 00:32:47,266] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70000, global step 1120981: loss 0.1930
[2019-04-28 00:32:47,268] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70000, global step 1120981: learning rate 0.0000
[2019-04-28 00:32:48,517] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70000, global step 1121574: loss 1.1116
[2019-04-28 00:32:48,519] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70000, global step 1121574: learning rate 0.0000
[2019-04-28 00:32:49,336] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.6714511e-09 9.9858046e-01 3.5737362e-15 1.4195916e-03 1.9100757e-11], sum to 1.0000
[2019-04-28 00:32:49,349] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0503
[2019-04-28 00:32:49,357] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1930505.420926798 W.
[2019-04-28 00:32:49,362] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.4, 69.0, 1.0, 2.0, 0.4602547145558408, 1.0, 1.0, 0.4602547145558408, 1.0, 2.0, 0.7949001484420698, 6.9112, 6.9112, 170.5573041426782, 1930505.420926798, 1930505.420926798, 388024.7623215702], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2365200.0000, 
sim time next is 2365800.0000, 
raw observation next is [30.55, 68.5, 1.0, 2.0, 0.8408718807379629, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992795670171128, 6.9112, 168.9124712064425, 2072276.731823172, 2014390.121412098, 418682.5002792943], 
processed observation next is [1.0, 0.391304347826087, 0.6469194312796209, 0.685, 1.0, 1.0, 0.8082793743830878, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008159567017112757, 0.0, 0.8294375620875459, 0.5756324255064367, 0.5595528115033606, 0.6248992541482005], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.48110974], dtype=float32), 1.5115836]. 
=============================================
[2019-04-28 00:32:50,811] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70000, global step 1122689: loss 0.4695
[2019-04-28 00:32:50,814] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70000, global step 1122689: learning rate 0.0000
[2019-04-28 00:32:51,102] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.8436176e-14 9.9999523e-01 5.1694477e-19 4.7410244e-06 4.2661516e-17], sum to 1.0000
[2019-04-28 00:32:51,110] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3662
[2019-04-28 00:32:51,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 78.66666666666667, 1.0, 2.0, 0.5676985508345127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 793305.8473093299, 793305.8473093299, 194882.5793136188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2323200.0000, 
sim time next is 2323800.0000, 
raw observation next is [29.4, 79.0, 1.0, 2.0, 0.5655846257790007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790350.7349213045, 790350.7349213045, 194509.6589926694], 
processed observation next is [1.0, 0.9130434782608695, 0.5924170616113744, 0.79, 1.0, 1.0, 0.4766079828662659, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21954187081147347, 0.21954187081147347, 0.29031292386965585], 
reward next is 0.7097, 
noisyNet noise sample is [array([0.8114524], dtype=float32), -0.06603987]. 
=============================================
[2019-04-28 00:32:52,284] A3C_AGENT_WORKER-Thread-18 INFO:Local step 70500, global step 1123406: loss -44.4980
[2019-04-28 00:32:52,285] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 70500, global step 1123407: learning rate 0.0000
[2019-04-28 00:32:54,362] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5435976e-12 9.9918956e-01 1.5723752e-18 8.1045111e-04 2.1558181e-13], sum to 1.0000
[2019-04-28 00:32:54,370] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0453
[2019-04-28 00:32:54,378] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.03333333333333, 81.66666666666667, 1.0, 2.0, 0.8144438761476739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1138294.243665228, 1138294.243665228, 247487.0289644283], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2432400.0000, 
sim time next is 2433000.0000, 
raw observation next is [27.96666666666667, 81.83333333333334, 1.0, 2.0, 0.7973981488146628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1114458.05784803, 1114458.057848029, 243272.9607770369], 
processed observation next is [1.0, 0.13043478260869565, 0.524486571879937, 0.8183333333333335, 1.0, 1.0, 0.7559013841140515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3095716827355639, 0.3095716827355636, 0.3630939713090103], 
reward next is 0.6369, 
noisyNet noise sample is [array([-0.48192978], dtype=float32), 0.2591759]. 
=============================================
[2019-04-28 00:32:54,409] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[52.56526 ]
 [52.308582]
 [52.63615 ]
 [52.58315 ]
 [52.087635]], R is [[52.8155098 ]
 [52.91796875]
 [53.01153183]
 [53.09904861]
 [53.16313934]].
[2019-04-28 00:32:55,603] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 00:32:55,605] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:32:55,606] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:32:55,608] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:32:55,608] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:32:55,609] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:32:55,611] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:32:55,613] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:32:55,613] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:32:55,613] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:32:55,614] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:32:55,636] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run46
[2019-04-28 00:32:55,637] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run46
[2019-04-28 00:32:55,698] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run46
[2019-04-28 00:32:55,725] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run46
[2019-04-28 00:32:55,727] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run46
[2019-04-28 00:33:03,996] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:03,998] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [18.227298385, 86.123498305, 1.0, 2.0, 0.219305955148296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 365708.413196467, 365708.4131964663, 157419.6224454322]
[2019-04-28 00:33:03,999] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:33:04,002] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.7556511e-15 9.9985027e-01 1.6760825e-23 1.4969689e-04 4.5785340e-18], sampled 0.36065221354811894
[2019-04-28 00:33:19,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:19,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.51666666666667, 94.0, 1.0, 2.0, 0.4682872902829175, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 657590.738664544, 657590.738664544, 179195.8874800015]
[2019-04-28 00:33:19,669] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:33:19,672] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8227435e-15 9.9989033e-01 2.6414524e-24 1.0962266e-04 1.1191178e-18], sampled 0.7327482226489548
[2019-04-28 00:33:23,027] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:23,027] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.6, 92.0, 1.0, 2.0, 0.3765075949743094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 573620.5032309451, 573620.5032309445, 172499.612472948]
[2019-04-28 00:33:23,027] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:33:23,030] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.4241708e-16 9.9992085e-01 3.7587728e-25 7.9104306e-05 2.5287322e-19], sampled 0.8588099531502436
[2019-04-28 00:33:34,264] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:34,265] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.86666666666667, 70.66666666666667, 1.0, 2.0, 0.5420130437969859, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757399.9468045603, 757399.9468045596, 190439.8873739907]
[2019-04-28 00:33:34,267] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:33:34,270] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.995958e-16 9.999397e-01 7.537429e-26 6.034201e-05 7.426542e-20], sampled 0.24385889615983192
[2019-04-28 00:33:34,898] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:34,899] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.83333333333333, 79.00000000000001, 1.0, 2.0, 0.5827288349026739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814317.3205550575, 814317.3205550575, 197572.4307188338]
[2019-04-28 00:33:34,899] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:33:34,900] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.3102646e-16 9.9993086e-01 1.7014633e-25 6.9094807e-05 1.3827247e-19], sampled 0.9341592948840175
[2019-04-28 00:33:51,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:33:51,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.78948170333334, 73.38813566500001, 1.0, 2.0, 0.4199700197479272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610725.6221401768, 610725.6221401775, 175084.5525985845]
[2019-04-28 00:33:51,458] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:33:51,461] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0816084e-14 9.9978775e-01 1.3164193e-22 2.1228413e-04 2.2055436e-17], sampled 0.7029018321880801
[2019-04-28 00:34:03,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:34:03,881] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.1, 50.0, 1.0, 2.0, 0.6045015837093216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844755.1091505778, 844755.1091505778, 201586.7728702062]
[2019-04-28 00:34:03,883] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:34:03,886] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7289638e-16 9.9994206e-01 5.9849891e-26 5.7988436e-05 6.2330689e-20], sampled 0.675040719825802
[2019-04-28 00:34:04,565] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:34:04,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.45, 91.0, 1.0, 2.0, 0.7641237128576236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1067929.720005651, 1067929.720005651, 235294.4849185148]
[2019-04-28 00:34:04,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:34:04,571] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.3838754e-15 9.9983406e-01 3.0550623e-23 1.6590490e-04 7.2399521e-18], sampled 0.06430909827728659
[2019-04-28 00:34:07,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057947233]
[2019-04-28 00:34:07,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.4, 77.83333333333333, 1.0, 2.0, 0.5602494093510068, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9665258165932785, 6.911199999999999, 6.9112, 168.9126912054925, 1566362.525146968, 1566362.525146968, 341432.7717949823]
[2019-04-28 00:34:07,379] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:34:07,382] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4029089e-13 9.9958676e-01 6.6518367e-21 4.1330568e-04 4.4023129e-16], sampled 0.3118435855501356
[2019-04-28 00:34:30,025] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.8612 3163990816.6456 1775.0000
[2019-04-28 00:34:30,299] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.9281 2842426422.8214 1130.0000
[2019-04-28 00:34:30,433] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8662.3199 2779107164.9126 931.0000
[2019-04-28 00:34:30,508] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.7247 2927297902.8371 1337.0000
[2019-04-28 00:34:30,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2724 3007634432.0153 1766.0000
[2019-04-28 00:34:31,650] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1125000, evaluation results [1125000.0, 7883.861234875732, 3163990816.645608, 1775.0, 8253.724718363766, 2927297902.8371167, 1337.0, 8662.319920862119, 2779107164.912648, 931.0, 7998.272424742009, 3007634432.0153246, 1766.0, 8497.928086684944, 2842426422.821371, 1130.0]
[2019-04-28 00:34:35,068] A3C_AGENT_WORKER-Thread-2 INFO:Local step 70500, global step 1126428: loss 27.0680
[2019-04-28 00:34:35,069] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 70500, global step 1126428: learning rate 0.0000
[2019-04-28 00:34:36,552] A3C_AGENT_WORKER-Thread-20 INFO:Local step 70500, global step 1127056: loss 33.7519
[2019-04-28 00:34:36,554] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 70500, global step 1127056: learning rate 0.0000
[2019-04-28 00:34:37,347] A3C_AGENT_WORKER-Thread-9 INFO:Local step 70500, global step 1127368: loss 33.2811
[2019-04-28 00:34:37,350] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 70500, global step 1127368: learning rate 0.0000
[2019-04-28 00:34:37,726] A3C_AGENT_WORKER-Thread-10 INFO:Local step 70500, global step 1127501: loss 26.7063
[2019-04-28 00:34:37,729] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 70500, global step 1127501: learning rate 0.0000
[2019-04-28 00:34:38,118] A3C_AGENT_WORKER-Thread-14 INFO:Local step 70500, global step 1127671: loss 14.0941
[2019-04-28 00:34:38,118] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 70500, global step 1127671: learning rate 0.0000
[2019-04-28 00:34:38,178] A3C_AGENT_WORKER-Thread-21 INFO:Local step 70500, global step 1127690: loss 41.7495
[2019-04-28 00:34:38,180] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 70500, global step 1127690: learning rate 0.0000
[2019-04-28 00:34:38,339] A3C_AGENT_WORKER-Thread-13 INFO:Local step 70500, global step 1127763: loss 16.1768
[2019-04-28 00:34:38,340] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 70500, global step 1127763: learning rate 0.0000
[2019-04-28 00:34:39,844] A3C_AGENT_WORKER-Thread-16 INFO:Local step 70500, global step 1128382: loss 47.1187
[2019-04-28 00:34:39,847] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 70500, global step 1128383: learning rate 0.0000
[2019-04-28 00:34:40,207] A3C_AGENT_WORKER-Thread-12 INFO:Local step 70500, global step 1128550: loss 47.3906
[2019-04-28 00:34:40,210] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 70500, global step 1128550: learning rate 0.0000
[2019-04-28 00:34:40,498] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.4862455e-15 9.8631138e-01 2.7729530e-21 1.3688563e-02 2.0753215e-17], sum to 1.0000
[2019-04-28 00:34:40,501] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8831
[2019-04-28 00:34:40,503] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2042892.064675923 W.
[2019-04-28 00:34:40,510] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.8, 84.0, 1.0, 2.0, 0.8198767533979311, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.990589285130969, 6.9112, 168.9124836353278, 2042892.064675923, 1986570.731108801, 413381.7147986333], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2541600.0000, 
sim time next is 2542200.0000, 
raw observation next is [27.9, 83.33333333333333, 1.0, 2.0, 0.7388697499841497, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.9877817691927, 6.9112, 168.9124357806032, 1929522.727471778, 1875193.152154725, 393978.3508029772], 
processed observation next is [1.0, 0.43478260869565216, 0.5213270142180094, 0.8333333333333333, 1.0, 1.0, 0.6853852409447586, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007658176919269977, 0.0, 0.8294373881304571, 0.5359785354088272, 0.5208869867096458, 0.588027389258175], 
reward next is 0.0291, 
noisyNet noise sample is [array([0.7815856], dtype=float32), 0.27654734]. 
=============================================
[2019-04-28 00:34:40,857] A3C_AGENT_WORKER-Thread-22 INFO:Local step 70500, global step 1128832: loss 61.7567
[2019-04-28 00:34:40,860] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 70500, global step 1128833: learning rate 0.0000
[2019-04-28 00:34:41,038] A3C_AGENT_WORKER-Thread-3 INFO:Local step 70500, global step 1128910: loss 52.7218
[2019-04-28 00:34:41,039] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 70500, global step 1128910: learning rate 0.0000
[2019-04-28 00:34:41,105] A3C_AGENT_WORKER-Thread-19 INFO:Local step 70500, global step 1128950: loss 120.6322
[2019-04-28 00:34:41,108] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 70500, global step 1128951: learning rate 0.0000
[2019-04-28 00:34:41,388] A3C_AGENT_WORKER-Thread-11 INFO:Local step 70500, global step 1129055: loss 127.5375
[2019-04-28 00:34:41,390] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 70500, global step 1129055: learning rate 0.0000
[2019-04-28 00:34:43,187] A3C_AGENT_WORKER-Thread-17 INFO:Local step 70500, global step 1129808: loss 148.8459
[2019-04-28 00:34:43,188] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 70500, global step 1129808: learning rate 0.0000
[2019-04-28 00:34:43,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3416051e-16 9.9998415e-01 2.4558964e-27 1.5895153e-05 2.4872646e-19], sum to 1.0000
[2019-04-28 00:34:43,467] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0285
[2019-04-28 00:34:43,471] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.0, 1.0, 2.0, 0.4412583521901511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 633801.0473858996, 633801.047385899, 177121.2809928687], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2688000.0000, 
sim time next is 2688600.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.4404975202183125, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 632708.5283133048, 632708.5283133048, 177012.2143808495], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.32590062676905124, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17575236897591798, 0.17575236897591798, 0.2641973348967903], 
reward next is 0.7358, 
noisyNet noise sample is [array([-1.3938367], dtype=float32), -1.4115999]. 
=============================================
[2019-04-28 00:34:45,251] A3C_AGENT_WORKER-Thread-15 INFO:Local step 70500, global step 1130704: loss 55.9912
[2019-04-28 00:34:45,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 70500, global step 1130704: learning rate 0.0000
[2019-04-28 00:34:47,261] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71000, global step 1131524: loss 0.0522
[2019-04-28 00:34:47,263] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71000, global step 1131524: learning rate 0.0000
[2019-04-28 00:34:47,323] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1133417e-16 9.9995995e-01 2.3623433e-25 3.9994142e-05 5.3221948e-20], sum to 1.0000
[2019-04-28 00:34:47,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3217
[2019-04-28 00:34:47,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 100.0, 1.0, 2.0, 0.4759084983839051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 664997.6485445339, 664997.6485445339, 179907.4664712944], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2703000.0000, 
sim time next is 2703600.0000, 
raw observation next is [24.0, 100.0, 1.0, 2.0, 0.475867988442044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664941.0253757458, 664941.0253757464, 179901.4110463232], 
processed observation next is [0.0, 0.30434782608695654, 0.3364928909952607, 1.0, 1.0, 1.0, 0.36851564872535425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18470584038215163, 0.1847058403821518, 0.2685095687258555], 
reward next is 0.7315, 
noisyNet noise sample is [array([1.6643059], dtype=float32), -2.321395]. 
=============================================
[2019-04-28 00:34:49,333] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7853281e-18 9.9998713e-01 6.0129602e-30 1.2823389e-05 1.4072426e-21], sum to 1.0000
[2019-04-28 00:34:49,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5138
[2019-04-28 00:34:49,362] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3950110177672582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589415.3091307866, 589415.3091307866, 173564.0042775566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2677200.0000, 
sim time next is 2677800.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3948931446499609, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 589239.4057712951, 589239.4057712945, 173547.8825077951], 
processed observation next is [0.0, 1.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2709555959638083, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16367761271424863, 0.16367761271424847, 0.2590266903101419], 
reward next is 0.7410, 
noisyNet noise sample is [array([0.30527553], dtype=float32), 0.52900153]. 
=============================================
[2019-04-28 00:34:53,766] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71000, global step 1134204: loss 0.3848
[2019-04-28 00:34:53,768] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71000, global step 1134204: learning rate 0.0000
[2019-04-28 00:34:55,879] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71000, global step 1135100: loss 0.0575
[2019-04-28 00:34:55,880] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71000, global step 1135101: learning rate 0.0000
[2019-04-28 00:34:55,927] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71000, global step 1135126: loss 0.0653
[2019-04-28 00:34:55,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71000, global step 1135126: learning rate 0.0000
[2019-04-28 00:34:56,980] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71000, global step 1135556: loss 0.1527
[2019-04-28 00:34:56,984] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71000, global step 1135557: learning rate 0.0000
[2019-04-28 00:34:57,071] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71000, global step 1135585: loss 0.2730
[2019-04-28 00:34:57,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71000, global step 1135585: learning rate 0.0000
[2019-04-28 00:34:57,104] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71000, global step 1135602: loss 0.2001
[2019-04-28 00:34:57,106] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71000, global step 1135603: learning rate 0.0000
[2019-04-28 00:34:57,909] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71000, global step 1135922: loss 0.0051
[2019-04-28 00:34:57,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71000, global step 1135924: learning rate 0.0000
[2019-04-28 00:34:59,267] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71000, global step 1136474: loss 0.0410
[2019-04-28 00:34:59,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71000, global step 1136474: learning rate 0.0000
[2019-04-28 00:34:59,596] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71000, global step 1136595: loss 0.0400
[2019-04-28 00:34:59,599] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71000, global step 1136595: learning rate 0.0000
[2019-04-28 00:34:59,620] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71000, global step 1136601: loss 0.0283
[2019-04-28 00:34:59,627] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71000, global step 1136601: learning rate 0.0000
[2019-04-28 00:35:00,152] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71000, global step 1136820: loss 0.0040
[2019-04-28 00:35:00,154] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71000, global step 1136821: learning rate 0.0000
[2019-04-28 00:35:00,268] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71000, global step 1136871: loss 0.0081
[2019-04-28 00:35:00,269] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71000, global step 1136871: learning rate 0.0000
[2019-04-28 00:35:01,044] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71000, global step 1137202: loss 0.0084
[2019-04-28 00:35:01,047] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71000, global step 1137202: learning rate 0.0000
[2019-04-28 00:35:02,019] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.9114172e-16 9.9996674e-01 4.7001515e-27 3.3297129e-05 6.5243923e-20], sum to 1.0000
[2019-04-28 00:35:02,026] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1499
[2019-04-28 00:35:02,033] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 93.33333333333334, 1.0, 2.0, 0.5583344680340759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859560.8324298095, 859560.8324298095, 202779.7095041777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2884800.0000, 
sim time next is 2885400.0000, 
raw observation next is [22.15, 93.0, 1.0, 2.0, 0.5873530096808306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903994.8043460599, 903994.8043460599, 208475.8809406458], 
processed observation next is [1.0, 0.391304347826087, 0.24881516587677724, 0.93, 1.0, 1.0, 0.5028349514226874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25110966787390554, 0.25110966787390554, 0.3111580312546952], 
reward next is 0.6888, 
noisyNet noise sample is [array([0.760673], dtype=float32), 0.93561095]. 
=============================================
[2019-04-28 00:35:02,327] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.1573494e-16 9.9999809e-01 1.0597819e-27 1.8531006e-06 2.7108849e-21], sum to 1.0000
[2019-04-28 00:35:02,343] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1623
[2019-04-28 00:35:02,349] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3027017108702418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 482035.3906956342, 482035.3906956336, 165767.8371366799], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2939400.0000, 
sim time next is 2940000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3023815467460085, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 481525.9147603776, 481525.9147603782, 165731.2519388977], 
processed observation next is [1.0, 0.0, 0.1469194312796209, 1.0, 1.0, 1.0, 0.15949583945302226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13375719854454934, 0.1337571985445495, 0.24736007752074285], 
reward next is 0.7526, 
noisyNet noise sample is [array([-0.63008964], dtype=float32), -0.017632056]. 
=============================================
[2019-04-28 00:35:02,366] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[70.1057  ]
 [70.213425]
 [70.531685]
 [71.009575]
 [71.83751 ]], R is [[70.05592346]
 [70.1079483 ]
 [70.15936279]
 [70.2101593 ]
 [70.26031494]].
[2019-04-28 00:35:02,965] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71000, global step 1137982: loss 0.1484
[2019-04-28 00:35:02,972] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71000, global step 1137982: learning rate 0.0000
[2019-04-28 00:35:04,857] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71000, global step 1138766: loss 0.3397
[2019-04-28 00:35:04,858] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71000, global step 1138766: learning rate 0.0000
[2019-04-28 00:35:06,158] A3C_AGENT_WORKER-Thread-18 INFO:Local step 71500, global step 1139301: loss 0.3389
[2019-04-28 00:35:06,159] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 71500, global step 1139301: learning rate 0.0000
[2019-04-28 00:35:12,756] A3C_AGENT_WORKER-Thread-2 INFO:Local step 71500, global step 1142039: loss 0.1903
[2019-04-28 00:35:12,758] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 71500, global step 1142039: learning rate 0.0000
[2019-04-28 00:35:13,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.3609255e-18 1.0000000e+00 4.2647508e-27 2.4834499e-08 1.6677499e-21], sum to 1.0000
[2019-04-28 00:35:13,235] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9722
[2019-04-28 00:35:13,239] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.4581022426458296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683473.5253452257, 683473.5253452263, 182696.7856311444], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3070800.0000, 
sim time next is 3071400.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.5296480638879271, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 787887.8744350799, 787887.8744350792, 194310.1199839892], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.95, 1.0, 1.0, 0.43331092034690005, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2188577428986333, 0.2188577428986331, 0.2900151044537152], 
reward next is 0.7100, 
noisyNet noise sample is [array([-0.68794596], dtype=float32), -0.75527716]. 
=============================================
[2019-04-28 00:35:15,105] A3C_AGENT_WORKER-Thread-9 INFO:Local step 71500, global step 1143030: loss 0.3797
[2019-04-28 00:35:15,106] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 71500, global step 1143030: learning rate 0.0000
[2019-04-28 00:35:15,732] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.9945920e-18 1.0000000e+00 2.7492479e-27 4.0653405e-09 3.0316884e-22], sum to 1.0000
[2019-04-28 00:35:15,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3874
[2019-04-28 00:35:15,753] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 95.0, 1.0, 2.0, 0.3557463601685748, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548977.406688696, 548977.4066886967, 170587.4430615893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3052200.0000, 
sim time next is 3052800.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3514967435835583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541706.0223128739, 541706.0223128739, 169964.0671676623], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2186707754018775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1504738950869094, 0.1504738950869094, 0.25367771219054075], 
reward next is 0.7463, 
noisyNet noise sample is [array([-0.9845005], dtype=float32), 2.1018214]. 
=============================================
[2019-04-28 00:35:15,843] A3C_AGENT_WORKER-Thread-20 INFO:Local step 71500, global step 1143350: loss 0.8371
[2019-04-28 00:35:15,858] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 71500, global step 1143350: learning rate 0.0000
[2019-04-28 00:35:16,166] A3C_AGENT_WORKER-Thread-14 INFO:Local step 71500, global step 1143474: loss 0.5203
[2019-04-28 00:35:16,168] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 71500, global step 1143475: learning rate 0.0000
[2019-04-28 00:35:16,201] A3C_AGENT_WORKER-Thread-21 INFO:Local step 71500, global step 1143496: loss 0.6063
[2019-04-28 00:35:16,204] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 71500, global step 1143497: learning rate 0.0000
[2019-04-28 00:35:16,230] A3C_AGENT_WORKER-Thread-10 INFO:Local step 71500, global step 1143511: loss 0.7401
[2019-04-28 00:35:16,234] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 71500, global step 1143511: learning rate 0.0000
[2019-04-28 00:35:17,368] A3C_AGENT_WORKER-Thread-13 INFO:Local step 71500, global step 1143997: loss 2.1011
[2019-04-28 00:35:17,369] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 71500, global step 1143998: learning rate 0.0000
[2019-04-28 00:35:18,479] A3C_AGENT_WORKER-Thread-16 INFO:Local step 71500, global step 1144463: loss 2.2978
[2019-04-28 00:35:18,480] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 71500, global step 1144463: learning rate 0.0000
[2019-04-28 00:35:19,172] A3C_AGENT_WORKER-Thread-22 INFO:Local step 71500, global step 1144749: loss 2.4504
[2019-04-28 00:35:19,191] A3C_AGENT_WORKER-Thread-12 INFO:Local step 71500, global step 1144756: loss 2.5781
[2019-04-28 00:35:19,192] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 71500, global step 1144756: learning rate 0.0000
[2019-04-28 00:35:19,192] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 71500, global step 1144756: learning rate 0.0000
[2019-04-28 00:35:19,301] A3C_AGENT_WORKER-Thread-3 INFO:Local step 71500, global step 1144793: loss 2.6319
[2019-04-28 00:35:19,303] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 71500, global step 1144794: learning rate 0.0000
[2019-04-28 00:35:19,595] A3C_AGENT_WORKER-Thread-19 INFO:Local step 71500, global step 1144915: loss 2.2386
[2019-04-28 00:35:19,597] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 71500, global step 1144915: learning rate 0.0000
[2019-04-28 00:35:20,389] A3C_AGENT_WORKER-Thread-11 INFO:Local step 71500, global step 1145238: loss 2.7909
[2019-04-28 00:35:20,393] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 71500, global step 1145238: learning rate 0.0000
[2019-04-28 00:35:20,496] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7293917e-19 1.0000000e+00 8.9906629e-29 6.4978384e-11 5.1138157e-24], sum to 1.0000
[2019-04-28 00:35:20,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5822
[2019-04-28 00:35:20,505] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.83333333333333, 84.83333333333333, 1.0, 2.0, 0.5022602342015936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 701831.6854603944, 701831.685460395, 183951.3551339485], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3175800.0000, 
sim time next is 3176400.0000, 
raw observation next is [26.66666666666667, 85.66666666666667, 1.0, 2.0, 0.504063230437664, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704351.9315865864, 704351.9315865864, 184235.3274688484], 
processed observation next is [1.0, 0.782608695652174, 0.4628751974723541, 0.8566666666666667, 1.0, 1.0, 0.4024858198044144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19565331432960734, 0.19565331432960734, 0.27497810069977374], 
reward next is 0.7250, 
noisyNet noise sample is [array([0.36585227], dtype=float32), 0.19052236]. 
=============================================
[2019-04-28 00:35:22,666] A3C_AGENT_WORKER-Thread-17 INFO:Local step 71500, global step 1146206: loss 3.1424
[2019-04-28 00:35:22,667] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 71500, global step 1146207: learning rate 0.0000
[2019-04-28 00:35:23,190] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.5422325e-23 1.0000000e+00 2.3478998e-34 9.1079488e-13 7.7195130e-29], sum to 1.0000
[2019-04-28 00:35:23,199] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6473
[2019-04-28 00:35:23,204] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2576783.015753721 W.
[2019-04-28 00:35:23,208] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.66666666666666, 64.16666666666667, 1.0, 2.0, 0.6141645118056732, 1.0, 2.0, 0.6141645118056732, 1.0, 1.0, 1.03, 6.952345650429232, 6.9112, 170.5573041426782, 2576783.015753721, 2547308.712653212, 493171.8585101063], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3427800.0000, 
sim time next is 3428400.0000, 
raw observation next is [32.33333333333334, 65.33333333333334, 1.0, 2.0, 0.6016016557520468, 1.0, 2.0, 0.6016016557520468, 1.0, 2.0, 1.03, 6.927818513619016, 6.9112, 170.5573041426782, 2524021.173844456, 2512116.656541241, 488559.9480712346], 
processed observation next is [1.0, 0.6956521739130435, 0.7314375987361774, 0.6533333333333334, 1.0, 1.0, 0.5200019948819841, 1.0, 1.0, 0.5200019948819841, 1.0, 1.0, 1.0365853658536586, 0.0016618513619016007, 0.0, 0.8375144448122397, 0.7011169927345712, 0.6978101823725669, 0.7291939523451263], 
reward next is 0.1877, 
noisyNet noise sample is [array([0.44742277], dtype=float32), 0.32826895]. 
=============================================
[2019-04-28 00:35:23,586] A3C_AGENT_WORKER-Thread-15 INFO:Local step 71500, global step 1146615: loss 3.3728
[2019-04-28 00:35:23,588] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 71500, global step 1146616: learning rate 0.0000
[2019-04-28 00:35:24,431] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6357972e-15 1.0000000e+00 3.1955327e-24 2.9837897e-09 9.7081083e-21], sum to 1.0000
[2019-04-28 00:35:24,448] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9550
[2019-04-28 00:35:24,455] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4913679868666576, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686606.5206404879, 686606.5206404879, 182254.3936055178], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3182400.0000, 
sim time next is 3183000.0000, 
raw observation next is [25.0, 94.00000000000001, 1.0, 2.0, 0.4893401349047127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683772.0167078648, 683772.0167078648, 181942.5830266677], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.9400000000000002, 1.0, 1.0, 0.38474715048760566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18993667130774022, 0.18993667130774022, 0.2715560940696533], 
reward next is 0.7284, 
noisyNet noise sample is [array([-1.0159615], dtype=float32), -1.8701166]. 
=============================================
[2019-04-28 00:35:24,471] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[64.78015 ]
 [64.296524]
 [64.15125 ]
 [64.41016 ]
 [64.033066]], R is [[64.8757019 ]
 [64.95492554]
 [65.03320312]
 [65.11083984]
 [65.18745422]].
[2019-04-28 00:35:24,513] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5161741e-17 1.0000000e+00 2.5190243e-27 1.3337672e-08 2.5776085e-21], sum to 1.0000
[2019-04-28 00:35:24,525] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7886
[2019-04-28 00:35:24,531] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 77.50000000000001, 1.0, 2.0, 0.5387596731589183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752852.1300982146, 752852.1300982139, 189892.6394897953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3233400.0000, 
sim time next is 3234000.0000, 
raw observation next is [29.33333333333334, 76.0, 1.0, 2.0, 0.5378160647359543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 751533.0838607838, 751533.0838607844, 189733.9379110298], 
processed observation next is [0.0, 0.43478260869565216, 0.5892575039494474, 0.76, 1.0, 1.0, 0.44315188522404136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20875918996132883, 0.208759189961329, 0.2831849819567609], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.34843764], dtype=float32), -0.83601433]. 
=============================================
[2019-04-28 00:35:24,548] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[70.29499]
 [70.3054 ]
 [70.26792]
 [70.25019]
 [70.23926]], R is [[70.31308746]
 [70.32653046]
 [70.34082794]
 [70.35738373]
 [70.37606049]].
[2019-04-28 00:35:24,984] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72000, global step 1147167: loss 1.4711
[2019-04-28 00:35:24,986] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72000, global step 1147169: learning rate 0.0000
[2019-04-28 00:35:31,758] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9053929e-12 9.9999905e-01 1.0464783e-18 9.9865326e-07 1.3863904e-14], sum to 1.0000
[2019-04-28 00:35:31,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0426
[2019-04-28 00:35:31,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2239811.474551394 W.
[2019-04-28 00:35:31,776] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.5339207924879964, 1.0, 2.0, 0.5339207924879964, 1.0, 1.0, 0.9272438462474216, 6.9112, 6.9112, 170.5573041426782, 2239811.474551394, 2239811.474551394, 439397.1269117893], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3415200.0000, 
sim time next is 3415800.0000, 
raw observation next is [33.0, 67.0, 1.0, 2.0, 0.9667526811523494, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.00599153690079, 6.9112, 168.9123931394257, 2248467.237639788, 2181219.083181226, 453250.0308364908], 
processed observation next is [1.0, 0.5217391304347826, 0.7630331753554502, 0.67, 1.0, 1.0, 0.95994298934018, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479153690079034, 0.0, 0.8294371787427542, 0.6245742326777189, 0.6058941897725628, 0.676492583338046], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.22278176], dtype=float32), 0.3390742]. 
=============================================
[2019-04-28 00:35:31,881] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72000, global step 1149975: loss 5.3079
[2019-04-28 00:35:31,885] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72000, global step 1149978: learning rate 0.0000
[2019-04-28 00:35:31,930] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 00:35:31,933] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:35:31,934] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:35:31,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:35:31,935] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:35:31,936] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:35:31,938] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:35:31,937] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:35:31,938] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:35:31,940] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:35:31,939] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:35:31,957] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run47
[2019-04-28 00:35:31,988] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run47
[2019-04-28 00:35:32,033] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run47
[2019-04-28 00:35:32,033] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run47
[2019-04-28 00:35:32,059] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run47
[2019-04-28 00:35:35,638] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:35:35,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.77838658666666, 95.66760723833335, 1.0, 2.0, 0.2786315435328685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 451422.9503204501, 451422.9503204501, 163683.3762860287]
[2019-04-28 00:35:35,641] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:35:35,644] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6784181e-17 1.0000000e+00 1.8655151e-26 1.6261005e-08 3.6411727e-21], sampled 0.13874121616951574
[2019-04-28 00:35:51,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:35:51,228] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.40304463, 88.97978700499999, 1.0, 2.0, 0.4955795566147571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 692493.4198039519, 692493.4198039513, 182905.2530566187]
[2019-04-28 00:35:51,229] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:35:51,231] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2628791e-17 1.0000000e+00 2.9548295e-26 1.8692649e-08 5.2495725e-21], sampled 0.3423010460884295
[2019-04-28 00:35:53,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:35:53,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.55, 93.0, 1.0, 2.0, 0.4100794608734377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602658.7465041735, 602658.7465041735, 174514.2317436172]
[2019-04-28 00:35:53,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:35:53,425] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9325194e-17 1.0000000e+00 4.4036404e-26 2.1095667e-08 7.2105738e-21], sampled 0.44626800285298907
[2019-04-28 00:36:10,615] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:10,616] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [16.95301631, 100.0, 1.0, 2.0, 0.2377262736899428, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 395014.0346640667, 395014.0346640667, 159421.8609442622]
[2019-04-28 00:36:10,617] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:36:10,623] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2136395e-17 1.0000000e+00 7.6939773e-26 2.4968150e-08 1.1237999e-20], sampled 0.2227188212505251
[2019-04-28 00:36:21,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:21,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.83333333333333, 71.5, 1.0, 2.0, 0.5427197607385906, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 758387.8537620524, 758387.8537620524, 190559.7242899053]
[2019-04-28 00:36:21,969] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:36:21,979] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.8266816e-17 1.0000000e+00 4.1603859e-26 2.0718717e-08 6.8965642e-21], sampled 0.17495418739688617
[2019-04-28 00:36:43,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:43,948] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.33307558, 81.20319486, 1.0, 2.0, 0.5135607238888649, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 717627.7213900483, 717627.7213900489, 185747.6248525041]
[2019-04-28 00:36:43,949] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:36:43,950] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.5767387e-17 1.0000000e+00 3.6084337e-26 1.9860721e-08 6.1543461e-21], sampled 0.5244916917842896
[2019-04-28 00:36:46,450] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:46,451] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.26666666666667, 95.0, 1.0, 2.0, 0.6198026821504385, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.945794182159929, 6.9112, 168.9126812943907, 1732999.334298893, 1708457.070822448, 368957.6160846757]
[2019-04-28 00:36:46,451] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:36:46,456] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.2194024e-12 9.9999523e-01 2.5213214e-18 4.7243966e-06 1.0709988e-14], sampled 0.9621050390210039
[2019-04-28 00:36:46,456] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1732999.334298893 W.
[2019-04-28 00:36:49,482] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:49,483] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.18830234666667, 85.00212543666666, 1.0, 2.0, 0.7065805105278854, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005975693351704, 6.9112, 168.9123178223385, 1884337.299782895, 1817100.415226422, 386079.284091675]
[2019-04-28 00:36:49,484] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:36:49,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.1801264e-11 9.9998438e-01 1.3061766e-16 1.5595093e-05 2.4752140e-13], sampled 0.4884992460837321
[2019-04-28 00:36:49,502] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1884337.299782895 W.
[2019-04-28 00:36:57,135] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.058582257]
[2019-04-28 00:36:57,136] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.03333333333333, 90.66666666666667, 1.0, 2.0, 0.5486667191462722, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9146170408667602, 6.911199999999999, 6.9112, 168.9129563610818, 1533955.881878245, 1533955.881878245, 327842.7048170493]
[2019-04-28 00:36:57,136] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:36:57,138] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.1891789e-13 9.9999785e-01 1.9936436e-19 2.1901799e-06 1.4230087e-15], sampled 0.02728745046384129
[2019-04-28 00:37:09,757] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:37:10,452] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:37:10,508] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:37:10,644] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:37:10,837] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:37:11,854] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 1150000, evaluation results [1150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:37:12,680] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2638647e-11 9.9998605e-01 2.7164777e-18 1.3902502e-05 1.4264866e-15], sum to 1.0000
[2019-04-28 00:37:12,687] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1229
[2019-04-28 00:37:12,694] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2448472.059390577 W.
[2019-04-28 00:37:12,696] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.33333333333334, 65.66666666666667, 1.0, 2.0, 0.8754181702787973, 1.0, 2.0, 0.8754181702787973, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2448472.059390577, 2448472.059390577, 458268.2282841421], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3417600.0000, 
sim time next is 3418200.0000, 
raw observation next is [33.5, 65.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 9.084274694300726, 6.9112, 168.9006964032341, 3826642.995392012, 2285100.912736734, 470139.495138635], 
processed observation next is [1.0, 0.5652173913043478, 0.7867298578199052, 0.65, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.2173074694300726, 0.0, 0.8293797424132645, 1.062956387608892, 0.6347502535379816, 0.701700739012888], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8449078], dtype=float32), -2.23534]. 
=============================================
[2019-04-28 00:37:13,261] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3528039e-15 9.9999821e-01 4.2047877e-23 1.8401498e-06 2.5293632e-18], sum to 1.0000
[2019-04-28 00:37:13,271] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4789
[2019-04-28 00:37:13,276] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2660479.224817477 W.
[2019-04-28 00:37:13,285] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.9511378586520485, 1.0, 2.0, 0.9511378586520485, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2660479.224817477, 2660479.224817477, 500161.538089817], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3427200.0000, 
sim time next is 3427800.0000, 
raw observation next is [32.66666666666666, 64.16666666666667, 1.0, 2.0, 0.6207980796760322, 1.0, 2.0, 0.6207980796760322, 1.0, 1.0, 1.03, 6.965297137749371, 6.9112, 170.5573041426782, 2604643.762554015, 2565891.782253271, 495643.1860648135], 
processed observation next is [1.0, 0.6956521739130435, 0.7472353870458132, 0.6416666666666667, 1.0, 1.0, 0.5431302164771472, 1.0, 1.0, 0.5431302164771472, 1.0, 0.5, 1.0365853658536586, 0.005409713774937064, 0.0, 0.8375144448122397, 0.7235121562650042, 0.7127477172925752, 0.7397659493504679], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.11577082], dtype=float32), -2.69159]. 
=============================================
[2019-04-28 00:37:13,826] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72000, global step 1151002: loss 8.5074
[2019-04-28 00:37:13,828] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72000, global step 1151003: learning rate 0.0000
[2019-04-28 00:37:13,921] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72000, global step 1151052: loss 7.4188
[2019-04-28 00:37:13,924] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72000, global step 1151053: learning rate 0.0000
[2019-04-28 00:37:14,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72000, global step 1151422: loss 7.4102
[2019-04-28 00:37:14,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72000, global step 1151422: learning rate 0.0000
[2019-04-28 00:37:14,635] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72000, global step 1151422: loss 7.2315
[2019-04-28 00:37:14,642] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72000, global step 1151422: learning rate 0.0000
[2019-04-28 00:37:14,710] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72000, global step 1151460: loss 7.1210
[2019-04-28 00:37:14,713] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72000, global step 1151460: learning rate 0.0000
[2019-04-28 00:37:15,914] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72000, global step 1152080: loss 5.9637
[2019-04-28 00:37:15,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72000, global step 1152080: learning rate 0.0000
[2019-04-28 00:37:16,899] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72000, global step 1152587: loss 6.8457
[2019-04-28 00:37:16,901] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72000, global step 1152588: learning rate 0.0000
[2019-04-28 00:37:17,341] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72000, global step 1152815: loss 9.2052
[2019-04-28 00:37:17,343] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72000, global step 1152815: learning rate 0.0000
[2019-04-28 00:37:17,387] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72000, global step 1152839: loss 7.3498
[2019-04-28 00:37:17,390] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72000, global step 1152839: learning rate 0.0000
[2019-04-28 00:37:17,562] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72000, global step 1152929: loss 10.1062
[2019-04-28 00:37:17,566] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72000, global step 1152931: learning rate 0.0000
[2019-04-28 00:37:17,625] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72000, global step 1152963: loss 10.6965
[2019-04-28 00:37:17,626] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72000, global step 1152963: learning rate 0.0000
[2019-04-28 00:37:18,294] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72000, global step 1153303: loss 8.5675
[2019-04-28 00:37:18,296] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72000, global step 1153304: learning rate 0.0000
[2019-04-28 00:37:20,229] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72000, global step 1154266: loss 3.4648
[2019-04-28 00:37:20,231] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72000, global step 1154266: learning rate 0.0000
[2019-04-28 00:37:20,510] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.3624141e-14 9.9248695e-01 1.2422921e-21 7.5130123e-03 1.3417146e-17], sum to 1.0000
[2019-04-28 00:37:20,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0327
[2019-04-28 00:37:20,522] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5110902771966108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714174.4647164326, 714174.4647164332, 185351.8733841149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3451800.0000, 
sim time next is 3452400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5116776294071385, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 714995.4803789494, 714995.4803789487, 185445.8447813016], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41165979446643186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1986098556608193, 0.1986098556608191, 0.27678484295716654], 
reward next is 0.7232, 
noisyNet noise sample is [array([-0.5448], dtype=float32), -1.077605]. 
=============================================
[2019-04-28 00:37:20,750] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72000, global step 1154521: loss 9.4706
[2019-04-28 00:37:20,760] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72000, global step 1154522: learning rate 0.0000
[2019-04-28 00:37:21,896] A3C_AGENT_WORKER-Thread-18 INFO:Local step 72500, global step 1155078: loss 4.8763
[2019-04-28 00:37:21,897] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 72500, global step 1155078: learning rate 0.0000
[2019-04-28 00:37:22,402] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.4437369e-11 9.8413807e-01 1.9852994e-16 1.5861876e-02 3.4604037e-14], sum to 1.0000
[2019-04-28 00:37:22,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9896
[2019-04-28 00:37:22,406] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 68.0, 1.0, 2.0, 0.9393256108046714, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.991104212706684, 6.9112, 168.9124808482393, 2210080.104311544, 2153393.465357796, 445750.4264718157], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3576600.0000, 
sim time next is 3577200.0000, 
raw observation next is [30.66666666666666, 67.33333333333334, 1.0, 2.0, 0.7155705118383088, 1.0, 1.0, 0.7155705118383088, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2001005.268474911, 2001005.268474911, 380860.6250158455], 
processed observation next is [1.0, 0.391304347826087, 0.6524486571879934, 0.6733333333333335, 1.0, 1.0, 0.6573138696847094, 1.0, 0.5, 0.6573138696847094, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5558347967985864, 0.5558347967985864, 0.5684486940535007], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.34413466], dtype=float32), 0.23398863]. 
=============================================
[2019-04-28 00:37:23,617] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.5536521e-15 9.9904662e-01 2.0133396e-22 9.5330825e-04 1.8410632e-16], sum to 1.0000
[2019-04-28 00:37:23,624] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5364
[2019-04-28 00:37:23,631] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5243107355649014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732654.5068667321, 732654.5068667321, 187492.39677599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3621000.0000, 
sim time next is 3621600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5219144311735413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729304.8407424163, 729304.8407424163, 187100.5978990221], 
processed observation next is [1.0, 0.9565217391304348, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42399329057053164, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20258467798400454, 0.20258467798400454, 0.27925462372988374], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.6956466], dtype=float32), 0.70997214]. 
=============================================
[2019-04-28 00:37:25,334] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1476722e-11 8.3457744e-01 2.3108153e-20 1.6542254e-01 1.2773213e-14], sum to 1.0000
[2019-04-28 00:37:25,353] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5185
[2019-04-28 00:37:25,361] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2870940.66471868 W.
[2019-04-28 00:37:25,366] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 1.026292773606093, 1.0, 2.0, 1.026292773606093, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2870940.66471868, 2870940.66471868, 545077.5468899225], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3601800.0000, 
sim time next is 3602400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 1.018871177150046, 1.0, 2.0, 1.018871177150046, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2850155.883517165, 2850155.883517165, 540494.4775592748], 
processed observation next is [1.0, 0.6956521739130435, 0.7630331753554502, 0.63, 1.0, 1.0, 1.0227363580121036, 1.0, 1.0, 1.0227363580121036, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.791709967643657, 0.791709967643657, 0.8067081754616041], 
reward next is 0.1933, 
noisyNet noise sample is [array([0.36720997], dtype=float32), -0.7630319]. 
=============================================
[2019-04-28 00:37:27,825] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.8759956e-10 9.9470204e-01 1.7596624e-16 5.2978899e-03 2.7802172e-12], sum to 1.0000
[2019-04-28 00:37:27,838] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1815
[2019-04-28 00:37:27,848] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2363371.248171351 W.
[2019-04-28 00:37:27,853] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 64.0, 1.0, 2.0, 0.8450202682771181, 1.0, 2.0, 0.8450202682771181, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2363371.248171351, 2363371.248171351, 442378.108036349], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3752400.0000, 
sim time next is 3753000.0000, 
raw observation next is [31.0, 64.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.177767495960283, 6.9112, 168.9113636011236, 2486444.500518031, 2297334.1277336, 476170.500543505], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.645, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.026656749596028283, 0.0, 0.8294321232383663, 0.6906790279216752, 0.6381483688148889, 0.7107022396171716], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.4586008], dtype=float32), 0.60122657]. 
=============================================
[2019-04-28 00:37:27,869] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[44.950775]
 [44.83083 ]
 [43.682983]
 [44.075237]
 [43.178123]], R is [[45.16374969]
 [44.71211243]
 [44.26499176]
 [44.24636078]
 [43.86719131]].
[2019-04-28 00:37:28,042] A3C_AGENT_WORKER-Thread-2 INFO:Local step 72500, global step 1158012: loss -2.2875
[2019-04-28 00:37:28,044] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 72500, global step 1158012: learning rate 0.0000
[2019-04-28 00:37:30,096] A3C_AGENT_WORKER-Thread-20 INFO:Local step 72500, global step 1158992: loss -3.2631
[2019-04-28 00:37:30,097] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 72500, global step 1158993: learning rate 0.0000
[2019-04-28 00:37:30,264] A3C_AGENT_WORKER-Thread-9 INFO:Local step 72500, global step 1159071: loss 25.3355
[2019-04-28 00:37:30,266] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 72500, global step 1159071: learning rate 0.0000
[2019-04-28 00:37:30,809] A3C_AGENT_WORKER-Thread-10 INFO:Local step 72500, global step 1159335: loss -5.7981
[2019-04-28 00:37:30,819] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 72500, global step 1159337: learning rate 0.0000
[2019-04-28 00:37:31,000] A3C_AGENT_WORKER-Thread-14 INFO:Local step 72500, global step 1159421: loss -16.9646
[2019-04-28 00:37:31,003] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 72500, global step 1159422: learning rate 0.0000
[2019-04-28 00:37:31,140] A3C_AGENT_WORKER-Thread-21 INFO:Local step 72500, global step 1159487: loss 9.0018
[2019-04-28 00:37:31,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 72500, global step 1159488: learning rate 0.0000
[2019-04-28 00:37:32,721] A3C_AGENT_WORKER-Thread-13 INFO:Local step 72500, global step 1160200: loss -1.7324
[2019-04-28 00:37:32,723] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 72500, global step 1160200: learning rate 0.0000
[2019-04-28 00:37:33,447] A3C_AGENT_WORKER-Thread-16 INFO:Local step 72500, global step 1160554: loss -19.4340
[2019-04-28 00:37:33,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 72500, global step 1160554: learning rate 0.0000
[2019-04-28 00:37:33,886] A3C_AGENT_WORKER-Thread-3 INFO:Local step 72500, global step 1160775: loss -17.8269
[2019-04-28 00:37:33,888] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 72500, global step 1160775: learning rate 0.0000
[2019-04-28 00:37:34,012] A3C_AGENT_WORKER-Thread-12 INFO:Local step 72500, global step 1160836: loss -37.3345
[2019-04-28 00:37:34,015] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 72500, global step 1160836: learning rate 0.0000
[2019-04-28 00:37:34,102] A3C_AGENT_WORKER-Thread-19 INFO:Local step 72500, global step 1160886: loss -22.3452
[2019-04-28 00:37:34,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 72500, global step 1160886: learning rate 0.0000
[2019-04-28 00:37:34,142] A3C_AGENT_WORKER-Thread-22 INFO:Local step 72500, global step 1160904: loss 8.9061
[2019-04-28 00:37:34,144] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 72500, global step 1160904: learning rate 0.0000
[2019-04-28 00:37:35,049] A3C_AGENT_WORKER-Thread-11 INFO:Local step 72500, global step 1161337: loss -15.7095
[2019-04-28 00:37:35,050] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 72500, global step 1161337: learning rate 0.0000
[2019-04-28 00:37:36,370] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.9068961e-14 9.9785465e-01 7.7669115e-24 2.1453532e-03 4.7435443e-17], sum to 1.0000
[2019-04-28 00:37:36,380] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3185
[2019-04-28 00:37:36,385] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.00000000000001, 1.0, 2.0, 0.5195840069510677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726047.2751243067, 726047.2751243067, 186721.2861602187], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3798600.0000, 
sim time next is 3799200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5205810441340722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727440.9746776012, 727440.9746776005, 186883.3720751748], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.4223868001615328, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2020669374104448, 0.2020669374104446, 0.2789304060823504], 
reward next is 0.7211, 
noisyNet noise sample is [array([0.34374768], dtype=float32), 0.3628251]. 
=============================================
[2019-04-28 00:37:37,412] A3C_AGENT_WORKER-Thread-17 INFO:Local step 72500, global step 1162472: loss 25.5217
[2019-04-28 00:37:37,418] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 72500, global step 1162473: learning rate 0.0000
[2019-04-28 00:37:37,805] A3C_AGENT_WORKER-Thread-15 INFO:Local step 72500, global step 1162679: loss 6.4439
[2019-04-28 00:37:37,809] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 72500, global step 1162681: learning rate 0.0000
[2019-04-28 00:37:38,440] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73000, global step 1162993: loss 0.0037
[2019-04-28 00:37:38,441] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73000, global step 1162993: learning rate 0.0000
[2019-04-28 00:37:43,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0351955e-13 9.9996901e-01 1.6227327e-22 3.0996624e-05 1.4627207e-15], sum to 1.0000
[2019-04-28 00:37:43,619] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6472
[2019-04-28 00:37:43,623] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 72.33333333333333, 1.0, 2.0, 0.619145371748943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865227.269599746, 865227.269599746, 204367.1399305151], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3958800.0000, 
sim time next is 3959400.0000, 
raw observation next is [32.0, 71.66666666666667, 1.0, 2.0, 0.6163266092105485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861286.581036685, 861286.581036685, 203827.0705384729], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.7166666666666667, 1.0, 1.0, 0.5377429026633114, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2392462725101903, 0.2392462725101903, 0.30421950826637745], 
reward next is 0.6958, 
noisyNet noise sample is [array([-0.26767614], dtype=float32), -0.43479893]. 
=============================================
[2019-04-28 00:37:44,572] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73000, global step 1166035: loss 2.3660
[2019-04-28 00:37:44,572] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73000, global step 1166035: learning rate 0.0000
[2019-04-28 00:37:44,587] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.1779427e-13 9.9971920e-01 3.7021175e-21 2.8082932e-04 1.2090111e-13], sum to 1.0000
[2019-04-28 00:37:44,596] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6925
[2019-04-28 00:37:44,601] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 73.0, 1.0, 2.0, 0.6041539172788504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844269.0728390906, 844269.0728390906, 201522.530803418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3965400.0000, 
sim time next is 3966000.0000, 
raw observation next is [31.66666666666666, 72.33333333333333, 1.0, 2.0, 0.605403271208467, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 846015.6657879984, 846015.665787999, 201757.0875099954], 
processed observation next is [0.0, 0.9130434782608695, 0.6998420221169034, 0.7233333333333333, 1.0, 1.0, 0.5245822544680325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23500435160777733, 0.23500435160777752, 0.3011299813582021], 
reward next is 0.6989, 
noisyNet noise sample is [array([0.08966459], dtype=float32), 1.3573946]. 
=============================================
[2019-04-28 00:37:44,614] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[57.43175 ]
 [57.601246]
 [57.74222 ]
 [57.90872 ]
 [58.01409 ]], R is [[57.43144608]
 [57.55635071]
 [57.67987061]
 [57.8004837 ]
 [57.92237473]].
[2019-04-28 00:37:46,604] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73000, global step 1167037: loss 1.7865
[2019-04-28 00:37:46,607] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73000, global step 1167037: learning rate 0.0000
[2019-04-28 00:37:46,669] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73000, global step 1167063: loss 0.9127
[2019-04-28 00:37:46,672] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73000, global step 1167064: learning rate 0.0000
[2019-04-28 00:37:47,289] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73000, global step 1167365: loss 1.0293
[2019-04-28 00:37:47,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73000, global step 1167365: learning rate 0.0000
[2019-04-28 00:37:47,442] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73000, global step 1167443: loss 1.0420
[2019-04-28 00:37:47,446] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73000, global step 1167444: learning rate 0.0000
[2019-04-28 00:37:47,502] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73000, global step 1167475: loss 0.6217
[2019-04-28 00:37:47,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73000, global step 1167477: learning rate 0.0000
[2019-04-28 00:37:49,042] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73000, global step 1168209: loss 0.1878
[2019-04-28 00:37:49,044] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73000, global step 1168209: learning rate 0.0000
[2019-04-28 00:37:49,662] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73000, global step 1168507: loss 0.1556
[2019-04-28 00:37:49,664] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73000, global step 1168508: learning rate 0.0000
[2019-04-28 00:37:50,164] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73000, global step 1168752: loss 0.3156
[2019-04-28 00:37:50,165] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73000, global step 1168752: learning rate 0.0000
[2019-04-28 00:37:50,380] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73000, global step 1168863: loss 0.4827
[2019-04-28 00:37:50,382] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73000, global step 1168863: learning rate 0.0000
[2019-04-28 00:37:50,407] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73000, global step 1168877: loss 0.4624
[2019-04-28 00:37:50,410] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73000, global step 1168877: learning rate 0.0000
[2019-04-28 00:37:50,453] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73000, global step 1168896: loss 0.3845
[2019-04-28 00:37:50,455] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73000, global step 1168897: learning rate 0.0000
[2019-04-28 00:37:51,260] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73000, global step 1169295: loss 0.2167
[2019-04-28 00:37:51,264] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73000, global step 1169296: learning rate 0.0000
[2019-04-28 00:37:53,492] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73000, global step 1170456: loss 0.5805
[2019-04-28 00:37:53,500] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73000, global step 1170459: learning rate 0.0000
[2019-04-28 00:37:53,829] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73000, global step 1170632: loss 0.8131
[2019-04-28 00:37:53,830] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73000, global step 1170632: learning rate 0.0000
[2019-04-28 00:37:54,888] A3C_AGENT_WORKER-Thread-18 INFO:Local step 73500, global step 1171194: loss 4.3155
[2019-04-28 00:37:54,890] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 73500, global step 1171194: learning rate 0.0000
[2019-04-28 00:37:57,796] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0802402e-09 8.4138244e-01 5.7422846e-16 1.5861753e-01 3.5334253e-11], sum to 1.0000
[2019-04-28 00:37:57,805] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0636
[2019-04-28 00:37:57,810] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.6178954176097894, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863479.8059524378, 863479.8059524378, 204127.464731868], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4410600.0000, 
sim time next is 4411200.0000, 
raw observation next is [30.0, 84.0, 1.0, 2.0, 0.617690814464908, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 863193.7664855998, 863193.7664855991, 204088.2841738068], 
processed observation next is [0.0, 0.043478260869565216, 0.6208530805687204, 0.84, 1.0, 1.0, 0.5393865234516964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23977604624599994, 0.23977604624599974, 0.30460937936389076], 
reward next is 0.6954, 
noisyNet noise sample is [array([-0.38929877], dtype=float32), 0.27400273]. 
=============================================
[2019-04-28 00:38:00,490] A3C_AGENT_WORKER-Thread-2 INFO:Local step 73500, global step 1173984: loss -35.1561
[2019-04-28 00:38:00,491] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 73500, global step 1173985: learning rate 0.0000
[2019-04-28 00:38:00,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7479638e-07 7.9889786e-01 5.6322037e-13 2.0110184e-01 1.4096736e-08], sum to 1.0000
[2019-04-28 00:38:00,535] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1133
[2019-04-28 00:38:00,539] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.9182105830617069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1283409.746753589, 1283409.746753589, 274997.6915617574], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4250400.0000, 
sim time next is 4251000.0000, 
raw observation next is [29.16666666666667, 78.33333333333334, 1.0, 2.0, 0.9087680681200019, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1270203.77279028, 1270203.77279028, 272362.4198184701], 
processed observation next is [1.0, 0.17391304347826086, 0.581358609794629, 0.7833333333333334, 1.0, 1.0, 0.8900820097831348, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.35283438133063333, 0.35283438133063333, 0.4065110743559255], 
reward next is 0.5935, 
noisyNet noise sample is [array([0.67955047], dtype=float32), 0.53975266]. 
=============================================
[2019-04-28 00:38:00,558] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[36.014507]
 [36.747284]
 [36.08772 ]
 [35.826187]
 [34.215584]], R is [[35.44265747]
 [35.67778397]
 [35.91353607]
 [36.14143753]
 [36.34053802]].
[2019-04-28 00:38:02,396] A3C_AGENT_WORKER-Thread-20 INFO:Local step 73500, global step 1174918: loss -1.6193
[2019-04-28 00:38:02,402] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 73500, global step 1174918: learning rate 0.0000
[2019-04-28 00:38:02,570] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-28 00:38:02,571] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:38:02,571] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:38:02,571] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:38:02,572] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:38:02,573] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:38:02,574] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:38:02,572] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:38:02,576] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:38:02,576] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:38:02,577] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:38:02,596] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run48
[2019-04-28 00:38:02,622] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run48
[2019-04-28 00:38:02,622] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run48
[2019-04-28 00:38:02,622] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run48
[2019-04-28 00:38:02,656] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run48
[2019-04-28 00:38:11,727] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.055366173]
[2019-04-28 00:38:11,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.45, 53.0, 1.0, 2.0, 0.8322783261398411, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1170483.025464056, 1170483.025464056, 253025.6729915141]
[2019-04-28 00:38:11,729] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:38:11,731] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.72427557e-11 9.98023272e-01 1.04532455e-16 1.97673892e-03
 7.07193158e-12], sampled 0.35743514727489056
[2019-04-28 00:38:17,565] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.055366173]
[2019-04-28 00:38:17,566] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.33333333333333, 81.33333333333334, 1.0, 2.0, 0.2281659983864285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 379012.6130046913, 379012.6130046913, 158556.4332251404]
[2019-04-28 00:38:17,566] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:38:17,569] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2362136e-13 9.9956542e-01 1.7265694e-20 4.3462243e-04 1.5495287e-14], sampled 0.9592220967617806
[2019-04-28 00:38:21,140] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.055366173]
[2019-04-28 00:38:21,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.01666666666667, 96.0, 1.0, 2.0, 0.4104344112541614, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 606509.0010864856, 606509.001086485, 174972.3094376981]
[2019-04-28 00:38:21,141] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:38:21,144] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.9663522e-12 9.9910742e-01 9.9982644e-19 8.9254661e-04 2.7266075e-13], sampled 0.43065970053188585
[2019-04-28 00:38:49,786] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.055366173]
[2019-04-28 00:38:49,787] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.33333333333334, 58.0, 1.0, 2.0, 0.6274279312420187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 876806.5443675154, 876806.5443675154, 205967.7192028473]
[2019-04-28 00:38:49,788] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:38:49,789] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3903917e-13 9.9962091e-01 8.2533699e-21 3.7911849e-04 9.1660806e-15], sampled 0.05356584244450746
[2019-04-28 00:39:25,926] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.055366173]
[2019-04-28 00:39:25,927] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.8, 63.0, 1.0, 2.0, 0.6421607210603265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1006985.398919583, 1006985.398919584, 221935.5449274878]
[2019-04-28 00:39:25,928] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:39:25,931] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5442643e-12 9.9895298e-01 2.6738018e-18 1.0470418e-03 5.3920605e-13], sampled 0.16173405891187365
[2019-04-28 00:39:27,074] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8652.2645 2779335165.2683 926.0000
[2019-04-28 00:39:27,129] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7993.7572 3007831372.5871 1763.0000
[2019-04-28 00:39:27,265] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8247.2494 2927594717.3615 1330.0000
[2019-04-28 00:39:27,319] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7879.6068 3163824386.1296 1769.0000
[2019-04-28 00:39:27,588] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8492.5602 2842596327.8363 1125.0000
[2019-04-28 00:39:28,602] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1175000, evaluation results [1175000.0, 7879.606822799332, 3163824386.129602, 1769.0, 8247.249396930714, 2927594717.361524, 1330.0, 8652.26445185001, 2779335165.268323, 926.0, 7993.757202250196, 3007831372.5871053, 1763.0, 8492.560211394833, 2842596327.8362904, 1125.0]
[2019-04-28 00:39:28,744] A3C_AGENT_WORKER-Thread-9 INFO:Local step 73500, global step 1175068: loss -12.5511
[2019-04-28 00:39:28,745] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 73500, global step 1175068: learning rate 0.0000
[2019-04-28 00:39:29,430] A3C_AGENT_WORKER-Thread-10 INFO:Local step 73500, global step 1175406: loss -18.9723
[2019-04-28 00:39:29,445] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 73500, global step 1175406: learning rate 0.0000
[2019-04-28 00:39:29,490] A3C_AGENT_WORKER-Thread-14 INFO:Local step 73500, global step 1175428: loss -22.6745
[2019-04-28 00:39:29,492] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 73500, global step 1175430: learning rate 0.0000
[2019-04-28 00:39:29,669] A3C_AGENT_WORKER-Thread-21 INFO:Local step 73500, global step 1175519: loss -60.9655
[2019-04-28 00:39:29,672] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 73500, global step 1175521: learning rate 0.0000
[2019-04-28 00:39:30,268] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.1159245e-17 9.9996841e-01 4.7434655e-25 3.1593732e-05 1.1055000e-17], sum to 1.0000
[2019-04-28 00:39:30,278] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5745
[2019-04-28 00:39:30,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5803590755950957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811004.5056470414, 811004.5056470414, 197143.9614597708], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4424400.0000, 
sim time next is 4425000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.580097601090425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 810638.9769055882, 810638.9769055882, 197096.7969505157], 
processed observation next is [0.0, 0.21739130434782608, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4940934952896686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22517749358488562, 0.22517749358488562, 0.29417432380673986], 
reward next is 0.7058, 
noisyNet noise sample is [array([-0.54686046], dtype=float32), 0.04219017]. 
=============================================
[2019-04-28 00:39:30,294] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.992584]
 [72.84485 ]
 [72.78581 ]
 [72.709366]
 [72.84804 ]], R is [[73.0454483 ]
 [73.02075195]
 [72.99613953]
 [72.97151184]
 [72.9467926 ]].
[2019-04-28 00:39:31,138] A3C_AGENT_WORKER-Thread-13 INFO:Local step 73500, global step 1176228: loss -15.9325
[2019-04-28 00:39:31,139] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 73500, global step 1176228: learning rate 0.0000
[2019-04-28 00:39:31,509] A3C_AGENT_WORKER-Thread-16 INFO:Local step 73500, global step 1176410: loss 20.7012
[2019-04-28 00:39:31,511] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 73500, global step 1176410: learning rate 0.0000
[2019-04-28 00:39:32,305] A3C_AGENT_WORKER-Thread-19 INFO:Local step 73500, global step 1176788: loss -19.0141
[2019-04-28 00:39:32,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 73500, global step 1176788: learning rate 0.0000
[2019-04-28 00:39:32,343] A3C_AGENT_WORKER-Thread-3 INFO:Local step 73500, global step 1176809: loss -40.5398
[2019-04-28 00:39:32,347] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 73500, global step 1176810: learning rate 0.0000
[2019-04-28 00:39:32,373] A3C_AGENT_WORKER-Thread-12 INFO:Local step 73500, global step 1176823: loss -3.8479
[2019-04-28 00:39:32,374] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 73500, global step 1176823: learning rate 0.0000
[2019-04-28 00:39:32,612] A3C_AGENT_WORKER-Thread-22 INFO:Local step 73500, global step 1176944: loss -10.7094
[2019-04-28 00:39:32,617] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 73500, global step 1176944: learning rate 0.0000
[2019-04-28 00:39:33,312] A3C_AGENT_WORKER-Thread-11 INFO:Local step 73500, global step 1177291: loss 3.3414
[2019-04-28 00:39:33,313] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 73500, global step 1177292: learning rate 0.0000
[2019-04-28 00:39:35,906] A3C_AGENT_WORKER-Thread-17 INFO:Local step 73500, global step 1178551: loss -23.8634
[2019-04-28 00:39:35,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 73500, global step 1178552: learning rate 0.0000
[2019-04-28 00:39:36,284] A3C_AGENT_WORKER-Thread-15 INFO:Local step 73500, global step 1178734: loss -3.4426
[2019-04-28 00:39:36,286] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 73500, global step 1178734: learning rate 0.0000
[2019-04-28 00:39:37,151] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74000, global step 1179142: loss 1.1087
[2019-04-28 00:39:37,153] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74000, global step 1179142: learning rate 0.0000
[2019-04-28 00:39:41,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9715209e-13 9.9998248e-01 2.5719700e-17 1.7512259e-05 1.5961536e-13], sum to 1.0000
[2019-04-28 00:39:41,810] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6383
[2019-04-28 00:39:41,818] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 88.16666666666667, 1.0, 2.0, 0.5706578546402168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 797442.7510791074, 797442.7510791081, 195406.6287452879], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4582200.0000, 
sim time next is 4582800.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.5749748095201753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803477.5868381604, 803477.5868381604, 196176.1679645576], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4879214572532233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22318821856615567, 0.22318821856615567, 0.29280025069336957], 
reward next is 0.7072, 
noisyNet noise sample is [array([0.21724875], dtype=float32), -0.52042484]. 
=============================================
[2019-04-28 00:39:43,314] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74000, global step 1182128: loss 2.5390
[2019-04-28 00:39:43,316] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74000, global step 1182128: learning rate 0.0000
[2019-04-28 00:39:44,807] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74000, global step 1182854: loss 1.2210
[2019-04-28 00:39:44,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74000, global step 1182854: learning rate 0.0000
[2019-04-28 00:39:45,133] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74000, global step 1183010: loss 0.2955
[2019-04-28 00:39:45,137] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74000, global step 1183010: learning rate 0.0000
[2019-04-28 00:39:45,884] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74000, global step 1183380: loss 1.4134
[2019-04-28 00:39:45,887] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74000, global step 1183382: learning rate 0.0000
[2019-04-28 00:39:46,076] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74000, global step 1183471: loss 2.1378
[2019-04-28 00:39:46,078] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74000, global step 1183471: learning rate 0.0000
[2019-04-28 00:39:46,209] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74000, global step 1183539: loss 0.3841
[2019-04-28 00:39:46,213] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74000, global step 1183539: learning rate 0.0000
[2019-04-28 00:39:46,591] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.19983771e-11 9.99879956e-01 1.40734762e-16 1.20078876e-04
 9.51422708e-12], sum to 1.0000
[2019-04-28 00:39:46,597] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5167
[2019-04-28 00:39:46,601] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.6367575036421909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 889849.7268055704, 889849.726805571, 207786.8547402394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4684800.0000, 
sim time next is 4685400.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.634153649034275, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 886209.3990846409, 886209.3990846416, 207274.5878739175], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.5592212638967168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24616927752351137, 0.24616927752351156, 0.3093650565282351], 
reward next is 0.6906, 
noisyNet noise sample is [array([1.5063865], dtype=float32), 0.5220488]. 
=============================================
[2019-04-28 00:39:47,471] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74000, global step 1184146: loss 0.1584
[2019-04-28 00:39:47,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74000, global step 1184146: learning rate 0.0000
[2019-04-28 00:39:48,158] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74000, global step 1184468: loss 2.3777
[2019-04-28 00:39:48,160] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74000, global step 1184469: learning rate 0.0000
[2019-04-28 00:39:48,642] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74000, global step 1184705: loss 3.1748
[2019-04-28 00:39:48,643] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74000, global step 1184705: learning rate 0.0000
[2019-04-28 00:39:48,791] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74000, global step 1184775: loss 3.2337
[2019-04-28 00:39:48,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74000, global step 1184776: learning rate 0.0000
[2019-04-28 00:39:48,817] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74000, global step 1184784: loss 1.3001
[2019-04-28 00:39:48,819] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74000, global step 1184784: learning rate 0.0000
[2019-04-28 00:39:49,422] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74000, global step 1185085: loss 0.2896
[2019-04-28 00:39:49,424] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74000, global step 1185085: learning rate 0.0000
[2019-04-28 00:39:49,850] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74000, global step 1185294: loss 0.3772
[2019-04-28 00:39:49,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74000, global step 1185295: learning rate 0.0000
[2019-04-28 00:39:52,526] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74000, global step 1186566: loss 0.0892
[2019-04-28 00:39:52,529] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74000, global step 1186566: learning rate 0.0000
[2019-04-28 00:39:52,776] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74000, global step 1186683: loss 0.0771
[2019-04-28 00:39:52,778] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74000, global step 1186683: learning rate 0.0000
[2019-04-28 00:39:53,722] A3C_AGENT_WORKER-Thread-18 INFO:Local step 74500, global step 1187144: loss 19.5223
[2019-04-28 00:39:53,724] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 74500, global step 1187145: learning rate 0.0000
[2019-04-28 00:39:55,895] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3826822e-09 9.8912466e-01 1.4812214e-13 1.0875345e-02 1.8847406e-09], sum to 1.0000
[2019-04-28 00:39:55,902] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6438
[2019-04-28 00:39:55,907] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2151845.386244726 W.
[2019-04-28 00:39:55,912] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.8977201764154289, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.98946468370494, 6.9112, 168.9124264290673, 2151845.386244726, 2096321.900205846, 434038.82309304], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4787400.0000, 
sim time next is 4788000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.5143454014440405, 1.0, 1.0, 0.5143454014440405, 1.0, 2.0, 0.8897916190277945, 6.9112, 6.9112, 170.5573041426782, 2157613.406758713, 2157613.406758713, 424564.4404567627], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.66, 1.0, 1.0, 0.41487397764342226, 1.0, 0.5, 0.41487397764342226, 1.0, 1.0, 0.8655995353997492, 0.0, 0.0, 0.8375144448122397, 0.5993370574329758, 0.5993370574329758, 0.6336782693384517], 
reward next is 0.3663, 
noisyNet noise sample is [array([0.20320354], dtype=float32), -1.0318892]. 
=============================================
[2019-04-28 00:39:55,931] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[39.605133]
 [39.16461 ]
 [39.346626]
 [38.98191 ]
 [39.374725]], R is [[39.81423187]
 [39.4160881 ]
 [39.02192688]
 [38.63170624]
 [38.24538803]].
[2019-04-28 00:39:56,083] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0500374e-12 9.9982446e-01 1.6473104e-18 1.7553588e-04 9.9030173e-14], sum to 1.0000
[2019-04-28 00:39:56,092] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8531
[2019-04-28 00:39:56,095] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.00000000000001, 1.0, 2.0, 0.658356801585593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 920047.1792237106, 920047.1792237106, 212110.0031102683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4767000.0000, 
sim time next is 4767600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.6178432015270224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 863406.806771601, 863406.806771601, 204107.879601269], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.79, 1.0, 1.0, 0.5395701223217138, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2398352241032225, 0.2398352241032225, 0.30463862627055077], 
reward next is 0.6954, 
noisyNet noise sample is [array([0.23020811], dtype=float32), -0.30465683]. 
=============================================
[2019-04-28 00:39:56,585] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.0926520e-13 9.9973196e-01 6.3958363e-22 2.6797978e-04 1.2784035e-15], sum to 1.0000
[2019-04-28 00:39:56,590] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7096
[2019-04-28 00:39:56,593] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.33333333333334, 68.66666666666667, 1.0, 2.0, 0.4979166936402861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 695760.2656839355, 695760.2656839362, 183273.0398552371], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4815600.0000, 
sim time next is 4816200.0000, 
raw observation next is [30.16666666666666, 69.33333333333333, 1.0, 2.0, 0.5034934892079779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703555.5409655685, 703555.5409655678, 184147.5331988882], 
processed observation next is [1.0, 0.7391304347826086, 0.6287519747235385, 0.6933333333333332, 1.0, 1.0, 0.40179938458792513, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1954320947126579, 0.1954320947126577, 0.27484706447595253], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.23959395], dtype=float32), -0.8152893]. 
=============================================
[2019-04-28 00:39:59,773] A3C_AGENT_WORKER-Thread-2 INFO:Local step 74500, global step 1190089: loss -24.5425
[2019-04-28 00:39:59,775] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 74500, global step 1190089: learning rate 0.0000
[2019-04-28 00:40:01,078] A3C_AGENT_WORKER-Thread-20 INFO:Local step 74500, global step 1190725: loss 21.3049
[2019-04-28 00:40:01,081] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 74500, global step 1190726: learning rate 0.0000
[2019-04-28 00:40:01,548] A3C_AGENT_WORKER-Thread-9 INFO:Local step 74500, global step 1190952: loss -4.0068
[2019-04-28 00:40:01,551] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 74500, global step 1190952: learning rate 0.0000
[2019-04-28 00:40:02,040] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2027192e-13 9.6236187e-01 5.2737373e-21 3.7638161e-02 1.4898701e-15], sum to 1.0000
[2019-04-28 00:40:02,046] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0209
[2019-04-28 00:40:02,050] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.493160040738451, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 689111.4355003275, 689111.4355003275, 182531.3177667337], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4909200.0000, 
sim time next is 4909800.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.491514018420261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 686810.6418548356, 686810.6418548356, 182277.1349528615], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.38736628725332645, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19078073384856542, 0.19078073384856542, 0.2720554253027784], 
reward next is 0.7279, 
noisyNet noise sample is [array([-1.7600241], dtype=float32), 0.87032115]. 
=============================================
[2019-04-28 00:40:02,340] A3C_AGENT_WORKER-Thread-14 INFO:Local step 74500, global step 1191344: loss -19.3667
[2019-04-28 00:40:02,348] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 74500, global step 1191344: learning rate 0.0000
[2019-04-28 00:40:02,392] A3C_AGENT_WORKER-Thread-10 INFO:Local step 74500, global step 1191365: loss -22.7787
[2019-04-28 00:40:02,395] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 74500, global step 1191367: learning rate 0.0000
[2019-04-28 00:40:02,629] A3C_AGENT_WORKER-Thread-21 INFO:Local step 74500, global step 1191479: loss 13.8064
[2019-04-28 00:40:02,631] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 74500, global step 1191481: learning rate 0.0000
[2019-04-28 00:40:03,378] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2434005e-14 8.9656842e-01 7.7648103e-23 1.0343159e-01 6.3535248e-16], sum to 1.0000
[2019-04-28 00:40:03,387] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0607
[2019-04-28 00:40:03,394] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.83333333333334, 64.16666666666667, 1.0, 2.0, 0.3664763156925425, 1.0, 1.0, 0.3664763156925425, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1024339.632999482, 1024339.632999482, 264011.6245932384], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4986600.0000, 
sim time next is 4987200.0000, 
raw observation next is [30.66666666666667, 65.33333333333334, 1.0, 2.0, 0.4972723719295376, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129564971034, 694859.6329664508, 694859.6329664515, 183171.8485298743], 
processed observation next is [1.0, 0.7391304347826086, 0.6524486571879939, 0.6533333333333334, 1.0, 1.0, 0.394304062565708, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399450868581, 0.193016564712903, 0.19301656471290318, 0.2733908187013049], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.4430887], dtype=float32), -0.29315317]. 
=============================================
[2019-04-28 00:40:04,084] A3C_AGENT_WORKER-Thread-13 INFO:Local step 74500, global step 1192187: loss -6.7936
[2019-04-28 00:40:04,086] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 74500, global step 1192188: learning rate 0.0000
[2019-04-28 00:40:04,547] A3C_AGENT_WORKER-Thread-16 INFO:Local step 74500, global step 1192409: loss -9.4225
[2019-04-28 00:40:04,549] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 74500, global step 1192409: learning rate 0.0000
[2019-04-28 00:40:05,044] A3C_AGENT_WORKER-Thread-19 INFO:Local step 74500, global step 1192646: loss 12.2606
[2019-04-28 00:40:05,046] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 74500, global step 1192646: learning rate 0.0000
[2019-04-28 00:40:05,318] A3C_AGENT_WORKER-Thread-3 INFO:Local step 74500, global step 1192781: loss 19.3995
[2019-04-28 00:40:05,319] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 74500, global step 1192782: learning rate 0.0000
[2019-04-28 00:40:05,504] A3C_AGENT_WORKER-Thread-12 INFO:Local step 74500, global step 1192873: loss 36.8010
[2019-04-28 00:40:05,509] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 74500, global step 1192873: learning rate 0.0000
[2019-04-28 00:40:05,742] A3C_AGENT_WORKER-Thread-22 INFO:Local step 74500, global step 1192985: loss 19.8763
[2019-04-28 00:40:05,744] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 74500, global step 1192987: learning rate 0.0000
[2019-04-28 00:40:06,285] A3C_AGENT_WORKER-Thread-11 INFO:Local step 74500, global step 1193256: loss 7.9216
[2019-04-28 00:40:06,292] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 74500, global step 1193256: learning rate 0.0000
[2019-04-28 00:40:09,269] A3C_AGENT_WORKER-Thread-17 INFO:Local step 74500, global step 1194645: loss 19.1332
[2019-04-28 00:40:09,271] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 74500, global step 1194646: learning rate 0.0000
[2019-04-28 00:40:09,754] A3C_AGENT_WORKER-Thread-15 INFO:Local step 74500, global step 1194884: loss -18.6253
[2019-04-28 00:40:09,762] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 74500, global step 1194884: learning rate 0.0000
[2019-04-28 00:40:10,108] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75000, global step 1195062: loss 0.0526
[2019-04-28 00:40:10,115] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75000, global step 1195063: learning rate 0.0000
[2019-04-28 00:40:16,158] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75000, global step 1198007: loss 0.4827
[2019-04-28 00:40:16,160] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75000, global step 1198007: learning rate 0.0000
[2019-04-28 00:40:17,642] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75000, global step 1198720: loss 0.0976
[2019-04-28 00:40:17,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75000, global step 1198721: learning rate 0.0000
[2019-04-28 00:40:18,104] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75000, global step 1198935: loss 1.9598
[2019-04-28 00:40:18,105] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75000, global step 1198935: learning rate 0.0000
[2019-04-28 00:40:19,067] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75000, global step 1199412: loss 1.4432
[2019-04-28 00:40:19,072] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75000, global step 1199413: learning rate 0.0000
[2019-04-28 00:40:19,140] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75000, global step 1199438: loss 0.9810
[2019-04-28 00:40:19,142] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75000, global step 1199438: learning rate 0.0000
[2019-04-28 00:40:19,225] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75000, global step 1199481: loss 0.3974
[2019-04-28 00:40:19,227] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75000, global step 1199481: learning rate 0.0000
[2019-04-28 00:40:20,273] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-28 00:40:20,274] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:40:20,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:40:20,275] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:40:20,275] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:40:20,276] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:40:20,277] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:40:20,278] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:40:20,275] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:40:20,278] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:40:20,278] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:40:20,296] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run49
[2019-04-28 00:40:20,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run49
[2019-04-28 00:40:20,297] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run49
[2019-04-28 00:40:20,329] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run49
[2019-04-28 00:40:20,416] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run49
[2019-04-28 00:40:39,338] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:40:39,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.1, 71.33333333333334, 1.0, 2.0, 0.425355337139168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 615654.7900610234, 615654.7900610229, 175472.2527111693]
[2019-04-28 00:40:39,339] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:40:39,342] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1272445e-13 9.9980253e-01 5.2113112e-20 1.9744973e-04 9.9007928e-15], sampled 0.8461002501985366
[2019-04-28 00:40:58,462] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:40:58,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.1, 91.0, 1.0, 2.0, 0.5222188265336719, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729730.3381986142, 729730.3381986136, 187149.9506153538]
[2019-04-28 00:40:58,463] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:40:58,465] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.1965424e-13 9.9977630e-01 9.7380206e-20 2.2371180e-04 1.5667916e-14], sampled 0.3164690266561393
[2019-04-28 00:41:00,734] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:41:00,736] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.11950919, 85.94346494, 1.0, 2.0, 0.9812608811945833, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1371593.918094995, 1371593.918094995, 293272.8172632743]
[2019-04-28 00:41:00,748] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:41:00,758] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5235440e-13 9.9977320e-01 1.1236228e-19 2.2682591e-04 1.7154597e-14], sampled 0.6632825695575115
[2019-04-28 00:42:21,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:42:21,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.782851515, 50.161977085, 1.0, 2.0, 0.3943466916678852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577700.4612760544, 577700.4612760544, 172140.8488443068]
[2019-04-28 00:42:21,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:42:21,048] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8370190e-13 9.9972242e-01 3.1093254e-19 2.7755689e-04 3.6160860e-14], sampled 0.5255061194598453
[2019-04-28 00:42:25,918] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:42:25,919] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.49070147666666, 52.167177105, 1.0, 2.0, 0.6966280115117595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1005347.683332921, 1005347.683332921, 224340.5928428179]
[2019-04-28 00:42:25,919] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:42:25,923] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4987923e-13 9.9982387e-01 3.0421389e-20 1.7619391e-04 6.6276474e-15], sampled 0.371608985658088
[2019-04-28 00:42:30,808] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.057794638]
[2019-04-28 00:42:30,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.13333333333333, 87.0, 1.0, 2.0, 0.5096943948186288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712223.2678376733, 712223.2678376739, 185127.9777475068]
[2019-04-28 00:42:30,809] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:42:30,831] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0445196e-13 9.9980503e-01 4.9056059e-20 1.9500307e-04 9.4678463e-15], sampled 0.6254286525771975
[2019-04-28 00:43:21,917] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7880.6128 3163977724.5355 1773.0000
[2019-04-28 00:43:22,291] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.3126 2842552360.4526 1130.0000
[2019-04-28 00:43:22,681] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.5850 3007607719.2777 1766.0000
[2019-04-28 00:43:22,950] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8256.2738 2927250479.1270 1334.0000
[2019-04-28 00:43:24,573] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8658.8893 2779214665.8494 932.0000
[2019-04-28 00:43:25,589] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1200000, evaluation results [1200000.0, 7880.612794383182, 3163977724.5354795, 1773.0, 8256.273836294333, 2927250479.1270065, 1334.0, 8658.88926503367, 2779214665.8494225, 932.0, 7998.585022533326, 3007607719.2776685, 1766.0, 8495.312646319444, 2842552360.452614, 1130.0]
[2019-04-28 00:43:27,189] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75000, global step 1200235: loss 0.8230
[2019-04-28 00:43:27,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75000, global step 1200235: learning rate 0.0000
[2019-04-28 00:43:28,128] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75000, global step 1200532: loss 0.0898
[2019-04-28 00:43:28,141] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75000, global step 1200533: learning rate 0.0000
[2019-04-28 00:43:28,238] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75000, global step 1200577: loss 0.1131
[2019-04-28 00:43:28,241] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75000, global step 1200578: learning rate 0.0000
[2019-04-28 00:43:28,540] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75000, global step 1200685: loss 0.1010
[2019-04-28 00:43:28,544] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75000, global step 1200686: learning rate 0.0000
[2019-04-28 00:43:28,792] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75000, global step 1200787: loss 0.4110
[2019-04-28 00:43:28,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75000, global step 1200788: learning rate 0.0000
[2019-04-28 00:43:29,956] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75000, global step 1201241: loss 0.4088
[2019-04-28 00:43:29,957] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75000, global step 1201241: learning rate 0.0000
[2019-04-28 00:43:30,003] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75000, global step 1201259: loss 0.2414
[2019-04-28 00:43:30,006] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75000, global step 1201260: learning rate 0.0000
[2019-04-28 00:43:32,914] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.5281775e-08 9.9129021e-01 4.0849416e-12 8.7097408e-03 2.0170944e-10], sum to 1.0000
[2019-04-28 00:43:32,920] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7079
[2019-04-28 00:43:32,936] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2272687.725011968 W.
[2019-04-28 00:43:32,948] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 77.33333333333334, 1.0, 2.0, 0.8126259503244446, 1.0, 1.0, 0.8126259503244446, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2272687.725011968, 2272687.725011968, 426077.0739579955], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5300400.0000, 
sim time next is 5301000.0000, 
raw observation next is [31.5, 75.5, 1.0, 2.0, 0.9042834954127018, 1.0, 2.0, 0.9042834954127018, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2529287.691783512, 2529287.691783512, 473843.7311498473], 
processed observation next is [1.0, 0.34782608695652173, 0.6919431279620853, 0.755, 1.0, 1.0, 0.8846789101357853, 1.0, 1.0, 0.8846789101357853, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7025799143843089, 0.7025799143843089, 0.7072294494773841], 
reward next is 0.2928, 
noisyNet noise sample is [array([1.981069], dtype=float32), 0.79928017]. 
=============================================
[2019-04-28 00:43:32,972] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[35.127384]
 [34.531815]
 [34.726154]
 [34.09879 ]
 [34.037582]], R is [[35.69081879]
 [35.3339119 ]
 [34.98057175]
 [35.15726471]
 [34.80569077]].
[2019-04-28 00:43:33,063] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75000, global step 1202573: loss 0.3037
[2019-04-28 00:43:33,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75000, global step 1202573: learning rate 0.0000
[2019-04-28 00:43:33,557] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75000, global step 1202779: loss 0.0566
[2019-04-28 00:43:33,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75000, global step 1202779: learning rate 0.0000
[2019-04-28 00:43:34,781] A3C_AGENT_WORKER-Thread-18 INFO:Local step 75500, global step 1203280: loss 0.3588
[2019-04-28 00:43:34,782] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 75500, global step 1203280: learning rate 0.0000
[2019-04-28 00:43:36,573] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.02355256e-10 9.99275148e-01 1.71334013e-14 7.24823622e-04
 2.26135235e-12], sum to 1.0000
[2019-04-28 00:43:36,580] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0443
[2019-04-28 00:43:36,590] A3C_AGENT_WORKER-Thread-2 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3121777.872180487 W.
[2019-04-28 00:43:36,601] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [35.7, 58.0, 1.0, 2.0, 0.8466188160386985, 1.0, 2.0, 0.7438994475336117, 1.0, 1.0, 1.03, 7.005109295762353, 6.9112, 170.5573041426782, 3121777.872180487, 3054506.823639996, 571639.6728889245], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5486400.0000, 
sim time next is 5487000.0000, 
raw observation next is [35.8, 57.0, 1.0, 2.0, 0.814595480550013, 1.0, 2.0, 0.7278877797892691, 1.0, 2.0, 1.03, 7.005106769643354, 6.9112, 170.5573041426782, 3054502.658829899, 2987233.419851203, 559967.9823395908], 
processed observation next is [1.0, 0.5217391304347826, 0.895734597156398, 0.57, 1.0, 1.0, 0.77662106090363, 1.0, 1.0, 0.6721539515533362, 1.0, 1.0, 1.0365853658536586, 0.009390676964335399, 0.0, 0.8375144448122397, 0.848472960786083, 0.8297870610697786, 0.8357731079695385], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0571892], dtype=float32), 0.7072092]. 
=============================================
[2019-04-28 00:43:36,630] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[42.395912]
 [42.067696]
 [41.68023 ]
 [41.607033]
 [41.373917]], R is [[42.17480469]
 [41.75305557]
 [41.33552551]
 [40.92217255]
 [40.5129509 ]].
[2019-04-28 00:43:41,226] A3C_AGENT_WORKER-Thread-2 INFO:Local step 75500, global step 1205901: loss 0.0308
[2019-04-28 00:43:41,228] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 75500, global step 1205901: learning rate 0.0000
[2019-04-28 00:43:42,558] A3C_AGENT_WORKER-Thread-20 INFO:Local step 75500, global step 1206452: loss 0.4654
[2019-04-28 00:43:42,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 75500, global step 1206453: learning rate 0.0000
[2019-04-28 00:43:43,351] A3C_AGENT_WORKER-Thread-9 INFO:Local step 75500, global step 1206760: loss 0.4896
[2019-04-28 00:43:43,352] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 75500, global step 1206760: learning rate 0.0000
[2019-04-28 00:43:44,000] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4668357e-13 9.9580967e-01 1.3109633e-19 4.1903383e-03 2.0749161e-14], sum to 1.0000
[2019-04-28 00:43:44,005] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6405
[2019-04-28 00:43:44,020] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.41666666666667, 85.16666666666667, 1.0, 2.0, 0.5371257793989258, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750568.1525656576, 750568.1525656576, 189616.9457800235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5692200.0000, 
sim time next is 5692800.0000, 
raw observation next is [27.33333333333334, 85.33333333333334, 1.0, 2.0, 0.5352230429333, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747908.3730064328, 747908.3730064334, 189298.3995895732], 
processed observation next is [0.0, 0.9130434782608695, 0.4944707740916275, 0.8533333333333334, 1.0, 1.0, 0.44002776257024095, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2077523258351202, 0.20775232583512038, 0.282534924760557], 
reward next is 0.7175, 
noisyNet noise sample is [array([-0.90529686], dtype=float32), -1.2978994]. 
=============================================
[2019-04-28 00:43:45,246] A3C_AGENT_WORKER-Thread-10 INFO:Local step 75500, global step 1207476: loss -1.5968
[2019-04-28 00:43:45,247] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 75500, global step 1207476: learning rate 0.0000
[2019-04-28 00:43:45,510] A3C_AGENT_WORKER-Thread-14 INFO:Local step 75500, global step 1207564: loss 1.8167
[2019-04-28 00:43:45,512] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 75500, global step 1207566: learning rate 0.0000
[2019-04-28 00:43:45,682] A3C_AGENT_WORKER-Thread-21 INFO:Local step 75500, global step 1207646: loss 0.5583
[2019-04-28 00:43:45,690] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 75500, global step 1207648: learning rate 0.0000
[2019-04-28 00:43:46,479] A3C_AGENT_WORKER-Thread-13 INFO:Local step 75500, global step 1207951: loss 0.0875
[2019-04-28 00:43:46,480] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 75500, global step 1207952: learning rate 0.0000
[2019-04-28 00:43:47,555] A3C_AGENT_WORKER-Thread-19 INFO:Local step 75500, global step 1208360: loss 0.5637
[2019-04-28 00:43:47,572] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 75500, global step 1208360: learning rate 0.0000
[2019-04-28 00:43:48,290] A3C_AGENT_WORKER-Thread-3 INFO:Local step 75500, global step 1208649: loss 4.3279
[2019-04-28 00:43:48,292] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 75500, global step 1208651: learning rate 0.0000
[2019-04-28 00:43:48,338] A3C_AGENT_WORKER-Thread-12 INFO:Local step 75500, global step 1208674: loss 1.0960
[2019-04-28 00:43:48,345] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 75500, global step 1208674: learning rate 0.0000
[2019-04-28 00:43:48,418] A3C_AGENT_WORKER-Thread-16 INFO:Local step 75500, global step 1208707: loss 1.4628
[2019-04-28 00:43:48,419] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 75500, global step 1208707: learning rate 0.0000
[2019-04-28 00:43:49,997] A3C_AGENT_WORKER-Thread-22 INFO:Local step 75500, global step 1209332: loss 0.1050
[2019-04-28 00:43:49,999] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 75500, global step 1209332: learning rate 0.0000
[2019-04-28 00:43:50,106] A3C_AGENT_WORKER-Thread-11 INFO:Local step 75500, global step 1209369: loss 0.4778
[2019-04-28 00:43:50,112] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 75500, global step 1209370: learning rate 0.0000
[2019-04-28 00:43:53,138] A3C_AGENT_WORKER-Thread-15 INFO:Local step 75500, global step 1210532: loss -2.9641
[2019-04-28 00:43:53,141] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 75500, global step 1210532: learning rate 0.0000
[2019-04-28 00:43:53,540] A3C_AGENT_WORKER-Thread-17 INFO:Local step 75500, global step 1210678: loss 0.1309
[2019-04-28 00:43:53,542] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 75500, global step 1210679: learning rate 0.0000
[2019-04-28 00:43:55,374] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76000, global step 1211412: loss 0.3074
[2019-04-28 00:43:55,376] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76000, global step 1211413: learning rate 0.0000
[2019-04-28 00:43:59,352] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2535187e-16 9.8932379e-01 8.5595305e-24 1.0676237e-02 1.5603273e-19], sum to 1.0000
[2019-04-28 00:43:59,353] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1822
[2019-04-28 00:43:59,358] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.76666666666667, 83.66666666666667, 1.0, 2.0, 0.5383356371834498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752259.380780187, 752259.3807801863, 189820.2847899604], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5689200.0000, 
sim time next is 5689800.0000, 
raw observation next is [27.7, 84.0, 1.0, 2.0, 0.5376063055533814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751239.8670519596, 751239.8670519596, 189697.8121916432], 
processed observation next is [0.0, 0.8695652173913043, 0.5118483412322274, 0.84, 1.0, 1.0, 0.442899163317327, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20867774084776655, 0.20867774084776655, 0.28313106297260177], 
reward next is 0.7169, 
noisyNet noise sample is [array([-0.00757495], dtype=float32), -0.5464965]. 
=============================================
[2019-04-28 00:43:59,663] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.0345588e-10 9.5771366e-01 5.3294016e-16 4.2286340e-02 1.2421437e-13], sum to 1.0000
[2019-04-28 00:43:59,668] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6670
[2019-04-28 00:43:59,672] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 89.0, 1.0, 2.0, 0.5372087543229461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750684.140932508, 750684.1409325086, 189630.9578230292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5792400.0000, 
sim time next is 5793000.0000, 
raw observation next is [26.88333333333333, 89.0, 1.0, 2.0, 0.5368667373309692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750206.0448565988, 750206.0448565994, 189573.5949418262], 
processed observation next is [1.0, 0.043478260869565216, 0.47314375987361756, 0.89, 1.0, 1.0, 0.44200811726622796, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2083905680157219, 0.20839056801572206, 0.2829456640922779], 
reward next is 0.7171, 
noisyNet noise sample is [array([1.8472509], dtype=float32), 1.8186979]. 
=============================================
[2019-04-28 00:43:59,703] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[45.77537 ]
 [47.094364]
 [48.0035  ]
 [49.941494]
 [51.111126]], R is [[45.38108444]
 [45.64424133]
 [45.90459061]
 [46.16217804]
 [46.41710663]].
[2019-04-28 00:44:00,441] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1465278e-13 9.9963117e-01 6.7749554e-21 3.6884658e-04 1.9663473e-18], sum to 1.0000
[2019-04-28 00:44:00,458] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3939
[2019-04-28 00:44:00,467] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333333, 84.33333333333333, 1.0, 2.0, 0.5468479034284822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764158.5288643345, 764158.5288643345, 191261.2868749911], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5778600.0000, 
sim time next is 5779200.0000, 
raw observation next is [27.76666666666667, 84.66666666666667, 1.0, 2.0, 0.5463694601758492, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 763489.7178750446, 763489.717875044, 191179.7054464957], 
processed observation next is [0.0, 0.9130434782608695, 0.515007898894155, 0.8466666666666667, 1.0, 1.0, 0.45345718093475806, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21208047718751238, 0.21208047718751222, 0.2853428439499936], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.61741537], dtype=float32), 1.1492418]. 
=============================================
[2019-04-28 00:44:02,482] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76000, global step 1214212: loss 5.8603
[2019-04-28 00:44:02,490] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76000, global step 1214213: learning rate 0.0000
[2019-04-28 00:44:03,269] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76000, global step 1214483: loss 2.0948
[2019-04-28 00:44:03,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76000, global step 1214483: learning rate 0.0000
[2019-04-28 00:44:03,971] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76000, global step 1214764: loss 0.1898
[2019-04-28 00:44:03,975] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76000, global step 1214765: learning rate 0.0000
[2019-04-28 00:44:05,822] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76000, global step 1215504: loss 0.4746
[2019-04-28 00:44:05,827] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76000, global step 1215505: learning rate 0.0000
[2019-04-28 00:44:05,829] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76000, global step 1215510: loss 4.3298
[2019-04-28 00:44:05,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76000, global step 1215510: learning rate 0.0000
[2019-04-28 00:44:06,353] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76000, global step 1215731: loss 0.1589
[2019-04-28 00:44:06,357] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76000, global step 1215732: learning rate 0.0000
[2019-04-28 00:44:06,907] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76000, global step 1215952: loss 0.6454
[2019-04-28 00:44:06,909] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76000, global step 1215952: learning rate 0.0000
[2019-04-28 00:44:08,388] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76000, global step 1216550: loss 0.0982
[2019-04-28 00:44:08,389] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76000, global step 1216550: learning rate 0.0000
[2019-04-28 00:44:08,441] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76000, global step 1216566: loss 0.1606
[2019-04-28 00:44:08,443] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76000, global step 1216566: learning rate 0.0000
[2019-04-28 00:44:08,511] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76000, global step 1216599: loss 0.2564
[2019-04-28 00:44:08,511] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76000, global step 1216599: learning rate 0.0000
[2019-04-28 00:44:08,856] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.4309521e-11 9.5194823e-01 4.6373347e-17 4.8051767e-02 1.1253218e-13], sum to 1.0000
[2019-04-28 00:44:08,863] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6202
[2019-04-28 00:44:08,869] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.03333333333333, 88.33333333333334, 1.0, 2.0, 0.5393116438702765, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 753623.7166893932, 753623.7166893926, 189984.2432891647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5790000.0000, 
sim time next is 5790600.0000, 
raw observation next is [27.0, 88.5, 1.0, 2.0, 0.5388806213244006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753021.2005838873, 753021.2005838873, 189911.7203498606], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.885, 1.0, 1.0, 0.4444344835233742, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20917255571774646, 0.20917255571774646, 0.28345032888038896], 
reward next is 0.7165, 
noisyNet noise sample is [array([1.771573], dtype=float32), 0.3124999]. 
=============================================
[2019-04-28 00:44:09,489] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76000, global step 1216997: loss -2.7868
[2019-04-28 00:44:09,491] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76000, global step 1216998: learning rate 0.0000
[2019-04-28 00:44:10,107] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76000, global step 1217261: loss 0.5297
[2019-04-28 00:44:10,110] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76000, global step 1217262: learning rate 0.0000
[2019-04-28 00:44:10,252] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76000, global step 1217312: loss 1.7819
[2019-04-28 00:44:10,253] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76000, global step 1217312: learning rate 0.0000
[2019-04-28 00:44:13,103] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76000, global step 1218392: loss 0.2190
[2019-04-28 00:44:13,107] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76000, global step 1218395: learning rate 0.0000
[2019-04-28 00:44:14,183] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76000, global step 1218805: loss 4.8205
[2019-04-28 00:44:14,190] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76000, global step 1218806: learning rate 0.0000
[2019-04-28 00:44:14,534] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1756934e-12 9.9988198e-01 1.3492376e-19 1.1805725e-04 3.7971840e-14], sum to 1.0000
[2019-04-28 00:44:14,543] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8316
[2019-04-28 00:44:14,558] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.45, 91.0, 1.0, 2.0, 0.5259219246361109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 734906.7080904213, 734906.7080904213, 187757.2462098839], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5968200.0000, 
sim time next is 5968800.0000, 
raw observation next is [26.4, 91.0, 1.0, 2.0, 0.5240565623480476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 732299.2111800533, 732299.2111800539, 187451.1534132516], 
processed observation next is [1.0, 0.08695652173913043, 0.45023696682464454, 0.91, 1.0, 1.0, 0.42657417150367183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2034164475500148, 0.20341644755001498, 0.2797778409153009], 
reward next is 0.7202, 
noisyNet noise sample is [array([-0.00629076], dtype=float32), 0.6106532]. 
=============================================
[2019-04-28 00:44:15,356] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.0149355e-09 9.8872840e-01 2.1803275e-13 1.1271588e-02 1.7748384e-11], sum to 1.0000
[2019-04-28 00:44:15,367] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2411
[2019-04-28 00:44:15,376] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2490340.263613979 W.
[2019-04-28 00:44:15,385] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.3, 76.33333333333334, 1.0, 2.0, 0.5935817866861129, 1.0, 1.0, 0.5935817866861129, 1.0, 2.0, 1.03, 6.912161434721716, 6.9112, 170.5573041426782, 2490340.263613979, 2489651.548814151, 485662.7842695987], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5996400.0000, 
sim time next is 5997000.0000, 
raw observation next is [30.45, 75.66666666666666, 1.0, 2.0, 0.5713920243906362, 1.0, 2.0, 0.5713920243906362, 1.0, 2.0, 0.992318984136557, 6.911200000000001, 6.9112, 170.5573041426782, 2397155.020022604, 2397155.020022604, 468025.5258004421], 
processed observation next is [1.0, 0.391304347826087, 0.6421800947867299, 0.7566666666666666, 1.0, 1.0, 0.4836048486634171, 1.0, 1.0, 0.4836048486634171, 1.0, 1.0, 0.990632907483606, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6658763944507233, 0.6658763944507233, 0.6985455608961822], 
reward next is 0.3015, 
noisyNet noise sample is [array([-0.00524059], dtype=float32), -0.33849105]. 
=============================================
[2019-04-28 00:44:15,405] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[36.72209 ]
 [36.740307]
 [37.10143 ]
 [37.68546 ]
 [38.530113]], R is [[36.5524826 ]
 [36.18695831]
 [35.8250885 ]
 [35.78688049]
 [35.76188278]].
[2019-04-28 00:44:15,584] A3C_AGENT_WORKER-Thread-18 INFO:Local step 76500, global step 1219361: loss 0.2481
[2019-04-28 00:44:15,588] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 76500, global step 1219363: learning rate 0.0000
[2019-04-28 00:44:18,378] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3301398e-14 9.9724030e-01 5.0839777e-21 2.7596976e-03 3.7551400e-16], sum to 1.0000
[2019-04-28 00:44:18,387] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8176
[2019-04-28 00:44:18,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2350230.925565519 W.
[2019-04-28 00:44:18,394] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.26666666666667, 72.33333333333333, 1.0, 2.0, 0.8403263771787743, 1.0, 2.0, 0.8403263771787743, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2350230.925565519, 2350230.925565519, 439987.3905375014], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6000000.0000, 
sim time next is 6000600.0000, 
raw observation next is [31.43333333333333, 71.66666666666667, 1.0, 2.0, 0.849963489895401, 1.0, 2.0, 0.849963489895401, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2377209.708966985, 2377209.708966984, 444934.6974989294], 
processed observation next is [1.0, 0.43478260869565216, 0.6887835703001578, 0.7166666666666667, 1.0, 1.0, 0.8192331203559049, 1.0, 1.0, 0.8192331203559049, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6603360302686069, 0.6603360302686067, 0.6640816380581036], 
reward next is 0.3359, 
noisyNet noise sample is [array([-0.20471133], dtype=float32), 0.062197268]. 
=============================================
[2019-04-28 00:44:22,423] A3C_AGENT_WORKER-Thread-2 INFO:Local step 76500, global step 1222033: loss 0.1145
[2019-04-28 00:44:22,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 76500, global step 1222033: learning rate 0.0000
[2019-04-28 00:44:23,403] A3C_AGENT_WORKER-Thread-20 INFO:Local step 76500, global step 1222445: loss 0.0666
[2019-04-28 00:44:23,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 76500, global step 1222445: learning rate 0.0000
[2019-04-28 00:44:24,283] A3C_AGENT_WORKER-Thread-9 INFO:Local step 76500, global step 1222814: loss 0.0642
[2019-04-28 00:44:24,289] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 76500, global step 1222814: learning rate 0.0000
[2019-04-28 00:44:25,886] A3C_AGENT_WORKER-Thread-21 INFO:Local step 76500, global step 1223474: loss 0.0428
[2019-04-28 00:44:25,886] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 76500, global step 1223474: learning rate 0.0000
[2019-04-28 00:44:25,940] A3C_AGENT_WORKER-Thread-10 INFO:Local step 76500, global step 1223490: loss 0.0671
[2019-04-28 00:44:25,944] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 76500, global step 1223490: learning rate 0.0000
[2019-04-28 00:44:26,169] A3C_AGENT_WORKER-Thread-14 INFO:Local step 76500, global step 1223583: loss 0.0783
[2019-04-28 00:44:26,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 76500, global step 1223583: learning rate 0.0000
[2019-04-28 00:44:27,015] A3C_AGENT_WORKER-Thread-13 INFO:Local step 76500, global step 1223934: loss 0.1373
[2019-04-28 00:44:27,019] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 76500, global step 1223935: learning rate 0.0000
[2019-04-28 00:44:27,593] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8131684e-16 9.9998307e-01 2.4895458e-22 1.6913300e-05 5.7084512e-18], sum to 1.0000
[2019-04-28 00:44:27,596] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0190
[2019-04-28 00:44:27,598] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.63333333333333, 89.33333333333334, 1.0, 2.0, 0.5213215263481616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728476.0523341007, 728476.0523341007, 187004.4168281422], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6222000.0000, 
sim time next is 6222600.0000, 
raw observation next is [26.6, 89.5, 1.0, 2.0, 0.5209585752752517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727968.7037008121, 727968.7037008121, 186945.2724549527], 
processed observation next is [0.0, 0.0, 0.4597156398104266, 0.895, 1.0, 1.0, 0.4228416569581345, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20221352880578114, 0.20221352880578114, 0.27902279470888464], 
reward next is 0.7210, 
noisyNet noise sample is [array([1.6351714], dtype=float32), -1.4003431]. 
=============================================
[2019-04-28 00:44:28,021] A3C_AGENT_WORKER-Thread-16 INFO:Local step 76500, global step 1224347: loss 0.1880
[2019-04-28 00:44:28,022] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 76500, global step 1224347: learning rate 0.0000
[2019-04-28 00:44:28,790] A3C_AGENT_WORKER-Thread-19 INFO:Local step 76500, global step 1224642: loss 0.0703
[2019-04-28 00:44:28,791] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 76500, global step 1224642: learning rate 0.0000
[2019-04-28 00:44:28,930] A3C_AGENT_WORKER-Thread-12 INFO:Local step 76500, global step 1224699: loss 0.1306
[2019-04-28 00:44:28,932] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 76500, global step 1224699: learning rate 0.0000
[2019-04-28 00:44:29,424] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3953882e-16 9.9998987e-01 6.2731942e-23 1.0148673e-05 2.0973212e-19], sum to 1.0000
[2019-04-28 00:44:29,433] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9746
[2019-04-28 00:44:29,436] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.25, 86.16666666666667, 1.0, 2.0, 0.5296273506821226, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740086.3573434277, 740086.3573434271, 188368.3708849643], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6210600.0000, 
sim time next is 6211200.0000, 
raw observation next is [27.2, 86.33333333333334, 1.0, 2.0, 0.528573020536152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 738612.5538424747, 738612.5538424747, 188194.0872299482], 
processed observation next is [1.0, 0.9130434782608695, 0.4881516587677725, 0.8633333333333334, 1.0, 1.0, 0.43201568739295415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20517015384513188, 0.20517015384513188, 0.28088669735813165], 
reward next is 0.7191, 
noisyNet noise sample is [array([1.9581854], dtype=float32), -0.5010052]. 
=============================================
[2019-04-28 00:44:29,691] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-28 00:44:29,696] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:44:29,697] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:44:29,698] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:44:29,699] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:44:29,699] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:44:29,700] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:44:29,700] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:44:29,702] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:44:29,703] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:44:29,704] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:44:29,707] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run50
[2019-04-28 00:44:29,708] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run50
[2019-04-28 00:44:29,788] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run50
[2019-04-28 00:44:29,822] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run50
[2019-04-28 00:44:29,862] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run50
[2019-04-28 00:45:17,537] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.054257464]
[2019-04-28 00:45:17,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.83333333333334, 58.00000000000001, 1.0, 2.0, 0.5852886687966733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 817895.8626328991, 817895.8626328997, 198036.9862681575]
[2019-04-28 00:45:17,538] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:45:17,540] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0008911e-16 9.9994457e-01 1.6814985e-23 5.5416640e-05 2.8844920e-19], sampled 0.41777734357299323
[2019-04-28 00:45:19,670] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.054257464]
[2019-04-28 00:45:19,672] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.5, 68.0, 1.0, 2.0, 0.5778988950669013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 807565.2969479054, 807565.2969479061, 196700.3908199703]
[2019-04-28 00:45:19,673] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:45:19,675] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8539308e-16 9.9995160e-01 7.8758208e-24 4.8395090e-05 1.5499401e-19], sampled 0.491952918366525
[2019-04-28 00:46:06,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.6589 3007632146.2353 1765.0000
[2019-04-28 00:46:06,399] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.5160 2927213662.6833 1337.0000
[2019-04-28 00:46:06,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8495.9279 2842511096.2894 1130.0000
[2019-04-28 00:46:07,037] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.7893 2779092635.6005 932.0000
[2019-04-28 00:46:07,065] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7885.5402 3164348448.1752 1780.0000
[2019-04-28 00:46:08,081] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1225000, evaluation results [1225000.0, 7885.540232755035, 3164348448.1751738, 1780.0, 8255.515975951992, 2927213662.683334, 1337.0, 8661.7892953408, 2779092635.6005187, 932.0, 7998.658945410793, 3007632146.2352667, 1765.0, 8495.92793946984, 2842511096.2893577, 1130.0]
[2019-04-28 00:46:08,133] A3C_AGENT_WORKER-Thread-3 INFO:Local step 76500, global step 1225034: loss 0.1849
[2019-04-28 00:46:08,135] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 76500, global step 1225035: learning rate 0.0000
[2019-04-28 00:46:08,581] A3C_AGENT_WORKER-Thread-22 INFO:Local step 76500, global step 1225252: loss 0.1158
[2019-04-28 00:46:08,582] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 76500, global step 1225252: learning rate 0.0000
[2019-04-28 00:46:08,608] A3C_AGENT_WORKER-Thread-11 INFO:Local step 76500, global step 1225266: loss 0.1742
[2019-04-28 00:46:08,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 76500, global step 1225267: learning rate 0.0000
[2019-04-28 00:46:11,243] A3C_AGENT_WORKER-Thread-15 INFO:Local step 76500, global step 1226618: loss 0.1445
[2019-04-28 00:46:11,247] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 76500, global step 1226619: learning rate 0.0000
[2019-04-28 00:46:11,421] A3C_AGENT_WORKER-Thread-17 INFO:Local step 76500, global step 1226711: loss 0.3759
[2019-04-28 00:46:11,426] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 76500, global step 1226712: learning rate 0.0000
[2019-04-28 00:46:12,624] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77000, global step 1227324: loss 170.8972
[2019-04-28 00:46:12,626] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77000, global step 1227325: learning rate 0.0000
[2019-04-28 00:46:17,661] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.64097657e-17 9.99899268e-01 1.43011484e-25 1.00767225e-04
 4.57125132e-19], sum to 1.0000
[2019-04-28 00:46:17,667] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1145
[2019-04-28 00:46:17,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2738676.816718884 W.
[2019-04-28 00:46:17,684] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.88333333333333, 69.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.530708526484212, 6.9112, 168.9094790770794, 2738676.816718884, 2299185.194251404, 475451.5964941797], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6436200.0000, 
sim time next is 6436800.0000, 
raw observation next is [29.9, 69.0, 1.0, 2.0, 0.887705896380922, 1.0, 1.0, 0.887705896380922, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2482873.955371002, 2482873.955371002, 464820.9274937701], 
processed observation next is [1.0, 0.5217391304347826, 0.6161137440758293, 0.69, 1.0, 1.0, 0.8647058992541229, 1.0, 0.5, 0.8647058992541229, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6896872098252784, 0.6896872098252784, 0.6937625783489105], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.9167935], dtype=float32), 1.382861]. 
=============================================
[2019-04-28 00:46:18,183] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77000, global step 1230101: loss -218.4046
[2019-04-28 00:46:18,189] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77000, global step 1230102: learning rate 0.0000
[2019-04-28 00:46:18,840] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77000, global step 1230416: loss -201.4429
[2019-04-28 00:46:18,841] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77000, global step 1230416: learning rate 0.0000
[2019-04-28 00:46:19,768] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6723899e-17 9.9999917e-01 2.7653413e-25 8.1121902e-07 2.1896065e-21], sum to 1.0000
[2019-04-28 00:46:19,773] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0066
[2019-04-28 00:46:19,777] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.3, 85.33333333333334, 1.0, 2.0, 0.5297373591888676, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740240.133698212, 740240.1336982113, 188386.3710716541], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6308400.0000, 
sim time next is 6309000.0000, 
raw observation next is [27.3, 85.5, 1.0, 2.0, 0.5304310276095777, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741209.7846891206, 741209.78468912, 188501.2586362029], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.855, 1.0, 1.0, 0.43425425013202135, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20589160685808905, 0.20589160685808888, 0.2813451621435864], 
reward next is 0.7187, 
noisyNet noise sample is [array([-0.87137043], dtype=float32), 0.820257]. 
=============================================
[2019-04-28 00:46:19,801] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[69.57588]
 [69.34253]
 [69.30854]
 [70.13942]
 [70.13959]], R is [[70.03052521]
 [70.04904938]
 [70.06758881]
 [70.08612061]
 [70.10456848]].
[2019-04-28 00:46:20,038] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77000, global step 1230989: loss 250.3058
[2019-04-28 00:46:20,042] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77000, global step 1230989: learning rate 0.0000
[2019-04-28 00:46:20,202] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.3748621e-19 9.9999988e-01 2.2974777e-26 6.8723431e-08 6.8542256e-23], sum to 1.0000
[2019-04-28 00:46:20,210] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0509
[2019-04-28 00:46:20,216] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.11666666666667, 82.83333333333334, 1.0, 2.0, 0.5123660861901176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 715957.8233583921, 715957.8233583928, 185555.9964617115], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6393000.0000, 
sim time next is 6393600.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 0.5125969023207223, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 716280.4644052545, 716280.4644052538, 185593.0189704876], 
processed observation next is [1.0, 0.0, 0.4834123222748816, 0.83, 1.0, 1.0, 0.4127673521936413, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19896679566812625, 0.19896679566812606, 0.27700450592610093], 
reward next is 0.7230, 
noisyNet noise sample is [array([-0.18121138], dtype=float32), 1.4120481]. 
=============================================
[2019-04-28 00:46:20,755] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77000, global step 1231329: loss 261.5976
[2019-04-28 00:46:20,756] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77000, global step 1231329: learning rate 0.0000
[2019-04-28 00:46:21,127] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.5181679e-15 9.9999988e-01 1.6187093e-21 1.5768923e-07 8.9636538e-19], sum to 1.0000
[2019-04-28 00:46:21,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1894
[2019-04-28 00:46:21,139] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.1, 83.0, 1.0, 2.0, 0.5152703903697284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 720017.545643684, 720017.545643684, 186022.7732378798], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6397200.0000, 
sim time next is 6397800.0000, 
raw observation next is [27.08333333333334, 83.0, 1.0, 2.0, 0.5149003875599738, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719500.3439099116, 719500.3439099122, 185963.1328636322], 
processed observation next is [1.0, 0.043478260869565216, 0.4826224328594, 0.83, 1.0, 1.0, 0.4155426356144262, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19986120664164211, 0.19986120664164228, 0.2775569147218391], 
reward next is 0.7224, 
noisyNet noise sample is [array([0.22324431], dtype=float32), 0.6088065]. 
=============================================
[2019-04-28 00:46:21,252] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77000, global step 1231562: loss 145.1668
[2019-04-28 00:46:21,254] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77000, global step 1231563: learning rate 0.0000
[2019-04-28 00:46:21,489] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77000, global step 1231674: loss 280.6641
[2019-04-28 00:46:21,490] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77000, global step 1231674: learning rate 0.0000
[2019-04-28 00:46:21,992] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77000, global step 1231913: loss 277.3151
[2019-04-28 00:46:21,996] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77000, global step 1231915: learning rate 0.0000
[2019-04-28 00:46:22,793] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.3531982e-20 1.0000000e+00 4.3870889e-27 2.4097511e-09 3.0220914e-23], sum to 1.0000
[2019-04-28 00:46:22,802] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6549
[2019-04-28 00:46:22,803] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77000, global step 1232289: loss 126.3903
[2019-04-28 00:46:22,804] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77000, global step 1232289: learning rate 0.0000
[2019-04-28 00:46:22,806] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.06666666666667, 85.0, 1.0, 2.0, 0.5151451390202629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 719842.4652826025, 719842.465282603, 186003.2007643265], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6637200.0000, 
sim time next is 6637800.0000, 
raw observation next is [27.03333333333333, 85.0, 1.0, 2.0, 0.514325485959629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718696.72860822, 718696.7286082207, 185871.1614102765], 
processed observation next is [1.0, 0.8260869565217391, 0.48025276461295413, 0.85, 1.0, 1.0, 0.4148499830838904, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19963798016895, 0.1996379801689502, 0.27741964389593504], 
reward next is 0.7226, 
noisyNet noise sample is [array([0.11955578], dtype=float32), -1.3174987]. 
=============================================
[2019-04-28 00:46:23,759] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77000, global step 1232738: loss -46.5237
[2019-04-28 00:46:23,765] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77000, global step 1232738: learning rate 0.0000
[2019-04-28 00:46:23,874] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77000, global step 1232795: loss -32.4466
[2019-04-28 00:46:23,876] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77000, global step 1232795: learning rate 0.0000
[2019-04-28 00:46:24,541] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77000, global step 1233112: loss 2.9843
[2019-04-28 00:46:24,544] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77000, global step 1233113: learning rate 0.0000
[2019-04-28 00:46:24,881] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77000, global step 1233274: loss 30.2105
[2019-04-28 00:46:24,883] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77000, global step 1233274: learning rate 0.0000
[2019-04-28 00:46:24,888] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77000, global step 1233278: loss 63.7310
[2019-04-28 00:46:24,890] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77000, global step 1233279: learning rate 0.0000
[2019-04-28 00:46:25,989] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6364429e-19 1.0000000e+00 5.8762813e-29 1.3234464e-11 1.6281830e-23], sum to 1.0000
[2019-04-28 00:46:25,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1536
[2019-04-28 00:46:25,997] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.2, 87.0, 1.0, 2.0, 0.5307494053681528, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 741654.8323595145, 741654.8323595152, 188554.3217904331], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6476400.0000, 
sim time next is 6477000.0000, 
raw observation next is [27.15, 87.16666666666667, 1.0, 2.0, 0.5316882047698023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 742967.1439800152, 742967.1439800158, 188709.9675605736], 
processed observation next is [1.0, 1.0, 0.485781990521327, 0.8716666666666667, 1.0, 1.0, 0.4357689214094003, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2063797622166709, 0.20637976221667104, 0.2816566680008561], 
reward next is 0.7183, 
noisyNet noise sample is [array([-0.6358771], dtype=float32), -0.7925563]. 
=============================================
[2019-04-28 00:46:26,019] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[72.856094]
 [72.71436 ]
 [72.68667 ]
 [72.62857 ]
 [72.56874 ]], R is [[72.92340851]
 [72.91275024]
 [72.90254974]
 [72.8927536 ]
 [72.88332367]].
[2019-04-28 00:46:26,630] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.6850708e-17 1.0000000e+00 2.9973581e-25 1.3936510e-10 5.2624245e-21], sum to 1.0000
[2019-04-28 00:46:26,638] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0777
[2019-04-28 00:46:26,645] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 91.0, 1.0, 2.0, 0.5249302965196438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 733520.5598054687, 733520.5598054692, 187594.3296114696], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6487200.0000, 
sim time next is 6487800.0000, 
raw observation next is [26.38333333333333, 91.00000000000001, 1.0, 2.0, 1.002744314557542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1401643.011032975, 1401643.011032975, 299762.3359514407], 
processed observation next is [1.0, 0.08695652173913043, 0.44944707740916257, 0.9100000000000001, 1.0, 1.0, 1.003306403081376, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3893452808424931, 0.3893452808424931, 0.44740647156931446], 
reward next is 0.5526, 
noisyNet noise sample is [array([1.907738], dtype=float32), 0.120007366]. 
=============================================
[2019-04-28 00:46:27,695] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77000, global step 1234616: loss 126.8076
[2019-04-28 00:46:27,698] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77000, global step 1234619: learning rate 0.0000
[2019-04-28 00:46:28,099] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77000, global step 1234810: loss 227.4716
[2019-04-28 00:46:28,101] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77000, global step 1234810: learning rate 0.0000
[2019-04-28 00:46:29,363] A3C_AGENT_WORKER-Thread-18 INFO:Local step 77500, global step 1235413: loss 0.0745
[2019-04-28 00:46:29,367] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 77500, global step 1235413: learning rate 0.0000
[2019-04-28 00:46:34,847] A3C_AGENT_WORKER-Thread-2 INFO:Local step 77500, global step 1238085: loss 0.0918
[2019-04-28 00:46:34,850] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 77500, global step 1238085: learning rate 0.0000
[2019-04-28 00:46:35,263] A3C_AGENT_WORKER-Thread-20 INFO:Local step 77500, global step 1238306: loss 0.2019
[2019-04-28 00:46:35,270] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 77500, global step 1238307: learning rate 0.0000
[2019-04-28 00:46:36,465] A3C_AGENT_WORKER-Thread-9 INFO:Local step 77500, global step 1238895: loss 0.1125
[2019-04-28 00:46:36,466] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 77500, global step 1238895: learning rate 0.0000
[2019-04-28 00:46:37,409] A3C_AGENT_WORKER-Thread-21 INFO:Local step 77500, global step 1239360: loss 0.0818
[2019-04-28 00:46:37,410] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 77500, global step 1239360: learning rate 0.0000
[2019-04-28 00:46:37,826] A3C_AGENT_WORKER-Thread-14 INFO:Local step 77500, global step 1239546: loss 0.1471
[2019-04-28 00:46:37,830] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 77500, global step 1239548: learning rate 0.0000
[2019-04-28 00:46:37,938] A3C_AGENT_WORKER-Thread-10 INFO:Local step 77500, global step 1239601: loss 0.1885
[2019-04-28 00:46:37,940] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 77500, global step 1239601: learning rate 0.0000
[2019-04-28 00:46:38,723] A3C_AGENT_WORKER-Thread-13 INFO:Local step 77500, global step 1239987: loss 0.0768
[2019-04-28 00:46:38,729] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 77500, global step 1239987: learning rate 0.0000
[2019-04-28 00:46:39,173] A3C_AGENT_WORKER-Thread-16 INFO:Local step 77500, global step 1240211: loss 0.0008
[2019-04-28 00:46:39,176] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 77500, global step 1240211: learning rate 0.0000
[2019-04-28 00:46:40,329] A3C_AGENT_WORKER-Thread-19 INFO:Local step 77500, global step 1240777: loss 0.0115
[2019-04-28 00:46:40,330] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 77500, global step 1240777: learning rate 0.0000
[2019-04-28 00:46:40,387] A3C_AGENT_WORKER-Thread-12 INFO:Local step 77500, global step 1240802: loss 0.0164
[2019-04-28 00:46:40,391] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 77500, global step 1240802: learning rate 0.0000
[2019-04-28 00:46:40,429] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0518212e-20 1.0000000e+00 4.1965366e-26 8.0599399e-12 1.0399418e-24], sum to 1.0000
[2019-04-28 00:46:40,438] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8517
[2019-04-28 00:46:40,445] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 76.0, 1.0, 2.0, 0.3432735805155303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 533249.7136302579, 533249.7136302573, 169393.0204603326], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6739200.0000, 
sim time next is 6739800.0000, 
raw observation next is [23.96666666666667, 76.5, 1.0, 2.0, 0.3415297842794948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 531305.4473124315, 531305.4473124315, 169256.6529758127], 
processed observation next is [1.0, 0.0, 0.33491311216429714, 0.765, 1.0, 1.0, 0.20666239069818654, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14758484647567544, 0.14758484647567544, 0.2526218701131533], 
reward next is 0.7474, 
noisyNet noise sample is [array([0.79327834], dtype=float32), -1.0298386]. 
=============================================
[2019-04-28 00:46:41,079] A3C_AGENT_WORKER-Thread-3 INFO:Local step 77500, global step 1241141: loss 0.0240
[2019-04-28 00:46:41,080] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 77500, global step 1241141: learning rate 0.0000
[2019-04-28 00:46:41,285] A3C_AGENT_WORKER-Thread-22 INFO:Local step 77500, global step 1241243: loss 0.3290
[2019-04-28 00:46:41,287] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 77500, global step 1241243: learning rate 0.0000
[2019-04-28 00:46:41,721] A3C_AGENT_WORKER-Thread-11 INFO:Local step 77500, global step 1241454: loss 0.0621
[2019-04-28 00:46:41,723] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 77500, global step 1241454: learning rate 0.0000
[2019-04-28 00:46:44,357] A3C_AGENT_WORKER-Thread-15 INFO:Local step 77500, global step 1242751: loss 0.1091
[2019-04-28 00:46:44,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 77500, global step 1242751: learning rate 0.0000
[2019-04-28 00:46:44,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.5356771e-23 1.0000000e+00 1.7358918e-30 3.2997501e-13 7.3616076e-27], sum to 1.0000
[2019-04-28 00:46:44,496] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6538
[2019-04-28 00:46:44,501] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.7, 37.0, 1.0, 2.0, 0.2719459181547744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 440749.8059422675, 440749.8059422681, 162980.1806510437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6879600.0000, 
sim time next is 6880200.0000, 
raw observation next is [29.65, 38.0, 1.0, 2.0, 0.2775613735108938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 448367.320643843, 448367.320643843, 163490.4800756555], 
processed observation next is [0.0, 0.6521739130434783, 0.6042654028436019, 0.38, 1.0, 1.0, 0.1295920162781853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12454647795662305, 0.12454647795662305, 0.24401564190396344], 
reward next is 0.7560, 
noisyNet noise sample is [array([-0.00621314], dtype=float32), -0.31468982]. 
=============================================
[2019-04-28 00:46:44,684] A3C_AGENT_WORKER-Thread-17 INFO:Local step 77500, global step 1242908: loss 0.0031
[2019-04-28 00:46:44,687] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 77500, global step 1242909: learning rate 0.0000
[2019-04-28 00:46:45,518] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78000, global step 1243326: loss -60.3701
[2019-04-28 00:46:45,529] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78000, global step 1243327: learning rate 0.0000
[2019-04-28 00:46:47,810] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5709132e-21 1.0000000e+00 7.5815826e-31 3.0956173e-12 2.2786711e-24], sum to 1.0000
[2019-04-28 00:46:47,819] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5697
[2019-04-28 00:46:47,823] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.25, 61.5, 1.0, 2.0, 0.4478497822007853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 639732.0855265777, 639732.0855265782, 177625.4804319562], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6948600.0000, 
sim time next is 6949200.0000, 
raw observation next is [29.4, 61.0, 1.0, 2.0, 0.4492118888303161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 640416.241973249, 640416.2419732496, 177662.1532866675], 
processed observation next is [0.0, 0.43478260869565216, 0.5924170616113744, 0.61, 1.0, 1.0, 0.33639986606062183, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17789340054812472, 0.17789340054812489, 0.2651673929651754], 
reward next is 0.7348, 
noisyNet noise sample is [array([0.44649926], dtype=float32), 0.3146832]. 
=============================================
[2019-04-28 00:46:48,533] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2832233e-21 1.0000000e+00 1.6201892e-32 3.8871270e-14 3.8554563e-26], sum to 1.0000
[2019-04-28 00:46:48,538] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6639
[2019-04-28 00:46:48,544] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 92.0, 1.0, 2.0, 0.4145781653214795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610895.9947022655, 610895.9947022649, 175335.389138141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6930000.0000, 
sim time next is 6930600.0000, 
raw observation next is [23.8, 90.66666666666667, 1.0, 2.0, 0.4149436429970368, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611070.153209249, 611070.153209249, 175341.5404123613], 
processed observation next is [0.0, 0.21739130434782608, 0.3270142180094788, 0.9066666666666667, 1.0, 1.0, 0.2951128228879961, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1697417092247914, 0.1697417092247914, 0.2617037916602407], 
reward next is 0.7383, 
noisyNet noise sample is [array([0.6000967], dtype=float32), -0.36663502]. 
=============================================
[2019-04-28 00:46:50,893] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78000, global step 1246047: loss 23.0895
[2019-04-28 00:46:50,895] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78000, global step 1246048: learning rate 0.0000
[2019-04-28 00:46:51,298] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78000, global step 1246256: loss 40.8041
[2019-04-28 00:46:51,299] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78000, global step 1246256: learning rate 0.0000
[2019-04-28 00:46:52,437] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78000, global step 1246863: loss 23.2320
[2019-04-28 00:46:52,440] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78000, global step 1246866: learning rate 0.0000
[2019-04-28 00:46:53,477] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78000, global step 1247403: loss 85.0511
[2019-04-28 00:46:53,478] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78000, global step 1247403: learning rate 0.0000
[2019-04-28 00:46:53,601] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78000, global step 1247472: loss 20.3055
[2019-04-28 00:46:53,605] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78000, global step 1247473: learning rate 0.0000
[2019-04-28 00:46:53,798] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78000, global step 1247572: loss 35.4486
[2019-04-28 00:46:53,800] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78000, global step 1247572: learning rate 0.0000
[2019-04-28 00:46:54,539] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78000, global step 1247967: loss 70.8917
[2019-04-28 00:46:54,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78000, global step 1247968: learning rate 0.0000
[2019-04-28 00:46:54,960] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78000, global step 1248180: loss 34.0843
[2019-04-28 00:46:54,962] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78000, global step 1248181: learning rate 0.0000
[2019-04-28 00:46:56,208] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78000, global step 1248774: loss 69.0837
[2019-04-28 00:46:56,211] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78000, global step 1248774: learning rate 0.0000
[2019-04-28 00:46:56,341] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78000, global step 1248835: loss 0.1376
[2019-04-28 00:46:56,343] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78000, global step 1248835: learning rate 0.0000
[2019-04-28 00:46:56,990] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78000, global step 1249154: loss 92.3238
[2019-04-28 00:46:56,993] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78000, global step 1249155: learning rate 0.0000
[2019-04-28 00:46:57,224] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78000, global step 1249269: loss 79.6787
[2019-04-28 00:46:57,226] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78000, global step 1249269: learning rate 0.0000
[2019-04-28 00:46:57,471] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78000, global step 1249389: loss 74.0232
[2019-04-28 00:46:57,473] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78000, global step 1249389: learning rate 0.0000
[2019-04-28 00:46:58,723] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-28 00:46:58,725] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:46:58,726] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:46:58,730] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:46:58,731] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:46:58,732] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:46:58,732] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:46:58,734] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:46:58,734] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:46:58,735] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:46:58,737] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:46:58,754] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run51
[2019-04-28 00:46:58,754] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run51
[2019-04-28 00:46:58,776] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run51
[2019-04-28 00:46:58,802] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run51
[2019-04-28 00:46:58,856] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run51
[2019-04-28 00:47:01,747] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049689226]
[2019-04-28 00:47:01,748] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.73333333333333, 73.5, 1.0, 2.0, 0.2395320903824479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 396773.5016751452, 396773.5016751452, 159750.7947765286]
[2019-04-28 00:47:01,749] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:47:01,751] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.8010765e-20 1.0000000e+00 1.0403682e-28 1.6406250e-11 2.6996067e-24], sampled 0.1385656375748865
[2019-04-28 00:48:10,887] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049689226]
[2019-04-28 00:48:10,888] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.43333333333334, 56.16666666666667, 1.0, 2.0, 0.5052075931848571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705951.5363531626, 705951.5363531626, 184415.183190326]
[2019-04-28 00:48:10,889] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:48:10,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9603639e-21 1.0000000e+00 6.2649850e-30 5.5451442e-12 2.5236434e-25], sampled 0.7815764034957009
[2019-04-28 00:48:22,488] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:48:22,709] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:48:22,757] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:48:22,896] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:48:22,923] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:48:23,933] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1250000, evaluation results [1250000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:48:24,788] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.33786259e-21 1.00000000e+00 1.25340085e-30 2.62381089e-12
 2.24602258e-25], sum to 1.0000
[2019-04-28 00:48:24,795] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3572
[2019-04-28 00:48:24,800] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 86.0, 1.0, 2.0, 0.4720728632724427, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660374.3298207412, 660374.3298207412, 179431.4498755454], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7171200.0000, 
sim time next is 7171800.0000, 
raw observation next is [25.71666666666667, 86.0, 1.0, 2.0, 0.473376425093435, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 661640.8689527683, 661640.8689527677, 179553.2254955652], 
processed observation next is [1.0, 0.0, 0.41785150078988953, 0.86, 1.0, 1.0, 0.36551376517281325, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18378913026465785, 0.18378913026465768, 0.26798988879935104], 
reward next is 0.7320, 
noisyNet noise sample is [array([0.15889044], dtype=float32), -0.7203956]. 
=============================================
[2019-04-28 00:48:25,635] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78000, global step 1250830: loss 83.4262
[2019-04-28 00:48:25,636] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78000, global step 1250830: learning rate 0.0000
[2019-04-28 00:48:25,722] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78000, global step 1250873: loss 24.4693
[2019-04-28 00:48:25,723] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78000, global step 1250873: learning rate 0.0000
[2019-04-28 00:48:26,716] A3C_AGENT_WORKER-Thread-18 INFO:Local step 78500, global step 1251341: loss 0.0096
[2019-04-28 00:48:26,717] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 78500, global step 1251341: learning rate 0.0000
[2019-04-28 00:48:31,828] A3C_AGENT_WORKER-Thread-2 INFO:Local step 78500, global step 1253835: loss 0.0554
[2019-04-28 00:48:31,833] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 78500, global step 1253839: learning rate 0.0000
[2019-04-28 00:48:32,457] A3C_AGENT_WORKER-Thread-20 INFO:Local step 78500, global step 1254139: loss 0.1496
[2019-04-28 00:48:32,458] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 78500, global step 1254140: learning rate 0.0000
[2019-04-28 00:48:33,605] A3C_AGENT_WORKER-Thread-9 INFO:Local step 78500, global step 1254700: loss 0.1435
[2019-04-28 00:48:33,607] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 78500, global step 1254700: learning rate 0.0000
[2019-04-28 00:48:34,706] A3C_AGENT_WORKER-Thread-21 INFO:Local step 78500, global step 1255251: loss 0.0263
[2019-04-28 00:48:34,713] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 78500, global step 1255251: learning rate 0.0000
[2019-04-28 00:48:35,170] A3C_AGENT_WORKER-Thread-10 INFO:Local step 78500, global step 1255483: loss 0.0319
[2019-04-28 00:48:35,172] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 78500, global step 1255483: learning rate 0.0000
[2019-04-28 00:48:35,194] A3C_AGENT_WORKER-Thread-14 INFO:Local step 78500, global step 1255491: loss 0.0170
[2019-04-28 00:48:35,196] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 78500, global step 1255492: learning rate 0.0000
[2019-04-28 00:48:35,940] A3C_AGENT_WORKER-Thread-13 INFO:Local step 78500, global step 1255862: loss 0.0123
[2019-04-28 00:48:35,947] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 78500, global step 1255862: learning rate 0.0000
[2019-04-28 00:48:36,181] A3C_AGENT_WORKER-Thread-16 INFO:Local step 78500, global step 1255981: loss 0.0301
[2019-04-28 00:48:36,183] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 78500, global step 1255981: learning rate 0.0000
[2019-04-28 00:48:37,959] A3C_AGENT_WORKER-Thread-19 INFO:Local step 78500, global step 1256848: loss 0.0274
[2019-04-28 00:48:37,960] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 78500, global step 1256848: learning rate 0.0000
[2019-04-28 00:48:38,174] A3C_AGENT_WORKER-Thread-12 INFO:Local step 78500, global step 1256954: loss 0.1782
[2019-04-28 00:48:38,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 78500, global step 1256955: learning rate 0.0000
[2019-04-28 00:48:38,785] A3C_AGENT_WORKER-Thread-3 INFO:Local step 78500, global step 1257244: loss 0.0290
[2019-04-28 00:48:38,786] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 78500, global step 1257244: learning rate 0.0000
[2019-04-28 00:48:38,990] A3C_AGENT_WORKER-Thread-22 INFO:Local step 78500, global step 1257351: loss 0.0253
[2019-04-28 00:48:38,991] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 78500, global step 1257352: learning rate 0.0000
[2019-04-28 00:48:39,050] A3C_AGENT_WORKER-Thread-11 INFO:Local step 78500, global step 1257383: loss 0.0221
[2019-04-28 00:48:39,053] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 78500, global step 1257384: learning rate 0.0000
[2019-04-28 00:48:40,661] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.8188519e-24 1.0000000e+00 3.1772977e-33 3.9421043e-13 1.5294406e-26], sum to 1.0000
[2019-04-28 00:48:40,668] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4789
[2019-04-28 00:48:40,671] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 92.5, 1.0, 2.0, 0.3999967981806018, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633311.3583937364, 633311.3583937371, 178143.4362224409], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7389000.0000, 
sim time next is 7389600.0000, 
raw observation next is [21.06666666666667, 92.33333333333333, 1.0, 2.0, 0.4111758867973876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651858.8488127897, 651858.8488127897, 179845.8991403281], 
processed observation next is [1.0, 0.5217391304347826, 0.19747235387045833, 0.9233333333333333, 1.0, 1.0, 0.290573357587214, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18107190244799712, 0.18107190244799712, 0.2684267151348181], 
reward next is 0.7316, 
noisyNet noise sample is [array([0.8830296], dtype=float32), -0.33203542]. 
=============================================
[2019-04-28 00:48:42,308] A3C_AGENT_WORKER-Thread-17 INFO:Local step 78500, global step 1258965: loss 0.0293
[2019-04-28 00:48:42,309] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 78500, global step 1258966: learning rate 0.0000
[2019-04-28 00:48:42,425] A3C_AGENT_WORKER-Thread-15 INFO:Local step 78500, global step 1259024: loss 0.0340
[2019-04-28 00:48:42,428] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 78500, global step 1259024: learning rate 0.0000
[2019-04-28 00:48:43,369] A3C_AGENT_WORKER-Thread-18 INFO:Local step 79000, global step 1259481: loss 41.5959
[2019-04-28 00:48:43,375] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 79000, global step 1259482: learning rate 0.0000
[2019-04-28 00:48:46,991] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1376278e-23 1.0000000e+00 3.5814738e-33 4.2328174e-14 3.8399526e-28], sum to 1.0000
[2019-04-28 00:48:46,999] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7372
[2019-04-28 00:48:47,003] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.96666666666667, 82.33333333333334, 1.0, 2.0, 0.4056674306009919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 596655.12751652, 596655.12751652, 173970.7512253905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7500000.0000, 
sim time next is 7500600.0000, 
raw observation next is [24.85, 83.0, 1.0, 2.0, 0.4047198760001976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595707.356162778, 595707.356162778, 173897.1060322049], 
processed observation next is [0.0, 0.8260869565217391, 0.37677725118483424, 0.83, 1.0, 1.0, 0.2827950313255393, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16547426560077166, 0.16547426560077166, 0.2595479194510521], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.633551], dtype=float32), -0.852083]. 
=============================================
[2019-04-28 00:48:47,359] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2936641e-19 1.0000000e+00 6.8969123e-27 3.9111173e-11 9.8926218e-24], sum to 1.0000
[2019-04-28 00:48:47,365] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6166
[2019-04-28 00:48:47,374] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.1, 88.0, 1.0, 2.0, 0.4038394038054958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 595894.2537914377, 595894.2537914377, 173960.5580025301], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7507200.0000, 
sim time next is 7507800.0000, 
raw observation next is [24.05, 88.5, 1.0, 2.0, 0.4036417151594379, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 595228.1077983001, 595228.1077982995, 173887.0774089706], 
processed observation next is [0.0, 0.9130434782608695, 0.3388625592417062, 0.885, 1.0, 1.0, 0.2814960423607686, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16534114105508335, 0.1653411410550832, 0.25953295135667254], 
reward next is 0.7405, 
noisyNet noise sample is [array([-0.7296351], dtype=float32), -1.5829052]. 
=============================================
[2019-04-28 00:48:48,079] A3C_AGENT_WORKER-Thread-2 INFO:Local step 79000, global step 1261753: loss -17.6040
[2019-04-28 00:48:48,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 79000, global step 1261754: learning rate 0.0000
[2019-04-28 00:48:49,017] A3C_AGENT_WORKER-Thread-20 INFO:Local step 79000, global step 1262204: loss 114.1040
[2019-04-28 00:48:49,025] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 79000, global step 1262207: learning rate 0.0000
[2019-04-28 00:48:50,089] A3C_AGENT_WORKER-Thread-9 INFO:Local step 79000, global step 1262726: loss 102.7940
[2019-04-28 00:48:50,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 79000, global step 1262727: learning rate 0.0000
[2019-04-28 00:48:51,349] A3C_AGENT_WORKER-Thread-21 INFO:Local step 79000, global step 1263296: loss 22.3934
[2019-04-28 00:48:51,355] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 79000, global step 1263296: learning rate 0.0000
[2019-04-28 00:48:51,459] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0027690e-19 1.0000000e+00 6.8513704e-25 7.0302189e-12 4.0152886e-24], sum to 1.0000
[2019-04-28 00:48:51,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5409
[2019-04-28 00:48:51,470] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.55, 86.5, 1.0, 2.0, 0.591526285397432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 826615.8435479895, 826615.8435479889, 199171.7784399833], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7799400.0000, 
sim time next is 7800000.0000, 
raw observation next is [26.66666666666666, 86.0, 1.0, 2.0, 0.6004791451257872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839131.7693257811, 839131.7693257811, 200828.6099888635], 
processed observation next is [1.0, 0.2608695652173913, 0.4628751974723536, 0.86, 1.0, 1.0, 0.5186495724407074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23309215814605033, 0.23309215814605033, 0.2997441940132291], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.6711872], dtype=float32), -0.7616724]. 
=============================================
[2019-04-28 00:48:51,493] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.506832]
 [62.57911 ]
 [62.551826]
 [62.829964]
 [63.2202  ]], R is [[62.57250977]
 [62.64951324]
 [62.72340775]
 [62.78061295]
 [62.84579086]].
[2019-04-28 00:48:51,514] A3C_AGENT_WORKER-Thread-10 INFO:Local step 79000, global step 1263377: loss 39.6999
[2019-04-28 00:48:51,515] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 79000, global step 1263377: learning rate 0.0000
[2019-04-28 00:48:51,539] A3C_AGENT_WORKER-Thread-14 INFO:Local step 79000, global step 1263386: loss -18.0402
[2019-04-28 00:48:51,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 79000, global step 1263387: learning rate 0.0000
[2019-04-28 00:48:51,766] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.3839654e-19 1.0000000e+00 7.0002561e-28 1.8959075e-12 1.5020015e-24], sum to 1.0000
[2019-04-28 00:48:51,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1403
[2019-04-28 00:48:51,777] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666666, 82.5, 1.0, 2.0, 0.7792158234416188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1089033.080385263, 1089033.080385263, 238872.0986803748], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7804200.0000, 
sim time next is 7804800.0000, 
raw observation next is [27.7, 82.0, 1.0, 2.0, 0.7750000565356212, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1083138.113945953, 1083138.113945952, 237866.0188770856], 
processed observation next is [1.0, 0.34782608695652173, 0.5118483412322274, 0.82, 1.0, 1.0, 0.7289157307658086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008716983183203, 0.30087169831832, 0.35502390877176954], 
reward next is 0.6450, 
noisyNet noise sample is [array([-0.5471058], dtype=float32), -1.1161706]. 
=============================================
[2019-04-28 00:48:52,267] A3C_AGENT_WORKER-Thread-13 INFO:Local step 79000, global step 1263728: loss 14.9795
[2019-04-28 00:48:52,268] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 79000, global step 1263728: learning rate 0.0000
[2019-04-28 00:48:52,937] A3C_AGENT_WORKER-Thread-16 INFO:Local step 79000, global step 1264065: loss 36.8005
[2019-04-28 00:48:52,939] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 79000, global step 1264065: learning rate 0.0000
[2019-04-28 00:48:54,641] A3C_AGENT_WORKER-Thread-19 INFO:Local step 79000, global step 1264888: loss 46.1548
[2019-04-28 00:48:54,644] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 79000, global step 1264890: learning rate 0.0000
[2019-04-28 00:48:55,090] A3C_AGENT_WORKER-Thread-3 INFO:Local step 79000, global step 1265113: loss 22.1253
[2019-04-28 00:48:55,094] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 79000, global step 1265114: learning rate 0.0000
[2019-04-28 00:48:55,174] A3C_AGENT_WORKER-Thread-12 INFO:Local step 79000, global step 1265151: loss 1.1538
[2019-04-28 00:48:55,176] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 79000, global step 1265151: learning rate 0.0000
[2019-04-28 00:48:55,320] A3C_AGENT_WORKER-Thread-22 INFO:Local step 79000, global step 1265229: loss 33.9570
[2019-04-28 00:48:55,322] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 79000, global step 1265229: learning rate 0.0000
[2019-04-28 00:48:55,754] A3C_AGENT_WORKER-Thread-11 INFO:Local step 79000, global step 1265430: loss -37.0898
[2019-04-28 00:48:55,755] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 79000, global step 1265430: learning rate 0.0000
[2019-04-28 00:48:58,834] A3C_AGENT_WORKER-Thread-17 INFO:Local step 79000, global step 1266928: loss 36.7638
[2019-04-28 00:48:58,835] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 79000, global step 1266928: learning rate 0.0000
[2019-04-28 00:48:58,871] A3C_AGENT_WORKER-Thread-15 INFO:Local step 79000, global step 1266945: loss -14.8731
[2019-04-28 00:48:58,875] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 79000, global step 1266946: learning rate 0.0000
[2019-04-28 00:49:00,921] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:00,921] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:00,971] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run7
[2019-04-28 00:49:05,251] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:05,252] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:05,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run7
[2019-04-28 00:49:05,798] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [9.668514e-16 1.000000e+00 8.915583e-24 5.915140e-08 2.120634e-18], sum to 1.0000
[2019-04-28 00:49:05,802] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2756
[2019-04-28 00:49:05,805] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2118766.226997483 W.
[2019-04-28 00:49:05,811] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.46666666666667, 66.0, 1.0, 2.0, 0.757640875470315, 1.0, 1.0, 0.757640875470315, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2118766.226997483, 2118766.226997482, 399771.8016054934], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 7917600.0000, 
sim time next is 7918200.0000, 
raw observation next is [30.5, 65.5, 1.0, 2.0, 0.893616997216604, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.982693346267097, 6.9112, 168.9125309205629, 2146102.102677781, 2095382.390017568, 433128.1469439633], 
processed observation next is [1.0, 0.6521739130434783, 0.6445497630331753, 0.655, 1.0, 1.0, 0.8718277074898844, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.007149334626709703, 0.0, 0.8294378553112144, 0.5961394729660503, 0.5820506638937689, 0.6464599208118855], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.4159997], dtype=float32), -0.03908943]. 
=============================================
[2019-04-28 00:49:06,056] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:06,057] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:06,099] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run7
[2019-04-28 00:49:07,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:07,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:07,098] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run7
[2019-04-28 00:49:08,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:08,265] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:08,275] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:08,276] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:08,289] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run7
[2019-04-28 00:49:08,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:08,320] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:08,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run7
[2019-04-28 00:49:08,410] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run7
[2019-04-28 00:49:09,065] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:09,066] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:09,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run7
[2019-04-28 00:49:09,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:09,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:09,205] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run7
[2019-04-28 00:49:11,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:11,119] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:11,184] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run7
[2019-04-28 00:49:11,316] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:11,317] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:11,327] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run7
[2019-04-28 00:49:11,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:11,635] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:11,645] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run7
[2019-04-28 00:49:11,681] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:11,685] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:11,704] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:11,705] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:11,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run7
[2019-04-28 00:49:11,742] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run7
[2019-04-28 00:49:14,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:14,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:14,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run7
[2019-04-28 00:49:14,284] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 00:49:14,285] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:14,360] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run7
[2019-04-28 00:49:15,893] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-28 00:49:15,894] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:49:15,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:15,895] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:49:15,896] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:49:15,896] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:49:15,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:15,897] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:15,897] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:15,898] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:49:15,900] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:49:15,918] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run52
[2019-04-28 00:49:15,942] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run52
[2019-04-28 00:49:15,967] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run52
[2019-04-28 00:49:15,988] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run52
[2019-04-28 00:49:16,014] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run52
[2019-04-28 00:49:56,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:49:56,208] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.13333333333333, 61.0, 1.0, 2.0, 0.5293135415661455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739647.6966441431, 739647.6966441431, 188315.34380458]
[2019-04-28 00:49:56,209] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:49:56,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9398457e-21 1.0000000e+00 2.6629207e-30 3.0761885e-08 6.8123563e-25], sampled 0.8182403969438606
[2019-04-28 00:49:59,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:49:59,418] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.33333333333334, 73.33333333333333, 1.0, 2.0, 0.572311613424695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799754.5994621207, 799754.5994621207, 195700.5502394962]
[2019-04-28 00:49:59,419] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:49:59,421] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.7523302e-21 1.0000000e+00 2.1846508e-30 2.9319789e-08 5.8145271e-25], sampled 0.6435761018062475
[2019-04-28 00:50:03,083] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:50:03,084] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.27904355, 86.54234606, 1.0, 2.0, 0.5347661267661424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747269.6642747081, 747269.6642747081, 189222.4172230997]
[2019-04-28 00:50:03,084] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:50:03,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2414349e-20 1.0000000e+00 3.8102113e-30 3.3557907e-08 8.9812199e-25], sampled 0.3648321782092634
[2019-04-28 00:50:09,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:50:09,803] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.0, 89.0, 1.0, 2.0, 0.5080662834223602, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8823431142172143, 6.911199999999999, 6.9112, 168.9129537178607, 1420369.723529943, 1420369.723529944, 312585.4042529984]
[2019-04-28 00:50:09,804] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:50:09,814] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4565943e-17 9.9999952e-01 1.3499121e-25 4.8076862e-07 4.7269135e-21], sampled 0.30736526280073284
[2019-04-28 00:50:15,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:50:15,000] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.33867334666667, 84.96846096666667, 1.0, 2.0, 0.9508941791984222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1329121.159308507, 1329121.159308507, 284322.9943046583]
[2019-04-28 00:50:15,001] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:50:15,003] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.1879660e-20 1.0000000e+00 1.5398588e-29 4.7906831e-08 2.8209516e-24], sampled 0.09785149708999041
[2019-04-28 00:50:53,191] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:50:53,398] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.043864556]
[2019-04-28 00:50:53,398] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.35, 73.0, 1.0, 2.0, 0.3406587973831531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532193.1222646368, 532193.1222646368, 169383.6385613716]
[2019-04-28 00:50:53,398] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:50:53,400] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4208214e-20 1.0000000e+00 1.0232268e-29 4.3213895e-08 2.0206088e-24], sampled 0.5743135171628657
[2019-04-28 00:50:53,445] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:50:53,495] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:50:53,760] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:50:53,794] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:50:54,809] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1275000, evaluation results [1275000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:50:59,572] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.9708476e-18 1.0000000e+00 3.4258319e-26 5.8526528e-08 2.5219523e-23], sum to 1.0000
[2019-04-28 00:50:59,581] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8761
[2019-04-28 00:50:59,586] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.16666666666667, 86.0, 1.0, 2.0, 0.3201727374327979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504177.9168911105, 504177.9168911105, 167295.3436498964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 223800.0000, 
sim time next is 224400.0000, 
raw observation next is [22.13333333333333, 86.0, 1.0, 2.0, 0.3189720881451341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 502668.9189887386, 502668.9189887393, 167189.863854938], 
processed observation next is [0.0, 0.6086956521739131, 0.24802527646129527, 0.86, 1.0, 1.0, 0.17948444354835436, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13963025527464962, 0.1396302552746498, 0.24953711023125077], 
reward next is 0.7505, 
noisyNet noise sample is [array([1.1415131], dtype=float32), -0.45718277]. 
=============================================
[2019-04-28 00:51:09,884] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.05430364e-21 1.00000000e+00 6.13003729e-32 6.61318866e-09
 1.16931704e-26], sum to 1.0000
[2019-04-28 00:51:09,892] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8750
[2019-04-28 00:51:09,900] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333333, 88.66666666666666, 1.0, 2.0, 0.3008194031433185, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479127.6619617537, 479127.6619617537, 165560.8652732381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 240000.0000, 
sim time next is 240600.0000, 
raw observation next is [21.31666666666667, 88.83333333333334, 1.0, 2.0, 0.3006929564937792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 478882.1058054178, 478882.1058054178, 165542.645426857], 
processed observation next is [0.0, 0.782608695652174, 0.20932069510268583, 0.8883333333333334, 1.0, 1.0, 0.15746139336599901, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1330228071681716, 0.1330228071681716, 0.24707857526396565], 
reward next is 0.7529, 
noisyNet noise sample is [array([0.9987827], dtype=float32), -0.7584257]. 
=============================================
[2019-04-28 00:51:10,286] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5010580e-17 9.9999905e-01 2.0145018e-26 9.5907922e-07 1.3279366e-21], sum to 1.0000
[2019-04-28 00:51:10,294] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4622
[2019-04-28 00:51:10,301] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 77.0, 1.0, 2.0, 0.3131017586489281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 493328.0498188729, 493328.0498188722, 166489.0870010211], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 302400.0000, 
sim time next is 303000.0000, 
raw observation next is [23.43333333333333, 76.83333333333334, 1.0, 2.0, 0.3138066617509504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 494246.5703175248, 494246.5703175254, 166552.6161048481], 
processed observation next is [0.0, 0.5217391304347826, 0.30963665086887826, 0.7683333333333334, 1.0, 1.0, 0.1732610382541571, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13729071397709022, 0.1372907139770904, 0.24858599418634045], 
reward next is 0.7514, 
noisyNet noise sample is [array([-1.0627751], dtype=float32), 1.1052966]. 
=============================================
[2019-04-28 00:51:10,322] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[70.49217 ]
 [70.47148 ]
 [70.471436]
 [70.47017 ]
 [70.47846 ]], R is [[70.51717377]
 [70.56351471]
 [70.60969543]
 [70.65566254]
 [70.7012558 ]].
[2019-04-28 00:51:12,337] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.94771712e-17 9.99999881e-01 1.34008202e-25 1.07923874e-07
 5.12452671e-20], sum to 1.0000
[2019-04-28 00:51:12,344] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7560
[2019-04-28 00:51:12,346] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 86.0, 1.0, 2.0, 0.2767291524129772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 446506.1808600551, 446506.1808600551, 163368.3651714692], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 336600.0000, 
sim time next is 337200.0000, 
raw observation next is [20.96666666666667, 86.0, 1.0, 2.0, 0.275679208189076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 445079.7360947166, 445079.736094716, 163273.9126578792], 
processed observation next is [0.0, 0.9130434782608695, 0.1927330173775673, 0.86, 1.0, 1.0, 0.1273243472157542, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12363326002631016, 0.12363326002631, 0.24369240695205852], 
reward next is 0.7563, 
noisyNet noise sample is [array([1.395899], dtype=float32), 0.33955458]. 
=============================================
[2019-04-28 00:51:13,933] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.5587280e-19 9.9999988e-01 7.7719035e-28 8.0756969e-08 5.9407797e-24], sum to 1.0000
[2019-04-28 00:51:13,938] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-28 00:51:13,942] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.3, 87.0, 1.0, 2.0, 0.2569084511794936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 418598.7393006994, 418598.7393006994, 161538.0374686603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 367200.0000, 
sim time next is 367800.0000, 
raw observation next is [20.36666666666667, 86.5, 1.0, 2.0, 0.2609633783394137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 425208.3495388778, 425208.3495388784, 161948.2201520966], 
processed observation next is [1.0, 0.2608695652173913, 0.1642969984202214, 0.865, 1.0, 1.0, 0.10959443173423335, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11811343042746605, 0.11811343042746623, 0.24171376142103967], 
reward next is 0.7583, 
noisyNet noise sample is [array([-1.3528004], dtype=float32), 1.510659]. 
=============================================
[2019-04-28 00:51:16,220] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.9100586e-22 1.0000000e+00 3.4564085e-31 4.8166809e-10 1.2215239e-24], sum to 1.0000
[2019-04-28 00:51:16,237] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1350
[2019-04-28 00:51:16,242] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.2, 81.0, 1.0, 2.0, 0.2383487008260658, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 392901.3570151539, 392901.3570151546, 159759.5334614988], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 457200.0000, 
sim time next is 457800.0000, 
raw observation next is [20.23333333333333, 80.83333333333333, 1.0, 2.0, 0.2636482956071631, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 434520.2358408161, 434520.2358408161, 162264.0863842669], 
processed observation next is [1.0, 0.30434782608695654, 0.15797788309636643, 0.8083333333333332, 1.0, 1.0, 0.11282927181585911, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12070006551133781, 0.12070006551133781, 0.24218520355860731], 
reward next is 0.7578, 
noisyNet noise sample is [array([-1.2153889], dtype=float32), 0.42550594]. 
=============================================
[2019-04-28 00:51:16,646] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.6168682e-16 9.9999988e-01 4.6301926e-26 1.4164128e-07 9.3338406e-22], sum to 1.0000
[2019-04-28 00:51:16,651] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7688
[2019-04-28 00:51:16,656] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.65, 86.0, 1.0, 2.0, 0.2672657958880256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 433896.570959082, 433896.570959082, 162528.8583907534], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 343800.0000, 
sim time next is 344400.0000, 
raw observation next is [20.63333333333333, 86.0, 1.0, 2.0, 0.26683578064081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 433317.0466089039, 433317.0466089046, 162490.0351588493], 
processed observation next is [0.0, 1.0, 0.17693522906793036, 0.86, 1.0, 1.0, 0.11666961522989154, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12036584628025108, 0.12036584628025128, 0.24252244053559596], 
reward next is 0.7575, 
noisyNet noise sample is [array([1.0649337], dtype=float32), 1.7288823]. 
=============================================
[2019-04-28 00:51:27,973] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.2953727e-21 1.0000000e+00 4.6443027e-29 2.8835114e-09 3.9357807e-23], sum to 1.0000
[2019-04-28 00:51:27,982] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1729
[2019-04-28 00:51:27,991] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 61.33333333333333, 1.0, 2.0, 0.2504187298447559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412273.4695163005, 412273.4695163011, 160939.509386387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 670200.0000, 
sim time next is 670800.0000, 
raw observation next is [22.96666666666667, 62.66666666666667, 1.0, 2.0, 0.2497542569032178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411159.580403137, 411159.5804031377, 160874.469992949], 
processed observation next is [1.0, 0.782608695652174, 0.2875197472353872, 0.6266666666666667, 1.0, 1.0, 0.09608946614845515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11421099455642696, 0.11421099455642715, 0.24011114924320745], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.8798264], dtype=float32), 1.6395254]. 
=============================================
[2019-04-28 00:51:30,747] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.556347e-19 1.000000e+00 7.895826e-29 8.398654e-10 1.245826e-23], sum to 1.0000
[2019-04-28 00:51:30,758] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7978
[2019-04-28 00:51:30,765] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.83333333333333, 88.33333333333333, 1.0, 2.0, 0.2123993107496134, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 354389.3453061864, 354389.3453061858, 156746.184946145], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 607200.0000, 
sim time next is 607800.0000, 
raw observation next is [17.76666666666667, 88.66666666666667, 1.0, 2.0, 0.2114011963153425, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 352776.7222180887, 352776.722218088, 156639.0532893801], 
processed observation next is [1.0, 0.0, 0.04107424960505548, 0.8866666666666667, 1.0, 1.0, 0.049880959416075274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09799353394946908, 0.09799353394946889, 0.23378963177519418], 
reward next is 0.7662, 
noisyNet noise sample is [array([-0.8407597], dtype=float32), -0.087132886]. 
=============================================
[2019-04-28 00:51:56,852] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 00:51:56,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:51:56,855] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:51:56,856] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:51:56,857] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:51:56,857] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:51:56,857] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:51:56,858] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:51:56,859] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:51:56,859] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:51:56,859] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:51:56,866] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run53
[2019-04-28 00:51:56,872] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run53
[2019-04-28 00:51:56,934] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run53
[2019-04-28 00:51:56,941] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run53
[2019-04-28 00:51:57,032] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run53
[2019-04-28 00:52:04,241] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:04,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.53333333333333, 75.66666666666667, 1.0, 2.0, 0.2664992904213163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 435657.8535437597, 435657.8535437597, 162560.2011485076]
[2019-04-28 00:52:04,242] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:52:04,244] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4193610e-18 1.0000000e+00 1.0720928e-26 1.2378368e-08 3.5483125e-22], sampled 0.2511446471270772
[2019-04-28 00:52:11,190] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:11,191] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.14756975, 87.901841935, 1.0, 2.0, 0.2905044366734638, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 473161.2967110304, 473161.2967110304, 165102.7380216917]
[2019-04-28 00:52:11,192] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:52:11,194] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1486533e-18 1.0000000e+00 7.8685925e-27 1.1262466e-08 2.7476611e-22], sampled 0.5167923088922876
[2019-04-28 00:52:15,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:15,464] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.7, 47.0, 1.0, 2.0, 0.2989478298222476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480462.0571958406, 480462.0571958406, 165693.8829653929]
[2019-04-28 00:52:15,465] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:52:15,468] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2361695e-19 1.0000000e+00 5.7228034e-27 1.0220811e-08 2.1114760e-22], sampled 0.01323420882358517
[2019-04-28 00:52:19,494] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:19,495] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.3, 86.33333333333334, 1.0, 2.0, 0.6870285752447873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 975677.0719355877, 975677.071935587, 220142.5759964593]
[2019-04-28 00:52:19,501] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:52:19,506] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0737917e-18 1.0000000e+00 7.1300268e-27 1.0928539e-08 2.5326454e-22], sampled 0.9328997106404497
[2019-04-28 00:52:26,468] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:26,471] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.01171934, 59.55624837666667, 1.0, 2.0, 0.7714808544330295, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1078217.188975956, 1078217.188975957, 237033.1415432686]
[2019-04-28 00:52:26,472] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:52:26,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.1202894e-19 1.0000000e+00 2.4175184e-27 7.8613285e-09 1.0353038e-22], sampled 0.4995643651668762
[2019-04-28 00:52:44,161] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:52:44,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [37.51666666666667, 44.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.391137885596201, 6.9112, 168.9101613151846, 1794466.949675181, 1453988.129830425, 311354.5510313082]
[2019-04-28 00:52:44,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:52:44,165] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.1304519e-18 1.0000000e+00 5.0983640e-26 1.9893630e-08 1.2883047e-21], sampled 0.08224530720459733
[2019-04-28 00:52:44,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1794466.949675181 W.
[2019-04-28 00:53:03,129] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:53:03,130] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.79268632, 80.31353179, 1.0, 2.0, 0.6884174669290257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 962075.6924160682, 962075.6924160676, 218377.0229513592]
[2019-04-28 00:53:03,130] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:53:03,131] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1040739e-19 1.0000000e+00 6.5981190e-28 5.2938969e-09 3.5369185e-23], sampled 0.73019684799777
[2019-04-28 00:53:17,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:53:17,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.26666666666667, 87.0, 1.0, 2.0, 0.5833285459014076, 0.0, 2.0, 0.0, 1.0, 2.0, 0.9961689864289005, 6.911200000000001, 6.9112, 168.9129565034625, 1630937.458876089, 1630937.458876089, 353401.4521879779]
[2019-04-28 00:53:17,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:53:17,554] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9769667e-16 9.9999988e-01 1.4478982e-23 1.1112908e-07 1.3771297e-19], sampled 0.7811233895103512
[2019-04-28 00:53:20,852] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:53:20,852] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.70364200333334, 59.7523133, 1.0, 2.0, 0.4383220537933121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 640895.3758615173, 640895.3758615173, 178116.1223638536]
[2019-04-28 00:53:20,853] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:53:20,856] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.4866507e-19 1.0000000e+00 2.6745713e-27 8.1069924e-09 1.1255879e-22], sampled 0.9278930792880917
[2019-04-28 00:53:27,369] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.034891073]
[2019-04-28 00:53:27,370] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.68333333333333, 90.83333333333334, 1.0, 2.0, 0.563201918617114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 903653.6607290354, 903653.6607290361, 207022.7656497492]
[2019-04-28 00:53:27,372] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:53:27,375] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9599197e-18 1.0000000e+00 1.7169286e-26 1.4283357e-08 5.2384873e-22], sampled 0.7267729897493571
[2019-04-28 00:53:33,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:53:33,345] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:53:33,570] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:53:33,776] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:53:33,801] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:53:34,817] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1300000, evaluation results [1300000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:53:35,053] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.2969612e-18 9.9999988e-01 2.1247223e-26 7.0113110e-08 4.9076965e-23], sum to 1.0000
[2019-04-28 00:53:35,064] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5410
[2019-04-28 00:53:35,068] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.0, 1.0, 2.0, 0.3374410147626812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 522754.490826634, 522754.4908266347, 168511.450275072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 948000.0000, 
sim time next is 948600.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3374390906288617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 522751.8247870384, 522751.824787039, 168511.2479659854], 
processed observation next is [0.0, 1.0, 0.23222748815165886, 0.94, 1.0, 1.0, 0.20173384413115864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14520884021862177, 0.14520884021862196, 0.2515093253223663], 
reward next is 0.7485, 
noisyNet noise sample is [array([-0.2783603], dtype=float32), 2.226543]. 
=============================================
[2019-04-28 00:53:35,702] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.0567938e-17 1.0000000e+00 1.8881389e-26 4.4244423e-08 6.1140481e-22], sum to 1.0000
[2019-04-28 00:53:35,709] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5271
[2019-04-28 00:53:35,723] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 97.66666666666667, 1.0, 2.0, 0.3574286508168133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548152.3398437325, 548152.3398437331, 170420.9135442379], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1012800.0000, 
sim time next is 1013400.0000, 
raw observation next is [21.7, 97.5, 1.0, 2.0, 0.3553740872370029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 545365.865114062, 545365.8651140614, 170199.1461675087], 
processed observation next is [1.0, 0.7391304347826086, 0.2274881516587678, 0.975, 1.0, 1.0, 0.2233422737795216, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15149051808723943, 0.1514905180872393, 0.254028576369416], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.651981], dtype=float32), -0.9900939]. 
=============================================
[2019-04-28 00:53:44,955] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4215788e-20 1.0000000e+00 8.6524675e-30 3.4094867e-09 1.2568754e-25], sum to 1.0000
[2019-04-28 00:53:44,965] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0501
[2019-04-28 00:53:44,972] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.51666666666667, 83.33333333333333, 1.0, 2.0, 0.4578460485082757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681742.7895175884, 681742.7895175884, 182498.1328286929], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1239000.0000, 
sim time next is 1239600.0000, 
raw observation next is [24.73333333333333, 82.66666666666667, 1.0, 2.0, 0.7353700167537872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1090409.627690335, 1090409.627690334, 236977.5782757195], 
processed observation next is [1.0, 0.34782608695652173, 0.3712480252764612, 0.8266666666666667, 1.0, 1.0, 0.6811686948840809, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3028915632473153, 0.30289156324731503, 0.353697878023462], 
reward next is 0.6463, 
noisyNet noise sample is [array([0.21336716], dtype=float32), -0.7799788]. 
=============================================
[2019-04-28 00:53:48,403] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1555826e-18 1.0000000e+00 3.5087057e-26 9.9574538e-10 1.5633815e-20], sum to 1.0000
[2019-04-28 00:53:48,411] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2178
[2019-04-28 00:53:48,416] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.5, 92.0, 1.0, 2.0, 0.4858640249733721, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691015.7770398007, 691015.7770398012, 182960.9799245575], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1310400.0000, 
sim time next is 1311000.0000, 
raw observation next is [24.51666666666667, 91.83333333333334, 1.0, 2.0, 0.5596482083041264, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795960.9681756579, 795960.9681756579, 195251.3883863686], 
processed observation next is [1.0, 0.17391304347826086, 0.36097946287519767, 0.9183333333333334, 1.0, 1.0, 0.469455672655574, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22110026893768275, 0.22110026893768275, 0.2914199826662218], 
reward next is 0.7086, 
noisyNet noise sample is [array([0.89823854], dtype=float32), 0.5199158]. 
=============================================
[2019-04-28 00:53:48,440] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[68.746765]
 [68.79621 ]
 [68.87439 ]
 [68.97154 ]
 [69.04329 ]], R is [[68.65406036]
 [68.69444275]
 [68.73498535]
 [68.77416229]
 [68.80976105]].
[2019-04-28 00:53:51,712] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.2070057e-19 1.0000000e+00 7.6049119e-28 1.0303129e-09 4.2769741e-25], sum to 1.0000
[2019-04-28 00:53:51,719] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7406
[2019-04-28 00:53:51,724] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 95.83333333333333, 1.0, 2.0, 0.3149105760068865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 498500.6409171044, 498500.6409171037, 166923.4013324424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1378200.0000, 
sim time next is 1378800.0000, 
raw observation next is [20.7, 96.0, 1.0, 2.0, 0.315274935364724, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499237.9888956547, 499237.9888956547, 166981.3637735872], 
processed observation next is [1.0, 1.0, 0.18009478672985785, 0.96, 1.0, 1.0, 0.1750300426081012, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13867721913768186, 0.13867721913768186, 0.24922591607998087], 
reward next is 0.7508, 
noisyNet noise sample is [array([0.28741184], dtype=float32), 0.22031818]. 
=============================================
[2019-04-28 00:53:57,829] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1825177e-20 1.0000000e+00 3.6149570e-30 4.2282933e-09 8.1984483e-26], sum to 1.0000
[2019-04-28 00:53:57,836] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2873
[2019-04-28 00:53:57,841] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.7, 98.0, 1.0, 2.0, 0.4674243400664169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 665774.3025583858, 665774.3025583858, 180271.479460674], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1670400.0000, 
sim time next is 1671000.0000, 
raw observation next is [23.81666666666667, 97.33333333333334, 1.0, 2.0, 0.5338018664416964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 759378.0177450682, 759378.0177450682, 190781.882326112], 
processed observation next is [1.0, 0.34782608695652173, 0.3278041074249607, 0.9733333333333334, 1.0, 1.0, 0.4383155017369835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21093833826251893, 0.21093833826251893, 0.28474907809867467], 
reward next is 0.7153, 
noisyNet noise sample is [array([-0.37039587], dtype=float32), 1.6240405]. 
=============================================
[2019-04-28 00:53:57,856] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[77.44823 ]
 [77.3303  ]
 [77.30157 ]
 [77.207504]
 [77.10702 ]], R is [[77.28949738]
 [77.24754333]
 [77.20675659]
 [77.16942596]
 [77.13561249]].
[2019-04-28 00:54:04,932] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.7929070e-18 1.0000000e+00 5.0394880e-25 4.7814946e-10 4.9525162e-22], sum to 1.0000
[2019-04-28 00:54:04,941] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3085
[2019-04-28 00:54:04,950] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.61666666666667, 90.0, 1.0, 2.0, 0.3226006728800868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 508495.0533680868, 508495.0533680868, 167634.0193741018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1569000.0000, 
sim time next is 1569600.0000, 
raw observation next is [21.6, 90.0, 1.0, 2.0, 0.3197523380591949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 504207.7298161962, 504207.7298161956, 167312.8241175561], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.9, 1.0, 1.0, 0.18042450368577698, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14005770272672116, 0.140057702726721, 0.24972063301127775], 
reward next is 0.7503, 
noisyNet noise sample is [array([-1.0346023], dtype=float32), 0.7210341]. 
=============================================
[2019-04-28 00:54:05,780] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.50698745e-18 1.00000000e+00 2.38990033e-27 2.20140864e-10
 1.05270724e-22], sum to 1.0000
[2019-04-28 00:54:05,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8551
[2019-04-28 00:54:05,793] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.4, 85.0, 1.0, 2.0, 0.3990236293460754, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 610780.2515295085, 610780.2515295091, 175897.4022419333], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1584000.0000, 
sim time next is 1584600.0000, 
raw observation next is [23.41666666666667, 85.00000000000001, 1.0, 2.0, 0.3914057450573952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598482.06138325, 598482.0613832506, 174773.1832725219], 
processed observation next is [1.0, 0.34782608695652173, 0.3088467614533968, 0.8500000000000001, 1.0, 1.0, 0.2667539097077051, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1662450170509028, 0.16624501705090294, 0.2608554974216745], 
reward next is 0.7391, 
noisyNet noise sample is [array([-1.1752012], dtype=float32), 0.3404342]. 
=============================================
[2019-04-28 00:54:10,573] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.9729372e-19 1.0000000e+00 2.1391346e-25 9.1741637e-10 2.0048023e-22], sum to 1.0000
[2019-04-28 00:54:10,582] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3228
[2019-04-28 00:54:10,587] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.21666666666667, 87.00000000000001, 1.0, 2.0, 0.8346412022571453, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1228578.621944959, 1228578.621944959, 261279.0360663521], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1764600.0000, 
sim time next is 1765200.0000, 
raw observation next is [24.13333333333334, 86.0, 1.0, 2.0, 0.8258029945387197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1225044.500419657, 1225044.500419656, 260161.9662070169], 
processed observation next is [1.0, 0.43478260869565216, 0.3428120063191157, 0.86, 1.0, 1.0, 0.7901240898056864, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3402901390054603, 0.34029013900546, 0.3883014421000252], 
reward next is 0.6117, 
noisyNet noise sample is [array([-0.42637938], dtype=float32), 1.2717752]. 
=============================================
[2019-04-28 00:54:11,111] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8880352e-18 1.0000000e+00 1.6674594e-26 3.7457296e-10 2.7802060e-24], sum to 1.0000
[2019-04-28 00:54:11,123] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6430
[2019-04-28 00:54:11,126] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.43333333333334, 94.0, 1.0, 2.0, 0.4719187475649988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664982.1040135298, 664982.1040135304, 180030.1480528139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1740000.0000, 
sim time next is 1740600.0000, 
raw observation next is [24.4, 94.0, 1.0, 2.0, 0.4600946989538006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 649294.4116061226, 649294.4116061226, 178406.6916801751], 
processed observation next is [1.0, 0.13043478260869565, 0.3554502369668246, 0.94, 1.0, 1.0, 0.34951168548650674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1803595587794785, 0.1803595587794785, 0.2662786442987688], 
reward next is 0.7337, 
noisyNet noise sample is [array([0.09958111], dtype=float32), 0.56727767]. 
=============================================
[2019-04-28 00:54:13,354] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.1035763e-20 1.0000000e+00 1.2833663e-27 3.7068369e-11 3.0741925e-24], sum to 1.0000
[2019-04-28 00:54:13,366] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3121
[2019-04-28 00:54:13,372] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.56666666666667, 98.33333333333334, 1.0, 2.0, 0.4781503930941028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683549.6838074777, 683549.6838074777, 182215.226587118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1665600.0000, 
sim time next is 1666200.0000, 
raw observation next is [23.58333333333333, 98.16666666666667, 1.0, 2.0, 0.4773386055273855, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 682501.2686907761, 682501.2686907767, 182104.4983785113], 
processed observation next is [1.0, 0.2608695652173913, 0.31674565560821466, 0.9816666666666667, 1.0, 1.0, 0.3702874765390186, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1895836857474378, 0.18958368574743797, 0.2717977587738975], 
reward next is 0.7282, 
noisyNet noise sample is [array([0.40997207], dtype=float32), 0.6959106]. 
=============================================
[2019-04-28 00:54:20,689] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.0896202e-18 1.0000000e+00 3.5875560e-27 3.3116720e-10 3.5314869e-22], sum to 1.0000
[2019-04-28 00:54:20,698] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8732
[2019-04-28 00:54:20,703] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 95.0, 1.0, 2.0, 0.4625432617026303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652647.5764196299, 652647.5764196299, 178752.9072299544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1904400.0000, 
sim time next is 1905000.0000, 
raw observation next is [24.28333333333333, 95.16666666666667, 1.0, 2.0, 0.463167910437884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 653043.0560201956, 653043.0560201956, 178782.3635346329], 
processed observation next is [1.0, 0.043478260869565216, 0.34992101105845175, 0.9516666666666667, 1.0, 1.0, 0.3532143499251615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18140084889449878, 0.18140084889449878, 0.2668393485591536], 
reward next is 0.7332, 
noisyNet noise sample is [array([1.0788183], dtype=float32), -1.4066966]. 
=============================================
[2019-04-28 00:54:20,734] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[67.783714]
 [67.81396 ]
 [67.97962 ]
 [68.385826]
 [68.80355 ]], R is [[67.72410583]
 [67.78007507]
 [67.83562469]
 [67.89087677]
 [67.94594574]].
[2019-04-28 00:54:23,799] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.1476718e-21 1.0000000e+00 5.1990434e-32 5.3676542e-12 2.3810917e-27], sum to 1.0000
[2019-04-28 00:54:23,807] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9834
[2019-04-28 00:54:23,813] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.38333333333333, 92.16666666666667, 1.0, 2.0, 0.4517948544353128, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 645023.1309467133, 645023.1309467133, 178154.6848170711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1897800.0000, 
sim time next is 1898400.0000, 
raw observation next is [24.36666666666667, 92.33333333333334, 1.0, 2.0, 0.4523419070209481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 645666.4144399094, 645666.41443991, 178216.9511501752], 
processed observation next is [1.0, 1.0, 0.3538704581358612, 0.9233333333333335, 1.0, 1.0, 0.3401709723143953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1793517817888637, 0.17935178178886388, 0.26599544947787346], 
reward next is 0.7340, 
noisyNet noise sample is [array([0.6532531], dtype=float32), 0.6325042]. 
=============================================
[2019-04-28 00:54:25,882] A3C_AGENT_WORKER-Thread-21 INFO:Evaluating...
[2019-04-28 00:54:25,884] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:54:25,884] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:54:25,885] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:54:25,885] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:54:25,886] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:54:25,886] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:54:25,887] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:54:25,888] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:54:25,888] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:54:25,889] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:54:25,909] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run54
[2019-04-28 00:54:25,933] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run54
[2019-04-28 00:54:25,934] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run54
[2019-04-28 00:54:25,957] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run54
[2019-04-28 00:54:25,994] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run54
[2019-04-28 00:55:04,484] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.030545093]
[2019-04-28 00:55:04,486] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.16666666666667, 66.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.664182782538736, 6.9112, 168.9088572653211, 2818349.505882908, 2284170.568258773, 474399.9723966279]
[2019-04-28 00:55:04,487] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:55:04,488] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7553288e-18 1.0000000e+00 1.8053977e-25 1.7028470e-10 1.8406164e-21], sampled 0.833237593695242
[2019-04-28 00:55:04,489] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2818349.505882908 W.
[2019-04-28 00:55:09,358] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.030545093]
[2019-04-28 00:55:09,359] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 72.33333333333333, 1.0, 2.0, 0.619145371748943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 865227.269599746, 865227.269599746, 204367.1399305151]
[2019-04-28 00:55:09,360] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:55:09,364] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.9824035e-21 1.0000000e+00 3.3194880e-29 5.7000789e-12 1.3523874e-24], sampled 0.04909008068141163
[2019-04-28 00:55:40,703] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.030545093]
[2019-04-28 00:55:40,704] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.825579685, 42.47135551166667, 1.0, 2.0, 0.6290895732018704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 925738.8901578087, 925738.8901578081, 212246.9834230983]
[2019-04-28 00:55:40,705] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:55:40,708] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8749602e-20 1.0000000e+00 2.6658506e-28 1.2981333e-11 7.7671190e-24], sampled 0.3549572592407171
[2019-04-28 00:55:47,911] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.030545093]
[2019-04-28 00:55:47,913] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.8, 61.5, 1.0, 2.0, 0.6012065268644007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840148.6414902016, 840148.6414902016, 200971.3982028326]
[2019-04-28 00:55:47,913] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:55:47,916] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.1402694e-21 1.0000000e+00 1.7514298e-30 1.7831477e-12 1.1456210e-25], sampled 0.23051038838493132
[2019-04-28 00:55:50,018] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:55:50,056] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:55:50,114] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:55:50,409] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:55:50,756] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:55:51,770] A3C_AGENT_WORKER-Thread-21 INFO:Global step: 1325000, evaluation results [1325000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:55:54,410] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.4589700e-22 1.0000000e+00 1.8022484e-30 3.4561585e-12 1.3177469e-25], sum to 1.0000
[2019-04-28 00:55:54,419] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.5256
[2019-04-28 00:55:54,425] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.1, 83.0, 1.0, 2.0, 0.5417498416376717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757032.0214540289, 757032.0214540289, 190396.1012867672], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2237400.0000, 
sim time next is 2238000.0000, 
raw observation next is [28.0, 83.33333333333334, 1.0, 2.0, 0.5399945089657583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 754578.278169787, 754578.2781697877, 190099.8050241836], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.8333333333333335, 1.0, 1.0, 0.4457765168262148, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2096050772693853, 0.2096050772693855, 0.28373105227490086], 
reward next is 0.7163, 
noisyNet noise sample is [array([2.7907252], dtype=float32), -1.042475]. 
=============================================
[2019-04-28 00:55:54,442] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[80.04546 ]
 [79.928535]
 [80.41499 ]
 [80.77288 ]
 [80.691284]], R is [[79.79442596]
 [79.71231079]
 [79.63062286]
 [79.54944611]
 [79.46876526]].
[2019-04-28 00:55:58,908] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.66361841e-21 1.00000000e+00 1.13047160e-30 1.33604915e-11
 3.80759296e-24], sum to 1.0000
[2019-04-28 00:55:58,915] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1527
[2019-04-28 00:55:58,920] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
processed observation next is [0.0, 0.9130434782608695, 0.4715639810426541, 0.9, 1.0, 1.0, 0.4436370423865538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20891555011729224, 0.2089155501172924, 0.28328456911258376], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.35139483], dtype=float32), -1.4813563]. 
=============================================
[2019-04-28 00:55:58,933] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[71.08965]
 [71.07659]
 [71.08442]
 [71.05859]
 [71.05743]], R is [[71.11561584]
 [71.12094879]
 [71.12666321]
 [71.13198853]
 [71.13683319]].
[2019-04-28 00:56:07,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.1041695e-11 9.9986744e-01 1.9236790e-17 1.3255618e-04 6.9254369e-14], sum to 1.0000
[2019-04-28 00:56:07,920] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3654
[2019-04-28 00:56:07,928] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1971526.588685908 W.
[2019-04-28 00:56:07,932] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.7050384643546334, 1.0, 2.0, 0.7050384643546334, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1971526.588685908, 1971526.588685908, 376291.5383538485], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2292000.0000, 
sim time next is 2292600.0000, 
raw observation next is [31.91666666666666, 63.83333333333334, 1.0, 2.0, 0.8547929212088347, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.999506302864901, 6.9112, 168.9124312439286, 2091760.994236131, 2029113.658529624, 422072.0743778642], 
processed observation next is [1.0, 0.5217391304347826, 0.7116903633491308, 0.6383333333333334, 1.0, 1.0, 0.8250517122998008, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008830630286490138, 0.0, 0.8294373658533079, 0.5810447206211475, 0.5636426829248956, 0.6299583199669615], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.82810354], dtype=float32), 0.17748652]. 
=============================================
[2019-04-28 00:56:12,724] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.2354581e-16 9.9978215e-01 4.8263397e-23 2.1787717e-04 1.7921875e-18], sum to 1.0000
[2019-04-28 00:56:12,733] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6685
[2019-04-28 00:56:12,737] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.93333333333334, 61.66666666666667, 1.0, 2.0, 0.5309042377494474, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565007688, 741871.2664859605, 741871.266485961, 188583.322129664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2395200.0000, 
sim time next is 2395800.0000, 
raw observation next is [32.8, 62.5, 1.0, 2.0, 0.5263079505121329, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104286, 735446.3152094106, 735446.3152094106, 187824.6164211081], 
processed observation next is [1.0, 0.7391304347826086, 0.7535545023696681, 0.625, 1.0, 1.0, 0.4292866873640155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451522908, 0.20429064311372516, 0.20429064311372516, 0.2803352483897136], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.03913448], dtype=float32), -1.8374658]. 
=============================================
[2019-04-28 00:56:12,850] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [4.56737729e-13 9.99891281e-01 2.32805024e-20 1.08720524e-04
 1.92717082e-16], sum to 1.0000
[2019-04-28 00:56:12,859] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-28 00:56:12,863] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.85, 81.0, 1.0, 2.0, 0.5239434131993869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732141.0458341854, 732141.0458341854, 187432.570382807], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2339400.0000, 
sim time next is 2340000.0000, 
raw observation next is [27.8, 81.0, 1.0, 2.0, 0.5220923383277316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729553.5273032588, 729553.5273032588, 187129.9057731547], 
processed observation next is [1.0, 0.08695652173913043, 0.5165876777251186, 0.81, 1.0, 1.0, 0.42420763653943566, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20265375758423854, 0.20265375758423854, 0.279298366825604], 
reward next is 0.7207, 
noisyNet noise sample is [array([0.04564137], dtype=float32), -1.2129428]. 
=============================================
[2019-04-28 00:56:12,887] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[54.444912]
 [54.396782]
 [54.677185]
 [54.880783]
 [54.88552 ]], R is [[54.8829689 ]
 [55.05438995]
 [55.223629  ]
 [55.39067841]
 [55.55551529]].
[2019-04-28 00:56:30,032] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2858069e-18 9.9997270e-01 4.4869961e-27 2.7297270e-05 5.1213335e-22], sum to 1.0000
[2019-04-28 00:56:30,038] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8244
[2019-04-28 00:56:30,045] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4770567381368315, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 666730.5403852839, 666730.5403852839, 180095.5526591972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2629800.0000, 
sim time next is 2630400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4769526541853535, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 666584.9123931888, 666584.9123931894, 180079.9414579404], 
processed observation next is [0.0, 0.43478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3698224749221126, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18516247566477467, 0.18516247566477484, 0.26877603202677675], 
reward next is 0.7312, 
noisyNet noise sample is [array([-0.7603905], dtype=float32), -0.8243714]. 
=============================================
[2019-04-28 00:56:32,180] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4677828e-19 9.9999487e-01 5.2152705e-30 5.0954782e-06 1.6305257e-21], sum to 1.0000
[2019-04-28 00:56:32,186] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6290
[2019-04-28 00:56:32,191] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 88.0, 1.0, 2.0, 0.7977978938658373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1173371.202606536, 1173371.202606536, 251394.3311992045], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2821800.0000, 
sim time next is 2822400.0000, 
raw observation next is [24.0, 89.0, 1.0, 2.0, 0.7982621086786734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1175395.883493164, 1175395.883493165, 251692.2016356019], 
processed observation next is [1.0, 0.6956521739130435, 0.3364928909952607, 0.89, 1.0, 1.0, 0.7569422996128595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3264988565258789, 0.3264988565258792, 0.37566000244119685], 
reward next is 0.6243, 
noisyNet noise sample is [array([-1.563873], dtype=float32), 0.18869565]. 
=============================================
[2019-04-28 00:56:39,660] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.3883758e-16 9.9994564e-01 9.0218977e-26 5.4354172e-05 4.1552376e-21], sum to 1.0000
[2019-04-28 00:56:39,664] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0509
[2019-04-28 00:56:39,669] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 98.0, 1.0, 2.0, 0.3202592044005328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501052.8149874106, 501052.8149874106, 166975.7383874612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3040800.0000, 
sim time next is 3041400.0000, 
raw observation next is [21.0, 99.0, 1.0, 2.0, 0.3242680589624113, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505771.4355117322, 505771.4355117328, 167290.6144457537], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.99, 1.0, 1.0, 0.18586513128001358, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1404920654199256, 0.1404920654199258, 0.24968748424739357], 
reward next is 0.7503, 
noisyNet noise sample is [array([0.52157485], dtype=float32), 0.58816975]. 
=============================================
[2019-04-28 00:56:42,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.0197810e-16 9.9997544e-01 9.0254020e-26 2.4573948e-05 3.2357929e-20], sum to 1.0000
[2019-04-28 00:56:42,392] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2200
[2019-04-28 00:56:42,398] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.1, 93.33333333333334, 1.0, 2.0, 0.5583344680340759, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859560.8324298095, 859560.8324298095, 202779.7095041777], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2884800.0000, 
sim time next is 2885400.0000, 
raw observation next is [22.15, 93.0, 1.0, 2.0, 0.5873530096808306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903994.8043460599, 903994.8043460599, 208475.8809406458], 
processed observation next is [1.0, 0.391304347826087, 0.24881516587677724, 0.93, 1.0, 1.0, 0.5028349514226874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25110966787390554, 0.25110966787390554, 0.3111580312546952], 
reward next is 0.6888, 
noisyNet noise sample is [array([-0.57317734], dtype=float32), 0.68689585]. 
=============================================
[2019-04-28 00:56:45,269] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.8820006e-17 9.9999774e-01 2.7566649e-28 2.2978666e-06 2.1166323e-22], sum to 1.0000
[2019-04-28 00:56:45,287] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6935
[2019-04-28 00:56:45,291] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.00000000000001, 1.0, 2.0, 0.3450129291095888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531487.0762492359, 531487.0762492366, 169121.4220597318], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2873400.0000, 
sim time next is 2874000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3445782197527655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 530816.2143129116, 530816.2143129123, 169067.0264720253], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.94, 1.0, 1.0, 0.2103352045214042, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14744894842025322, 0.1474489484202534, 0.25233884548063473], 
reward next is 0.7477, 
noisyNet noise sample is [array([-0.43904418], dtype=float32), 1.5205066]. 
=============================================
[2019-04-28 00:56:45,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[78.60261 ]
 [78.63521 ]
 [78.60991 ]
 [78.6104  ]
 [78.595314]], R is [[78.57095337]
 [78.53282928]
 [78.49576569]
 [78.45883179]
 [78.4209671 ]].
[2019-04-28 00:56:45,889] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-28 00:56:45,893] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 00:56:45,893] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 00:56:45,895] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 00:56:45,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:56:45,897] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:56:45,898] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:56:45,894] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 00:56:45,896] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 00:56:45,913] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:56:45,913] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 00:56:45,920] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run55
[2019-04-28 00:56:45,965] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run55
[2019-04-28 00:56:45,995] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run55
[2019-04-28 00:56:45,995] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run55
[2019-04-28 00:56:46,058] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run55
[2019-04-28 00:56:53,244] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:56:53,246] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 72.5, 1.0, 2.0, 0.6708647531984812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 994194.0624699829, 994194.0624699822, 221997.7922173454]
[2019-04-28 00:56:53,247] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 00:56:53,249] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.46378698e-18 9.99993563e-01 1.31418295e-27 6.45429736e-06
 1.12622491e-22], sampled 0.8671627303033991
[2019-04-28 00:57:31,788] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:57:31,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.87076989, 67.60234623000001, 1.0, 2.0, 0.5839221673883909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 815985.5485923479, 815985.5485923486, 197788.7925136395]
[2019-04-28 00:57:31,791] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:57:31,793] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3036295e-19 9.9999619e-01 8.0118192e-29 3.7663663e-06 1.1442552e-23], sampled 0.2867726746851311
[2019-04-28 00:57:34,711] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:57:34,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.4, 70.0, 1.0, 2.0, 0.5534149910195432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 773338.6368223235, 773338.6368223235, 192387.8272857706]
[2019-04-28 00:57:34,713] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:57:34,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.6831239e-19 9.9999535e-01 2.3434400e-28 4.6311907e-06 2.7515487e-23], sampled 0.9135652507147837
[2019-04-28 00:57:38,280] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:57:38,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.25746409, 74.707151295, 1.0, 2.0, 0.6210865133864808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 867941.034420091, 867941.0344200903, 204741.9854652794]
[2019-04-28 00:57:38,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:57:38,286] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0142674e-19 9.9999702e-01 2.3149482e-29 2.9665032e-06 4.1493780e-24], sampled 0.7617822898057874
[2019-04-28 00:57:57,478] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:57:57,478] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.7, 51.66666666666666, 1.0, 2.0, 0.5995612297320693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 837848.5344671881, 837848.5344671874, 200663.8172139398]
[2019-04-28 00:57:57,479] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:57:57,484] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.2406493e-19 9.9999523e-01 2.7784060e-28 4.7858066e-06 3.1625459e-23], sampled 0.2686331793230551
[2019-04-28 00:58:00,009] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:58:00,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.1, 74.0, 1.0, 2.0, 0.9116636823601187, 1.0, 1.0, 0.9116636823601187, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2549951.193509882, 2549951.193509883, 477900.1916675373]
[2019-04-28 00:58:00,010] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 00:58:00,012] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3667114e-16 9.9997163e-01 2.8828356e-24 2.8354094e-05 6.0571451e-20], sampled 0.3535833021051308
[2019-04-28 00:58:00,012] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2549951.193509882 W.
[2019-04-28 00:58:39,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:58:39,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.14982354, 83.45859894333333, 1.0, 2.0, 0.8805937688952661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104154, 1230801.151117273, 1230801.151117273, 264651.3595690264]
[2019-04-28 00:58:39,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 00:58:39,648] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6927852e-18 9.9999225e-01 3.3050614e-27 7.7077584e-06 2.3931084e-22], sampled 0.9893434689818681
[2019-04-28 00:58:56,915] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:58:56,915] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.113863515, 79.389297145, 1.0, 2.0, 0.4939963259338814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 690280.3885482937, 690280.3885482944, 182660.4312593943]
[2019-04-28 00:58:56,915] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:58:56,934] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.6771346e-19 9.9999464e-01 4.9502832e-28 5.3481481e-06 5.0704540e-23], sampled 0.2148876985469651
[2019-04-28 00:59:00,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:59:00,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.93333333333334, 68.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.990436331899878, 6.9112, 168.9124549009827, 1510006.237345029, 1453793.423323088, 311356.9523488975]
[2019-04-28 00:59:00,843] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:59:00,873] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.779844e-19 9.999944e-01 6.063269e-28 5.561044e-06 5.983577e-23], sampled 0.7576043055017777
[2019-04-28 00:59:18,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:59:18,172] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.1, 86.0, 1.0, 2.0, 0.5326349405478313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744290.5511980874, 744290.551198088, 188866.6870567744]
[2019-04-28 00:59:18,173] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 00:59:18,197] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.3936496e-19 9.9999487e-01 3.7534141e-28 5.0713456e-06 4.0443393e-23], sampled 0.3325360907129292
[2019-04-28 00:59:43,153] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 00:59:44,630] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 00:59:46,153] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 00:59:46,402] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 00:59:46,477] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.029956913]
[2019-04-28 00:59:46,478] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.5138762, 70.12691286333333, 1.0, 2.0, 0.941840497162242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1316458.445126501, 1316458.445126502, 281707.5052466026]
[2019-04-28 00:59:46,478] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 00:59:46,480] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0980926e-18 9.9999404e-01 8.5057473e-28 5.9357990e-06 7.8914067e-23], sampled 0.6412602631134315
[2019-04-28 00:59:46,877] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 00:59:47,893] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1350000, evaluation results [1350000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 00:59:53,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.4229278e-17 9.9998784e-01 7.0941088e-25 1.2106251e-05 5.3890832e-22], sum to 1.0000
[2019-04-28 00:59:53,272] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7485
[2019-04-28 00:59:53,289] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.5432639263750915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 837069.2685101344, 837069.2685101344, 199991.506046124], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2970000.0000, 
sim time next is 2970600.0000, 
raw observation next is [22.0, 93.00000000000001, 1.0, 2.0, 0.540854919852412, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 835031.0591947987, 835031.0591947987, 199714.5929616853], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9300000000000002, 1.0, 1.0, 0.4468131564486891, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2319530719985552, 0.2319530719985552, 0.2980814820323661], 
reward next is 0.7019, 
noisyNet noise sample is [array([-0.79112244], dtype=float32), 0.025094118]. 
=============================================
[2019-04-28 00:59:56,071] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.9097681e-17 9.9995685e-01 2.8427652e-27 4.3095384e-05 1.2529122e-22], sum to 1.0000
[2019-04-28 00:59:56,081] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4110
[2019-04-28 00:59:56,089] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3064534176698839, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 488008.0487519372, 488008.0487519379, 166199.4117093935], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3008400.0000, 
sim time next is 3009000.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3060499230171704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487365.5165384243, 487365.5165384243, 166152.7471790351], 
processed observation next is [1.0, 0.8260869565217391, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16391556990020525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1353793101495623, 0.1353793101495623, 0.24798917489408223], 
reward next is 0.7520, 
noisyNet noise sample is [array([-0.44913247], dtype=float32), 0.47293824]. 
=============================================
[2019-04-28 00:59:56,105] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[75.56598 ]
 [75.528694]
 [75.4936  ]
 [75.4823  ]
 [75.41444 ]], R is [[75.59526825]
 [75.59125519]
 [75.58731842]
 [75.58335114]
 [75.57925415]].
[2019-04-28 01:00:04,545] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.4568190e-18 9.9999809e-01 8.6035882e-27 1.9112770e-06 1.2361951e-22], sum to 1.0000
[2019-04-28 01:00:04,551] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8339
[2019-04-28 01:00:04,556] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4294383130842323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623325.327861791, 623325.3278617903, 176266.7906144356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3088200.0000, 
sim time next is 3088800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4318133549108204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 626771.4657089156, 626771.4657089162, 176603.617787309], 
processed observation next is [1.0, 0.782608695652174, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3154377770009884, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17410318491914323, 0.1741031849191434, 0.26358748923478953], 
reward next is 0.7364, 
noisyNet noise sample is [array([-1.2133418], dtype=float32), 1.6841357]. 
=============================================
[2019-04-28 01:00:18,018] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8299860e-17 9.9997818e-01 6.3466724e-25 2.1762196e-05 8.3124462e-21], sum to 1.0000
[2019-04-28 01:00:18,026] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9676
[2019-04-28 01:00:18,029] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 83.16666666666666, 1.0, 2.0, 0.4809847450995685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 672093.0460930732, 672093.0460930738, 180669.9039054583], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3286200.0000, 
sim time next is 3286800.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4792880274897444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 669721.429146401, 669721.4291464004, 180414.0429977915], 
processed observation next is [0.0, 0.043478260869565216, 0.4312796208530806, 0.84, 1.0, 1.0, 0.3726361776984872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18603373031844472, 0.18603373031844456, 0.26927469104147983], 
reward next is 0.7307, 
noisyNet noise sample is [array([-0.63233525], dtype=float32), -1.6804326]. 
=============================================
[2019-04-28 01:00:21,230] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1998749e-17 9.9999893e-01 3.3573136e-25 1.0688011e-06 5.0900202e-19], sum to 1.0000
[2019-04-28 01:00:21,236] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0838
[2019-04-28 01:00:21,243] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 69.0, 1.0, 2.0, 0.5652948924024637, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789945.7095237701, 789945.7095237701, 194459.1426914459], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3324000.0000, 
sim time next is 3324600.0000, 
raw observation next is [31.5, 68.5, 1.0, 2.0, 0.5677779483641817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793416.8394003384, 793416.8394003378, 194897.1476478967], 
processed observation next is [0.0, 0.4782608695652174, 0.6919431279620853, 0.685, 1.0, 1.0, 0.47925054019780927, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22039356650009398, 0.22039356650009384, 0.29089126514611446], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.7694218], dtype=float32), -1.3875587]. 
=============================================
[2019-04-28 01:00:21,430] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0798588e-17 9.9999952e-01 6.1980489e-28 4.3571484e-07 1.7395780e-21], sum to 1.0000
[2019-04-28 01:00:21,435] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5868
[2019-04-28 01:00:21,439] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.5758696029024122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 804728.4569005421, 804728.4569005414, 196337.1040093072], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3327600.0000, 
sim time next is 3328200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5736982499282367, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 801693.0319605313, 801693.0319605318, 195948.9383480781], 
processed observation next is [0.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.4863834336484779, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22269250887792535, 0.22269250887792552, 0.2924611020120569], 
reward next is 0.7075, 
noisyNet noise sample is [array([-0.44967172], dtype=float32), 1.3988347]. 
=============================================
[2019-04-28 01:00:22,075] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1641021e-16 9.9999976e-01 1.8544905e-24 1.9193747e-07 1.3076790e-21], sum to 1.0000
[2019-04-28 01:00:22,082] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0310
[2019-04-28 01:00:22,086] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 79.00000000000001, 1.0, 2.0, 0.5863806676398069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819422.435591174, 819422.435591174, 198236.2585279407], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3345000.0000, 
sim time next is 3345600.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5863972459824612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 819445.6115022216, 819445.6115022221, 198239.279135261], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.5016834288945315, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2276237809728393, 0.22762378097283947, 0.29587952109740445], 
reward next is 0.7041, 
noisyNet noise sample is [array([-0.4857481], dtype=float32), -1.4154546]. 
=============================================
[2019-04-28 01:00:25,751] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.4303299e-12 9.8491067e-01 2.4691840e-19 1.5089373e-02 2.8420624e-15], sum to 1.0000
[2019-04-28 01:00:25,757] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1538
[2019-04-28 01:00:25,768] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2268421.570942929 W.
[2019-04-28 01:00:25,775] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.8111019204170694, 1.0, 2.0, 0.8111019204170694, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268421.570942929, 2268421.570942929, 425318.6635369243], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3502800.0000, 
sim time next is 3503400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.06730590931318, 6.9112, 168.9120138105979, 2394526.052922308, 2283779.766181047, 475729.3924205455], 
processed observation next is [1.0, 0.5652173913043478, 0.7630331753554502, 0.63, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.015610590931317957, 0.0, 0.8294353160645523, 0.6651461258117523, 0.6343832683836241, 0.7100438692843963], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.31347], dtype=float32), 0.40483925]. 
=============================================
[2019-04-28 01:00:28,679] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4721497e-10 8.9841861e-01 1.9172067e-14 1.0158145e-01 7.5945335e-12], sum to 1.0000
[2019-04-28 01:00:28,685] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5142
[2019-04-28 01:00:28,690] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7497790294148525, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047871.878819223, 1047871.878819223, 231956.1629594292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3386400.0000, 
sim time next is 3387000.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7356346908802169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1028094.546264971, 1028094.546264971, 228724.2557365517], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.6814875793737554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2855818184069364, 0.2855818184069364, 0.3413794861739578], 
reward next is 0.6586, 
noisyNet noise sample is [array([-1.3050642], dtype=float32), -0.4802562]. 
=============================================
[2019-04-28 01:00:28,710] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[41.727196]
 [41.634857]
 [43.61    ]
 [44.03048 ]
 [44.28937 ]], R is [[42.1398468 ]
 [41.71844864]
 [41.90003204]
 [42.13173676]
 [42.33229065]].
[2019-04-28 01:00:33,614] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [4.9339936e-15 9.8272473e-01 4.9656197e-23 1.7275274e-02 3.3063717e-19], sum to 1.0000
[2019-04-28 01:00:33,621] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4464
[2019-04-28 01:00:33,625] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 79.0, 1.0, 2.0, 0.5192437556441211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 725571.6582813936, 725571.6582813943, 186666.0397329655], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3624600.0000, 
sim time next is 3625200.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5191185488877897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 725396.6393389122, 725396.6393389116, 186645.7186011872], 
processed observation next is [1.0, 1.0, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42062475769613217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20149906648303118, 0.201499066483031, 0.278575699404757], 
reward next is 0.7214, 
noisyNet noise sample is [array([-0.01971332], dtype=float32), 0.103925645]. 
=============================================
[2019-04-28 01:00:35,384] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.7123549e-13 9.7499675e-01 5.2872471e-18 2.5003275e-02 1.6554922e-14], sum to 1.0000
[2019-04-28 01:00:35,404] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6528
[2019-04-28 01:00:35,409] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.9389799422950671, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129060778765, 1312457.630642753, 1312457.630642754, 280881.563930819], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3637800.0000, 
sim time next is 3638400.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.8955024469003753, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129564978379, 1251651.21244208, 1251651.21244208, 268699.9941978502], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.79, 1.0, 1.0, 0.87409933361491, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399450904648, 0.34768089234502225, 0.34768089234502225, 0.40104476745947787], 
reward next is 0.5990, 
noisyNet noise sample is [array([-0.03411312], dtype=float32), -0.53866917]. 
=============================================
[2019-04-28 01:00:36,848] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.0848940e-11 9.3552953e-01 1.8292459e-17 6.4470448e-02 1.9879207e-13], sum to 1.0000
[2019-04-28 01:00:36,856] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7303
[2019-04-28 01:00:36,861] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.33333333333334, 82.0, 1.0, 2.0, 0.6731278364712743, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 940698.6977311402, 940698.6977311409, 215147.9812941711], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3558000.0000, 
sim time next is 3558600.0000, 
raw observation next is [26.25, 82.5, 1.0, 2.0, 0.6740105933406464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 941932.9008730919, 941932.9008730913, 215331.5916166144], 
processed observation next is [1.0, 0.17391304347826086, 0.4431279620853081, 0.825, 1.0, 1.0, 0.6072416787236703, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2616480280203033, 0.2616480280203031, 0.3213904352486782], 
reward next is 0.6786, 
noisyNet noise sample is [array([-1.2539608], dtype=float32), 0.0047701113]. 
=============================================
[2019-04-28 01:00:40,262] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.1811614e-13 8.4056300e-01 1.7794014e-18 1.5943699e-01 6.5870364e-14], sum to 1.0000
[2019-04-28 01:00:40,286] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3565
[2019-04-28 01:00:40,297] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 83.16666666666667, 1.0, 2.0, 0.6266798677945643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875760.7233066966, 875760.7233066966, 205811.3681028784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3561000.0000, 
sim time next is 3561600.0000, 
raw observation next is [26.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5875451582531311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 821050.3517116226, 821050.3517116226, 198440.8181525871], 
processed observation next is [1.0, 0.21739130434782608, 0.44707740916271754, 0.8233333333333335, 1.0, 1.0, 0.503066455726664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2280695421421174, 0.2280695421421174, 0.29618032560087626], 
reward next is 0.7038, 
noisyNet noise sample is [array([-0.09062549], dtype=float32), -0.38584223]. 
=============================================
[2019-04-28 01:00:41,483] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.1485531e-14 9.9672544e-01 9.4211318e-22 3.2744901e-03 5.6416821e-17], sum to 1.0000
[2019-04-28 01:00:41,497] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9695
[2019-04-28 01:00:41,502] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5823400120669868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813773.7633415911, 813773.7633415911, 197501.9384960767], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3887400.0000, 
sim time next is 3888000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5827663419174642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814369.7537258837, 814369.753725883, 197579.1329577842], 
processed observation next is [0.0, 0.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4973088456836918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22621382047941213, 0.22621382047941194, 0.2948942282952003], 
reward next is 0.7051, 
noisyNet noise sample is [array([0.22939752], dtype=float32), -1.0526224]. 
=============================================
[2019-04-28 01:00:41,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[66.48421]
 [66.51792]
 [66.56102]
 [66.59847]
 [66.64927]], R is [[65.00171661]
 [65.05692291]
 [65.11167908]
 [65.16584778]
 [65.21924591]].
[2019-04-28 01:00:42,180] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.40131361e-11 8.78293812e-01 1.41021179e-16 1.21706136e-01
 3.41915054e-13], sum to 1.0000
[2019-04-28 01:00:42,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.0456
[2019-04-28 01:00:42,189] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2362894.29785797 W.
[2019-04-28 01:00:42,195] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.5632332642636793, 1.0, 1.0, 0.5632332642636793, 1.0, 2.0, 0.9781499159392368, 6.9112, 6.9112, 170.5573041426782, 2362894.29785797, 2362894.29785797, 461626.6008410142], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3670800.0000, 
sim time next is 3671400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5227460492531709, 1.0, 2.0, 0.5227460492531709, 1.0, 2.0, 0.9078370127925885, 6.9112, 6.9112, 170.5573041426782, 2192889.125833781, 2192889.125833781, 431231.5358936179], 
processed observation next is [1.0, 0.4782608695652174, 0.7156398104265403, 0.63, 1.0, 1.0, 0.4249952400640613, 1.0, 1.0, 0.4249952400640613, 1.0, 1.0, 0.8876061131616934, 0.0, 0.0, 0.8375144448122397, 0.6091358682871614, 0.6091358682871614, 0.643629158050176], 
reward next is 0.3564, 
noisyNet noise sample is [array([1.574597], dtype=float32), -0.37019056]. 
=============================================
[2019-04-28 01:00:52,708] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.8405450e-15 9.6144915e-01 4.1228723e-23 3.8550865e-02 1.2195649e-16], sum to 1.0000
[2019-04-28 01:00:52,721] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8403
[2019-04-28 01:00:52,735] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5352077536145072, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747887.0005426102, 747887.0005426102, 189296.364717383], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3815400.0000, 
sim time next is 3816000.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5369645573922477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 750342.7847877281, 750342.7847877281, 189590.2899696619], 
processed observation next is [0.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.4421259727617442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20842855132992447, 0.20842855132992447, 0.2829705820442715], 
reward next is 0.7170, 
noisyNet noise sample is [array([0.4126918], dtype=float32), 1.0536814]. 
=============================================
[2019-04-28 01:00:52,748] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[67.65339 ]
 [67.44914 ]
 [67.281944]
 [66.636566]
 [68.40423 ]], R is [[67.72027588]
 [67.76054382]
 [67.80050659]
 [67.12250519]
 [67.08715057]].
[2019-04-28 01:00:52,812] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-28 01:00:52,813] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:00:52,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:00:52,815] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:00:52,817] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:00:52,818] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:00:52,819] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:00:52,819] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:00:52,820] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:00:52,820] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:00:52,822] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:00:52,845] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run56
[2019-04-28 01:00:52,887] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run56
[2019-04-28 01:00:52,919] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run56
[2019-04-28 01:00:52,953] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run56
[2019-04-28 01:00:52,994] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run56
[2019-04-28 01:01:03,636] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:03,637] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.56666666666667, 95.0, 1.0, 2.0, 0.4372197476603219, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 635248.2646773752, 635248.2646773745, 177455.7051511552]
[2019-04-28 01:01:03,638] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:01:03,640] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.53837266e-15 9.98566806e-01 1.09226004e-23 1.43319159e-03
 4.37048900e-18], sampled 0.8372392976539709
[2019-04-28 01:01:08,100] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:08,102] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.1780978, 64.05640222, 1.0, 2.0, 0.8697681685928741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1215661.605906234, 1215661.605906234, 261752.6571985922]
[2019-04-28 01:01:08,103] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:01:08,107] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.0857035e-14 9.9791747e-01 2.2981444e-22 2.0825048e-03 4.3838743e-17], sampled 0.7535064237734885
[2019-04-28 01:01:14,712] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:14,714] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.66666666666667, 92.66666666666667, 1.0, 2.0, 0.4147424723665157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 607931.0390725817, 607931.0390725811, 174962.6283805698]
[2019-04-28 01:01:14,715] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:01:14,718] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8619995e-15 9.9851352e-01 1.4709241e-23 1.4865205e-03 5.4749046e-18], sampled 0.36387098446202726
[2019-04-28 01:01:19,456] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:19,457] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.75872282166667, 90.52410666666668, 1.0, 2.0, 0.5787659194769105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 808777.3522149342, 808777.3522149342, 196856.178235338]
[2019-04-28 01:01:19,459] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:01:19,460] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.0532714e-15 9.9813700e-01 9.2426061e-23 1.8630401e-03 2.2006088e-17], sampled 0.36157143287648075
[2019-04-28 01:01:29,641] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:29,641] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.0, 89.0, 1.0, 2.0, 0.5395591959381556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844189.1259799454, 844189.1259799454, 200583.1075371584]
[2019-04-28 01:01:29,643] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:01:29,645] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.2812064e-15 9.9834359e-01 3.5576276e-23 1.6563665e-03 1.0682864e-17], sampled 0.9461080574471503
[2019-04-28 01:01:52,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025233267]
[2019-04-28 01:01:52,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.85, 82.0, 1.0, 2.0, 0.6119755284586065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 855203.7084944184, 855203.7084944184, 202997.5590391438]
[2019-04-28 01:01:52,333] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:01:52,336] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0661338e-14 9.9792409e-01 2.2329573e-22 2.0759020e-03 4.2900515e-17], sampled 0.5454365291121308
[2019-04-28 01:02:27,438] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8242.3332 2928288015.2279 1325.0000
[2019-04-28 01:02:27,596] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7980.7720 3008359968.3604 1754.0000
[2019-04-28 01:02:27,715] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8475.7205 2843366909.9060 1124.0000
[2019-04-28 01:02:27,726] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7870.1714 3164589857.6993 1767.0000
[2019-04-28 01:02:28,159] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8639.0065 2780279648.8327 929.0000
[2019-04-28 01:02:29,175] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1375000, evaluation results [1375000.0, 7870.171398814952, 3164589857.699317, 1767.0, 8242.333225203278, 2928288015.227881, 1325.0, 8639.006506007603, 2780279648.8326693, 929.0, 7980.771984672581, 3008359968.360354, 1754.0, 8475.720545990669, 2843366909.9060073, 1124.0]
[2019-04-28 01:02:34,125] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5888109e-07 9.7999495e-01 8.2798985e-12 2.0004854e-02 1.3781886e-09], sum to 1.0000
[2019-04-28 01:02:34,134] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5572
[2019-04-28 01:02:34,141] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1970854.295398447 W.
[2019-04-28 01:02:34,150] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 82.33333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.639608210378489, 6.9112, 168.9088871958571, 1970854.295398447, 1454108.890393151, 311354.1403777419], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3987600.0000, 
sim time next is 3988200.0000, 
raw observation next is [29.16666666666667, 83.16666666666666, 1.0, 2.0, 0.396517947479095, 1.0, 1.0, 0.396517947479095, 1.0, 1.0, 0.6886205442821655, 6.9112, 6.9112, 170.5573041426782, 1662958.667808813, 1662958.667808813, 350969.8899200812], 
processed observation next is [1.0, 0.13043478260869565, 0.581358609794629, 0.8316666666666666, 1.0, 1.0, 0.2729131897338494, 1.0, 0.5, 0.2729131897338494, 1.0, 0.5, 0.6202689564416651, 0.0, 0.0, 0.8375144448122397, 0.4619329632802259, 0.4619329632802259, 0.523835656597136], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.84737915], dtype=float32), -0.9352095]. 
=============================================
[2019-04-28 01:02:35,902] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.5184130e-12 9.9850661e-01 1.5822536e-18 1.4933455e-03 1.4815520e-15], sum to 1.0000
[2019-04-28 01:02:35,910] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9289
[2019-04-28 01:02:35,920] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2697349.673297028 W.
[2019-04-28 01:02:35,924] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666666, 73.66666666666666, 1.0, 2.0, 0.964305058992741, 1.0, 2.0, 0.964305058992741, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2697349.673297028, 2697349.673297027, 507795.8502508089], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4099200.0000, 
sim time next is 4099800.0000, 
raw observation next is [31.83333333333333, 72.33333333333334, 1.0, 2.0, 0.9964681212859074, 1.0, 2.0, 0.9964681212859074, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2787416.410930175, 2787416.410930174, 526860.1557300332], 
processed observation next is [1.0, 0.43478260869565216, 0.7077409162717218, 0.7233333333333334, 1.0, 1.0, 0.9957447244408524, 1.0, 1.0, 0.9957447244408524, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7742823363694931, 0.7742823363694927, 0.7863584413881092], 
reward next is 0.2136, 
noisyNet noise sample is [array([-0.661195], dtype=float32), 0.42029136]. 
=============================================
[2019-04-28 01:02:36,541] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.1496655e-11 9.9742067e-01 4.1469701e-16 2.5793824e-03 2.0176304e-14], sum to 1.0000
[2019-04-28 01:02:36,550] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3287
[2019-04-28 01:02:36,560] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2626626.463477538 W.
[2019-04-28 01:02:36,566] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [36.0, 53.66666666666667, 1.0, 2.0, 0.6260319964448562, 1.0, 1.0, 0.6260319964448562, 1.0, 2.0, 1.03, 6.975516126804655, 6.9112, 170.5573041426782, 2626626.463477538, 2580554.205634595, 497611.6387738208], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4194600.0000, 
sim time next is 4195200.0000, 
raw observation next is [36.0, 54.33333333333334, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.16024210078491, 6.9112, 170.5573041426782, 3087936.52933188, 2909537.542154686, 552348.0783819861], 
processed observation next is [1.0, 0.5652173913043478, 0.9052132701421801, 0.5433333333333334, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.02490421007849104, 0.0, 0.8375144448122397, 0.8577601470366334, 0.8082048728207462, 0.824400116988039], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.014439], dtype=float32), -0.47765604]. 
=============================================
[2019-04-28 01:02:36,675] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5613530e-10 9.9743527e-01 7.3449852e-15 2.5646957e-03 6.4950337e-11], sum to 1.0000
[2019-04-28 01:02:36,684] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2389
[2019-04-28 01:02:36,689] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 84.0, 1.0, 2.0, 0.6049915084141082, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 845440.0223513092, 845440.0223513092, 201679.2590424644], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3976200.0000, 
sim time next is 3976800.0000, 
raw observation next is [29.33333333333334, 84.0, 1.0, 2.0, 0.6003891332512326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 839005.9337217159, 839005.9337217159, 200818.0299813241], 
processed observation next is [1.0, 0.0, 0.5892575039494474, 0.84, 1.0, 1.0, 0.5185411243990753, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23305720381158776, 0.23305720381158776, 0.29972840295720016], 
reward next is 0.7003, 
noisyNet noise sample is [array([-0.04364404], dtype=float32), -1.1424501]. 
=============================================
[2019-04-28 01:02:39,062] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2458503e-12 9.9744415e-01 2.4786753e-20 2.5558015e-03 1.2878475e-15], sum to 1.0000
[2019-04-28 01:02:39,070] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7719
[2019-04-28 01:02:39,077] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2615632.767251001 W.
[2019-04-28 01:02:39,080] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.66666666666667, 66.33333333333334, 1.0, 2.0, 0.9351217365937016, 1.0, 2.0, 0.9351217365937016, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2615632.767251001, 2615632.767251001, 491020.5009722295], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4033200.0000, 
sim time next is 4033800.0000, 
raw observation next is [32.0, 69.5, 1.0, 2.0, 0.9187641596962833, 1.0, 2.0, 0.9187641596962833, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2569831.869601576, 2569831.869601576, 481837.6127637615], 
processed observation next is [1.0, 0.6956521739130435, 0.7156398104265403, 0.695, 1.0, 1.0, 0.90212549360998, 1.0, 1.0, 0.90212549360998, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7138421860004378, 0.7138421860004378, 0.7191606160653157], 
reward next is 0.2808, 
noisyNet noise sample is [array([-0.4781304], dtype=float32), -0.42092195]. 
=============================================
[2019-04-28 01:02:39,650] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.9729117e-14 9.9407756e-01 4.1321471e-20 5.9224390e-03 9.1332454e-15], sum to 1.0000
[2019-04-28 01:02:39,660] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8477
[2019-04-28 01:02:39,664] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5826754100228536, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814242.634907639, 814242.634907639, 197562.6636269549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4140000.0000, 
sim time next is 4140600.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5828694763048515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814513.9311146375, 814513.9311146375, 197597.8153263974], 
processed observation next is [1.0, 0.9565217391304348, 0.5734597156398105, 0.84, 1.0, 1.0, 0.4974331039817488, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22625386975406597, 0.22625386975406597, 0.2949221124274588], 
reward next is 0.7051, 
noisyNet noise sample is [array([1.820857], dtype=float32), 0.900323]. 
=============================================
[2019-04-28 01:02:41,188] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4193838e-15 9.9556798e-01 1.0209858e-24 4.4320310e-03 4.2557950e-17], sum to 1.0000
[2019-04-28 01:02:41,193] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1511
[2019-04-28 01:02:41,199] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 85.66666666666667, 1.0, 2.0, 0.5417132703311206, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 756980.8991221943, 756980.8991221943, 190389.8431550976], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4054800.0000, 
sim time next is 4055400.0000, 
raw observation next is [27.5, 86.5, 1.0, 2.0, 0.5403003727860822, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 755005.838478563, 755005.8384785623, 190151.2808299438], 
processed observation next is [1.0, 0.9565217391304348, 0.5023696682464456, 0.865, 1.0, 1.0, 0.4461450274531111, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20972384402182306, 0.20972384402182287, 0.283807881835737], 
reward next is 0.7162, 
noisyNet noise sample is [array([-1.2270635], dtype=float32), 1.7102474]. 
=============================================
[2019-04-28 01:02:42,145] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.4379250e-14 9.5956069e-01 1.7561010e-19 4.0439367e-02 1.8819492e-15], sum to 1.0000
[2019-04-28 01:02:42,157] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1452
[2019-04-28 01:02:42,160] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.6184073565569635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 864195.5076228321, 864195.5076228321, 204226.0443763449], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4129200.0000, 
sim time next is 4129800.0000, 
raw observation next is [31.0, 78.33333333333334, 1.0, 2.0, 0.6175177135275672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862951.7678225458, 862951.7678225458, 204055.3345715984], 
processed observation next is [1.0, 0.8260869565217391, 0.6682464454976303, 0.7833333333333334, 1.0, 1.0, 0.5391779681055027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23970882439515162, 0.23970882439515162, 0.3045602008531319], 
reward next is 0.6954, 
noisyNet noise sample is [array([-0.36362553], dtype=float32), -0.17127284]. 
=============================================
[2019-04-28 01:02:48,929] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.2924206e-12 9.5172125e-01 7.1167253e-18 4.8278812e-02 2.8821550e-14], sum to 1.0000
[2019-04-28 01:02:48,939] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8697
[2019-04-28 01:02:48,948] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 79.0, 1.0, 2.0, 0.621937577563629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869130.8457266962, 869130.8457266962, 204904.5806662329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4315200.0000, 
sim time next is 4315800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6229624248749964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 870563.612440204, 870563.612440204, 205102.2556775321], 
processed observation next is [1.0, 0.9565217391304348, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5457378612951764, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24182322567783446, 0.24182322567783446, 0.30612276966795837], 
reward next is 0.6939, 
noisyNet noise sample is [array([-0.9386845], dtype=float32), 1.1673776]. 
=============================================
[2019-04-28 01:03:05,384] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.9796664e-16 9.9390078e-01 6.8010965e-24 6.0992269e-03 3.5094345e-17], sum to 1.0000
[2019-04-28 01:03:05,393] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7404
[2019-04-28 01:03:05,399] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333334, 79.0, 1.0, 2.0, 0.5251308930394936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733800.9636913787, 733800.9636913781, 187627.6321228274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4530000.0000, 
sim time next is 4530600.0000, 
raw observation next is [28.5, 79.0, 1.0, 2.0, 0.5298833543897158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 740444.2144029543, 740444.2144029537, 188411.1109437175], 
processed observation next is [0.0, 0.43478260869565216, 0.5497630331753555, 0.79, 1.0, 1.0, 0.4335944028791756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2056789484452651, 0.20567894844526494, 0.2812106133488321], 
reward next is 0.7188, 
noisyNet noise sample is [array([-0.84696436], dtype=float32), 0.25024936]. 
=============================================
[2019-04-28 01:03:11,171] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1707742e-13 9.7794992e-01 7.1320934e-20 2.2050086e-02 9.7271603e-14], sum to 1.0000
[2019-04-28 01:03:11,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7775
[2019-04-28 01:03:11,182] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3220562.773018518 W.
[2019-04-28 01:03:11,186] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.3451707884366, 6.9112, 170.5573041426782, 3220562.773018518, 2909691.844458358, 551332.6597958638], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4632000.0000, 
sim time next is 4632600.0000, 
raw observation next is [35.0, 60.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.925883974773758, 6.9112, 170.5573041426782, 3637035.897020645, 2910176.491185049, 547788.8872858208], 
processed observation next is [1.0, 0.6086956521739131, 0.8578199052132701, 0.6, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.10146839747737584, 0.0, 0.8375144448122397, 1.0102877491724014, 0.8083823586625136, 0.8175953541579415], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3251691], dtype=float32), -0.43151745]. 
=============================================
[2019-04-28 01:03:11,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8950137e-10 9.7793716e-01 1.4953108e-16 2.2062831e-02 6.9211355e-12], sum to 1.0000
[2019-04-28 01:03:11,483] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1966
[2019-04-28 01:03:11,500] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7864823679441934, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1099194.069068189, 1099194.069068189, 240619.8717489537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4681200.0000, 
sim time next is 4681800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.7798236279340122, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1089882.984695318, 1089882.984695318, 239018.6264797779], 
processed observation next is [1.0, 0.17391304347826086, 0.4786729857819906, 0.89, 1.0, 1.0, 0.734727262571099, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3027452735264772, 0.3027452735264772, 0.35674421862653416], 
reward next is 0.6433, 
noisyNet noise sample is [array([1.6274599], dtype=float32), 1.911381]. 
=============================================
[2019-04-28 01:03:12,575] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [8.3049036e-12 9.9249095e-01 2.7933767e-18 7.5090458e-03 1.1298950e-14], sum to 1.0000
[2019-04-28 01:03:12,582] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4272
[2019-04-28 01:03:12,586] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5400444388344, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 754648.074023497, 754648.074023497, 190107.6739553453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4648800.0000, 
sim time next is 4649400.0000, 
raw observation next is [29.5, 70.0, 1.0, 2.0, 0.5263492198758447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 735504.0036963735, 735504.0036963728, 187826.8308498702], 
processed observation next is [1.0, 0.8260869565217391, 0.5971563981042655, 0.7, 1.0, 1.0, 0.4293364094889695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20430666769343708, 0.20430666769343692, 0.28033855350726894], 
reward next is 0.7197, 
noisyNet noise sample is [array([0.19326498], dtype=float32), -0.2725104]. 
=============================================
[2019-04-28 01:03:15,181] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1859835e-10 9.0753466e-01 2.4123822e-15 9.2465296e-02 1.6245626e-11], sum to 1.0000
[2019-04-28 01:03:15,186] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9043
[2019-04-28 01:03:15,194] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4910199902297619, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686120.0951298968, 686120.0951298961, 182200.797383525], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4664400.0000, 
sim time next is 4665000.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.491204384838853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686377.8396673821, 686377.8396673815, 182229.1919985354], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.386993234745606, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19066051101871725, 0.1906605110187171, 0.27198386865453045], 
reward next is 0.7280, 
noisyNet noise sample is [array([0.77659005], dtype=float32), 0.61186993]. 
=============================================
[2019-04-28 01:03:15,204] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.0640104e-10 9.8600799e-01 2.7202560e-14 1.3991990e-02 1.1082842e-10], sum to 1.0000
[2019-04-28 01:03:15,207] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6641
[2019-04-28 01:03:15,221] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[43.616096]
 [44.515865]
 [44.5421  ]
 [44.772816]
 [44.511387]], R is [[43.77744675]
 [44.06772995]
 [44.35505295]
 [44.63939285]
 [44.92079544]].
[2019-04-28 01:03:15,226] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2076616.682175058 W.
[2019-04-28 01:03:15,230] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.0, 67.33333333333334, 1.0, 2.0, 0.4950555923372547, 1.0, 2.0, 0.4950555923372547, 1.0, 1.0, 0.8597478465036771, 6.9112, 6.9112, 170.5573041426782, 2076616.682175058, 2076616.682175058, 411743.1214827417], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4707600.0000, 
sim time next is 4708200.0000, 
raw observation next is [31.0, 66.66666666666666, 1.0, 2.0, 0.8691289772564985, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.995560605306924, 6.9112, 168.9124542940015, 2111826.49684711, 2051978.357917005, 426030.7643820963], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.6666666666666665, 1.0, 1.0, 0.8423240689837331, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008436060530692392, 0.0, 0.8294374790397181, 0.5866184713464194, 0.5699939883102791, 0.6358668125105915], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.07465749], dtype=float32), 0.89554626]. 
=============================================
[2019-04-28 01:03:18,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8408793e-13 9.9776220e-01 1.0866997e-20 2.2377837e-03 4.7591519e-14], sum to 1.0000
[2019-04-28 01:03:18,647] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6058
[2019-04-28 01:03:18,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 69.33333333333333, 1.0, 2.0, 0.5034934892078169, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703555.5409653429, 703555.5409653423, 184147.533198418], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4816200.0000, 
sim time next is 4816800.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5084562045137168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710492.4996086985, 710492.4996086991, 184933.4620825716], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4077785596550804, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19735902766908292, 0.1973590276690831, 0.27602009266055466], 
reward next is 0.7240, 
noisyNet noise sample is [array([0.10446511], dtype=float32), -0.07646756]. 
=============================================
[2019-04-28 01:03:20,163] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 01:03:20,164] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:03:20,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:03:20,165] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:03:20,166] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:03:20,167] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:03:20,168] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:03:20,168] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:03:20,167] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:03:20,170] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:03:20,176] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:03:20,183] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run57
[2019-04-28 01:03:20,207] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run57
[2019-04-28 01:03:20,235] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run57
[2019-04-28 01:03:20,236] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run57
[2019-04-28 01:03:20,295] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run57
[2019-04-28 01:03:28,956] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:03:28,957] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.03333333333333, 65.33333333333333, 1.0, 2.0, 0.2687519367232816, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 439920.4390850153, 439920.4390850153, 162806.9478327608]
[2019-04-28 01:03:28,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:03:28,961] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.6281394e-14 9.9708360e-01 7.2236576e-22 2.9163624e-03 3.6118451e-16], sampled 0.9074501874283226
[2019-04-28 01:03:29,527] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:03:29,530] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.95, 84.0, 1.0, 2.0, 0.4281786814701893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624799.7074695091, 624799.7074695097, 176500.92063704]
[2019-04-28 01:03:29,531] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:03:29,532] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6019999e-15 9.9808359e-01 1.9362028e-23 1.9163878e-03 2.5580828e-17], sampled 0.10781128264104478
[2019-04-28 01:03:39,269] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:03:39,270] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.6, 83.83333333333333, 1.0, 2.0, 0.420631012203694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621596.5709057864, 621596.570905787, 176400.7937135208]
[2019-04-28 01:03:39,270] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:03:39,273] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6198902e-13 9.9600106e-01 1.2461112e-20 3.9988742e-03 2.8946623e-15], sampled 0.5018177649138092
[2019-04-28 01:03:51,710] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:03:51,711] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.5, 64.5, 1.0, 2.0, 0.7490025189375764, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046786.112874035, 1046786.112874035, 231776.6326204244]
[2019-04-28 01:03:51,713] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:03:51,715] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5106547e-14 9.9662089e-01 2.3050660e-21 3.3790907e-03 8.4452827e-16], sampled 0.5296489103334878
[2019-04-28 01:03:56,719] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:03:56,719] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.15912500833333, 91.63807265000001, 1.0, 2.0, 0.6924820477588642, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 967758.6047663495, 967758.6047663488, 219237.8964388348]
[2019-04-28 01:03:56,720] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:03:56,722] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2454566e-14 9.9743623e-01 2.2460607e-22 2.5638568e-03 1.5373337e-16], sampled 0.060217606861206585
[2019-04-28 01:04:00,261] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:00,263] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.13333333333334, 62.33333333333334, 1.0, 2.0, 0.5610334981850477, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783988.6198070841, 783988.6198070834, 193710.7564236449]
[2019-04-28 01:04:00,263] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:04:00,267] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.0009692e-13 9.9635857e-01 5.8597107e-21 3.6414410e-03 1.6647782e-15], sampled 0.5188100041751837
[2019-04-28 01:04:01,043] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:01,044] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.16666666666667, 58.66666666666667, 1.0, 2.0, 0.918855283687806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1284311.408683834, 1284311.408683835, 275181.5562842684]
[2019-04-28 01:04:01,045] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:04:01,047] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.7691665e-13 9.9465561e-01 1.7656897e-19 5.3444472e-03 2.0084981e-14], sampled 0.11923879413358418
[2019-04-28 01:04:01,235] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:01,237] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.27452831833333, 79.27304333333333, 1.0, 2.0, 0.4998826919664378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 698508.3423457536, 698508.3423457529, 183575.1209814293]
[2019-04-28 01:04:01,238] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:04:01,242] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.4888304e-14 9.9669600e-01 2.2870732e-21 3.3039984e-03 8.3759102e-16], sampled 0.5209863713373288
[2019-04-28 01:04:17,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:17,421] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.0, 75.0, 1.0, 2.0, 0.5736812492442596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801669.2660291322, 801669.2660291322, 195944.5456930544]
[2019-04-28 01:04:17,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:04:17,424] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.8255047e-14 9.9678493e-01 1.8705282e-21 3.2150797e-03 7.2280752e-16], sampled 0.12136638336896222
[2019-04-28 01:04:32,273] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:32,277] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.55, 78.5, 1.0, 2.0, 0.5423090090483408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757813.67121776, 757813.67121776, 190489.9808927933]
[2019-04-28 01:04:32,278] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:04:32,281] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [8.0296765e-14 9.9647433e-01 4.1515048e-21 3.5256709e-03 1.2949709e-15], sampled 0.41131398597698987
[2019-04-28 01:04:42,138] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.024142532]
[2019-04-28 01:04:42,139] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.5, 56.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.398620032566422, 6.9112, 168.9098022517628, 1799777.857391932, 1453991.767871695, 311354.6575737594]
[2019-04-28 01:04:42,140] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:04:42,144] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9403290e-12 9.9351221e-01 6.0966070e-19 6.4877407e-03 4.9924786e-14], sampled 0.03952550050730841
[2019-04-28 01:04:42,146] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1799777.857391932 W.
[2019-04-28 01:04:44,598] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8627.7700 2781032465.8388 928.0000
[2019-04-28 01:04:44,762] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8473.7164 2843608283.2522 1119.0000
[2019-04-28 01:04:44,854] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7857.6898 3165604709.7129 1764.0000
[2019-04-28 01:04:44,866] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7975.3634 3008623299.2236 1747.0000
[2019-04-28 01:04:44,987] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8220.1188 2929030715.9838 1323.0000
[2019-04-28 01:04:46,002] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1400000, evaluation results [1400000.0, 7857.689767518252, 3165604709.7128506, 1764.0, 8220.118846302632, 2929030715.9837523, 1323.0, 8627.769966977023, 2781032465.838808, 928.0, 7975.363357719439, 3008623299.2236094, 1747.0, 8473.716419895427, 2843608283.25215, 1119.0]
[2019-04-28 01:04:49,475] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.9020676e-15 9.9741685e-01 6.9451112e-22 2.5831491e-03 1.1945996e-14], sum to 1.0000
[2019-04-28 01:04:49,488] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0340
[2019-04-28 01:04:49,496] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 70.0, 1.0, 2.0, 0.5105331950420744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713395.7618843167, 713395.7618843167, 185264.1819900369], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4817400.0000, 
sim time next is 4818000.0000, 
raw observation next is [29.66666666666667, 70.0, 1.0, 2.0, 0.5098914425048809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712498.7054627757, 712498.7054627764, 185161.3104872896], 
processed observation next is [1.0, 0.782608695652174, 0.6050552922590839, 0.7, 1.0, 1.0, 0.4095077620540734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19791630707299324, 0.19791630707299343, 0.27636016490640236], 
reward next is 0.7236, 
noisyNet noise sample is [array([-0.2767632], dtype=float32), 0.46317956]. 
=============================================
[2019-04-28 01:04:49,511] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.10496 ]
 [64.13043 ]
 [63.55548 ]
 [64.34414 ]
 [64.069244]], R is [[64.08002472]
 [64.1627121 ]
 [64.24506378]
 [64.32776642]
 [64.41094971]].
[2019-04-28 01:04:50,122] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7420743e-16 9.9975830e-01 2.2881646e-22 2.4168310e-04 4.4421220e-17], sum to 1.0000
[2019-04-28 01:04:50,130] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5059
[2019-04-28 01:04:50,133] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4915355374368663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686840.7208903376, 686840.7208903369, 182280.3306714947], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4829400.0000, 
sim time next is 4830000.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4914819092138431, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686765.759974616, 686765.7599746166, 182272.0662400373], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.74, 1.0, 1.0, 0.3873276014624616, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19076826665961555, 0.19076826665961572, 0.2720478600597572], 
reward next is 0.7280, 
noisyNet noise sample is [array([1.5864797], dtype=float32), -0.067359164]. 
=============================================
[2019-04-28 01:04:50,152] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[64.53919 ]
 [64.621895]
 [64.49062 ]
 [64.82505 ]
 [64.82058 ]], R is [[64.92481995]
 [65.00351715]
 [65.08126831]
 [65.15794373]
 [65.2336731 ]].
[2019-04-28 01:05:00,784] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5708368e-16 9.9682260e-01 2.8032367e-25 3.1774277e-03 3.5324857e-19], sum to 1.0000
[2019-04-28 01:05:00,789] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9960
[2019-04-28 01:05:00,792] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4807277927597731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 671733.8860906981, 671733.8860906975, 180631.5584659869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5019600.0000, 
sim time next is 5020200.0000, 
raw observation next is [26.0, 86.5, 1.0, 2.0, 0.4841421065163933, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676506.317761109, 676506.3177611083, 181148.8198019235], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.865, 1.0, 1.0, 0.37848446568240157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18791842160030806, 0.18791842160030786, 0.2703713728386918], 
reward next is 0.7296, 
noisyNet noise sample is [array([-0.30673558], dtype=float32), -1.182475]. 
=============================================
[2019-04-28 01:05:03,384] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.52797244e-10 9.73272324e-01 1.00133615e-16 2.67277099e-02
 4.34213592e-13], sum to 1.0000
[2019-04-28 01:05:03,392] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7625
[2019-04-28 01:05:03,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3142943.606114318 W.
[2019-04-28 01:05:03,409] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 8.121248955490273, 6.9112, 168.9061393084435, 3142943.606114318, 2284527.876840796, 473191.4761145629], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5224200.0000, 
sim time next is 5224800.0000, 
raw observation next is [31.33333333333334, 69.0, 1.0, 2.0, 0.9760933215412279, 1.0, 1.0, 0.9760933215412279, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2730359.779592058, 2730359.779592058, 514705.1464290952], 
processed observation next is [1.0, 0.4782608695652174, 0.6840442338072673, 0.69, 1.0, 1.0, 0.9711967729412384, 1.0, 0.5, 0.9711967729412384, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.758433272108905, 0.758433272108905, 0.7682166364613361], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8945549], dtype=float32), -0.8498515]. 
=============================================
[2019-04-28 01:05:05,154] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1072687e-16 9.9928564e-01 7.1748224e-26 7.1436662e-04 2.9699096e-20], sum to 1.0000
[2019-04-28 01:05:05,159] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7736
[2019-04-28 01:05:05,163] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 74.0, 1.0, 2.0, 0.52713651146288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736604.5218805672, 736604.5218805678, 187956.8702118441], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5166000.0000, 
sim time next is 5166600.0000, 
raw observation next is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5311699439436665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 742242.6868791814, 742242.6868791814, 188623.3436413643], 
processed observation next is [0.0, 0.8260869565217391, 0.5655608214849924, 0.7483333333333334, 1.0, 1.0, 0.4351445107755017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20617852413310594, 0.20617852413310594, 0.28152737856920046], 
reward next is 0.7185, 
noisyNet noise sample is [array([0.9094299], dtype=float32), -0.17713073]. 
=============================================
[2019-04-28 01:05:13,279] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.4472302e-13 9.9175256e-01 1.7645375e-18 8.2475105e-03 3.3969323e-17], sum to 1.0000
[2019-04-28 01:05:13,284] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9992
[2019-04-28 01:05:13,289] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.83333333333334, 78.66666666666667, 1.0, 2.0, 0.54229619340932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757795.756466966, 757795.7564669666, 190488.6051340579], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5254800.0000, 
sim time next is 5255400.0000, 
raw observation next is [28.81666666666667, 78.83333333333334, 1.0, 2.0, 0.2730321313981822, 1.0, 1.0, 0.2730321313981822, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 763060.4433326588, 763060.4433326588, 244881.5582222779], 
processed observation next is [1.0, 0.8260869565217391, 0.5647709320695105, 0.7883333333333334, 1.0, 1.0, 0.12413509807009905, 1.0, 0.5, 0.12413509807009905, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2119612342590719, 0.2119612342590719, 0.36549486301832523], 
reward next is 0.6345, 
noisyNet noise sample is [array([-0.24952997], dtype=float32), -0.7903886]. 
=============================================
[2019-04-28 01:05:13,768] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.4128096e-08 9.5973152e-01 7.9628309e-12 4.0268391e-02 1.3222817e-09], sum to 1.0000
[2019-04-28 01:05:13,775] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9639
[2019-04-28 01:05:13,780] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.5027000804344399, 1.0, 1.0, 0.5027000804344399, 1.0, 2.0, 0.8693824627402567, 6.9112, 6.9112, 170.5573041426782, 2108714.685921839, 2108714.685921839, 416357.2331891593], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5220000.0000, 
sim time next is 5220600.0000, 
raw observation next is [31.0, 66.66666666666667, 1.0, 2.0, 0.8538154728861865, 1.0, 2.0, 0.8538154728861865, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2387993.372915657, 2387993.372915657, 446918.2200867292], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.6666666666666667, 1.0, 1.0, 0.823874063718297, 1.0, 1.0, 0.823874063718297, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6633314924765713, 0.6633314924765713, 0.6670421195324316], 
reward next is 0.3330, 
noisyNet noise sample is [array([-1.294995], dtype=float32), -0.07803896]. 
=============================================
[2019-04-28 01:05:17,772] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2965544e-12 9.5362532e-01 7.6051606e-18 4.6374664e-02 2.3580777e-14], sum to 1.0000
[2019-04-28 01:05:17,782] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8982
[2019-04-28 01:05:17,788] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.88333333333333, 75.66666666666667, 1.0, 2.0, 0.5821301382215515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813480.3690571294, 813480.3690571294, 197465.3251338598], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5422200.0000, 
sim time next is 5422800.0000, 
raw observation next is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.5872359064941861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820618.0285986264, 820618.0285986264, 198393.2900724651], 
processed observation next is [1.0, 0.782608695652174, 0.6619273301737759, 0.7633333333333334, 1.0, 1.0, 0.5026938632460073, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22794945238850733, 0.22794945238850733, 0.29610938816785837], 
reward next is 0.7039, 
noisyNet noise sample is [array([-0.62313634], dtype=float32), 0.40610328]. 
=============================================
[2019-04-28 01:05:22,001] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.3936445e-07 6.0328615e-01 7.5660528e-12 3.9671358e-01 1.2671402e-08], sum to 1.0000
[2019-04-28 01:05:22,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0456
[2019-04-28 01:05:22,018] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.33333333333334, 1.0, 2.0, 0.5876312677755323, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821170.7298352455, 821170.7298352448, 198464.5141984941], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5446200.0000, 
sim time next is 5446800.0000, 
raw observation next is [28.5, 89.0, 1.0, 2.0, 0.5879812838135997, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 821660.0402485244, 821660.0402485238, 198528.4320130318], 
processed observation next is [1.0, 0.043478260869565216, 0.5497630331753555, 0.89, 1.0, 1.0, 0.5035919082091562, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.22823890006903455, 0.22823890006903438, 0.29631109255676386], 
reward next is 0.7037, 
noisyNet noise sample is [array([0.54356074], dtype=float32), -0.3844471]. 
=============================================
[2019-04-28 01:05:25,558] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.5224306e-08 8.4520566e-01 5.2985806e-13 1.5479428e-01 9.3926443e-11], sum to 1.0000
[2019-04-28 01:05:25,567] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2068
[2019-04-28 01:05:25,572] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3165851.435542423 W.
[2019-04-28 01:05:25,577] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.56666666666667, 66.0, 1.0, 2.0, 0.8675971157599859, 1.0, 2.0, 0.7543885973942557, 1.0, 1.0, 1.03, 7.005110950755588, 6.9112, 170.5573041426782, 3165851.435542423, 3098579.201462958, 579493.648208207], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5481600.0000, 
sim time next is 5482200.0000, 
raw observation next is [34.78333333333333, 65.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.359915074804808, 6.9112, 170.5573041426782, 3231137.008266245, 2909704.147621315, 551255.4937364039], 
processed observation next is [1.0, 0.43478260869565216, 0.8475513428120063, 0.655, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.044871507480480764, 0.0, 0.8375144448122397, 0.8975380578517347, 0.8082511521170319, 0.8227693936364237], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46510485], dtype=float32), -1.0205355]. 
=============================================
[2019-04-28 01:05:29,436] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5370946e-13 9.1484267e-01 3.1156420e-20 8.5157268e-02 1.8134982e-16], sum to 1.0000
[2019-04-28 01:05:29,439] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5250
[2019-04-28 01:05:29,467] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2386481.494445114 W.
[2019-04-28 01:05:29,476] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.13333333333334, 48.33333333333333, 1.0, 2.0, 0.5688502824908889, 1.0, 2.0, 0.5688502824908889, 1.0, 1.0, 0.9762349997960442, 6.911200000000001, 6.9112, 170.5573041426782, 2386481.494445114, 2386481.494445114, 463577.7182414183], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5580600.0000, 
sim time next is 5581200.0000, 
raw observation next is [34.06666666666667, 48.66666666666666, 1.0, 2.0, 0.8017893464909213, 1.0, 2.0, 0.8017893464909213, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2242353.58316446, 2242353.58316446, 420736.4882877485], 
processed observation next is [1.0, 0.6086956521739131, 0.8135860979462877, 0.4866666666666666, 1.0, 1.0, 0.7611919837240015, 1.0, 1.0, 0.7611919837240015, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6228759953234612, 0.6228759953234612, 0.6279649078921619], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.083411], dtype=float32), -0.43778923]. 
=============================================
[2019-04-28 01:05:30,065] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.1723546e-16 9.0523970e-01 2.6522675e-22 9.4760388e-02 3.3816736e-19], sum to 1.0000
[2019-04-28 01:05:30,071] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1140
[2019-04-28 01:05:30,076] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 74.66666666666667, 1.0, 2.0, 0.5730722300147552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800817.8942086841, 800817.8942086835, 195837.3381750357], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5514000.0000, 
sim time next is 5514600.0000, 
raw observation next is [30.35, 76.33333333333333, 1.0, 2.0, 0.576406859247662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805479.5113056763, 805479.5113056763, 196433.5117151019], 
processed observation next is [1.0, 0.8260869565217391, 0.637440758293839, 0.7633333333333333, 1.0, 1.0, 0.4896468183706771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2237443086960212, 0.2237443086960212, 0.29318434584343567], 
reward next is 0.7068, 
noisyNet noise sample is [array([-0.25555223], dtype=float32), -0.19838075]. 
=============================================
[2019-04-28 01:05:31,473] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.1173099e-10 9.4616425e-01 3.2453030e-15 5.3835727e-02 5.0538094e-12], sum to 1.0000
[2019-04-28 01:05:31,485] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2066
[2019-04-28 01:05:31,493] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3105746.295382418 W.
[2019-04-28 01:05:31,496] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.76666666666667, 70.66666666666667, 1.0, 2.0, 0.8389878624595036, 1.0, 1.0, 0.7400839707440143, 1.0, 2.0, 1.03, 7.005108693779861, 6.9112, 170.5573041426782, 3105746.295382418, 3038475.678066474, 568823.4227153698], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5476800.0000, 
sim time next is 5477400.0000, 
raw observation next is [33.0, 70.0, 1.0, 2.0, 0.7159697475951181, 1.0, 2.0, 0.6785749133118216, 1.0, 2.0, 1.03, 7.005098991389501, 6.9112, 170.5573041426782, 2847330.664100986, 2780066.997001925, 526421.9224268714], 
processed observation next is [1.0, 0.391304347826087, 0.7630331753554502, 0.7, 1.0, 1.0, 0.6577948766206242, 1.0, 1.0, 0.6127408594118333, 1.0, 1.0, 1.0365853658536586, 0.009389899138950141, 0.0, 0.8375144448122397, 0.7909251844724962, 0.7722408325005348, 0.7857043618311513], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1864794], dtype=float32), -0.0068694116]. 
=============================================
[2019-04-28 01:05:35,098] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2034998e-18 9.9822980e-01 3.0356593e-25 1.7702399e-03 3.3103167e-20], sum to 1.0000
[2019-04-28 01:05:35,101] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4615
[2019-04-28 01:05:35,107] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.45, 60.33333333333333, 1.0, 2.0, 0.5475165141936236, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 765093.1738036061, 765093.1738036054, 191376.2337570608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5665800.0000, 
sim time next is 5666400.0000, 
raw observation next is [32.5, 60.0, 1.0, 2.0, 0.547191515452865, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 764638.8609016353, 764638.8609016358, 191320.7237545327], 
processed observation next is [0.0, 0.6086956521739131, 0.7393364928909952, 0.6, 1.0, 1.0, 0.45444760897935543, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21239968358378758, 0.21239968358378775, 0.28555331903661596], 
reward next is 0.7144, 
noisyNet noise sample is [array([1.6231102], dtype=float32), -1.8194277]. 
=============================================
[2019-04-28 01:05:40,681] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-28 01:05:40,681] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:05:40,681] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:05:40,682] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:05:40,682] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:05:40,682] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:05:40,682] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:05:40,682] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:05:40,683] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:05:40,683] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:05:40,682] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:05:40,693] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run58
[2019-04-28 01:05:40,728] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run58
[2019-04-28 01:05:40,728] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run58
[2019-04-28 01:05:40,810] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run58
[2019-04-28 01:05:40,861] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run58
[2019-04-28 01:06:16,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.021684311]
[2019-04-28 01:06:16,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.587514, 89.78658001, 1.0, 2.0, 0.5442851307860318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 760576.0597917761, 760576.0597917761, 190824.0980213847]
[2019-04-28 01:06:16,128] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:06:16,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3589607e-14 9.9586099e-01 1.6761983e-22 4.1389847e-03 4.5303048e-18], sampled 0.635099937205535
[2019-04-28 01:06:29,082] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.021684311]
[2019-04-28 01:06:29,083] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.56666666666667, 70.33333333333334, 1.0, 2.0, 0.5287552030832836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738867.2189705841, 738867.2189705848, 188223.6225542702]
[2019-04-28 01:06:29,083] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:06:29,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.57052837e-15 9.96592224e-01 1.19678435e-23 3.40776285e-03
 5.49576628e-19], sampled 0.8075301159885453
[2019-04-28 01:06:55,953] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.021684311]
[2019-04-28 01:06:55,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.10772012, 86.5473359, 1.0, 2.0, 0.762466337451657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1065612.230707434, 1065612.230707434, 234925.398286714]
[2019-04-28 01:06:55,954] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:06:55,956] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.1809617e-14 9.9550796e-01 2.3475194e-21 4.4920039e-03 3.7458990e-17], sampled 0.6974569339006518
[2019-04-28 01:07:04,433] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.021684311]
[2019-04-28 01:07:04,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.69276815666667, 46.05995909166666, 1.0, 2.0, 0.2865377407392752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462978.8076331698, 462978.8076331698, 164472.087189861]
[2019-04-28 01:07:04,434] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:07:04,437] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2800932e-15 9.9625528e-01 6.2486221e-23 3.7447608e-03 2.0577307e-18], sampled 0.12471988530584688
[2019-04-28 01:07:04,721] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.021684311]
[2019-04-28 01:07:04,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.73333333333333, 47.0, 1.0, 2.0, 0.5773590861830135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 806810.6727555229, 806810.6727555223, 196604.4495825386]
[2019-04-28 01:07:04,722] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:07:04,724] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.9366403e-15 9.9616855e-01 7.1509522e-23 3.8314515e-03 2.2931242e-18], sampled 0.8229798357120376
[2019-04-28 01:07:17,219] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8451.4620 2844244922.5412 1124.0000
[2019-04-28 01:07:17,496] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8213.7130 2929292190.9698 1317.0000
[2019-04-28 01:07:17,611] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7963.7601 3009328354.6264 1743.0000
[2019-04-28 01:07:17,843] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8607.5462 2781821065.8202 917.0000
[2019-04-28 01:07:17,940] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7851.5343 3166036879.5480 1770.0000
[2019-04-28 01:07:18,965] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 1425000, evaluation results [1425000.0, 7851.534336808723, 3166036879.547994, 1770.0, 8213.713025588482, 2929292190.969821, 1317.0, 8607.546233301975, 2781821065.820243, 917.0, 7963.76012905171, 3009328354.6264005, 1743.0, 8451.461975324166, 2844244922.5412254, 1124.0]
[2019-04-28 01:07:20,649] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2378221e-16 9.9971753e-01 7.3037926e-26 2.8250340e-04 9.4466433e-21], sum to 1.0000
[2019-04-28 01:07:20,660] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7184
[2019-04-28 01:07:20,664] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 57.5, 1.0, 2.0, 0.5215649810163725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728816.3638848484, 728816.3638848478, 187044.1820415007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5743800.0000, 
sim time next is 5744400.0000, 
raw observation next is [32.4, 56.66666666666667, 1.0, 2.0, 0.5211209160873153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728195.6306460764, 728195.6306460764, 186971.8663684944], 
processed observation next is [0.0, 0.4782608695652174, 0.7345971563981042, 0.5666666666666668, 1.0, 1.0, 0.4230372482979702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20227656406835456, 0.20227656406835456, 0.2790624871171558], 
reward next is 0.7209, 
noisyNet noise sample is [array([-1.1195493], dtype=float32), -1.9452951]. 
=============================================
[2019-04-28 01:07:23,072] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.2062265e-14 9.9949300e-01 3.5943295e-24 5.0698401e-04 4.2507338e-20], sum to 1.0000
[2019-04-28 01:07:23,078] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.6535
[2019-04-28 01:07:23,085] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.48333333333333, 71.0, 1.0, 2.0, 0.5716510713747606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 798831.2035700041, 798831.2035700041, 195582.3402300803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5767800.0000, 
sim time next is 5768400.0000, 
raw observation next is [30.26666666666667, 72.0, 1.0, 2.0, 0.5582309294339238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 780070.8701151449, 780070.8701151442, 193222.2889117747], 
processed observation next is [0.0, 0.782608695652174, 0.6334913112164299, 0.72, 1.0, 1.0, 0.46774810775171544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21668635280976248, 0.21668635280976228, 0.2883914759877234], 
reward next is 0.7116, 
noisyNet noise sample is [array([0.8338755], dtype=float32), 0.82678324]. 
=============================================
[2019-04-28 01:07:25,039] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.4616951e-13 9.9740249e-01 2.0647941e-21 2.5974445e-03 5.9722112e-16], sum to 1.0000
[2019-04-28 01:07:25,046] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9281
[2019-04-28 01:07:25,050] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 87.0, 1.0, 2.0, 0.5132456136693408, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 717187.2512455037, 717187.2512455037, 185696.9988128121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5707200.0000, 
sim time next is 5707800.0000, 
raw observation next is [26.45, 87.0, 1.0, 2.0, 0.5122068554158608, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715735.2463210775, 715735.2463210775, 185530.3552951152], 
processed observation next is [0.0, 0.043478260869565216, 0.45260663507109006, 0.87, 1.0, 1.0, 0.41229741616368765, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19881534620029928, 0.19881534620029928, 0.27691097805241077], 
reward next is 0.7231, 
noisyNet noise sample is [array([0.63612044], dtype=float32), -0.9290443]. 
=============================================
[2019-04-28 01:07:30,382] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8165109e-10 9.4440186e-01 9.6136266e-15 5.5598181e-02 5.1848643e-12], sum to 1.0000
[2019-04-28 01:07:30,407] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2368
[2019-04-28 01:07:30,417] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 95.0, 1.0, 2.0, 0.8046205679501416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1124557.586466148, 1124557.586466147, 245046.9223346355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5889600.0000, 
sim time next is 5890200.0000, 
raw observation next is [25.68333333333333, 95.16666666666667, 1.0, 2.0, 0.6947619233359846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 970946.2365854465, 970946.2365854458, 219717.5590062633], 
processed observation next is [1.0, 0.17391304347826086, 0.41627172195892564, 0.9516666666666667, 1.0, 1.0, 0.6322432811276923, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26970728794040183, 0.2697072879404016, 0.3279366552332288], 
reward next is 0.6721, 
noisyNet noise sample is [array([-0.82002497], dtype=float32), -1.2082875]. 
=============================================
[2019-04-28 01:07:34,277] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.1003218e-13 9.9967086e-01 1.5762443e-18 3.2911191e-04 1.7120507e-15], sum to 1.0000
[2019-04-28 01:07:34,292] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3329
[2019-04-28 01:07:34,314] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2387460.863781739 W.
[2019-04-28 01:07:34,319] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.4, 79.0, 1.0, 2.0, 0.5690835056855807, 1.0, 2.0, 0.5690835056855807, 1.0, 1.0, 0.9883098505846775, 6.9112, 6.9112, 170.5573041426782, 2387460.863781739, 2387460.863781739, 466206.4269581707], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5936400.0000, 
sim time next is 5937000.0000, 
raw observation next is [30.3, 79.16666666666667, 1.0, 2.0, 0.2674097811271435, 1.0, 2.0, 0.2674097811271435, 1.0, 2.0, 0.4644023560518823, 6.9112, 6.9112, 170.5573041426782, 1121208.080525702, 1121208.080525702, 291646.0925952492], 
processed observation next is [1.0, 0.7391304347826086, 0.6350710900473934, 0.7916666666666667, 1.0, 1.0, 0.11736118208089574, 1.0, 1.0, 0.11736118208089574, 1.0, 1.0, 0.3468321415266857, 0.0, 0.0, 0.8375144448122397, 0.31144668903491723, 0.31144668903491723, 0.43529267551529727], 
reward next is 0.5647, 
noisyNet noise sample is [array([-0.76204175], dtype=float32), -0.8302008]. 
=============================================
[2019-04-28 01:07:34,336] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[50.047394]
 [49.95163 ]
 [49.061974]
 [49.358055]
 [49.00844 ]], R is [[52.86832047]
 [52.33963776]
 [52.1878891 ]
 [52.0074234 ]
 [51.81222153]].
[2019-04-28 01:07:34,586] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.5499590e-18 9.9998188e-01 2.3564063e-28 1.8094444e-05 3.0298428e-21], sum to 1.0000
[2019-04-28 01:07:34,598] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3780
[2019-04-28 01:07:34,603] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.01666666666667, 86.5, 1.0, 2.0, 0.5603987752780217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 783101.330594362, 783101.3305943614, 193600.1619550526], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5951400.0000, 
sim time next is 5952000.0000, 
raw observation next is [27.93333333333334, 87.0, 1.0, 2.0, 0.5608901216826657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 783788.1914569404, 783788.191456941, 193685.9157795505], 
processed observation next is [1.0, 0.9130434782608695, 0.5229067930489735, 0.87, 1.0, 1.0, 0.470951953834537, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21771894207137232, 0.2177189420713725, 0.2890834563873888], 
reward next is 0.7109, 
noisyNet noise sample is [array([-1.0344396], dtype=float32), -0.0867259]. 
=============================================
[2019-04-28 01:07:34,626] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[75.60132 ]
 [75.36692 ]
 [75.51464 ]
 [75.381996]
 [75.38888 ]], R is [[75.7693634 ]
 [75.72271729]
 [75.67713165]
 [75.632164  ]
 [75.58769989]].
[2019-04-28 01:07:37,088] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4809872e-16 9.6915340e-01 8.7140149e-25 3.0846663e-02 3.9104436e-19], sum to 1.0000
[2019-04-28 01:07:37,095] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0128
[2019-04-28 01:07:37,098] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.41666666666666, 91.00000000000001, 1.0, 2.0, 0.5217229452310012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729037.1732101989, 729037.1732101989, 187069.8875459699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6228600.0000, 
sim time next is 6229200.0000, 
raw observation next is [26.43333333333333, 91.0, 1.0, 2.0, 0.5218866625747017, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729266.0245607866, 729266.0245607866, 187096.631130235], 
processed observation next is [0.0, 0.08695652173913043, 0.4518167456556081, 0.91, 1.0, 1.0, 0.42395983442735147, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2025738957113296, 0.2025738957113296, 0.2792487031794552], 
reward next is 0.7208, 
noisyNet noise sample is [array([0.5653429], dtype=float32), -0.38604462]. 
=============================================
[2019-04-28 01:07:44,053] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.8039306e-18 9.9999595e-01 7.9721667e-26 4.0299637e-06 2.4960944e-20], sum to 1.0000
[2019-04-28 01:07:44,065] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9595
[2019-04-28 01:07:44,074] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 75.66666666666667, 1.0, 2.0, 0.5220087075708716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729436.6244912896, 729436.6244912896, 187116.3963388392], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6337200.0000, 
sim time next is 6337800.0000, 
raw observation next is [28.85, 75.0, 1.0, 2.0, 0.5235939093401797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 731652.492605104, 731652.4926051034, 187375.4744279057], 
processed observation next is [0.0, 0.34782608695652173, 0.5663507109004741, 0.75, 1.0, 1.0, 0.4260167582411803, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2032368035014178, 0.20323680350141762, 0.2796648872058294], 
reward next is 0.7203, 
noisyNet noise sample is [array([1.6658523], dtype=float32), 0.5208126]. 
=============================================
[2019-04-28 01:07:52,959] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.9185600e-19 1.0000000e+00 3.4593024e-28 5.2757931e-09 4.8632675e-23], sum to 1.0000
[2019-04-28 01:07:52,975] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0082
[2019-04-28 01:07:53,005] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 87.5, 1.0, 2.0, 0.5321549901357916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743619.6459154987, 743619.6459154981, 188787.3500403001], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6478200.0000, 
sim time next is 6478800.0000, 
raw observation next is [27.0, 87.66666666666667, 1.0, 2.0, 0.5308826972178836, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741841.1558172234, 741841.1558172228, 188576.1381549454], 
processed observation next is [1.0, 1.0, 0.4786729857819906, 0.8766666666666667, 1.0, 1.0, 0.43479843038299226, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2060669877270065, 0.20606698772700632, 0.2814569226193215], 
reward next is 0.7185, 
noisyNet noise sample is [array([-1.2526804], dtype=float32), 0.9205376]. 
=============================================
[2019-04-28 01:08:02,969] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.5494680e-16 1.0000000e+00 1.4330129e-23 6.7022388e-10 3.8316735e-19], sum to 1.0000
[2019-04-28 01:08:02,976] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6538
[2019-04-28 01:08:02,978] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 85.66666666666666, 1.0, 2.0, 0.8356262942694533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1167915.787792461, 1167915.787792461, 252841.8606759437], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6406800.0000, 
sim time next is 6407400.0000, 
raw observation next is [26.73333333333333, 85.83333333333334, 1.0, 2.0, 0.820116806739823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1146227.202344312, 1146227.202344312, 248906.8426001346], 
processed observation next is [1.0, 0.13043478260869565, 0.4660347551342811, 0.8583333333333334, 1.0, 1.0, 0.7832732611323169, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31839644509564224, 0.31839644509564224, 0.37150275014945466], 
reward next is 0.6285, 
noisyNet noise sample is [array([-1.1753881], dtype=float32), -0.06963281]. 
=============================================
[2019-04-28 01:08:04,454] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.5232483e-18 1.0000000e+00 4.9906419e-27 2.0590168e-12 8.1603362e-22], sum to 1.0000
[2019-04-28 01:08:04,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8411
[2019-04-28 01:08:04,468] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 83.0, 1.0, 2.0, 0.5120595916267434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 715529.397097342, 715529.3970973414, 185506.7909953896], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6399000.0000, 
sim time next is 6399600.0000, 
raw observation next is [27.03333333333333, 83.0, 1.0, 2.0, 0.5110430279115776, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 714108.4185074257, 714108.4185074263, 185344.0614224369], 
processed observation next is [1.0, 0.043478260869565216, 0.48025276461295413, 0.83, 1.0, 1.0, 0.41089521435129833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19836344958539603, 0.1983634495853962, 0.27663292749617446], 
reward next is 0.7234, 
noisyNet noise sample is [array([0.38150412], dtype=float32), -0.5359479]. 
=============================================
[2019-04-28 01:08:08,177] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.4327696e-16 1.0000000e+00 4.4438029e-26 1.1148954e-08 2.0228969e-19], sum to 1.0000
[2019-04-28 01:08:08,183] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2910
[2019-04-28 01:08:08,190] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.95, 86.66666666666666, 1.0, 2.0, 0.5201588571803704, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726850.8241783377, 726850.8241783377, 186814.997434339], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6565800.0000, 
sim time next is 6566400.0000, 
raw observation next is [26.9, 87.0, 1.0, 2.0, 0.5200690135913529, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726725.237111787, 726725.237111787, 186800.3863176981], 
processed observation next is [1.0, 0.0, 0.4739336492890995, 0.87, 1.0, 1.0, 0.4217698958931963, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20186812141994082, 0.20186812141994082, 0.278806546742833], 
reward next is 0.7212, 
noisyNet noise sample is [array([1.2714334], dtype=float32), -0.77929604]. 
=============================================
[2019-04-28 01:08:12,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.3905401e-15 1.0000000e+00 2.8781185e-22 5.6330673e-09 7.7902024e-17], sum to 1.0000
[2019-04-28 01:08:12,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6132
[2019-04-28 01:08:12,224] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 88.33333333333334, 1.0, 2.0, 0.6823152980453945, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 953543.9740699849, 953543.9740699856, 217072.957834213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6591000.0000, 
sim time next is 6591600.0000, 
raw observation next is [26.6, 88.0, 1.0, 2.0, 0.6854201674262496, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 957885.0238343929, 957885.0238343923, 217728.3920143394], 
processed observation next is [1.0, 0.30434782608695654, 0.4597156398104266, 0.88, 1.0, 1.0, 0.6209881535256019, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2660791732873314, 0.2660791732873312, 0.32496774927513344], 
reward next is 0.6750, 
noisyNet noise sample is [array([-0.9403744], dtype=float32), 0.5749392]. 
=============================================
[2019-04-28 01:08:14,825] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1038836e-16 1.0000000e+00 9.1129543e-24 6.1108940e-09 6.7839512e-20], sum to 1.0000
[2019-04-28 01:08:14,848] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9507
[2019-04-28 01:08:14,855] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2258382.130983005 W.
[2019-04-28 01:08:14,865] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 66.0, 1.0, 2.0, 0.9738365965146909, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.993271474307753, 6.9112, 168.9124685323034, 2258382.130983005, 2200157.970633091, 455836.39833862], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6610200.0000, 
sim time next is 6610800.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.422900509945684, 1.0, 1.0, 0.422900509945684, 1.0, 2.0, 0.7309117200176771, 6.9112, 6.9112, 170.5573041426782, 1773696.202863333, 1773696.202863333, 365402.2831299265], 
processed observation next is [1.0, 0.5217391304347826, 0.6682464454976303, 0.66, 1.0, 1.0, 0.3046994095731133, 1.0, 0.5, 0.3046994095731133, 1.0, 1.0, 0.6718435609971672, 0.0, 0.0, 0.8375144448122397, 0.4926933896842592, 0.4926933896842592, 0.5453765419849649], 
reward next is 0.0000, 
noisyNet noise sample is [array([-2.251884], dtype=float32), -2.3968487]. 
=============================================
[2019-04-28 01:08:16,860] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.01174315e-19 1.00000000e+00 3.09197911e-29 9.94698421e-12
 1.47621422e-22], sum to 1.0000
[2019-04-28 01:08:16,861] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7642
[2019-04-28 01:08:16,867] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 85.33333333333334, 1.0, 2.0, 0.5140166846084263, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 718265.0767454109, 718265.0767454115, 185821.3811270893], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6639600.0000, 
sim time next is 6640200.0000, 
raw observation next is [26.9, 85.5, 1.0, 2.0, 0.5137748956836732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717927.0970003083, 717927.0970003078, 185782.4968478045], 
processed observation next is [1.0, 0.8695652173913043, 0.4739336492890995, 0.855, 1.0, 1.0, 0.41418662130563033, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19942419361119676, 0.1994241936111966, 0.2772873087280664], 
reward next is 0.7227, 
noisyNet noise sample is [array([0.16710468], dtype=float32), -0.33749577]. 
=============================================
[2019-04-28 01:08:19,645] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-28 01:08:19,649] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:08:19,649] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:08:19,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:08:19,650] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:08:19,651] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:08:19,652] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:08:19,652] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:08:19,653] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:08:19,654] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:08:19,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:08:19,672] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run59
[2019-04-28 01:08:19,712] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run59
[2019-04-28 01:08:19,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run59
[2019-04-28 01:08:19,778] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run59
[2019-04-28 01:08:19,842] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run59
[2019-04-28 01:08:24,733] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:08:24,734] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.86666666666667, 78.33333333333334, 1.0, 2.0, 0.3051888988695611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484268.9324126781, 484268.9324126774, 165899.7505884923]
[2019-04-28 01:08:24,735] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:08:24,737] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.1109764e-20 1.0000000e+00 1.7344299e-29 2.3849245e-11 2.9630512e-23], sampled 0.5203384623139024
[2019-04-28 01:08:25,186] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:08:25,201] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.5, 54.83333333333333, 1.0, 2.0, 0.2092848977704102, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 349296.7506862957, 349296.7506862951, 156437.5121361042]
[2019-04-28 01:08:25,202] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:08:25,215] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1008977e-19 1.0000000e+00 1.1279743e-28 4.2227600e-11 1.2882648e-22], sampled 0.05258027440926416
[2019-04-28 01:08:32,485] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:08:32,486] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [20.46666666666667, 82.83333333333334, 1.0, 2.0, 0.2789059712361553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456891.1233104236, 456891.1233104236, 163895.0208436498]
[2019-04-28 01:08:32,488] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:08:32,491] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.0575798e-20 1.0000000e+00 6.1043749e-30 1.6541518e-11 1.3064963e-23], sampled 0.7768730059523951
[2019-04-28 01:08:38,282] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:08:38,283] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.706101995, 94.38144183333333, 1.0, 2.0, 0.2760060383829633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448433.0399887825, 448433.0399887831, 163466.2300905165]
[2019-04-28 01:08:38,284] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:08:38,286] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.32364480e-20 1.00000000e+00 1.73583878e-30 1.01884655e-11
 4.87498527e-24], sampled 0.7977796856240459
[2019-04-28 01:09:01,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:01,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.83333333333334, 79.83333333333334, 1.0, 2.0, 0.5566791690368277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777901.6489690338, 777901.6489690333, 192952.5473174102]
[2019-04-28 01:09:01,408] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:09:01,411] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0969053e-19 1.0000000e+00 4.2142889e-29 3.1073852e-11 5.9384950e-23], sampled 0.5254834655972381
[2019-04-28 01:09:08,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:08,480] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.35752870666667, 85.23846803166667, 1.0, 2.0, 0.535857550101533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 748795.3312470567, 748795.3312470574, 189402.0649628919]
[2019-04-28 01:09:08,482] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:09:08,484] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2664523e-20 1.0000000e+00 3.8887715e-30 1.4135601e-11 9.1666118e-24], sampled 0.1639251756615242
[2019-04-28 01:09:10,284] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:10,286] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 69.5, 1.0, 2.0, 0.9185798849338402, 1.0, 2.0, 0.9185798849338402, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2569315.913408739, 2569315.913408739, 481735.0578362709]
[2019-04-28 01:09:10,288] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:09:10,290] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.6368028e-21 1.0000000e+00 1.5281304e-31 4.3605770e-12 7.2008836e-25], sampled 0.9541396127084396
[2019-04-28 01:09:10,291] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2569315.913408739 W.
[2019-04-28 01:09:16,304] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:16,305] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.73180748833333, 85.03892808500001, 1.0, 2.0, 0.5136353355987093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717732.0158147009, 717732.0158147003, 185759.5367900772]
[2019-04-28 01:09:16,306] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:09:16,311] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2108426e-19 1.0000000e+00 4.8919439e-29 3.2368441e-11 6.6950657e-23], sampled 0.2942781355349027
[2019-04-28 01:09:30,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:30,966] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.5, 76.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 10.0739548631696, 6.9112, 168.8949309045779, 3698673.011771567, 1455145.302622348, 306049.6748903951]
[2019-04-28 01:09:30,967] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:09:30,970] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2176239e-15 1.0000000e+00 1.2812319e-22 6.4449797e-09 7.2706595e-18], sampled 0.604651537163283
[2019-04-28 01:09:30,971] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 0, 1] for the demand 3698673.011771567 W.
[2019-04-28 01:09:48,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:48,272] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.05727597666667, 71.63222812333333, 1.0, 2.0, 0.3777608638548565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569177.4447443136, 569177.4447443143, 171920.4963272333]
[2019-04-28 01:09:48,273] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:09:48,276] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.5461528e-19 1.0000000e+00 2.5496597e-28 4.4559571e-11 2.4558853e-22], sampled 0.5994443475401381
[2019-04-28 01:09:52,823] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:09:53,454] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:09:53,616] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:09:53,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017219344]
[2019-04-28 01:09:53,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.3, 89.0, 1.0, 2.0, 0.62097983022792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 867791.8885017479, 867791.8885017479, 204711.8173570828]
[2019-04-28 01:09:53,752] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:09:53,753] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.1177146e-19 1.0000000e+00 1.1196832e-28 4.9147152e-11 1.2795762e-22], sampled 0.9053143976756769
[2019-04-28 01:09:53,758] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:09:53,997] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:09:55,012] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1450000, evaluation results [1450000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:09:55,302] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.2045637e-18 1.0000000e+00 2.5271944e-28 1.2544911e-12 1.4720473e-22], sum to 1.0000
[2019-04-28 01:09:55,310] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3206
[2019-04-28 01:09:55,316] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 89.0, 1.0, 2.0, 0.4989342556943306, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 697182.6152101486, 697182.6152101492, 183429.3100557103], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6652800.0000, 
sim time next is 6653400.0000, 
raw observation next is [25.91666666666667, 89.5, 1.0, 2.0, 0.4981941303632086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 696148.0668973145, 696148.0668973145, 183313.6742048307], 
processed observation next is [1.0, 0.0, 0.4273301737756717, 0.895, 1.0, 1.0, 0.395414614895432, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19337446302703182, 0.19337446302703182, 0.27360249881318016], 
reward next is 0.7264, 
noisyNet noise sample is [array([0.71654594], dtype=float32), -0.65882295]. 
=============================================
[2019-04-28 01:09:58,146] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.0110030e-20 1.0000000e+00 1.3469323e-28 2.3779238e-12 5.5462952e-24], sum to 1.0000
[2019-04-28 01:09:58,154] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6839
[2019-04-28 01:09:58,156] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 49.33333333333334, 1.0, 2.0, 0.9085777504663121, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1404029.505111161, 1404029.505111161, 290895.8092599801], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6798000.0000, 
sim time next is 6798600.0000, 
raw observation next is [29.15, 49.5, 1.0, 2.0, 0.932907198460317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1440898.914236708, 1440898.914236708, 298401.7275727542], 
processed observation next is [1.0, 0.6956521739130435, 0.5805687203791469, 0.495, 1.0, 1.0, 0.9191652993497795, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.4002496983990856, 0.4002496983990856, 0.44537571279515553], 
reward next is 0.5546, 
noisyNet noise sample is [array([1.5998129], dtype=float32), 1.6016611]. 
=============================================
[2019-04-28 01:10:05,913] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2079445e-21 1.0000000e+00 3.2851588e-30 5.9238048e-10 1.6609105e-23], sum to 1.0000
[2019-04-28 01:10:05,919] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1776
[2019-04-28 01:10:05,921] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666667, 65.66666666666667, 1.0, 2.0, 0.3727971163437828, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 565719.840333914, 565719.840333914, 171743.2627513431], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6898800.0000, 
sim time next is 6899400.0000, 
raw observation next is [26.6, 66.5, 1.0, 2.0, 0.3763526246168568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 569809.6062249024, 569809.606224903, 172061.2672329463], 
processed observation next is [0.0, 0.8695652173913043, 0.4597156398104266, 0.665, 1.0, 1.0, 0.2486176200203094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.158280446173584, 0.15828044617358417, 0.25680786154171087], 
reward next is 0.7432, 
noisyNet noise sample is [array([-0.2750112], dtype=float32), -0.72591156]. 
=============================================
[2019-04-28 01:10:08,228] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.2862213e-22 1.0000000e+00 5.7820641e-30 8.1267144e-13 1.1152377e-27], sum to 1.0000
[2019-04-28 01:10:08,237] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1763
[2019-04-28 01:10:08,242] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 88.66666666666667, 1.0, 2.0, 0.4221723106715592, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 619006.1099668499, 619006.1099668499, 176022.4753719938], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6924000.0000, 
sim time next is 6924600.0000, 
raw observation next is [24.1, 89.0, 1.0, 2.0, 0.4201001189778401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616474.4550568531, 616474.4550568538, 175793.987232664], 
processed observation next is [0.0, 0.13043478260869565, 0.3412322274881518, 0.89, 1.0, 1.0, 0.30132544455161464, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17124290418245922, 0.17124290418245938, 0.26237908542188654], 
reward next is 0.7376, 
noisyNet noise sample is [array([2.1006615], dtype=float32), -0.96719885]. 
=============================================
[2019-04-28 01:10:09,179] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.8644068e-20 1.0000000e+00 3.0225865e-31 2.2985256e-11 6.3178580e-22], sum to 1.0000
[2019-04-28 01:10:09,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5992
[2019-04-28 01:10:09,197] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.06666666666667, 72.33333333333333, 1.0, 2.0, 0.3956558540815243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590778.5889644243, 590778.5889644243, 173701.1333464653], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6903600.0000, 
sim time next is 6904200.0000, 
raw observation next is [25.98333333333333, 73.16666666666667, 1.0, 2.0, 0.3983812793188612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593862.1654945082, 593862.1654945075, 173955.9070935067], 
processed observation next is [0.0, 0.9130434782608695, 0.43048973143759867, 0.7316666666666667, 1.0, 1.0, 0.2751581678540496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1649617126373634, 0.1649617126373632, 0.2596356822291145], 
reward next is 0.7404, 
noisyNet noise sample is [array([0.10306378], dtype=float32), 0.29363438]. 
=============================================
[2019-04-28 01:10:18,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.0066683e-16 1.0000000e+00 1.2045102e-23 1.0732606e-08 1.0806052e-17], sum to 1.0000
[2019-04-28 01:10:18,115] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5006
[2019-04-28 01:10:18,125] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 84.0, 1.0, 2.0, 0.7503280934906575, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1048639.615524592, 1048639.615524593, 232085.2367678844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7200600.0000, 
sim time next is 7201200.0000, 
raw observation next is [28.46666666666667, 84.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.55787012185742, 6.9112, 168.9097407400411, 1912830.466628258, 1454069.159593734, 311352.5832319461], 
processed observation next is [1.0, 0.34782608695652173, 0.5481832543443919, 0.84, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.06466701218574196, 0.0, 0.8294241542475022, 0.5313417962856272, 0.4039080998871483, 0.46470534810738223], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.17643045], dtype=float32), 0.8083216]. 
=============================================
[2019-04-28 01:10:22,875] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1308704e-19 1.0000000e+00 6.6596575e-31 7.2416219e-11 1.8999675e-25], sum to 1.0000
[2019-04-28 01:10:22,885] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6999
[2019-04-28 01:10:22,890] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 81.0, 1.0, 2.0, 0.4831327692617746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 675095.4923229417, 675095.4923229422, 180995.5421858316], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7067400.0000, 
sim time next is 7068000.0000, 
raw observation next is [26.63333333333333, 82.0, 1.0, 2.0, 0.4842626667678272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 676674.833879201, 676674.8338792004, 181167.0911568115], 
processed observation next is [1.0, 0.8260869565217391, 0.46129541864139006, 0.82, 1.0, 1.0, 0.3786297189973822, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18796523163311138, 0.1879652316331112, 0.2703986435176291], 
reward next is 0.7296, 
noisyNet noise sample is [array([-0.40872857], dtype=float32), -0.84236383]. 
=============================================
[2019-04-28 01:10:22,909] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[75.566986]
 [75.7246  ]
 [75.54568 ]
 [75.41204 ]
 [75.60124 ]], R is [[75.1727066 ]
 [75.15084076]
 [75.12940979]
 [75.108284  ]
 [75.08753204]].
[2019-04-28 01:10:24,396] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.6217180e-18 1.0000000e+00 5.0924411e-24 2.8512970e-10 2.7808996e-21], sum to 1.0000
[2019-04-28 01:10:24,403] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.9283
[2019-04-28 01:10:24,409] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.8, 89.0, 1.0, 2.0, 0.5763088049091502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 805342.4367698091, 805342.4367698084, 196409.3213774699], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7182000.0000, 
sim time next is 7182600.0000, 
raw observation next is [25.8, 89.16666666666667, 1.0, 2.0, 0.6429331572530604, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 898483.67006563, 898483.67006563, 209006.9241447223], 
processed observation next is [1.0, 0.13043478260869565, 0.42180094786729866, 0.8916666666666667, 1.0, 1.0, 0.5697989846422414, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2495787972404528, 0.2495787972404528, 0.3119506330518243], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.6674179], dtype=float32), -0.7441317]. 
=============================================
[2019-04-28 01:10:27,299] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.0790053e-16 1.0000000e+00 4.5558579e-24 3.5294462e-11 3.7887796e-20], sum to 1.0000
[2019-04-28 01:10:27,304] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3751
[2019-04-28 01:10:27,310] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.93333333333333, 84.66666666666667, 1.0, 2.0, 0.6808634087049914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 951514.0320012324, 951514.0320012324, 216769.8172252924], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7198800.0000, 
sim time next is 7199400.0000, 
raw observation next is [28.06666666666667, 84.33333333333333, 1.0, 2.0, 0.7052206546148992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 985569.34343799, 985569.34343799, 221977.3092704298], 
processed observation next is [1.0, 0.30434782608695654, 0.529225908372828, 0.8433333333333333, 1.0, 1.0, 0.6448441621866255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.27376926206610835, 0.27376926206610835, 0.33130941682153703], 
reward next is 0.6687, 
noisyNet noise sample is [array([-1.1448579], dtype=float32), 1.1873635]. 
=============================================
[2019-04-28 01:10:30,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1367170e-21 1.0000000e+00 1.1656144e-29 3.3962697e-14 2.3830024e-25], sum to 1.0000
[2019-04-28 01:10:30,445] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0497
[2019-04-28 01:10:30,449] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666666, 74.0, 1.0, 2.0, 0.7306945428776342, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1131166.859759534, 1131166.859759534, 241497.8793449228], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7292400.0000, 
sim time next is 7293000.0000, 
raw observation next is [24.78333333333333, 73.0, 1.0, 2.0, 0.7518275171007635, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1161936.169716239, 1161936.169716239, 246643.2278884322], 
processed observation next is [1.0, 0.391304347826087, 0.37361769352290675, 0.73, 1.0, 1.0, 0.7009970085551367, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.32276004714339973, 0.32276004714339973, 0.36812422072900325], 
reward next is 0.6319, 
noisyNet noise sample is [array([0.6908649], dtype=float32), 0.11806904]. 
=============================================
[2019-04-28 01:10:30,470] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[71.83666 ]
 [71.96945 ]
 [71.99479 ]
 [72.04805 ]
 [71.897385]], R is [[71.64235687]
 [71.56549072]
 [71.5045166 ]
 [71.45466614]
 [71.41757202]].
[2019-04-28 01:10:31,601] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.1833830e-21 1.0000000e+00 6.2209004e-31 5.4599429e-13 3.4639065e-23], sum to 1.0000
[2019-04-28 01:10:31,610] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1900
[2019-04-28 01:10:31,614] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.46666666666667, 90.66666666666667, 1.0, 2.0, 0.3311335112969416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 522813.9779869437, 522813.9779869437, 168755.0481316826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7274400.0000, 
sim time next is 7275000.0000, 
raw observation next is [21.43333333333333, 90.83333333333334, 1.0, 2.0, 0.3283024194652454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 518500.3955674687, 518500.3955674687, 168422.2432845208], 
processed observation next is [1.0, 0.17391304347826086, 0.21484992101105835, 0.9083333333333334, 1.0, 1.0, 0.190725806584633, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1440278876576302, 0.1440278876576302, 0.2513764825142102], 
reward next is 0.7486, 
noisyNet noise sample is [array([-0.62543076], dtype=float32), 0.7861618]. 
=============================================
[2019-04-28 01:10:31,639] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[68.634026]
 [68.52269 ]
 [68.51843 ]
 [68.39614 ]
 [68.44072 ]], R is [[68.71896362]
 [68.7798996 ]
 [68.83938599]
 [68.89951324]
 [68.95323181]].
[2019-04-28 01:10:35,108] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4448534e-22 1.0000000e+00 8.3639503e-32 1.3571677e-13 1.0232177e-24], sum to 1.0000
[2019-04-28 01:10:35,116] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7288
[2019-04-28 01:10:35,122] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.5, 93.0, 1.0, 2.0, 0.4091832003651533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 602415.1954450014, 602415.1954450014, 174523.8407494047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7524000.0000, 
sim time next is 7524600.0000, 
raw observation next is [23.46666666666667, 92.83333333333333, 1.0, 2.0, 0.407581719654442, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600886.2128957993, 600886.2128957993, 174406.3971178725], 
processed observation next is [0.0, 0.08695652173913043, 0.31121642969984215, 0.9283333333333332, 1.0, 1.0, 0.2862430357282434, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1669128369154998, 0.1669128369154998, 0.26030805539980967], 
reward next is 0.7397, 
noisyNet noise sample is [array([1.8894498], dtype=float32), 0.7898841]. 
=============================================
[2019-04-28 01:10:46,572] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-28 01:10:46,573] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:10:46,574] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:10:46,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:10:46,578] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:10:46,576] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:10:46,579] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:10:46,581] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:10:46,581] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:10:46,584] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:10:46,586] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:10:46,607] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run60
[2019-04-28 01:10:46,641] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run60
[2019-04-28 01:10:46,668] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run60
[2019-04-28 01:10:46,695] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run60
[2019-04-28 01:10:46,723] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run60
[2019-04-28 01:10:50,730] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.012850274]
[2019-04-28 01:10:50,731] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.95451667333333, 88.91088756, 1.0, 2.0, 0.2812830689344668, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456606.435363916, 456606.435363916, 164015.3748723328]
[2019-04-28 01:10:50,732] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:10:50,736] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.1391660e-22 1.0000000e+00 4.7619282e-32 1.2739080e-13 1.6514258e-25], sampled 0.4691216141552844
[2019-04-28 01:11:26,626] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.012850274]
[2019-04-28 01:11:26,627] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.75380821, 80.87830726, 1.0, 2.0, 0.5697681047688495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796198.9400845289, 796198.9400845289, 195247.8327685684]
[2019-04-28 01:11:26,629] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:11:26,631] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.0034807e-21 1.0000000e+00 4.8407779e-31 3.3172488e-13 1.0361641e-24], sampled 0.6940228330478593
[2019-04-28 01:11:27,210] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.012850274]
[2019-04-28 01:11:27,212] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [26.962412195, 77.14844534, 1.0, 2.0, 0.5972310364610534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 844176.8472977718, 844176.8472977718, 201468.683680915]
[2019-04-28 01:11:27,213] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:11:27,214] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.7723332e-21 1.0000000e+00 4.0429247e-31 3.0788395e-13 8.9817681e-25], sampled 0.08565990517931121
[2019-04-28 01:11:34,165] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.012850274]
[2019-04-28 01:11:34,168] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.9, 54.0, 1.0, 2.0, 0.7608590222487376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1063364.744856178, 1063364.744856178, 234538.5770678313]
[2019-04-28 01:11:34,169] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:11:34,171] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.3171566e-22 1.0000000e+00 1.5704797e-31 2.0836445e-13 4.2463461e-25], sampled 0.10134377864844735
[2019-04-28 01:11:40,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.012850274]
[2019-04-28 01:11:40,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.5208317382904694, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 727791.405560802, 727791.4055608013, 186924.6560493465]
[2019-04-28 01:11:40,929] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:11:40,933] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.5538836e-22 1.0000000e+00 1.3847703e-31 1.9787581e-13 3.8436661e-25], sampled 0.26495119398450306
[2019-04-28 01:12:11,019] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:12:11,024] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:12:11,050] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:12:11,111] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:12:11,192] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:12:12,208] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 1475000, evaluation results [1475000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:12:13,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.2234958e-21 1.0000000e+00 8.3782015e-30 4.0782299e-14 1.0337839e-23], sum to 1.0000
[2019-04-28 01:12:13,227] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6389
[2019-04-28 01:12:13,237] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 88.0, 1.0, 2.0, 0.4853202563005086, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678153.1073449855, 678153.1073449855, 181327.514845985], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7684800.0000, 
sim time next is 7685400.0000, 
raw observation next is [25.53333333333333, 88.0, 1.0, 2.0, 0.4831288606916362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675090.0290286097, 675090.0290286097, 180994.4590735345], 
processed observation next is [1.0, 0.9565217391304348, 0.4091627172195892, 0.88, 1.0, 1.0, 0.37726368758028456, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1875250080635027, 0.1875250080635027, 0.27014098369184253], 
reward next is 0.7299, 
noisyNet noise sample is [array([0.803457], dtype=float32), -0.5895684]. 
=============================================
[2019-04-28 01:12:14,090] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3419108e-23 1.0000000e+00 1.6286717e-33 1.6889742e-13 2.4294108e-24], sum to 1.0000
[2019-04-28 01:12:14,095] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5013
[2019-04-28 01:12:14,101] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 87.0, 1.0, 2.0, 0.521490308296969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 728711.9830900668, 728711.9830900675, 187031.7457191566], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7765200.0000, 
sim time next is 7765800.0000, 
raw observation next is [26.86666666666667, 87.33333333333333, 1.0, 2.0, 0.5220659538473954, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 729516.6458953219, 729516.6458953219, 187125.6552924545], 
processed observation next is [1.0, 0.9130434782608695, 0.4723538704581361, 0.8733333333333333, 1.0, 1.0, 0.4241758480089101, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20264351274870052, 0.20264351274870052, 0.27929202282455895], 
reward next is 0.7207, 
noisyNet noise sample is [array([-0.06919977], dtype=float32), 0.60568845]. 
=============================================
[2019-04-28 01:12:14,770] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.9569974e-19 1.0000000e+00 5.0398837e-28 5.8540252e-12 5.5602824e-22], sum to 1.0000
[2019-04-28 01:12:14,777] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7069
[2019-04-28 01:12:14,783] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 87.0, 1.0, 2.0, 0.4894267325398958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683893.0615516657, 683893.0615516651, 181955.9810322973], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7590600.0000, 
sim time next is 7591200.0000, 
raw observation next is [25.93333333333333, 87.33333333333334, 1.0, 2.0, 0.488751183620441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 682948.7900184183, 682948.7900184183, 181852.3147598252], 
processed observation next is [0.0, 0.8695652173913043, 0.42812006319115314, 0.8733333333333334, 1.0, 1.0, 0.3840375706270373, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18970799722733842, 0.18970799722733842, 0.27142136531317196], 
reward next is 0.7286, 
noisyNet noise sample is [array([0.34200448], dtype=float32), -0.42899486]. 
=============================================
[2019-04-28 01:12:19,958] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:19,959] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:20,025] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run8
[2019-04-28 01:12:22,645] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.9284413e-14 1.0000000e+00 2.6832847e-22 2.7012614e-08 4.0524166e-18], sum to 1.0000
[2019-04-28 01:12:22,652] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2419
[2019-04-28 01:12:22,662] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2305389.03070673 W.
[2019-04-28 01:12:22,669] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.48333333333333, 60.16666666666666, 1.0, 2.0, 0.5495386052578373, 1.0, 2.0, 0.5495386052578373, 1.0, 2.0, 0.9440377446433568, 6.9112, 6.9112, 170.5573041426782, 2305389.03070673, 2305389.03070673, 449012.2331252858], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7744200.0000, 
sim time next is 7744800.0000, 
raw observation next is [31.36666666666667, 60.33333333333334, 1.0, 2.0, 0.8571395145569499, 1.0, 2.0, 0.8571395145569499, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2397299.134791138, 2397299.134791139, 448639.2492085979], 
processed observation next is [1.0, 0.6521739130434783, 0.6856240126382308, 0.6033333333333334, 1.0, 1.0, 0.8278789332011445, 1.0, 1.0, 0.8278789332011445, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6659164263308717, 0.665916426330872, 0.6696108197143252], 
reward next is 0.3304, 
noisyNet noise sample is [array([-0.8284493], dtype=float32), -1.3073956]. 
=============================================
[2019-04-28 01:12:22,887] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.6962410e-17 1.0000000e+00 8.8094470e-27 1.6097044e-09 9.5422610e-21], sum to 1.0000
[2019-04-28 01:12:22,896] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2789
[2019-04-28 01:12:22,901] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.41666666666666, 77.66666666666667, 1.0, 2.0, 0.518541432997288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724589.924356995, 724589.924356995, 186552.6544562156], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7931400.0000, 
sim time next is 7932000.0000, 
raw observation next is [28.33333333333334, 78.33333333333334, 1.0, 2.0, 0.520807005037641, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 727756.832369921, 727756.832369921, 186920.7514987606], 
processed observation next is [1.0, 0.8260869565217391, 0.5418641390205374, 0.7833333333333334, 1.0, 1.0, 0.4226590422140253, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20215467565831138, 0.20215467565831138, 0.2789861962668069], 
reward next is 0.7210, 
noisyNet noise sample is [array([0.09742128], dtype=float32), 0.078824766]. 
=============================================
[2019-04-28 01:12:22,922] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[63.024   ]
 [62.85099 ]
 [63.16395 ]
 [63.178745]
 [63.354446]], R is [[63.0004921 ]
 [63.09204865]
 [63.18339157]
 [63.27461624]
 [63.36572266]].
[2019-04-28 01:12:23,254] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3608690e-15 9.9999893e-01 4.4581524e-23 1.0750297e-06 3.5583616e-17], sum to 1.0000
[2019-04-28 01:12:23,262] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.7561
[2019-04-28 01:12:23,268] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 83.66666666666667, 1.0, 2.0, 0.6462760089666977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 903157.2114684619, 903157.2114684612, 209674.6952352519], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7888200.0000, 
sim time next is 7888800.0000, 
raw observation next is [27.1, 83.33333333333334, 1.0, 2.0, 0.661616269386269, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924604.2363353607, 924604.2363353601, 212776.644352342], 
processed observation next is [1.0, 0.30434782608695654, 0.4834123222748816, 0.8333333333333335, 1.0, 1.0, 0.5923087582967097, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2568345100931557, 0.25683451009315555, 0.3175770811228985], 
reward next is 0.6824, 
noisyNet noise sample is [array([-1.6681238], dtype=float32), -0.14558817]. 
=============================================
[2019-04-28 01:12:24,801] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:24,801] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:24,851] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run8
[2019-04-28 01:12:25,281] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:25,282] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:25,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run8
[2019-04-28 01:12:26,951] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.5826294e-14 9.9999881e-01 3.4975940e-20 1.2093777e-06 7.6738406e-16], sum to 1.0000
[2019-04-28 01:12:26,959] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6676
[2019-04-28 01:12:26,964] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2152477.168266934 W.
[2019-04-28 01:12:26,966] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.65, 69.0, 1.0, 2.0, 0.8981715395203463, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.993415544592528, 6.9112, 168.9124006921486, 2152477.168266934, 2094150.823219321, 434023.9704957907], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7828200.0000, 
sim time next is 7828800.0000, 
raw observation next is [30.76666666666667, 69.33333333333333, 1.0, 2.0, 0.5216535260577656, 1.0, 1.0, 0.5216535260577656, 1.0, 2.0, 0.9059396613051113, 6.911199999999999, 6.9112, 170.5573041426782, 2188301.373283818, 2188301.373283818, 430442.6405641672], 
processed observation next is [1.0, 0.6086956521739131, 0.6571879936808849, 0.6933333333333332, 1.0, 1.0, 0.4236789470575489, 1.0, 0.5, 0.4236789470575489, 1.0, 1.0, 0.8852922698842819, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6078614925788383, 0.6078614925788383, 0.6424517023345778], 
reward next is 0.3575, 
noisyNet noise sample is [array([-0.3630763], dtype=float32), -0.08173884]. 
=============================================
[2019-04-28 01:12:27,452] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:27,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:27,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run8
[2019-04-28 01:12:27,549] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.8690180e-18 9.9999988e-01 1.6588697e-25 6.7109561e-08 4.2958539e-20], sum to 1.0000
[2019-04-28 01:12:27,559] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6231
[2019-04-28 01:12:27,562] A3C_AGENT_WORKER-Thread-15 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2513493.321240344 W.
[2019-04-28 01:12:27,568] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 64.16666666666666, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.216428869522133, 6.9112, 168.911111899659, 2513493.321240344, 2296955.820753934, 476069.7595501192], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7833000.0000, 
sim time next is 7833600.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.8375465039066706, 1.0, 1.0, 0.8375465039066706, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2342448.869048224, 2342448.869048224, 438557.6949034031], 
processed observation next is [1.0, 0.6956521739130435, 0.6682464454976303, 0.63, 1.0, 1.0, 0.8042728962730972, 1.0, 0.5, 0.8042728962730972, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6506802414022844, 0.6506802414022844, 0.6545637237364226], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.3132485], dtype=float32), -0.99488956]. 
=============================================
[2019-04-28 01:12:27,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:27,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:27,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run8
[2019-04-28 01:12:29,641] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:29,642] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:29,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run8
[2019-04-28 01:12:30,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:30,853] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:30,896] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run8
[2019-04-28 01:12:30,962] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:30,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:30,967] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run8
[2019-04-28 01:12:32,112] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:32,113] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:32,170] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run8
[2019-04-28 01:12:32,524] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:32,525] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:32,529] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run8
[2019-04-28 01:12:33,310] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:33,311] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:33,318] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run8
[2019-04-28 01:12:34,869] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:34,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:34,875] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:34,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:34,888] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run8
[2019-04-28 01:12:34,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run8
[2019-04-28 01:12:34,961] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:34,964] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:35,023] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run8
[2019-04-28 01:12:35,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:35,245] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:35,253] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run8
[2019-04-28 01:12:36,764] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.3413668e-18 9.9998951e-01 2.4017459e-28 1.0537880e-05 3.6589846e-21], sum to 1.0000
[2019-04-28 01:12:36,785] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9197
[2019-04-28 01:12:36,801] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.9, 96.0, 1.0, 2.0, 0.2860759064107357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 460796.9032282293, 460796.9032282293, 164327.4826521666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 186000.0000, 
sim time next is 186600.0000, 
raw observation next is [19.9, 96.0, 1.0, 2.0, 0.286127378626733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 460879.9610897925, 460879.9610897932, 164333.1420590986], 
processed observation next is [0.0, 0.13043478260869565, 0.14218009478672985, 0.96, 1.0, 1.0, 0.1399125043695578, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12802221141383124, 0.12802221141383144, 0.2452733463568636], 
reward next is 0.7547, 
noisyNet noise sample is [array([-0.67411155], dtype=float32), -1.9762043]. 
=============================================
[2019-04-28 01:12:37,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:12:37,545] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:12:37,551] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run8
[2019-04-28 01:12:40,104] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [7.55054017e-20 9.99999881e-01 3.04211824e-27 1.03189706e-07
 9.50372104e-20], sum to 1.0000
[2019-04-28 01:12:40,111] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4590
[2019-04-28 01:12:40,116] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 91.0, 1.0, 2.0, 0.3864606354715272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 589678.446230795, 589678.446230795, 173951.0997592188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 108000.0000, 
sim time next is 108600.0000, 
raw observation next is [22.73333333333333, 91.00000000000001, 1.0, 2.0, 0.3612280467567916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 550705.361605188, 550705.361605188, 170535.4212980481], 
processed observation next is [1.0, 0.2608695652173913, 0.27646129541864134, 0.9100000000000001, 1.0, 1.0, 0.2303952370563754, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15297371155699666, 0.15297371155699666, 0.25453047954932556], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.74086505], dtype=float32), -0.14195839]. 
=============================================
[2019-04-28 01:12:41,497] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0329681e-18 9.9999762e-01 1.8938178e-27 2.3464502e-06 3.1180393e-23], sum to 1.0000
[2019-04-28 01:12:41,504] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4838
[2019-04-28 01:12:41,511] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.16666666666667, 93.66666666666667, 1.0, 2.0, 0.2822112784495689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 454520.7118120827, 454520.7118120834, 163902.4201794936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 269400.0000, 
sim time next is 270000.0000, 
raw observation next is [20.1, 94.0, 1.0, 2.0, 0.2814696938757787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 453563.7445835401, 453563.7445835401, 163838.4135973949], 
processed observation next is [0.0, 0.13043478260869565, 0.15165876777251197, 0.94, 1.0, 1.0, 0.13430083599491408, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12598992905098336, 0.12598992905098336, 0.24453494566775355], 
reward next is 0.7555, 
noisyNet noise sample is [array([1.5126935], dtype=float32), -0.80896175]. 
=============================================
[2019-04-28 01:12:41,538] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.396515]
 [72.51072 ]
 [72.451256]
 [72.44302 ]
 [72.61854 ]], R is [[72.47855377]
 [72.50914001]
 [72.53930664]
 [72.56904602]
 [72.5983963 ]].
[2019-04-28 01:12:49,378] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3158450e-18 9.9998760e-01 7.4264422e-28 1.2420721e-05 3.1367671e-20], sum to 1.0000
[2019-04-28 01:12:49,385] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3976
[2019-04-28 01:12:49,406] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.03333333333333, 94.83333333333334, 1.0, 2.0, 0.2842843622234943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457962.4109121758, 457962.4109121758, 164134.9558894765], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 195000.0000, 
sim time next is 195600.0000, 
raw observation next is [20.06666666666667, 94.66666666666667, 1.0, 2.0, 0.2870589814165657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 462308.2144084498, 462308.2144084498, 164430.5828139944], 
processed observation next is [0.0, 0.2608695652173913, 0.1500789889415484, 0.9466666666666668, 1.0, 1.0, 0.14103491736935628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1284189484467916, 0.1284189484467916, 0.24541878031939462], 
reward next is 0.7546, 
noisyNet noise sample is [array([0.9095028], dtype=float32), -1.021893]. 
=============================================
[2019-04-28 01:12:55,005] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.78483165e-19 9.99999523e-01 1.37186975e-27 4.92725803e-07
 4.25148873e-22], sum to 1.0000
[2019-04-28 01:12:55,017] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0309
[2019-04-28 01:12:55,021] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.95, 92.66666666666667, 1.0, 2.0, 0.2684822967204594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 435299.8850105178, 435299.8850105172, 162626.2074436057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 280200.0000, 
sim time next is 280800.0000, 
raw observation next is [20.1, 92.0, 1.0, 2.0, 0.2706050916224741, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 438197.227455016, 438197.227455016, 162818.2025584676], 
processed observation next is [0.0, 0.2608695652173913, 0.15165876777251197, 0.92, 1.0, 1.0, 0.12121095376201697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12172145207083777, 0.12172145207083777, 0.24301224262457852], 
reward next is 0.7570, 
noisyNet noise sample is [array([-0.31011403], dtype=float32), 0.80586034]. 
=============================================
[2019-04-28 01:13:02,171] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.8780836e-19 9.9999571e-01 8.5703717e-30 4.2334136e-06 1.1646097e-21], sum to 1.0000
[2019-04-28 01:13:02,203] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5159
[2019-04-28 01:13:02,207] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.6, 87.0, 1.0, 2.0, 0.2230419871355725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 370663.4714237471, 370663.4714237471, 158067.2075455481], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 525600.0000, 
sim time next is 526200.0000, 
raw observation next is [18.55, 87.33333333333333, 1.0, 2.0, 0.2453003539403731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 407695.6163674646, 407695.6163674646, 160130.3803430362], 
processed observation next is [1.0, 0.08695652173913043, 0.07819905213270152, 0.8733333333333333, 1.0, 1.0, 0.09072331800044953, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11324878232429572, 0.11324878232429572, 0.23900056767617345], 
reward next is 0.7610, 
noisyNet noise sample is [array([-0.42957303], dtype=float32), -0.026270922]. 
=============================================
[2019-04-28 01:13:05,116] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.9627755e-18 9.9999976e-01 1.3455544e-30 2.7810330e-07 2.6839494e-22], sum to 1.0000
[2019-04-28 01:13:05,125] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5630
[2019-04-28 01:13:05,138] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.6, 85.0, 1.0, 2.0, 0.2375883467431448, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 392025.2315532363, 392025.2315532357, 159672.5643642883], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 439200.0000, 
sim time next is 439800.0000, 
raw observation next is [19.6, 84.83333333333334, 1.0, 2.0, 0.259362040747021, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 428018.0119751465, 428018.0119751459, 161804.3780396897], 
processed observation next is [1.0, 0.08695652173913043, 0.127962085308057, 0.8483333333333334, 1.0, 1.0, 0.10766510933376025, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11889389221531847, 0.1188938922153183, 0.2414990717010294], 
reward next is 0.7585, 
noisyNet noise sample is [array([0.41629335], dtype=float32), 0.6933645]. 
=============================================
[2019-04-28 01:13:09,322] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.7565772e-19 9.9999988e-01 2.3549571e-28 9.9832306e-08 2.9434950e-22], sum to 1.0000
[2019-04-28 01:13:09,336] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.3709
[2019-04-28 01:13:09,339] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 74.0, 1.0, 2.0, 0.2443662082116722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 403511.8456770882, 403511.8456770882, 160304.7886285869], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 633600.0000, 
sim time next is 634200.0000, 
raw observation next is [21.16666666666667, 73.16666666666667, 1.0, 2.0, 0.3049239794157823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 166812.8907504143], 
processed observation next is [1.0, 0.34782608695652173, 0.2022116903633494, 0.7316666666666667, 1.0, 1.0, 0.16255901134431602, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13976901878634496, 0.13976901878634496, 0.24897446380658853], 
reward next is 0.7510, 
noisyNet noise sample is [array([0.3642128], dtype=float32), 0.5429945]. 
=============================================
[2019-04-28 01:13:10,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.1222106e-20 1.0000000e+00 1.9366685e-29 1.3359645e-08 2.3478861e-23], sum to 1.0000
[2019-04-28 01:13:10,517] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0861
[2019-04-28 01:13:10,526] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 75.0, 1.0, 2.0, 0.4174778319579757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686437.3715234355, 686437.3715234349, 182086.0896672615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 550800.0000, 
sim time next is 551400.0000, 
raw observation next is [21.4, 73.5, 1.0, 2.0, 0.4935744429945579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 811601.5395526467, 811601.5395526467, 194680.2085834122], 
processed observation next is [1.0, 0.391304347826087, 0.21327014218009477, 0.735, 1.0, 1.0, 0.3898487264994673, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2254448720979574, 0.2254448720979574, 0.2905674754976301], 
reward next is 0.7094, 
noisyNet noise sample is [array([0.43002874], dtype=float32), -0.67522174]. 
=============================================
[2019-04-28 01:13:11,034] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 01:13:11,036] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:13:11,037] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:13:11,038] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:13:11,038] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:13:11,039] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:13:11,040] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:13:11,041] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:13:11,041] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:13:11,042] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:13:11,042] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:13:11,059] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run61
[2019-04-28 01:13:11,095] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run61
[2019-04-28 01:13:11,130] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run61
[2019-04-28 01:13:11,167] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run61
[2019-04-28 01:13:11,212] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run61
[2019-04-28 01:13:13,168] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:13:13,173] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.170489, 81.02468399, 1.0, 2.0, 0.3839447908578975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 582952.5290724367, 582952.5290724362, 173272.6129556393]
[2019-04-28 01:13:13,174] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:13:13,188] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.5619901e-12 9.9995995e-01 6.2958929e-18 4.0100538e-05 3.6604112e-14], sampled 0.08617809714328972
[2019-04-28 01:13:27,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:13:27,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.8, 95.66666666666667, 1.0, 2.0, 0.4544907371352003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652648.7875512383, 652648.7875512383, 179027.0900895277]
[2019-04-28 01:13:27,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:13:27,169] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9356223e-19 1.0000000e+00 3.4095301e-29 5.0434743e-08 5.4587073e-23], sampled 0.7450118718776304
[2019-04-28 01:13:34,747] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:13:34,750] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.23333333333333, 51.00000000000001, 1.0, 2.0, 0.2807627868745183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 455597.0635380739, 455597.0635380746, 163951.2364826456]
[2019-04-28 01:13:34,752] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:13:34,755] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.4077393e-19 9.9999988e-01 8.0916454e-29 6.2988072e-08 1.0743343e-22], sampled 0.5017093283798278
[2019-04-28 01:13:35,728] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:13:35,730] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.76666666666667, 96.0, 1.0, 2.0, 0.3552193625251076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546872.3453331747, 546872.3453331747, 170375.8955436849]
[2019-04-28 01:13:35,733] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:13:35,734] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.0949996e-19 1.0000000e+00 3.8475311e-29 5.2036903e-08 5.9977328e-23], sampled 0.4985199768897203
[2019-04-28 01:13:59,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:13:59,422] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.15, 81.0, 1.0, 2.0, 0.4831268849277418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675087.2673589304, 675087.2673589297, 180995.5027593236]
[2019-04-28 01:13:59,423] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:13:59,425] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0943842e-19 1.0000000e+00 3.8447436e-29 5.2016560e-08 5.9948967e-23], sampled 0.07911209585749668
[2019-04-28 01:14:11,193] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.017111698]
[2019-04-28 01:14:11,194] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.721029985, 81.047941185, 1.0, 2.0, 0.5827725443207065, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 814378.4244153509, 814378.4244153509, 197580.9279975476]
[2019-04-28 01:14:11,194] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:14:11,195] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6959029e-19 1.0000000e+00 2.7857511e-29 4.7885635e-08 4.6568772e-23], sampled 0.6858419135470796
[2019-04-28 01:16:20,609] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:16:21,029] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:16:21,108] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:16:21,200] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:16:21,493] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:16:22,508] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1500000, evaluation results [1500000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:16:23,158] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.6206174e-18 9.9999869e-01 1.7030732e-24 1.2668331e-06 2.6620078e-19], sum to 1.0000
[2019-04-28 01:16:23,164] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9515
[2019-04-28 01:16:23,169] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.16666666666667, 91.33333333333334, 1.0, 2.0, 0.2182630173528393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 364737.7274437897, 364737.7274437897, 156963.1787796891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 613200.0000, 
sim time next is 613800.0000, 
raw observation next is [17.15, 91.5, 1.0, 2.0, 0.2087223887586454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 348792.9853543976, 348792.9853543976, 156150.0011927825], 
processed observation next is [1.0, 0.08695652173913043, 0.011848341232227487, 0.915, 1.0, 1.0, 0.046653480432102885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09688694037622156, 0.09688694037622156, 0.23305970327280973], 
reward next is 0.7669, 
noisyNet noise sample is [array([-0.30959126], dtype=float32), -0.76274097]. 
=============================================
[2019-04-28 01:16:23,908] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [8.8821397e-17 9.9999964e-01 4.1027013e-26 3.5073870e-07 9.7008071e-22], sum to 1.0000
[2019-04-28 01:16:23,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8363
[2019-04-28 01:16:23,918] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 78.0, 1.0, 2.0, 0.310669089155688, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 492077.1015525088, 492077.1015525082, 166453.102154276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 842400.0000, 
sim time next is 843000.0000, 
raw observation next is [22.93333333333333, 78.16666666666667, 1.0, 2.0, 0.3094560225271693, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 490571.6716944638, 490571.6716944638, 166350.3921104927], 
processed observation next is [0.0, 0.782608695652174, 0.28593996840442326, 0.7816666666666667, 1.0, 1.0, 0.16801930424960157, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13626990880401774, 0.13626990880401774, 0.2482841673290936], 
reward next is 0.7517, 
noisyNet noise sample is [array([-3.0757818], dtype=float32), -1.2104652]. 
=============================================
[2019-04-28 01:16:23,942] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[68.99602]
 [68.97509]
 [68.96369]
 [68.95302]
 [68.95103]], R is [[69.04417419]
 [69.1053009 ]
 [69.16590881]
 [69.2259903 ]
 [69.28544617]].
[2019-04-28 01:16:32,429] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.3559119e-21 1.0000000e+00 1.2333087e-32 1.0490827e-08 2.1051684e-24], sum to 1.0000
[2019-04-28 01:16:32,434] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8283
[2019-04-28 01:16:32,440] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 61.33333333333333, 1.0, 2.0, 0.2504187298447559, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 412273.4695163005, 412273.4695163011, 160939.509386387], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 670200.0000, 
sim time next is 670800.0000, 
raw observation next is [22.96666666666667, 62.66666666666667, 1.0, 2.0, 0.2497542569032178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411159.580403137, 411159.5804031377, 160874.469992949], 
processed observation next is [1.0, 0.782608695652174, 0.2875197472353872, 0.6266666666666667, 1.0, 1.0, 0.09608946614845515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11421099455642696, 0.11421099455642715, 0.24011114924320745], 
reward next is 0.7599, 
noisyNet noise sample is [array([0.65153307], dtype=float32), -0.15990706]. 
=============================================
[2019-04-28 01:16:50,127] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4149054e-18 9.9999917e-01 4.1016043e-26 8.4261137e-07 3.0966770e-20], sum to 1.0000
[2019-04-28 01:16:50,135] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0878
[2019-04-28 01:16:50,140] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 88.33333333333334, 1.0, 2.0, 0.3242670206660002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 520236.3845931807, 520236.3845931814, 168625.8043795586], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1147200.0000, 
sim time next is 1147800.0000, 
raw observation next is [21.16666666666666, 87.66666666666666, 1.0, 2.0, 0.3188953490194287, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 511114.2906416905, 511114.2906416905, 167933.7240781819], 
processed observation next is [1.0, 0.2608695652173913, 0.2022116903633489, 0.8766666666666666, 1.0, 1.0, 0.179391986770396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14197619184491403, 0.14197619184491403, 0.25064734937042077], 
reward next is 0.7494, 
noisyNet noise sample is [array([0.5131966], dtype=float32), 0.5549862]. 
=============================================
[2019-04-28 01:17:01,701] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.3511363e-19 1.0000000e+00 8.3602230e-31 2.2988349e-09 8.0741231e-24], sum to 1.0000
[2019-04-28 01:17:01,709] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1239
[2019-04-28 01:17:01,712] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.1, 88.66666666666666, 1.0, 2.0, 0.2974904764655094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476011.9620746989, 476011.9620746995, 165366.9566344492], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1122000.0000, 
sim time next is 1122600.0000, 
raw observation next is [21.05, 88.83333333333334, 1.0, 2.0, 0.2963207938900543, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 474421.9454683191, 474421.9454683198, 165257.3690576749], 
processed observation next is [1.0, 1.0, 0.1966824644549764, 0.8883333333333334, 1.0, 1.0, 0.15219372757837865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13178387374119976, 0.13178387374119996, 0.24665278963832074], 
reward next is 0.7533, 
noisyNet noise sample is [array([1.3207455], dtype=float32), 2.3283098]. 
=============================================
[2019-04-28 01:17:05,856] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.1718365e-17 1.0000000e+00 1.2406683e-27 5.8006311e-08 9.0010923e-22], sum to 1.0000
[2019-04-28 01:17:05,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9174
[2019-04-28 01:17:05,864] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 57.5, 1.0, 2.0, 0.8694337795922499, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1338358.83683975, 1338358.836839749, 278471.2090432497], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1182600.0000, 
sim time next is 1183200.0000, 
raw observation next is [27.6, 57.33333333333334, 1.0, 2.0, 0.8756463434796307, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1349039.152794541, 1349039.152794542, 280439.4418665444], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.5733333333333335, 1.0, 1.0, 0.8501763174453382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.37473309799848364, 0.3747330979984839, 0.41856633114409614], 
reward next is 0.5814, 
noisyNet noise sample is [array([0.8866756], dtype=float32), -0.006401869]. 
=============================================
[2019-04-28 01:17:08,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [4.6875172e-19 9.9999845e-01 1.0483468e-27 1.5453516e-06 2.6488043e-22], sum to 1.0000
[2019-04-28 01:17:08,972] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0316
[2019-04-28 01:17:08,975] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.8, 95.5, 1.0, 2.0, 0.3160292879083856, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499949.0761886003, 499949.0761886003, 167025.4563251972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1377000.0000, 
sim time next is 1377600.0000, 
raw observation next is [20.76666666666667, 95.66666666666667, 1.0, 2.0, 0.3150424282966958, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 498548.6015814939, 498548.6015814944, 166923.9400284876], 
processed observation next is [1.0, 0.9565217391304348, 0.18325434439178534, 0.9566666666666667, 1.0, 1.0, 0.1747499136104769, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1384857226615261, 0.13848572266152623, 0.24914020899774267], 
reward next is 0.7509, 
noisyNet noise sample is [array([-0.35673246], dtype=float32), -0.13208713]. 
=============================================
[2019-04-28 01:17:09,980] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.4001688e-19 9.9999976e-01 2.3998657e-28 1.8016316e-07 8.2065629e-23], sum to 1.0000
[2019-04-28 01:17:09,988] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1497
[2019-04-28 01:17:09,992] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 94.0, 1.0, 2.0, 0.4601472896373651, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 651474.5764389839, 651474.5764389839, 178684.6456285214], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1294800.0000, 
sim time next is 1295400.0000, 
raw observation next is [24.31666666666667, 94.0, 1.0, 2.0, 0.4592063636277687, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 650618.5539348436, 650618.5539348436, 178607.5266730238], 
processed observation next is [1.0, 1.0, 0.3515007898894157, 0.94, 1.0, 1.0, 0.3484414019611671, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18072737609301212, 0.18072737609301212, 0.2665783980194385], 
reward next is 0.7334, 
noisyNet noise sample is [array([-0.8037464], dtype=float32), -1.34527]. 
=============================================
[2019-04-28 01:17:15,906] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.8711787e-16 9.9999833e-01 1.4171462e-25 1.6108390e-06 5.4150102e-19], sum to 1.0000
[2019-04-28 01:17:15,917] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5170
[2019-04-28 01:17:15,921] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.21666666666667, 98.0, 1.0, 2.0, 0.3043332204446107, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 484784.5682728032, 484784.5682728025, 165968.0686210966], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1390200.0000, 
sim time next is 1390800.0000, 
raw observation next is [20.23333333333333, 98.0, 1.0, 2.0, 0.3046216930557488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485071.9430994448, 485071.9430994448, 165986.3676641155], 
processed observation next is [0.0, 0.08695652173913043, 0.15797788309636643, 0.98, 1.0, 1.0, 0.16219481091054075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13474220641651244, 0.13474220641651244, 0.2477408472598739], 
reward next is 0.7523, 
noisyNet noise sample is [array([-1.0861796], dtype=float32), 1.2446576]. 
=============================================
[2019-04-28 01:17:17,457] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.6145434e-17 9.9999726e-01 2.0865243e-26 2.7694111e-06 4.2398548e-20], sum to 1.0000
[2019-04-28 01:17:17,461] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6769
[2019-04-28 01:17:17,467] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.7, 90.16666666666666, 1.0, 2.0, 0.3298121731139325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 518533.8750696515, 518533.8750696522, 168379.4633056991], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1565400.0000, 
sim time next is 1566000.0000, 
raw observation next is [21.7, 90.0, 1.0, 2.0, 0.3265040407911278, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 513596.4148940176, 513596.4148940182, 168002.772027921], 
processed observation next is [1.0, 0.13043478260869565, 0.2274881516587678, 0.9, 1.0, 1.0, 0.18855908529051546, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14266567080389378, 0.14266567080389395, 0.25075040601182236], 
reward next is 0.7492, 
noisyNet noise sample is [array([-0.5695039], dtype=float32), -0.43299454]. 
=============================================
[2019-04-28 01:17:17,485] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[72.73956 ]
 [73.02698 ]
 [73.048134]
 [73.190865]
 [73.12721 ]], R is [[72.8884964 ]
 [72.90830231]
 [72.92687988]
 [72.94539642]
 [72.9549942 ]].
[2019-04-28 01:17:19,645] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8085325e-19 9.9999893e-01 2.7918000e-30 1.0422576e-06 8.8056124e-26], sum to 1.0000
[2019-04-28 01:17:19,655] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6250
[2019-04-28 01:17:19,663] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333333, 97.16666666666667, 1.0, 2.0, 0.3254563981357907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510594.4042223032, 510594.4042223032, 167740.3976730386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1480200.0000, 
sim time next is 1480800.0000, 
raw observation next is [20.86666666666667, 97.33333333333334, 1.0, 2.0, 0.323469055500303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508083.6761096269, 508083.6761096276, 167562.6779779367], 
processed observation next is [0.0, 0.13043478260869565, 0.18799368088467638, 0.9733333333333334, 1.0, 1.0, 0.1849024765063892, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14113435447489636, 0.14113435447489656, 0.250093549220801], 
reward next is 0.7499, 
noisyNet noise sample is [array([-0.45809135], dtype=float32), 0.28014746]. 
=============================================
[2019-04-28 01:17:23,037] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-28 01:17:23,038] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:17:23,040] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:17:23,041] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:17:23,041] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:17:23,043] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:17:23,044] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:17:23,052] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:17:23,052] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:17:23,043] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:17:23,055] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:17:23,056] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run62
[2019-04-28 01:17:23,103] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run62
[2019-04-28 01:17:23,140] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run62
[2019-04-28 01:17:23,212] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run62
[2019-04-28 01:17:23,253] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run62
[2019-04-28 01:17:56,887] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:17:56,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.8, 68.66666666666667, 1.0, 2.0, 0.6152075104115894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859722.0617312401, 859722.0617312401, 203606.4892857031]
[2019-04-28 01:17:56,889] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:17:56,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2982276e-18 9.9999988e-01 4.2163007e-28 1.4979231e-07 3.2749738e-22], sampled 0.6921366530089947
[2019-04-28 01:18:05,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:05,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.95, 82.16666666666666, 1.0, 2.0, 0.6766920890274095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 945681.971858635, 945681.971858635, 215894.7211577838]
[2019-04-28 01:18:05,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:18:05,903] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2292544e-18 9.9999988e-01 3.8767034e-28 1.4667991e-07 3.0657869e-22], sampled 0.007248182742793263
[2019-04-28 01:18:33,284] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:33,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [29.20250368333333, 74.49922789666667, 1.0, 2.0, 1.011104407451214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565057433, 1413336.586412407, 1413336.586412408, 302327.5027198523]
[2019-04-28 01:18:33,287] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:18:33,291] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.6580649e-18 9.9999976e-01 1.2693873e-27 1.9728320e-07 7.7864075e-22], sampled 0.3007578357008588
[2019-04-28 01:18:38,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:38,854] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.32455540333333, 73.10499903, 1.0, 2.0, 0.9304695760168611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564994616, 1300554.994271853, 1300554.994271852, 278458.7521287897]
[2019-04-28 01:18:38,855] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:18:38,856] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7625587e-18 9.9999976e-01 1.3468763e-27 2.0024707e-07 8.1603034e-22], sampled 0.36898971571254935
[2019-04-28 01:18:39,046] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:39,046] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.8, 67.0, 1.0, 2.0, 0.5486896964765844, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 766733.1566900198, 766733.1566900198, 191575.9012487023]
[2019-04-28 01:18:39,048] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:18:39,052] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.678321e-18 9.999999e-01 6.257968e-28 1.653078e-07 4.463315e-22], sampled 0.627247702492514
[2019-04-28 01:18:49,410] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:49,411] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.5, 73.0, 1.0, 2.0, 0.5097672204514047, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712325.0651136544, 712325.065113655, 185140.0513191358]
[2019-04-28 01:18:49,411] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:18:49,414] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [9.2124170e-19 9.9999988e-01 2.4876719e-28 1.3127854e-07 2.1629501e-22], sampled 0.5381239471071717
[2019-04-28 01:18:49,663] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:49,664] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.48954071666667, 86.92751726666667, 1.0, 2.0, 0.3478202363369786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 540194.6555227892, 540194.6555227892, 169953.8589516643]
[2019-04-28 01:18:49,665] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:18:49,668] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.1459529e-18 9.9999988e-01 3.4801087e-28 1.4276935e-07 2.8162556e-22], sampled 0.7077290919796487
[2019-04-28 01:18:50,704] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:50,705] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.0, 60.5, 1.0, 2.0, 0.6085974583439534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850481.1426324806, 850481.1426324806, 202358.5298192069]
[2019-04-28 01:18:50,707] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:18:50,711] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.1540335e-18 9.9999988e-01 3.5178822e-28 1.4316393e-07 2.8373153e-22], sampled 0.5413275366205105
[2019-04-28 01:18:52,894] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.011927595]
[2019-04-28 01:18:52,895] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.2, 61.16666666666667, 1.0, 2.0, 0.4344574003626109, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 694731.2149788564, 694731.2149788571, 183851.3910972406]
[2019-04-28 01:18:52,896] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:18:52,898] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6678287e-18 9.9999988e-01 6.1981452e-28 1.6493034e-07 4.4337027e-22], sampled 0.8166374006536896
[2019-04-28 01:18:53,715] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:18:53,991] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:18:54,141] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:18:54,288] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:18:54,291] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:18:55,308] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1525000, evaluation results [1525000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:19:01,878] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.2292943e-20 1.0000000e+00 2.1889837e-27 2.7008222e-09 4.2585981e-23], sum to 1.0000
[2019-04-28 01:19:01,891] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8823
[2019-04-28 01:19:01,896] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666667, 94.0, 1.0, 2.0, 0.5065683184638683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 707853.5785466613, 707853.5785466619, 184631.7498514981], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1725600.0000, 
sim time next is 1726200.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5047705006445073, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705340.5625705486, 705340.5625705493, 184347.0012442233], 
processed observation next is [1.0, 1.0, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4033379525837438, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1959279340473746, 0.1959279340473748, 0.2751447779764527], 
reward next is 0.7249, 
noisyNet noise sample is [array([-0.3597894], dtype=float32), 0.6802923]. 
=============================================
[2019-04-28 01:19:04,534] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1985304e-18 1.0000000e+00 1.4203185e-27 2.3518235e-10 8.9203836e-23], sum to 1.0000
[2019-04-28 01:19:04,545] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6291
[2019-04-28 01:19:04,548] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.73333333333333, 85.33333333333334, 1.0, 2.0, 0.3208506649395171, 1.0, 1.0, 0.3208506649395171, 1.0, 1.0, 0.5453270532031456, 6.911199999999999, 6.9112, 170.5573041426782, 1345418.085254436, 1345418.085254437, 312115.8088049701], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1686000.0000, 
sim time next is 1686600.0000, 
raw observation next is [26.85, 85.0, 1.0, 2.0, 0.8176992240782671, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129563573426, 1142846.476253308, 1142846.476253309, 248299.8620626072], 
processed observation next is [1.0, 0.5217391304347826, 0.4715639810426541, 0.85, 1.0, 1.0, 0.7803605109376712, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399444005686, 0.3174573545148077, 0.31745735451480805, 0.37059680904866743], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.5427468], dtype=float32), -1.175397]. 
=============================================
[2019-04-28 01:19:04,704] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3334106e-20 1.0000000e+00 4.9880216e-32 2.2105728e-10 1.0546998e-24], sum to 1.0000
[2019-04-28 01:19:04,711] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3741
[2019-04-28 01:19:04,715] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.16666666666667, 1.0, 2.0, 0.4168699389410615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611053.8734260148, 611053.8734260142, 175258.3735891297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1635000.0000, 
sim time next is 1635600.0000, 
raw observation next is [23.1, 97.33333333333334, 1.0, 2.0, 0.4170438703713091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610836.2120430194, 610836.2120430188, 175224.0002879854], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.9733333333333334, 1.0, 1.0, 0.29764321731483023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16967672556750538, 0.1696767255675052, 0.26152835863878415], 
reward next is 0.7385, 
noisyNet noise sample is [array([0.07243658], dtype=float32), -1.1582633]. 
=============================================
[2019-04-28 01:19:06,525] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.3536753e-20 1.0000000e+00 7.8142032e-29 6.9679533e-09 1.9212513e-23], sum to 1.0000
[2019-04-28 01:19:06,548] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8108
[2019-04-28 01:19:06,553] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.35, 99.0, 1.0, 2.0, 0.4353314581547272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 626098.9645301371, 626098.9645301371, 176378.8258898749], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1657800.0000, 
sim time next is 1658400.0000, 
raw observation next is [23.36666666666667, 99.0, 1.0, 2.0, 0.4450125520757766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 639576.0243611212, 639576.0243611212, 177710.6118858715], 
processed observation next is [1.0, 0.17391304347826086, 0.30647709320695127, 0.99, 1.0, 1.0, 0.3313404241876826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17766000676697813, 0.17766000676697813, 0.26523971923264406], 
reward next is 0.7348, 
noisyNet noise sample is [array([-0.845994], dtype=float32), -1.0743724]. 
=============================================
[2019-04-28 01:19:09,414] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.8773811e-18 1.0000000e+00 9.7906363e-28 1.6085911e-08 2.0952053e-21], sum to 1.0000
[2019-04-28 01:19:09,420] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.2106
[2019-04-28 01:19:09,426] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.66666666666667, 95.66666666666666, 1.0, 2.0, 0.4799949164347742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 670709.4949860029, 670709.4949860035, 180521.0908855588], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2010000.0000, 
sim time next is 2010600.0000, 
raw observation next is [24.75, 95.5, 1.0, 2.0, 0.4817375651242758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 673145.3152481278, 673145.3152481284, 180784.2955881653], 
processed observation next is [0.0, 0.2608695652173913, 0.3720379146919432, 0.955, 1.0, 1.0, 0.3755874278605732, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18698480979114662, 0.18698480979114676, 0.2698273068480079], 
reward next is 0.7302, 
noisyNet noise sample is [array([1.0909077], dtype=float32), -0.33404902]. 
=============================================
[2019-04-28 01:19:12,154] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6201239e-19 9.9999988e-01 1.8236822e-28 1.0487610e-07 1.1356591e-21], sum to 1.0000
[2019-04-28 01:19:12,166] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3447
[2019-04-28 01:19:12,170] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 0.5082964139928445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 710269.1412918553, 710269.141291856, 184906.389975793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1711800.0000, 
sim time next is 1712400.0000, 
raw observation next is [26.93333333333333, 84.66666666666666, 1.0, 2.0, 0.5090827636249359, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711368.3166479467, 711368.3166479474, 185031.53148485], 
processed observation next is [1.0, 0.8260869565217391, 0.4755134281200631, 0.8466666666666666, 1.0, 1.0, 0.4085334501505251, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1976023101799852, 0.1976023101799854, 0.2761664649027612], 
reward next is 0.7238, 
noisyNet noise sample is [array([-0.82693845], dtype=float32), -0.28924122]. 
=============================================
[2019-04-28 01:19:14,503] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7293220e-18 1.0000000e+00 7.4379068e-27 3.1750965e-08 5.9212189e-21], sum to 1.0000
[2019-04-28 01:19:14,511] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8973
[2019-04-28 01:19:14,516] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.9, 87.0, 1.0, 2.0, 0.4856036854836487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678549.2782577785, 678549.2782577785, 181371.1205841529], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1882800.0000, 
sim time next is 1883400.0000, 
raw observation next is [25.81666666666666, 87.0, 1.0, 2.0, 0.48264653024892, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 674415.8407097274, 674415.8407097274, 180921.6819981368], 
processed observation next is [1.0, 0.8260869565217391, 0.4225908372827801, 0.87, 1.0, 1.0, 0.37668256656496385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18733773353047983, 0.18733773353047983, 0.27003236119124896], 
reward next is 0.7300, 
noisyNet noise sample is [array([-0.44187835], dtype=float32), 1.954207]. 
=============================================
[2019-04-28 01:19:24,875] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8187026e-17 9.9999976e-01 1.7510187e-26 1.8832615e-07 1.0672901e-18], sum to 1.0000
[2019-04-28 01:19:24,882] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5402
[2019-04-28 01:19:24,889] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 77.0, 1.0, 2.0, 0.5688649918539387, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 794936.4499355687, 794936.4499355693, 195089.4460440906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2124000.0000, 
sim time next is 2124600.0000, 
raw observation next is [30.05, 76.83333333333334, 1.0, 2.0, 0.5691461926551131, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 795329.5492814106, 795329.5492814106, 195139.2908913219], 
processed observation next is [0.0, 0.6086956521739131, 0.6232227488151659, 0.7683333333333334, 1.0, 1.0, 0.48089902729531697, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22092487480039186, 0.22092487480039186, 0.29125267297212226], 
reward next is 0.7087, 
noisyNet noise sample is [array([-0.78110456], dtype=float32), 1.6210961]. 
=============================================
[2019-04-28 01:19:26,629] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9436395e-18 9.9999738e-01 1.1273187e-26 2.6688597e-06 3.9027339e-21], sum to 1.0000
[2019-04-28 01:19:26,635] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0125
[2019-04-28 01:19:26,642] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 94.00000000000001, 1.0, 2.0, 0.5042688363017573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704639.3298944306, 704639.3298944306, 184267.7597389171], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2023800.0000, 
sim time next is 2024400.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5040597605871551, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 704347.0813891853, 704347.0813891859, 184234.7563698511], 
processed observation next is [0.0, 0.43478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4024816392616326, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19565196705255147, 0.19565196705255164, 0.2749772483132106], 
reward next is 0.7250, 
noisyNet noise sample is [array([-0.88825774], dtype=float32), 0.3554968]. 
=============================================
[2019-04-28 01:19:27,859] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.2013643e-10 9.9927002e-01 4.6387332e-15 7.3002698e-04 2.2676869e-11], sum to 1.0000
[2019-04-28 01:19:27,869] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1359
[2019-04-28 01:19:27,877] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2190612.126992404 W.
[2019-04-28 01:19:27,883] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.8, 70.0, 1.0, 2.0, 0.9254157588830377, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.002388817030681, 6.9112, 168.9124140441937, 2190612.126992404, 2125919.84947476, 441329.9025307394], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2203200.0000, 
sim time next is 2203800.0000, 
raw observation next is [30.91666666666667, 69.5, 1.0, 2.0, 0.8066551112349076, 1.0, 1.0, 0.8066551112349076, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2255973.884621182, 2255973.884621182, 423125.6431808143], 
processed observation next is [1.0, 0.5217391304347826, 0.6642969984202214, 0.695, 1.0, 1.0, 0.7670543508854308, 1.0, 0.5, 0.7670543508854308, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6266594123947727, 0.6266594123947727, 0.6315308107176333], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.7025479], dtype=float32), -1.2873085]. 
=============================================
[2019-04-28 01:19:29,640] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.5919598e-10 9.9938130e-01 4.1371271e-14 6.1866181e-04 2.5351317e-11], sum to 1.0000
[2019-04-28 01:19:29,649] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5253
[2019-04-28 01:19:29,658] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1799465.467707837 W.
[2019-04-28 01:19:29,665] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.98333333333333, 69.16666666666667, 1.0, 2.0, 0.6435592406635219, 1.0, 2.0, 0.6435592406635219, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1799465.467707837, 1799465.467707837, 350895.6794136002], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2279400.0000, 
sim time next is 2280000.0000, 
raw observation next is [30.16666666666667, 68.33333333333334, 1.0, 2.0, 0.6868034791600198, 1.0, 2.0, 0.6868034791600198, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1920489.685216105, 1920489.685216104, 368523.6204774316], 
processed observation next is [1.0, 0.391304347826087, 0.6287519747235389, 0.6833333333333335, 1.0, 1.0, 0.6226547941686985, 1.0, 1.0, 0.6226547941686985, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5334693570044736, 0.5334693570044733, 0.5500352544439278], 
reward next is 0.4500, 
noisyNet noise sample is [array([-0.23700903], dtype=float32), 0.032748327]. 
=============================================
[2019-04-28 01:19:29,678] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[38.007465]
 [37.368847]
 [37.149826]
 [37.84374 ]
 [37.110035]], R is [[38.350914  ]
 [37.96740341]
 [37.58773041]
 [37.21185303]
 [37.32388306]].
[2019-04-28 01:19:30,201] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.6704628e-17 9.9965107e-01 9.8210294e-28 3.4887518e-04 4.0793769e-20], sum to 1.0000
[2019-04-28 01:19:30,206] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6875
[2019-04-28 01:19:30,212] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.73333333333333, 76.83333333333334, 1.0, 2.0, 0.5561859792124533, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 777212.2149826646, 777212.2149826653, 192868.1196246198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2229000.0000, 
sim time next is 2229600.0000, 
raw observation next is [29.56666666666667, 77.66666666666667, 1.0, 2.0, 0.5564116770647739, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 777527.7197537712, 777527.7197537706, 192907.1602445457], 
processed observation next is [1.0, 0.8260869565217391, 0.6003159557661929, 0.7766666666666667, 1.0, 1.0, 0.4655562374274384, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21597992215382533, 0.21597992215382517, 0.2879211346933518], 
reward next is 0.7121, 
noisyNet noise sample is [array([-0.38208055], dtype=float32), 0.9634592]. 
=============================================
[2019-04-28 01:19:31,355] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5631566e-15 9.9965787e-01 1.4400438e-25 3.4218514e-04 1.0019737e-18], sum to 1.0000
[2019-04-28 01:19:31,361] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9375
[2019-04-28 01:19:31,365] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 86.5, 1.0, 2.0, 0.5107083068985722, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713640.5373868372, 713640.5373868379, 185291.4615950815], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2104200.0000, 
sim time next is 2104800.0000, 
raw observation next is [27.1, 85.66666666666667, 1.0, 2.0, 0.5132600484130145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 717207.4285421473, 717207.4285421467, 185700.380978169], 
processed observation next is [0.0, 0.34782608695652173, 0.4834123222748816, 0.8566666666666667, 1.0, 1.0, 0.4135663233891741, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19922428570615203, 0.19922428570615186, 0.27716474772861044], 
reward next is 0.7228, 
noisyNet noise sample is [array([-1.0466657], dtype=float32), -1.1711756]. 
=============================================
[2019-04-28 01:19:34,510] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7000074e-13 9.8848510e-01 1.2421866e-22 1.1514877e-02 2.2942136e-16], sum to 1.0000
[2019-04-28 01:19:34,518] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-28 01:19:34,524] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.3, 70.0, 1.0, 2.0, 0.5267534544518675, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565104163, 736069.0642111723, 736069.0642111716, 187897.8505899362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2223000.0000, 
sim time next is 2223600.0000, 
raw observation next is [31.13333333333333, 70.66666666666667, 1.0, 2.0, 0.5390484974611177, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753255.8705786614, 753255.8705786614, 189943.1877752293], 
processed observation next is [1.0, 0.7391304347826086, 0.6745655608214848, 0.7066666666666667, 1.0, 1.0, 0.44463674392905744, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20923774182740595, 0.20923774182740595, 0.2834972951869094], 
reward next is 0.7165, 
noisyNet noise sample is [array([-0.00677298], dtype=float32), 0.9118298]. 
=============================================
[2019-04-28 01:19:41,536] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.2128710e-11 9.9894780e-01 9.8725874e-20 1.0522607e-03 1.5785608e-13], sum to 1.0000
[2019-04-28 01:19:41,542] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9066
[2019-04-28 01:19:41,546] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666667, 82.0, 1.0, 2.0, 0.6655486001863871, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930102.0482365907, 930102.0482365907, 213582.6950682274], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2349600.0000, 
sim time next is 2350200.0000, 
raw observation next is [27.13333333333333, 82.0, 1.0, 2.0, 0.6682611215593884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 933894.4548958407, 933894.4548958407, 214141.528037051], 
processed observation next is [1.0, 0.17391304347826086, 0.484992101105845, 0.82, 1.0, 1.0, 0.6003146042884198, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2594151263599558, 0.2594151263599558, 0.31961422095082237], 
reward next is 0.6804, 
noisyNet noise sample is [array([1.4564524], dtype=float32), 1.854776]. 
=============================================
[2019-04-28 01:19:41,626] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.8708814e-09 9.5574200e-01 7.2347718e-15 4.4257950e-02 7.7121504e-11], sum to 1.0000
[2019-04-28 01:19:41,632] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0077
[2019-04-28 01:19:41,646] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1887797.614229261 W.
[2019-04-28 01:19:41,650] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.16666666666667, 88.33333333333334, 1.0, 2.0, 0.6751224663151597, 1.0, 2.0, 0.6751224663151597, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1887797.614229261, 1887797.614229262, 363648.4947321188], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2456400.0000, 
sim time next is 2457000.0000, 
raw observation next is [26.05, 88.5, 1.0, 2.0, 0.4442484618674583, 1.0, 2.0, 0.4442484618674583, 1.0, 1.0, 0.7547924078216819, 6.9112, 6.9112, 170.5573041426782, 1863309.961673871, 1863309.961673871, 376055.0677694287], 
processed observation next is [1.0, 0.43478260869565216, 0.43364928909952616, 0.885, 1.0, 1.0, 0.330419833575251, 1.0, 1.0, 0.330419833575251, 1.0, 0.5, 0.7009663510020511, 0.0, 0.0, 0.8375144448122397, 0.5175861004649641, 0.5175861004649641, 0.5612762205513862], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.55120254], dtype=float32), -0.27534303]. 
=============================================
[2019-04-28 01:19:41,671] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[42.749577]
 [43.087543]
 [42.686684]
 [42.646404]
 [43.561283]], R is [[42.62475586]
 [42.19850922]
 [41.77652359]
 [41.35875702]
 [40.94517136]].
[2019-04-28 01:19:45,343] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1418212e-16 9.9254298e-01 1.2132227e-27 7.4569643e-03 4.3199501e-17], sum to 1.0000
[2019-04-28 01:19:45,352] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8865
[2019-04-28 01:19:45,363] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.75, 94.0, 1.0, 2.0, 0.551534643858489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770710.0982300511, 770710.0982300511, 192064.3922580315], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2503800.0000, 
sim time next is 2504400.0000, 
raw observation next is [26.73333333333333, 94.0, 1.0, 2.0, 0.5508428459820665, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769743.0351386375, 769743.035138637, 191945.510242919], 
processed observation next is [1.0, 1.0, 0.4660347551342811, 0.94, 1.0, 1.0, 0.4588468023880318, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21381750976073263, 0.2138175097607325, 0.2864858361834612], 
reward next is 0.7135, 
noisyNet noise sample is [array([-0.39975867], dtype=float32), 0.96660423]. 
=============================================
[2019-04-28 01:19:46,330] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2159988e-14 4.6902907e-01 1.4889805e-22 5.3097093e-01 2.5929333e-16], sum to 1.0000
[2019-04-28 01:19:46,338] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8678
[2019-04-28 01:19:46,341] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 84.66666666666667, 1.0, 2.0, 0.6421290393029553, 1.0, 2.0, 0.6421290393029553, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1795463.110205633, 1795463.110205632, 350331.9714885192], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2475600.0000, 
sim time next is 2476200.0000, 
raw observation next is [27.6, 84.33333333333333, 1.0, 2.0, 0.6221226035142199, 1.0, 2.0, 0.6221226035142199, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1739477.520178594, 1739477.520178594, 342565.3988678909], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.8433333333333333, 1.0, 1.0, 0.5447260283303854, 1.0, 1.0, 0.5447260283303854, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.48318820004960944, 0.48318820004960944, 0.5112916401013297], 
reward next is 0.4887, 
noisyNet noise sample is [array([-0.24646309], dtype=float32), 0.246574]. 
=============================================
[2019-04-28 01:19:46,928] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 01:19:46,929] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:19:46,929] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:19:46,929] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:19:46,930] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:19:46,931] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:19:46,931] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:19:46,932] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:19:46,932] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:19:46,932] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:19:46,934] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:19:46,955] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run63
[2019-04-28 01:19:46,984] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run63
[2019-04-28 01:19:47,024] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run63
[2019-04-28 01:19:47,057] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run63
[2019-04-28 01:19:47,091] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run63
[2019-04-28 01:20:06,871] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:06,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.73333333333333, 89.66666666666667, 1.0, 2.0, 0.4472913763868975, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 660988.8252991544, 660988.8252991538, 180283.4225605447]
[2019-04-28 01:20:06,873] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:20:06,877] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.0912192e-15 6.1501378e-01 5.7943091e-26 3.8498619e-01 1.4985723e-18], sampled 0.07004553971224159
[2019-04-28 01:20:08,797] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:08,797] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.762001385, 78.24412755, 1.0, 2.0, 0.2738964594075666, 1.0, 1.0, 0.2738964594075666, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 797428.3050057765, 797428.3050057765, 249972.6756378986]
[2019-04-28 01:20:08,799] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:20:08,801] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.9047255e-15 6.6668248e-01 9.7515130e-25 3.3331755e-01 1.1111835e-17], sampled 0.7721419799836765
[2019-04-28 01:20:16,425] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:16,427] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.20855747, 90.32072091, 1.0, 2.0, 0.3618094241469478, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 559622.8898193499, 559622.8898193504, 171515.1122718886]
[2019-04-28 01:20:16,428] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:20:16,429] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.0983020e-15 9.4285613e-01 6.2089683e-26 5.7143908e-02 1.6138290e-18], sampled 0.4769995433724247
[2019-04-28 01:20:20,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:20,386] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.0, 100.0, 1.0, 2.0, 0.3013878625879061, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 479043.8532769291, 479043.8532769296, 165538.2683403468]
[2019-04-28 01:20:20,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:20:20,391] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.2636191e-16 6.2449616e-01 2.2243807e-26 3.7550381e-01 7.6569064e-19], sampled 0.6170216425793517
[2019-04-28 01:20:31,784] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:31,786] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [34.5, 67.0, 1.0, 2.0, 0.8915224674449853, 1.0, 2.0, 0.7663512732367552, 1.0, 1.0, 1.03, 7.005112838386381, 6.9112, 170.5573041426782, 3216118.394580826, 3148844.808314639, 588650.9102546753]
[2019-04-28 01:20:31,787] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:20:31,788] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.82326224e-17 6.44566059e-01 5.10236680e-29 3.55434000e-01
 1.06059305e-20], sampled 0.6867497828715047
[2019-04-28 01:20:40,707] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:40,708] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.05880517666667, 70.11811661, 1.0, 2.0, 0.4653168027425997, 1.0, 2.0, 0.4653168027425997, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 172.86236624, 1300766.1994506, 1300766.1994506, 290331.5901563829]
[2019-04-28 01:20:40,709] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:20:40,712] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.5149629e-15 6.1505759e-01 9.1672264e-25 3.8494238e-01 1.0501097e-17], sampled 0.6245534079973967
[2019-04-28 01:20:57,209] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:20:57,211] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.43333333333334, 86.0, 1.0, 2.0, 0.2553598410949141, 1.0, 1.0, 0.2553598410949141, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 713650.9610073756, 713650.961007375, 242322.8469429175]
[2019-04-28 01:20:57,211] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:20:57,213] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.7545713e-15 6.0850614e-01 7.1831450e-25 3.9149386e-01 8.8221798e-18], sampled 0.20487005801905978
[2019-04-28 01:21:10,254] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.01231512]
[2019-04-28 01:21:10,256] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.96666666666667, 67.0, 1.0, 2.0, 0.3277760330914876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515914.880189379, 515914.8801893797, 168188.5550306034]
[2019-04-28 01:21:10,256] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:21:10,257] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6112974e-15 9.5068276e-01 1.2293538e-25 4.9317218e-02 2.6127275e-18], sampled 0.30626378585439873
[2019-04-28 01:21:11,276] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6406.8694 3136831848.9667 1182.0000
[2019-04-28 01:21:12,017] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6290.7612 3291831298.0849 1299.0000
[2019-04-28 01:21:12,057] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6598.1836 2996463923.0185 770.0000
[2019-04-28 01:21:12,082] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6682.8953 2945298794.4817 628.0000
[2019-04-28 01:21:12,203] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6505.3799 3067800805.9958 904.0000
[2019-04-28 01:21:13,218] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1550000, evaluation results [1550000.0, 6290.761178888601, 3291831298.084936, 1299.0, 6505.379891668212, 3067800805.9958096, 904.0, 6682.895281952386, 2945298794.4817395, 628.0, 6406.869437708526, 3136831848.966701, 1182.0, 6598.183593723688, 2996463923.018543, 770.0]
[2019-04-28 01:21:13,738] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3444522e-16 9.9780673e-01 1.4539939e-24 2.1932914e-03 2.7848666e-19], sum to 1.0000
[2019-04-28 01:21:13,743] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1351
[2019-04-28 01:21:13,748] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 92.0, 1.0, 2.0, 0.4417324161473611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 636596.4127780722, 636596.4127780729, 177456.4061942771], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2601600.0000, 
sim time next is 2602200.0000, 
raw observation next is [24.13333333333333, 92.0, 1.0, 2.0, 0.4399230405821846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 634846.1989082854, 634846.1989082847, 177304.0313787904], 
processed observation next is [0.0, 0.08695652173913043, 0.3428120063191152, 0.92, 1.0, 1.0, 0.32520848262913815, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1763461663634126, 0.1763461663634124, 0.26463288265491103], 
reward next is 0.7354, 
noisyNet noise sample is [array([2.8061652], dtype=float32), -0.5294904]. 
=============================================
[2019-04-28 01:21:15,284] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [7.1701051e-15 9.9952245e-01 3.3977091e-23 4.7759456e-04 3.0202786e-16], sum to 1.0000
[2019-04-28 01:21:15,291] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2782
[2019-04-28 01:21:15,296] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.18333333333334, 86.66666666666666, 1.0, 2.0, 0.5332053691326285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 745087.9331896056, 745087.9331896056, 188962.0568143222], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2580600.0000, 
sim time next is 2581200.0000, 
raw observation next is [27.1, 87.0, 1.0, 2.0, 0.5330832358044094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 744917.2072691167, 744917.2072691167, 188941.636561287], 
processed observation next is [1.0, 0.9130434782608695, 0.4834123222748816, 0.87, 1.0, 1.0, 0.4374496816920595, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20692144646364352, 0.20692144646364352, 0.2820024426287866], 
reward next is 0.7180, 
noisyNet noise sample is [array([0.13069156], dtype=float32), -0.19046713]. 
=============================================
[2019-04-28 01:21:21,817] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.4292454e-14 9.9970239e-01 3.4137455e-22 2.9755931e-04 2.3736162e-14], sum to 1.0000
[2019-04-28 01:21:21,829] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5338
[2019-04-28 01:21:21,833] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 96.83333333333334, 1.0, 2.0, 0.6362318134670158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889114.7828770584, 889114.7828770584, 207684.0529895017], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2523000.0000, 
sim time next is 2523600.0000, 
raw observation next is [26.2, 97.0, 1.0, 2.0, 0.6396759181259455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893929.8408518926, 893929.8408518926, 208364.1872846041], 
processed observation next is [1.0, 0.21739130434782608, 0.44075829383886256, 0.97, 1.0, 1.0, 0.5658746001517415, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24831384468108128, 0.24831384468108128, 0.31099132430537924], 
reward next is 0.6890, 
noisyNet noise sample is [array([-0.09244431], dtype=float32), -1.2840165]. 
=============================================
[2019-04-28 01:21:22,347] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.0819274e-15 9.9410951e-01 5.9086618e-27 5.8905440e-03 3.6824574e-19], sum to 1.0000
[2019-04-28 01:21:22,377] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.7537
[2019-04-28 01:21:22,390] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.78333333333333, 89.66666666666667, 1.0, 2.0, 0.4503228765350923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 642213.2562571305, 642213.2562571305, 177850.370632033], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2616600.0000, 
sim time next is 2617200.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4573981071325827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 648801.6628119738, 648801.6628119744, 178438.1370797681], 
processed observation next is [0.0, 0.30434782608695654, 0.38388625592417064, 0.89, 1.0, 1.0, 0.34626277967781055, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18022268411443718, 0.18022268411443734, 0.26632557773099713], 
reward next is 0.7337, 
noisyNet noise sample is [array([-0.05993991], dtype=float32), 1.9725423]. 
=============================================
[2019-04-28 01:21:22,659] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.7116023e-15 9.9981827e-01 2.2005500e-26 1.8176145e-04 5.1900513e-18], sum to 1.0000
[2019-04-28 01:21:22,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3947
[2019-04-28 01:21:22,668] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 88.33333333333334, 1.0, 2.0, 0.5450397238837826, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761630.895157924, 761630.8951579246, 190954.3357564987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2487000.0000, 
sim time next is 2487600.0000, 
raw observation next is [27.5, 89.0, 1.0, 2.0, 0.545366144040152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 762087.1937526363, 762087.1937526357, 191009.9090404581], 
processed observation next is [1.0, 0.8260869565217391, 0.5023696682464456, 0.89, 1.0, 1.0, 0.4522483663134362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2116908871535101, 0.21169088715350992, 0.28508941647829567], 
reward next is 0.7149, 
noisyNet noise sample is [array([1.6450216], dtype=float32), 0.9753995]. 
=============================================
[2019-04-28 01:21:25,731] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.0197104e-18 9.9975055e-01 6.1825074e-27 2.4945266e-04 1.0699097e-18], sum to 1.0000
[2019-04-28 01:21:25,735] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4912
[2019-04-28 01:21:25,742] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.46666666666667, 93.33333333333334, 1.0, 2.0, 0.7492603171921808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1047146.582536944, 1047146.582536944, 231837.6183765737], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2533200.0000, 
sim time next is 2533800.0000, 
raw observation next is [26.48333333333333, 93.16666666666666, 1.0, 2.0, 0.7783651110863742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1087843.514641576, 1087843.514641576, 238669.8148637448], 
processed observation next is [1.0, 0.30434782608695654, 0.4541864139020536, 0.9316666666666665, 1.0, 1.0, 0.7329700133570773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3021787540671044, 0.3021787540671044, 0.356223604274246], 
reward next is 0.6438, 
noisyNet noise sample is [array([0.7039647], dtype=float32), 0.57073116]. 
=============================================
[2019-04-28 01:21:26,161] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.3325729e-13 9.9771899e-01 2.6640032e-21 2.2809792e-03 9.1116790e-17], sum to 1.0000
[2019-04-28 01:21:26,177] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0665
[2019-04-28 01:21:26,183] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.4687521976355929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 735760.4568515499, 735760.4568515499, 188259.7691952905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2800800.0000, 
sim time next is 2801400.0000, 
raw observation next is [22.16666666666667, 87.16666666666667, 1.0, 2.0, 0.5352413794238481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 840160.105262556, 840160.105262556, 200026.4735460519], 
processed observation next is [1.0, 0.43478260869565216, 0.24960505529225935, 0.8716666666666667, 1.0, 1.0, 0.4400498547275278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23337780701737668, 0.23337780701737668, 0.2985469754418685], 
reward next is 0.7015, 
noisyNet noise sample is [array([-0.4543165], dtype=float32), -0.20083131]. 
=============================================
[2019-04-28 01:21:26,460] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.7923840e-18 9.9975067e-01 2.6580495e-27 2.4932786e-04 9.2440515e-19], sum to 1.0000
[2019-04-28 01:21:26,483] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5461
[2019-04-28 01:21:26,487] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 96.0, 1.0, 2.0, 0.3638917805954814, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 555837.0626616417, 555837.0626616417, 171003.5475770769], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2752800.0000, 
sim time next is 2753400.0000, 
raw observation next is [22.0, 95.0, 1.0, 2.0, 0.3585729966423505, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 549726.1945690847, 549726.1945690847, 170547.4506901155], 
processed observation next is [0.0, 0.8695652173913043, 0.2417061611374408, 0.95, 1.0, 1.0, 0.22719638149680782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15270172071363464, 0.15270172071363464, 0.254548433865844], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.21638437], dtype=float32), -0.2942488]. 
=============================================
[2019-04-28 01:21:37,069] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.1661909e-16 9.9994993e-01 2.5542851e-26 5.0029899e-05 3.0523706e-19], sum to 1.0000
[2019-04-28 01:21:37,076] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1368
[2019-04-28 01:21:37,083] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 91.0, 1.0, 2.0, 0.5540920708112779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 861238.8819793045, 861238.8819793045, 202808.7056951553], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2799000.0000, 
sim time next is 2799600.0000, 
raw observation next is [22.0, 90.0, 1.0, 2.0, 0.5349092040805988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834201.3221805632, 834201.3221805632, 199447.1048913579], 
processed observation next is [1.0, 0.391304347826087, 0.2417061611374408, 0.9, 1.0, 1.0, 0.439649643470601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2317225894946009, 0.2317225894946009, 0.29768224610650434], 
reward next is 0.7023, 
noisyNet noise sample is [array([-2.0762196], dtype=float32), -0.21994169]. 
=============================================
[2019-04-28 01:21:46,424] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.1643157e-19 9.9999249e-01 1.8337709e-27 7.5501343e-06 1.7438921e-20], sum to 1.0000
[2019-04-28 01:21:46,431] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4413
[2019-04-28 01:21:46,435] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.5608023860212256, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880231.2487849245, 880231.2487849245, 204935.8393119968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2973600.0000, 
sim time next is 2974200.0000, 
raw observation next is [22.0, 88.00000000000001, 1.0, 2.0, 0.6197742906219627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 973746.0532342028, 973746.0532342022, 217192.9058727489], 
processed observation next is [1.0, 0.43478260869565216, 0.2417061611374408, 0.8800000000000001, 1.0, 1.0, 0.5418967356891116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.27048501478727854, 0.2704850147872784, 0.3241685162279834], 
reward next is 0.6758, 
noisyNet noise sample is [array([0.40028235], dtype=float32), 0.21691976]. 
=============================================
[2019-04-28 01:21:51,367] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9833156e-17 9.9999964e-01 3.2004814e-26 3.1237082e-07 8.6398716e-21], sum to 1.0000
[2019-04-28 01:21:51,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6649
[2019-04-28 01:21:51,389] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.5808879828875657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912708.653975482, 912708.653975482, 209038.4272906284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2980800.0000, 
sim time next is 2981400.0000, 
raw observation next is [21.66666666666667, 90.0, 1.0, 2.0, 0.6884180546139048, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1083074.385866981, 1083074.385866981, 232968.1364439665], 
processed observation next is [1.0, 0.5217391304347826, 0.22590837282780438, 0.9, 1.0, 1.0, 0.6246000657998853, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3008539960741614, 0.3008539960741614, 0.3477136364835321], 
reward next is 0.6523, 
noisyNet noise sample is [array([-0.92953867], dtype=float32), 0.5701669]. 
=============================================
[2019-04-28 01:22:06,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2567160e-17 9.9999881e-01 1.1353684e-27 1.1839736e-06 1.0431893e-19], sum to 1.0000
[2019-04-28 01:22:06,542] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6102
[2019-04-28 01:22:06,550] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5489947093149598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 767159.5323086148, 767159.5323086142, 191628.2826581257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3355200.0000, 
sim time next is 3355800.0000, 
raw observation next is [28.0, 83.16666666666667, 1.0, 2.0, 0.5461071983525783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 763123.1049005303, 763123.1049005303, 191135.0438016164], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.8316666666666667, 1.0, 1.0, 0.4531412028344316, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21197864025014732, 0.21197864025014732, 0.28527618477853195], 
reward next is 0.7147, 
noisyNet noise sample is [array([-1.1187499], dtype=float32), 1.1538801]. 
=============================================
[2019-04-28 01:22:07,548] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1429467e-12 9.9999845e-01 9.7495198e-19 1.5240070e-06 4.3625873e-14], sum to 1.0000
[2019-04-28 01:22:07,563] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8943
[2019-04-28 01:22:07,580] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.8045236478928445, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1124422.05687882, 1124422.05687882, 245024.2878413632], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3391200.0000, 
sim time next is 3391800.0000, 
raw observation next is [27.16666666666666, 89.0, 1.0, 2.0, 0.7703848615489096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1076684.659099937, 1076684.659099936, 236771.8059754824], 
processed observation next is [1.0, 0.2608695652173913, 0.4865718799368086, 0.89, 1.0, 1.0, 0.7233552548782043, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.29907907197220474, 0.29907907197220446, 0.35339075518728713], 
reward next is 0.6466, 
noisyNet noise sample is [array([0.48966193], dtype=float32), 0.97608757]. 
=============================================
[2019-04-28 01:22:11,556] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.7134642e-17 9.9999654e-01 6.1103304e-26 3.5096430e-06 1.2708232e-18], sum to 1.0000
[2019-04-28 01:22:11,561] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1710
[2019-04-28 01:22:11,588] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4906160109873476, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685555.4180864805, 685555.4180864798, 182138.3131725201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3279600.0000, 
sim time next is 3280200.0000, 
raw observation next is [27.0, 79.00000000000001, 1.0, 2.0, 0.4883599958353328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682401.9943390093, 682401.9943390087, 181791.9972873562], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.7900000000000001, 1.0, 1.0, 0.38356626004256966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18955610953861368, 0.18955610953861352, 0.27133133923486], 
reward next is 0.7287, 
noisyNet noise sample is [array([-0.21401982], dtype=float32), 0.5785076]. 
=============================================
[2019-04-28 01:22:14,197] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 01:22:14,199] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:22:14,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:22:14,201] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:22:14,203] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:22:14,204] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:22:14,205] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:22:14,206] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:22:14,206] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:22:14,217] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:22:14,218] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:22:14,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run64
[2019-04-28 01:22:14,270] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run64
[2019-04-28 01:22:14,292] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run64
[2019-04-28 01:22:14,334] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run64
[2019-04-28 01:22:14,368] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run64
[2019-04-28 01:22:20,393] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:22:20,394] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.68333333333333, 80.66666666666667, 1.0, 2.0, 0.3032175268576951, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 502697.9086534971, 502697.9086534964, 166474.7669825246]
[2019-04-28 01:22:20,396] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:22:20,399] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.0229345e-16 9.9990201e-01 2.3491585e-25 9.7929857e-05 1.3475370e-18], sampled 0.3200499792677636
[2019-04-28 01:22:22,870] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:22:22,872] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.3, 87.0, 1.0, 2.0, 0.3039573601594314, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486426.5236767048, 486426.5236767041, 166110.9550063779]
[2019-04-28 01:22:22,873] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:22:22,875] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.5926910e-16 9.9991035e-01 1.3650046e-25 8.9667024e-05 9.0792326e-19], sampled 0.5863931139501967
[2019-04-28 01:22:57,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:22:57,754] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.45000415, 57.07927378, 1.0, 2.0, 0.743780433168976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1039484.291888648, 1039484.291888648, 230576.5119588459]
[2019-04-28 01:22:57,756] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:22:57,768] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8341544e-15 9.9983263e-01 6.3202171e-24 1.6731209e-04 1.4757119e-17], sampled 0.3569137712771864
[2019-04-28 01:23:07,682] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:23:07,683] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.0, 65.5, 1.0, 2.0, 0.9401028465231246, 1.0, 2.0, 0.9401028465231246, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2629580.111806007, 2629580.111806007, 493849.0881531499]
[2019-04-28 01:23:07,684] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:23:07,688] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.03200556e-13 9.99600828e-01 1.30777083e-21 3.99221492e-04
 7.14107273e-16], sampled 0.6576557247114352
[2019-04-28 01:23:07,688] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2629580.111806007 W.
[2019-04-28 01:23:22,055] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:23:22,055] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.66752969, 47.22223398, 1.0, 2.0, 0.9457065322801222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104269, 1321865.563493964, 1321865.563493964, 282825.8606003163]
[2019-04-28 01:23:22,055] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:23:22,059] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.8192328e-13 9.9953711e-01 3.2904619e-21 4.6292364e-04 1.3940709e-15], sampled 0.49098500592565897
[2019-04-28 01:23:33,174] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:23:33,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.73333333333333, 81.33333333333334, 1.0, 2.0, 0.5352779320425503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747985.1007908304, 747985.1007908304, 189307.0216361299]
[2019-04-28 01:23:33,176] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:23:33,181] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.2928915e-15 9.9985385e-01 2.7388961e-24 1.4614199e-04 8.0355451e-18], sampled 0.6209846892972204
[2019-04-28 01:23:46,104] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.013937415]
[2019-04-28 01:23:46,105] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.22994784166666, 65.76973309499999, 1.0, 2.0, 0.4477887499272606, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 642291.8019287835, 642291.8019287842, 177952.3479086778]
[2019-04-28 01:23:46,105] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:23:46,107] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.9470735e-16 9.9991488e-01 9.8993952e-26 8.5086300e-05 7.1890949e-19], sampled 0.9357907770531759
[2019-04-28 01:23:50,643] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8498.1101 2842390821.6255 1128.0000
[2019-04-28 01:23:50,802] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.7875 3007718941.6548 1766.0000
[2019-04-28 01:23:51,106] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.4920 3163971238.0859 1775.0000
[2019-04-28 01:23:51,164] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.7383 2779169280.4331 933.0000
[2019-04-28 01:23:51,315] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.0696 2927225378.6933 1334.0000
[2019-04-28 01:23:52,327] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1575000, evaluation results [1575000.0, 7884.491960096223, 3163971238.0858746, 1775.0, 8255.069590636649, 2927225378.693341, 1334.0, 8660.738250248294, 2779169280.4330955, 933.0, 7996.787538129812, 3007718941.654839, 1766.0, 8498.11011540473, 2842390821.6255183, 1128.0]
[2019-04-28 01:24:06,821] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.5523561e-13 5.2593468e-04 4.0013057e-21 9.9947411e-01 1.9604618e-15], sum to 1.0000
[2019-04-28 01:24:06,840] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1706
[2019-04-28 01:24:06,848] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.9461638884168714, 1.0, 2.0, 0.9461638884168714, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2646551.527186305, 2646551.527186305, 497305.9273885209], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3603600.0000, 
sim time next is 3604200.0000, 
raw observation next is [32.83333333333334, 63.66666666666666, 1.0, 2.0, 0.4108124532104296, 1.0, 2.0, 0.4108124532104296, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1148330.134428066, 1148330.134428066, 274896.0208778601], 
processed observation next is [1.0, 0.7391304347826086, 0.7551342812006324, 0.6366666666666666, 1.0, 1.0, 0.2901354857956983, 1.0, 1.0, 0.2901354857956983, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.318980592896685, 0.318980592896685, 0.41029256847441803], 
reward next is 0.5897, 
noisyNet noise sample is [array([-1.9009426], dtype=float32), -1.4437578]. 
=============================================
[2019-04-28 01:24:09,647] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1448082e-12 1.3491118e-04 2.3052523e-19 9.9986506e-01 2.1136653e-14], sum to 1.0000
[2019-04-28 01:24:09,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9271
[2019-04-28 01:24:09,659] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.3622607464911161, 1.0, 2.0, 0.3622607464911161, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1012551.112942693, 1012551.112942693, 263029.5224600254], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3642000.0000, 
sim time next is 3642600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.3515425842387562, 1.0, 2.0, 0.3515425842387562, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 982579.1674410587, 982579.1674410587, 260601.0190849081], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.21872600510693516, 1.0, 1.0, 0.21872600510693516, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2729386576225163, 0.2729386576225163, 0.38895674490284793], 
reward next is 0.6110, 
noisyNet noise sample is [array([-0.21983285], dtype=float32), 0.12252752]. 
=============================================
[2019-04-28 01:24:12,146] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2805163e-14 2.4152551e-04 3.7100207e-23 9.9975842e-01 2.4295658e-16], sum to 1.0000
[2019-04-28 01:24:12,159] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5127
[2019-04-28 01:24:12,173] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 59.0, 1.0, 2.0, 0.8879320232642165, 1.0, 2.0, 0.8879320232642165, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2483507.050746921, 2483507.050746921, 464949.5419742773], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3681600.0000, 
sim time next is 3682200.0000, 
raw observation next is [33.0, 59.0, 1.0, 2.0, 0.8915140380165107, 1.0, 2.0, 0.8915140380165107, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2493535.786022185, 2493535.786022185, 466882.0653748653], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.59, 1.0, 1.0, 0.8692940217066395, 1.0, 1.0, 0.8692940217066395, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6926488294506069, 0.6926488294506069, 0.6968389035445751], 
reward next is 0.3032, 
noisyNet noise sample is [array([-0.89376897], dtype=float32), -1.4955096]. 
=============================================
[2019-04-28 01:24:12,624] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.7202615e-14 3.1225070e-05 1.7487366e-24 9.9996877e-01 5.2689370e-18], sum to 1.0000
[2019-04-28 01:24:12,642] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1396
[2019-04-28 01:24:12,649] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.33333333333334, 85.66666666666666, 1.0, 2.0, 0.2642613078100987, 1.0, 2.0, 0.2642613078100987, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 738539.619721958, 738539.619721958, 243345.6728321922], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3811200.0000, 
sim time next is 3811800.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.2657590890931704, 1.0, 2.0, 0.2657590890931704, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 742726.9667998853, 742726.9667998846, 243604.7580073689], 
processed observation next is [0.0, 0.08695652173913043, 0.4865718799368086, 0.8733333333333334, 1.0, 1.0, 0.11537239649779568, 1.0, 1.0, 0.11537239649779568, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20631304633330147, 0.20631304633330128, 0.3635891910557745], 
reward next is 0.6364, 
noisyNet noise sample is [array([0.0251106], dtype=float32), 2.1156678]. 
=============================================
[2019-04-28 01:24:13,471] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3971480e-15 1.3192180e-04 2.3907489e-22 9.9986804e-01 4.0504815e-15], sum to 1.0000
[2019-04-28 01:24:13,477] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6458
[2019-04-28 01:24:13,480] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.16666666666666, 65.16666666666667, 1.0, 2.0, 0.2821270982407577, 1.0, 2.0, 0.2821270982407577, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 788488.0748901808, 788488.0748901808, 246524.5409615827], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3869400.0000, 
sim time next is 3870000.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.2861149152179181, 1.0, 2.0, 0.2861149152179181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 799637.3700596414, 799637.3700596414, 247258.9345195547], 
processed observation next is [0.0, 0.8260869565217391, 0.7156398104265403, 0.67, 1.0, 1.0, 0.13989748821435916, 1.0, 1.0, 0.13989748821435916, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22212149168323372, 0.22212149168323372, 0.3690431858500816], 
reward next is 0.6310, 
noisyNet noise sample is [array([1.1569892], dtype=float32), -0.39874896]. 
=============================================
[2019-04-28 01:24:13,499] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[61.988953]
 [61.996193]
 [61.99758 ]
 [61.992374]
 [61.96807 ]], R is [[62.02990341]
 [62.04165649]
 [62.05422211]
 [62.06744766]
 [62.08131409]].
[2019-04-28 01:24:23,191] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.4732183e-14 2.9129325e-05 1.6977476e-22 9.9997091e-01 1.3247499e-17], sum to 1.0000
[2019-04-28 01:24:23,195] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8038
[2019-04-28 01:24:23,200] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.0, 55.33333333333333, 1.0, 2.0, 0.2952464452669002, 1.0, 2.0, 0.2952464452669002, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 825168.0966255767, 825168.0966255767, 248976.0673249613], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3858000.0000, 
sim time next is 3858600.0000, 
raw observation next is [35.0, 55.16666666666667, 1.0, 2.0, 0.2944247389696206, 1.0, 2.0, 0.2944247389696206, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 822870.673749934, 822870.673749934, 248819.5869448148], 
processed observation next is [0.0, 0.6521739130434783, 0.8578199052132701, 0.5516666666666667, 1.0, 1.0, 0.14990932405978388, 1.0, 1.0, 0.14990932405978388, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22857518715275943, 0.22857518715275943, 0.3713725178280818], 
reward next is 0.6286, 
noisyNet noise sample is [array([0.11193939], dtype=float32), 0.7012043]. 
=============================================
[2019-04-28 01:24:25,254] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.4801863e-14 1.1039164e-05 1.8949127e-21 9.9998891e-01 3.3675952e-16], sum to 1.0000
[2019-04-28 01:24:25,281] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7473
[2019-04-28 01:24:25,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.2716193935584713, 1.0, 2.0, 0.2716193935584713, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 759110.7768922251, 759110.7768922251, 244631.4261013746], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4050000.0000, 
sim time next is 4050600.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.2719001068960289, 1.0, 2.0, 0.2719001068960289, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 759895.5809668204, 759895.5809668204, 244680.9917155463], 
processed observation next is [1.0, 0.9130434782608695, 0.5260663507109005, 0.84, 1.0, 1.0, 0.12277121312774565, 1.0, 1.0, 0.12277121312774565, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21108210582411677, 0.21108210582411677, 0.36519551002320344], 
reward next is 0.6348, 
noisyNet noise sample is [array([1.748701], dtype=float32), 0.8394544]. 
=============================================
[2019-04-28 01:24:27,450] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.3711702e-15 3.5043754e-06 1.5135696e-23 9.9999654e-01 4.3571608e-16], sum to 1.0000
[2019-04-28 01:24:27,552] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6613
[2019-04-28 01:24:27,560] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.5, 61.5, 1.0, 2.0, 0.2946271427967512, 1.0, 2.0, 0.2946271427967512, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 823436.5776607764, 823436.5776607764, 248857.5528245069], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3929400.0000, 
sim time next is 3930000.0000, 
raw observation next is [33.66666666666666, 61.0, 1.0, 2.0, 0.2951600355837171, 1.0, 2.0, 0.2951600355837171, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 824926.502063447, 824926.502063447, 248959.2760371204], 
processed observation next is [0.0, 0.4782608695652174, 0.7946287519747232, 0.61, 1.0, 1.0, 0.1507952235948399, 1.0, 1.0, 0.1507952235948399, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22914625057317972, 0.22914625057317972, 0.37158100901062746], 
reward next is 0.6284, 
noisyNet noise sample is [array([0.64279574], dtype=float32), 0.15223545]. 
=============================================
[2019-04-28 01:24:27,578] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[60.512184]
 [60.518745]
 [60.513145]
 [60.578148]
 [60.573994]], R is [[60.53530884]
 [60.55852509]
 [60.58135986]
 [60.60122681]
 [60.62429428]].
[2019-04-28 01:24:37,202] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.7081723e-11 5.9074719e-02 1.8334390e-20 9.4092530e-01 2.1211878e-12], sum to 1.0000
[2019-04-28 01:24:37,211] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4463
[2019-04-28 01:24:37,214] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 75.0, 1.0, 2.0, 0.3006159783843954, 1.0, 2.0, 0.3006159783843954, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 840180.9884718884, 840180.9884718884, 250006.7252565762], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4234200.0000, 
sim time next is 4234800.0000, 
raw observation next is [31.0, 75.0, 1.0, 2.0, 0.3006643696753269, 1.0, 2.0, 0.3006643696753269, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 840316.2885681621, 840316.2885681621, 250016.0907822722], 
processed observation next is [1.0, 0.0, 0.6682464454976303, 0.75, 1.0, 1.0, 0.15742695141605648, 1.0, 1.0, 0.15742695141605648, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23342119126893393, 0.23342119126893393, 0.37315834445115253], 
reward next is 0.6268, 
noisyNet noise sample is [array([-0.70161206], dtype=float32), 1.4326744]. 
=============================================
[2019-04-28 01:24:41,816] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.1149602e-10 9.9994087e-01 2.3681728e-17 5.9113707e-05 1.1861412e-12], sum to 1.0000
[2019-04-28 01:24:41,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6289
[2019-04-28 01:24:41,823] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2856106.760321675 W.
[2019-04-28 01:24:41,853] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.66666666666666, 75.83333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.71735846396452, 6.9112, 168.9084694931317, 2856106.760321675, 2284205.392589514, 474272.6031380324], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4035000.0000, 
sim time next is 4035600.0000, 
raw observation next is [30.0, 79.0, 1.0, 2.0, 0.5733581734390465, 1.0, 1.0, 0.5733581734390465, 1.0, 2.0, 0.9957335348182194, 6.9112, 6.9112, 170.5573041426782, 2405411.523608865, 2405411.523608865, 469581.2233490639], 
processed observation next is [1.0, 0.7391304347826086, 0.6208530805687204, 0.79, 1.0, 1.0, 0.4859737029386102, 1.0, 0.5, 0.4859737029386102, 1.0, 1.0, 0.9947969936807551, 0.0, 0.0, 0.8375144448122397, 0.6681698676691292, 0.6681698676691292, 0.7008674975359163], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.5952524], dtype=float32), 2.1505861]. 
=============================================
[2019-04-28 01:24:45,919] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.9457975e-15 9.9994373e-01 2.3748759e-23 5.6224570e-05 1.5415632e-16], sum to 1.0000
[2019-04-28 01:24:45,926] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.7600
[2019-04-28 01:24:45,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2791933.138563629 W.
[2019-04-28 01:24:45,941] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.0, 56.5, 1.0, 2.0, 0.9980809952140076, 1.0, 2.0, 0.9980809952140076, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2791933.138563629, 2791933.138563629, 527831.4900474277], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4375800.0000, 
sim time next is 4376400.0000, 
raw observation next is [35.33333333333334, 56.66666666666667, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.193272660726373, 6.9112, 170.5573041426782, 3111625.222120791, 2909565.101260253, 552169.6623941424], 
processed observation next is [1.0, 0.6521739130434783, 0.8736176935229073, 0.5666666666666668, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.02820726607263726, 0.0, 0.8375144448122397, 0.8643403394779976, 0.8082125281278482, 0.8241338244688692], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8406162], dtype=float32), 0.56491745]. 
=============================================
[2019-04-28 01:24:51,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0877435e-15 9.9999893e-01 3.0636062e-24 1.0516570e-06 1.1396263e-18], sum to 1.0000
[2019-04-28 01:24:51,093] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5534
[2019-04-28 01:24:51,097] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.33333333333334, 73.66666666666667, 1.0, 2.0, 0.635418770645873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 887978.1050642777, 887978.1050642777, 207530.7629131568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4220400.0000, 
sim time next is 4221000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.6333670517561034, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 885109.6965596054, 885109.696559606, 207127.5277510237], 
processed observation next is [1.0, 0.8695652173913043, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5582735563326547, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2458638045998904, 0.24586380459989055, 0.3091455638074981], 
reward next is 0.6909, 
noisyNet noise sample is [array([-0.4401151], dtype=float32), -0.33808467]. 
=============================================
[2019-04-28 01:24:51,134] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[61.95661 ]
 [61.986618]
 [61.780525]
 [61.58893 ]
 [61.055416]], R is [[62.38571548]
 [62.45211411]
 [62.51760864]
 [62.58367157]
 [62.65124512]].
[2019-04-28 01:24:53,884] A3C_AGENT_WORKER-Thread-20 INFO:Evaluating...
[2019-04-28 01:24:53,911] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:24:53,912] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:24:53,912] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:24:53,912] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:24:53,914] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:24:53,914] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:24:53,913] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:24:53,917] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:24:53,918] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:24:53,915] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:24:53,927] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run65
[2019-04-28 01:24:53,927] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run65
[2019-04-28 01:24:53,990] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run65
[2019-04-28 01:24:54,022] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run65
[2019-04-28 01:24:54,070] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run65
[2019-04-28 01:25:09,122] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:09,124] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [17.55, 92.0, 1.0, 2.0, 0.2173138512856028, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 362310.1262825103, 362310.1262825103, 157272.0792552113]
[2019-04-28 01:25:09,125] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:25:09,128] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.9688759e-18 9.9999964e-01 8.5313653e-29 3.8332780e-07 4.0898827e-21], sampled 0.04145967519232596
[2019-04-28 01:25:17,306] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:17,307] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.72369785, 68.10500395, 1.0, 2.0, 0.476160770505095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775491.3802724762, 775491.3802724757, 191479.8027742758]
[2019-04-28 01:25:17,308] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:25:17,310] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8921336e-17 9.9999928e-01 1.4583822e-27 6.8821413e-07 3.2141444e-20], sampled 0.4310484120395712
[2019-04-28 01:25:18,262] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:18,263] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.8, 88.0, 1.0, 2.0, 0.4859981027887919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 679100.5861074259, 679100.5861074264, 181431.3260307892]
[2019-04-28 01:25:18,265] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:25:18,268] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.6584165e-17 9.9999940e-01 2.0493108e-27 5.5174189e-07 4.0517300e-20], sampled 0.4755557160885744
[2019-04-28 01:25:37,069] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:37,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.13333333333333, 51.33333333333333, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.031432568316177, 6.9112, 168.9118535684472, 1539109.908823316, 1453813.343734279, 311353.1167624866]
[2019-04-28 01:25:37,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:25:37,075] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.5080914e-16 9.9999988e-01 2.6028288e-25 8.6650495e-08 1.1164303e-18], sampled 0.8027153066086805
[2019-04-28 01:25:42,963] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:42,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [32.55, 65.5, 1.0, 2.0, 0.883165453533167, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1234397.670320383, 1234397.670320383, 265353.0701657871]
[2019-04-28 01:25:42,964] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:25:42,967] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6762821e-14 9.9999535e-01 3.3327291e-23 4.6855312e-06 4.6678609e-17], sampled 0.4365276361105457
[2019-04-28 01:25:48,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:25:48,862] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 79.0, 1.0, 2.0, 0.5172390205246137, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722769.3639248497, 722769.3639248497, 186341.2278548926]
[2019-04-28 01:25:48,863] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:25:48,867] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4092246e-16 9.9999964e-01 1.4444202e-26 3.4006044e-07 1.5996934e-19], sampled 0.7029215263233116
[2019-04-28 01:26:01,868] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:26:01,869] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.83333333333334, 95.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.176008540880376, 6.9112, 168.9113405154512, 1641746.084886825, 1453883.589282636, 311352.2733899283]
[2019-04-28 01:26:01,871] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:26:01,872] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5752646e-13 9.9997783e-01 1.0317494e-20 2.2199007e-05 3.0636841e-15], sampled 0.6318386379819211
[2019-04-28 01:26:13,545] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:26:13,546] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.0, 87.0, 1.0, 2.0, 0.5672606847495993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 792693.7419200784, 792693.741920079, 194804.7336453683]
[2019-04-28 01:26:13,547] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:26:13,550] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6607574e-16 9.9999964e-01 1.9270245e-26 3.2938357e-07 1.9617431e-19], sampled 0.6177951528568455
[2019-04-28 01:26:19,470] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:26:19,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.65, 67.5, 1.0, 2.0, 0.5724744598618413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799982.2485763573, 799982.2485763573, 195730.1771249994]
[2019-04-28 01:26:19,472] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:26:19,475] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4008305e-17 9.9999940e-01 6.5637975e-28 5.8538302e-07 1.8003668e-20], sampled 0.12796645061992373
[2019-04-28 01:26:19,645] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:26:19,646] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.13333333333333, 69.66666666666667, 1.0, 2.0, 0.5738519564484171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801907.9044344551, 801907.9044344551, 195975.493736766]
[2019-04-28 01:26:19,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:26:19,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.0482022e-17 9.9999940e-01 2.3418688e-27 5.6432106e-07 4.4611214e-20], sampled 0.8424008812609874
[2019-04-28 01:26:22,566] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.015455587]
[2019-04-28 01:26:22,567] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.0, 71.0, 1.0, 2.0, 0.6071248059162768, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.921811624715662, 6.9112, 168.9127919193481, 1697523.006231149, 1689994.761769553, 366534.8361727269]
[2019-04-28 01:26:22,568] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:26:22,570] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2393866e-14 9.9999952e-01 8.2698535e-23 4.7829320e-07 7.5683413e-17], sampled 0.35520340149578855
[2019-04-28 01:26:22,573] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1697523.006231149 W.
[2019-04-28 01:26:22,761] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:26:22,777] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:26:22,889] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:26:22,920] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:26:23,101] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:26:24,116] A3C_AGENT_WORKER-Thread-20 INFO:Global step: 1600000, evaluation results [1600000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:26:34,403] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.7394940e-10 9.9833101e-01 6.3121259e-15 1.6689769e-03 4.2525363e-12], sum to 1.0000
[2019-04-28 01:26:34,425] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6765
[2019-04-28 01:26:34,428] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 88.16666666666667, 1.0, 2.0, 0.5249932849139376, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 733608.6081372208, 733608.6081372208, 187605.2002836865], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4672200.0000, 
sim time next is 4672800.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5290199997329466, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739237.3668997014, 739237.366899702, 188268.3894604913], 
processed observation next is [1.0, 0.08695652173913043, 0.4786729857819906, 0.89, 1.0, 1.0, 0.43255421654571874, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20534371302769483, 0.205343713027695, 0.2809975962096885], 
reward next is 0.7190, 
noisyNet noise sample is [array([2.4132748], dtype=float32), 0.33725694]. 
=============================================
[2019-04-28 01:26:38,543] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3087288e-12 9.9996603e-01 1.3730911e-19 3.3994234e-05 3.0192889e-14], sum to 1.0000
[2019-04-28 01:26:38,550] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.5349
[2019-04-28 01:26:38,555] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.7433204213897285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1038841.079339711, 1038841.079339712, 230473.8937004936], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4683600.0000, 
sim time next is 4684200.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.6741954722451413, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 942191.3845995613, 942191.3845995606, 215374.1392244629], 
processed observation next is [1.0, 0.21739130434782608, 0.4786729857819906, 0.89, 1.0, 1.0, 0.6074644243917365, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26171982905543373, 0.2617198290554335, 0.3214539391409894], 
reward next is 0.6785, 
noisyNet noise sample is [array([-0.22954893], dtype=float32), -0.44705]. 
=============================================
[2019-04-28 01:26:38,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.3229768e-17 9.9995875e-01 4.4642074e-28 4.1271000e-05 2.5646551e-20], sum to 1.0000
[2019-04-28 01:26:38,952] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8612
[2019-04-28 01:26:38,957] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.5, 1.0, 2.0, 0.5577636451529746, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779417.6483492494, 779417.64834925, 193140.5940172905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4476600.0000, 
sim time next is 4477200.0000, 
raw observation next is [28.33333333333333, 82.33333333333334, 1.0, 2.0, 0.5571239812961392, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 778523.4559554188, 778523.4559554193, 193029.4314875403], 
processed observation next is [0.0, 0.8260869565217391, 0.541864139020537, 0.8233333333333335, 1.0, 1.0, 0.4664144352965532, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21625651554317188, 0.21625651554317205, 0.28810362908588105], 
reward next is 0.7119, 
noisyNet noise sample is [array([0.11264393], dtype=float32), 0.52435315]. 
=============================================
[2019-04-28 01:26:39,623] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.7377174e-12 9.9999905e-01 1.9257652e-20 9.4365555e-07 8.6239602e-14], sum to 1.0000
[2019-04-28 01:26:39,631] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2810
[2019-04-28 01:26:39,638] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2492694.042873582 W.
[2019-04-28 01:26:39,641] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 72.5, 1.0, 2.0, 0.8912133894093857, 1.0, 2.0, 0.8912133894093857, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2492694.042873582, 2492694.042873582, 466720.5217479084], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4703400.0000, 
sim time next is 4704000.0000, 
raw observation next is [30.66666666666666, 71.66666666666667, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.960145621915401, 6.9112, 168.9071004502661, 3029650.690495475, 2285518.363679003, 473694.0568826231], 
processed observation next is [1.0, 0.43478260869565216, 0.6524486571879934, 0.7166666666666667, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.10489456219154007, 0.0, 0.8294111892159771, 0.8415696362487431, 0.6348662121330564, 0.7070060550486912], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.3646407], dtype=float32), -0.32984206]. 
=============================================
[2019-04-28 01:26:39,659] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[47.375626]
 [46.39114 ]
 [46.23436 ]
 [45.685562]
 [45.069077]], R is [[47.1243248 ]
 [46.95648575]
 [46.48692322]
 [46.02205276]
 [45.56183243]].
[2019-04-28 01:27:00,421] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.1016011e-15 5.8686048e-05 5.5061047e-25 9.9994135e-01 1.1128953e-18], sum to 1.0000
[2019-04-28 01:27:00,432] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6262
[2019-04-28 01:27:00,438] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.16666666666666, 68.83333333333333, 1.0, 2.0, 0.2532775306407561, 1.0, 2.0, 0.2532775306407561, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 707832.773788991, 707832.7737889905, 241490.5367204892], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4989000.0000, 
sim time next is 4989600.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.2560183106190568, 1.0, 2.0, 0.2560183106190568, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 715494.9653233495, 715494.9653233495, 241947.2516683563], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.10363651881814069, 1.0, 1.0, 0.10363651881814069, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19874860147870818, 0.19874860147870818, 0.36111530099754674], 
reward next is 0.6389, 
noisyNet noise sample is [array([-1.3896769], dtype=float32), 0.48609227]. 
=============================================
[2019-04-28 01:27:01,190] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1429042e-14 2.4748498e-04 1.3766173e-24 9.9975246e-01 2.1784175e-16], sum to 1.0000
[2019-04-28 01:27:01,199] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6431
[2019-04-28 01:27:01,205] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 88.16666666666667, 1.0, 2.0, 0.2513557000389932, 1.0, 2.0, 0.2513557000389932, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 702460.0898053955, 702460.0898053955, 241165.9959519285], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5105400.0000, 
sim time next is 5106000.0000, 
raw observation next is [26.0, 87.33333333333334, 1.0, 2.0, 0.2496830371946438, 1.0, 2.0, 0.2496830371946438, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 697784.0029693702, 697784.0029693695, 240890.7917569078], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8733333333333334, 1.0, 1.0, 0.09600365927065517, 1.0, 1.0, 0.09600365927065517, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.19382888971371395, 0.19382888971371376, 0.3595384951595639], 
reward next is 0.6405, 
noisyNet noise sample is [array([-1.3944758], dtype=float32), -1.2692302]. 
=============================================
[2019-04-28 01:27:01,229] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[62.031544]
 [61.833153]
 [61.960556]
 [61.919815]
 [61.898933]], R is [[62.01290894]
 [62.03282928]
 [62.05221176]
 [62.07131577]
 [62.090168  ]].
[2019-04-28 01:27:04,947] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.2149053e-15 6.3901738e-05 1.9724421e-23 9.9993610e-01 3.3852022e-17], sum to 1.0000
[2019-04-28 01:27:04,950] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6727
[2019-04-28 01:27:04,955] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666666, 84.0, 1.0, 2.0, 0.2636357276912618, 1.0, 2.0, 0.2636357276912618, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 736790.6902465819, 736790.6902465819, 243238.6694375571], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5038800.0000, 
sim time next is 5039400.0000, 
raw observation next is [27.83333333333334, 84.0, 1.0, 2.0, 0.265844189502685, 1.0, 2.0, 0.265844189502685, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 742964.8825024308, 742964.8825024308, 243620.7126860309], 
processed observation next is [0.0, 0.30434782608695654, 0.5181674565560824, 0.84, 1.0, 1.0, 0.11547492711166868, 1.0, 1.0, 0.11547492711166868, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.206379134028453, 0.206379134028453, 0.3636130040090013], 
reward next is 0.6364, 
noisyNet noise sample is [array([-0.7859907], dtype=float32), -0.17706595]. 
=============================================
[2019-04-28 01:27:08,053] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9233399e-16 8.3417406e-05 1.5040334e-22 9.9991655e-01 4.2631663e-16], sum to 1.0000
[2019-04-28 01:27:08,062] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.7015
[2019-04-28 01:27:08,065] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.33333333333333, 61.66666666666667, 1.0, 2.0, 0.2607006679569299, 1.0, 2.0, 0.2607006679569299, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 728585.2046394638, 728585.2046394638, 242735.4257564479], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5067600.0000, 
sim time next is 5068200.0000, 
raw observation next is [31.16666666666667, 62.33333333333333, 1.0, 2.0, 0.2605277297957824, 1.0, 2.0, 0.2605277297957824, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 728101.7269685225, 728101.7269685225, 242705.8131272306], 
processed observation next is [0.0, 0.6521739130434783, 0.6761453396524489, 0.6233333333333333, 1.0, 1.0, 0.10906955397082219, 1.0, 1.0, 0.10906955397082219, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20225047971347848, 0.20225047971347848, 0.36224748227944864], 
reward next is 0.6378, 
noisyNet noise sample is [array([0.29636025], dtype=float32), -0.3225667]. 
=============================================
[2019-04-28 01:27:14,053] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [9.8879128e-15 9.1648551e-05 1.4948570e-23 9.9990833e-01 1.2144565e-17], sum to 1.0000
[2019-04-28 01:27:14,060] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9266
[2019-04-28 01:27:14,063] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 63.0, 1.0, 2.0, 0.2763520633142276, 1.0, 2.0, 0.2763520633142276, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 772342.2111075565, 772342.2111075565, 245475.4981680424], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5146200.0000, 
sim time next is 5146800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.2758634526468949, 1.0, 2.0, 0.2758634526468949, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 770976.162767793, 770976.162767793, 245387.9755003988], 
processed observation next is [0.0, 0.5652173913043478, 0.7156398104265403, 0.63, 1.0, 1.0, 0.12754632849023478, 1.0, 1.0, 0.12754632849023478, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21416004521327583, 0.21416004521327583, 0.3662507097020878], 
reward next is 0.6337, 
noisyNet noise sample is [array([-0.9951295], dtype=float32), 0.6370332]. 
=============================================
[2019-04-28 01:27:15,185] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.80135241e-13 1.06630294e-04 1.39181652e-20 9.99893308e-01
 6.26235440e-16], sum to 1.0000
[2019-04-28 01:27:15,199] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8622
[2019-04-28 01:27:15,205] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 67.0, 1.0, 2.0, 0.8589987907546964, 1.0, 2.0, 0.8589987907546964, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2402504.268699925, 2402504.268699925, 449620.2335107015], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5241600.0000, 
sim time next is 5242200.0000, 
raw observation next is [31.83333333333334, 67.5, 1.0, 2.0, 0.9337102986465354, 1.0, 2.0, 0.9337102986465354, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2611680.702222744, 2611680.702222744, 490218.088299713], 
processed observation next is [1.0, 0.6956521739130435, 0.7077409162717223, 0.675, 1.0, 1.0, 0.9201328899355848, 1.0, 1.0, 0.9201328899355848, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.72546686172854, 0.72546686172854, 0.7316687885070343], 
reward next is 0.2683, 
noisyNet noise sample is [array([1.0549394], dtype=float32), -0.17947043]. 
=============================================
[2019-04-28 01:27:16,077] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-28 01:27:16,078] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:27:16,079] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:27:16,081] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:27:16,080] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:27:16,085] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:27:16,085] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:27:16,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:27:16,084] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:27:16,088] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:27:16,090] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:27:16,103] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run66
[2019-04-28 01:27:16,128] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run66
[2019-04-28 01:27:16,156] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run66
[2019-04-28 01:27:16,182] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run66
[2019-04-28 01:27:16,210] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run66
[2019-04-28 01:28:00,295] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:00,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.53287187333333, 90.01817675999999, 1.0, 2.0, 0.2815039798476894, 1.0, 2.0, 0.2815039798476894, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 786744.3247778746, 786744.3247778746, 246603.1534148415]
[2019-04-28 01:28:00,298] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:28:00,300] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.76258227e-14 1.18386764e-04 3.54663406e-22 9.99881625e-01
 6.03472594e-16], sampled 0.686547330889007
[2019-04-28 01:28:00,354] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:00,355] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.32739962, 73.24251059, 1.0, 2.0, 0.410430721607349, 1.0, 2.0, 0.410430721607349, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1147254.337303622, 1147254.337303622, 275284.3218637506]
[2019-04-28 01:28:00,356] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:28:00,358] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.9692986e-14 1.2321385e-04 4.4167330e-22 9.9987674e-01 7.0239900e-16], sampled 0.4028540181574265
[2019-04-28 01:28:03,368] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:03,369] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [32.7, 57.0, 1.0, 2.0, 0.2686164664283671, 1.0, 2.0, 0.2686164664283671, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 750711.8785672844, 750711.8785672844, 244587.7897037558]
[2019-04-28 01:28:03,370] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:28:03,372] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.3556391e-13 1.3512014e-04 7.3193039e-22 9.9986482e-01 1.0077184e-15], sampled 0.22858911482063426
[2019-04-28 01:28:22,030] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:22,031] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.6, 91.0, 1.0, 2.0, 0.2566786321190015, 1.0, 2.0, 0.2566786321190015, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 172.86236624, 717337.7858178567, 717337.785817856, 242535.4882732547]
[2019-04-28 01:28:22,033] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:28:22,035] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.9925146e-13 1.5195693e-04 1.3817072e-21 9.9984801e-01 1.5863955e-15], sampled 0.7977826248220353
[2019-04-28 01:28:27,990] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:27,991] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.2, 92.0, 1.0, 2.0, 0.3628766456834479, 1.0, 2.0, 0.3628766456834479, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1014273.420312289, 1014273.420312289, 263178.2520991253]
[2019-04-28 01:28:27,991] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:28:27,994] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.8963203e-14 1.1481945e-04 3.0029236e-22 9.9988520e-01 5.3374828e-16], sampled 0.9309905955052242
[2019-04-28 01:28:37,397] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:37,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.85168597666667, 88.62654086, 1.0, 2.0, 0.4177185655819021, 1.0, 2.0, 0.4177185655819021, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1167641.494828518, 1167641.494828518, 276883.7881409304]
[2019-04-28 01:28:37,398] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:28:37,402] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.36209289e-14 1.16824056e-04 3.30179865e-22 9.99883175e-01
 5.71089307e-16], sampled 0.8655181207806082
[2019-04-28 01:28:40,844] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.018615719]
[2019-04-28 01:28:40,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.6, 92.66666666666667, 1.0, 2.0, 0.2054780445345763, 1.0, 2.0, 0.2054780445345763, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 597909.4036345892, 597909.4036345892, 238181.2407804725]
[2019-04-28 01:28:40,847] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:28:40,851] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.02579085e-13 1.24217899e-04 4.60393209e-22 9.99875784e-01
 7.26737963e-16], sampled 0.34828622140619236
[2019-04-28 01:28:46,021] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6505.3662 3686430707.6794 230.0000
[2019-04-28 01:28:46,166] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6699.9615 3429591565.4343 33.0000
[2019-04-28 01:28:46,223] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6793.3763 3393492602.1780 10.0000
[2019-04-28 01:28:46,295] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.4528 3474885819.9512 8.0000
[2019-04-28 01:28:46,556] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6797.6748 3511140253.9192 0.0000
[2019-04-28 01:28:47,569] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1625000, evaluation results [1625000.0, 6505.366234882388, 3686430707.679393, 230.0, 6857.452808766937, 3474885819.951206, 8.0, 6793.376311882319, 3393492602.1780233, 10.0, 6797.6747755118495, 3511140253.9192166, 0.0, 6699.9614964401835, 3429591565.4342756, 33.0]
[2019-04-28 01:28:49,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.2487095e-10 1.0311976e-03 4.5988929e-16 9.9896884e-01 2.9861428e-12], sum to 1.0000
[2019-04-28 01:28:49,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5215
[2019-04-28 01:28:49,607] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.6, 88.00000000000001, 1.0, 2.0, 0.4385801195569095, 1.0, 2.0, 0.4385801195569095, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1225992.530181478, 1225992.530181478, 282292.7102775024], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5289000.0000, 
sim time next is 5289600.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.4120794957079575, 1.0, 2.0, 0.4120794957079575, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1151873.757914485, 1151873.757914485, 275223.3149290676], 
processed observation next is [1.0, 0.21739130434782608, 0.5545023696682465, 0.88, 1.0, 1.0, 0.2916620430216355, 1.0, 1.0, 0.2916620430216355, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3199649327540236, 0.3199649327540236, 0.4107810670583099], 
reward next is 0.5892, 
noisyNet noise sample is [array([0.11917577], dtype=float32), 0.8945494]. 
=============================================
[2019-04-28 01:28:50,842] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.9550418e-12 1.1857790e-03 1.1807292e-18 9.9881423e-01 7.8759753e-13], sum to 1.0000
[2019-04-28 01:28:50,851] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0214
[2019-04-28 01:28:50,865] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 76.0, 1.0, 2.0, 0.5369593807662407, 1.0, 2.0, 0.5369593807662407, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1501191.264476559, 1501191.264476559, 312147.8931023308], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5472000.0000, 
sim time next is 5472600.0000, 
raw observation next is [31.21666666666667, 75.33333333333334, 1.0, 2.0, 0.541618940420592, 1.0, 2.0, 0.541618940420592, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1514227.318263202, 1514227.318263203, 313703.4321548525], 
processed observation next is [1.0, 0.34782608695652173, 0.6785150078988943, 0.7533333333333334, 1.0, 1.0, 0.44773366315733976, 1.0, 1.0, 0.44773366315733976, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4206186995175561, 0.42061869951755637, 0.4682140778430634], 
reward next is 0.5318, 
noisyNet noise sample is [array([0.44186154], dtype=float32), 1.0509127]. 
=============================================
[2019-04-28 01:28:57,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.3473136e-14 3.8179386e-04 8.3988309e-24 9.9961817e-01 6.5285536e-16], sum to 1.0000
[2019-04-28 01:28:57,561] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0782
[2019-04-28 01:28:57,569] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.5, 72.0, 1.0, 2.0, 0.3118470350614793, 1.0, 2.0, 0.3118470350614793, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 871583.0273776349, 871583.0273776349, 252223.8549308628], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5340000.0000, 
sim time next is 5340600.0000, 
raw observation next is [32.25, 73.5, 1.0, 2.0, 0.31192554180573, 1.0, 2.0, 0.31192554180573, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 871802.5355016807, 871802.5355016807, 252239.7485408963], 
processed observation next is [1.0, 0.8260869565217391, 0.7274881516587678, 0.735, 1.0, 1.0, 0.17099462868160242, 1.0, 1.0, 0.17099462868160242, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2421673709726891, 0.2421673709726891, 0.3764772366282034], 
reward next is 0.6235, 
noisyNet noise sample is [array([0.12171065], dtype=float32), 0.32623878]. 
=============================================
[2019-04-28 01:28:57,874] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.1972543e-12 1.5409470e-04 4.1232941e-20 9.9984586e-01 5.2855057e-16], sum to 1.0000
[2019-04-28 01:28:57,880] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2863
[2019-04-28 01:28:57,888] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.08333333333334, 61.66666666666666, 1.0, 2.0, 0.9692846138998711, 1.0, 2.0, 0.9692846138998711, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2711293.575700964, 2711293.575700964, 510708.251716932], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5305800.0000, 
sim time next is 5306400.0000, 
raw observation next is [34.4, 60.0, 1.0, 2.0, 0.9420433089457967, 1.0, 2.0, 0.9420433089457967, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2635013.540961918, 2635013.540961918, 494956.1100612533], 
processed observation next is [1.0, 0.43478260869565216, 0.8293838862559241, 0.6, 1.0, 1.0, 0.9301726613804779, 1.0, 1.0, 0.9301726613804779, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.731948205822755, 0.731948205822755, 0.73874046277799], 
reward next is 0.2613, 
noisyNet noise sample is [array([-0.8063428], dtype=float32), 0.26088458]. 
=============================================
[2019-04-28 01:29:06,465] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.4843317e-11 4.9968815e-04 2.0162655e-17 9.9950027e-01 1.1023791e-12], sum to 1.0000
[2019-04-28 01:29:06,471] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.1661
[2019-04-28 01:29:06,482] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.86666666666667, 73.33333333333334, 1.0, 2.0, 0.848383077439766, 1.0, 2.0, 0.848383077439766, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2372785.355806811, 2372785.355806811, 444124.5776773212], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5474400.0000, 
sim time next is 5475000.0000, 
raw observation next is [32.08333333333333, 72.66666666666666, 1.0, 2.0, 0.8449445834347157, 1.0, 2.0, 0.8449445834347157, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2363159.370995425, 2363159.370995425, 442357.0681330772], 
processed observation next is [1.0, 0.34782608695652173, 0.7195892575039492, 0.7266666666666666, 1.0, 1.0, 0.813186245102067, 1.0, 1.0, 0.813186245102067, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6564331586098403, 0.6564331586098403, 0.6602344300493689], 
reward next is 0.3398, 
noisyNet noise sample is [array([0.78242815], dtype=float32), -0.61552215]. 
=============================================
[2019-04-28 01:29:06,511] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[44.149887]
 [44.068897]
 [44.404694]
 [45.247902]
 [45.2727  ]], R is [[44.1031723 ]
 [43.99926758]
 [43.8784523 ]
 [43.82924652]
 [43.92273712]].
[2019-04-28 01:29:22,382] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2831936e-11 3.0151017e-05 5.3947536e-19 9.9996984e-01 6.7319638e-14], sum to 1.0000
[2019-04-28 01:29:22,390] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3619
[2019-04-28 01:29:22,393] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.6, 87.66666666666666, 1.0, 2.0, 0.3766443949871396, 1.0, 2.0, 0.3766443949871396, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1052774.430104926, 1052774.430104926, 266407.3923184903], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5899200.0000, 
sim time next is 5899800.0000, 
raw observation next is [27.8, 86.83333333333333, 1.0, 2.0, 0.3787589497106695, 1.0, 2.0, 0.3787589497106695, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1058687.827889059, 1058687.827889059, 266912.8887537231], 
processed observation next is [1.0, 0.2608695652173913, 0.5165876777251186, 0.8683333333333333, 1.0, 1.0, 0.2515168068803247, 1.0, 1.0, 0.2515168068803247, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.29407995219140526, 0.29407995219140526, 0.39837744590107926], 
reward next is 0.6016, 
noisyNet noise sample is [array([-0.05432998], dtype=float32), -0.71827334]. 
=============================================
[2019-04-28 01:29:37,650] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.1064271e-16 3.9038645e-07 1.5122568e-25 9.9999964e-01 2.3013801e-19], sum to 1.0000
[2019-04-28 01:29:37,661] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7454
[2019-04-28 01:29:37,670] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.95, 89.66666666666666, 1.0, 2.0, 0.2704206919439063, 1.0, 2.0, 0.2704206919439063, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 755759.5143350707, 755759.5143350702, 244418.721346934], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6043800.0000, 
sim time next is 6044400.0000, 
raw observation next is [26.9, 90.0, 1.0, 2.0, 0.2699235954079552, 1.0, 2.0, 0.2699235954079552, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 754369.7628688933, 754369.7628688933, 244331.3895856808], 
processed observation next is [1.0, 1.0, 0.4739336492890995, 0.9, 1.0, 1.0, 0.12038987398548817, 1.0, 1.0, 0.12038987398548817, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20954715635247037, 0.20954715635247037, 0.36467371579952357], 
reward next is 0.6353, 
noisyNet noise sample is [array([0.17517301], dtype=float32), -0.972684]. 
=============================================
[2019-04-28 01:29:40,662] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [6.0966897e-14 9.0601643e-06 9.1762598e-23 9.9999094e-01 1.2271650e-15], sum to 1.0000
[2019-04-28 01:29:40,670] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1290
[2019-04-28 01:29:40,674] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.0, 1.0, 2.0, 0.8421676655146577, 1.0, 2.0, 0.8421676655146577, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2355385.50732503, 2355385.507325031, 440917.7320199401], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6091200.0000, 
sim time next is 6091800.0000, 
raw observation next is [31.01666666666667, 65.0, 1.0, 2.0, 0.6983701767978857, 1.0, 2.0, 0.6983701767978857, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1952862.801105285, 1952862.801105285, 373426.4238374186], 
processed observation next is [1.0, 0.5217391304347826, 0.6690363349131123, 0.65, 1.0, 1.0, 0.636590574455284, 1.0, 1.0, 0.636590574455284, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5424618891959125, 0.5424618891959125, 0.5573528713991323], 
reward next is 0.4426, 
noisyNet noise sample is [array([0.7762721], dtype=float32), 1.8551407]. 
=============================================
[2019-04-28 01:29:44,450] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2918414e-16 3.6844412e-08 3.0717144e-27 1.0000000e+00 1.1796257e-19], sum to 1.0000
[2019-04-28 01:29:44,469] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3206
[2019-04-28 01:29:44,474] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.65, 85.66666666666667, 1.0, 2.0, 0.2692550141149264, 1.0, 2.0, 0.2692550141149264, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 752500.5878627541, 752500.5878627541, 244215.0913916329], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6036600.0000, 
sim time next is 6037200.0000, 
raw observation next is [27.6, 86.0, 1.0, 2.0, 0.269470787431191, 1.0, 2.0, 0.269470787431191, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 753103.8321592122, 753103.8321592122, 244252.8828142442], 
processed observation next is [1.0, 0.9130434782608695, 0.5071090047393366, 0.86, 1.0, 1.0, 0.11984432220625423, 1.0, 1.0, 0.11984432220625423, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2091955089331145, 0.2091955089331145, 0.3645565415137973], 
reward next is 0.6354, 
noisyNet noise sample is [array([0.6714531], dtype=float32), 0.77079815]. 
=============================================
[2019-04-28 01:29:47,765] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.9549345e-15 8.1118834e-08 1.1051557e-24 9.9999988e-01 4.8006125e-17], sum to 1.0000
[2019-04-28 01:29:47,777] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8776
[2019-04-28 01:29:47,786] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 65.33333333333334, 1.0, 2.0, 0.9183181284505982, 1.0, 2.0, 0.9183181284505982, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2568583.014289741, 2568583.014289742, 481577.914847698], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6099600.0000, 
sim time next is 6100200.0000, 
raw observation next is [30.95, 65.5, 1.0, 2.0, 0.9183924962544286, 1.0, 2.0, 0.9183924962544286, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2568791.238688962, 2568791.238688962, 481619.196140935], 
processed observation next is [1.0, 0.6086956521739131, 0.6658767772511848, 0.655, 1.0, 1.0, 0.9016777063306369, 1.0, 1.0, 0.9016777063306369, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.713553121858045, 0.713553121858045, 0.7188346211058732], 
reward next is 0.2812, 
noisyNet noise sample is [array([1.0312856], dtype=float32), -1.2283893]. 
=============================================
[2019-04-28 01:29:49,647] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-28 01:29:49,648] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:29:49,648] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:29:49,649] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:29:49,650] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:29:49,650] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:29:49,650] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:29:49,651] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:29:49,657] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:29:49,652] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:29:49,659] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:29:49,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run67
[2019-04-28 01:29:49,709] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run67
[2019-04-28 01:29:49,739] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run67
[2019-04-28 01:29:49,771] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run67
[2019-04-28 01:29:49,829] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run67
[2019-04-28 01:30:26,300] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.022671495]
[2019-04-28 01:30:26,300] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.41031823666667, 93.79216336666667, 1.0, 2.0, 0.1865085138824907, 1.0, 2.0, 0.1865085138824907, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 562778.8897249583, 562778.8897249583, 239070.3173155469]
[2019-04-28 01:30:26,301] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:30:26,306] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.4661338e-16 5.4441088e-08 7.6752746e-26 1.0000000e+00 2.0228429e-19], sampled 0.6809710292515351
[2019-04-28 01:31:56,014] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.022671495]
[2019-04-28 01:31:56,014] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.93333333333333, 79.66666666666666, 1.0, 2.0, 0.4082926482015217, 1.0, 2.0, 0.4082926482015217, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1141288.311176294, 1141288.311176294, 273952.6126666677]
[2019-04-28 01:31:56,014] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:31:56,041] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.12253594e-16 1.12940384e-07 9.56177120e-25 9.99999881e-01
 1.32740289e-18], sampled 0.3681756138193989
[2019-04-28 01:32:38,021] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.022671495]
[2019-04-28 01:32:38,022] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.31917605333334, 59.31487577333333, 1.0, 2.0, 0.3445135828242289, 1.0, 2.0, 0.3445135828242289, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 983396.2604207603, 983396.2604207603, 262152.9880065878]
[2019-04-28 01:32:38,023] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:32:38,028] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.0272988e-16 9.6158985e-08 5.4834328e-25 9.9999988e-01 8.7683109e-19], sampled 0.6086554336006508
[2019-04-28 01:32:49,166] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.022671495]
[2019-04-28 01:32:49,167] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.26666666666667, 67.33333333333333, 1.0, 2.0, 0.2784018349119181, 1.0, 2.0, 0.2784018349119181, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 778069.1788513311, 778069.1788513311, 246330.2660369822]
[2019-04-28 01:32:49,168] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:32:49,170] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.1092039e-16 6.4397845e-08 1.3711811e-25 9.9999988e-01 3.1186715e-19], sampled 0.06141144707384638
[2019-04-28 01:32:50,461] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.022671495]
[2019-04-28 01:32:50,461] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.48333333333333, 54.0, 1.0, 2.0, 0.554168293919917, 1.0, 2.0, 0.554168293919917, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1549322.510586587, 1549322.510586587, 318438.8244866676]
[2019-04-28 01:32:50,462] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:32:50,463] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [8.8733414e-16 1.2498607e-07 1.3575198e-24 9.9999988e-01 1.7240236e-18], sampled 0.7508009009651356
[2019-04-28 01:32:53,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:32:53,499] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:32:53,587] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:32:53,662] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:32:53,724] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:32:54,737] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1650000, evaluation results [1650000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:33:00,064] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.8690129e-18 5.6244598e-08 1.1757939e-26 1.0000000e+00 1.4219073e-19], sum to 1.0000
[2019-04-28 01:33:00,074] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3016
[2019-04-28 01:33:00,082] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.3, 85.5, 1.0, 2.0, 0.2653279571358568, 1.0, 2.0, 0.2653279571358568, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 741521.6495271543, 741521.6495271549, 243529.2706593988], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6309000.0000, 
sim time next is 6309600.0000, 
raw observation next is [27.3, 85.66666666666666, 1.0, 2.0, 0.2656640932161989, 1.0, 2.0, 0.2656640932161989, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 742461.386344598, 742461.3863445986, 243587.5621956115], 
processed observation next is [0.0, 0.0, 0.4928909952606636, 0.8566666666666666, 1.0, 1.0, 0.11525794363397454, 1.0, 1.0, 0.11525794363397454, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20623927398461056, 0.20623927398461073, 0.3635635256650918], 
reward next is 0.6364, 
noisyNet noise sample is [array([-2.2567594], dtype=float32), -1.4017657]. 
=============================================
[2019-04-28 01:33:01,284] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0126400e-17 3.9538765e-08 1.1692980e-28 1.0000000e+00 4.1999446e-20], sum to 1.0000
[2019-04-28 01:33:01,287] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3980
[2019-04-28 01:33:01,289] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.63333333333333, 89.33333333333334, 1.0, 2.0, 0.2607725903268814, 1.0, 2.0, 0.2607725903268814, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 728786.2757667287, 728786.2757667294, 242747.4086020697], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6222000.0000, 
sim time next is 6222600.0000, 
raw observation next is [26.6, 89.5, 1.0, 2.0, 0.2605910949304865, 1.0, 2.0, 0.2605910949304865, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 728278.8748181799, 728278.8748181792, 242716.4700199427], 
processed observation next is [0.0, 0.0, 0.4597156398104266, 0.895, 1.0, 1.0, 0.10914589750661022, 1.0, 1.0, 0.10914589750661022, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20229968744949442, 0.20229968744949423, 0.36226338808946673], 
reward next is 0.6377, 
noisyNet noise sample is [array([-0.93981135], dtype=float32), -0.8662382]. 
=============================================
[2019-04-28 01:33:26,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [9.34035428e-15 2.30723543e-07 1.98005592e-21 9.99999762e-01
 1.36705005e-17], sum to 1.0000
[2019-04-28 01:33:26,222] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9565
[2019-04-28 01:33:26,225] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.11666666666667, 71.66666666666666, 1.0, 2.0, 0.1810026636658046, 1.0, 2.0, 0.1810026636658046, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 552501.0517280854, 552501.0517280854, 239112.4827342283], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6735000.0000, 
sim time next is 6735600.0000, 
raw observation next is [25.0, 72.0, 1.0, 2.0, 0.1790830027151286, 1.0, 2.0, 0.1790830027151286, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 547631.2857859787, 547631.2857859787, 239005.0496142786], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.72, 1.0, 1.0, 0.01094337676521515, 1.0, 1.0, 0.01094337676521515, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1521198016072163, 0.1521198016072163, 0.356723954648177], 
reward next is 0.6433, 
noisyNet noise sample is [array([0.24899165], dtype=float32), 0.7559798]. 
=============================================
[2019-04-28 01:33:31,766] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.6974341e-17 3.8113370e-08 2.2749364e-24 1.0000000e+00 7.2352099e-20], sum to 1.0000
[2019-04-28 01:33:31,768] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1444
[2019-04-28 01:33:31,772] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.23333333333333, 46.0, 1.0, 2.0, 0.4795484529263092, 1.0, 2.0, 0.4795484529263092, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1477743.88736163, 1477743.88736163, 309080.825297232], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6788400.0000, 
sim time next is 6789000.0000, 
raw observation next is [29.31666666666667, 45.5, 1.0, 2.0, 0.4854762033114445, 1.0, 2.0, 0.4854762033114445, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1496714.572040116, 1496714.572040116, 311101.563985512], 
processed observation next is [1.0, 0.5652173913043478, 0.5884676145339655, 0.455, 1.0, 1.0, 0.3800918112186078, 1.0, 1.0, 0.3800918112186078, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4157540477889211, 0.4157540477889211, 0.4643306925156896], 
reward next is 0.5357, 
noisyNet noise sample is [array([1.7952331], dtype=float32), -0.72662836]. 
=============================================
[2019-04-28 01:33:31,786] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[58.11499]
 [57.93853]
 [57.7582 ]
 [57.76468]
 [57.94993]], R is [[58.1879425 ]
 [58.14474869]
 [58.10551071]
 [58.08380508]
 [58.10374451]].
[2019-04-28 01:33:36,818] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.52062329e-14 1.19097514e-07 1.19616860e-21 9.99999881e-01
 2.63623599e-16], sum to 1.0000
[2019-04-28 01:33:36,833] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5802
[2019-04-28 01:33:36,837] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.26666666666667, 47.66666666666667, 1.0, 2.0, 0.4675607291538755, 1.0, 2.0, 0.4675607291538755, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1428777.968218376, 1428777.968218376, 304057.522520569], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6794400.0000, 
sim time next is 6795000.0000, 
raw observation next is [29.25, 48.0, 1.0, 2.0, 0.4885118950129897, 1.0, 2.0, 0.4885118950129897, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1489606.99497032, 1489606.99497032, 310552.5899134207], 
processed observation next is [1.0, 0.6521739130434783, 0.5853080568720379, 0.48, 1.0, 1.0, 0.38374927109998763, 1.0, 1.0, 0.38374927109998763, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.4137797208250889, 0.4137797208250889, 0.46351132822898616], 
reward next is 0.5365, 
noisyNet noise sample is [array([0.6844343], dtype=float32), 1.5510523]. 
=============================================
[2019-04-28 01:33:36,847] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[53.319473]
 [53.37989 ]
 [53.391014]
 [53.31273 ]
 [53.32128 ]], R is [[53.32222366]
 [53.335186  ]
 [53.36115265]
 [53.36579895]
 [53.36071777]].
[2019-04-28 01:33:38,937] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.8374607e-14 9.7951059e-08 3.9165499e-23 9.9999988e-01 5.5842467e-19], sum to 1.0000
[2019-04-28 01:33:38,939] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3769
[2019-04-28 01:33:38,945] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.8, 86.33333333333334, 1.0, 2.0, 0.2385687250391181, 1.0, 2.0, 0.2385687250391181, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 666713.4121844035, 666713.4121844042, 239106.525967304], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7176000.0000, 
sim time next is 7176600.0000, 
raw observation next is [25.8, 86.5, 1.0, 2.0, 0.2388378761678543, 1.0, 2.0, 0.2388378761678543, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 667465.8263939783, 667465.8263939783, 239148.9715008523], 
processed observation next is [1.0, 0.043478260869565216, 0.42180094786729866, 0.865, 1.0, 1.0, 0.08293720020223408, 1.0, 1.0, 0.08293720020223408, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18540717399832732, 0.18540717399832732, 0.3569387634341079], 
reward next is 0.6431, 
noisyNet noise sample is [array([0.95217603], dtype=float32), -0.7585672]. 
=============================================
[2019-04-28 01:33:40,278] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0484238e-16 3.4460652e-07 8.8039149e-24 9.9999964e-01 7.1463442e-17], sum to 1.0000
[2019-04-28 01:33:40,286] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4452
[2019-04-28 01:33:40,290] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.0, 84.0, 1.0, 2.0, 0.1727588505751386, 1.0, 2.0, 0.1727588505751386, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 532334.0970050745, 532334.0970050745, 238784.4889384157], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6843600.0000, 
sim time next is 6844200.0000, 
raw observation next is [23.1, 83.66666666666667, 1.0, 2.0, 0.1738530001195824, 1.0, 2.0, 0.1738530001195824, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 534978.9298614356, 534978.9298614349, 238820.2635422765], 
processed observation next is [0.0, 0.21739130434782608, 0.2938388625592418, 0.8366666666666667, 1.0, 1.0, 0.00464216881877397, 1.0, 1.0, 0.00464216881877397, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.1486052582948432, 0.148605258294843, 0.35644815454071116], 
reward next is 0.6436, 
noisyNet noise sample is [array([-0.7708673], dtype=float32), -0.916021]. 
=============================================
[2019-04-28 01:33:44,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.7280111e-16 9.0992366e-08 2.3362956e-25 9.9999988e-01 1.2221200e-17], sum to 1.0000
[2019-04-28 01:33:44,289] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2065
[2019-04-28 01:33:44,293] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.76666666666667, 75.66666666666667, 1.0, 2.0, 0.2048210984141403, 1.0, 2.0, 0.2048210984141403, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 601043.2319420204, 601043.2319420204, 238894.2630050369], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6906000.0000, 
sim time next is 6906600.0000, 
raw observation next is [25.7, 76.5, 1.0, 2.0, 0.2059455504875123, 1.0, 2.0, 0.2059455504875123, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 603014.6076301388, 603014.6076301388, 238846.6630665266], 
processed observation next is [0.0, 0.9565217391304348, 0.4170616113744076, 0.765, 1.0, 1.0, 0.043307892153629284, 1.0, 1.0, 0.043307892153629284, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16750405767503854, 0.16750405767503854, 0.35648755681571137], 
reward next is 0.6435, 
noisyNet noise sample is [array([-0.09697521], dtype=float32), -0.30862126]. 
=============================================
[2019-04-28 01:33:48,617] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4254194e-15 9.6883966e-08 6.5472305e-25 9.9999988e-01 3.0392553e-18], sum to 1.0000
[2019-04-28 01:33:48,624] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9061
[2019-04-28 01:33:48,626] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.9, 81.0, 1.0, 2.0, 0.2942237412593703, 1.0, 2.0, 0.2942237412593703, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 830047.6016634062, 830047.6016634062, 249823.8144236914], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7020000.0000, 
sim time next is 7020600.0000, 
raw observation next is [26.05, 80.0, 1.0, 2.0, 0.3107401105598665, 1.0, 2.0, 0.3107401105598665, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 876374.5696963333, 876374.5696963333, 253031.9256765271], 
processed observation next is [1.0, 0.2608695652173913, 0.43364928909952616, 0.8, 1.0, 1.0, 0.1695663982648994, 1.0, 1.0, 0.1695663982648994, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2434373804712037, 0.2434373804712037, 0.3776595905619808], 
reward next is 0.6223, 
noisyNet noise sample is [array([-0.58606], dtype=float32), -0.3832475]. 
=============================================
[2019-04-28 01:33:51,849] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-28 01:33:51,851] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:33:51,854] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:33:51,854] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:33:51,855] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:33:51,855] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:33:51,856] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:33:51,855] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:33:51,856] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:33:51,856] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:33:51,858] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:33:51,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run68
[2019-04-28 01:33:51,878] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run68
[2019-04-28 01:33:51,948] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run68
[2019-04-28 01:33:51,949] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run68
[2019-04-28 01:33:52,017] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run68
[2019-04-28 01:33:57,544] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:33:57,545] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.5, 67.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 381761.4678619601, 381761.4678619608, 209060.1742528669]
[2019-04-28 01:33:57,546] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:33:57,548] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.4064610e-17 1.7725986e-08 4.8930900e-26 1.0000000e+00 1.1933578e-19], sampled 0.002678575617812151
[2019-04-28 01:34:00,305] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:00,307] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [19.93333333333333, 92.33333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 462642.0721703151, 462642.0721703151, 226233.4383299458]
[2019-04-28 01:34:00,308] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:34:00,312] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.1944838e-17 1.3276377e-08 1.9047925e-26 1.0000000e+00 5.8862470e-20], sampled 0.2983572943533249
[2019-04-28 01:34:17,254] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:17,255] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.83333333333334, 87.33333333333334, 1.0, 2.0, 0.2467792421964874, 1.0, 2.0, 0.2467792421964874, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 689668.208144072, 689668.2081440714, 240129.7243731551]
[2019-04-28 01:34:17,256] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:34:17,259] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.1408658e-17 8.6204048e-09 4.6604754e-27 1.0000000e+00 2.0497414e-20], sampled 0.7487745704859061
[2019-04-28 01:34:27,854] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:27,855] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.93448448, 98.457559, 1.0, 2.0, 0.1760191096790209, 1.0, 2.0, 0.1760191096790209, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 171.5212843490159, 544756.0889981993, 544756.0889981999, 239747.2648424866]
[2019-04-28 01:34:27,857] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:34:27,860] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.4097045e-17 9.1305967e-09 5.6267338e-27 1.0000000e+00 2.3604533e-20], sampled 0.767076477966484
[2019-04-28 01:34:39,399] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:39,400] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [34.6754614, 75.72313923, 1.0, 2.0, 0.6269674844692381, 1.0, 2.0, 0.6269674844692381, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1753015.944127035, 1753015.944127035, 344949.8023019689]
[2019-04-28 01:34:39,401] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:34:39,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.6201004e-17 1.1128027e-08 1.0740167e-26 1.0000000e+00 3.8307506e-20], sampled 0.21045708896548887
[2019-04-28 01:34:39,768] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:39,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.08333333333334, 74.5, 1.0, 2.0, 0.2848278924547893, 1.0, 2.0, 0.2848278924547893, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 796035.1094209693, 796035.1094209693, 247502.8742741121]
[2019-04-28 01:34:39,769] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:34:39,773] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.33373824e-17 6.84233648e-09 2.20003388e-27 1.00000000e+00
 1.16771593e-20], sampled 0.7713226275228203
[2019-04-28 01:34:41,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:41,256] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [37.81788958833334, 36.99576786666667, 1.0, 2.0, 0.5120857879547509, 1.0, 2.0, 0.5120857879547509, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1431592.327943929, 1431592.327943929, 304533.6985009195]
[2019-04-28 01:34:41,257] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:34:41,262] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3141981e-17 6.7888819e-09 2.1492669e-27 1.0000000e+00 1.1470014e-20], sampled 0.003945222207506038
[2019-04-28 01:34:41,526] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:34:41,527] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.5, 86.5, 1.0, 2.0, 0.4871677073324093, 1.0, 2.0, 0.4871677073324093, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1361898.958042628, 1361898.958042628, 296330.2786291078]
[2019-04-28 01:34:41,528] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:34:41,531] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.0218878e-16 1.8447830e-08 5.5816500e-26 1.0000000e+00 1.3170374e-19], sampled 0.7180109063232852
[2019-04-28 01:35:01,513] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.026765974]
[2019-04-28 01:35:01,514] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.39731866, 92.042942455, 1.0, 2.0, 0.2758916359282823, 1.0, 2.0, 0.2758916359282823, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 771051.2612393065, 771051.2612393065, 245872.9117408307]
[2019-04-28 01:35:01,515] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:35:01,517] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.06413474e-17 1.02679785e-08 8.23329505e-27 1.00000000e+00
 3.13987065e-20], sampled 0.24082476386809204
[2019-04-28 01:35:19,284] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:35:19,349] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:35:19,448] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:35:19,459] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:35:19,488] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:35:20,501] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1675000, evaluation results [1675000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:35:21,533] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3102940e-17 1.1523568e-08 5.6831900e-26 1.0000000e+00 8.5212221e-21], sum to 1.0000
[2019-04-28 01:35:21,540] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1390
[2019-04-28 01:35:21,545] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.63333333333334, 61.66666666666667, 1.0, 2.0, 0.4536793747369873, 1.0, 2.0, 0.4536793747369873, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1341055.109808217, 1341055.109808217, 294981.8679540043], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7314600.0000, 
sim time next is 7315200.0000, 
raw observation next is [27.6, 62.0, 1.0, 2.0, 0.4205954023368238, 1.0, 2.0, 0.4205954023368238, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1243032.955398284, 1243032.955398284, 285373.8830492143], 
processed observation next is [1.0, 0.6956521739130435, 0.5071090047393366, 0.62, 1.0, 1.0, 0.3019221714901491, 1.0, 1.0, 0.3019221714901491, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3452869320550789, 0.3452869320550789, 0.4259311687301706], 
reward next is 0.5741, 
noisyNet noise sample is [array([-0.8782157], dtype=float32), 0.9766109]. 
=============================================
[2019-04-28 01:35:27,390] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.7864626e-15 9.3118388e-08 2.9445363e-24 9.9999988e-01 6.1525911e-19], sum to 1.0000
[2019-04-28 01:35:27,406] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0810
[2019-04-28 01:35:27,410] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.0, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 518921.4894075326, 518921.489407532, 237344.9627042039], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7261200.0000, 
sim time next is 7261800.0000, 
raw observation next is [21.98333333333333, 88.83333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 517160.3852052891, 517160.3852052891, 236996.4726589375], 
processed observation next is [1.0, 0.043478260869565216, 0.24091627172195884, 0.8883333333333334, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.14365566255702475, 0.14365566255702475, 0.3537260785954291], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.15330741], dtype=float32), -1.2135592]. 
=============================================
[2019-04-28 01:35:36,847] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.3624700e-14 1.0561563e-07 1.2352783e-22 9.9999988e-01 6.7344837e-16], sum to 1.0000
[2019-04-28 01:35:36,855] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5524
[2019-04-28 01:35:36,860] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.66666666666666, 90.66666666666667, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 524935.4753501728, 524935.4753501728, 238391.9221591967], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7368000.0000, 
sim time next is 7368600.0000, 
raw observation next is [21.48333333333333, 90.83333333333334, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 516709.5315741389, 516709.5315741389, 236776.7563831902], 
processed observation next is [1.0, 0.2608695652173913, 0.21721958925750387, 0.9083333333333334, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1435304254372608, 0.1435304254372608, 0.3533981438555078], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8131302], dtype=float32), -1.0262651]. 
=============================================
[2019-04-28 01:35:38,450] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.5963632e-14 8.1286260e-08 2.1236640e-23 9.9999988e-01 2.5723681e-17], sum to 1.0000
[2019-04-28 01:35:38,459] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4413
[2019-04-28 01:35:38,463] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.21666666666667, 84.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 461482.9809774167, 461482.9809774167, 225792.5346822839], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7411800.0000, 
sim time next is 7412400.0000, 
raw observation next is [21.3, 84.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 460954.8706484271, 460954.8706484271, 225731.9557343884], 
processed observation next is [1.0, 0.8260869565217391, 0.2085308056872039, 0.84, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.12804301962456308, 0.12804301962456308, 0.3369133667677439], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.72359866], dtype=float32), 1.2884427]. 
=============================================
[2019-04-28 01:35:41,199] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3604911e-14 4.0187774e-07 2.4660906e-21 9.9999964e-01 7.3596528e-16], sum to 1.0000
[2019-04-28 01:35:41,206] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0806
[2019-04-28 01:35:41,213] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.6, 88.0, 1.0, 2.0, 0.2428044107221879, 1.0, 2.0, 0.2428044107221879, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 678554.3677958595, 678554.3677958595, 239777.3865147037], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7684200.0000, 
sim time next is 7684800.0000, 
raw observation next is [25.56666666666667, 88.0, 1.0, 2.0, 0.2427719766635116, 1.0, 2.0, 0.2427719766635116, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 678463.697150921, 678463.697150921, 239772.0039479839], 
processed observation next is [1.0, 0.9565217391304348, 0.41074249605055313, 0.88, 1.0, 1.0, 0.08767708031748385, 1.0, 1.0, 0.08767708031748385, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18846213809747805, 0.18846213809747805, 0.3578686626089312], 
reward next is 0.6421, 
noisyNet noise sample is [array([0.63938683], dtype=float32), 0.564868]. 
=============================================
[2019-04-28 01:35:51,044] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:35:51,046] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:35:51,078] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run9
[2019-04-28 01:35:56,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:35:56,479] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:35:56,507] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run9
[2019-04-28 01:35:56,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:35:56,691] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:35:56,724] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run9
[2019-04-28 01:36:00,757] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:00,758] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:00,775] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.143032e-16 1.687366e-07 9.507567e-27 9.999999e-01 3.801059e-18], sum to 1.0000
[2019-04-28 01:36:00,779] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run9
[2019-04-28 01:36:00,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4220
[2019-04-28 01:36:00,819] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.5, 62.33333333333334, 1.0, 2.0, 0.8072014128492434, 1.0, 2.0, 0.8072014128492434, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2257503.107295009, 2257503.107295009, 423382.3322374315], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7749600.0000, 
sim time next is 7750200.0000, 
raw observation next is [30.4, 62.66666666666666, 1.0, 2.0, 0.8110228386389969, 1.0, 2.0, 0.8110228386389969, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2268200.200980328, 2268200.200980328, 425262.9144711777], 
processed observation next is [1.0, 0.6956521739130435, 0.6398104265402843, 0.6266666666666666, 1.0, 1.0, 0.7723166730590324, 1.0, 1.0, 0.7723166730590324, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6300556113834245, 0.6300556113834245, 0.6347207678674294], 
reward next is 0.3653, 
noisyNet noise sample is [array([0.7029835], dtype=float32), -0.522601]. 
=============================================
[2019-04-28 01:36:00,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:00,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:00,892] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:00,894] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:00,916] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run9
[2019-04-28 01:36:00,951] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run9
[2019-04-28 01:36:02,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0151730e-14 6.2399153e-08 9.0609965e-23 9.9999988e-01 2.4707395e-16], sum to 1.0000
[2019-04-28 01:36:02,808] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5513
[2019-04-28 01:36:02,812] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.81666666666667, 64.16666666666667, 1.0, 2.0, 0.7882797166684661, 1.0, 2.0, 0.7882797166684661, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2204536.868909821, 2204536.86890982, 414200.4724995787], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7834200.0000, 
sim time next is 7834800.0000, 
raw observation next is [30.63333333333333, 65.33333333333334, 1.0, 2.0, 0.7412783856418576, 1.0, 2.0, 0.7412783856418576, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2072963.741885376, 2072963.741885376, 392292.1475945483], 
processed observation next is [1.0, 0.6956521739130435, 0.6508688783570299, 0.6533333333333334, 1.0, 1.0, 0.6882872116166958, 1.0, 1.0, 0.6882872116166958, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5758232616348267, 0.5758232616348267, 0.5855106680515646], 
reward next is 0.4145, 
noisyNet noise sample is [array([0.32344174], dtype=float32), 0.4442864]. 
=============================================
[2019-04-28 01:36:03,209] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:03,210] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:03,220] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run9
[2019-04-28 01:36:03,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:03,518] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:03,546] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run9
[2019-04-28 01:36:05,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:05,567] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:05,602] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run9
[2019-04-28 01:36:05,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:05,702] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:05,755] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run9
[2019-04-28 01:36:05,830] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:05,832] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:05,876] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run9
[2019-04-28 01:36:07,969] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4433543e-16 2.7615972e-09 8.7364626e-26 1.0000000e+00 9.9959086e-18], sum to 1.0000
[2019-04-28 01:36:07,973] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8600
[2019-04-28 01:36:07,980] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.86666666666667, 85.0, 1.0, 2.0, 0.3128597450538131, 1.0, 2.0, 0.3128597450538131, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 874414.6101092801, 874414.6101092801, 252402.393424152], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7886400.0000, 
sim time next is 7887000.0000, 
raw observation next is [26.93333333333333, 84.5, 1.0, 2.0, 0.3124143148995465, 1.0, 2.0, 0.3124143148995465, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 873169.1664232954, 873169.1664232954, 252312.9974971589], 
processed observation next is [1.0, 0.2608695652173913, 0.4755134281200631, 0.845, 1.0, 1.0, 0.17158351192716448, 1.0, 1.0, 0.17158351192716448, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.24254699067313762, 0.24254699067313762, 0.37658656342859537], 
reward next is 0.6234, 
noisyNet noise sample is [array([-0.02288015], dtype=float32), 1.227566]. 
=============================================
[2019-04-28 01:36:08,000] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[61.953384]
 [61.94896 ]
 [61.987873]
 [62.06673 ]
 [62.07801 ]], R is [[61.96300125]
 [61.96665192]
 [61.96903229]
 [61.97278214]
 [61.97959518]].
[2019-04-28 01:36:08,191] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:08,192] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:08,237] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run9
[2019-04-28 01:36:08,877] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:08,878] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:08,890] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run9
[2019-04-28 01:36:09,726] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:09,727] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:09,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run9
[2019-04-28 01:36:09,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:09,873] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:09,889] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run9
[2019-04-28 01:36:10,391] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.0450218e-13 1.0037493e-07 2.0881444e-20 9.9999988e-01 5.2158113e-16], sum to 1.0000
[2019-04-28 01:36:10,396] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6919
[2019-04-28 01:36:10,400] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [21.2, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 476694.1167101208, 476694.1167101201, 229057.500911], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 243000.0000, 
sim time next is 243600.0000, 
raw observation next is [21.16666666666667, 89.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 474833.1241908503, 474833.1241908503, 228691.9903277231], 
processed observation next is [0.0, 0.8260869565217391, 0.2022116903633494, 0.89, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.13189809005301398, 0.13189809005301398, 0.3413313288473479], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7024812], dtype=float32), -0.11346482]. 
=============================================
[2019-04-28 01:36:12,002] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:36:12,002] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:12,031] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run9
[2019-04-28 01:36:13,170] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-28 01:36:13,171] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:36:13,172] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:13,174] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:36:13,176] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:36:13,177] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:13,178] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:36:13,180] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:13,177] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:36:13,178] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:13,182] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:36:13,194] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run69
[2019-04-28 01:36:13,215] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run69
[2019-04-28 01:36:13,215] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run69
[2019-04-28 01:36:13,282] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run69
[2019-04-28 01:36:13,313] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run69
[2019-04-28 01:36:17,735] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:36:17,736] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.42210553666667, 71.42154743, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 171.5212843490159, 425724.8975607993, 425724.8975607986, 218875.2186715892]
[2019-04-28 01:36:17,736] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:36:17,742] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0086062e-10 2.9270586e-05 3.6558559e-16 9.9997067e-01 2.8556463e-12], sampled 0.17963177105482897
[2019-04-28 01:36:20,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:36:20,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.31821817333334, 81.46774534000001, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 403171.2874634861, 403171.2874634861, 214227.444235163]
[2019-04-28 01:36:20,151] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:36:20,153] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [9.6254837e-10 6.1334933e-05 4.5379248e-15 9.9993861e-01 1.8916563e-11], sampled 0.8450065294211948
[2019-04-28 01:36:34,958] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:36:34,959] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.83333333333334, 95.0, 1.0, 2.0, 0.2549771908282652, 1.0, 2.0, 0.2549771908282652, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 712581.2225994697, 712581.2225994697, 242249.5252225719]
[2019-04-28 01:36:34,960] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:36:34,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.7475526e-12 4.4758717e-06 6.0929591e-19 9.9999547e-01 2.3466951e-14], sampled 0.5038441958782119
[2019-04-28 01:36:42,199] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:36:42,200] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.27933450333333, 89.75855975166667, 1.0, 2.0, 0.2877264359974053, 1.0, 2.0, 0.2877264359974053, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 804141.2616944272, 804141.2616944272, 247754.0099550161]
[2019-04-28 01:36:42,203] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:36:42,205] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.4960662e-11 1.7182017e-05 5.9569703e-17 9.9998283e-01 7.3177721e-13], sampled 0.30385468638426827
[2019-04-28 01:37:14,950] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:37:14,951] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.67466321333333, 74.38559014, 1.0, 2.0, 0.7287833213248531, 1.0, 2.0, 0.7287833213248531, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2037962.566678502, 2037962.566678502, 387189.1785556645]
[2019-04-28 01:37:14,952] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:37:14,955] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9579061e-12 4.0052619e-06 4.1704738e-19 9.9999595e-01 1.7674982e-14], sampled 0.5051238311231454
[2019-04-28 01:37:28,361] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.025813421]
[2019-04-28 01:37:28,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.56964471666667, 80.30377623666668, 1.0, 2.0, 0.5786725636698477, 1.0, 2.0, 0.5786725636698477, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1617891.050494044, 1617891.050494044, 326718.8993378372]
[2019-04-28 01:37:28,362] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:37:28,365] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.2974378e-12 5.7190564e-06 1.4032290e-18 9.9999428e-01 4.3931554e-14], sampled 0.29145749468986804
[2019-04-28 01:37:45,827] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:37:45,860] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:37:46,268] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:37:46,270] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:37:46,402] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:37:47,424] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 1700000, evaluation results [1700000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:37:56,498] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.0245283e-15 9.9993217e-01 1.6548821e-26 6.7866225e-05 2.9868688e-19], sum to 1.0000
[2019-04-28 01:37:56,508] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1506
[2019-04-28 01:37:56,515] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.73333333333333, 90.83333333333334, 1.0, 2.0, 0.293988935910098, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471426.8767904231, 471426.8767904231, 165053.0886931898], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 251400.0000, 
sim time next is 252000.0000, 
raw observation next is [20.7, 91.0, 1.0, 2.0, 0.2919226770016959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 468235.5028795577, 468235.5028795577, 164831.5760485278], 
processed observation next is [0.0, 0.9565217391304348, 0.18009478672985785, 0.91, 1.0, 1.0, 0.14689479156830829, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13006541746654382, 0.13006541746654382, 0.24601727768436982], 
reward next is 0.7540, 
noisyNet noise sample is [array([0.9487184], dtype=float32), -0.7726377]. 
=============================================
[2019-04-28 01:37:56,544] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[67.376205]
 [67.40641 ]
 [67.4291  ]
 [67.48422 ]
 [67.5253  ]], R is [[67.46517944]
 [67.54418182]
 [67.62234497]
 [67.70034027]
 [67.77774811]].
[2019-04-28 01:38:02,104] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.3133617e-16 9.9997163e-01 1.1999118e-25 2.8356555e-05 9.0007477e-19], sum to 1.0000
[2019-04-28 01:38:02,111] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2845
[2019-04-28 01:38:02,120] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [18.3, 89.0, 1.0, 2.0, 0.2196828848417186, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 365275.653738632, 365275.6537386326, 157732.9339472855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 529200.0000, 
sim time next is 529800.0000, 
raw observation next is [18.23333333333333, 89.16666666666667, 1.0, 2.0, 0.2364949611716479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 393337.1472689391, 393337.1472689398, 159239.9564376285], 
processed observation next is [1.0, 0.13043478260869565, 0.06319115323854654, 0.8916666666666667, 1.0, 1.0, 0.08011441105017818, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10926031868581643, 0.10926031868581662, 0.23767157677257986], 
reward next is 0.7623, 
noisyNet noise sample is [array([-0.25882018], dtype=float32), 0.035495494]. 
=============================================
[2019-04-28 01:38:12,134] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8177734e-18 9.9999881e-01 3.1983486e-30 1.1677897e-06 1.8652543e-21], sum to 1.0000
[2019-04-28 01:38:12,142] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8879
[2019-04-28 01:38:12,146] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 53.66666666666667, 1.0, 2.0, 0.5577225204564762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 916337.0853982749, 916337.0853982749, 206833.5854249532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 492000.0000, 
sim time next is 492600.0000, 
raw observation next is [24.65, 53.83333333333333, 1.0, 2.0, 0.5499152841677151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903703.1610937818, 903703.1610937818, 205279.6561704139], 
processed observation next is [1.0, 0.6956521739130435, 0.3672985781990521, 0.5383333333333333, 1.0, 1.0, 0.4577292580333916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25102865585938383, 0.25102865585938383, 0.3063875465230058], 
reward next is 0.6936, 
noisyNet noise sample is [array([0.3835876], dtype=float32), -0.8442557]. 
=============================================
[2019-04-28 01:38:17,079] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.3228155e-16 9.9980515e-01 3.0543154e-27 1.9480979e-04 2.9460584e-20], sum to 1.0000
[2019-04-28 01:38:17,094] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0035
[2019-04-28 01:38:17,099] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.83333333333334, 48.33333333333333, 1.0, 2.0, 0.5711676738858369, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 937474.5379057351, 937474.5379057344, 209554.2178707939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 742800.0000, 
sim time next is 743400.0000, 
raw observation next is [25.75, 48.5, 1.0, 2.0, 0.5667411950691573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930732.9112582929, 930732.9112582929, 208653.4123552284], 
processed observation next is [1.0, 0.6086956521739131, 0.41943127962085314, 0.485, 1.0, 1.0, 0.4780014398423582, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.25853691979397025, 0.25853691979397025, 0.3114230035152663], 
reward next is 0.6886, 
noisyNet noise sample is [array([-1.8477429], dtype=float32), 0.7822834]. 
=============================================
[2019-04-28 01:38:21,408] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.4856078e-17 9.9998426e-01 2.4525264e-27 1.5773730e-05 1.4113742e-19], sum to 1.0000
[2019-04-28 01:38:21,416] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8776
[2019-04-28 01:38:21,420] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.31666666666667, 66.66666666666667, 1.0, 2.0, 0.2479955038477095, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408306.4379170497, 408306.4379170503, 160700.9683011164], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 672600.0000, 
sim time next is 673200.0000, 
raw observation next is [22.1, 68.0, 1.0, 2.0, 0.2483964544502939, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 409016.1123429117, 409016.1123429111, 160739.05740801], 
processed observation next is [1.0, 0.8260869565217391, 0.24644549763033188, 0.68, 1.0, 1.0, 0.09445355957866734, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11361558676191992, 0.11361558676191974, 0.2399090409074776], 
reward next is 0.7601, 
noisyNet noise sample is [array([0.80922186], dtype=float32), 1.658515]. 
=============================================
[2019-04-28 01:38:23,503] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.6977519e-16 9.9999809e-01 1.5493556e-27 1.9476824e-06 3.8924179e-21], sum to 1.0000
[2019-04-28 01:38:23,511] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1349
[2019-04-28 01:38:23,523] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.83333333333334, 1.0, 2.0, 0.4098947061082622, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633078.9493693406, 633078.9493693413, 178043.0394659328], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 958200.0000, 
sim time next is 958800.0000, 
raw observation next is [21.8, 94.66666666666667, 1.0, 2.0, 0.3648637457433531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 563820.101702934, 563820.1017029347, 171859.8805726792], 
processed observation next is [1.0, 0.08695652173913043, 0.23222748815165886, 0.9466666666666668, 1.0, 1.0, 0.23477559728114833, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15661669491748167, 0.15661669491748187, 0.2565072844368346], 
reward next is 0.7435, 
noisyNet noise sample is [array([2.0288694], dtype=float32), 1.1316863]. 
=============================================
[2019-04-28 01:38:23,602] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.2917334e-15 9.9997616e-01 2.8225607e-26 2.3892457e-05 5.6131967e-21], sum to 1.0000
[2019-04-28 01:38:23,611] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.2210
[2019-04-28 01:38:23,616] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 54.0, 1.0, 2.0, 0.6459675601153325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1060564.243799926, 1060564.243799927, 225845.9538199728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 664200.0000, 
sim time next is 664800.0000, 
raw observation next is [24.7, 54.0, 1.0, 2.0, 0.6503903047182602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1067798.144919577, 1067798.144919577, 226868.7953992902], 
processed observation next is [1.0, 0.6956521739130435, 0.3696682464454976, 0.54, 1.0, 1.0, 0.5787834996605544, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29661059581099364, 0.29661059581099364, 0.33861014238700027], 
reward next is 0.6614, 
noisyNet noise sample is [array([0.41910872], dtype=float32), 0.58515584]. 
=============================================
[2019-04-28 01:38:30,155] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.33754999e-16 9.99999881e-01 1.57839709e-29 1.06593916e-07
 5.80961347e-22], sum to 1.0000
[2019-04-28 01:38:30,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5028
[2019-04-28 01:38:30,175] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.01666666666667, 89.0, 1.0, 2.0, 0.2899520473730447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 464383.0859698807, 464383.0859698813, 164558.8641335424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 874200.0000, 
sim time next is 874800.0000, 
raw observation next is [21.0, 89.0, 1.0, 2.0, 0.2893319553539936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463545.5184226623, 463545.5184226629, 164502.6423600968], 
processed observation next is [0.0, 0.13043478260869565, 0.19431279620853087, 0.89, 1.0, 1.0, 0.14377344018553442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12876264400629508, 0.12876264400629525, 0.2455263318807415], 
reward next is 0.7545, 
noisyNet noise sample is [array([-1.148482], dtype=float32), -2.6927428]. 
=============================================
[2019-04-28 01:38:42,481] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8224330e-19 9.9999988e-01 2.0917682e-31 8.3822265e-08 6.3381346e-22], sum to 1.0000
[2019-04-28 01:38:42,483] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9129
[2019-04-28 01:38:42,501] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 94.16666666666667, 1.0, 2.0, 0.3442852464206104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532953.7233094486, 532953.7233094486, 169317.6355052818], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 960600.0000, 
sim time next is 961200.0000, 
raw observation next is [21.8, 94.0, 1.0, 2.0, 0.3433815576369807, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 531856.691001625, 531856.6910016245, 169237.5690268535], 
processed observation next is [1.0, 0.13043478260869565, 0.23222748815165886, 0.94, 1.0, 1.0, 0.2088934429361213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1477379697226736, 0.14773796972267345, 0.25259338660724406], 
reward next is 0.7474, 
noisyNet noise sample is [array([-1.0327587], dtype=float32), -0.85952616]. 
=============================================
[2019-04-28 01:38:49,080] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 01:38:49,084] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:38:49,085] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:38:49,088] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:38:49,087] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:38:49,092] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:38:49,090] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:38:49,093] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:38:49,093] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:38:49,089] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:38:49,096] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:38:49,115] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run70
[2019-04-28 01:38:49,155] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run70
[2019-04-28 01:38:49,182] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run70
[2019-04-28 01:38:49,185] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run70
[2019-04-28 01:38:49,248] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run70
[2019-04-28 01:39:10,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.03687486]
[2019-04-28 01:39:10,520] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [23.26666666666667, 95.0, 1.0, 2.0, 0.4254648239440919, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625776.7104243601, 625776.7104243601, 176726.6843406443]
[2019-04-28 01:39:10,521] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:39:10,522] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.8127385e-19 1.0000000e+00 1.1292389e-30 6.4928649e-09 4.3243742e-23], sampled 0.38168321850112796
[2019-04-28 01:39:13,234] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.03687486]
[2019-04-28 01:39:13,234] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.65, 87.5, 1.0, 2.0, 0.4862461486126139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679447.2992606371, 679447.2992606364, 181470.9392752565]
[2019-04-28 01:39:13,235] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:39:13,236] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.7893430e-19 1.0000000e+00 7.1748993e-30 1.0782692e-08 1.7252571e-22], sampled 0.07703417270592039
[2019-04-28 01:39:18,551] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.03687486]
[2019-04-28 01:39:18,554] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.16666666666667, 72.66666666666667, 1.0, 2.0, 0.5372195880071869, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768420.8207371898, 768420.8207371893, 191893.7520416712]
[2019-04-28 01:39:18,556] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:39:18,557] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6693557e-19 1.0000000e+00 4.8411841e-31 5.1489124e-09 2.2955630e-23], sampled 0.11928985080105858
[2019-04-28 01:39:55,622] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.03687486]
[2019-04-28 01:39:55,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.99790577, 65.02155832, 1.0, 2.0, 0.7436659904649265, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1039324.272072968, 1039324.272072968, 230548.5788862778]
[2019-04-28 01:39:55,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:39:55,625] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3584195e-19 1.0000000e+00 3.4636679e-31 4.7016959e-09 1.7875544e-23], sampled 0.6959412396761536
[2019-04-28 01:39:57,603] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.03687486]
[2019-04-28 01:39:57,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.68333333333333, 75.33333333333334, 1.0, 2.0, 0.5029184814889895, 0.0, 2.0, 0.0, 1.0, 2.0, 0.8734030846630632, 6.911199999999999, 6.9112, 168.9128793129575, 1405968.799746692, 1405968.799746693, 309777.9082853617]
[2019-04-28 01:39:57,604] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:39:57,618] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2270584e-16 9.9999988e-01 2.1743536e-26 9.6956796e-08 6.9511354e-20], sampled 0.8494246541039436
[2019-04-28 01:40:25,288] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:40:25,820] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:40:25,878] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:40:26,082] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:40:26,145] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:40:27,165] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1725000, evaluation results [1725000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:40:29,694] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8731403e-16 1.0000000e+00 5.6903863e-28 2.5030040e-08 5.4982918e-22], sum to 1.0000
[2019-04-28 01:40:29,704] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2363
[2019-04-28 01:40:29,706] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.95, 93.83333333333334, 1.0, 2.0, 0.2808686411031585, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 453996.2622211621, 453996.2622211621, 163863.8587897993], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1137000.0000, 
sim time next is 1137600.0000, 
raw observation next is [19.9, 94.0, 1.0, 2.0, 0.2792906045621624, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451695.6538115449, 451695.6538115443, 163708.5847561406], 
processed observation next is [1.0, 0.17391304347826086, 0.14218009478672985, 0.94, 1.0, 1.0, 0.13167542718332823, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12547101494765137, 0.1254710149476512, 0.2443411712778218], 
reward next is 0.7557, 
noisyNet noise sample is [array([0.26653254], dtype=float32), -2.2074702]. 
=============================================
[2019-04-28 01:40:34,810] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4763164e-16 1.0000000e+00 2.1485619e-27 2.7888273e-08 1.2757265e-21], sum to 1.0000
[2019-04-28 01:40:34,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6119
[2019-04-28 01:40:34,812] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1773120.232645628 W.
[2019-04-28 01:40:34,818] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.15, 71.5, 1.0, 2.0, 0.4227632955027453, 1.0, 1.0, 0.4227632955027453, 1.0, 1.0, 0.7151079160151979, 6.9112, 6.9112, 170.5573041426782, 1773120.232645628, 1773120.232645628, 362943.4992738596], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1251000.0000, 
sim time next is 1251600.0000, 
raw observation next is [28.16666666666666, 71.66666666666667, 1.0, 2.0, 0.403869534057663, 1.0, 2.0, 0.403869534057663, 1.0, 2.0, 0.6815560861032826, 6.911199999999999, 6.9112, 170.5573041426782, 1693814.890949372, 1693814.890949372, 352173.3089716416], 
processed observation next is [1.0, 0.4782608695652174, 0.5339652448657185, 0.7166666666666667, 1.0, 1.0, 0.28177052296103977, 1.0, 1.0, 0.28177052296103977, 1.0, 1.0, 0.6116537635405885, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4705041363748255, 0.4705041363748255, 0.5256318044352859], 
reward next is 0.4744, 
noisyNet noise sample is [array([-0.94693094], dtype=float32), -0.22949599]. 
=============================================
[2019-04-28 01:40:39,655] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.0657329e-18 1.0000000e+00 4.2562019e-27 3.8405314e-09 2.8905436e-20], sum to 1.0000
[2019-04-28 01:40:39,661] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8838
[2019-04-28 01:40:39,665] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 59.33333333333334, 1.0, 2.0, 0.9917971786540183, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1512080.254822638, 1512080.254822638, 315012.3284870182], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1176600.0000, 
sim time next is 1177200.0000, 
raw observation next is [27.6, 59.0, 1.0, 2.0, 0.99189320344214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1514832.984715675, 1514832.984715675, 315390.8894443341], 
processed observation next is [1.0, 0.6521739130434783, 0.5071090047393366, 0.59, 1.0, 1.0, 0.990232775231494, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.4207869401987986, 0.4207869401987986, 0.4707326708124389], 
reward next is 0.5293, 
noisyNet noise sample is [array([-2.3446865], dtype=float32), 0.77009636]. 
=============================================
[2019-04-28 01:40:50,284] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.9815925e-18 1.0000000e+00 3.4464471e-28 4.1404863e-10 1.2815796e-21], sum to 1.0000
[2019-04-28 01:40:50,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0860
[2019-04-28 01:40:50,297] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 54.0, 1.0, 2.0, 0.367701528525152, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557318.4125408985, 557318.4125408985, 170995.7174390102], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1522800.0000, 
sim time next is 1523400.0000, 
raw observation next is [28.75, 54.5, 1.0, 2.0, 0.3634607022821571, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 551734.8920864526, 551734.892086452, 170546.3071172856], 
processed observation next is [0.0, 0.6521739130434783, 0.561611374407583, 0.545, 1.0, 1.0, 0.23308518347247842, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1532596922462368, 0.15325969224623667, 0.2545467270407248], 
reward next is 0.7455, 
noisyNet noise sample is [array([0.7019599], dtype=float32), -0.2046809]. 
=============================================
[2019-04-28 01:40:51,639] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.3415367e-18 1.0000000e+00 1.5191313e-26 8.8654906e-09 3.3894029e-22], sum to 1.0000
[2019-04-28 01:40:51,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9236
[2019-04-28 01:40:51,651] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.83333333333334, 92.33333333333334, 1.0, 2.0, 0.3012415671543864, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 480381.7677212527, 480381.7677212527, 165658.9165345882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1358400.0000, 
sim time next is 1359000.0000, 
raw observation next is [20.85, 92.5, 1.0, 2.0, 0.298805755712317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 476170.2542246285, 476170.2542246285, 165354.0656031102], 
processed observation next is [1.0, 0.7391304347826086, 0.18720379146919444, 0.925, 1.0, 1.0, 0.15518765748471924, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13226951506239681, 0.13226951506239681, 0.246797112840463], 
reward next is 0.7532, 
noisyNet noise sample is [array([-1.353113], dtype=float32), -0.4437752]. 
=============================================
[2019-04-28 01:40:51,668] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[68.63347 ]
 [68.54968 ]
 [68.39305 ]
 [68.247925]
 [68.20144 ]], R is [[68.70709229]
 [68.77276611]
 [68.82591248]
 [68.81305695]
 [68.80036163]].
[2019-04-28 01:40:53,085] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [9.2549588e-21 1.0000000e+00 1.1392809e-31 5.6448418e-10 6.3270102e-26], sum to 1.0000
[2019-04-28 01:40:53,093] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6391
[2019-04-28 01:40:53,101] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.8, 96.0, 1.0, 2.0, 0.3566330248468683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548512.7026637949, 548512.7026637949, 170497.4604648544], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1468800.0000, 
sim time next is 1469400.0000, 
raw observation next is [21.76666666666667, 96.0, 1.0, 2.0, 0.3552193625251076, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 546872.3453331747, 546872.3453331747, 170375.8955436849], 
processed observation next is [0.0, 0.0, 0.23064770932069528, 0.96, 1.0, 1.0, 0.22315585846398506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15190898481477075, 0.15190898481477075, 0.2542923814084849], 
reward next is 0.7457, 
noisyNet noise sample is [array([0.31222722], dtype=float32), -1.66604]. 
=============================================
[2019-04-28 01:40:54,790] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.4977340e-18 1.0000000e+00 8.0395792e-31 6.3603717e-10 5.9146437e-23], sum to 1.0000
[2019-04-28 01:40:54,797] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2949
[2019-04-28 01:40:54,803] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.6, 88.5, 1.0, 2.0, 0.3895339520646209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 582382.9778227152, 582382.9778227152, 172958.5707820453], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1420200.0000, 
sim time next is 1420800.0000, 
raw observation next is [23.46666666666667, 88.33333333333334, 1.0, 2.0, 0.3838733222462882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576914.3328353864, 576914.3328353864, 172560.3639169473], 
processed observation next is [0.0, 0.43478260869565216, 0.31121642969984215, 0.8833333333333334, 1.0, 1.0, 0.25767870150155203, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16025398134316288, 0.16025398134316288, 0.257552781965593], 
reward next is 0.7424, 
noisyNet noise sample is [array([-0.6006798], dtype=float32), 0.014171681]. 
=============================================
[2019-04-28 01:40:57,979] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.6078153e-20 1.0000000e+00 3.1910161e-30 1.9747495e-10 5.2937079e-23], sum to 1.0000
[2019-04-28 01:40:57,985] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7441
[2019-04-28 01:40:57,990] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.86666666666667, 70.16666666666667, 1.0, 2.0, 0.3682610225889591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558880.3561709941, 558880.3561709941, 171152.7829510424], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1501800.0000, 
sim time next is 1502400.0000, 
raw observation next is [26.13333333333334, 68.33333333333334, 1.0, 2.0, 0.3658982396448303, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 555886.3358045889, 555886.3358045883, 170914.7795841877], 
processed observation next is [0.0, 0.391304347826087, 0.43759873617693557, 0.6833333333333335, 1.0, 1.0, 0.23602197547569917, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15441287105683024, 0.15441287105683008, 0.2550966859465488], 
reward next is 0.7449, 
noisyNet noise sample is [array([-2.5104783], dtype=float32), -0.69818324]. 
=============================================
[2019-04-28 01:40:59,795] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0790022e-18 1.0000000e+00 1.0780141e-27 1.2523748e-09 4.4887760e-21], sum to 1.0000
[2019-04-28 01:40:59,801] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1108
[2019-04-28 01:40:59,804] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.2, 99.0, 1.0, 2.0, 0.4300133032605817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622147.4625190102, 622147.4625190095, 176095.591419972], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1648800.0000, 
sim time next is 1649400.0000, 
raw observation next is [23.21666666666667, 99.0, 1.0, 2.0, 0.6278438088402513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 907952.4709085738, 907952.4709085738, 209994.7039322603], 
processed observation next is [1.0, 0.08695652173913043, 0.29936808846761465, 0.99, 1.0, 1.0, 0.5516190467954835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2522090196968261, 0.2522090196968261, 0.31342493124217957], 
reward next is 0.6866, 
noisyNet noise sample is [array([-0.29678172], dtype=float32), 1.2914556]. 
=============================================
[2019-04-28 01:41:01,528] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7575201e-16 1.0000000e+00 1.9384889e-27 7.1764950e-09 4.1770822e-19], sum to 1.0000
[2019-04-28 01:41:01,537] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9154
[2019-04-28 01:41:01,544] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 72.0, 1.0, 2.0, 0.3695166607759714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 560039.5352985215, 560039.5352985215, 171229.0261122018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1501200.0000, 
sim time next is 1501800.0000, 
raw observation next is [25.86666666666667, 70.16666666666667, 1.0, 2.0, 0.3682610225889591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558880.3561709941, 558880.3561709941, 171152.7829510424], 
processed observation next is [0.0, 0.391304347826087, 0.42496050552922615, 0.7016666666666667, 1.0, 1.0, 0.23886870191440854, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15524454338083168, 0.15524454338083168, 0.2554519148523021], 
reward next is 0.7445, 
noisyNet noise sample is [array([-0.24372308], dtype=float32), -1.6443728]. 
=============================================
[2019-04-28 01:41:03,955] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.9026463e-19 1.0000000e+00 5.1443106e-27 8.4227292e-11 2.7464233e-23], sum to 1.0000
[2019-04-28 01:41:03,964] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6418
[2019-04-28 01:41:03,970] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 97.16666666666667, 1.0, 2.0, 0.4168699389410615, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 611053.8734260148, 611053.8734260142, 175258.3735891297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1635000.0000, 
sim time next is 1635600.0000, 
raw observation next is [23.1, 97.33333333333334, 1.0, 2.0, 0.4170438703713091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 610836.2120430194, 610836.2120430188, 175224.0002879854], 
processed observation next is [1.0, 0.9565217391304348, 0.2938388625592418, 0.9733333333333334, 1.0, 1.0, 0.29764321731483023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16967672556750538, 0.1696767255675052, 0.26152835863878415], 
reward next is 0.7385, 
noisyNet noise sample is [array([0.16359603], dtype=float32), 0.4149187]. 
=============================================
[2019-04-28 01:41:07,300] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7858891e-18 1.0000000e+00 9.8206683e-29 6.9064232e-10 5.3676033e-23], sum to 1.0000
[2019-04-28 01:41:07,306] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1218
[2019-04-28 01:41:07,312] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5829598386777449, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 921903.4441623366, 921903.4441623372, 209993.3208716435], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1785600.0000, 
sim time next is 1786200.0000, 
raw observation next is [21.23333333333333, 92.33333333333334, 1.0, 2.0, 0.6651258961280077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1050948.669598726, 1050948.669598726, 227896.1378704305], 
processed observation next is [1.0, 0.6956521739130435, 0.2053712480252764, 0.9233333333333335, 1.0, 1.0, 0.5965372242506116, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29193018599964615, 0.29193018599964615, 0.3401434893588515], 
reward next is 0.6599, 
noisyNet noise sample is [array([0.3919632], dtype=float32), 0.025072861]. 
=============================================
[2019-04-28 01:41:07,938] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [6.0267379e-18 1.0000000e+00 3.7575182e-27 3.9759720e-09 9.8136256e-22], sum to 1.0000
[2019-04-28 01:41:07,942] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0001
[2019-04-28 01:41:07,945] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.3, 93.66666666666667, 1.0, 2.0, 0.4933639726387568, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 701216.7262283522, 701216.7262283515, 184076.0281955756], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1747200.0000, 
sim time next is 1747800.0000, 
raw observation next is [24.35, 93.5, 1.0, 2.0, 0.4873468549343948, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 691831.4654513524, 691831.465451353, 183027.3932835453], 
processed observation next is [1.0, 0.21739130434782608, 0.35308056872037924, 0.935, 1.0, 1.0, 0.3823456083546925, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19217540706982011, 0.19217540706982028, 0.27317521385603777], 
reward next is 0.7268, 
noisyNet noise sample is [array([-0.44377503], dtype=float32), -0.18826205]. 
=============================================
[2019-04-28 01:41:14,068] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.8747543e-19 1.0000000e+00 1.1390436e-28 2.1991718e-10 1.0707324e-25], sum to 1.0000
[2019-04-28 01:41:14,073] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5003
[2019-04-28 01:41:14,080] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1657558.662863713 W.
[2019-04-28 01:41:14,088] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [26.9, 88.0, 1.0, 2.0, 0.5928425779546759, 0.0, 1.0, 0.0, 1.0, 1.0, 1.009955882276768, 6.9112, 6.9112, 168.912956510431, 1657558.662863713, 1657558.662863713, 358790.4077967346], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1875600.0000, 
sim time next is 1876200.0000, 
raw observation next is [26.81666666666667, 87.83333333333334, 1.0, 2.0, 0.3171928551957647, 0.0, 2.0, 0.0, 1.0, 2.0, 0.538932095129981, 6.911199999999999, 6.9112, 168.912956510431, 886533.8327417544, 886533.8327417551, 226333.0848794523], 
processed observation next is [1.0, 0.7391304347826086, 0.46998420221169057, 0.8783333333333334, 1.0, 1.0, 0.17734078939248757, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 0.4377220672316841, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24625939798382065, 0.24625939798382085, 0.33781057444694373], 
reward next is 0.6622, 
noisyNet noise sample is [array([0.68794185], dtype=float32), 1.7841957]. 
=============================================
[2019-04-28 01:41:14,536] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.4569614e-20 1.0000000e+00 2.0905858e-33 1.0111768e-10 2.8651034e-23], sum to 1.0000
[2019-04-28 01:41:14,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1137
[2019-04-28 01:41:14,551] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.56666666666666, 87.0, 1.0, 2.0, 0.4755833633266069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 664543.1879125264, 664543.187912527, 179858.5614614788], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1885200.0000, 
sim time next is 1885800.0000, 
raw observation next is [25.48333333333333, 87.0, 1.0, 2.0, 0.4713921879715211, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 660599.020540633, 660599.0205406336, 179482.4295388486], 
processed observation next is [1.0, 0.8260869565217391, 0.40679304897314367, 0.87, 1.0, 1.0, 0.36312311803797725, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1834997279279536, 0.18349972792795377, 0.26788422319231137], 
reward next is 0.7321, 
noisyNet noise sample is [array([-0.21903121], dtype=float32), -0.73751426]. 
=============================================
[2019-04-28 01:41:17,755] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1782861e-18 1.0000000e+00 5.8392578e-28 9.8765591e-09 8.5822913e-21], sum to 1.0000
[2019-04-28 01:41:17,764] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3029
[2019-04-28 01:41:17,768] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 83.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.152305130557446, 6.9112, 168.9115474263927, 1624918.922166852, 1453872.071248269, 311348.3894367213], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1864200.0000, 
sim time next is 1864800.0000, 
raw observation next is [27.1, 83.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.447655113339775, 6.9112, 168.9096263515161, 1834587.815335496, 1454015.597791453, 311348.5156116763], 
processed observation next is [1.0, 0.6086956521739131, 0.4834123222748816, 0.83, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.053645511333977505, 0.0, 0.8294235925474771, 0.5096077264820823, 0.4038932216087369, 0.4646992770323527], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.32402465], dtype=float32), -1.7520399]. 
=============================================
[2019-04-28 01:41:19,407] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [6.21538756e-19 1.00000000e+00 1.00557834e-28 1.59966529e-09
 1.94144315e-18], sum to 1.0000
[2019-04-28 01:41:19,417] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3336
[2019-04-28 01:41:19,428] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.76666666666667, 95.0, 1.0, 2.0, 0.4317784329814955, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 622597.5390074137, 622597.5390074137, 176080.103448], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1914000.0000, 
sim time next is 1914600.0000, 
raw observation next is [23.73333333333333, 95.0, 1.0, 2.0, 0.4284719967872407, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 618673.8991838568, 618673.8991838562, 175721.5042489512], 
processed observation next is [1.0, 0.13043478260869565, 0.3238546603475513, 0.95, 1.0, 1.0, 0.31141204432197667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17185386088440466, 0.1718538608844045, 0.2622709018641063], 
reward next is 0.7377, 
noisyNet noise sample is [array([1.4973143], dtype=float32), -0.45655867]. 
=============================================
[2019-04-28 01:41:21,052] A3C_AGENT_WORKER-Thread-19 INFO:Evaluating...
[2019-04-28 01:41:21,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:41:21,055] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:41:21,056] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:41:21,057] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:41:21,058] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:41:21,058] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:41:21,059] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:41:21,058] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:41:21,060] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:41:21,061] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:41:21,081] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run71
[2019-04-28 01:41:21,082] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run71
[2019-04-28 01:41:21,131] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run71
[2019-04-28 01:41:21,166] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run71
[2019-04-28 01:41:21,192] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run71
[2019-04-28 01:41:26,550] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:41:26,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.83333333333334, 71.33333333333333, 1.0, 2.0, 0.2587202569919039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 424761.7324634299, 424761.7324634299, 161782.7777224516]
[2019-04-28 01:41:26,552] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:41:26,555] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6499222e-16 1.0000000e+00 4.1227532e-26 2.0339307e-08 6.4741061e-20], sampled 0.5277640036355778
[2019-04-28 01:41:28,125] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:41:28,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.81381496166667, 64.366381495, 1.0, 2.0, 0.3141098766451737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 510435.1438131496, 510435.1438131496, 167799.1677286312]
[2019-04-28 01:41:28,126] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:41:28,130] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.7769419e-16 1.0000000e+00 9.5837873e-26 2.6279032e-08 1.2266308e-19], sampled 0.7254339725135495
[2019-04-28 01:41:44,582] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:41:44,582] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [27.08333333333334, 81.5, 1.0, 2.0, 0.5047080748553062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705253.302998479, 705253.3029984796, 184336.6975541065]
[2019-04-28 01:41:44,583] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:41:44,586] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.9579695e-17 1.0000000e+00 1.8211954e-26 1.5879321e-08 3.4859963e-20], sampled 0.09735921760667288
[2019-04-28 01:41:47,127] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:41:47,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [29.95, 77.5, 1.0, 2.0, 0.5752929894162296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803922.383998383, 803922.383998383, 196233.4834917669]
[2019-04-28 01:41:47,131] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:41:47,134] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.85276840e-17 1.00000000e+00 3.91355513e-27 9.95903537e-09
 1.08738926e-20], sampled 0.5536123327906125
[2019-04-28 01:41:51,804] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:41:51,805] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.873684785, 100.0, 1.0, 2.0, 0.3902102787063758, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587250.2242502442, 587250.2242502448, 173514.6468465509]
[2019-04-28 01:41:51,805] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:41:51,808] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4156368e-16 1.0000000e+00 3.2200273e-26 1.8878465e-08 5.3673659e-20], sampled 0.7548016542444772
[2019-04-28 01:42:03,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:42:03,733] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.83333333333334, 58.00000000000001, 1.0, 2.0, 0.5852886687966733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 817895.8626328991, 817895.8626328997, 198036.9862681575]
[2019-04-28 01:42:03,734] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:42:03,736] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.3809236e-17 1.0000000e+00 4.8186790e-27 1.0608903e-08 1.2731504e-20], sampled 0.1337083919341726
[2019-04-28 01:42:10,777] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:42:10,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [33.68873833333333, 53.71753661666666, 1.0, 2.0, 0.54509989291916, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 761715.0046742702, 761715.0046742702, 190963.992761027]
[2019-04-28 01:42:10,782] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:42:10,784] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.5165927e-17 1.0000000e+00 1.4136545e-26 1.4703713e-08 2.8773859e-20], sampled 0.09031825354548084
[2019-04-28 01:42:10,918] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:42:10,919] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [32.87463793166666, 62.90845743833334, 1.0, 2.0, 0.5839593479869051, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 816037.5255290811, 816037.5255290817, 197795.7012363465]
[2019-04-28 01:42:10,920] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:42:10,922] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.3563958e-17 1.0000000e+00 1.1153489e-26 1.3685384e-08 2.4044578e-20], sampled 0.4028023848288652
[2019-04-28 01:42:29,317] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:42:29,318] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.3, 85.16666666666667, 1.0, 2.0, 0.5289172763333795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 739093.7742306674, 739093.7742306667, 188250.7419989714]
[2019-04-28 01:42:29,319] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:42:29,320] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.6780733e-16 1.0000000e+00 4.2430246e-26 2.0527782e-08 6.6112223e-20], sampled 0.43739383048176983
[2019-04-28 01:42:30,079] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.040727075]
[2019-04-28 01:42:30,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.41666666666667, 82.66666666666666, 1.0, 2.0, 0.5321839862161647, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743660.178473106, 743660.1784731053, 188791.2457624983]
[2019-04-28 01:42:30,081] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:42:30,083] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.9145717e-17 1.0000000e+00 7.8361221e-27 1.2296501e-08 1.8400697e-20], sampled 0.381558984792844
[2019-04-28 01:42:46,429] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:42:46,526] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:42:46,543] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 01:42:46,567] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:42:46,605] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:42:47,621] A3C_AGENT_WORKER-Thread-19 INFO:Global step: 1750000, evaluation results [1750000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:42:48,784] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5740125e-16 1.0000000e+00 9.9373509e-27 3.9738360e-08 2.3605691e-21], sum to 1.0000
[2019-04-28 01:42:48,793] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4483
[2019-04-28 01:42:48,799] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.56666666666667, 94.0, 1.0, 2.0, 0.4698459700214774, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 658284.6062091653, 658284.6062091653, 179234.0391854889], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2071200.0000, 
sim time next is 2071800.0000, 
raw observation next is [24.55, 94.0, 1.0, 2.0, 0.4691981276105786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 657875.5376726583, 657875.5376726589, 179202.5531599442], 
processed observation next is [0.0, 1.0, 0.3625592417061612, 0.94, 1.0, 1.0, 0.36047967181997426, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18274320490907175, 0.18274320490907192, 0.267466497253648], 
reward next is 0.7325, 
noisyNet noise sample is [array([-1.780428], dtype=float32), -0.2260037]. 
=============================================
[2019-04-28 01:42:54,079] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.1021099e-16 9.9999917e-01 1.0278006e-26 8.0823600e-07 2.6100433e-19], sum to 1.0000
[2019-04-28 01:42:54,087] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8303
[2019-04-28 01:42:54,095] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.33333333333334, 1.0, 2.0, 0.3977087150440519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 594618.8468944151, 594618.8468944144, 174077.3228205855], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1974000.0000, 
sim time next is 1974600.0000, 
raw observation next is [22.85, 95.5, 1.0, 2.0, 0.3992332777631029, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 595434.7233784811, 595434.7233784818, 174110.2442602914], 
processed observation next is [1.0, 0.8695652173913043, 0.28199052132701435, 0.955, 1.0, 1.0, 0.2761846720037385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16539853427180032, 0.1653985342718005, 0.25986603620939014], 
reward next is 0.7401, 
noisyNet noise sample is [array([1.5181036], dtype=float32), -1.2984226]. 
=============================================
[2019-04-28 01:42:54,951] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [9.8798325e-16 9.9999928e-01 1.6179694e-26 7.4448258e-07 5.7967731e-21], sum to 1.0000
[2019-04-28 01:42:54,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9161
[2019-04-28 01:42:54,974] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.01666666666667, 85.33333333333334, 1.0, 2.0, 0.5163664178829157, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 721549.6093522084, 721549.6093522079, 186200.1765266009], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2034600.0000, 
sim time next is 2035200.0000, 
raw observation next is [27.13333333333333, 84.66666666666667, 1.0, 2.0, 0.5167793546206902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722126.8266303921, 722126.8266303921, 186266.9261211203], 
processed observation next is [0.0, 0.5652173913043478, 0.484992101105845, 0.8466666666666667, 1.0, 1.0, 0.4178064513502292, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20059078517510892, 0.20059078517510892, 0.27801033749420945], 
reward next is 0.7220, 
noisyNet noise sample is [array([-0.36181062], dtype=float32), -2.6976554]. 
=============================================
[2019-04-28 01:42:55,591] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0291062e-15 9.9998045e-01 2.3187049e-26 1.9589430e-05 2.8905849e-21], sum to 1.0000
[2019-04-28 01:42:55,596] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.6843
[2019-04-28 01:42:55,603] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 89.66666666666667, 1.0, 2.0, 0.5391069384648353, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753337.5638576915, 753337.5638576915, 189950.0373977612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2150400.0000, 
sim time next is 2151000.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5382187451808397, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752095.9804222521, 752095.9804222527, 189800.6613054311], 
processed observation next is [0.0, 0.9130434782608695, 0.4715639810426541, 0.9, 1.0, 1.0, 0.4436370423865538, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20891555011729224, 0.2089155501172924, 0.28328456911258376], 
reward next is 0.7167, 
noisyNet noise sample is [array([0.22850819], dtype=float32), -0.48891822]. 
=============================================
[2019-04-28 01:42:55,626] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[69.47876]
 [69.48703]
 [69.50785]
 [69.48955]
 [69.4942 ]], R is [[69.49599457]
 [69.51752472]
 [69.53927612]
 [69.56047821]
 [69.58103943]].
[2019-04-28 01:42:56,854] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.2064703e-14 9.9996197e-01 2.5263214e-25 3.8027167e-05 7.4475107e-18], sum to 1.0000
[2019-04-28 01:42:56,861] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0536
[2019-04-28 01:42:56,872] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 94.0, 1.0, 2.0, 0.472007039567803, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 660407.1712579873, 660407.1712579873, 179437.8043038484], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2070000.0000, 
sim time next is 2070600.0000, 
raw observation next is [24.58333333333334, 94.00000000000001, 1.0, 2.0, 0.4709198399032556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 659292.5582719826, 659292.5582719819, 179329.0786444363], 
processed observation next is [0.0, 1.0, 0.3641390205371251, 0.9400000000000002, 1.0, 1.0, 0.362554023979826, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18313682174221738, 0.18313682174221718, 0.2676553412603527], 
reward next is 0.7323, 
noisyNet noise sample is [array([-0.71813864], dtype=float32), -0.8165055]. 
=============================================
[2019-04-28 01:42:58,458] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8650864e-10 9.9819970e-01 5.6546385e-18 1.8002891e-03 3.6193157e-13], sum to 1.0000
[2019-04-28 01:42:58,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5014
[2019-04-28 01:42:58,473] A3C_AGENT_WORKER-Thread-12 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2174654.535085524 W.
[2019-04-28 01:42:58,477] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.1, 73.5, 1.0, 2.0, 0.7776054792463697, 1.0, 1.0, 0.7776054792463697, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2174654.535085524, 2174654.535085524, 409117.5871417539], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2199000.0000, 
sim time next is 2199600.0000, 
raw observation next is [30.2, 73.0, 1.0, 2.0, 0.9704280398682823, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 7.000196756361241, 6.9112, 168.9124272085792, 2253611.387170047, 2190474.223021243, 454556.2861661902], 
processed observation next is [1.0, 0.4782608695652174, 0.6303317535545023, 0.73, 1.0, 1.0, 0.9643711323714245, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008899675636124104, 0.0, 0.8294373460378949, 0.6260031631027908, 0.6084650619503452, 0.6784422181584929], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46953496], dtype=float32), -0.17020683]. 
=============================================
[2019-04-28 01:43:01,672] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0514610e-08 9.4072771e-01 2.0299256e-14 5.9272300e-02 3.2097821e-11], sum to 1.0000
[2019-04-28 01:43:01,677] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3880
[2019-04-28 01:43:01,684] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2058183.014306735 W.
[2019-04-28 01:43:01,691] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.7, 80.0, 1.0, 2.0, 0.8308020888640424, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.992143855976874, 6.9112, 168.9115868268401, 2058183.014306735, 2000759.1226476, 416112.479292169], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2191200.0000, 
sim time next is 2191800.0000, 
raw observation next is [28.8, 79.5, 1.0, 2.0, 0.6919806326880582, 1.0, 1.0, 0.6919806326880582, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1934979.490138309, 1934979.490138309, 370711.5670573479], 
processed observation next is [1.0, 0.34782608695652173, 0.5639810426540285, 0.795, 1.0, 1.0, 0.6288923285398291, 1.0, 0.5, 0.6288923285398291, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5374943028161969, 0.5374943028161969, 0.5533008463542506], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.94201], dtype=float32), -0.18210933]. 
=============================================
[2019-04-28 01:43:16,391] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [6.09112717e-12 9.55839038e-01 1.35291231e-19 4.41608913e-02
 1.08821094e-13], sum to 1.0000
[2019-04-28 01:43:16,399] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2639
[2019-04-28 01:43:16,402] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.45, 80.5, 1.0, 2.0, 0.3492685278078584, 1.0, 2.0, 0.3492685278078584, 1.0, 1.0, 0.602578348062562, 6.911200000000001, 6.9112, 170.5573041426782, 1464663.679368004, 1464663.679368003, 326111.4072608741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2428200.0000, 
sim time next is 2428800.0000, 
raw observation next is [28.4, 80.66666666666667, 1.0, 2.0, 0.996118804109452, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911199999999999, 6.9112, 168.9129565103914, 1392375.75264825, 1392375.752648251, 297746.026358713], 
processed observation next is [1.0, 0.08695652173913043, 0.5450236966824644, 0.8066666666666668, 1.0, 1.0, 0.9953238603728337, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451521083, 0.38677104240229165, 0.38677104240229193, 0.4443970542667358], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.52854073], dtype=float32), -1.1712526]. 
=============================================
[2019-04-28 01:43:18,044] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.5516198e-16 9.9833763e-01 5.3114744e-27 1.6623938e-03 9.1823097e-19], sum to 1.0000
[2019-04-28 01:43:18,052] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5432
[2019-04-28 01:43:18,054] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3911240502363542, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 584766.6491672514, 584766.649167252, 173175.038959612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2731800.0000, 
sim time next is 2732400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3920897423274511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585345.7712773788, 585345.7712773794, 173201.1637201645], 
processed observation next is [0.0, 0.6521739130434783, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26757800280415794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16259604757704965, 0.16259604757704982, 0.2585091995823351], 
reward next is 0.7415, 
noisyNet noise sample is [array([-1.9124992], dtype=float32), -0.20675178]. 
=============================================
[2019-04-28 01:43:19,769] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4905339e-15 9.9951065e-01 6.2577974e-27 4.8937130e-04 1.0258773e-18], sum to 1.0000
[2019-04-28 01:43:19,776] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0947
[2019-04-28 01:43:19,779] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3519854848168404, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 541574.4567812396, 541574.4567812396, 169927.0531026155], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2761200.0000, 
sim time next is 2761800.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.349903153279406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 538978.7727720743, 538978.772772075, 169731.5328070277], 
processed observation next is [0.0, 1.0, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.21675078708362172, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14971632577002064, 0.14971632577002084, 0.2533306459806383], 
reward next is 0.7467, 
noisyNet noise sample is [array([-1.0179014], dtype=float32), 0.30330163]. 
=============================================
[2019-04-28 01:43:20,679] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3398739e-14 9.9669445e-01 9.0990609e-24 3.3056133e-03 2.7676992e-16], sum to 1.0000
[2019-04-28 01:43:20,687] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8565
[2019-04-28 01:43:20,692] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 92.33333333333334, 1.0, 2.0, 0.516654771935371, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777291.1511361116, 777291.1511361116, 193052.4794744494], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2896800.0000, 
sim time next is 2897400.0000, 
raw observation next is [23.0, 93.16666666666667, 1.0, 2.0, 0.5213168263441044, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781545.8609867118, 781545.8609867118, 193553.6008072828], 
processed observation next is [1.0, 0.5217391304347826, 0.28909952606635075, 0.9316666666666668, 1.0, 1.0, 0.42327328475193293, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21709607249630883, 0.21709607249630883, 0.28888597135415345], 
reward next is 0.7111, 
noisyNet noise sample is [array([-0.22498833], dtype=float32), 1.2252126]. 
=============================================
[2019-04-28 01:43:24,914] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.2247109e-14 9.9965787e-01 8.5046574e-24 3.4218383e-04 1.4990779e-17], sum to 1.0000
[2019-04-28 01:43:24,922] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1011
[2019-04-28 01:43:24,927] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.580029367333351, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893389.2896647172, 893389.2896647172, 207076.6346491899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2795400.0000, 
sim time next is 2796000.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.6041777446004031, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 930524.5553125858, 930524.5553125851, 211986.2257014145], 
processed observation next is [1.0, 0.34782608695652173, 0.2417061611374408, 0.94, 1.0, 1.0, 0.5231057163860278, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.25847904314238496, 0.25847904314238473, 0.316397351793156], 
reward next is 0.6836, 
noisyNet noise sample is [array([2.3072839], dtype=float32), 0.7868627]. 
=============================================
[2019-04-28 01:43:24,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[66.16422 ]
 [66.608986]
 [67.14809 ]
 [67.163826]
 [67.02046 ]], R is [[65.84159088]
 [65.87410736]
 [65.92305756]
 [66.00302124]
 [66.08653259]].
[2019-04-28 01:43:30,053] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.8890177e-17 9.9986470e-01 4.9425294e-27 1.3530244e-04 1.6475393e-19], sum to 1.0000
[2019-04-28 01:43:30,061] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.6020
[2019-04-28 01:43:30,068] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3473481168878181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 535081.3018466077, 535081.3018466084, 169413.6413674864], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2772000.0000, 
sim time next is 2772600.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.453426618058298, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698485.0806789106, 698485.0806789106, 184441.2760997399], 
processed observation next is [1.0, 0.08695652173913043, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.3414778530822867, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19402363352191962, 0.19402363352191962, 0.2752854867160297], 
reward next is 0.7247, 
noisyNet noise sample is [array([1.4173502], dtype=float32), 0.6041766]. 
=============================================
[2019-04-28 01:43:36,551] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.3578932e-15 9.9996138e-01 7.2791978e-27 3.8647504e-05 1.5078947e-19], sum to 1.0000
[2019-04-28 01:43:36,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5935
[2019-04-28 01:43:36,566] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 97.0, 1.0, 2.0, 0.8339151105066627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1229316.07873723, 1229316.078737229, 261326.0559229381], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3072600.0000, 
sim time next is 3073200.0000, 
raw observation next is [23.0, 98.0, 1.0, 2.0, 0.8501543741860447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1247407.785213543, 1247407.785213543, 264967.8854743962], 
processed observation next is [1.0, 0.5652173913043478, 0.28909952606635075, 0.98, 1.0, 1.0, 0.8194631014289695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.34650216255931754, 0.34650216255931754, 0.3954744559319346], 
reward next is 0.6045, 
noisyNet noise sample is [array([0.23677143], dtype=float32), 1.2415209]. 
=============================================
[2019-04-28 01:43:37,276] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [9.4717448e-16 9.9999201e-01 1.3644008e-28 7.9797846e-06 1.5326687e-20], sum to 1.0000
[2019-04-28 01:43:37,283] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.2866
[2019-04-28 01:43:37,288] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.5216300405108971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 824621.5375973925, 824621.5375973919, 198019.8620681615], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2996400.0000, 
sim time next is 2997000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.5037235837189078, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 796358.1181283569, 796358.1181283569, 194773.7812918411], 
processed observation next is [1.0, 0.6956521739130435, 0.19431279620853087, 0.94, 1.0, 1.0, 0.4020766068902503, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22121058836898802, 0.22121058836898802, 0.29070713625647926], 
reward next is 0.7093, 
noisyNet noise sample is [array([0.4171792], dtype=float32), -0.15935147]. 
=============================================
[2019-04-28 01:43:37,310] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.49182 ]
 [76.37898 ]
 [76.361946]
 [76.31589 ]
 [76.31592 ]], R is [[76.4430542 ]
 [76.3830719 ]
 [76.2996521 ]
 [76.21820068]
 [76.15364838]].
[2019-04-28 01:43:38,005] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.0791982e-14 9.9999106e-01 4.0101840e-25 8.9531004e-06 3.3439295e-18], sum to 1.0000
[2019-04-28 01:43:38,013] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8405
[2019-04-28 01:43:38,017] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 100.0, 1.0, 2.0, 0.3388692469458607, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526324.1206999633, 526324.1206999633, 168834.9751701775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2965200.0000, 
sim time next is 2965800.0000, 
raw observation next is [21.0, 100.0, 1.0, 2.0, 0.3387240040536023, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 526098.6374142431, 526098.6374142431, 168816.9948964921], 
processed observation next is [1.0, 0.30434782608695654, 0.19431279620853087, 1.0, 1.0, 1.0, 0.20328193259470156, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1461385103928453, 0.1461385103928453, 0.25196566402461507], 
reward next is 0.7480, 
noisyNet noise sample is [array([-0.12637576], dtype=float32), 0.18684463]. 
=============================================
[2019-04-28 01:43:39,263] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-28 01:43:39,264] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:43:39,264] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:43:39,266] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:43:39,267] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:43:39,269] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:43:39,274] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:43:39,275] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:43:39,275] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:43:39,276] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:43:39,278] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:43:39,295] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run72
[2019-04-28 01:43:39,319] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run72
[2019-04-28 01:43:39,347] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run72
[2019-04-28 01:43:39,347] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run72
[2019-04-28 01:43:39,427] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run72
[2019-04-28 01:43:41,491] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:43:41,492] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.75, 87.0, 1.0, 2.0, 0.4023576323288061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 603310.1283434866, 603310.1283434872, 174920.9918076802]
[2019-04-28 01:43:41,492] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:43:41,495] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.46431031e-17 9.99987721e-01 6.55453423e-29 1.23353275e-05
 7.30071386e-21], sampled 0.9356975317226568
[2019-04-28 01:43:51,737] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:43:51,739] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.16666666666667, 88.50000000000001, 1.0, 2.0, 0.2961551314830686, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 473600.9973161604, 473600.9973161598, 165194.124527132]
[2019-04-28 01:43:51,739] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:43:51,742] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.25944624e-17 9.99985695e-01 1.52344874e-28 1.42857762e-05
 1.33564676e-20], sampled 0.2848991738322981
[2019-04-28 01:43:54,153] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:43:54,155] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [22.4, 53.0, 1.0, 2.0, 0.2097098558692989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 350535.848185152, 350535.848185152, 156166.1406197708]
[2019-04-28 01:43:54,155] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:43:54,158] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.6591697e-16 9.9998164e-01 6.3933156e-28 1.8339797e-05 3.7298007e-20], sampled 0.5032916281401904
[2019-04-28 01:44:24,295] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:44:24,295] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.23333333333333, 78.5, 1.0, 2.0, 0.5646680883666534, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789069.4853079118, 789069.4853079118, 194347.8063102561]
[2019-04-28 01:44:24,296] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:44:24,297] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3253766e-17 9.9998868e-01 3.9338630e-29 1.1289433e-05 5.0648422e-21], sampled 0.5416205583699782
[2019-04-28 01:44:32,418] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:44:32,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.91666666666666, 51.66666666666667, 1.0, 2.0, 0.7489432098781795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1046703.183250568, 1046703.183250568, 231771.4698798735]
[2019-04-28 01:44:32,421] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:44:32,425] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5924375e-17 9.9998951e-01 2.5533206e-29 1.0469854e-05 3.7166760e-21], sampled 0.2554568203957107
[2019-04-28 01:45:11,583] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.045918237]
[2019-04-28 01:45:11,584] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.78333333333333, 82.66666666666667, 1.0, 2.0, 0.5739226107777129, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 802006.6749534471, 802006.6749534465, 195987.7354590795]
[2019-04-28 01:45:11,585] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:45:11,587] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.2918930e-17 9.9998879e-01 3.8637549e-29 1.1251889e-05 5.0003581e-21], sampled 0.27227936301937394
[2019-04-28 01:45:14,147] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 01:45:14,202] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 01:45:14,348] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 01:45:14,386] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1317 2927197757.6835 1337.0000
[2019-04-28 01:45:14,427] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 01:45:15,443] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1775000, evaluation results [1775000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.13174521227, 2927197757.683502, 1337.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 01:45:19,132] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.9287843e-18 9.9999630e-01 5.4852454e-28 3.7069526e-06 1.1859415e-21], sum to 1.0000
[2019-04-28 01:45:19,143] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7928
[2019-04-28 01:45:19,154] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3857020380339127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 580940.6695455962, 580940.6695455955, 172959.6041456942], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3106800.0000, 
sim time next is 3107400.0000, 
raw observation next is [22.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3860861780381882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580864.0421613574, 580864.0421613574, 172933.2392535416], 
processed observation next is [1.0, 1.0, 0.24960505529225935, 0.9900000000000001, 1.0, 1.0, 0.26034479281709416, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16135112282259925, 0.16135112282259925, 0.2581093123187188], 
reward next is 0.7419, 
noisyNet noise sample is [array([-0.06728528], dtype=float32), -0.919791]. 
=============================================
[2019-04-28 01:45:26,013] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0128012e-17 9.9999309e-01 2.5245780e-26 6.8849640e-06 1.1094044e-19], sum to 1.0000
[2019-04-28 01:45:26,027] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4399
[2019-04-28 01:45:26,032] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.0, 1.0, 2.0, 0.3966181482058797, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 591816.2734095218, 591816.2734095218, 173784.5850322915], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3111600.0000, 
sim time next is 3112200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3930914692769988, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586550.9406073227, 586550.9406073234, 173302.0483535628], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.268784902743372, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16293081683536742, 0.16293081683536761, 0.258659773662034], 
reward next is 0.7413, 
noisyNet noise sample is [array([0.7493821], dtype=float32), -0.605512]. 
=============================================
[2019-04-28 01:45:29,525] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.2159178e-17 9.9999917e-01 3.8028769e-29 8.3905104e-07 2.0502167e-20], sum to 1.0000
[2019-04-28 01:45:29,534] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5069
[2019-04-28 01:45:29,563] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.5, 73.0, 1.0, 2.0, 0.5976650794464909, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 835197.743714509, 835197.7437145095, 200312.3403662385], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3259800.0000, 
sim time next is 3260400.0000, 
raw observation next is [31.33333333333333, 73.66666666666667, 1.0, 2.0, 0.5970997092513171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 834407.3656158712, 834407.3656158712, 200207.3675610389], 
processed observation next is [0.0, 0.7391304347826086, 0.6840442338072668, 0.7366666666666667, 1.0, 1.0, 0.514577962953394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23177982378218642, 0.23177982378218642, 0.2988169665090133], 
reward next is 0.7012, 
noisyNet noise sample is [array([0.68036294], dtype=float32), 0.6893115]. 
=============================================
[2019-04-28 01:45:29,938] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2281399e-16 9.9999952e-01 1.2048439e-25 5.1061431e-07 2.2173985e-19], sum to 1.0000
[2019-04-28 01:45:29,944] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2181
[2019-04-28 01:45:29,950] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4880088542916172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681911.1748592041, 681911.1748592034, 181738.5422265647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3184800.0000, 
sim time next is 3185400.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.489418320808139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 683881.303761665, 683881.303761665, 181954.5824861134], 
processed observation next is [1.0, 0.8695652173913043, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3848413503712519, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18996702882268474, 0.18996702882268474, 0.271574003710617], 
reward next is 0.7284, 
noisyNet noise sample is [array([-1.5638689], dtype=float32), 0.27118972]. 
=============================================
[2019-04-28 01:45:38,045] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.8337792e-11 9.9997556e-01 1.1265876e-21 2.4397508e-05 6.5585787e-15], sum to 1.0000
[2019-04-28 01:45:38,057] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7081
[2019-04-28 01:45:38,072] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 94.0, 1.0, 2.0, 0.7686523751373718, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1074262.123078537, 1074262.123078537, 236360.9018411846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3383400.0000, 
sim time next is 3384000.0000, 
raw observation next is [26.0, 94.0, 1.0, 2.0, 0.7715458770249043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1078308.110315211, 1078308.110315211, 237045.5242425283], 
processed observation next is [1.0, 0.17391304347826086, 0.4312796208530806, 0.94, 1.0, 1.0, 0.7247540687047039, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2995300306431141, 0.2995300306431141, 0.3537992899142213], 
reward next is 0.6462, 
noisyNet noise sample is [array([1.104627], dtype=float32), 0.19512822]. 
=============================================
[2019-04-28 01:45:38,093] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[53.32471 ]
 [54.025753]
 [53.907597]
 [54.34135 ]
 [54.892155]], R is [[53.08401871]
 [53.20040131]
 [53.31309891]
 [53.42461395]
 [53.52998734]].
[2019-04-28 01:45:51,535] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7214112e-06 9.1878420e-01 6.3654138e-11 8.1214055e-02 2.9710744e-08], sum to 1.0000
[2019-04-28 01:45:51,543] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7208
[2019-04-28 01:45:51,556] A3C_AGENT_WORKER-Thread-10 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1991693.660057047 W.
[2019-04-28 01:45:51,560] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.83333333333334, 66.66666666666666, 1.0, 2.0, 0.7832944824947617, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.990887385295956, 6.9112, 168.9124822795563, 1991693.660057047, 1935160.845018876, 404319.1849449982], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3577800.0000, 
sim time next is 3578400.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.7982792190694742, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989981180449307, 6.9112, 168.912422954956, 2012665.12200691, 1956775.217700545, 408012.1277226138], 
processed observation next is [1.0, 0.43478260869565216, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7569629145415352, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007878118044930726, 0.0, 0.8294373251506577, 0.5590736450019195, 0.5435486715834847, 0.608973324959125], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.8883658], dtype=float32), -0.9656579]. 
=============================================
[2019-04-28 01:45:53,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.9677372e-12 9.5697016e-01 5.0824364e-21 4.3029889e-02 3.5524253e-14], sum to 1.0000
[2019-04-28 01:45:53,674] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-28 01:45:53,687] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 71.66666666666667, 1.0, 2.0, 0.55988969282904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 782389.6767051055, 782389.676705106, 193511.9434719728], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3522000.0000, 
sim time next is 3522600.0000, 
raw observation next is [30.5, 72.5, 1.0, 2.0, 0.5586218292961201, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780617.3135819572, 780617.3135819572, 193291.0778714405], 
processed observation next is [1.0, 0.782608695652174, 0.6445497630331753, 0.725, 1.0, 1.0, 0.4682190714411086, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2168381426616548, 0.2168381426616548, 0.28849414607677687], 
reward next is 0.7115, 
noisyNet noise sample is [array([0.10728689], dtype=float32), -0.2743136]. 
=============================================
[2019-04-28 01:45:57,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3106350e-15 2.9827025e-02 5.6917016e-24 9.7017294e-01 3.0803113e-15], sum to 1.0000
[2019-04-28 01:45:57,689] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5547
[2019-04-28 01:45:57,695] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.33333333333334, 79.0, 1.0, 2.0, 0.3862771265067849, 1.0, 2.0, 0.3862771265067849, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1079712.835177254, 1079712.835177254, 268717.962887207], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3565200.0000, 
sim time next is 3565800.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.39533828691692, 1.0, 2.0, 0.39533828691692, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1105053.431430385, 1105053.431430385, 270952.4430770706], 
processed observation next is [1.0, 0.2608695652173913, 0.5023696682464456, 0.79, 1.0, 1.0, 0.2714919119480964, 1.0, 1.0, 0.2714919119480964, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3069592865084403, 0.3069592865084403, 0.4044066314583143], 
reward next is 0.5956, 
noisyNet noise sample is [array([-1.2899818], dtype=float32), 0.43707663]. 
=============================================
[2019-04-28 01:45:58,756] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.1585105e-10 3.3297047e-02 1.2637778e-18 9.6670294e-01 2.8452679e-11], sum to 1.0000
[2019-04-28 01:45:58,772] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9891
[2019-04-28 01:45:58,790] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.8444077657219283, 1.0, 2.0, 0.8444077657219283, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2361656.568857902, 2361656.568857902, 442075.3196580823], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3510000.0000, 
sim time next is 3510600.0000, 
raw observation next is [33.0, 62.33333333333333, 1.0, 2.0, 0.887560910160994, 1.0, 2.0, 0.887560910160994, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2482468.032528446, 2482468.032528445, 464754.1972922586], 
processed observation next is [1.0, 0.6521739130434783, 0.7630331753554502, 0.6233333333333333, 1.0, 1.0, 0.8645312170614385, 1.0, 1.0, 0.8645312170614385, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6895744534801239, 0.6895744534801237, 0.6936629810332218], 
reward next is 0.3063, 
noisyNet noise sample is [array([-0.75536823], dtype=float32), 1.409457]. 
=============================================
[2019-04-28 01:45:59,563] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [9.2593517e-15 1.3623545e-03 2.2038137e-24 9.9863762e-01 2.9492740e-18], sum to 1.0000
[2019-04-28 01:45:59,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3821
[2019-04-28 01:45:59,580] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.5, 77.0, 1.0, 2.0, 0.2782902668730139, 1.0, 2.0, 0.2782902668730139, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 777761.0206121317, 777761.0206121324, 245824.5885129666], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3789000.0000, 
sim time next is 3789600.0000, 
raw observation next is [29.33333333333334, 77.66666666666667, 1.0, 2.0, 0.2772469325076887, 1.0, 2.0, 0.2772469325076887, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 774844.0737601715, 774844.0737601722, 245636.2915065776], 
processed observation next is [1.0, 0.8695652173913043, 0.5892575039494474, 0.7766666666666667, 1.0, 1.0, 0.12921317169601051, 1.0, 1.0, 0.12921317169601051, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.21523446493338097, 0.21523446493338116, 0.3666213306068322], 
reward next is 0.6334, 
noisyNet noise sample is [array([-0.30099306], dtype=float32), -0.8905211]. 
=============================================
[2019-04-28 01:45:59,671] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.3843825e-12 3.2085687e-02 8.1205499e-24 9.6791428e-01 5.6871449e-16], sum to 1.0000
[2019-04-28 01:45:59,677] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8708
[2019-04-28 01:45:59,688] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 75.0, 1.0, 2.0, 0.2786827190742198, 1.0, 2.0, 0.2786827190742198, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 778858.2377585608, 778858.2377585614, 245896.5511940835], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3524400.0000, 
sim time next is 3525000.0000, 
raw observation next is [29.83333333333333, 75.66666666666667, 1.0, 2.0, 0.2791682019666834, 1.0, 2.0, 0.2791682019666834, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 780215.5514946034, 780215.5514946034, 245983.9570073277], 
processed observation next is [1.0, 0.8260869565217391, 0.6129541864139019, 0.7566666666666667, 1.0, 1.0, 0.131527954176727, 1.0, 1.0, 0.131527954176727, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21672654208183428, 0.21672654208183428, 0.36714023433929505], 
reward next is 0.6329, 
noisyNet noise sample is [array([3.5686364], dtype=float32), 0.3025465]. 
=============================================
[2019-04-28 01:45:59,709] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[69.48805]
 [69.37469]
 [69.38003]
 [69.41756]
 [69.31832]], R is [[69.3588028 ]
 [69.29820251]
 [69.23827362]
 [69.17889404]
 [69.11989594]].
[2019-04-28 01:46:02,833] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.8092337e-11 2.8375874e-03 1.5830437e-18 9.9716240e-01 2.7658811e-11], sum to 1.0000
[2019-04-28 01:46:02,840] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3912
[2019-04-28 01:46:02,844] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.33333333333334, 72.66666666666667, 1.0, 2.0, 0.639540339134586, 1.0, 2.0, 0.639540339134586, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1788218.781224018, 1788218.781224018, 349312.8924503251], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3572400.0000, 
sim time next is 3573000.0000, 
raw observation next is [29.5, 72.0, 1.0, 2.0, 0.7076038806074504, 1.0, 2.0, 0.7076038806074504, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1978706.987117442, 1978706.987117442, 377393.6842782607], 
processed observation next is [1.0, 0.34782608695652173, 0.5971563981042655, 0.72, 1.0, 1.0, 0.6477155188041571, 1.0, 1.0, 0.6477155188041571, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5496408297548451, 0.5496408297548451, 0.563274155639195], 
reward next is 0.4367, 
noisyNet noise sample is [array([0.5539167], dtype=float32), -0.39185974]. 
=============================================
[2019-04-28 01:46:02,886] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[46.660904]
 [47.96136 ]
 [47.999554]
 [47.89872 ]
 [47.76596 ]], R is [[46.32904434]
 [46.34439087]
 [46.46595383]
 [46.59382629]
 [46.72354889]].
[2019-04-28 01:46:05,460] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.5105982e-14 2.0884909e-03 5.3909896e-25 9.9791151e-01 6.9860220e-17], sum to 1.0000
[2019-04-28 01:46:05,466] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5212
[2019-04-28 01:46:05,487] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 79.0, 1.0, 2.0, 0.275511136020001, 1.0, 2.0, 0.275511136020001, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 769991.1637738596, 769991.1637738589, 245324.2800278806], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3697200.0000, 
sim time next is 3697800.0000, 
raw observation next is [29.0, 78.16666666666667, 1.0, 2.0, 0.2743776627023441, 1.0, 2.0, 0.2743776627023441, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 766822.2304045692, 766822.2304045692, 245121.0859628796], 
processed observation next is [1.0, 0.8260869565217391, 0.5734597156398105, 0.7816666666666667, 1.0, 1.0, 0.12575622012330612, 1.0, 1.0, 0.12575622012330612, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21300617511238032, 0.21300617511238032, 0.3658523671087755], 
reward next is 0.6341, 
noisyNet noise sample is [array([-0.03221375], dtype=float32), -0.64696205]. 
=============================================
[2019-04-28 01:46:18,433] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-28 01:46:18,434] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:46:18,435] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:46:18,438] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:46:18,439] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:46:18,439] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:46:18,442] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:46:18,442] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:46:18,445] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:46:18,447] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run73
[2019-04-28 01:46:18,444] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:46:18,481] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:46:18,490] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run73
[2019-04-28 01:46:18,537] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run73
[2019-04-28 01:46:18,579] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run73
[2019-04-28 01:46:18,616] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run73
[2019-04-28 01:47:05,847] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049096573]
[2019-04-28 01:47:05,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.98333333333333, 56.0, 1.0, 2.0, 0.3909245464148208, 1.0, 2.0, 0.3909245464148208, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 1092702.3898363, 1092702.3898363, 270345.1139720848]
[2019-04-28 01:47:05,855] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:47:05,874] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.0073518e-13 1.2793046e-04 3.9503781e-23 9.9987209e-01 2.4947541e-15], sampled 0.590941806823021
[2019-04-28 01:48:43,214] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049096573]
[2019-04-28 01:48:43,215] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.51639364, 82.70176736, 1.0, 2.0, 0.2660332813351964, 1.0, 2.0, 0.2660332813351964, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 743490.0916273935, 743490.0916273935, 244135.03038848]
[2019-04-28 01:48:43,216] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:48:43,219] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4369704e-13 1.1395177e-04 2.1934443e-23 9.9988604e-01 1.6938124e-15], sampled 0.2418096306646509
[2019-04-28 01:48:55,648] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049096573]
[2019-04-28 01:48:55,650] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.99240825666666, 95.21023565666667, 1.0, 2.0, 0.218586630898503, 1.0, 2.0, 0.218586630898503, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 637884.6633884333, 637884.6633884333, 240590.8499661672]
[2019-04-28 01:48:55,652] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:48:55,658] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.3641116e-14 8.4936648e-05 3.7673228e-24 9.9991500e-01 5.3697675e-16], sampled 0.030851643302232556
[2019-04-28 01:49:08,494] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.049096573]
[2019-04-28 01:49:08,495] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.73333333333333, 67.0, 1.0, 2.0, 0.2171246613945831, 1.0, 2.0, 0.2171246613945831, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 624015.5366022513, 624015.5366022513, 238670.2889039309]
[2019-04-28 01:49:08,496] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:49:08,498] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2910570e-13 1.1112508e-04 1.8037947e-23 9.9988890e-01 1.4927598e-15], sampled 0.8875739834255386
[2019-04-28 01:49:22,931] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.1334 3511159153.5249 1.0000
[2019-04-28 01:49:23,059] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.2542 3429597744.1928 33.0000
[2019-04-28 01:49:23,145] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6856.1742 3475025256.5085 9.0000
[2019-04-28 01:49:23,177] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:49:23,307] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.1696 3393597582.5656 9.0000
[2019-04-28 01:49:24,323] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 1800000, evaluation results [1800000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6856.174199475163, 3475025256.508531, 9.0, 6794.169577998088, 3393597582.5656285, 9.0, 6798.133399660622, 3511159153.524861, 1.0, 6701.254244449667, 3429597744.1928453, 33.0]
[2019-04-28 01:49:24,971] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [7.3115680e-12 1.2541715e-03 3.1816713e-21 9.9874580e-01 4.4520170e-14], sum to 1.0000
[2019-04-28 01:49:24,979] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4145
[2019-04-28 01:49:24,985] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.73333333333333, 85.33333333333334, 1.0, 2.0, 0.2703898636867463, 1.0, 2.0, 0.2703898636867463, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 755673.3265656435, 755673.3265656435, 244414.3974402787], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4065600.0000, 
sim time next is 4066200.0000, 
raw observation next is [27.7, 85.5, 1.0, 2.0, 0.2701425587204138, 1.0, 2.0, 0.2701425587204138, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 754981.9266081522, 754981.9266081522, 244370.8857628552], 
processed observation next is [1.0, 0.043478260869565216, 0.5118483412322274, 0.855, 1.0, 1.0, 0.12065368520531784, 1.0, 1.0, 0.12065368520531784, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20971720183559783, 0.20971720183559783, 0.3647326653176943], 
reward next is 0.6353, 
noisyNet noise sample is [array([-0.6841455], dtype=float32), 0.5559975]. 
=============================================
[2019-04-28 01:49:35,748] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0900719e-11 4.2474121e-04 3.2883913e-21 9.9957520e-01 1.1112617e-11], sum to 1.0000
[2019-04-28 01:49:35,753] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9399
[2019-04-28 01:49:35,758] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.06666666666667, 88.66666666666666, 1.0, 2.0, 0.3905700315721866, 1.0, 2.0, 0.3905700315721866, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1091718.372285775, 1091718.372285775, 269778.2244931583], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4077600.0000, 
sim time next is 4078200.0000, 
raw observation next is [27.03333333333333, 88.83333333333334, 1.0, 2.0, 0.3865547355233114, 1.0, 2.0, 0.3865547355233114, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1080489.192410146, 1080489.192410146, 268793.7269138787], 
processed observation next is [1.0, 0.17391304347826086, 0.48025276461295413, 0.8883333333333334, 1.0, 1.0, 0.2609093199076041, 1.0, 1.0, 0.2609093199076041, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3001358867805961, 0.3001358867805961, 0.4011846670356398], 
reward next is 0.5988, 
noisyNet noise sample is [array([-0.5383535], dtype=float32), 0.22876509]. 
=============================================
[2019-04-28 01:49:35,984] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.7260652e-10 1.0253924e-02 4.1491380e-17 9.8974609e-01 7.0285261e-10], sum to 1.0000
[2019-04-28 01:49:35,992] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3387
[2019-04-28 01:49:35,998] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.6694322965600052, 1.0, 2.0, 0.6694322965600052, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1871872.692685907, 1871872.692685907, 361320.5135970702], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3989400.0000, 
sim time next is 3990000.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.6457162928832204, 1.0, 2.0, 0.6457162928832204, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1805501.914612501, 1805501.914612501, 351759.1633973592], 
processed observation next is [1.0, 0.17391304347826086, 0.5734597156398105, 0.84, 1.0, 1.0, 0.5731521601002655, 1.0, 1.0, 0.5731521601002655, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5015283096145836, 0.5015283096145836, 0.5250136767124765], 
reward next is 0.4750, 
noisyNet noise sample is [array([-0.48108107], dtype=float32), 0.41858935]. 
=============================================
[2019-04-28 01:49:36,014] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[46.909298]
 [47.615757]
 [47.65398 ]
 [47.755093]
 [47.498463]], R is [[46.52055359]
 [46.51606369]
 [46.57797623]
 [46.63665771]
 [46.68690491]].
[2019-04-28 01:49:36,056] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.16514376e-10 5.44972427e-04 1.03200145e-19 9.99455035e-01
 1.01709125e-12], sum to 1.0000
[2019-04-28 01:49:36,063] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3825
[2019-04-28 01:49:36,067] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 91.5, 1.0, 2.0, 0.4337275213733307, 1.0, 2.0, 0.4337275213733307, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1212420.065706084, 1212420.065706084, 280959.4001435804], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4084200.0000, 
sim time next is 4084800.0000, 
raw observation next is [27.0, 92.33333333333334, 1.0, 2.0, 0.3945205709936578, 1.0, 2.0, 0.3945205709936578, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1102766.568502244, 1102766.568502244, 270759.2726217615], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.9233333333333335, 1.0, 1.0, 0.27050671204055154, 1.0, 1.0, 0.27050671204055154, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3063240468061789, 0.3063240468061789, 0.40411831734591264], 
reward next is 0.5959, 
noisyNet noise sample is [array([1.2931448], dtype=float32), 0.5101494]. 
=============================================
[2019-04-28 01:49:37,404] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4329518e-08 7.3441439e-03 3.8673013e-15 9.9265587e-01 1.8684286e-08], sum to 1.0000
[2019-04-28 01:49:37,412] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9906
[2019-04-28 01:49:37,417] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [37.5, 46.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.309878879236424, 6.9112, 170.5573041426782, 3195252.294459781, 2909662.396042787, 551525.5821959855], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4293000.0000, 
sim time next is 4293600.0000, 
raw observation next is [37.33333333333333, 47.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.66253486087345, 6.9112, 170.5573041426782, 3448168.410660114, 2909956.687468356, 549455.9307976699], 
processed observation next is [1.0, 0.6956521739130435, 0.9684044233807265, 0.47, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.07513348608734498, 0.0, 0.8375144448122397, 0.9578245585166983, 0.8083213020745433, 0.8200834788024924], 
reward next is 0.0000, 
noisyNet noise sample is [array([2.669451], dtype=float32), 0.10569667]. 
=============================================
[2019-04-28 01:49:38,988] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.2035667e-12 1.9038012e-04 2.8087761e-24 9.9980968e-01 1.7386612e-13], sum to 1.0000
[2019-04-28 01:49:38,994] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4194
[2019-04-28 01:49:38,997] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.66666666666667, 72.33333333333334, 1.0, 2.0, 0.3021842802120896, 1.0, 2.0, 0.3021842802120896, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 844565.9057697704, 844565.9057697704, 250312.2625560232], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4227600.0000, 
sim time next is 4228200.0000, 
raw observation next is [31.5, 73.0, 1.0, 2.0, 0.3012256066790437, 1.0, 2.0, 0.3012256066790437, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 841885.4858248645, 841885.4858248645, 250125.8146574023], 
processed observation next is [1.0, 0.9565217391304348, 0.6919431279620853, 0.73, 1.0, 1.0, 0.15810314057716107, 1.0, 1.0, 0.15810314057716107, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2338570793957957, 0.2338570793957957, 0.37332211142895866], 
reward next is 0.6267, 
noisyNet noise sample is [array([-0.9755821], dtype=float32), 0.55223733]. 
=============================================
[2019-04-28 01:49:39,229] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9057802e-08 1.1721991e-02 1.8326909e-15 9.8827797e-01 9.7605817e-09], sum to 1.0000
[2019-04-28 01:49:39,238] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4707
[2019-04-28 01:49:39,245] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 84.0, 1.0, 2.0, 0.5161480823747993, 1.0, 2.0, 0.5161480823747993, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1442969.413662723, 1442969.413662723, 305361.1165948943], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3999600.0000, 
sim time next is 4000200.0000, 
raw observation next is [29.83333333333333, 84.0, 1.0, 2.0, 0.583386934073525, 1.0, 2.0, 0.583386934073525, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1631088.749146975, 1631088.749146975, 328210.9190372276], 
processed observation next is [1.0, 0.30434782608695654, 0.6129541864139019, 0.84, 1.0, 1.0, 0.4980565470765362, 1.0, 1.0, 0.4980565470765362, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.453080208096382, 0.453080208096382, 0.48986704333914566], 
reward next is 0.5101, 
noisyNet noise sample is [array([-0.12060393], dtype=float32), -0.7532375]. 
=============================================
[2019-04-28 01:50:00,208] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3011246e-12 7.1147377e-05 2.5053025e-21 9.9992883e-01 5.1289406e-10], sum to 1.0000
[2019-04-28 01:50:00,213] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8222
[2019-04-28 01:50:00,218] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 72.5, 1.0, 2.0, 0.2749091585443294, 1.0, 2.0, 0.2749091585443294, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 768308.1706217976, 768308.1706217976, 245216.0830580219], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4534200.0000, 
sim time next is 4534800.0000, 
raw observation next is [30.33333333333333, 70.33333333333334, 1.0, 2.0, 0.274930345335755, 1.0, 2.0, 0.274930345335755, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 768367.4040638003, 768367.4040638003, 245219.4962091525], 
processed observation next is [0.0, 0.4782608695652174, 0.6366508688783569, 0.7033333333333335, 1.0, 1.0, 0.12642210281416263, 1.0, 1.0, 0.12642210281416263, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21343539001772233, 0.21343539001772233, 0.3659992480733619], 
reward next is 0.6340, 
noisyNet noise sample is [array([1.1055076], dtype=float32), -0.38191608]. 
=============================================
[2019-04-28 01:50:04,719] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.22108790e-11 4.31061962e-05 4.78669741e-22 9.99956846e-01
 1.03677705e-14], sum to 1.0000
[2019-04-28 01:50:04,725] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7235
[2019-04-28 01:50:04,730] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.2560505594746915, 1.0, 2.0, 0.2560505594746915, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 715585.1213386283, 715585.1213386283, 241947.6813448659], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4518000.0000, 
sim time next is 4518600.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.2562905626300445, 1.0, 2.0, 0.2562905626300445, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 716256.0826918965, 716256.0826918965, 241987.8744694229], 
processed observation next is [0.0, 0.30434782608695654, 0.4786729857819906, 0.84, 1.0, 1.0, 0.10396453328921025, 1.0, 1.0, 0.10396453328921025, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19896002296997126, 0.19896002296997126, 0.3611759320439148], 
reward next is 0.6388, 
noisyNet noise sample is [array([-1.3525486], dtype=float32), 1.2843742]. 
=============================================
[2019-04-28 01:50:06,681] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1719399e-11 2.3672752e-05 1.3500379e-21 9.9997628e-01 4.4220574e-09], sum to 1.0000
[2019-04-28 01:50:06,688] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.0574
[2019-04-28 01:50:06,693] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.33333333333333, 72.66666666666666, 1.0, 2.0, 0.2464418988003544, 1.0, 2.0, 0.2464418988003544, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 688723.1527323221, 688723.1527323221, 240363.9873527504], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4822800.0000, 
sim time next is 4823400.0000, 
raw observation next is [28.16666666666667, 73.33333333333334, 1.0, 2.0, 0.2460452558936403, 1.0, 2.0, 0.2460452558936403, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 687614.3124111339, 687614.3124111339, 240299.5742884073], 
processed observation next is [1.0, 0.8260869565217391, 0.5339652448657191, 0.7333333333333334, 1.0, 1.0, 0.09162079023330154, 1.0, 1.0, 0.09162079023330154, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1910039756697594, 0.1910039756697594, 0.3586560810274736], 
reward next is 0.6413, 
noisyNet noise sample is [array([1.6300805], dtype=float32), -0.4429796]. 
=============================================
[2019-04-28 01:50:15,298] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.58397349e-11 1.04134255e-04 9.82238284e-21 9.99895811e-01
 1.25312889e-12], sum to 1.0000
[2019-04-28 01:50:15,305] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9073
[2019-04-28 01:50:15,321] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.5, 86.5, 1.0, 2.0, 0.4797800038776583, 1.0, 2.0, 0.4797800038776583, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1341233.384759208, 1341233.384759208, 294093.015048721], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4689000.0000, 
sim time next is 4689600.0000, 
raw observation next is [27.66666666666666, 85.66666666666667, 1.0, 2.0, 0.4155197934463387, 1.0, 2.0, 0.4155197934463387, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1161495.530459013, 1161495.530459013, 276108.6268422609], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.8566666666666667, 1.0, 1.0, 0.2958069800558298, 1.0, 1.0, 0.2958069800558298, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.32263764734972583, 0.32263764734972583, 0.4121024281227774], 
reward next is 0.5879, 
noisyNet noise sample is [array([-1.262439], dtype=float32), -0.21283291]. 
=============================================
[2019-04-28 01:50:16,012] A3C_AGENT_WORKER-Thread-14 INFO:Evaluating...
[2019-04-28 01:50:16,013] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:50:16,015] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:50:16,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:50:16,016] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:50:16,020] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:50:16,019] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:50:16,025] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:50:16,023] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:50:16,026] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:50:16,027] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:50:16,045] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run74
[2019-04-28 01:50:16,068] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run74
[2019-04-28 01:50:16,090] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run74
[2019-04-28 01:50:16,127] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run74
[2019-04-28 01:50:16,163] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run74
[2019-04-28 01:50:17,840] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:17,841] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.53333333333333, 72.83333333333334, 1.0, 2.0, 0.2281907180905625, 1.0, 2.0, 0.2281907180905625, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 661875.5165793764, 661875.5165793764, 241762.1972331211]
[2019-04-28 01:50:17,842] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:50:17,845] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.5760886e-13 4.0412556e-06 1.5860126e-23 9.9999595e-01 2.9951925e-13], sampled 0.07482265670753596
[2019-04-28 01:50:25,540] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:25,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.66666666666667, 70.16666666666667, 1.0, 2.0, 0.2082438271518244, 1.0, 2.0, 0.2082438271518244, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 609940.1600271248, 609940.1600271242, 238923.7094316761]
[2019-04-28 01:50:25,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:50:25,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.2852538e-13 5.0742287e-06 4.1223293e-23 9.9999487e-01 5.0674449e-13], sampled 0.30413278378025255
[2019-04-28 01:50:35,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:35,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.15815719333334, 97.31508259666666, 1.0, 2.0, 0.2376998217831091, 1.0, 2.0, 0.2376998217831091, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 664283.233509696, 664283.233509696, 239164.8180449745]
[2019-04-28 01:50:35,266] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:50:35,269] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [4.8436391e-13 3.7977238e-06 1.2174865e-23 9.9999619e-01 2.5896974e-13], sampled 0.8634828670969951
[2019-04-28 01:50:49,003] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:49,004] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.60818583, 84.71673749499999, 1.0, 2.0, 0.1869848081176992, 1.0, 2.0, 0.1869848081176992, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 582096.3343846984, 582096.3343846984, 241689.7206565273]
[2019-04-28 01:50:49,004] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:50:49,006] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.8027593e-13 4.4157350e-06 2.3043178e-23 9.9999559e-01 3.6786012e-13], sampled 0.6620863944200254
[2019-04-28 01:50:53,851] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:53,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.98786580666667, 69.99996047333333, 1.0, 2.0, 0.2341264902057983, 1.0, 2.0, 0.2341264902057983, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 654294.0626437258, 654294.0626437258, 238607.9254756426]
[2019-04-28 01:50:53,852] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:50:53,854] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.1227664e-13 3.8922612e-06 1.3495741e-23 9.9999607e-01 2.7414125e-13], sampled 0.3380775623389247
[2019-04-28 01:50:57,528] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:50:57,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.20254898833333, 72.46806939833334, 1.0, 2.0, 0.3155990456599984, 1.0, 2.0, 0.3155990456599984, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 882069.0182675794, 882069.0182675794, 253469.1705685661]
[2019-04-28 01:50:57,529] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:50:57,533] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.8934848e-13 4.4428793e-06 2.3570290e-23 9.9999559e-01 3.7254862e-13], sampled 0.4174638142952636
[2019-04-28 01:51:24,185] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:51:24,186] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.06666666666666, 89.66666666666667, 1.0, 2.0, 0.2933288988574965, 1.0, 2.0, 0.2933288988574965, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 819802.6249809398, 819802.6249809398, 249093.0319750414]
[2019-04-28 01:51:24,187] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:51:24,190] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.0947807e-13 4.2065226e-06 1.8698419e-23 9.9999583e-01 3.2797018e-13], sampled 0.8407490713614417
[2019-04-28 01:51:27,340] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:51:27,341] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.6, 69.33333333333334, 1.0, 2.0, 0.5260339642031003, 1.0, 2.0, 0.5260339642031003, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 1470634.893426943, 1470634.893426942, 308253.9799429757]
[2019-04-28 01:51:27,342] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:51:27,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4669277e-12 6.2173704e-06 9.7429856e-23 9.9999380e-01 8.1352871e-13], sampled 0.8450848747736054
[2019-04-28 01:51:28,805] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:51:28,808] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.15638508, 78.64049263999999, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 483373.4863802353, 483373.4863802353, 230223.3006733705]
[2019-04-28 01:51:28,809] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:51:28,813] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [5.0453278e-13 3.8658268e-06 1.3147105e-23 9.9999619e-01 2.7013935e-13], sampled 0.4886614930498262
[2019-04-28 01:51:32,085] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:51:32,086] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.96590326, 87.099406625, 1.0, 2.0, 0.256008066387443, 1.0, 2.0, 0.256008066387443, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 715464.9851026328, 715464.9851026328, 242132.6214725429]
[2019-04-28 01:51:32,087] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:51:32,090] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.8535431e-13 4.7085646e-06 3.0108467e-23 9.9999535e-01 4.2627772e-13], sampled 0.9930610267021228
[2019-04-28 01:51:39,475] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.052115146]
[2019-04-28 01:51:39,476] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.95, 63.5, 1.0, 2.0, 0.1738367130500485, 1.0, 2.0, 0.1738367130500485, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 537968.878787081, 537968.878787081, 238971.6510668462]
[2019-04-28 01:51:39,478] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:51:39,480] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2296546e-12 5.7491384e-06 6.9879880e-23 9.9999428e-01 6.7753111e-13], sampled 0.24095555144853653
[2019-04-28 01:51:40,598] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:51:41,104] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:51:41,132] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:51:41,190] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:51:41,208] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:51:42,220] A3C_AGENT_WORKER-Thread-14 INFO:Global step: 1825000, evaluation results [1825000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:51:48,286] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.6671351e-13 1.0804918e-05 1.0523189e-21 9.9998915e-01 3.9937459e-12], sum to 1.0000
[2019-04-28 01:51:48,294] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6822
[2019-04-28 01:51:48,301] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 84.83333333333333, 1.0, 2.0, 0.2390255024381162, 1.0, 2.0, 0.2390255024381162, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 667990.3374986737, 667990.3374986737, 239178.2268913274], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5019000.0000, 
sim time next is 5019600.0000, 
raw observation next is [26.0, 85.66666666666667, 1.0, 2.0, 0.2404775258418989, 1.0, 2.0, 0.2404775258418989, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 672049.4920389858, 672049.4920389858, 239408.0533560982], 
processed observation next is [0.0, 0.08695652173913043, 0.4312796208530806, 0.8566666666666667, 1.0, 1.0, 0.08491268173722759, 1.0, 1.0, 0.08491268173722759, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18668041445527384, 0.18668041445527384, 0.3573254527702958], 
reward next is 0.6427, 
noisyNet noise sample is [array([-1.5776515], dtype=float32), -0.4024718]. 
=============================================
[2019-04-28 01:51:51,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.3208624e-11 5.0176673e-06 1.1002471e-21 9.9999499e-01 6.6079885e-12], sum to 1.0000
[2019-04-28 01:51:51,505] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1679
[2019-04-28 01:51:51,508] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.83333333333333, 63.5, 1.0, 2.0, 0.8791098607104305, 1.0, 2.0, 0.8791098607104305, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2458807.570302365, 2458807.570302364, 460220.6354130557], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4888200.0000, 
sim time next is 4888800.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.8642964056886583, 1.0, 2.0, 0.8642964056886583, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2417335.317593557, 2417335.317593557, 452385.8681791347], 
processed observation next is [1.0, 0.6086956521739131, 0.7156398104265403, 0.63, 1.0, 1.0, 0.8365016936007931, 1.0, 1.0, 0.8365016936007931, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6714820326648769, 0.6714820326648769, 0.6752027883270667], 
reward next is 0.3248, 
noisyNet noise sample is [array([0.8250014], dtype=float32), -0.08275207]. 
=============================================
[2019-04-28 01:52:06,853] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [4.1628944e-12 5.1597381e-06 1.5037668e-22 9.9999487e-01 8.0159966e-14], sum to 1.0000
[2019-04-28 01:52:06,862] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7496
[2019-04-28 01:52:06,866] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 69.0, 1.0, 2.0, 0.3089678688864586, 1.0, 2.0, 0.3089678688864586, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 863532.7899716777, 863532.7899716777, 251649.651166643], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5338800.0000, 
sim time next is 5339400.0000, 
raw observation next is [32.75, 70.5, 1.0, 2.0, 0.3110713770667367, 1.0, 2.0, 0.3110713770667367, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 869414.2577595784, 869414.2577595784, 252068.5913346431], 
processed observation next is [1.0, 0.8260869565217391, 0.7511848341232228, 0.705, 1.0, 1.0, 0.169965514538237, 1.0, 1.0, 0.169965514538237, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2415039604887718, 0.2415039604887718, 0.3762217781114076], 
reward next is 0.6238, 
noisyNet noise sample is [array([0.01227947], dtype=float32), -0.5222486]. 
=============================================
[2019-04-28 01:52:08,524] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.0651585e-08 1.1571812e-04 1.3050192e-17 9.9988425e-01 3.7876994e-08], sum to 1.0000
[2019-04-28 01:52:08,531] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1757
[2019-04-28 01:52:08,540] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.8, 50.66666666666667, 1.0, 2.0, 0.7874942415488431, 1.0, 2.0, 0.7874942415488431, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2202337.917160165, 2202337.917160165, 413823.7532448135], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5577600.0000, 
sim time next is 5578200.0000, 
raw observation next is [33.90000000000001, 50.0, 1.0, 2.0, 0.8264905264537044, 1.0, 2.0, 0.8264905264537044, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2311498.930088149, 2311498.930088149, 432967.4524366694], 
processed observation next is [1.0, 0.5652173913043478, 0.8056872037914699, 0.5, 1.0, 1.0, 0.7909524415104872, 1.0, 1.0, 0.7909524415104872, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6420830361355969, 0.6420830361355969, 0.6462200782636857], 
reward next is 0.3538, 
noisyNet noise sample is [array([-0.57208556], dtype=float32), -1.2370709]. 
=============================================
[2019-04-28 01:52:11,862] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.6562753e-13 1.9414106e-06 6.5534222e-23 9.9999809e-01 2.8529562e-10], sum to 1.0000
[2019-04-28 01:52:11,870] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1331
[2019-04-28 01:52:11,873] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.93333333333334, 63.66666666666667, 1.0, 2.0, 0.3030162687027779, 1.0, 2.0, 0.3030162687027779, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 846892.1238960366, 846892.1238960366, 250478.1483964691], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5336400.0000, 
sim time next is 5337000.0000, 
raw observation next is [33.7, 65.0, 1.0, 2.0, 0.3043845602816329, 1.0, 2.0, 0.3043845602816329, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 850717.8424455863, 850717.8424455863, 250745.7104545615], 
processed observation next is [1.0, 0.782608695652174, 0.7962085308056873, 0.65, 1.0, 1.0, 0.1619091087730517, 1.0, 1.0, 0.1619091087730517, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.23631051179044066, 0.23631051179044066, 0.3742473290366589], 
reward next is 0.6258, 
noisyNet noise sample is [array([-1.8389176], dtype=float32), 0.69682485]. 
=============================================
[2019-04-28 01:52:11,891] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[62.090256]
 [61.95482 ]
 [61.883522]
 [61.542755]
 [61.725304]], R is [[61.85931396]
 [61.86687469]
 [61.87479401]
 [61.88396072]
 [61.89425278]].
[2019-04-28 01:52:14,140] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.5818600e-10 1.6388174e-05 2.8146061e-17 9.9998367e-01 4.8199816e-11], sum to 1.0000
[2019-04-28 01:52:14,149] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9347
[2019-04-28 01:52:14,160] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.4227719774002656, 1.0, 2.0, 0.4227719774002656, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1181778.611435994, 1181778.611435994, 278026.0376133867], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5287800.0000, 
sim time next is 5288400.0000, 
raw observation next is [28.6, 88.0, 1.0, 2.0, 0.4194192149320367, 1.0, 2.0, 0.4194192149320367, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1172401.477741235, 1172401.477741235, 277139.9776213026], 
processed observation next is [1.0, 0.21739130434782608, 0.5545023696682465, 0.88, 1.0, 1.0, 0.3005050782313695, 1.0, 1.0, 0.3005050782313695, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.32566707715034304, 0.32566707715034304, 0.41364175764373523], 
reward next is 0.5864, 
noisyNet noise sample is [array([0.24026386], dtype=float32), -0.34603903]. 
=============================================
[2019-04-28 01:52:16,939] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.8294439e-11 4.6017813e-06 2.8761230e-19 9.9999535e-01 9.3706056e-09], sum to 1.0000
[2019-04-28 01:52:16,946] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0608
[2019-04-28 01:52:16,951] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.66666666666667, 95.0, 1.0, 2.0, 0.3364804238391864, 1.0, 2.0, 0.3364804238391864, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 940461.2123568647, 940461.2123568647, 257308.5220713364], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5546400.0000, 
sim time next is 5547000.0000, 
raw observation next is [25.63333333333333, 95.0, 1.0, 2.0, 0.3334563921766244, 1.0, 2.0, 0.3334563921766244, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 932005.3851021401, 932005.3851021401, 256662.053117814], 
processed observation next is [1.0, 0.17391304347826086, 0.4139020537124801, 0.95, 1.0, 1.0, 0.1969354122609933, 1.0, 1.0, 0.1969354122609933, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2588903847505945, 0.2588903847505945, 0.3830776912206179], 
reward next is 0.6169, 
noisyNet noise sample is [array([0.83930534], dtype=float32), 1.1804541]. 
=============================================
[2019-04-28 01:52:16,968] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[48.503643]
 [48.62884 ]
 [48.284054]
 [48.203213]
 [47.80997 ]], R is [[48.72756577]
 [48.85625076]
 [48.98239517]
 [49.10411072]
 [49.21522141]].
[2019-04-28 01:52:19,420] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.2419508e-13 3.0460589e-07 1.1658539e-22 9.9999964e-01 9.6601465e-15], sum to 1.0000
[2019-04-28 01:52:19,428] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8237
[2019-04-28 01:52:19,432] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.2, 60.0, 1.0, 2.0, 0.243693546135551, 1.0, 2.0, 0.243693546135551, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 681039.9826548509, 681039.9826548509, 239931.3208619145], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5592600.0000, 
sim time next is 5593200.0000, 
raw observation next is [31.93333333333333, 61.66666666666666, 1.0, 2.0, 0.250437688263993, 1.0, 2.0, 0.250437688263993, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 699893.6987287364, 699893.6987287364, 241024.8565057675], 
processed observation next is [1.0, 0.7391304347826086, 0.7124802527646128, 0.6166666666666666, 1.0, 1.0, 0.09691287742649757, 1.0, 1.0, 0.09691287742649757, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19441491631353788, 0.19441491631353788, 0.359738591799653], 
reward next is 0.6403, 
noisyNet noise sample is [array([0.29977334], dtype=float32), 0.5651584]. 
=============================================
[2019-04-28 01:52:21,159] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.7853410e-14 1.3637413e-08 1.7336224e-22 1.0000000e+00 2.3436463e-14], sum to 1.0000
[2019-04-28 01:52:21,166] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0150
[2019-04-28 01:52:21,174] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.98333333333333, 73.33333333333333, 1.0, 2.0, 0.2742032889137582, 1.0, 2.0, 0.2742032889137582, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 766334.7218175835, 766334.7218175842, 245091.2273184185], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5597400.0000, 
sim time next is 5598000.0000, 
raw observation next is [29.7, 75.0, 1.0, 2.0, 0.2735283173664944, 1.0, 2.0, 0.2735283173664944, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 764447.6603292987, 764447.6603292987, 244971.242721891], 
processed observation next is [1.0, 0.8260869565217391, 0.6066350710900474, 0.75, 1.0, 1.0, 0.1247329124897523, 1.0, 1.0, 0.1247329124897523, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.21234657231369408, 0.21234657231369408, 0.3656287204804343], 
reward next is 0.6344, 
noisyNet noise sample is [array([1.2071491], dtype=float32), 0.099250294]. 
=============================================
[2019-04-28 01:52:21,190] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[59.78506 ]
 [59.726524]
 [59.625366]
 [59.61543 ]
 [59.662514]], R is [[59.7988739 ]
 [59.83507919]
 [59.87098694]
 [59.90716553]
 [59.9438591 ]].
[2019-04-28 01:52:23,818] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.1248054e-13 4.5909418e-09 1.9992536e-26 1.0000000e+00 1.2068108e-14], sum to 1.0000
[2019-04-28 01:52:23,826] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8920
[2019-04-28 01:52:23,829] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.26666666666667, 60.0, 1.0, 2.0, 0.2800932054402232, 1.0, 2.0, 0.2800932054402232, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 782801.6822278036, 782801.6822278036, 246148.0178404322], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5670600.0000, 
sim time next is 5671200.0000, 
raw observation next is [32.23333333333333, 60.00000000000001, 1.0, 2.0, 0.2707086879482766, 1.0, 2.0, 0.2707086879482766, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 756564.6764110741, 756564.6764110741, 244470.7033498453], 
processed observation next is [0.0, 0.6521739130434783, 0.7266982622432857, 0.6000000000000001, 1.0, 1.0, 0.12133576861238148, 1.0, 1.0, 0.12133576861238148, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2101568545586317, 0.2101568545586317, 0.3648816467908139], 
reward next is 0.6351, 
noisyNet noise sample is [array([0.6301802], dtype=float32), 0.6093245]. 
=============================================
[2019-04-28 01:52:32,549] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.3180005e-14 6.3795351e-09 1.0072653e-25 1.0000000e+00 5.3362786e-18], sum to 1.0000
[2019-04-28 01:52:32,558] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5981
[2019-04-28 01:52:32,563] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.15, 62.0, 1.0, 2.0, 0.2600168317813183, 1.0, 2.0, 0.2600168317813183, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 726673.4270470379, 726673.4270470379, 242618.5744828288], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5740200.0000, 
sim time next is 5740800.0000, 
raw observation next is [31.3, 61.33333333333334, 1.0, 2.0, 0.2599556521836149, 1.0, 2.0, 0.2599556521836149, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 726502.3894877324, 726502.3894877324, 242608.2925127818], 
processed observation next is [0.0, 0.43478260869565216, 0.6824644549763034, 0.6133333333333334, 1.0, 1.0, 0.10838030383568058, 1.0, 1.0, 0.10838030383568058, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2018062193021479, 0.2018062193021479, 0.36210192912355493], 
reward next is 0.6379, 
noisyNet noise sample is [array([1.6472431], dtype=float32), 0.44706637]. 
=============================================
[2019-04-28 01:52:33,631] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 01:52:33,633] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:52:33,633] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:52:33,634] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:52:33,635] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:52:33,635] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:52:33,635] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:52:33,636] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:52:33,637] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:52:33,637] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:52:33,637] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:52:33,656] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run75
[2019-04-28 01:52:33,678] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run75
[2019-04-28 01:52:33,706] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run75
[2019-04-28 01:52:33,742] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run75
[2019-04-28 01:52:33,743] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run75
[2019-04-28 01:52:49,522] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:52:49,525] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.76666666666667, 66.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 397123.9487307193, 397123.94873072, 211968.3566733625]
[2019-04-28 01:52:49,526] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:52:49,527] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.57234106e-14 2.41112232e-08 3.74698911e-24 1.00000000e+00
 1.04061775e-14], sampled 0.3060818183792956
[2019-04-28 01:52:50,971] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:52:50,972] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.7, 59.0, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 521856.4121288254, 521856.4121288254, 237647.7811662464]
[2019-04-28 01:52:50,973] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:52:50,975] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6423945e-14 9.8394048e-09 2.3963662e-25 1.0000000e+00 2.0094724e-15], sampled 0.1747878783866743
[2019-04-28 01:53:18,964] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:53:18,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.95261659666667, 79.85351641333334, 1.0, 2.0, 0.3042851881161872, 1.0, 2.0, 0.3042851881161872, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 850435.5029388417, 850435.5029388417, 251204.5516540135]
[2019-04-28 01:53:18,967] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:53:18,969] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3420491e-14 8.7411385e-09 1.6656739e-25 1.0000000e+00 1.6164313e-15], sampled 0.6166748671169637
[2019-04-28 01:53:29,681] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:53:29,683] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.70947438, 66.97699113, 1.0, 2.0, 0.4739132106350757, 1.0, 2.0, 0.4739132106350757, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 1324817.926318494, 1324817.926318494, 292533.5787412773]
[2019-04-28 01:53:29,683] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:53:29,687] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [8.8760596e-14 2.6448038e-08 4.9990904e-24 1.0000000e+00 1.2372783e-14], sampled 0.1416185741649587
[2019-04-28 01:53:38,971] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:53:38,974] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.90627585, 90.84282832999999, 1.0, 2.0, 0.8378191799691422, 1.0, 2.0, 0.8378191799691422, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2343182.910759286, 2343182.910759286, 439185.0476336379]
[2019-04-28 01:53:38,975] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:53:38,978] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2540892e-13 6.6238584e-08 8.4242191e-23 9.9999988e-01 6.6998219e-14], sampled 0.19631275799120007
[2019-04-28 01:53:42,124] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.056741904]
[2019-04-28 01:53:42,126] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.25, 74.0, 1.0, 2.0, 0.3049670436110344, 1.0, 2.0, 0.3049670436110344, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 852341.9430029114, 852341.9430029114, 251339.9725772834]
[2019-04-28 01:53:42,128] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:53:42,131] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.5545787e-14 9.5269881e-09 2.1685567e-25 1.0000000e+00 1.8924400e-15], sampled 0.8644380122932039
[2019-04-28 01:54:08,985] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:54:09,278] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:54:09,562] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:54:09,613] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:54:09,767] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:54:10,780] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 1850000, evaluation results [1850000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:54:22,428] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.1829208e-14 1.4075975e-08 4.0212933e-26 1.0000000e+00 7.6403507e-15], sum to 1.0000
[2019-04-28 01:54:22,439] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7147
[2019-04-28 01:54:22,450] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 76.66666666666667, 1.0, 2.0, 0.8562025203316224, 1.0, 2.0, 0.8562025203316224, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2394675.982220328, 2394675.982220328, 448162.7272941574], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5930400.0000, 
sim time next is 5931000.0000, 
raw observation next is [30.05, 77.0, 1.0, 2.0, 0.857803926969731, 1.0, 2.0, 0.857803926969731, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2399159.187081685, 2399159.187081685, 448996.6431208393], 
processed observation next is [1.0, 0.6521739130434783, 0.6232227488151659, 0.77, 1.0, 1.0, 0.8286794300840132, 1.0, 1.0, 0.8286794300840132, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6664331075226902, 0.6664331075226902, 0.6701442434639393], 
reward next is 0.3299, 
noisyNet noise sample is [array([-0.14911196], dtype=float32), -0.74597466]. 
=============================================
[2019-04-28 01:54:22,500] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[64.28635 ]
 [64.07671 ]
 [63.882896]
 [63.739666]
 [63.61038 ]], R is [[64.23449707]
 [63.92325211]
 [63.62752914]
 [63.36888504]
 [63.12294006]].
[2019-04-28 01:54:30,031] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.2681638e-13 1.6597853e-07 1.9618474e-20 9.9999988e-01 1.4523200e-13], sum to 1.0000
[2019-04-28 01:54:30,039] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.9149
[2019-04-28 01:54:30,042] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.1, 93.0, 1.0, 2.0, 0.3497929106651327, 1.0, 2.0, 0.3497929106651327, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 977686.5125944719, 977686.5125944719, 260218.4150018982], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6065400.0000, 
sim time next is 6066000.0000, 
raw observation next is [26.1, 93.0, 1.0, 2.0, 0.3368539897688506, 1.0, 2.0, 0.3368539897688506, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 941505.7857324998, 941505.7857324998, 257389.9393038293], 
processed observation next is [1.0, 0.21739130434782608, 0.4360189573459717, 0.93, 1.0, 1.0, 0.2010289033359646, 1.0, 1.0, 0.2010289033359646, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2615293849256944, 0.2615293849256944, 0.38416408851317807], 
reward next is 0.6158, 
noisyNet noise sample is [array([0.3924499], dtype=float32), -0.09341191]. 
=============================================
[2019-04-28 01:54:30,074] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[54.942696]
 [54.978592]
 [55.295387]
 [55.460983]
 [55.576702]], R is [[54.95473099]
 [55.01679993]
 [55.07591629]
 [55.13158798]
 [55.19444275]].
[2019-04-28 01:54:39,917] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [8.1849207e-14 5.4138178e-09 5.9240989e-25 1.0000000e+00 2.0701312e-14], sum to 1.0000
[2019-04-28 01:54:39,926] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0379
[2019-04-28 01:54:39,933] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 72.0, 1.0, 2.0, 0.8301687723786543, 1.0, 2.0, 0.8301687723786543, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2321795.671976353, 2321795.671976353, 434822.9233348778], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6190200.0000, 
sim time next is 6190800.0000, 
raw observation next is [29.8, 72.33333333333333, 1.0, 2.0, 0.7909576104591259, 1.0, 2.0, 0.7909576104591259, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2212033.714756694, 2212033.714756694, 415490.9358611555], 
processed observation next is [1.0, 0.6521739130434783, 0.6113744075829385, 0.7233333333333333, 1.0, 1.0, 0.7481416993483445, 1.0, 1.0, 0.7481416993483445, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6144538096546373, 0.6144538096546373, 0.6201357251659038], 
reward next is 0.3799, 
noisyNet noise sample is [array([0.84146005], dtype=float32), 2.4522789]. 
=============================================
[2019-04-28 01:54:42,827] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.1277264e-12 1.4901693e-07 1.1561026e-21 9.9999988e-01 1.7718501e-13], sum to 1.0000
[2019-04-28 01:54:42,837] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4557
[2019-04-28 01:54:42,842] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.9, 80.0, 1.0, 2.0, 0.3685702396804233, 1.0, 2.0, 0.3685702396804233, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1030195.181048626, 1030195.181048626, 264497.423836852], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6422400.0000, 
sim time next is 6423000.0000, 
raw observation next is [28.0, 79.5, 1.0, 2.0, 0.4155199517000208, 1.0, 2.0, 0.4155199517000208, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1161495.973062498, 1161495.973062498, 276104.3064759917], 
processed observation next is [1.0, 0.34782608695652173, 0.5260663507109005, 0.795, 1.0, 1.0, 0.2958071707229166, 1.0, 1.0, 0.2958071707229166, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.32263777029513835, 0.32263777029513835, 0.41209597981491297], 
reward next is 0.5879, 
noisyNet noise sample is [array([0.14594574], dtype=float32), -1.5703514]. 
=============================================
[2019-04-28 01:54:42,864] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[55.0014  ]
 [54.87151 ]
 [54.799088]
 [54.66947 ]
 [54.63616 ]], R is [[54.88520432]
 [54.94158173]
 [54.9990921 ]
 [55.05426407]
 [55.10244751]].
[2019-04-28 01:54:47,509] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.7869165e-14 1.5415789e-07 4.7039760e-23 9.9999988e-01 1.5374631e-15], sum to 1.0000
[2019-04-28 01:54:47,512] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2711
[2019-04-28 01:54:47,520] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.88333333333333, 79.5, 1.0, 2.0, 0.7448640029848058, 1.0, 2.0, 0.7448640029848058, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2083000.566665227, 2083000.566665227, 393922.5844545057], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6171000.0000, 
sim time next is 6171600.0000, 
raw observation next is [28.96666666666667, 79.0, 1.0, 2.0, 0.7425255359795867, 1.0, 2.0, 0.7425255359795867, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2076454.742171447, 2076454.742171447, 392861.5156220048], 
processed observation next is [1.0, 0.43478260869565216, 0.5718799368088469, 0.79, 1.0, 1.0, 0.6897898023850442, 1.0, 1.0, 0.6897898023850442, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.576792983936513, 0.576792983936513, 0.5863604710776191], 
reward next is 0.4136, 
noisyNet noise sample is [array([1.3688096], dtype=float32), 0.36051372]. 
=============================================
[2019-04-28 01:54:47,840] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.7950291e-10 3.2319929e-06 1.1467313e-19 9.9999678e-01 5.4303493e-12], sum to 1.0000
[2019-04-28 01:54:47,852] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7207
[2019-04-28 01:54:47,858] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.66666666666667, 86.66666666666667, 1.0, 2.0, 0.3942677859138067, 1.0, 2.0, 0.3942677859138067, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1102059.618809332, 1102059.618809332, 270688.2836510594], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6412200.0000, 
sim time next is 6412800.0000, 
raw observation next is [26.73333333333334, 86.33333333333334, 1.0, 2.0, 0.3751536170915794, 1.0, 2.0, 0.3751536170915794, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1048605.457242552, 1048605.457242552, 266045.4151474033], 
processed observation next is [1.0, 0.21739130434782608, 0.4660347551342816, 0.8633333333333334, 1.0, 1.0, 0.24717303264045712, 1.0, 1.0, 0.24717303264045712, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.29127929367848665, 0.29127929367848665, 0.3970827091752288], 
reward next is 0.6029, 
noisyNet noise sample is [array([0.45031288], dtype=float32), -0.40619263]. 
=============================================
[2019-04-28 01:54:50,335] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.3344073e-15 6.2408432e-08 5.2694264e-24 9.9999988e-01 3.7385981e-14], sum to 1.0000
[2019-04-28 01:54:50,343] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6606
[2019-04-28 01:54:50,356] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.93333333333334, 87.33333333333334, 1.0, 2.0, 0.2620958043207028, 1.0, 2.0, 0.2620958043207028, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 732485.5505998755, 732485.5505998755, 242973.1415802211], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6214800.0000, 
sim time next is 6215400.0000, 
raw observation next is [26.9, 87.5, 1.0, 2.0, 0.2616578343702232, 1.0, 2.0, 0.2616578343702232, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 731261.1280255822, 731261.1280255822, 242898.2821967694], 
processed observation next is [1.0, 0.9565217391304348, 0.4739336492890995, 0.875, 1.0, 1.0, 0.11043112574725689, 1.0, 1.0, 0.11043112574725689, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20312809111821728, 0.20312809111821728, 0.36253474954741705], 
reward next is 0.6375, 
noisyNet noise sample is [array([0.520502], dtype=float32), 1.7718446]. 
=============================================
[2019-04-28 01:54:50,804] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [5.3153583e-16 5.1681848e-10 1.1223350e-27 1.0000000e+00 3.3718343e-17], sum to 1.0000
[2019-04-28 01:54:50,815] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8074
[2019-04-28 01:54:50,821] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6177786e-15 1.5081937e-09 3.4082589e-27 1.0000000e+00 6.3312818e-19], sum to 1.0000
[2019-04-28 01:54:50,823] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.75, 84.5, 1.0, 2.0, 0.8331380822725725, 1.0, 2.0, 0.8331380822725725, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2330107.907162898, 2330107.907162898, 436322.7495221561], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6625800.0000, 
sim time next is 6626400.0000, 
raw observation next is [27.66666666666666, 84.66666666666667, 1.0, 2.0, 0.7160103623675229, 1.0, 2.0, 0.7160103623675229, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2002236.405988092, 2002236.405988091, 381052.2324288384], 
processed observation next is [1.0, 0.6956521739130435, 0.5102685624012636, 0.8466666666666667, 1.0, 1.0, 0.6578438100813528, 1.0, 1.0, 0.6578438100813528, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5561767794411366, 0.5561767794411364, 0.568734675266923], 
reward next is 0.4313, 
noisyNet noise sample is [array([0.37966302], dtype=float32), 1.5372891]. 
=============================================
[2019-04-28 01:54:50,830] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.7733
[2019-04-28 01:54:50,854] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.66666666666666, 89.16666666666667, 1.0, 2.0, 0.2609336600361628, 1.0, 2.0, 0.2609336600361628, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 729236.5734678387, 729236.5734678387, 242774.8886457575], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6221400.0000, 
sim time next is 6222000.0000, 
raw observation next is [26.63333333333333, 89.33333333333334, 1.0, 2.0, 0.2607725903268814, 1.0, 2.0, 0.2607725903268814, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 728786.2757667287, 728786.2757667294, 242747.4086020697], 
processed observation next is [0.0, 0.0, 0.46129541864139006, 0.8933333333333334, 1.0, 1.0, 0.10936456665889323, 1.0, 1.0, 0.10936456665889323, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20244063215742464, 0.20244063215742483, 0.362309565077716], 
reward next is 0.6377, 
noisyNet noise sample is [array([-1.1337876], dtype=float32), -0.3470163]. 
=============================================
[2019-04-28 01:54:50,881] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.47265]
 [71.04629]
 [71.52206]
 [71.50835]
 [71.49173]], R is [[71.78399658]
 [71.70380402]
 [71.62438965]
 [71.54579163]
 [71.46797943]].
[2019-04-28 01:54:57,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4652770e-14 8.7821483e-09 4.1081393e-25 1.0000000e+00 6.0465053e-15], sum to 1.0000
[2019-04-28 01:54:57,616] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8257
[2019-04-28 01:54:57,621] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.41666666666666, 82.0, 1.0, 2.0, 0.2602897240448753, 1.0, 2.0, 0.2602897240448753, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 727436.3422799787, 727436.3422799787, 242663.2357599009], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6385800.0000, 
sim time next is 6386400.0000, 
raw observation next is [27.4, 82.0, 1.0, 2.0, 0.259807321538214, 1.0, 2.0, 0.259807321538214, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 726087.7070513509, 726087.7070513509, 242581.2559255778], 
processed observation next is [0.0, 0.9565217391304348, 0.4976303317535545, 0.82, 1.0, 1.0, 0.10820159221471566, 1.0, 1.0, 0.10820159221471566, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20169102973648634, 0.20169102973648634, 0.36206157600832506], 
reward next is 0.6379, 
noisyNet noise sample is [array([0.7235746], dtype=float32), 1.2818758]. 
=============================================
[2019-04-28 01:55:00,394] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.1036516e-11 3.3088693e-07 6.8787182e-22 9.9999964e-01 5.7786062e-13], sum to 1.0000
[2019-04-28 01:55:00,405] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3525
[2019-04-28 01:55:00,414] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.91666666666667, 74.83333333333334, 1.0, 2.0, 0.7106963688807931, 1.0, 2.0, 0.7106963688807931, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1987362.68267522, 1987362.68267522, 378733.5327531783], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6601800.0000, 
sim time next is 6602400.0000, 
raw observation next is [29.1, 74.0, 1.0, 2.0, 0.708422573585468, 1.0, 2.0, 0.708422573585468, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1980998.454537372, 1980998.454537372, 377747.6950027549], 
processed observation next is [1.0, 0.43478260869565216, 0.5781990521327015, 0.74, 1.0, 1.0, 0.648701895886106, 1.0, 1.0, 0.648701895886106, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5502773484826033, 0.5502773484826033, 0.5638025298548581], 
reward next is 0.4362, 
noisyNet noise sample is [array([-0.7299525], dtype=float32), 1.6310939]. 
=============================================
[2019-04-28 01:55:12,443] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.2544752e-12 4.1216900e-08 1.8817567e-23 1.0000000e+00 4.5026317e-14], sum to 1.0000
[2019-04-28 01:55:12,450] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2025
[2019-04-28 01:55:12,454] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.15, 40.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 469320.4266718476, 469320.4266718476, 227391.0834654121], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6870600.0000, 
sim time next is 6871200.0000, 
raw observation next is [29.23333333333333, 39.33333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 462002.4560249479, 462002.4560249473, 225911.2149369634], 
processed observation next is [0.0, 0.5217391304347826, 0.5845181674565559, 0.3933333333333333, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.12833401556248553, 0.12833401556248536, 0.3371809178163633], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.21898438], dtype=float32), -0.46250215]. 
=============================================
[2019-04-28 01:55:12,661] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 01:55:12,662] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:55:12,663] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:55:12,664] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:55:12,665] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:55:12,665] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:55:12,666] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:55:12,668] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:55:12,669] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:55:12,667] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:55:12,676] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:55:12,690] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run76
[2019-04-28 01:55:12,717] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run76
[2019-04-28 01:55:12,758] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run76
[2019-04-28 01:55:12,792] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run76
[2019-04-28 01:55:12,826] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run76
[2019-04-28 01:55:19,715] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061037943]
[2019-04-28 01:55:19,716] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [19.85525379666667, 89.36576505333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 444367.5755926158, 444367.5755926158, 222323.8347384022]
[2019-04-28 01:55:19,717] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 01:55:19,720] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.6370261e-12 9.4221612e-08 5.4410587e-22 9.9999988e-01 2.0712064e-13], sampled 0.07786070858247152
[2019-04-28 01:55:37,996] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061037943]
[2019-04-28 01:55:37,996] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.03333333333333, 96.16666666666666, 1.0, 2.0, 0.2073990407541957, 1.0, 2.0, 0.2073990407541957, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 606498.1668563299, 606498.1668563299, 238935.3987682377]
[2019-04-28 01:55:37,997] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:55:37,999] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.3452308e-13 6.7142466e-08 1.9598629e-22 9.9999988e-01 1.1236709e-13], sampled 0.9594335709718284
[2019-04-28 01:56:39,482] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061037943]
[2019-04-28 01:56:39,483] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.08333333333333, 79.33333333333334, 1.0, 2.0, 0.575908114418447, 1.0, 2.0, 0.575908114418447, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 169.0403247858759, 1610173.899835306, 1610173.899835306, 325254.0516618787]
[2019-04-28 01:56:39,484] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:56:39,488] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.6600834e-12 9.4987229e-08 5.5890545e-22 9.9999988e-01 2.1045897e-13], sampled 0.3408761300148141
[2019-04-28 01:56:44,249] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.061037943]
[2019-04-28 01:56:44,253] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.2, 69.5, 1.0, 2.0, 0.2682173002285193, 1.0, 2.0, 0.2682173002285193, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 749595.9301390206, 749595.9301390206, 244518.1930740044]
[2019-04-28 01:56:44,253] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 01:56:44,257] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.2095652e-13 5.2470146e-08 9.3137765e-23 1.0000000e+00 7.1987128e-14], sampled 0.16550774965393777
[2019-04-28 01:56:48,578] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:56:48,591] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:56:48,657] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:56:48,687] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:56:48,854] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:56:49,873] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 1875000, evaluation results [1875000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:56:53,248] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.1937031e-12 3.6976772e-07 2.2935415e-22 9.9999964e-01 8.9840597e-14], sum to 1.0000
[2019-04-28 01:56:53,254] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8557
[2019-04-28 01:56:53,259] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [23.8, 76.0, 1.0, 2.0, 0.1864487540339435, 1.0, 2.0, 0.1864487540339435, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 579202.2900084801, 579202.2900084801, 241259.4045597434], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6768000.0000, 
sim time next is 6768600.0000, 
raw observation next is [24.03333333333333, 74.66666666666667, 1.0, 2.0, 0.2088304086019585, 1.0, 2.0, 0.2088304086019585, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 647848.8005523897, 647848.8005523904, 244445.2751094586], 
processed observation next is [1.0, 0.34782608695652173, 0.3380726698262243, 0.7466666666666667, 1.0, 1.0, 0.04678362482163674, 1.0, 1.0, 0.04678362482163674, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.17995800015344157, 0.17995800015344177, 0.36484369419322177], 
reward next is 0.6352, 
noisyNet noise sample is [array([0.05701029], dtype=float32), -0.233441]. 
=============================================
[2019-04-28 01:56:56,860] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4492899e-13 1.4167824e-08 4.2621691e-25 1.0000000e+00 1.6291054e-12], sum to 1.0000
[2019-04-28 01:56:56,870] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7864
[2019-04-28 01:56:56,875] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.26666666666667, 54.83333333333333, 1.0, 2.0, 0.1775498707785082, 1.0, 2.0, 0.1775498707785082, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 540645.4574396154, 540645.4574396154, 238450.3722895397], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6891000.0000, 
sim time next is 6891600.0000, 
raw observation next is [28.13333333333333, 55.66666666666667, 1.0, 2.0, 0.1798741748562946, 1.0, 2.0, 0.1798741748562946, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 547182.6675530031, 547182.6675530031, 238675.7670601856], 
processed observation next is [0.0, 0.782608695652174, 0.532385466034755, 0.5566666666666668, 1.0, 1.0, 0.011896596212403104, 1.0, 1.0, 0.011896596212403104, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15199518543138973, 0.15199518543138973, 0.35623248814953073], 
reward next is 0.6438, 
noisyNet noise sample is [array([-1.089106], dtype=float32), -0.7618918]. 
=============================================
[2019-04-28 01:56:58,553] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3474298e-15 4.7956816e-09 3.5907596e-27 1.0000000e+00 6.1971621e-14], sum to 1.0000
[2019-04-28 01:56:58,560] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3460
[2019-04-28 01:56:58,563] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.6, 56.33333333333334, 1.0, 2.0, 0.1749351999507378, 1.0, 2.0, 0.1749351999507378, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 537274.030514759, 537274.030514759, 238808.631139411], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6862800.0000, 
sim time next is 6863400.0000, 
raw observation next is [27.8, 55.0, 1.0, 2.0, 0.1739722803499378, 1.0, 2.0, 0.1739722803499378, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 535196.6922849163, 535196.6922849163, 238813.6378952225], 
processed observation next is [0.0, 0.43478260869565216, 0.5165876777251186, 0.55, 1.0, 1.0, 0.004785879939684092, 1.0, 1.0, 0.004785879939684092, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1486657478569212, 0.1486657478569212, 0.3564382655152574], 
reward next is 0.6436, 
noisyNet noise sample is [array([0.63101304], dtype=float32), 1.8549296]. 
=============================================
[2019-04-28 01:57:01,797] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4865737e-13 4.5888957e-08 2.0535352e-22 1.0000000e+00 4.6254043e-13], sum to 1.0000
[2019-04-28 01:57:01,803] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3106
[2019-04-28 01:57:01,809] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.71666666666667, 86.0, 1.0, 2.0, 0.2368658303633788, 1.0, 2.0, 0.2368658303633788, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 661952.9682475986, 661952.9682475992, 238839.0843717031], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7171800.0000, 
sim time next is 7172400.0000, 
raw observation next is [25.73333333333333, 86.0, 1.0, 2.0, 0.2369509093516218, 1.0, 2.0, 0.2369509093516218, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 662190.8061513022, 662190.8061513022, 238852.4686011056], 
processed observation next is [1.0, 0.0, 0.41864139020537117, 0.86, 1.0, 1.0, 0.08066374620677325, 1.0, 1.0, 0.08066374620677325, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18394189059758395, 0.18394189059758395, 0.3564962217926949], 
reward next is 0.6435, 
noisyNet noise sample is [array([0.7936097], dtype=float32), 0.3358118]. 
=============================================
[2019-04-28 01:57:11,488] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.21267940e-15 1.41835335e-08 9.96412280e-25 1.00000000e+00
 5.90966078e-15], sum to 1.0000
[2019-04-28 01:57:11,495] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8855
[2019-04-28 01:57:11,500] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.7, 55.0, 1.0, 2.0, 0.1918698847981242, 1.0, 2.0, 0.1918698847981242, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 575981.4721936123, 575981.4721936116, 239158.7348759916], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6984000.0000, 
sim time next is 6984600.0000, 
raw observation next is [28.58333333333334, 56.16666666666667, 1.0, 2.0, 0.1913193499063105, 1.0, 2.0, 0.1913193499063105, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 573280.2559507323, 573280.2559507323, 238918.971315566], 
processed observation next is [0.0, 0.8695652173913043, 0.5537124802527649, 0.5616666666666668, 1.0, 1.0, 0.025685963742542756, 1.0, 1.0, 0.025685963742542756, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.15924451554187008, 0.15924451554187008, 0.35659547957547166], 
reward next is 0.6434, 
noisyNet noise sample is [array([-0.2627624], dtype=float32), -0.1634542]. 
=============================================
[2019-04-28 01:57:15,294] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.3364344e-13 8.6238590e-09 1.2566273e-23 1.0000000e+00 2.4822595e-13], sum to 1.0000
[2019-04-28 01:57:15,303] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8183
[2019-04-28 01:57:15,307] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.85, 52.5, 1.0, 2.0, 0.673284112637229, 1.0, 2.0, 0.673284112637229, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 1889199.90044824, 1889199.900448241, 363701.2153152824], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7039800.0000, 
sim time next is 7040400.0000, 
raw observation next is [30.93333333333334, 52.0, 1.0, 2.0, 0.6913572817973438, 1.0, 2.0, 0.6913572817973438, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1941276.843046312, 1941276.843046312, 371444.3823612195], 
processed observation next is [1.0, 0.4782608695652174, 0.6650868878357034, 0.52, 1.0, 1.0, 0.6281413033702937, 1.0, 1.0, 0.6281413033702937, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5392435675128644, 0.5392435675128644, 0.5543946005391336], 
reward next is 0.4456, 
noisyNet noise sample is [array([-0.4156089], dtype=float32), -1.1274729]. 
=============================================
[2019-04-28 01:57:18,860] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.4845611e-14 4.2692254e-09 2.8928528e-25 1.0000000e+00 2.1428862e-15], sum to 1.0000
[2019-04-28 01:57:18,868] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8353
[2019-04-28 01:57:18,872] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.23333333333333, 83.83333333333333, 1.0, 2.0, 0.6027675716922516, 1.0, 2.0, 0.6027675716922516, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1685317.588920088, 1685317.588920088, 335265.624722909], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7141800.0000, 
sim time next is 7142400.0000, 
raw observation next is [26.2, 84.0, 1.0, 2.0, 0.5913522500473897, 1.0, 2.0, 0.5913522500473897, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1653376.117344011, 1653376.117344011, 331069.0407629274], 
processed observation next is [1.0, 0.6956521739130435, 0.44075829383886256, 0.84, 1.0, 1.0, 0.507653313310108, 1.0, 1.0, 0.507653313310108, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.45927114370666977, 0.45927114370666977, 0.4941328966610856], 
reward next is 0.5059, 
noisyNet noise sample is [array([-0.03511514], dtype=float32), 0.6953757]. 
=============================================
[2019-04-28 01:57:19,915] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.3192480e-13 1.0410819e-07 3.1680081e-22 9.9999988e-01 4.8453779e-16], sum to 1.0000
[2019-04-28 01:57:19,923] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.4283
[2019-04-28 01:57:19,928] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.21666666666667, 90.16666666666666, 1.0, 2.0, 0.1729195594122019, 1.0, 2.0, 0.1729195594122019, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 532291.3992820231, 532291.3992820225, 238725.8817538219], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7257000.0000, 
sim time next is 7257600.0000, 
raw observation next is [22.2, 90.0, 1.0, 2.0, 0.1753467802892376, 1.0, 2.0, 0.1753467802892376, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 540309.673299299, 540309.673299299, 239123.4978853167], 
processed observation next is [1.0, 0.0, 0.2511848341232228, 0.9, 1.0, 1.0, 0.00644190396293687, 1.0, 1.0, 0.00644190396293687, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1500860203609164, 0.1500860203609164, 0.356900743112413], 
reward next is 0.6431, 
noisyNet noise sample is [array([1.8700056], dtype=float32), -0.26276094]. 
=============================================
[2019-04-28 01:57:33,312] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.1251711e-16 1.3923180e-09 4.3245657e-26 1.0000000e+00 7.5117105e-17], sum to 1.0000
[2019-04-28 01:57:33,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1924
[2019-04-28 01:57:33,329] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.55, 79.5, 1.0, 2.0, 0.2092110619111412, 1.0, 2.0, 0.2092110619111412, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 605464.3585067814, 605464.3585067821, 238191.6366871934], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7497000.0000, 
sim time next is 7497600.0000, 
raw observation next is [25.43333333333333, 80.0, 1.0, 2.0, 0.2083163046763716, 1.0, 2.0, 0.2083163046763716, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 603828.6737714983, 603828.6737714983, 238215.8686424526], 
processed observation next is [0.0, 0.782608695652174, 0.40442338072669815, 0.8, 1.0, 1.0, 0.04616422250165252, 1.0, 1.0, 0.04616422250165252, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16773018715874952, 0.16773018715874952, 0.35554607260067556], 
reward next is 0.6445, 
noisyNet noise sample is [array([-0.290712], dtype=float32), 0.62268347]. 
=============================================
[2019-04-28 01:57:39,618] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0480545e-12 3.9038621e-09 2.5146429e-22 1.0000000e+00 2.4598708e-14], sum to 1.0000
[2019-04-28 01:57:39,630] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0203
[2019-04-28 01:57:39,634] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [25.5, 90.33333333333334, 1.0, 2.0, 0.2941752754126596, 1.0, 2.0, 0.2941752754126596, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 822173.1952419996, 822173.1952419996, 248750.7934023051], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7792800.0000, 
sim time next is 7793400.0000, 
raw observation next is [25.45, 90.66666666666667, 1.0, 2.0, 0.2904098799234607, 1.0, 2.0, 0.2904098799234607, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 811645.5305370558, 811645.5305370558, 248040.4913420875], 
processed observation next is [1.0, 0.17391304347826086, 0.4052132701421801, 0.9066666666666667, 1.0, 1.0, 0.1450721444860972, 1.0, 1.0, 0.1450721444860972, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.22545709181584883, 0.22545709181584883, 0.37020968857027986], 
reward next is 0.6298, 
noisyNet noise sample is [array([-1.096908], dtype=float32), 1.7850199]. 
=============================================
[2019-04-28 01:57:40,573] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-28 01:57:40,573] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:57:40,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:57:40,575] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:57:40,576] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:57:40,576] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:57:40,577] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:57:40,578] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:57:40,578] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:57:40,580] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:57:40,580] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:57:40,603] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run77
[2019-04-28 01:57:40,604] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run77
[2019-04-28 01:57:40,664] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run77
[2019-04-28 01:57:40,697] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run77
[2019-04-28 01:57:40,734] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run77
[2019-04-28 01:57:46,154] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06326396]
[2019-04-28 01:57:46,155] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [18.95, 89.83333333333333, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 419874.6003311156, 419874.6003311156, 216394.8954991577]
[2019-04-28 01:57:46,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 01:57:46,158] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3795079e-13 1.9525157e-08 5.8744357e-24 1.0000000e+00 1.3333588e-14], sampled 0.4702112455912745
[2019-04-28 01:57:47,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06326396]
[2019-04-28 01:57:47,792] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.0, 82.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 377024.4928834801, 377024.4928834801, 208091.0659265468]
[2019-04-28 01:57:47,793] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 01:57:47,796] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8501414e-13 2.3327271e-08 1.0022682e-23 1.0000000e+00 1.8363764e-14], sampled 0.8429383807834967
[2019-04-28 01:58:11,848] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06326396]
[2019-04-28 01:58:11,849] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.95657915, 88.84288133, 1.0, 2.0, 0.1895229931958091, 1.0, 2.0, 0.1895229931958091, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 572627.8602665595, 572627.8602665595, 239889.4359763378]
[2019-04-28 01:58:11,850] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 01:58:11,852] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.1425522e-14 7.9609466e-09 3.9766601e-25 1.0000000e+00 2.6551406e-15], sampled 0.5754944064873675
[2019-04-28 01:59:05,059] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 01:59:05,086] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 01:59:05,199] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 01:59:05,202] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 01:59:05,359] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 01:59:06,373] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1900000, evaluation results [1900000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 01:59:07,567] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.7064375e-13 1.0224261e-08 6.4170005e-24 1.0000000e+00 7.6916394e-16], sum to 1.0000
[2019-04-28 01:59:07,574] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0468
[2019-04-28 01:59:07,585] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 67.66666666666667, 1.0, 2.0, 0.8310100586389574, 1.0, 2.0, 0.8310100586389574, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2324150.748690235, 2324150.748690235, 435249.7155147599], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7831200.0000, 
sim time next is 7831800.0000, 
raw observation next is [31.0, 66.5, 1.0, 2.0, 0.8201360156231222, 1.0, 2.0, 0.8201360156231222, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2293710.553790683, 2293710.553790683, 429791.3183538], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.665, 1.0, 1.0, 0.7832964043652075, 1.0, 1.0, 0.7832964043652075, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.637141820497412, 0.637141820497412, 0.6414795796325373], 
reward next is 0.3585, 
noisyNet noise sample is [array([-2.1466272], dtype=float32), -0.33824277]. 
=============================================
[2019-04-28 01:59:10,411] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:10,414] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:10,436] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run10
[2019-04-28 01:59:13,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:13,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:13,904] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run10
[2019-04-28 01:59:14,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:14,683] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:14,703] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run10
[2019-04-28 01:59:19,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:19,010] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:19,064] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run10
[2019-04-28 01:59:19,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:19,700] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:19,707] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run10
[2019-04-28 01:59:20,164] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:20,165] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:20,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run10
[2019-04-28 01:59:20,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:20,496] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:20,522] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run10
[2019-04-28 01:59:21,407] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0824867e-13 1.2498507e-08 2.0664068e-22 1.0000000e+00 2.9356031e-13], sum to 1.0000
[2019-04-28 01:59:21,415] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8712
[2019-04-28 01:59:21,420] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [22.5, 90.0, 1.0, 2.0, 0.1914984549538145, 1.0, 2.0, 0.1914984549538145, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 584841.9749845053, 584841.9749845053, 240601.8285390607], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 100800.0000, 
sim time next is 101400.0000, 
raw observation next is [22.51666666666667, 90.16666666666667, 1.0, 2.0, 0.191006208325435, 1.0, 2.0, 0.191006208325435, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 582849.8853595826, 582849.8853595826, 240461.8529615288], 
processed observation next is [1.0, 0.17391304347826086, 0.2661927330173777, 0.9016666666666667, 1.0, 1.0, 0.02530868472943974, 1.0, 1.0, 0.02530868472943974, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.16190274593321738, 0.16190274593321738, 0.3588982880022818], 
reward next is 0.6411, 
noisyNet noise sample is [array([-0.7839598], dtype=float32), -0.6959177]. 
=============================================
[2019-04-28 01:59:23,470] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:23,473] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:23,483] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:23,483] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:23,516] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run10
[2019-04-28 01:59:23,574] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run10
[2019-04-28 01:59:24,296] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:24,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:24,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:24,297] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:24,311] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run10
[2019-04-28 01:59:24,348] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run10
[2019-04-28 01:59:24,795] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.8388829e-13 7.9392969e-08 4.8825462e-22 9.9999988e-01 1.6468748e-13], sum to 1.0000
[2019-04-28 01:59:24,801] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6474
[2019-04-28 01:59:24,813] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.11666666666667, 90.83333333333334, 1.0, 2.0, 0.4192979980158973, 1.0, 2.0, 0.4192979980158973, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1172062.45519221, 1172062.45519221, 277093.0603269489], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7873800.0000, 
sim time next is 7874400.0000, 
raw observation next is [26.13333333333333, 90.66666666666667, 1.0, 2.0, 0.3491230269894305, 1.0, 2.0, 0.3491230269894305, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 975813.3064591421, 975813.3064591421, 260067.5775646083], 
processed observation next is [1.0, 0.13043478260869565, 0.43759873617693507, 0.9066666666666667, 1.0, 1.0, 0.2158108758908801, 1.0, 1.0, 0.2158108758908801, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.27105925179420615, 0.27105925179420615, 0.38816056352926614], 
reward next is 0.6118, 
noisyNet noise sample is [array([-0.17604598], dtype=float32), 1.2414904]. 
=============================================
[2019-04-28 01:59:28,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:28,504] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:28,515] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run10
[2019-04-28 01:59:28,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:28,638] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:28,673] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run10
[2019-04-28 01:59:29,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:29,768] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:29,790] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run10
[2019-04-28 01:59:30,481] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:30,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:30,513] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run10
[2019-04-28 01:59:31,492] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 01:59:31,492] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:31,526] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run10
[2019-04-28 01:59:39,011] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7343736e-13 9.9969876e-01 4.0203909e-25 3.0130357e-04 2.1397814e-12], sum to 1.0000
[2019-04-28 01:59:39,017] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5849
[2019-04-28 01:59:39,022] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.01666666666667, 94.16666666666667, 1.0, 2.0, 0.2797420541595443, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 451222.5037970109, 451222.5037970115, 163681.6926408365], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 270600.0000, 
sim time next is 271200.0000, 
raw observation next is [19.93333333333334, 94.33333333333334, 1.0, 2.0, 0.2776022783389498, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 448295.1869766206, 448295.1869766201, 163486.0269349681], 
processed observation next is [0.0, 0.13043478260869565, 0.14375987361769393, 0.9433333333333335, 1.0, 1.0, 0.12964129920355397, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12452644082683904, 0.12452644082683892, 0.24400899542532553], 
reward next is 0.7560, 
noisyNet noise sample is [array([2.209337], dtype=float32), -1.0470072]. 
=============================================
[2019-04-28 01:59:42,176] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.8163301e-13 9.9951458e-01 1.5541727e-23 4.8547969e-04 8.9644112e-16], sum to 1.0000
[2019-04-28 01:59:42,185] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.6770
[2019-04-28 01:59:42,197] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 81.0, 1.0, 2.0, 0.2707184778256851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 440242.1385189432, 440242.1385189438, 162923.6554578215], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 412800.0000, 
sim time next is 413400.0000, 
raw observation next is [21.08333333333333, 81.0, 1.0, 2.0, 0.269360568144053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 438600.4658257866, 438600.4658257873, 162804.6934823243], 
processed observation next is [1.0, 0.782608695652174, 0.1982622432859398, 0.81, 1.0, 1.0, 0.11971152788440122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.12183346272938517, 0.12183346272938536, 0.24299207982436463], 
reward next is 0.7570, 
noisyNet noise sample is [array([-0.49056348], dtype=float32), -0.35756615]. 
=============================================
[2019-04-28 01:59:44,463] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.9317299e-12 9.9998236e-01 5.8704421e-29 1.7598730e-05 2.8421228e-19], sum to 1.0000
[2019-04-28 01:59:44,472] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9917
[2019-04-28 01:59:44,476] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 87.16666666666667, 1.0, 2.0, 0.2821006884393824, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459653.4282804437, 459653.4282804437, 164179.2948031635], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 353400.0000, 
sim time next is 354000.0000, 
raw observation next is [20.26666666666667, 87.33333333333334, 1.0, 2.0, 0.2630778905028809, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 428627.6173948369, 428627.6173948369, 162163.38724596], 
processed observation next is [1.0, 0.08695652173913043, 0.15955766192733034, 0.8733333333333334, 1.0, 1.0, 0.11214203675045889, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11906322705412137, 0.11906322705412137, 0.24203490633725375], 
reward next is 0.7580, 
noisyNet noise sample is [array([1.3043122], dtype=float32), -0.9675831]. 
=============================================
[2019-04-28 01:59:44,493] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[82.702644]
 [82.54556 ]
 [82.75025 ]
 [82.79694 ]
 [82.9676  ]], R is [[82.19134521]
 [82.12438965]
 [82.06154633]
 [81.99929047]
 [81.93763733]].
[2019-04-28 01:59:45,005] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.0643475e-14 9.9999070e-01 2.4270860e-28 9.2805030e-06 1.0302781e-16], sum to 1.0000
[2019-04-28 01:59:45,013] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9429
[2019-04-28 01:59:45,017] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.2, 75.0, 1.0, 2.0, 0.3653496452052277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 600724.2853475936, 600724.2853475936, 174600.5831611978], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 550200.0000, 
sim time next is 550800.0000, 
raw observation next is [21.2, 75.0, 1.0, 2.0, 0.4174778319579757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 686437.3715234355, 686437.3715234349, 182086.0896672615], 
processed observation next is [1.0, 0.391304347826087, 0.20379146919431282, 0.75, 1.0, 1.0, 0.29816606259997075, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19067704764539875, 0.19067704764539858, 0.2717702830854649], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.22925104], dtype=float32), -0.7809331]. 
=============================================
[2019-04-28 01:59:48,169] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.8711209e-13 9.9985373e-01 5.3773380e-28 1.4630065e-04 1.4815467e-14], sum to 1.0000
[2019-04-28 01:59:48,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2509
[2019-04-28 01:59:48,183] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.23333333333333, 85.0, 1.0, 2.0, 0.2357627921104252, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390452.6277067414, 390452.6277067414, 159402.8310996984], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 512400.0000, 
sim time next is 513000.0000, 
raw observation next is [19.15, 85.5, 1.0, 2.0, 0.2350162099216474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 389299.1388566919, 389299.1388566919, 159325.0359620243], 
processed observation next is [1.0, 0.9565217391304348, 0.10663507109004738, 0.855, 1.0, 1.0, 0.07833278303812939, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10813864968241442, 0.10813864968241442, 0.2377985611373497], 
reward next is 0.7622, 
noisyNet noise sample is [array([0.41506034], dtype=float32), -0.94773465]. 
=============================================
[2019-04-28 01:59:48,208] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[75.68618]
 [75.72288]
 [75.77021]
 [75.80177]
 [75.92567]], R is [[75.64414215]
 [75.6497879 ]
 [75.6552887 ]
 [75.66065979]
 [75.66592407]].
[2019-04-28 01:59:54,957] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [3.1086750e-14 9.9999881e-01 7.9855646e-30 1.2446370e-06 3.5854392e-15], sum to 1.0000
[2019-04-28 01:59:54,960] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5325
[2019-04-28 01:59:54,967] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 73.16666666666667, 1.0, 2.0, 0.3049239794157823, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 503168.4676308419, 503168.4676308419, 166812.8907504143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 634200.0000, 
sim time next is 634800.0000, 
raw observation next is [21.33333333333334, 72.33333333333334, 1.0, 2.0, 0.3499500078984439, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 577136.3519913014, 577136.351991302, 172492.5667516778], 
processed observation next is [1.0, 0.34782608695652173, 0.2101105845181678, 0.7233333333333334, 1.0, 1.0, 0.2168072384318601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16031565333091705, 0.1603156533309172, 0.2574515921666833], 
reward next is 0.7425, 
noisyNet noise sample is [array([-2.196988], dtype=float32), -0.48409072]. 
=============================================
[2019-04-28 01:59:57,352] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.2265085e-12 9.9981600e-01 6.7642042e-27 1.8402340e-04 6.9460648e-13], sum to 1.0000
[2019-04-28 01:59:57,357] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0983
[2019-04-28 01:59:57,360] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 64.33333333333334, 1.0, 2.0, 0.4487163907517874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 736011.6630349646, 736011.6630349639, 187009.2207271415], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 472800.0000, 
sim time next is 473400.0000, 
raw observation next is [23.15, 63.5, 1.0, 2.0, 0.459211428534984, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 753088.038394804, 753088.038394804, 188738.8580953126], 
processed observation next is [1.0, 0.4782608695652174, 0.2962085308056872, 0.635, 1.0, 1.0, 0.3484475042590168, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20919112177633445, 0.20919112177633445, 0.2816997882019591], 
reward next is 0.7183, 
noisyNet noise sample is [array([0.13437243], dtype=float32), 0.6493841]. 
=============================================
[2019-04-28 01:59:59,081] A3C_AGENT_WORKER-Thread-2 INFO:Evaluating...
[2019-04-28 01:59:59,087] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 01:59:59,088] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:59,088] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 01:59:59,090] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 01:59:59,090] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 01:59:59,091] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:59,092] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:59,092] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 01:59:59,093] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:59,093] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 01:59:59,118] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run78
[2019-04-28 01:59:59,149] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run78
[2019-04-28 01:59:59,200] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run78
[2019-04-28 01:59:59,231] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run78
[2019-04-28 01:59:59,274] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run78
[2019-04-28 02:00:09,735] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:00:09,736] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.06666666666666, 61.66666666666666, 1.0, 2.0, 0.2895305742930316, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 463387.3086991419, 463387.3086991413, 164486.7115358807]
[2019-04-28 02:00:09,737] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:00:09,740] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5965956e-13 9.9999237e-01 3.2058084e-27 7.6286924e-06 5.7663340e-16], sampled 0.7132360162112159
[2019-04-28 02:00:15,891] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:00:15,891] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.24802762666667, 76.94591840333332, 1.0, 2.0, 0.631281815260444, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986464251704184, 6.9112, 168.9124204960885, 1765122.260380705, 1711727.379265547, 370482.7128590386]
[2019-04-28 02:00:15,892] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:00:15,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3741739e-12 9.9998200e-01 2.8622524e-25 1.8041959e-05 7.6499527e-15], sampled 0.36458872932911757
[2019-04-28 02:00:15,894] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1765122.260380705 W.
[2019-04-28 02:00:24,195] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:00:24,197] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.4, 95.0, 1.0, 2.0, 0.379831016541772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575783.0084317538, 575783.0084317545, 172608.6016807375]
[2019-04-28 02:00:24,198] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:00:24,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [9.6317438e-14 9.9999380e-01 1.1133519e-27 6.2383720e-06 3.1345799e-16], sampled 0.965196153648479
[2019-04-28 02:00:58,697] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:00:58,698] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.45884634, 77.612250825, 1.0, 2.0, 0.4188786766211793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 621253.2706748529, 621253.2706748536, 176426.0406914644]
[2019-04-28 02:00:58,700] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:00:58,701] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.1392744e-14 9.9999416e-01 7.8537882e-28 5.8183045e-06 2.5649664e-16], sampled 0.5908301559470401
[2019-04-28 02:01:21,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:01:21,488] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.11666666666667, 80.83333333333333, 1.0, 2.0, 0.332959164817643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 521217.1551355576, 521217.1551355583, 168538.1069724946]
[2019-04-28 02:01:21,489] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:01:21,492] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.03904075e-13 9.99993563e-01 1.30590732e-27 6.44134116e-06
 3.43859272e-16], sampled 0.46640547720719816
[2019-04-28 02:01:32,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.06911134]
[2019-04-28 02:01:32,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [29.44568853666667, 62.98534114666667, 1.0, 2.0, 0.7626469650671444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1067977.970447369, 1067977.970447368, 235238.351569522]
[2019-04-28 02:01:32,504] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:01:32,523] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.6250019e-13 9.9999237e-01 3.3223035e-27 7.6878068e-06 5.8882125e-16], sampled 0.745932331990766
[2019-04-28 02:01:35,032] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:01:35,759] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3240 2927318304.0717 1338.0000
[2019-04-28 02:01:35,808] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:01:35,826] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7998.2677 3007635464.4703 1766.0000
[2019-04-28 02:01:35,870] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:01:36,887] A3C_AGENT_WORKER-Thread-2 INFO:Global step: 1925000, evaluation results [1925000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.324045648182, 2927318304.071684, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7998.267686494451, 3007635464.4702907, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:01:39,750] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.4964044e-16 9.9999976e-01 3.7042701e-30 2.7457196e-07 4.0318633e-17], sum to 1.0000
[2019-04-28 02:01:39,758] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.7288
[2019-04-28 02:01:39,764] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.75, 48.5, 1.0, 2.0, 0.5667411950691573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 930732.9112582929, 930732.9112582929, 208653.4123552284], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 743400.0000, 
sim time next is 744000.0000, 
raw observation next is [25.66666666666667, 48.66666666666666, 1.0, 2.0, 0.5705225740678224, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 937448.0702151209, 937448.0702151209, 209427.0785945094], 
processed observation next is [1.0, 0.6086956521739131, 0.4154818325434442, 0.4866666666666666, 1.0, 1.0, 0.4825573181540028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26040224172642246, 0.26040224172642246, 0.3125777292455364], 
reward next is 0.6874, 
noisyNet noise sample is [array([1.7726347], dtype=float32), -0.43210077]. 
=============================================
[2019-04-28 02:01:39,804] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[83.131325]
 [83.054474]
 [82.939644]
 [82.88822 ]
 [82.783745]], R is [[83.0316391 ]
 [82.88990021]
 [82.74823761]
 [82.59525299]
 [82.43781281]].
[2019-04-28 02:01:39,913] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2242698e-13 9.9997389e-01 1.2799411e-25 2.6113454e-05 1.8087909e-17], sum to 1.0000
[2019-04-28 02:01:39,922] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1202
[2019-04-28 02:01:39,928] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.5, 49.0, 1.0, 2.0, 0.6018195680063377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 989904.4425285937, 989904.4425285937, 216003.0288782971], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 745200.0000, 
sim time next is 745800.0000, 
raw observation next is [25.43333333333333, 49.16666666666667, 1.0, 2.0, 0.6247414232348923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027999.983668458, 1027999.983668458, 221030.3127641119], 
processed observation next is [1.0, 0.6521739130434783, 0.40442338072669815, 0.4916666666666667, 1.0, 1.0, 0.5478812328131233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2855555510190161, 0.2855555510190161, 0.32989598920016705], 
reward next is 0.6701, 
noisyNet noise sample is [array([-0.9976012], dtype=float32), -0.24802674]. 
=============================================
[2019-04-28 02:01:43,747] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.1580248e-12 9.9999821e-01 6.4053133e-27 1.8372215e-06 3.6629191e-14], sum to 1.0000
[2019-04-28 02:01:43,752] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.8890
[2019-04-28 02:01:43,756] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.75, 68.5, 1.0, 2.0, 0.3146142390191519, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 495163.9308618236, 495163.9308618236, 166612.2068282489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 917400.0000, 
sim time next is 918000.0000, 
raw observation next is [24.7, 69.0, 1.0, 2.0, 0.3156583322286775, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 496459.4292085069, 496459.4292085069, 166700.2157904235], 
processed observation next is [0.0, 0.6521739130434783, 0.3696682464454976, 0.69, 1.0, 1.0, 0.1754919665405753, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.137905397002363, 0.137905397002363, 0.2488062922245127], 
reward next is 0.7512, 
noisyNet noise sample is [array([0.8367928], dtype=float32), -0.90889347]. 
=============================================
[2019-04-28 02:01:43,768] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[74.031395]
 [74.02485 ]
 [74.0241  ]
 [74.03293 ]
 [74.02781 ]], R is [[74.05613708]
 [74.06690216]
 [74.07774353]
 [74.08872223]
 [74.09978485]].
[2019-04-28 02:02:00,037] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.2166258e-14 1.0000000e+00 5.3978941e-29 2.8625252e-08 1.1993466e-17], sum to 1.0000
[2019-04-28 02:02:00,043] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9270
[2019-04-28 02:02:00,047] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2938306209642257, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469033.7216901088, 469033.7216901088, 164862.9914155997], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 901200.0000, 
sim time next is 901800.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2938409864394043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 469050.3185091248, 469050.3185091242, 164864.1524690835], 
processed observation next is [0.0, 0.43478260869565216, 0.2654028436018958, 0.79, 1.0, 1.0, 0.14920600775831844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13029175514142355, 0.13029175514142338, 0.2460658992075873], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.7787029], dtype=float32), -0.16679773]. 
=============================================
[2019-04-28 02:02:02,805] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.5702659e-15 9.9999988e-01 6.3546194e-30 1.3065342e-07 6.2225311e-18], sum to 1.0000
[2019-04-28 02:02:02,811] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3282
[2019-04-28 02:02:02,815] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 79.0, 1.0, 2.0, 0.2936097048136517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 468681.2068542926, 468681.2068542932, 164838.3568301626], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 898200.0000, 
sim time next is 898800.0000, 
raw observation next is [22.5, 79.0, 1.0, 2.0, 0.2938548681027904, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 469072.5909474737, 469072.5909474737, 164865.7112779409], 
processed observation next is [0.0, 0.391304347826087, 0.2654028436018958, 0.79, 1.0, 1.0, 0.1492227326539643, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1302979419298538, 0.1302979419298538, 0.2460682257879715], 
reward next is 0.7539, 
noisyNet noise sample is [array([1.6372354], dtype=float32), -1.260734]. 
=============================================
[2019-04-28 02:02:03,533] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.9591927e-15 1.0000000e+00 1.5631827e-30 2.3089330e-09 2.0363929e-18], sum to 1.0000
[2019-04-28 02:02:03,555] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3738
[2019-04-28 02:02:03,559] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 97.0, 1.0, 2.0, 0.3809588480346077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 573181.2049272379, 573181.2049272379, 172248.3978779776], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1040400.0000, 
sim time next is 1041000.0000, 
raw observation next is [22.43333333333333, 96.83333333333334, 1.0, 2.0, 0.3810266070358634, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 572905.3294677648, 572905.3294677654, 172212.0456711038], 
processed observation next is [1.0, 0.043478260869565216, 0.2622432859399683, 0.9683333333333334, 1.0, 1.0, 0.25424892413959443, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15914036929660133, 0.1591403692966015, 0.2570329039867221], 
reward next is 0.7430, 
noisyNet noise sample is [array([1.2997663], dtype=float32), -1.2989601]. 
=============================================
[2019-04-28 02:02:03,584] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[77.766594]
 [77.73409 ]
 [77.88644 ]
 [78.15035 ]
 [78.32776 ]], R is [[77.76777649]
 [77.73300934]
 [77.69861603]
 [77.66465759]
 [77.63122559]].
[2019-04-28 02:02:04,837] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.9287792e-13 9.9999976e-01 1.6908975e-25 2.4669410e-07 6.3081948e-16], sum to 1.0000
[2019-04-28 02:02:04,845] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1248
[2019-04-28 02:02:04,848] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.3, 87.0, 1.0, 2.0, 0.3343944523434787, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 535428.7799681944, 535428.7799681938, 169806.1478887032], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1148400.0000, 
sim time next is 1149000.0000, 
raw observation next is [21.45, 86.33333333333333, 1.0, 2.0, 0.3332762654430594, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532985.2542815327, 532985.2542815327, 169614.2838501116], 
processed observation next is [1.0, 0.30434782608695654, 0.2156398104265403, 0.8633333333333333, 1.0, 1.0, 0.19671839210007155, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14805145952264798, 0.14805145952264798, 0.25315564753748], 
reward next is 0.7468, 
noisyNet noise sample is [array([-0.0250943], dtype=float32), 1.6411585]. 
=============================================
[2019-04-28 02:02:04,892] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.92406 ]
 [68.95499 ]
 [68.98615 ]
 [68.997025]
 [69.05907 ]], R is [[68.92893982]
 [68.98621368]
 [69.04570007]
 [69.10356903]
 [69.15822601]].
[2019-04-28 02:02:18,021] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.8103622e-12 1.0000000e+00 1.3837455e-24 3.1465308e-09 2.5369849e-16], sum to 1.0000
[2019-04-28 02:02:18,026] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6091
[2019-04-28 02:02:18,032] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2706537.704553578 W.
[2019-04-28 02:02:18,037] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.43333333333333, 73.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.676057095263893, 6.9112, 168.9028346419046, 2706537.704553578, 1454562.36681619, 310071.16082018], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 1257600.0000, 
sim time next is 1258200.0000, 
raw observation next is [28.45, 73.0, 1.0, 2.0, 0.7686183049937503, 1.0, 1.0, 0.7686183049937503, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2149495.750375871, 2149495.750375871, 404872.9144218082], 
processed observation next is [1.0, 0.5652173913043478, 0.54739336492891, 0.73, 1.0, 1.0, 0.7212268734864461, 1.0, 0.5, 0.7212268734864461, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5970821528821864, 0.5970821528821864, 0.604287931972848], 
reward next is 0.3957, 
noisyNet noise sample is [array([-1.2661282], dtype=float32), 2.0625918]. 
=============================================
[2019-04-28 02:02:18,331] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [2.3168888e-15 1.0000000e+00 2.2101783e-27 7.0018291e-10 6.9229670e-16], sum to 1.0000
[2019-04-28 02:02:18,334] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4886
[2019-04-28 02:02:18,341] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.73333333333333, 94.33333333333334, 1.0, 2.0, 0.8214807975876162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1233206.93885798, 1233206.93885798, 260891.7385488646], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1336800.0000, 
sim time next is 1337400.0000, 
raw observation next is [22.65, 94.0, 1.0, 2.0, 0.8230026852319834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1240633.399539506, 1240633.399539506, 261960.4021930824], 
processed observation next is [1.0, 0.4782608695652174, 0.2725118483412322, 0.94, 1.0, 1.0, 0.7867502231710644, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3446203887609739, 0.3446203887609739, 0.3909856749150484], 
reward next is 0.6090, 
noisyNet noise sample is [array([1.0454894], dtype=float32), -0.05645774]. 
=============================================
[2019-04-28 02:02:18,867] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0873097e-14 1.0000000e+00 2.9037275e-28 5.4282210e-09 1.2972178e-18], sum to 1.0000
[2019-04-28 02:02:18,867] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5873
[2019-04-28 02:02:18,873] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.66666666666667, 96.0, 1.0, 2.0, 0.349355125073672, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 539403.3320129109, 539403.3320129103, 169803.4621697461], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1471200.0000, 
sim time next is 1471800.0000, 
raw observation next is [21.63333333333334, 96.0, 1.0, 2.0, 0.3475423436181706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 537115.7424958579, 537115.7424958579, 169630.9236938214], 
processed observation next is [0.0, 0.0, 0.2243285939968408, 0.96, 1.0, 1.0, 0.21390643809418144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14919881735996052, 0.14919881735996052, 0.25318048312510655], 
reward next is 0.7468, 
noisyNet noise sample is [array([0.00678285], dtype=float32), -0.86923504]. 
=============================================
[2019-04-28 02:02:20,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.4051165e-16 1.0000000e+00 1.0644863e-30 6.3407342e-11 1.0278185e-16], sum to 1.0000
[2019-04-28 02:02:20,150] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8982
[2019-04-28 02:02:20,159] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333334, 84.0, 1.0, 2.0, 0.3581823112842414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 549437.9069466967, 549437.9069466974, 170532.5589414631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1542000.0000, 
sim time next is 1542600.0000, 
raw observation next is [23.35, 84.5, 1.0, 2.0, 0.3581996044590471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 549699.2639673849, 549699.2639673844, 170561.4200263185], 
processed observation next is [0.0, 0.8695652173913043, 0.3056872037914693, 0.845, 1.0, 1.0, 0.22674651139644225, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15269423999094026, 0.1526942399909401, 0.2545692836213709], 
reward next is 0.7454, 
noisyNet noise sample is [array([0.4516871], dtype=float32), 0.39719245]. 
=============================================
[2019-04-28 02:02:28,823] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [4.0408472e-14 1.0000000e+00 2.2406032e-27 8.1959239e-09 6.0757181e-19], sum to 1.0000
[2019-04-28 02:02:28,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4782
[2019-04-28 02:02:28,864] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.1, 95.66666666666666, 1.0, 2.0, 0.4117705017689461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 607493.277684888, 607493.2776848886, 175036.1080805592], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1621200.0000, 
sim time next is 1621800.0000, 
raw observation next is [23.1, 95.5, 1.0, 2.0, 0.4116340556942512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 607754.4664720419, 607754.4664720419, 175073.8917299772], 
processed observation next is [1.0, 0.782608695652174, 0.2938388625592418, 0.955, 1.0, 1.0, 0.29112536830632674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16882068513112275, 0.16882068513112275, 0.26130431601489135], 
reward next is 0.7387, 
noisyNet noise sample is [array([1.0240805], dtype=float32), 0.4724344]. 
=============================================
[2019-04-28 02:02:32,455] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.4958373e-15 1.0000000e+00 1.5357474e-28 6.2771655e-10 1.2060836e-14], sum to 1.0000
[2019-04-28 02:02:32,456] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3774
[2019-04-28 02:02:32,458] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.6, 89.33333333333333, 1.0, 2.0, 0.307159065139851, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485294.8178840481, 485294.8178840481, 165930.2536523599], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1572000.0000, 
sim time next is 1572600.0000, 
raw observation next is [21.6, 89.16666666666667, 1.0, 2.0, 0.3066885794240479, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 484786.7941328553, 484786.7941328558, 165898.6661183088], 
processed observation next is [1.0, 0.17391304347826086, 0.22274881516587688, 0.8916666666666667, 1.0, 1.0, 0.16468503545066016, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1346629983702376, 0.1346629983702377, 0.24760994943031167], 
reward next is 0.7524, 
noisyNet noise sample is [array([0.2846688], dtype=float32), 1.8691735]. 
=============================================
[2019-04-28 02:02:36,253] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [6.4055285e-17 1.0000000e+00 2.6882827e-31 5.2272708e-10 5.6682957e-18], sum to 1.0000
[2019-04-28 02:02:36,262] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7123
[2019-04-28 02:02:36,266] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 90.0, 1.0, 2.0, 0.3444579342784178, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 533972.0111294887, 533972.0111294887, 169421.1715931408], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1550400.0000, 
sim time next is 1551000.0000, 
raw observation next is [22.18333333333333, 90.5, 1.0, 2.0, 0.3435328483465633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 532828.480562943, 532828.480562943, 169336.7082593457], 
processed observation next is [0.0, 0.9565217391304348, 0.2503949447077408, 0.905, 1.0, 1.0, 0.20907572089947382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14800791126748417, 0.14800791126748417, 0.2527413556109637], 
reward next is 0.7473, 
noisyNet noise sample is [array([-0.15737677], dtype=float32), -0.38099048]. 
=============================================
[2019-04-28 02:02:36,275] A3C_AGENT_WORKER-Thread-12 DEBUG:Value prediction is [[79.5341  ]
 [79.547485]
 [79.551216]
 [79.54638 ]
 [79.56406 ]], R is [[79.47950745]
 [79.43184662]
 [79.38466644]
 [79.33790588]
 [79.29143524]].
[2019-04-28 02:02:37,501] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-28 02:02:37,504] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:02:37,506] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:02:37,508] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:02:37,509] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:02:37,509] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:02:37,510] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:02:37,508] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:02:37,514] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:02:37,514] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:02:37,514] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:02:37,531] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run79
[2019-04-28 02:02:37,558] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run79
[2019-04-28 02:02:37,590] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run79
[2019-04-28 02:02:37,614] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run79
[2019-04-28 02:02:37,649] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run79
[2019-04-28 02:02:42,876] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:02:42,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.36666666666667, 87.0, 1.0, 2.0, 0.2611485861593384, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425052.7378845677, 425052.7378845677, 161949.4078572044]
[2019-04-28 02:02:42,878] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:02:42,880] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9787042e-14 1.0000000e+00 7.2555266e-27 1.8815403e-09 7.7029401e-17], sampled 0.14059881605355928
[2019-04-28 02:02:47,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:02:47,868] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.80374108333334, 47.83908448, 1.0, 2.0, 0.2910111010887561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 477984.5825443338, 477984.5825443344, 165242.6664216839]
[2019-04-28 02:02:47,869] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:02:47,883] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.0042161e-14 1.0000000e+00 2.8408188e-26 2.9621963e-09 1.7871994e-16], sampled 0.3230220845266324
[2019-04-28 02:02:58,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:02:58,569] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.0, 91.0, 1.0, 2.0, 0.7091606161431733, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1130087.578516714, 1130087.578516714, 239288.0164668727]
[2019-04-28 02:02:58,570] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:02:58,592] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.9085620e-14 1.0000000e+00 1.9184288e-26 2.5996698e-09 1.4032492e-16], sampled 0.6288897491873108
[2019-04-28 02:03:35,456] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:03:35,457] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [23.494495805, 95.05924634, 1.0, 2.0, 0.44927139356139, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652355.1702575322, 652355.1702575316, 179164.1311071618]
[2019-04-28 02:03:35,457] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:03:35,490] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0399518e-14 1.0000000e+00 7.5474321e-27 1.9064081e-09 7.8949102e-17], sampled 0.1270890722353505
[2019-04-28 02:04:37,478] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:04:37,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.78534222, 89.48294296333333, 1.0, 2.0, 0.3985006243404703, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 620138.6525135669, 620138.6525135669, 176903.8362670321]
[2019-04-28 02:04:37,479] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:04:37,481] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.9627820e-14 1.0000000e+00 7.1796465e-27 1.8750095e-09 7.6552211e-17], sampled 0.6116025873401577
[2019-04-28 02:04:56,982] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:04:56,984] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [33.4, 53.5, 1.0, 2.0, 0.534628835442815, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 747077.7491291795, 747077.7491291801, 189199.6240860236]
[2019-04-28 02:04:56,986] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:04:56,989] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.4413031e-14 1.0000000e+00 1.7649738e-27 1.1758466e-09 3.2230037e-17], sampled 0.4940850646554741
[2019-04-28 02:05:01,881] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:05:01,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.43925822666667, 77.92040516833333, 1.0, 2.0, 0.4287937763853967, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 637353.3539341283, 637353.3539341277, 178011.7415906307]
[2019-04-28 02:05:01,882] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:05:01,886] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3199677e-14 1.0000000e+00 4.4585902e-27 1.6002361e-09 5.7069612e-17], sampled 0.5480603446958212
[2019-04-28 02:05:10,161] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.076409206]
[2019-04-28 02:05:10,161] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.08914389, 81.90401249, 1.0, 2.0, 0.5642606546696689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 788499.9244750348, 788499.9244750348, 194274.6436528752]
[2019-04-28 02:05:10,162] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:05:10,163] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.9019240e-14 1.0000000e+00 1.2273050e-26 2.2405895e-09 1.0653389e-16], sampled 0.8692654955742921
[2019-04-28 02:05:38,005] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:05:38,592] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:05:39,249] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:05:39,439] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:05:39,732] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:05:40,750] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 1950000, evaluation results [1950000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:05:41,930] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.7063649e-15 1.0000000e+00 1.0959259e-28 8.7925695e-10 2.0535581e-15], sum to 1.0000
[2019-04-28 02:05:41,939] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1561
[2019-04-28 02:05:41,944] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 87.33333333333334, 1.0, 2.0, 0.3288193058236363, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 512919.0675145621, 512919.0675145627, 167843.0005517647], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1578000.0000, 
sim time next is 1578600.0000, 
raw observation next is [22.5, 87.0, 1.0, 2.0, 0.3286454606919901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511899.3823875451, 511899.3823875444, 167742.5529905303], 
processed observation next is [1.0, 0.2608695652173913, 0.2654028436018958, 0.87, 1.0, 1.0, 0.19113910926745792, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14219427288542918, 0.142194272885429, 0.25036201938885116], 
reward next is 0.7496, 
noisyNet noise sample is [array([-0.1921542], dtype=float32), 0.006618706]. 
=============================================
[2019-04-28 02:05:51,340] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [3.5797737e-13 9.9999952e-01 9.0063202e-27 4.2123932e-07 3.1343140e-14], sum to 1.0000
[2019-04-28 02:05:51,346] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4438
[2019-04-28 02:05:51,351] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.43333333333333, 91.33333333333334, 1.0, 2.0, 0.4285184590579332, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 637823.9111242497, 637823.9111242504, 178076.8735267251], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1840800.0000, 
sim time next is 1841400.0000, 
raw observation next is [23.55, 91.0, 1.0, 2.0, 0.4267019877135209, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 633488.4147150277, 633488.4147150284, 177618.3907838576], 
processed observation next is [1.0, 0.30434782608695654, 0.3151658767772513, 0.91, 1.0, 1.0, 0.3092795032693023, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17596900408750768, 0.17596900408750787, 0.2651020757968024], 
reward next is 0.7349, 
noisyNet noise sample is [array([0.5620294], dtype=float32), 1.2089128]. 
=============================================
[2019-04-28 02:05:53,146] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.8861609e-13 1.0000000e+00 2.3430420e-26 5.1874185e-08 3.0074490e-17], sum to 1.0000
[2019-04-28 02:05:53,153] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3338
[2019-04-28 02:05:53,159] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.16666666666667, 1.0, 2.0, 0.886336154583685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1238831.934994465, 1238831.934994465, 266203.9479862201], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.5883646965], 
processed observation next is [1.0, 0.5652173913043478, 0.4549763033175356, 0.85, 1.0, 1.0, 0.695185568603881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2899976926052511, 0.2899976926052511, 0.34524714681297985], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.62974995], dtype=float32), -0.33284047]. 
=============================================
[2019-04-28 02:05:56,304] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.2365418e-13 9.9999917e-01 2.2614796e-24 8.3289632e-07 1.3453079e-14], sum to 1.0000
[2019-04-28 02:05:56,311] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1265
[2019-04-28 02:05:56,316] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 82.33333333333334, 1.0, 2.0, 0.944622855265667, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1351154.575088492, 1351154.575088492, 287167.0202374759], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1932000.0000, 
sim time next is 1932600.0000, 
raw observation next is [25.75, 82.16666666666667, 1.0, 2.0, 0.9591499955711602, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1370467.644170956, 1370467.644170956, 291273.3067042731], 
processed observation next is [1.0, 0.34782608695652173, 0.41943127962085314, 0.8216666666666668, 1.0, 1.0, 0.9507831271941689, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.38068545671415444, 0.38068545671415444, 0.43473627866309417], 
reward next is 0.5653, 
noisyNet noise sample is [array([0.542561], dtype=float32), 0.46549848]. 
=============================================
[2019-04-28 02:06:01,381] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3793421e-11 9.9999893e-01 1.4189877e-23 1.1204977e-06 1.8501518e-15], sum to 1.0000
[2019-04-28 02:06:01,385] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8938
[2019-04-28 02:06:01,389] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1909406.891908398 W.
[2019-04-28 02:06:01,394] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [27.4, 74.5, 1.0, 2.0, 0.4552290606537087, 1.0, 1.0, 0.4552290606537087, 1.0, 1.0, 0.7651448984945276, 6.9112, 6.9112, 170.5573041426782, 1909406.891908398, 1909406.891908398, 381327.4603807444], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 1949400.0000, 
sim time next is 1950000.0000, 
raw observation next is [27.46666666666667, 74.33333333333333, 1.0, 2.0, 0.4279826299348497, 1.0, 2.0, 0.4279826299348497, 1.0, 2.0, 0.7204167783324021, 6.9112, 6.9112, 170.5573041426782, 1795029.086039381, 1795029.086039381, 365375.9525151763], 
processed observation next is [1.0, 0.5652173913043478, 0.500789889415482, 0.7433333333333333, 1.0, 1.0, 0.3108224457046382, 1.0, 1.0, 0.3108224457046382, 1.0, 1.0, 0.6590448516248806, 0.0, 0.0, 0.8375144448122397, 0.49861919056649473, 0.49861919056649473, 0.5453372425599646], 
reward next is 0.4547, 
noisyNet noise sample is [array([0.30855212], dtype=float32), -1.4802938]. 
=============================================
[2019-04-28 02:06:01,423] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.88748 ]
 [63.166573]
 [64.071945]
 [63.65572 ]
 [63.44618 ]], R is [[62.20350266]
 [62.01232147]
 [61.39220047]
 [61.3537941 ]
 [61.3507576 ]].
[2019-04-28 02:06:01,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0510191e-12 9.9999607e-01 9.1150046e-24 3.9852366e-06 5.7358232e-14], sum to 1.0000
[2019-04-28 02:06:01,831] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5150
[2019-04-28 02:06:01,834] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.83333333333334, 1.0, 2.0, 0.554190505859246, 1.0, 1.0, 0.554190505859246, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1549399.587984747, 1549399.587984747, 317938.0900949306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1858200.0000, 
sim time next is 1858800.0000, 
raw observation next is [26.1, 85.66666666666667, 1.0, 2.0, 0.8730851263332321, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.9129565104073, 1220300.330960128, 1220300.330960128, 262638.0162152983], 
processed observation next is [1.0, 0.5217391304347826, 0.4360189573459717, 0.8566666666666667, 1.0, 1.0, 0.8470905136544965, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451521863, 0.3389723141555911, 0.3389723141555911, 0.39199703912731093], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.0352161], dtype=float32), 1.8500347]. 
=============================================
[2019-04-28 02:06:07,309] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2306805e-12 9.9999988e-01 4.0617608e-27 9.7360228e-08 5.4655280e-20], sum to 1.0000
[2019-04-28 02:06:07,318] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1365
[2019-04-28 02:06:07,321] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.7, 82.0, 1.0, 2.0, 0.55694627869216, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778275.0438719799, 778275.0438719799, 192999.4268690802], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2142000.0000, 
sim time next is 2142600.0000, 
raw observation next is [28.51666666666667, 82.83333333333334, 1.0, 2.0, 0.555518474074829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776279.1049432408, 776279.1049432408, 192751.8861035321], 
processed observation next is [0.0, 0.8260869565217391, 0.5505529225908374, 0.8283333333333335, 1.0, 1.0, 0.4644800892467819, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21563308470645576, 0.21563308470645576, 0.28768938224407775], 
reward next is 0.7123, 
noisyNet noise sample is [array([-1.4070119], dtype=float32), -0.47915903]. 
=============================================
[2019-04-28 02:06:15,727] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.7850288e-13 9.9996459e-01 2.6659015e-26 3.5433819e-05 5.2866808e-15], sum to 1.0000
[2019-04-28 02:06:15,740] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0374
[2019-04-28 02:06:15,743] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.4, 95.0, 1.0, 2.0, 0.4669897690462323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655613.5174679902, 655613.5174679902, 178984.1089243094], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2077200.0000, 
sim time next is 2077800.0000, 
raw observation next is [24.38333333333333, 95.16666666666667, 1.0, 2.0, 0.4671193770822873, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 655654.3432110912, 655654.3432110912, 178985.0317722916], 
processed observation next is [0.0, 0.043478260869565216, 0.3546603475513427, 0.9516666666666667, 1.0, 1.0, 0.35797515311118955, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18212620644752534, 0.18212620644752534, 0.2671418384661069], 
reward next is 0.7329, 
noisyNet noise sample is [array([-0.24449492], dtype=float32), -0.21351127]. 
=============================================
[2019-04-28 02:06:16,701] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.2200863e-09 9.9965596e-01 5.3142911e-19 3.4407194e-04 4.8542375e-08], sum to 1.0000
[2019-04-28 02:06:16,707] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0900
[2019-04-28 02:06:16,717] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2528844.238893589 W.
[2019-04-28 02:06:16,720] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.8, 68.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.256465432952332, 6.9112, 168.9112029122388, 2528844.238893589, 2283903.600556817, 475335.6567216447], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 2221200.0000, 
sim time next is 2221800.0000, 
raw observation next is [31.63333333333333, 68.66666666666667, 1.0, 2.0, 0.4141907566721096, 1.0, 1.0, 0.4141907566721096, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1157778.490327453, 1157778.490327453, 275770.4174049001], 
processed observation next is [1.0, 0.7391304347826086, 0.6982622432859398, 0.6866666666666668, 1.0, 1.0, 0.2942057309302525, 1.0, 0.5, 0.2942057309302525, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.32160513620207026, 0.32160513620207026, 0.4115976379177613], 
reward next is 0.5884, 
noisyNet noise sample is [array([0.48179978], dtype=float32), 0.65670425]. 
=============================================
[2019-04-28 02:06:27,271] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [7.0355194e-14 9.9999750e-01 3.7009036e-28 2.5276454e-06 6.6057920e-18], sum to 1.0000
[2019-04-28 02:06:27,290] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3501
[2019-04-28 02:06:27,293] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.5, 81.0, 1.0, 2.0, 0.5516730111975894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770903.5218070202, 770903.5218070202, 192087.7664275888], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2330400.0000, 
sim time next is 2331000.0000, 
raw observation next is [28.45, 81.0, 1.0, 2.0, 0.5492615862805145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767532.598159559, 767532.5981595595, 191673.9174232467], 
processed observation next is [1.0, 1.0, 0.54739336492891, 0.81, 1.0, 1.0, 0.4569416702174873, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21320349948876638, 0.21320349948876655, 0.28608047376603984], 
reward next is 0.7139, 
noisyNet noise sample is [array([0.0502527], dtype=float32), -0.78350216]. 
=============================================
[2019-04-28 02:06:27,320] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[80.53991 ]
 [80.49705 ]
 [80.455864]
 [80.34789 ]
 [80.32549 ]], R is [[80.50117493]
 [80.4094696 ]
 [80.31863403]
 [80.22901154]
 [80.13989258]].
[2019-04-28 02:06:31,569] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-28 02:06:31,574] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:06:31,576] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:06:31,577] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:06:31,577] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:06:31,577] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:06:31,580] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:06:31,578] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:06:31,586] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:06:31,586] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:06:31,589] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:06:31,609] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run80
[2019-04-28 02:06:31,644] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run80
[2019-04-28 02:06:31,670] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run80
[2019-04-28 02:06:31,670] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run80
[2019-04-28 02:06:31,732] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run80
[2019-04-28 02:06:34,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:06:34,541] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [24.61666666666667, 50.16666666666667, 1.0, 2.0, 0.2466949369342983, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 408299.9999893797, 408299.9999893804, 160469.1113057967]
[2019-04-28 02:06:34,542] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:06:34,546] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.48296523e-12 9.99987721e-01 6.31039667e-26 1.23087939e-05
 1.16447565e-14], sampled 0.7415591154165543
[2019-04-28 02:07:12,282] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:07:12,283] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.27005797, 68.70897510500001, 1.0, 2.0, 0.6974339200151887, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 974682.1287913386, 974682.1287913393, 220292.4626634712]
[2019-04-28 02:07:12,283] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:07:12,285] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2601338e-12 9.9998856e-01 4.4430400e-26 1.1500508e-05 9.5858515e-15], sampled 0.3216209720341149
[2019-04-28 02:07:14,489] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:07:14,490] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.34999999999999, 53.0, 1.0, 2.0, 0.5919267680147942, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 827175.7073965241, 827175.7073965241, 199251.6544817575]
[2019-04-28 02:07:14,491] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:07:14,493] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.8183970e-13 9.9999166e-01 8.4148825e-27 8.3329251e-06 3.8132911e-15], sampled 0.6771713435005906
[2019-04-28 02:07:44,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:07:44,705] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.73333333333333, 46.33333333333334, 1.0, 2.0, 0.3204887411047494, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 506663.5070713871, 506663.5070713878, 167524.4169923499]
[2019-04-28 02:07:44,707] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:07:44,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.6646041e-13 9.9999070e-01 1.5235625e-26 9.3483868e-06 5.2987107e-15], sampled 0.41422791504954604
[2019-04-28 02:07:48,110] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:07:48,111] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.13333333333333, 87.0, 1.0, 2.0, 0.6174904446249414, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 862913.6453420465, 862913.6453420465, 204043.490274068]
[2019-04-28 02:07:48,111] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:07:48,113] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.9410920e-13 9.9998963e-01 2.6670463e-26 1.0418509e-05 7.2245905e-15], sampled 0.03874743479419118
[2019-04-28 02:07:52,198] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.0799382]
[2019-04-28 02:07:52,199] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.66588473333334, 83.47351271166667, 1.0, 2.0, 0.4039141851181574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 598505.4931836578, 598505.4931836578, 174277.294257022]
[2019-04-28 02:07:52,200] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:07:52,206] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.7859492e-13 9.9999237e-01 5.5285217e-27 7.6833649e-06 3.0218271e-15], sampled 0.04830174645634666
[2019-04-28 02:07:56,121] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:07:56,259] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:07:56,440] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:07:56,448] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:07:56,577] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:07:57,590] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 1975000, evaluation results [1975000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:07:59,553] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [8.2782590e-14 1.0000000e+00 8.9565679e-29 5.7153208e-08 1.8448257e-15], sum to 1.0000
[2019-04-28 02:07:59,564] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1831
[2019-04-28 02:07:59,568] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3852189403496105, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580207.7230043663, 580207.7230043663, 172893.702197987], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2719200.0000, 
sim time next is 2719800.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3849806790613923, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 579853.4706101741, 579853.4706101741, 172862.0925145509], 
processed observation next is [0.0, 0.4782608695652174, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2590128663390268, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16107040850282614, 0.16107040850282614, 0.2580031231560461], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.70529115], dtype=float32), 0.8529315]. 
=============================================
[2019-04-28 02:08:01,806] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.6369528e-10 9.9994802e-01 3.1231836e-22 5.1928040e-05 6.8691645e-13], sum to 1.0000
[2019-04-28 02:08:01,819] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9073
[2019-04-28 02:08:01,822] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.68333333333333, 84.83333333333334, 1.0, 2.0, 0.7626785205240949, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1065908.923617191, 1065908.923617191, 234956.3210672491], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2443800.0000, 
sim time next is 2444400.0000, 
raw observation next is [27.7, 85.0, 1.0, 2.0, 0.7782386141392795, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1087666.631894783, 1087666.631894783, 238639.7128934825], 
processed observation next is [1.0, 0.30434782608695654, 0.5118483412322274, 0.85, 1.0, 1.0, 0.7328176073967223, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.30212961997077303, 0.30212961997077303, 0.35617867596042163], 
reward next is 0.6438, 
noisyNet noise sample is [array([-0.73286676], dtype=float32), 1.2396592]. 
=============================================
[2019-04-28 02:08:06,537] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5214949e-13 9.9999559e-01 4.6081952e-28 4.4059930e-06 1.0040282e-15], sum to 1.0000
[2019-04-28 02:08:06,546] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0618
[2019-04-28 02:08:06,551] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 77.5, 1.0, 2.0, 0.4997078685267168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 698263.9734801159, 698263.9734801159, 183553.20566355], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2568600.0000, 
sim time next is 2569200.0000, 
raw observation next is [28.83333333333334, 78.0, 1.0, 2.0, 0.5103374366023732, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 713122.126165316, 713122.1261653167, 185233.8617275152], 
processed observation next is [1.0, 0.7391304347826086, 0.5655608214849924, 0.78, 1.0, 1.0, 0.41004510434020863, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19808947949036554, 0.19808947949036573, 0.27646845033957496], 
reward next is 0.7235, 
noisyNet noise sample is [array([-1.0720676], dtype=float32), -0.98501563]. 
=============================================
[2019-04-28 02:08:13,430] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4507469e-14 9.9999952e-01 5.8071029e-31 4.3022195e-07 6.5931274e-15], sum to 1.0000
[2019-04-28 02:08:13,437] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9019
[2019-04-28 02:08:13,442] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3413625741345345, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525862.5531479786, 525862.5531479786, 168667.6136828982], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2872800.0000, 
sim time next is 2873400.0000, 
raw observation next is [22.0, 94.00000000000001, 1.0, 2.0, 0.3450129291095888, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 531487.0762492359, 531487.0762492366, 169121.4220597318], 
processed observation next is [1.0, 0.2608695652173913, 0.2417061611374408, 0.9400000000000002, 1.0, 1.0, 0.21085895073444433, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14763529895812108, 0.14763529895812127, 0.25242003292497284], 
reward next is 0.7476, 
noisyNet noise sample is [array([-0.10641894], dtype=float32), -1.2298368]. 
=============================================
[2019-04-28 02:08:18,124] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.6584243e-11 9.9999881e-01 2.3402981e-25 1.2173319e-06 3.5547121e-13], sum to 1.0000
[2019-04-28 02:08:18,136] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7756
[2019-04-28 02:08:18,141] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 94.00000000000001, 1.0, 2.0, 0.3937624702918596, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 587552.7706215467, 587552.7706215472, 173393.5499916874], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2736600.0000, 
sim time next is 2737200.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3932790645978489, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 586831.5939398808, 586831.5939398803, 173327.68430533], 
processed observation next is [0.0, 0.6956521739130435, 0.28909952606635075, 0.94, 1.0, 1.0, 0.2690109212022276, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16300877609441133, 0.1630087760944112, 0.258698036276612], 
reward next is 0.7413, 
noisyNet noise sample is [array([-2.308915], dtype=float32), 0.9947639]. 
=============================================
[2019-04-28 02:08:26,504] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4969951e-11 9.9999821e-01 2.7092553e-26 1.8376805e-06 2.0356256e-13], sum to 1.0000
[2019-04-28 02:08:26,512] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.3339
[2019-04-28 02:08:26,518] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.3179952030045133, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502849.9317344112, 502849.9317344112, 167239.0670701509], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2932800.0000, 
sim time next is 2933400.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3170238948433386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 501313.8800556624, 501313.8800556631, 167123.688294898], 
processed observation next is [1.0, 0.9565217391304348, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17713722270281756, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13925385557101733, 0.13925385557101752, 0.24943834073865373], 
reward next is 0.7506, 
noisyNet noise sample is [array([0.45075044], dtype=float32), -0.26492894]. 
=============================================
[2019-04-28 02:08:26,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.5623167e-15 9.9999988e-01 2.8378572e-29 1.4952285e-07 7.2912993e-17], sum to 1.0000
[2019-04-28 02:08:26,673] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6793
[2019-04-28 02:08:26,678] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.33333333333333, 98.0, 1.0, 2.0, 0.3084083045019793, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 489744.1291214715, 489744.1291214708, 166304.6750608207], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2936400.0000, 
sim time next is 2937000.0000, 
raw observation next is [20.16666666666667, 99.0, 1.0, 2.0, 0.3064968004933677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487289.087815614, 487289.0878156133, 166135.3062130659], 
processed observation next is [1.0, 1.0, 0.15481832543443946, 0.99, 1.0, 1.0, 0.16445397649803337, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13535807994878166, 0.13535807994878146, 0.24796314360159089], 
reward next is 0.7520, 
noisyNet noise sample is [array([-1.2093055], dtype=float32), 0.1766381]. 
=============================================
[2019-04-28 02:08:26,701] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[77.5839  ]
 [77.557625]
 [77.52901 ]
 [77.50144 ]
 [77.48417 ]], R is [[77.58300781]
 [77.55895996]
 [77.5348587 ]
 [77.51076508]
 [77.48671722]].
[2019-04-28 02:08:26,843] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.6394197e-13 9.9999988e-01 2.5175938e-26 1.2751832e-07 5.3925993e-16], sum to 1.0000
[2019-04-28 02:08:26,849] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.9752
[2019-04-28 02:08:26,853] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4566583325064311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646576.9607043526, 646576.9607043526, 178179.2799175668], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3217200.0000, 
sim time next is 3217800.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4563138439760834, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646089.1724248632, 646089.1724248632, 178129.0389977989], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3449564385254017, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.179469214562462, 0.179469214562462, 0.26586423731014763], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.20092419], dtype=float32), 0.08311885]. 
=============================================
[2019-04-28 02:08:30,696] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.09320177e-16 1.00000000e+00 5.43670403e-29 5.89699733e-10
 1.75115410e-17], sum to 1.0000
[2019-04-28 02:08:30,707] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1025
[2019-04-28 02:08:30,711] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4861444358929827, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679305.1273804711, 679305.1273804705, 181453.6716423996], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3196200.0000, 
sim time next is 3196800.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4865236335153158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 679835.1618634638, 679835.1618634638, 181511.5270902766], 
processed observation next is [0.0, 0.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.3813537753196576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.18884310051762884, 0.18884310051762884, 0.2709127270004128], 
reward next is 0.7291, 
noisyNet noise sample is [array([0.90993035], dtype=float32), 1.5690451]. 
=============================================
[2019-04-28 02:08:30,945] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.4272378e-12 9.9999988e-01 2.7603565e-26 7.6634954e-08 5.8652620e-14], sum to 1.0000
[2019-04-28 02:08:30,952] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5056
[2019-04-28 02:08:30,961] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3051706029095784, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485966.2386615312, 485966.2386615312, 166051.3320179218], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3016200.0000, 
sim time next is 3016800.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3058261297994068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 487009.8538300132, 487009.8538300132, 166126.951115905], 
processed observation next is [1.0, 0.9565217391304348, 0.1469194312796209, 1.0, 1.0, 1.0, 0.16364593951735762, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13528051495278146, 0.13528051495278146, 0.24795067330732087], 
reward next is 0.7520, 
noisyNet noise sample is [array([0.14536923], dtype=float32), 0.6979758]. 
=============================================
[2019-04-28 02:08:31,776] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1021980e-13 9.9999928e-01 2.1778140e-26 6.8650058e-07 1.7560053e-14], sum to 1.0000
[2019-04-28 02:08:31,785] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1714
[2019-04-28 02:08:31,789] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 89.0, 1.0, 2.0, 0.4564397554793406, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 646267.9345350269, 646267.9345350263, 178147.4588664022], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3216000.0000, 
sim time next is 3216600.0000, 
raw observation next is [25.0, 89.0, 1.0, 2.0, 0.4566762021398829, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 646602.4204941794, 646602.4204941794, 178181.9071402132], 
processed observation next is [0.0, 0.21739130434782608, 0.38388625592417064, 0.89, 1.0, 1.0, 0.3453930146263649, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17961178347060539, 0.17961178347060539, 0.26594314498539284], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.6213977], dtype=float32), -1.3089074]. 
=============================================
[2019-04-28 02:08:34,281] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.33111271e-13 9.99999881e-01 7.69141925e-27 1.16538054e-07
 1.52836579e-13], sum to 1.0000
[2019-04-28 02:08:34,286] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6525
[2019-04-28 02:08:34,290] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.33333333333334, 98.0, 1.0, 2.0, 0.3373982223982597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 522797.1969140139, 522797.1969140145, 168518.1553707968], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3050400.0000, 
sim time next is 3051000.0000, 
raw observation next is [21.5, 97.0, 1.0, 2.0, 0.3374536420638731, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 522158.545047688, 522158.5450476874, 168445.3945084656], 
processed observation next is [1.0, 0.30434782608695654, 0.21800947867298584, 0.97, 1.0, 1.0, 0.20175137598057002, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14504404029102444, 0.14504404029102427, 0.2514110365797994], 
reward next is 0.7486, 
noisyNet noise sample is [array([-1.3781669], dtype=float32), -0.0032362987]. 
=============================================
[2019-04-28 02:08:34,311] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[72.09979 ]
 [72.04872 ]
 [72.03429 ]
 [71.90394 ]
 [71.843605]], R is [[72.14893341]
 [72.17592621]
 [72.2011795 ]
 [72.22734833]
 [72.25379944]].
[2019-04-28 02:08:35,418] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.8862885e-14 1.0000000e+00 5.9023867e-29 2.4012952e-09 1.4667820e-17], sum to 1.0000
[2019-04-28 02:08:35,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4970
[2019-04-28 02:08:35,431] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3840270094326337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578417.5145680577, 578417.5145680577, 172733.6035332546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3102000.0000, 
sim time next is 3102600.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3835702333057812, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 577730.0896667422, 577730.0896667422, 172672.2091281781], 
processed observation next is [1.0, 0.9130434782608695, 0.2417061611374408, 1.0, 1.0, 1.0, 0.25731353410335084, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16048058046298397, 0.16048058046298397, 0.25771971511668373], 
reward next is 0.7423, 
noisyNet noise sample is [array([1.3557788], dtype=float32), -1.2979213]. 
=============================================
[2019-04-28 02:08:41,511] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [7.3797374e-12 9.9999917e-01 8.5101658e-22 8.6489894e-07 1.7217601e-12], sum to 1.0000
[2019-04-28 02:08:41,517] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9512
[2019-04-28 02:08:41,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2223260.080294497 W.
[2019-04-28 02:08:41,528] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.5, 66.0, 1.0, 2.0, 0.948742616024641, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.983122236538986, 6.9112, 168.9125285340643, 2223260.080294497, 2172236.099601671, 448757.7354149141], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3493800.0000, 
sim time next is 3494400.0000, 
raw observation next is [30.66666666666666, 66.0, 1.0, 2.0, 0.9800865199826109, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.984276219179018, 6.9112, 168.9124635076645, 2267129.807196765, 2215287.173534278, 458028.2492323186], 
processed observation next is [1.0, 0.43478260869565216, 0.6524486571879934, 0.66, 1.0, 1.0, 0.976007855400736, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.007307621917901841, 0.0, 0.8294375242830215, 0.6297582797768793, 0.615357548203966, 0.6836242525855502], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.2921479], dtype=float32), 0.113096036]. 
=============================================
[2019-04-28 02:08:41,647] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.0514222e-12 9.9999988e-01 5.4739050e-24 1.0316412e-07 3.5907025e-13], sum to 1.0000
[2019-04-28 02:08:41,653] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1270
[2019-04-28 02:08:41,664] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 2267023.133517226 W.
[2019-04-28 02:08:41,668] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [30.83333333333334, 66.0, 1.0, 2.0, 0.5404015633970735, 1.0, 1.0, 0.5404015633970735, 1.0, 2.0, 0.9328822564190317, 6.9112, 6.9112, 170.5573041426782, 2267023.133517226, 2267023.133517226, 443102.454429155], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3495000.0000, 
sim time next is 3495600.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 1.025596651954441, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992824980579799, 6.9112, 168.9124710147381, 2330829.714811505, 2272922.310713197, 471775.6621079605], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 1.0308393397041458, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008162498057979927, 0.0, 0.8294375611461896, 0.6474526985587513, 0.6313673085314436, 0.7041427792656126], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.1791353], dtype=float32), 0.6529307]. 
=============================================
[2019-04-28 02:08:44,899] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [8.4922256e-14 1.0000000e+00 1.2090837e-27 1.0965141e-08 1.1971140e-13], sum to 1.0000
[2019-04-28 02:08:44,901] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4583
[2019-04-28 02:08:44,904] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5192834623208975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725627.1618463501, 725627.1618463501, 186672.8798474257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3438600.0000, 
sim time next is 3439200.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5178026954821553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723557.2892918305, 723557.2892918298, 186432.7597208667], 
processed observation next is [1.0, 0.8260869565217391, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.419039392147175, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20098813591439735, 0.20098813591439715, 0.27825785032965183], 
reward next is 0.7217, 
noisyNet noise sample is [array([1.1078237], dtype=float32), 0.5631945]. 
=============================================
[2019-04-28 02:08:47,637] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.8565959e-11 9.9999988e-01 1.3002825e-21 1.3899512e-07 3.6362146e-11], sum to 1.0000
[2019-04-28 02:08:47,644] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6519
[2019-04-28 02:08:47,653] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2623097.556269967 W.
[2019-04-28 02:08:47,659] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.83333333333333, 67.5, 1.0, 2.0, 0.9377876943335242, 1.0, 1.0, 0.9377876943335242, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2623097.556269967, 2623097.556269967, 492528.1712638164], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3585000.0000, 
sim time next is 3585600.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.6034827352205602, 1.0, 2.0, 0.6034827352205602, 1.0, 1.0, 1.03, 6.931490979962581, 6.9112, 170.5573041426782, 2531921.243333834, 2517385.989002344, 489244.7817393871], 
processed observation next is [1.0, 0.5217391304347826, 0.7156398104265403, 0.67, 1.0, 1.0, 0.5222683556874219, 1.0, 1.0, 0.5222683556874219, 1.0, 0.5, 1.0365853658536586, 0.0020290979962580558, 0.0, 0.8375144448122397, 0.7033114564816206, 0.6992738858339845, 0.7302160921483389], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7318143], dtype=float32), 1.851507]. 
=============================================
[2019-04-28 02:08:51,098] A3C_AGENT_WORKER-Thread-17 INFO:Evaluating...
[2019-04-28 02:08:51,099] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:08:51,100] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:08:51,102] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:08:51,101] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:08:51,103] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:08:51,109] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:08:51,113] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:08:51,113] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:08:51,114] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:08:51,114] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:08:51,118] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run81
[2019-04-28 02:08:51,148] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run81
[2019-04-28 02:08:51,203] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run81
[2019-04-28 02:08:51,246] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run81
[2019-04-28 02:08:51,313] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run81
[2019-04-28 02:09:02,864] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.083373874]
[2019-04-28 02:09:02,866] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [25.86666666666667, 72.83333333333333, 1.0, 2.0, 0.5812225832593766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869303.1758718659, 869303.1758718659, 204413.1685832147]
[2019-04-28 02:09:02,867] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:09:02,869] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.7419108e-13 1.0000000e+00 1.2612557e-25 2.0322515e-08 3.1477835e-14], sampled 0.1731631146286028
[2019-04-28 02:09:03,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.083373874]
[2019-04-28 02:09:03,579] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.03333333333333, 74.0, 1.0, 2.0, 0.2185693694959573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 363843.1683797318, 363843.1683797311, 157542.4515035603]
[2019-04-28 02:09:03,580] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:09:03,582] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.18710028e-13 1.00000000e+00 1.23525225e-26 9.93134108e-09
 8.90349026e-15], sampled 0.3090325242193722
[2019-04-28 02:10:07,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.083373874]
[2019-04-28 02:10:07,026] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.8457863, 71.28040198666666, 1.0, 2.0, 0.5871851419145171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 820547.0615134669, 820547.0615134669, 198381.683135662]
[2019-04-28 02:10:07,026] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:10:07,027] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.3717532e-13 1.0000000e+00 5.0126434e-26 1.5291805e-08 1.9061779e-14], sampled 0.35197044020828283
[2019-04-28 02:10:13,779] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.083373874]
[2019-04-28 02:10:13,780] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.84851155, 93.60673863, 1.0, 2.0, 0.5185152569602012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724553.3344913363, 724553.3344913363, 186545.4724266656]
[2019-04-28 02:10:13,781] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:10:13,782] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2506654e-13 1.0000000e+00 1.3727663e-26 1.0260716e-08 9.4292843e-15], sampled 0.31857000082758824
[2019-04-28 02:10:29,541] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.083373874]
[2019-04-28 02:10:29,542] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.630372495, 72.32292705, 1.0, 2.0, 0.46539812686079, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 656989.8956206483, 656989.8956206489, 179214.4056957634]
[2019-04-28 02:10:29,542] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:10:29,544] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7234535e-13 1.0000000e+00 6.6313071e-26 1.6668738e-08 2.2193176e-14], sampled 0.7267376898467551
[2019-04-28 02:10:33,802] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:10:33,919] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:10:34,304] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:10:34,413] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:10:34,439] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:10:35,457] A3C_AGENT_WORKER-Thread-17 INFO:Global step: 2000000, evaluation results [2000000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:10:36,744] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.4483859e-12 9.9999976e-01 2.5980397e-23 2.7021099e-07 1.3457684e-10], sum to 1.0000
[2019-04-28 02:10:36,749] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2623
[2019-04-28 02:10:36,774] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 85.66666666666666, 1.0, 2.0, 0.5294299695632418, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739810.4464732249, 739810.4464732249, 188335.747852349], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3361200.0000, 
sim time next is 3361800.0000, 
raw observation next is [27.16666666666666, 87.33333333333334, 1.0, 2.0, 0.53204488616786, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743465.7356297971, 743465.7356297971, 188769.3043099465], 
processed observation next is [0.0, 0.9130434782608695, 0.4865718799368086, 0.8733333333333334, 1.0, 1.0, 0.4361986580335663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20651825989716585, 0.20651825989716585, 0.281745230313353], 
reward next is 0.7183, 
noisyNet noise sample is [array([-1.4974343], dtype=float32), -2.2344782]. 
=============================================
[2019-04-28 02:10:37,233] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.3778046e-11 9.9999988e-01 3.4272094e-23 1.3321153e-07 1.1904798e-11], sum to 1.0000
[2019-04-28 02:10:37,243] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1363
[2019-04-28 02:10:37,248] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.33333333333333, 79.0, 1.0, 2.0, 0.5347848399878199, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 747295.8228908767, 747295.8228908767, 189225.2850913939], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3534000.0000, 
sim time next is 3534600.0000, 
raw observation next is [28.16666666666667, 79.0, 1.0, 2.0, 0.5292705421878953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 739587.5896174719, 739587.5896174725, 188308.8996478552], 
processed observation next is [1.0, 0.9130434782608695, 0.5339652448657191, 0.79, 1.0, 1.0, 0.43285607492517497, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20544099711596442, 0.20544099711596459, 0.28105805917590326], 
reward next is 0.7189, 
noisyNet noise sample is [array([0.74863315], dtype=float32), 0.9258211]. 
=============================================
[2019-04-28 02:10:54,513] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0509095e-08 9.9774641e-01 3.6653696e-23 8.1585691e-04 1.4377601e-03], sum to 1.0000
[2019-04-28 02:10:54,521] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2920
[2019-04-28 02:10:54,525] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666666, 66.33333333333333, 1.0, 2.0, 0.5573724294602583, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 778870.7639970582, 778870.7639970582, 193075.2298733394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3606600.0000, 
sim time next is 3607200.0000, 
raw observation next is [32.0, 67.0, 1.0, 2.0, 0.5612244348728901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 784255.5333745743, 784255.5333745749, 193746.085146679], 
processed observation next is [1.0, 0.782608695652174, 0.7156398104265403, 0.67, 1.0, 1.0, 0.47135474081071094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21784875927071506, 0.21784875927071523, 0.28917326141295374], 
reward next is 0.7108, 
noisyNet noise sample is [array([0.5017742], dtype=float32), -1.3902663]. 
=============================================
[2019-04-28 02:11:11,151] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6010054e-08 9.9727339e-01 6.7465594e-21 2.7247430e-03 1.8235517e-06], sum to 1.0000
[2019-04-28 02:11:11,158] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2277
[2019-04-28 02:11:11,168] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 67.0, 1.0, 2.0, 0.6032944792888155, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843067.5828412719, 843067.5828412719, 201362.446010146], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3837600.0000, 
sim time next is 3838200.0000, 
raw observation next is [33.16666666666666, 66.33333333333334, 1.0, 2.0, 0.6152069348073272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 859721.2570272072, 859721.2570272079, 203613.6697779665], 
processed observation next is [0.0, 0.43478260869565216, 0.7709320695102682, 0.6633333333333334, 1.0, 1.0, 0.5363938973582256, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23881146028533531, 0.2388114602853355, 0.30390099966860673], 
reward next is 0.6961, 
noisyNet noise sample is [array([-0.63877803], dtype=float32), 0.27430418]. 
=============================================
[2019-04-28 02:11:11,838] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.8611195e-07 9.6936953e-01 1.5477486e-15 2.2547522e-03 2.8375013e-02], sum to 1.0000
[2019-04-28 02:11:11,844] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0120
[2019-04-28 02:11:11,853] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2574512.396169684 W.
[2019-04-28 02:11:11,858] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.66666666666667, 67.33333333333333, 1.0, 2.0, 0.9204358154295333, 1.0, 2.0, 0.9204358154295333, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2574512.396169684, 2574512.396169684, 482757.5434464164], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4013400.0000, 
sim time next is 4014000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8832910946536349, 1.0, 2.0, 0.8832910946536349, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2470513.744207249, 2470513.744207249, 462453.3850828112], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.66, 1.0, 1.0, 0.8593868610284757, 1.0, 1.0, 0.8593868610284757, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.686253817835347, 0.686253817835347, 0.6902289329594197], 
reward next is 0.3098, 
noisyNet noise sample is [array([-0.7167348], dtype=float32), 0.6314276]. 
=============================================
[2019-04-28 02:11:11,878] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[46.028908]
 [46.555767]
 [45.08203 ]
 [44.932625]
 [43.580727]], R is [[46.44882202]
 [46.26379776]
 [45.80115891]
 [45.34314728]
 [44.8897171 ]].
[2019-04-28 02:11:17,297] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0735878e-10 9.9991632e-01 2.0576132e-22 8.3397586e-05 1.8802476e-07], sum to 1.0000
[2019-04-28 02:11:17,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4982
[2019-04-28 02:11:17,307] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.83333333333334, 60.5, 1.0, 2.0, 0.5913252310518421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 826334.7751314177, 826334.7751314177, 199141.6164013741], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3930600.0000, 
sim time next is 3931200.0000, 
raw observation next is [34.0, 60.0, 1.0, 2.0, 0.5927436550983791, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 828317.6943218128, 828317.6943218135, 199402.568686852], 
processed observation next is [0.0, 0.5217391304347826, 0.8104265402843602, 0.6, 1.0, 1.0, 0.5093297049378062, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2300882484227258, 0.230088248422726, 0.2976157741594806], 
reward next is 0.7024, 
noisyNet noise sample is [array([0.7324418], dtype=float32), 0.008898949]. 
=============================================
[2019-04-28 02:11:20,300] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1768704e-05 9.9686486e-01 4.1448572e-12 2.0063499e-03 1.1170149e-03], sum to 1.0000
[2019-04-28 02:11:20,303] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8392
[2019-04-28 02:11:20,307] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1749219.847798674 W.
[2019-04-28 02:11:20,311] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.33333333333334, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.32740061447239, 6.9112, 168.9104688876174, 1749219.847798674, 1453957.155842921, 311354.7380057722], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4002000.0000, 
sim time next is 4002600.0000, 
raw observation next is [29.16666666666667, 84.0, 1.0, 2.0, 0.3653369712889767, 1.0, 1.0, 0.3653369712889767, 1.0, 1.0, 0.6344695003463281, 6.9112, 6.9112, 170.5573041426782, 1532095.13823495, 1532095.13823495, 334531.0192258341], 
processed observation next is [1.0, 0.30434782608695654, 0.581358609794629, 0.84, 1.0, 1.0, 0.23534574854093576, 1.0, 0.5, 0.23534574854093576, 1.0, 0.5, 0.5542310979833269, 0.0, 0.0, 0.8375144448122397, 0.42558198284304166, 0.42558198284304166, 0.4993000286952748], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.0057968], dtype=float32), -0.034460712]. 
=============================================
[2019-04-28 02:11:20,688] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.6195828e-10 9.9999666e-01 4.6934979e-21 3.2771491e-06 6.7846685e-08], sum to 1.0000
[2019-04-28 02:11:20,692] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2364
[2019-04-28 02:11:20,698] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.16666666666666, 55.0, 1.0, 2.0, 0.5658835847088696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 790768.6571081477, 790768.6571081472, 194564.8954435611], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4211400.0000, 
sim time next is 4212000.0000, 
raw observation next is [35.0, 56.0, 1.0, 2.0, 0.5756656329101973, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 804443.3183669555, 804443.3183669561, 196302.1556021487], 
processed observation next is [1.0, 0.782608695652174, 0.8578199052132701, 0.56, 1.0, 1.0, 0.4887537745905992, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2234564773241543, 0.22345647732415447, 0.2929882919435055], 
reward next is 0.7070, 
noisyNet noise sample is [array([-0.62981665], dtype=float32), -1.3683236]. 
=============================================
[2019-04-28 02:11:20,742] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[57.10332 ]
 [57.212822]
 [57.229477]
 [56.62438 ]
 [54.323315]], R is [[57.48294449]
 [57.61772156]
 [57.75284576]
 [57.88837814]
 [58.02129745]].
[2019-04-28 02:11:37,320] A3C_AGENT_WORKER-Thread-18 INFO:Evaluating...
[2019-04-28 02:11:37,321] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:11:37,321] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:11:37,322] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:11:37,322] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:11:37,322] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:11:37,322] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:11:37,323] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:11:37,323] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:11:37,325] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:11:37,327] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:11:37,360] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run82
[2019-04-28 02:11:37,389] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run82
[2019-04-28 02:11:37,419] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run82
[2019-04-28 02:11:37,447] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run82
[2019-04-28 02:11:37,495] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run82
[2019-04-28 02:12:15,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:15,249] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [20.08637946, 97.60785999333334, 1.0, 2.0, 0.29502082561539, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 472601.772780642, 472601.7727806427, 165131.8741281402]
[2019-04-28 02:12:15,251] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:12:15,253] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.1271078e-09 9.9999750e-01 8.9057925e-19 1.9155880e-06 6.1531421e-07], sampled 0.16062341084054443
[2019-04-28 02:12:16,126] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:16,128] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [26.2, 89.0, 1.0, 2.0, 0.5161625143128162, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721264.5859933973, 721264.5859933973, 186166.5828699093]
[2019-04-28 02:12:16,134] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:12:16,137] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3947261e-09 9.9999809e-01 3.6559364e-19 1.4449631e-06 4.5279032e-07], sampled 0.42956795059655617
[2019-04-28 02:12:28,559] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:28,559] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.484175345, 84.44243934166667, 1.0, 2.0, 0.849756656899238, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1187676.180165128, 1187676.180165128, 256492.9740569284]
[2019-04-28 02:12:28,560] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:12:28,562] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.3065655e-09 9.9999809e-01 3.1852265e-19 1.3819291e-06 4.3296794e-07], sampled 0.3523824647615301
[2019-04-28 02:12:34,804] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:34,805] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.85, 85.00000000000001, 1.0, 2.0, 0.6269216877618394, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 876098.7966764095, 876098.7966764101, 205868.3861055255]
[2019-04-28 02:12:34,807] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:12:34,810] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.4133028e-09 9.9999809e-01 3.7590173e-19 1.4570019e-06 4.5753646e-07], sampled 0.5875060100673086
[2019-04-28 02:12:39,398] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:39,398] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.09966686, 81.91798302, 1.0, 2.0, 0.5178558149241929, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723631.5416109837, 723631.5416109842, 186440.0132153803]
[2019-04-28 02:12:39,400] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:12:39,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [5.0675514e-10 9.9999905e-01 4.3200241e-20 7.3334934e-07 2.1680884e-07], sampled 0.6468564430465835
[2019-04-28 02:12:50,898] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:50,901] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.28431308, 95.55563693, 1.0, 2.0, 0.8127802254454423, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.989002891399627, 6.9112, 168.9124339804349, 2032959.987037125, 1977764.108857614, 411652.9918294303]
[2019-04-28 02:12:50,902] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:12:50,904] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.4274112e-08 9.9998665e-01 1.5148018e-16 9.7691827e-06 3.6316876e-06], sampled 0.3877672628662968
[2019-04-28 02:12:50,905] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2032959.987037125 W.
[2019-04-28 02:12:51,811] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:12:51,812] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.2, 65.0, 1.0, 2.0, 0.5458908313197897, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 762820.6478539583, 762820.6478539583, 191098.3854618844]
[2019-04-28 02:12:51,814] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:12:51,816] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.8710704e-10 9.9999881e-01 8.2111085e-20 8.9925447e-07 2.7059730e-07], sampled 0.6294251680694322
[2019-04-28 02:13:02,591] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:13:02,592] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.85, 85.0, 1.0, 2.0, 0.5897383955362768, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 824116.4255621025, 824116.4255621025, 198849.19791149]
[2019-04-28 02:13:02,593] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:13:02,596] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1462520e-10 9.9999917e-01 2.8297713e-20 6.4127136e-07 1.8729867e-07], sampled 0.6987438429318037
[2019-04-28 02:13:03,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:13:03,932] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.53333333333333, 85.0, 1.0, 2.0, 0.7536772827192297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1053322.679335393, 1053322.679335394, 232860.6820286057]
[2019-04-28 02:13:03,932] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:13:03,934] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.2246065e-09 9.9999738e-01 9.7995115e-19 1.9748236e-06 6.3728606e-07], sampled 0.8749651220813283
[2019-04-28 02:13:09,923] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.08537476]
[2019-04-28 02:13:09,924] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.95, 62.66666666666666, 1.0, 2.0, 0.4164837763575825, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 652068.1405718764, 652068.1405718764, 179889.4618195297]
[2019-04-28 02:13:09,926] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:13:09,928] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9412124e-10 9.9999917e-01 2.5419181e-20 6.1970019e-07 1.8068216e-07], sampled 0.9846112320274225
[2019-04-28 02:13:10,728] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:13:10,900] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:13:10,989] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3738 2927305459.3471 1338.0000
[2019-04-28 02:13:11,231] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:13:11,239] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:13:12,253] A3C_AGENT_WORKER-Thread-18 INFO:Global step: 2025000, evaluation results [2025000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8254.373774553256, 2927305459.3471045, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:13:16,966] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.4839950e-10 9.9999988e-01 2.5947147e-21 7.2754268e-08 5.7634481e-10], sum to 1.0000
[2019-04-28 02:13:16,980] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3998
[2019-04-28 02:13:16,988] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.0, 56.0, 1.0, 2.0, 0.5342712176942296, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 746577.8468902777, 746577.8468902771, 189140.3617933015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4552200.0000, 
sim time next is 4552800.0000, 
raw observation next is [32.66666666666666, 57.0, 1.0, 2.0, 0.5322790458376409, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 743793.0588434343, 743793.0588434337, 188808.4568185012], 
processed observation next is [0.0, 0.6956521739130435, 0.7472353870458132, 0.57, 1.0, 1.0, 0.4364807781176396, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2066091830120651, 0.20660918301206493, 0.2818036668932854], 
reward next is 0.7182, 
noisyNet noise sample is [array([0.28308252], dtype=float32), 1.3550123]. 
=============================================
[2019-04-28 02:13:19,588] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.16281808e-06 9.99755085e-01 7.39991781e-13 1.35455310e-04
 1.08167864e-04], sum to 1.0000
[2019-04-28 02:13:19,594] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.0387
[2019-04-28 02:13:19,600] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.16666666666666, 94.00000000000001, 1.0, 2.0, 0.9450630098866242, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1320965.517572562, 1320965.517572562, 282636.4787975511], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4597800.0000, 
sim time next is 4598400.0000, 
raw observation next is [27.33333333333334, 94.0, 1.0, 2.0, 0.9336194138699042, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1304960.355947803, 1304960.355947803, 279355.7588205832], 
processed observation next is [1.0, 0.21739130434782608, 0.4944707740916275, 0.94, 1.0, 1.0, 0.9200233902047039, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3624889877632786, 0.3624889877632786, 0.4169488937620645], 
reward next is 0.5831, 
noisyNet noise sample is [array([-1.3969071], dtype=float32), -0.6750048]. 
=============================================
[2019-04-28 02:13:21,224] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.0314007e-11 9.9999952e-01 1.3771998e-23 4.5258071e-07 1.1100797e-09], sum to 1.0000
[2019-04-28 02:13:21,225] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3765
[2019-04-28 02:13:21,229] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 86.5, 1.0, 2.0, 0.5992317017352357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837387.8586051587, 837387.8586051594, 200602.6772273063], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4419000.0000, 
sim time next is 4419600.0000, 
raw observation next is [29.0, 85.66666666666667, 1.0, 2.0, 0.5950050426232064, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 831479.0617492531, 831479.0617492524, 199818.7971498834], 
processed observation next is [0.0, 0.13043478260869565, 0.5734597156398105, 0.8566666666666667, 1.0, 1.0, 0.5120542682207306, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23096640604145918, 0.230966406041459, 0.2982370106714678], 
reward next is 0.7018, 
noisyNet noise sample is [array([-0.5367054], dtype=float32), -0.56242347]. 
=============================================
[2019-04-28 02:13:23,675] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1869476e-13 9.9999499e-01 1.6945587e-23 5.0054177e-06 2.8247452e-12], sum to 1.0000
[2019-04-28 02:13:23,681] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0198
[2019-04-28 02:13:23,693] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.5509894483882853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 191970.2304227675], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4480200.0000, 
sim time next is 4480800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5507253237462932, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769578.751089806, 769578.7510898054, 191924.8852004196], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.4587052093328834, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21377187530272387, 0.2137718753027237, 0.28645505253793974], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.00686914], dtype=float32), -0.59659356]. 
=============================================
[2019-04-28 02:13:26,306] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.3616062e-10 9.9998367e-01 1.6308424e-22 1.6324484e-05 2.1846134e-09], sum to 1.0000
[2019-04-28 02:13:26,323] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5158
[2019-04-28 02:13:26,327] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.0, 50.5, 1.0, 2.0, 0.5416952461503156, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756955.7034645606, 756955.7034645611, 190385.9096379447], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4543800.0000, 
sim time next is 4544400.0000, 
raw observation next is [34.0, 51.0, 1.0, 2.0, 0.5252597767891714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 733981.1239060323, 733981.1239060316, 187649.1260269228], 
processed observation next is [0.0, 0.6086956521739131, 0.8104265402843602, 0.51, 1.0, 1.0, 0.42802382745683293, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20388364552945343, 0.20388364552945323, 0.280073322428243], 
reward next is 0.7199, 
noisyNet noise sample is [array([0.05566869], dtype=float32), 0.090285756]. 
=============================================
[2019-04-28 02:13:27,348] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.6907749e-11 9.9999094e-01 5.1263643e-25 4.5427905e-06 4.4800668e-06], sum to 1.0000
[2019-04-28 02:13:27,358] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4810
[2019-04-28 02:13:27,365] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 85.66666666666667, 1.0, 2.0, 0.4953754810496339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692208.163827431, 692208.163827431, 182873.8577909603], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4506000.0000, 
sim time next is 4506600.0000, 
raw observation next is [26.0, 84.83333333333333, 1.0, 2.0, 0.49100172176835, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 686094.5597005656, 686094.5597005662, 182197.3889295147], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8483333333333333, 1.0, 1.0, 0.38674906237150597, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.190581822139046, 0.19058182213904618, 0.2719364013873354], 
reward next is 0.7281, 
noisyNet noise sample is [array([0.54514], dtype=float32), -0.29457226]. 
=============================================
[2019-04-28 02:13:32,673] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.1216262e-10 9.9927205e-01 1.2938705e-21 7.2161359e-04 6.3133161e-06], sum to 1.0000
[2019-04-28 02:13:32,677] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.8966
[2019-04-28 02:13:32,680] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4957283208447902, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 692701.3618745023, 692701.3618745023, 182929.238097918], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4915200.0000, 
sim time next is 4915800.0000, 
raw observation next is [27.0, 81.5, 1.0, 2.0, 0.4994365494971811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 697884.722825183, 697884.722825183, 183507.6516266482], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.815, 1.0, 1.0, 0.39691150541829046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1938568674514397, 0.1938568674514397, 0.2738920173532063], 
reward next is 0.7261, 
noisyNet noise sample is [array([0.2475521], dtype=float32), 0.2875006]. 
=============================================
[2019-04-28 02:13:32,810] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.19088535e-08 9.96743143e-01 1.61729212e-17 3.25310533e-03
 3.78357595e-06], sum to 1.0000
[2019-04-28 02:13:32,819] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.1341
[2019-04-28 02:13:32,823] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.485108267694878, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 677856.794554996, 677856.794554996, 181295.4846495466], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4757400.0000, 
sim time next is 4758000.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.4847111144948532, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 677301.663182531, 677301.6631825304, 181235.0571620968], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.79, 1.0, 1.0, 0.3791700174636786, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18813935088403638, 0.18813935088403624, 0.2705000853165624], 
reward next is 0.7295, 
noisyNet noise sample is [array([-0.47214374], dtype=float32), -0.27321133]. 
=============================================
[2019-04-28 02:13:32,837] A3C_AGENT_WORKER-Thread-21 DEBUG:Value prediction is [[53.52326 ]
 [53.535553]
 [53.818073]
 [54.206062]
 [54.411022]], R is [[53.66710663]
 [53.85984421]
 [54.05050659]
 [54.23904037]
 [54.42541122]].
[2019-04-28 02:13:36,584] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.4377814e-08 9.3808997e-01 1.8828996e-18 6.1567057e-02 3.4300715e-04], sum to 1.0000
[2019-04-28 02:13:36,591] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4431
[2019-04-28 02:13:36,598] A3C_AGENT_WORKER-Thread-19 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2619873.202086783 W.
[2019-04-28 02:13:36,601] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.384663059307401, 6.9112, 168.9102783090004, 2619873.202086783, 2283987.538196879, 475047.7581571369], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726800.0000, 
sim time next is 4727400.0000, 
raw observation next is [30.83333333333334, 70.0, 1.0, 2.0, 0.2734215375789188, 1.0, 1.0, 0.2734215375789188, 1.0, 2.0, 0.4748427888903773, 6.9112, 6.9112, 170.5573041426782, 1146427.926564149, 1146427.926564149, 293895.257390486], 
processed observation next is [1.0, 0.7391304347826086, 0.6603475513428123, 0.7, 1.0, 1.0, 0.12460426214327565, 1.0, 0.5, 0.12460426214327565, 1.0, 1.0, 0.35956437669558206, 0.0, 0.0, 0.8375144448122397, 0.31845220182337475, 0.31845220182337475, 0.4386496378962478], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.46988872], dtype=float32), 0.6189388]. 
=============================================
[2019-04-28 02:13:37,670] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.2571921e-08 9.9324715e-01 5.2002409e-20 6.5045902e-03 2.4820818e-04], sum to 1.0000
[2019-04-28 02:13:37,676] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.8351
[2019-04-28 02:13:37,681] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 70.0, 1.0, 2.0, 0.5299414630530386, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740525.4421315795, 740525.4421315801, 188420.6451424913], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5072400.0000, 
sim time next is 5073000.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5561092231212424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777104.9170712325, 777104.9170712325, 192852.4636751746], 
processed observation next is [0.0, 0.7391304347826086, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4651918350858342, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21586247696423125, 0.21586247696423125, 0.28783949802264863], 
reward next is 0.7122, 
noisyNet noise sample is [array([-0.412039], dtype=float32), 1.7634221]. 
=============================================
[2019-04-28 02:13:37,711] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[62.91915 ]
 [63.00749 ]
 [63.127556]
 [63.244495]
 [63.42699 ]], R is [[62.76541901]
 [62.85654068]
 [62.94720078]
 [63.03731155]
 [63.12680054]].
[2019-04-28 02:13:47,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1271742e-09 9.4244057e-01 7.4450360e-22 5.6058142e-02 1.5012970e-03], sum to 1.0000
[2019-04-28 02:13:47,132] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.2933
[2019-04-28 02:13:47,137] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.4853814110385091, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678238.5880444812, 678238.5880444812, 181337.0711617992], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4914000.0000, 
sim time next is 4914600.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.4903505526481806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 685184.3639620292, 685184.3639620292, 182097.7699068118], 
processed observation next is [1.0, 0.9130434782608695, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.3859645212628682, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19032898998945255, 0.19032898998945255, 0.2717877162788236], 
reward next is 0.7282, 
noisyNet noise sample is [array([-0.17889112], dtype=float32), -0.85156673]. 
=============================================
[2019-04-28 02:13:50,106] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.2571832e-10 9.9011821e-01 1.4576803e-20 7.8304475e-03 2.0513914e-03], sum to 1.0000
[2019-04-28 02:13:50,114] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5913
[2019-04-28 02:13:50,117] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 63.0, 1.0, 2.0, 0.5183199466982512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724280.3223779524, 724280.3223779524, 186516.61634275], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5053200.0000, 
sim time next is 5053800.0000, 
raw observation next is [31.0, 63.0, 1.0, 2.0, 0.5181273479274214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724011.1005587096, 724011.1005587096, 186485.4120597527], 
processed observation next is [0.0, 0.4782608695652174, 0.6682464454976303, 0.63, 1.0, 1.0, 0.41943053967159205, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20111419459964155, 0.20111419459964155, 0.27833643591007867], 
reward next is 0.7217, 
noisyNet noise sample is [array([-1.715194], dtype=float32), 0.7546661]. 
=============================================
[2019-04-28 02:13:52,455] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.4038018e-09 9.9564552e-01 3.5803091e-20 4.3532946e-03 1.2183764e-06], sum to 1.0000
[2019-04-28 02:13:52,462] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0994
[2019-04-28 02:13:52,467] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.7, 65.0, 1.0, 2.0, 0.6085412068360112, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 850402.50280511, 850402.5028051093, 202349.5238229676], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5337000.0000, 
sim time next is 5337600.0000, 
raw observation next is [33.46666666666667, 66.33333333333334, 1.0, 2.0, 0.612995612768629, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 856629.7976872203, 856629.7976872203, 203193.3872224316], 
processed observation next is [1.0, 0.782608695652174, 0.7851500789889416, 0.6633333333333334, 1.0, 1.0, 0.5337296539381072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23795272157978342, 0.23795272157978342, 0.30327371227228594], 
reward next is 0.6967, 
noisyNet noise sample is [array([-0.4468536], dtype=float32), 2.07883]. 
=============================================
[2019-04-28 02:13:52,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.9812490e-09 9.9606222e-01 4.0531343e-20 3.1166805e-03 8.2108739e-04], sum to 1.0000
[2019-04-28 02:13:52,994] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3853
[2019-04-28 02:13:52,997] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5220508323514502, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729495.5083918261, 729495.5083918266, 187123.1724871166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5125800.0000, 
sim time next is 5126400.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.5237423621435907, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 731860.0070143661, 731860.0070143661, 187399.706347503], 
processed observation next is [0.0, 0.34782608695652173, 0.5734597156398105, 0.74, 1.0, 1.0, 0.4261956170404707, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20329444639287947, 0.20329444639287947, 0.27970105425000447], 
reward next is 0.7203, 
noisyNet noise sample is [array([-1.0838279], dtype=float32), -0.07209542]. 
=============================================
[2019-04-28 02:13:53,681] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [7.3002774e-08 9.7250473e-01 7.3494939e-20 6.2279659e-03 2.1267140e-02], sum to 1.0000
[2019-04-28 02:13:53,700] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7317
[2019-04-28 02:13:53,704] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 66.0, 1.0, 2.0, 0.1703365742104236, 1.0, 1.0, 0.1703365742104236, 1.0, 1.0, 0.2886728792459988, 6.9112, 6.9112, 170.5573041426782, 714059.5497275637, 714059.5497275637, 261963.7887466329], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5133600.0000, 
sim time next is 5134200.0000, 
raw observation next is [30.16666666666666, 65.5, 1.0, 2.0, 0.5107319126258364, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713673.5340360918, 713673.5340360911, 185294.7223649726], 
processed observation next is [0.0, 0.43478260869565216, 0.6287519747235385, 0.655, 1.0, 1.0, 0.4105203766576342, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19824264834335883, 0.19824264834335864, 0.2765592871118994], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.14463198], dtype=float32), -0.37734085]. 
=============================================
[2019-04-28 02:13:54,899] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.6678327e-07 9.6844250e-01 2.8170183e-15 3.0686643e-02 8.7068020e-04], sum to 1.0000
[2019-04-28 02:13:54,903] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5370
[2019-04-28 02:13:54,914] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.5164342776490943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721644.4660628254, 721644.4660628254, 186210.6323128443], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5185800.0000, 
sim time next is 5186400.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5171155593673222, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 722596.7855046365, 722596.7855046365, 186320.7193050761], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41821151731002676, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20072132930684347, 0.20072132930684347, 0.2780906258284718], 
reward next is 0.7219, 
noisyNet noise sample is [array([0.793853], dtype=float32), 0.30276778]. 
=============================================
[2019-04-28 02:14:01,756] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.5734769e-05 9.7756702e-01 5.5657826e-12 2.1839548e-02 5.7763810e-04], sum to 1.0000
[2019-04-28 02:14:01,766] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5735
[2019-04-28 02:14:01,769] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.6, 88.0, 1.0, 2.0, 0.9837427349584081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9128929847002, 1375065.268637225, 1375065.268637225, 294015.5863604259], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5284800.0000, 
sim time next is 5285400.0000, 
raw observation next is [28.6, 88.00000000000001, 1.0, 2.0, 0.9223265759117579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129564945685, 1289166.283033006, 1289166.283033005, 276156.9458759727], 
processed observation next is [1.0, 0.17391304347826086, 0.5545023696682465, 0.8800000000000001, 1.0, 1.0, 0.9064175613394674, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399450744105, 0.3581017452869461, 0.3581017452869458, 0.4121745460835413], 
reward next is 0.5878, 
noisyNet noise sample is [array([0.99199575], dtype=float32), -2.651748]. 
=============================================
[2019-04-28 02:14:04,025] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 02:14:04,026] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:14:04,027] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:14:04,027] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:14:04,027] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:14:04,029] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:14:04,029] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:14:04,030] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:14:04,032] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:14:04,028] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:14:04,033] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:14:04,050] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run83
[2019-04-28 02:14:04,050] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run83
[2019-04-28 02:14:04,105] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run83
[2019-04-28 02:14:04,106] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run83
[2019-04-28 02:14:04,163] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run83
[2019-04-28 02:14:10,462] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.089215204]
[2019-04-28 02:14:10,462] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.76666666666667, 75.33333333333334, 1.0, 2.0, 0.2763211973056092, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 450511.2117588207, 450511.2117588213, 163563.4688042569]
[2019-04-28 02:14:10,463] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:14:10,466] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.7389590e-08 9.9697137e-01 1.2734048e-16 3.0182148e-03 1.0318996e-05], sampled 0.8740201389278546
[2019-04-28 02:14:18,263] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.089215204]
[2019-04-28 02:14:18,264] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.92040059, 89.75457926, 1.0, 2.0, 0.3828709547589002, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 580901.0696618807, 580901.0696618807, 173077.590119804]
[2019-04-28 02:14:18,265] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:14:18,268] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [6.5464847e-09 9.9780494e-01 1.6638578e-17 2.1896425e-03 5.4581024e-06], sampled 0.8601924005372029
[2019-04-28 02:14:25,582] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.089215204]
[2019-04-28 02:14:25,583] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.2, 87.66666666666667, 1.0, 2.0, 0.4665754608464636, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 659872.2914825506, 659872.2914825506, 179545.2313034845]
[2019-04-28 02:14:25,583] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:14:25,584] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8702828e-09 9.9854702e-01 1.2232802e-18 1.4505285e-03 2.4067815e-06], sampled 0.15547492738329338
[2019-04-28 02:15:16,994] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.089215204]
[2019-04-28 02:15:16,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.35156495333333, 65.50746421, 1.0, 2.0, 0.3332444083604488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 523884.2005265227, 523884.2005265233, 168796.9601791246]
[2019-04-28 02:15:16,997] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:15:17,000] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6241977e-09 9.9837530e-01 2.4760376e-18 1.6217621e-03 3.0050633e-06], sampled 0.04246940735086868
[2019-04-28 02:15:28,433] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7988.3762 3007841125.2970 1756.0000
[2019-04-28 02:15:28,683] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8242.4967 2927874780.2552 1331.0000
[2019-04-28 02:15:28,780] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8647.6830 2779903480.5357 930.0000
[2019-04-28 02:15:28,823] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8489.2711 2842929853.9872 1129.0000
[2019-04-28 02:15:28,977] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7876.0976 3164457801.7953 1778.0000
[2019-04-28 02:15:29,992] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2050000, evaluation results [2050000.0, 7876.097618260301, 3164457801.795336, 1778.0, 8242.49672177676, 2927874780.2551794, 1331.0, 8647.682966473527, 2779903480.5357156, 930.0, 7988.376190171547, 3007841125.2970266, 1756.0, 8489.27106703463, 2842929853.9872055, 1129.0]
[2019-04-28 02:15:31,016] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.94414778e-08 9.99552310e-01 3.17075963e-15 4.37563082e-04
 1.01063315e-05], sum to 1.0000
[2019-04-28 02:15:31,021] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4763
[2019-04-28 02:15:31,028] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2616663.35557411 W.
[2019-04-28 02:15:31,032] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [35.5, 53.16666666666667, 1.0, 2.0, 0.9354897995066915, 1.0, 2.0, 0.9354897995066915, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2616663.35557411, 2616663.35557411, 491228.6366385078], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5413800.0000, 
sim time next is 5414400.0000, 
raw observation next is [35.0, 53.0, 1.0, 2.0, 0.908407483136533, 1.0, 2.0, 0.908407483136533, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2540834.244352566, 2540834.244352567, 476101.7349686541], 
processed observation next is [1.0, 0.6956521739130435, 0.8578199052132701, 0.53, 1.0, 1.0, 0.8896475700440156, 1.0, 1.0, 0.8896475700440156, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7057872900979351, 0.7057872900979353, 0.710599604430827], 
reward next is 0.2894, 
noisyNet noise sample is [array([-0.02260696], dtype=float32), -0.28605872]. 
=============================================
[2019-04-28 02:15:37,084] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.2732059e-09 9.5220649e-01 3.3185923e-19 4.7784470e-02 8.9867453e-06], sum to 1.0000
[2019-04-28 02:15:37,091] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1879
[2019-04-28 02:15:37,097] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 81.5, 1.0, 2.0, 0.5558250749686444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 776707.7043472707, 776707.70434727, 192805.4338142631], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5599800.0000, 
sim time next is 5600400.0000, 
raw observation next is [28.63333333333333, 83.66666666666667, 1.0, 2.0, 0.5594036030144655, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781710.1650047363, 781710.1650047363, 193427.1474425982], 
processed observation next is [1.0, 0.8260869565217391, 0.55608214849921, 0.8366666666666667, 1.0, 1.0, 0.46916096748730773, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21714171250131564, 0.21714171250131564, 0.2886972349889525], 
reward next is 0.7113, 
noisyNet noise sample is [array([1.8096892], dtype=float32), 2.887827]. 
=============================================
[2019-04-28 02:15:43,899] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [8.8695371e-09 8.8248217e-01 1.9716450e-14 1.1751714e-01 7.5003919e-07], sum to 1.0000
[2019-04-28 02:15:43,899] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3974
[2019-04-28 02:15:43,915] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.78333333333333, 94.16666666666667, 1.0, 2.0, 0.7202963924824795, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1006648.174657119, 1006648.174657119, 225285.9607681066], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5548200.0000, 
sim time next is 5548800.0000, 
raw observation next is [25.96666666666667, 93.33333333333334, 1.0, 2.0, 0.6916741271159349, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 966629.0043881977, 966629.0043881984, 219057.373277575], 
processed observation next is [1.0, 0.21739130434782608, 0.42969984202211703, 0.9333333333333335, 1.0, 1.0, 0.6285230447179938, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26850805677449935, 0.26850805677449957, 0.3269513033993657], 
reward next is 0.6730, 
noisyNet noise sample is [array([0.47162914], dtype=float32), 0.9071846]. 
=============================================
[2019-04-28 02:15:46,078] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.16392209e-06 9.85608220e-01 1.30310155e-11 1.43870953e-02
 2.63202310e-06], sum to 1.0000
[2019-04-28 02:15:46,083] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7543
[2019-04-28 02:15:46,087] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.45, 78.5, 1.0, 2.0, 0.9404736971909127, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129565001065, 1314546.81640866, 1314546.81640866, 281318.459241306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5470200.0000, 
sim time next is 5470800.0000, 
raw observation next is [30.63333333333333, 77.66666666666667, 1.0, 2.0, 1.03947321444339, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9129385086566, 1453018.060712699, 1453018.060712698, 311188.6989512014], 
processed observation next is [1.0, 0.30434782608695654, 0.6508688783570299, 0.7766666666666667, 1.0, 1.0, 1.0475580896908314, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294398567553491, 0.40361612797574975, 0.4036161279757494, 0.4644607447032857], 
reward next is 0.5355, 
noisyNet noise sample is [array([0.2801757], dtype=float32), 1.030086]. 
=============================================
[2019-04-28 02:15:46,106] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.3195329e-11 9.6928304e-01 6.2544496e-20 3.0716931e-02 4.9998839e-08], sum to 1.0000
[2019-04-28 02:15:46,112] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2933
[2019-04-28 02:15:46,119] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.9, 65.0, 1.0, 2.0, 0.5598090003584952, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 782276.8755501541, 782276.8755501541, 193497.828176362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5763600.0000, 
sim time next is 5764200.0000, 
raw observation next is [31.7, 65.83333333333334, 1.0, 2.0, 0.567160298736382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 792553.4095215467, 792553.4095215461, 194787.3084200492], 
processed observation next is [0.0, 0.7391304347826086, 0.7014218009478673, 0.6583333333333334, 1.0, 1.0, 0.4785063840197373, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2201537248670963, 0.22015372486709614, 0.29072732600007345], 
reward next is 0.7093, 
noisyNet noise sample is [array([-0.76608527], dtype=float32), 1.0240045]. 
=============================================
[2019-04-28 02:15:47,378] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [3.5909520e-10 9.9657369e-01 1.1157183e-22 3.4263427e-03 2.3257178e-09], sum to 1.0000
[2019-04-28 02:15:47,379] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5528
[2019-04-28 02:15:47,393] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 93.0, 1.0, 2.0, 0.5118990840338326, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715305.0353794225, 715305.0353794225, 185481.2373962906], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5623200.0000, 
sim time next is 5623800.0000, 
raw observation next is [25.7, 92.83333333333333, 1.0, 2.0, 0.5120091590904304, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715458.9011718814, 715458.9011718814, 185498.8151693274], 
processed observation next is [0.0, 0.08695652173913043, 0.4170616113744076, 0.9283333333333332, 1.0, 1.0, 0.41205922781979565, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19873858365885594, 0.19873858365885594, 0.2768639032378021], 
reward next is 0.7231, 
noisyNet noise sample is [array([-0.18230729], dtype=float32), -0.9750267]. 
=============================================
[2019-04-28 02:15:59,104] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.4867324e-11 9.9960011e-01 2.3241950e-21 3.9995497e-04 4.8724297e-10], sum to 1.0000
[2019-04-28 02:15:59,112] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.9479
[2019-04-28 02:15:59,115] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.25, 63.0, 1.0, 2.0, 0.5272085017348208, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736705.1537804308, 736705.1537804314, 187969.3024147683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5679000.0000, 
sim time next is 5679600.0000, 
raw observation next is [31.03333333333333, 64.33333333333333, 1.0, 2.0, 0.5284826783616547, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738486.2684343568, 738486.2684343574, 188179.487977749], 
processed observation next is [0.0, 0.7391304347826086, 0.669826224328594, 0.6433333333333333, 1.0, 1.0, 0.431906841399584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2051350745650991, 0.20513507456509927, 0.2808649074294761], 
reward next is 0.7191, 
noisyNet noise sample is [array([0.04144993], dtype=float32), -1.378382]. 
=============================================
[2019-04-28 02:16:00,088] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1156512e-12 9.9582601e-01 4.1352467e-21 4.1739806e-03 6.4919424e-11], sum to 1.0000
[2019-04-28 02:16:00,098] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9318
[2019-04-28 02:16:00,110] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.31666666666667, 86.16666666666667, 1.0, 2.0, 0.5684665359561362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 794379.4362924184, 794379.4362924184, 195018.3479474342], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5861400.0000, 
sim time next is 5862000.0000, 
raw observation next is [28.23333333333333, 86.33333333333334, 1.0, 2.0, 0.5677987464327612, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 793445.9136206879, 793445.9136206872, 194900.128771883], 
processed observation next is [1.0, 0.8695652173913043, 0.537124802527646, 0.8633333333333334, 1.0, 1.0, 0.47927559811176046, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2204016426724133, 0.2204016426724131, 0.2908957145849], 
reward next is 0.7091, 
noisyNet noise sample is [array([0.04836113], dtype=float32), 0.22286235]. 
=============================================
[2019-04-28 02:16:00,161] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[68.479935]
 [68.52423 ]
 [68.6069  ]
 [68.63353 ]
 [68.97272 ]], R is [[68.65569305]
 [68.67807007]
 [68.70027161]
 [68.72276306]
 [68.7456131 ]].
[2019-04-28 02:16:01,895] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.9542675e-08 9.7487450e-01 2.6929537e-14 2.5119862e-02 5.4086313e-06], sum to 1.0000
[2019-04-28 02:16:01,900] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6289
[2019-04-28 02:16:01,906] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.3, 92.33333333333334, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.068299330485249, 6.9112, 168.9115733757019, 1565282.015902264, 1453831.256530426, 311349.7601418483], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5802000.0000, 
sim time next is 5802600.0000, 
raw observation next is [26.25, 92.66666666666666, 1.0, 2.0, 1.014961417201598, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.9127299146636, 1418731.575257973, 1418731.575257972, 303517.0126016011], 
processed observation next is [1.0, 0.13043478260869565, 0.4431279620853081, 0.9266666666666665, 1.0, 1.0, 1.0180258038573469, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294388324633487, 0.39409210423832586, 0.3940921042383256, 0.45301046656955385], 
reward next is 0.5470, 
noisyNet noise sample is [array([0.7440476], dtype=float32), -0.19372874]. 
=============================================
[2019-04-28 02:16:03,524] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [7.2734861e-12 9.9870801e-01 3.6981772e-20 1.2913959e-03 6.2119318e-07], sum to 1.0000
[2019-04-28 02:16:03,531] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.8847
[2019-04-28 02:16:03,536] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.51666666666667, 87.0, 1.0, 2.0, 0.5142109877555255, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 718536.6795371415, 718536.6795371415, 185852.2360275718], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5701800.0000, 
sim time next is 5702400.0000, 
raw observation next is [26.5, 87.0, 1.0, 2.0, 0.5142490701586254, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 718589.9122754354, 718589.9122754347, 185858.3229558025], 
processed observation next is [0.0, 0.0, 0.4549763033175356, 0.87, 1.0, 1.0, 0.41475791585376554, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.19960830896539872, 0.19960830896539852, 0.2774004820235858], 
reward next is 0.7226, 
noisyNet noise sample is [array([1.4056646], dtype=float32), 1.2545345]. 
=============================================
[2019-04-28 02:16:04,738] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.2727386e-10 9.9722517e-01 2.1388871e-17 2.7701873e-03 4.6619134e-06], sum to 1.0000
[2019-04-28 02:16:04,759] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1763
[2019-04-28 02:16:04,763] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2479679.324154906 W.
[2019-04-28 02:16:04,766] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.05, 77.0, 1.0, 2.0, 0.8865648469686056, 1.0, 1.0, 0.8865648469686056, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2479679.324154906, 2479679.324154906, 464217.2105652618], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5931000.0000, 
sim time next is 5931600.0000, 
raw observation next is [30.1, 77.33333333333334, 1.0, 2.0, 0.8150005237937724, 1.0, 2.0, 0.8150005237937724, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2279334.799620064, 2279334.799620064, 427245.0721116763], 
processed observation next is [1.0, 0.6521739130434783, 0.6255924170616115, 0.7733333333333334, 1.0, 1.0, 0.777109064811774, 1.0, 1.0, 0.777109064811774, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6331485554500178, 0.6331485554500178, 0.6376792121069795], 
reward next is 0.3623, 
noisyNet noise sample is [array([0.7244425], dtype=float32), 1.3212188]. 
=============================================
[2019-04-28 02:16:06,109] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.81269443e-08 9.86716270e-01 8.52499221e-17 1.32834865e-02
 2.32636481e-07], sum to 1.0000
[2019-04-28 02:16:06,114] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.5329
[2019-04-28 02:16:06,118] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 92.16666666666667, 1.0, 2.0, 0.5297481658025573, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740255.2398190206, 740255.2398190206, 188388.2603690794], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6052200.0000, 
sim time next is 6052800.0000, 
raw observation next is [26.33333333333334, 92.33333333333334, 1.0, 2.0, 0.528868062447663, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 739024.9802409156, 739024.9802409156, 188242.7509832972], 
processed observation next is [1.0, 0.043478260869565216, 0.44707740916271754, 0.9233333333333335, 1.0, 1.0, 0.4323711595754976, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20528471673358764, 0.20528471673358764, 0.28095932982581673], 
reward next is 0.7190, 
noisyNet noise sample is [array([-0.41707444], dtype=float32), -1.5188154]. 
=============================================
[2019-04-28 02:16:06,241] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.6523289e-11 9.7280687e-01 2.6465940e-22 2.7189521e-02 3.5523606e-06], sum to 1.0000
[2019-04-28 02:16:06,259] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.0854
[2019-04-28 02:16:06,263] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.15, 62.0, 1.0, 2.0, 0.5198103373176135, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 726363.6488525588, 726363.6488525594, 186758.3632864149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5740200.0000, 
sim time next is 5740800.0000, 
raw observation next is [31.3, 61.33333333333334, 1.0, 2.0, 0.5196873384969101, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 726191.7161211448, 726191.7161211448, 186738.4154674287], 
processed observation next is [0.0, 0.43478260869565216, 0.6824644549763034, 0.6133333333333334, 1.0, 1.0, 0.42131004638181935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20171992114476245, 0.20171992114476245, 0.2787140529364608], 
reward next is 0.7213, 
noisyNet noise sample is [array([2.6710832], dtype=float32), 1.4541718]. 
=============================================
[2019-04-28 02:16:06,497] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.5082514e-12 9.9942386e-01 1.1629003e-22 5.7612034e-04 8.3236346e-10], sum to 1.0000
[2019-04-28 02:16:06,504] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6350
[2019-04-28 02:16:06,542] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.2, 57.5, 1.0, 2.0, 0.5215649810163725, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728816.3638848484, 728816.3638848478, 187044.1820415007], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5743800.0000, 
sim time next is 5744400.0000, 
raw observation next is [32.4, 56.66666666666667, 1.0, 2.0, 0.5211209160873153, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 728195.6306460764, 728195.6306460764, 186971.8663684944], 
processed observation next is [0.0, 0.4782608695652174, 0.7345971563981042, 0.5666666666666668, 1.0, 1.0, 0.4230372482979702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20227656406835456, 0.20227656406835456, 0.2790624871171558], 
reward next is 0.7209, 
noisyNet noise sample is [array([0.28869838], dtype=float32), 2.27518]. 
=============================================
[2019-04-28 02:16:12,246] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.9771148e-11 9.9982303e-01 1.6505601e-18 1.7701331e-04 1.9734458e-08], sum to 1.0000
[2019-04-28 02:16:12,261] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3071
[2019-04-28 02:16:12,270] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.55, 84.5, 1.0, 2.0, 0.750790183519081, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049285.739953067, 1049285.739953067, 232192.690745666], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5989800.0000, 
sim time next is 5990400.0000, 
raw observation next is [28.7, 84.0, 1.0, 2.0, 0.751059915969747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1049662.897646974, 1049662.897646974, 232255.1746011922], 
processed observation next is [1.0, 0.34782608695652173, 0.5592417061611374, 0.84, 1.0, 1.0, 0.7000721879153577, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29157302712415945, 0.29157302712415945, 0.34664951433013763], 
reward next is 0.6534, 
noisyNet noise sample is [array([-0.4871177], dtype=float32), -0.02565061]. 
=============================================
[2019-04-28 02:16:12,584] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [2.2870134e-10 9.9914443e-01 5.7743075e-19 8.5559284e-04 7.7877127e-09], sum to 1.0000
[2019-04-28 02:16:12,589] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.1597
[2019-04-28 02:16:12,593] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.4, 85.5, 1.0, 2.0, 0.5302006209803654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740887.708495714, 740887.708495714, 188463.3732536661], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6208200.0000, 
sim time next is 6208800.0000, 
raw observation next is [27.36666666666667, 85.66666666666666, 1.0, 2.0, 0.5305047447765191, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 741312.8309831853, 741312.8309831853, 188513.7156267586], 
processed observation next is [1.0, 0.8695652173913043, 0.49605055292259104, 0.8566666666666666, 1.0, 1.0, 0.43434306599580613, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2059202308286626, 0.2059202308286626, 0.2813637546668039], 
reward next is 0.7186, 
noisyNet noise sample is [array([-0.6591368], dtype=float32), 0.2568887]. 
=============================================
[2019-04-28 02:16:19,078] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [2.3681541e-06 9.8402888e-01 3.8655229e-14 1.5455426e-02 5.1327929e-04], sum to 1.0000
[2019-04-28 02:16:19,085] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6485
[2019-04-28 02:16:19,090] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 87.5, 1.0, 2.0, 0.7724593662494886, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1079585.446619537, 1079585.446619537, 237264.1381100006], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5986200.0000, 
sim time next is 5986800.0000, 
raw observation next is [27.8, 87.0, 1.0, 2.0, 0.7406611294908749, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1035122.725842849, 1035122.72584285, 229868.1972868438], 
processed observation next is [1.0, 0.30434782608695654, 0.5165876777251186, 0.87, 1.0, 1.0, 0.6875435295070782, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2875340905119025, 0.2875340905119028, 0.3430868616221549], 
reward next is 0.6569, 
noisyNet noise sample is [array([0.96211666], dtype=float32), 0.20492835]. 
=============================================
[2019-04-28 02:16:20,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.6805908e-08 9.8990005e-01 2.0207454e-18 1.0008697e-02 9.1259193e-05], sum to 1.0000
[2019-04-28 02:16:20,015] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2976
[2019-04-28 02:16:20,023] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 91.00000000000001, 1.0, 2.0, 0.5419584445019228, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757323.6234659345, 757323.6234659345, 190431.1994464738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5962200.0000, 
sim time next is 5962800.0000, 
raw observation next is [26.83333333333333, 91.0, 1.0, 2.0, 0.5408372192221654, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 755756.2847229472, 755756.2847229472, 190241.7570227987], 
processed observation next is [1.0, 0.0, 0.470774091627172, 0.91, 1.0, 1.0, 0.4467918303881511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20993230131192978, 0.20993230131192978, 0.2839429209295503], 
reward next is 0.7161, 
noisyNet noise sample is [array([-0.12560323], dtype=float32), -0.48235723]. 
=============================================
[2019-04-28 02:16:20,619] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.4729286e-08 9.9171805e-01 2.3914279e-15 8.1004798e-03 1.8146548e-04], sum to 1.0000
[2019-04-28 02:16:20,646] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.5703
[2019-04-28 02:16:20,691] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.23333333333333, 91.83333333333333, 1.0, 2.0, 0.7524190764490069, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1051563.36746616, 1051563.367466161, 232565.8088089804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5971800.0000, 
sim time next is 5972400.0000, 
raw observation next is [26.2, 92.0, 1.0, 2.0, 0.7353930691574068, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1027756.701699502, 1027756.701699502, 228669.3756156479], 
processed observation next is [1.0, 0.13043478260869565, 0.44075829383886256, 0.92, 1.0, 1.0, 0.6811964688643455, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2854879726943061, 0.2854879726943061, 0.34129757554574314], 
reward next is 0.6587, 
noisyNet noise sample is [array([1.6087654], dtype=float32), -1.342089]. 
=============================================
[2019-04-28 02:16:24,944] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [5.7614113e-10 9.8604047e-01 7.8764907e-18 1.9777130e-04 1.3761724e-02], sum to 1.0000
[2019-04-28 02:16:24,947] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2030
[2019-04-28 02:16:24,954] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.76666666666667, 89.66666666666667, 1.0, 2.0, 0.674263405035288, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 942286.3631138303, 942286.3631138309, 215387.915329292], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6069000.0000, 
sim time next is 6069600.0000, 
raw observation next is [26.9, 89.0, 1.0, 2.0, 0.6834732248677882, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 955162.9186698908, 955162.9186698901, 217318.3066567561], 
processed observation next is [1.0, 0.2608695652173913, 0.4739336492890995, 0.89, 1.0, 1.0, 0.6186424395997447, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.26532303296385856, 0.2653230329638584, 0.32435568157724787], 
reward next is 0.6756, 
noisyNet noise sample is [array([0.39871833], dtype=float32), 0.14961615]. 
=============================================
[2019-04-28 02:16:27,332] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.3767088e-10 9.9988461e-01 4.1446447e-22 5.7396908e-05 5.7877649e-05], sum to 1.0000
[2019-04-28 02:16:27,342] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4411
[2019-04-28 02:16:27,354] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2184061.332276591 W.
[2019-04-28 02:16:27,362] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [30.63333333333334, 66.83333333333333, 1.0, 2.0, 0.5206438024102037, 1.0, 2.0, 0.5206438024102037, 1.0, 2.0, 0.8994672765959182, 6.9112, 6.9112, 170.5573041426782, 2184061.332276591, 2184061.332276591, 428823.688955909], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6105000.0000, 
sim time next is 6105600.0000, 
raw observation next is [30.6, 67.0, 1.0, 2.0, 0.5217641021046939, 1.0, 2.0, 0.5217641021046939, 1.0, 2.0, 0.9014379021489517, 6.911199999999999, 6.9112, 170.5573041426782, 2188765.706276424, 2188765.706276425, 429633.7551128984], 
processed observation next is [1.0, 0.6956521739130435, 0.6492890995260664, 0.67, 1.0, 1.0, 0.4238121712104746, 1.0, 1.0, 0.4238121712104746, 1.0, 1.0, 0.8798023196938434, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6079904739656734, 0.6079904739656736, 0.6412444106162662], 
reward next is 0.3588, 
noisyNet noise sample is [array([-0.12017667], dtype=float32), 0.3998023]. 
=============================================
[2019-04-28 02:16:28,694] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [9.0593089e-11 9.9971026e-01 4.3451507e-20 2.2944868e-04 6.0298775e-05], sum to 1.0000
[2019-04-28 02:16:28,711] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1008
[2019-04-28 02:16:28,718] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 72.33333333333334, 1.0, 2.0, 0.5419008612283716, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757243.1288852462, 757243.1288852456, 190422.0233671023], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6258000.0000, 
sim time next is 6258600.0000, 
raw observation next is [30.15, 71.5, 1.0, 2.0, 0.5420248353274145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 757416.4299662584, 757416.4299662578, 190442.9864727354], 
processed observation next is [0.0, 0.43478260869565216, 0.6279620853080569, 0.715, 1.0, 1.0, 0.4482226931655596, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2103934527684051, 0.21039345276840493, 0.2842432633921424], 
reward next is 0.7158, 
noisyNet noise sample is [array([-1.1108066], dtype=float32), 0.9900267]. 
=============================================
[2019-04-28 02:16:29,565] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-28 02:16:29,566] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:16:29,567] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:16:29,569] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:16:29,567] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:16:29,571] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:16:29,570] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:16:29,572] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:16:29,572] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:16:29,571] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:16:29,576] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:16:29,596] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run84
[2019-04-28 02:16:29,623] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run84
[2019-04-28 02:16:29,670] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run84
[2019-04-28 02:16:29,671] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run84
[2019-04-28 02:16:29,739] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run84
[2019-04-28 02:17:03,297] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.09243261]
[2019-04-28 02:17:03,297] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.94032238666667, 93.90452550666667, 1.0, 2.0, 0.3481003396114692, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 539709.1612731036, 539709.1612731043, 169890.8665408454]
[2019-04-28 02:17:03,298] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:17:03,300] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.8414401e-11 9.9982905e-01 3.1062395e-21 3.7542428e-05 1.3331402e-04], sampled 0.3927476672328334
[2019-04-28 02:17:07,728] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.09243261]
[2019-04-28 02:17:07,741] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 79.0, 1.0, 2.0, 0.5762410856777003, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805247.7690442551, 805247.7690442551, 196402.7410488893]
[2019-04-28 02:17:07,741] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:17:07,743] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5197763e-11 9.9982017e-01 4.0422333e-21 3.9749088e-05 1.4010906e-04], sampled 0.4977982804305713
[2019-04-28 02:17:08,285] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.09243261]
[2019-04-28 02:17:08,286] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.25, 51.0, 1.0, 2.0, 0.9469914034967003, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005990522102191, 6.9112, 168.912315943553, 2220809.087020241, 2153561.683225567, 447422.3337400532]
[2019-04-28 02:17:08,288] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:17:08,290] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3219805e-09 9.9936968e-01 2.3610024e-18 1.5833743e-04 4.7205505e-04], sampled 0.3702206218422741
[2019-04-28 02:17:08,291] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2220809.087020241 W.
[2019-04-28 02:17:25,148] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.09243261]
[2019-04-28 02:17:25,149] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [27.817983845, 86.63950963166667, 1.0, 2.0, 0.5486029887651311, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 766611.948531126, 766611.9485311266, 191561.8347293017]
[2019-04-28 02:17:25,150] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:17:25,153] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [8.4511717e-11 9.9982738e-01 1.0365092e-20 4.0313007e-05 1.3232214e-04], sampled 0.9422188442633189
[2019-04-28 02:18:04,981] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0471 3007549592.4283 1766.0000
[2019-04-28 02:18:05,054] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.7242 3163941635.4908 1774.0000
[2019-04-28 02:18:05,332] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8656.2015 2779371379.8359 933.0000
[2019-04-28 02:18:05,521] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.0907 2842527404.8204 1129.0000
[2019-04-28 02:18:05,608] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8253.5013 2927347979.7433 1337.0000
[2019-04-28 02:18:06,624] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2075000, evaluation results [2075000.0, 7884.724222045157, 3163941635.4908485, 1774.0, 8253.501299703892, 2927347979.743267, 1337.0, 8656.201515387625, 2779371379.8358746, 933.0, 7999.047104540193, 3007549592.4283476, 1766.0, 8496.09066676572, 2842527404.820404, 1129.0]
[2019-04-28 02:18:11,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.7452848e-08 9.9552673e-01 1.1588453e-18 4.1706557e-03 3.0261552e-04], sum to 1.0000
[2019-04-28 02:18:11,627] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9943
[2019-04-28 02:18:11,635] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.0, 64.5, 1.0, 2.0, 0.5319731121381953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 743365.4053665273, 743365.4053665273, 188757.3890295669], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6363000.0000, 
sim time next is 6363600.0000, 
raw observation next is [31.0, 64.0, 1.0, 2.0, 0.5296479157171096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 740115.104356034, 740115.104356034, 188371.7588722059], 
processed observation next is [0.0, 0.6521739130434783, 0.6682464454976303, 0.64, 1.0, 1.0, 0.4333107418278428, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2055875289877872, 0.2055875289877872, 0.2811518789137401], 
reward next is 0.7188, 
noisyNet noise sample is [array([1.3819873], dtype=float32), 0.4943274]. 
=============================================
[2019-04-28 02:18:11,696] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3995543e-11 9.9995673e-01 2.3207707e-21 3.5716694e-05 7.5606940e-06], sum to 1.0000
[2019-04-28 02:18:11,714] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9385
[2019-04-28 02:18:11,726] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.66666666666667, 63.00000000000001, 1.0, 2.0, 0.5446081712998979, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761027.6335776714, 761027.633577672, 190880.4809788294], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6351600.0000, 
sim time next is 6352200.0000, 
raw observation next is [31.5, 63.0, 1.0, 2.0, 0.5382711482541161, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 752169.2333426882, 752169.2333426889, 189809.7982757424], 
processed observation next is [0.0, 0.5217391304347826, 0.6919431279620853, 0.63, 1.0, 1.0, 0.443700178619417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20893589815074673, 0.20893589815074692, 0.28329820638170505], 
reward next is 0.7167, 
noisyNet noise sample is [array([1.3269509], dtype=float32), 1.3113496]. 
=============================================
[2019-04-28 02:18:13,009] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.1712875e-10 9.8866767e-01 5.8474251e-20 1.5140850e-04 1.1180850e-02], sum to 1.0000
[2019-04-28 02:18:13,013] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6090
[2019-04-28 02:18:13,017] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 77.0, 1.0, 2.0, 0.4936730051491451, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 689828.4530765115, 689828.4530765109, 182613.5669999954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6196800.0000, 
sim time next is 6197400.0000, 
raw observation next is [28.75, 77.5, 1.0, 2.0, 0.4875681866175854, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 681295.2176484088, 681295.2176484081, 181674.3817396295], 
processed observation next is [1.0, 0.7391304347826086, 0.561611374407583, 0.775, 1.0, 1.0, 0.38261227303323536, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18924867156900246, 0.18924867156900227, 0.27115579364123804], 
reward next is 0.7288, 
noisyNet noise sample is [array([-1.5143452], dtype=float32), -1.1324596]. 
=============================================
[2019-04-28 02:18:14,062] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.9335870e-11 9.9999678e-01 4.0173696e-19 1.9940710e-08 3.2664097e-06], sum to 1.0000
[2019-04-28 02:18:14,072] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0390
[2019-04-28 02:18:14,079] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.56666666666667, 82.5, 1.0, 2.0, 0.7477932858261217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1045095.289048516, 1045095.289048517, 231498.6661047348], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6419400.0000, 
sim time next is 6420000.0000, 
raw observation next is [27.63333333333333, 82.0, 1.0, 2.0, 0.7601599449829616, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1062387.236601977, 1062387.236601977, 234365.63250699], 
processed observation next is [1.0, 0.30434782608695654, 0.5086887835703, 0.82, 1.0, 1.0, 0.7110360782927247, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.29510756572277136, 0.29510756572277136, 0.34979945150297015], 
reward next is 0.6502, 
noisyNet noise sample is [array([-0.48903307], dtype=float32), -0.29209766]. 
=============================================
[2019-04-28 02:18:14,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[52.845737]
 [53.061897]
 [52.76674 ]
 [53.082035]
 [53.222515]], R is [[53.21569824]
 [53.33802032]
 [53.44139099]
 [53.53673935]
 [53.6301651 ]].
[2019-04-28 02:18:21,065] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.6161739e-10 9.9100596e-01 2.7122616e-21 3.4892789e-04 8.6450670e-03], sum to 1.0000
[2019-04-28 02:18:21,072] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.6036
[2019-04-28 02:18:21,085] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 87.66666666666667, 1.0, 2.0, 0.5242918556501751, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 732628.1155994362, 732628.1155994355, 187489.5026628953], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6326400.0000, 
sim time next is 6327000.0000, 
raw observation next is [26.9, 87.0, 1.0, 2.0, 0.5242368870535852, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 732551.2778130334, 732551.2778130334, 187480.5095918376], 
processed observation next is [0.0, 0.21739130434782608, 0.4739336492890995, 0.87, 1.0, 1.0, 0.42679143018504234, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.20348646605917595, 0.20348646605917595, 0.27982165610722026], 
reward next is 0.7202, 
noisyNet noise sample is [array([-1.3154268], dtype=float32), 0.36637422]. 
=============================================
[2019-04-28 02:18:21,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[73.65617 ]
 [73.668304]
 [73.69415 ]
 [73.68938 ]
 [73.69072 ]], R is [[73.62140656]
 [73.60535431]
 [73.58950806]
 [73.57386017]
 [73.55835724]].
[2019-04-28 02:18:26,258] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.6249450e-08 9.9998212e-01 3.3540195e-17 9.3066268e-07 1.6868114e-05], sum to 1.0000
[2019-04-28 02:18:26,265] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9991
[2019-04-28 02:18:26,272] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.26666666666667, 89.33333333333334, 1.0, 2.0, 0.6995581569466889, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 977652.1718730864, 977652.1718730864, 220748.2624320532], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6589200.0000, 
sim time next is 6589800.0000, 
raw observation next is [26.35, 89.0, 1.0, 2.0, 0.7170657950208603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1002131.132092961, 1002131.132092961, 224570.4285035005], 
processed observation next is [1.0, 0.2608695652173913, 0.4478672985781992, 0.89, 1.0, 1.0, 0.6591154156877835, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2783697589147114, 0.2783697589147114, 0.3351797440350754], 
reward next is 0.6648, 
noisyNet noise sample is [array([-0.9313444], dtype=float32), 0.9970624]. 
=============================================
[2019-04-28 02:18:31,909] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.5909645e-11 9.9782240e-01 2.9149643e-21 2.1052401e-06 2.1754738e-03], sum to 1.0000
[2019-04-28 02:18:31,928] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3635
[2019-04-28 02:18:31,939] A3C_AGENT_WORKER-Thread-9 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1820210.308239483 W.
[2019-04-28 02:18:31,943] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.6, 67.0, 1.0, 2.0, 0.6607533841195143, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.972007153254998, 6.9112, 168.9125941775023, 1820210.308239483, 1777071.713435911, 377524.356226396], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 6714000.0000, 
sim time next is 6714600.0000, 
raw observation next is [29.46666666666667, 67.0, 1.0, 2.0, 0.2126435214693482, 1.0, 1.0, 0.2126435214693482, 1.0, 2.0, 0.3598358537256529, 6.9112, 6.9112, 170.5573041426782, 891486.1252701706, 891486.1252701706, 272879.2483458088], 
processed observation next is [1.0, 0.7391304347826086, 0.5955766192733019, 0.67, 1.0, 1.0, 0.05137773671005806, 1.0, 0.5, 0.05137773671005806, 1.0, 1.0, 0.2193120167386011, 0.0, 0.0, 0.8375144448122397, 0.24763503479726962, 0.24763503479726962, 0.4072824602176251], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.58542234], dtype=float32), -0.7061548]. 
=============================================
[2019-04-28 02:18:41,931] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3324602e-12 1.0000000e+00 1.1579597e-23 2.5141428e-08 1.4714935e-08], sum to 1.0000
[2019-04-28 02:18:41,938] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4117
[2019-04-28 02:18:41,942] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.56666666666667, 48.66666666666667, 1.0, 2.0, 0.3283553708158922, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 515343.4530676911, 515343.4530676917, 168111.5912440516], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6866400.0000, 
sim time next is 6867000.0000, 
raw observation next is [28.65, 47.5, 1.0, 2.0, 0.3227754337636225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508441.1670018883, 508441.1670018889, 167622.8225025015], 
processed observation next is [0.0, 0.4782608695652174, 0.5568720379146919, 0.475, 1.0, 1.0, 0.18406678766701506, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14123365750052452, 0.1412336575005247, 0.2501833171679127], 
reward next is 0.7498, 
noisyNet noise sample is [array([1.2940633], dtype=float32), -1.8377125]. 
=============================================
[2019-04-28 02:18:41,962] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[70.19128 ]
 [70.20297 ]
 [70.230286]
 [70.222206]
 [70.23492 ]], R is [[70.23652649]
 [70.2832489 ]
 [70.3292923 ]
 [70.37388611]
 [70.41765594]].
[2019-04-28 02:18:59,385] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.2733670e-11 9.9999940e-01 2.2910948e-20 1.1849384e-07 4.8310568e-07], sum to 1.0000
[2019-04-28 02:18:59,385] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9023
[2019-04-28 02:18:59,517] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.33333333333334, 85.66666666666667, 1.0, 2.0, 0.6654128531676959, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 947290.1127677842, 947290.1127677842, 215851.9789133081], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7012200.0000, 
sim time next is 7012800.0000, 
raw observation next is [25.3, 86.0, 1.0, 2.0, 0.6550394125009066, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 931995.9920461454, 931995.9920461461, 213622.7756947928], 
processed observation next is [1.0, 0.17391304347826086, 0.39810426540284366, 0.86, 1.0, 1.0, 0.5843848343384417, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2588877755683737, 0.2588877755683739, 0.3188399637235713], 
reward next is 0.6812, 
noisyNet noise sample is [array([0.9401091], dtype=float32), 2.41427]. 
=============================================
[2019-04-28 02:19:30,277] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [6.5982747e-10 9.9998772e-01 8.3507367e-19 1.0979096e-05 1.2620490e-06], sum to 1.0000
[2019-04-28 02:19:30,288] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8079
[2019-04-28 02:19:30,289] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1668358.513533214 W.
[2019-04-28 02:19:30,394] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [28.35, 68.5, 1.0, 2.0, 0.5967067380017694, 1.0, 2.0, 0.5967067380017694, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1668358.513533214, 1668358.513533214, 333026.0325813065], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7122600.0000, 
sim time next is 7123200.0000, 
raw observation next is [28.43333333333334, 68.0, 1.0, 2.0, 0.4292523773603487, 1.0, 2.0, 0.4292523773603487, 1.0, 1.0, 0.7209423319999235, 6.9112, 6.9112, 170.5573041426782, 1800359.088196093, 1800359.088196093, 365845.8721950959], 
processed observation next is [1.0, 0.43478260869565216, 0.5466034755134285, 0.68, 1.0, 1.0, 0.3123522618799382, 1.0, 1.0, 0.3123522618799382, 1.0, 0.5, 0.659685770731614, 0.0, 0.0, 0.8375144448122397, 0.500099746721137, 0.500099746721137, 0.546038615216561], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.8532965], dtype=float32), 0.5289288]. 
=============================================
[2019-04-28 02:19:30,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.2095262e-13 9.9998379e-01 4.4422438e-24 1.5723814e-05 4.6312849e-07], sum to 1.0000
[2019-04-28 02:19:30,747] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5686
[2019-04-28 02:19:30,750] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.53333333333333, 66.66666666666667, 1.0, 2.0, 0.4144269176033511, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 608711.9609456295, 608711.9609456295, 175072.9004370057], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6990000.0000, 
sim time next is 6990600.0000, 
raw observation next is [27.41666666666666, 67.83333333333333, 1.0, 2.0, 0.4183159584869597, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 612532.4056348385, 612532.4056348385, 175380.4542522756], 
processed observation next is [0.0, 0.9130434782608695, 0.4984202211690361, 0.6783333333333332, 1.0, 1.0, 0.2991758535987466, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17014789045412182, 0.17014789045412182, 0.2617618720183218], 
reward next is 0.7382, 
noisyNet noise sample is [array([0.1574084], dtype=float32), -0.25567332]. 
=============================================
[2019-04-28 02:19:48,336] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [6.8291767e-14 1.0000000e+00 3.3567956e-23 4.3969450e-10 1.9016400e-09], sum to 1.0000
[2019-04-28 02:19:48,337] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.2438
[2019-04-28 02:19:48,428] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 92.66666666666666, 1.0, 2.0, 0.3879564240198343, 1.0, 2.0, 0.3879564240198343, 1.0, 1.0, 0.6490953164707318, 6.9112, 6.9112, 170.5573041426782, 1627025.193676746, 1627025.193676746, 342884.7100643004], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7220400.0000, 
sim time next is 7221000.0000, 
raw observation next is [24.75, 90.83333333333334, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 7.545809971692827, 6.9112, 168.9097964597072, 1904865.750322922, 1454660.017493191, 311442.1791373783], 
processed observation next is [1.0, 0.5652173913043478, 0.3720379146919432, 0.9083333333333334, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.20481927710843376, 0.0, 0.5, -0.2195121951219512, 0.06346099716928269, 0.0, 0.8294244278565713, 0.5291293750897005, 0.4040722270814419, 0.4648390733393706], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.7495662], dtype=float32), 1.7006285]. 
=============================================
[2019-04-28 02:19:49,020] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[61.64835 ]
 [62.257214]
 [63.36791 ]
 [63.21758 ]
 [62.875313]], R is [[61.04580688]
 [60.92358017]
 [60.81566238]
 [60.20750809]
 [60.21219254]].
[2019-04-28 02:19:54,743] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.5649604e-14 1.0000000e+00 3.6443630e-24 9.5721731e-10 9.9255174e-11], sum to 1.0000
[2019-04-28 02:19:54,744] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0974
[2019-04-28 02:19:54,759] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.46666666666667, 89.33333333333333, 1.0, 2.0, 0.5318350685422644, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 859660.5847458986, 859660.5847458986, 201295.2674595128], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7404000.0000, 
sim time next is 7404600.0000, 
raw observation next is [20.43333333333333, 89.16666666666667, 1.0, 2.0, 0.537329516017936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 869358.8287553352, 869358.8287553352, 202392.1847829527], 
processed observation next is [1.0, 0.6956521739130435, 0.1674565560821484, 0.8916666666666667, 1.0, 1.0, 0.44256568194932044, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24148856354314865, 0.24148856354314865, 0.30207788773575034], 
reward next is 0.6979, 
noisyNet noise sample is [array([0.24403138], dtype=float32), -0.3776554]. 
=============================================
[2019-04-28 02:19:58,415] A3C_AGENT_WORKER-Thread-16 INFO:Evaluating...
[2019-04-28 02:19:58,523] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:19:58,523] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:19:58,629] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:19:58,629] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:19:58,634] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run85
[2019-04-28 02:19:58,973] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:19:59,030] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run85
[2019-04-28 02:19:59,246] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:19:59,251] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run85
[2019-04-28 02:19:59,411] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:19:59,413] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:19:59,415] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run85
[2019-04-28 02:19:59,586] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:19:59,587] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:19:59,607] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run85
[2019-04-28 02:21:09,115] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:09,118] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [35.36666666666667, 52.0, 1.0, 2.0, 0.8045835777174596, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005981814510227, 6.9112, 168.9123160023244, 2021488.332707667, 1954247.106331602, 408964.8572614786]
[2019-04-28 02:21:09,120] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:21:09,124] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.0369340e-12 9.9999917e-01 2.0131934e-21 6.2327831e-07 2.7535165e-07], sampled 0.7662273112105225
[2019-04-28 02:21:09,124] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2021488.332707667 W.
[2019-04-28 02:21:10,717] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:10,718] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.16666666666667, 94.00000000000001, 1.0, 2.0, 0.4934154542202681, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 689468.4503746668, 689468.4503746675, 182570.842177091]
[2019-04-28 02:21:10,719] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:21:10,723] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.9466910e-13 9.9999988e-01 1.9742230e-24 1.1072476e-07 4.4213710e-08], sampled 0.5399035275126246
[2019-04-28 02:21:20,068] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:20,071] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [35.0, 48.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 7.190946214026279, 6.9112, 168.9117132339697, 1652350.976554784, 1453890.844970823, 311351.0342132759]
[2019-04-28 02:21:20,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:21:20,075] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.7754344e-12 9.9999857e-01 1.3321015e-21 9.5946029e-07 4.4247545e-07], sampled 0.6448207573463353
[2019-04-28 02:21:28,371] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:28,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 68.0, 1.0, 2.0, 0.9106114270254874, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.997159136116382, 6.9112, 168.9123723012221, 2169889.668476625, 2108907.509746622, 437350.0266311595]
[2019-04-28 02:21:28,377] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:21:28,380] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8961911e-10 9.9998641e-01 5.1403387e-19 8.8633724e-06 4.7876406e-06], sampled 0.10443334238695057
[2019-04-28 02:21:28,382] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2169889.668476625 W.
[2019-04-28 02:21:43,633] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:43,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.33333333333334, 55.66666666666667, 1.0, 2.0, 0.7434970186622347, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005977999550653, 6.9112, 168.9123160290228, 1935998.267116756, 1868759.747183422, 394353.9359259122]
[2019-04-28 02:21:43,634] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:21:43,639] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6968278e-10 9.9998343e-01 1.0016503e-18 1.0685303e-05 5.8251489e-06], sampled 0.7340676497419216
[2019-04-28 02:21:43,645] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1935998.267116756 W.
[2019-04-28 02:21:47,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:47,394] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 88.0, 1.0, 2.0, 0.7102669169436563, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 992624.9589280143, 992624.9589280137, 223076.7136654406]
[2019-04-28 02:21:47,394] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:21:47,396] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1145879e-12 9.9999893e-01 2.5324578e-23 7.6015118e-07 3.5621963e-07], sampled 0.45476706176481285
[2019-04-28 02:21:59,156] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.096435435]
[2019-04-28 02:21:59,157] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [34.2, 57.0, 1.0, 2.0, 0.9225721165317103, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005989184833456, 6.9112, 168.9123159542443, 2186631.690156106, 2119385.235058044, 440382.8458595654]
[2019-04-28 02:21:59,160] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:21:59,163] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.9513162e-10 9.9998271e-01 2.3347224e-18 1.1215247e-05 6.0495604e-06], sampled 0.6086006250372604
[2019-04-28 02:21:59,163] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2186631.690156106 W.
[2019-04-28 02:22:02,335] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:22:02,355] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:22:02,403] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:22:02,577] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:22:02,793] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:22:03,807] A3C_AGENT_WORKER-Thread-16 INFO:Global step: 2100000, evaluation results [2100000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:22:07,702] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.62088086e-11 9.99996424e-01 1.28010305e-23 3.32030640e-06
 2.69643493e-07], sum to 1.0000
[2019-04-28 02:22:07,704] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.8576
[2019-04-28 02:22:07,711] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.7, 78.0, 1.0, 2.0, 0.3764203951355077, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570445.5446265574, 570445.5446265574, 172133.074487773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7340400.0000, 
sim time next is 7341000.0000, 
raw observation next is [24.71666666666667, 77.66666666666667, 1.0, 2.0, 0.3752182289529033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 569105.6076972077, 569105.6076972077, 172030.0300545673], 
processed observation next is [1.0, 1.0, 0.3704581358609796, 0.7766666666666667, 1.0, 1.0, 0.24725087825651, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15808489102700216, 0.15808489102700216, 0.25676123888741387], 
reward next is 0.7432, 
noisyNet noise sample is [array([-1.1750758], dtype=float32), 0.5797364]. 
=============================================
[2019-04-28 02:22:07,722] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[76.22519]
 [76.14928]
 [76.16931]
 [76.17978]
 [76.19393]], R is [[76.20118713]
 [76.18226624]
 [76.16342926]
 [76.14470673]
 [76.12609863]].
[2019-04-28 02:22:15,868] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.7862443e-17 1.0000000e+00 6.9246907e-29 4.9605253e-10 1.8618173e-12], sum to 1.0000
[2019-04-28 02:22:15,877] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5649
[2019-04-28 02:22:15,884] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 89.83333333333333, 1.0, 2.0, 0.5581405899949702, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 899485.7371513245, 899485.7371513245, 206269.7489054998], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7402200.0000, 
sim time next is 7402800.0000, 
raw observation next is [20.53333333333333, 89.66666666666667, 1.0, 2.0, 0.5466142941974074, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 881804.8437253291, 881804.8437253286, 204049.5315351732], 
processed observation next is [1.0, 0.6956521739130435, 0.17219589257503945, 0.8966666666666667, 1.0, 1.0, 0.4537521616836233, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24494578992370253, 0.2449457899237024, 0.30455153960473613], 
reward next is 0.6954, 
noisyNet noise sample is [array([-1.2047478], dtype=float32), -0.2640646]. 
=============================================
[2019-04-28 02:22:20,490] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [4.9678965e-15 9.9999964e-01 1.8994824e-24 3.9197306e-07 1.1274713e-08], sum to 1.0000
[2019-04-28 02:22:20,496] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1270
[2019-04-28 02:22:20,501] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 63.0, 1.0, 2.0, 0.4435716753645846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 631728.9054534489, 631728.9054534482, 176767.8847282683], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7563600.0000, 
sim time next is 7564200.0000, 
raw observation next is [29.0, 62.33333333333333, 1.0, 2.0, 0.4392080589186769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 628318.064137571, 628318.0641375717, 176505.2226159334], 
processed observation next is [0.0, 0.5652173913043478, 0.5734597156398105, 0.6233333333333333, 1.0, 1.0, 0.324347058938165, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17453279559376972, 0.1745327955937699, 0.26344063077004987], 
reward next is 0.7366, 
noisyNet noise sample is [array([0.90650773], dtype=float32), 2.3789845]. 
=============================================
[2019-04-28 02:22:20,875] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0962737e-10 9.9999917e-01 1.8963451e-18 8.1556459e-07 8.2937418e-10], sum to 1.0000
[2019-04-28 02:22:20,876] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0057
[2019-04-28 02:22:20,884] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2089257.009672272 W.
[2019-04-28 02:22:20,888] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 71.0, 1.0, 2.0, 0.7470990763711874, 1.0, 1.0, 0.7470990763711874, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2089257.009672272, 2089257.009672272, 394937.0624008045], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7725600.0000, 
sim time next is 7726200.0000, 
raw observation next is [30.1, 70.0, 1.0, 2.0, 0.7152690461424228, 1.0, 2.0, 0.7152690461424228, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2000161.469631732, 2000161.469631732, 380728.27033363], 
processed observation next is [1.0, 0.43478260869565216, 0.6255924170616115, 0.7, 1.0, 1.0, 0.6569506580029191, 1.0, 1.0, 0.6569506580029191, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5556004082310366, 0.5556004082310366, 0.5682511497516866], 
reward next is 0.4317, 
noisyNet noise sample is [array([-0.03873137], dtype=float32), 0.552835]. 
=============================================
[2019-04-28 02:22:21,116] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.9572198e-15 1.0000000e+00 3.7167058e-26 5.2042903e-10 1.8938646e-13], sum to 1.0000
[2019-04-28 02:22:21,123] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8762
[2019-04-28 02:22:21,127] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 87.0, 1.0, 2.0, 0.2817976931909142, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456471.5194258331, 456471.5194258331, 164021.1520503433], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7407600.0000, 
sim time next is 7408200.0000, 
raw observation next is [20.73333333333333, 86.5, 1.0, 2.0, 0.2814994678713811, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 455956.5888777226, 455956.5888777226, 163986.9808121778], 
processed observation next is [1.0, 0.7391304347826086, 0.18167456556082143, 0.865, 1.0, 1.0, 0.13433670827877237, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12665460802158962, 0.12665460802158962, 0.24475668777936985], 
reward next is 0.7552, 
noisyNet noise sample is [array([-0.0420457], dtype=float32), -2.0272155]. 
=============================================
[2019-04-28 02:22:29,790] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [2.4791410e-07 8.6787975e-01 1.4412612e-16 1.3093722e-01 1.1828060e-03], sum to 1.0000
[2019-04-28 02:22:29,799] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.8628
[2019-04-28 02:22:29,805] A3C_AGENT_WORKER-Thread-14 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2321546.198034929 W.
[2019-04-28 02:22:29,809] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.43333333333333, 63.33333333333333, 1.0, 2.0, 1.01896422817002, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.992122755916869, 6.9112, 168.9124755554426, 2321546.198034929, 2264136.973306043, 469714.5300819277], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7732200.0000, 
sim time next is 7732800.0000, 
raw observation next is [31.5, 63.0, 1.0, 2.0, 0.5543385037994636, 1.0, 1.0, 0.5543385037994636, 1.0, 2.0, 0.9589014641639809, 6.9112, 6.9112, 170.5573041426782, 2325543.992072732, 2325543.992072732, 453986.7396834992], 
processed observation next is [1.0, 0.5217391304347826, 0.6919431279620853, 0.63, 1.0, 1.0, 0.46305843831260673, 1.0, 0.5, 0.46305843831260673, 1.0, 1.0, 0.9498798343463181, 0.0, 0.0, 0.8375144448122397, 0.6459844422424256, 0.6459844422424256, 0.6775921487813421], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.31324044], dtype=float32), -0.079223685]. 
=============================================
[2019-04-28 02:22:34,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:34,438] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:34,482] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run11
[2019-04-28 02:22:36,200] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:36,201] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:36,232] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run11
[2019-04-28 02:22:36,292] A3C_AGENT_WORKER-Thread-18 INFO:Local step 132500, global step 2115668: loss 3.4939
[2019-04-28 02:22:36,293] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 132500, global step 2115668: learning rate 0.0000
[2019-04-28 02:22:37,424] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.6769152e-06 9.2285478e-01 7.4478748e-13 5.2758183e-02 2.4385380e-02], sum to 1.0000
[2019-04-28 02:22:37,430] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3319
[2019-04-28 02:22:37,435] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.25, 75.0, 1.0, 2.0, 0.8198653639557903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1274433.207144546, 1274433.207144545, 265693.4114227602], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 34200.0000, 
sim time next is 34800.0000, 
raw observation next is [24.46666666666667, 74.0, 1.0, 2.0, 0.8061059699703014, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1250934.472634344, 1250934.472634345, 261632.8874783517], 
processed observation next is [1.0, 0.391304347826087, 0.3586097946287521, 0.74, 1.0, 1.0, 0.7663927349039775, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.34748179795398443, 0.34748179795398476, 0.3904968469826145], 
reward next is 0.6095, 
noisyNet noise sample is [array([-0.5933481], dtype=float32), -0.6369629]. 
=============================================
[2019-04-28 02:22:37,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:37,825] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:37,858] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run11
[2019-04-28 02:22:38,025] A3C_AGENT_WORKER-Thread-20 INFO:Local step 132500, global step 2116472: loss 0.3947
[2019-04-28 02:22:38,026] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 132500, global step 2116472: learning rate 0.0000
[2019-04-28 02:22:38,623] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [9.4329433e-10 9.9654239e-01 8.5200017e-18 3.4552026e-03 2.3430171e-06], sum to 1.0000
[2019-04-28 02:22:38,629] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1493
[2019-04-28 02:22:38,634] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.66666666666667, 1.0, 2.0, 0.5095597186048926, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 712035.0144809416, 712035.0144809422, 185107.3575488465], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7672800.0000, 
sim time next is 7673400.0000, 
raw observation next is [26.55, 86.5, 1.0, 2.0, 0.5077434834523744, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709496.244525124, 709496.244525124, 184818.1776674127], 
processed observation next is [1.0, 0.8260869565217391, 0.4573459715639811, 0.865, 1.0, 1.0, 0.40691985958117394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19708229014586776, 0.19708229014586776, 0.2758480263692727], 
reward next is 0.7242, 
noisyNet noise sample is [array([0.9570001], dtype=float32), 0.97623515]. 
=============================================
[2019-04-28 02:22:39,643] A3C_AGENT_WORKER-Thread-2 INFO:Local step 132500, global step 2117294: loss 9.3515
[2019-04-28 02:22:39,645] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 132500, global step 2117294: learning rate 0.0000
[2019-04-28 02:22:40,366] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.2741546e-10 9.9972445e-01 1.2007038e-19 2.7523388e-04 3.9947670e-07], sum to 1.0000
[2019-04-28 02:22:40,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6864
[2019-04-28 02:22:40,379] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.93333333333334, 69.0, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.975154783110121, 6.9112, 168.9124909156053, 1581166.673275207, 1535795.074046596, 324178.2410048766], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 58800.0000, 
sim time next is 59400.0000, 
raw observation next is [26.85, 69.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.288648322707923, 6.9112, 168.9107443645294, 1803600.544380913, 1535829.220360412, 324159.2842055288], 
processed observation next is [1.0, 0.6956521739130435, 0.4715639810426541, 0.695, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.03774483227079228, 0.0, 0.829429082503201, 0.5010001512169203, 0.42661922787789225, 0.48381982717243105], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.19389023], dtype=float32), 2.4290135]. 
=============================================
[2019-04-28 02:22:40,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:40,424] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:40,464] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run11
[2019-04-28 02:22:41,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:41,749] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:41,759] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run11
[2019-04-28 02:22:42,019] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:42,020] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:42,048] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run11
[2019-04-28 02:22:42,342] A3C_AGENT_WORKER-Thread-11 INFO:Local step 132500, global step 2118515: loss 1.6995
[2019-04-28 02:22:42,344] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 132500, global step 2118516: learning rate 0.0000
[2019-04-28 02:22:42,425] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:42,426] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:42,457] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run11
[2019-04-28 02:22:43,504] A3C_AGENT_WORKER-Thread-9 INFO:Local step 132500, global step 2118998: loss 1.0114
[2019-04-28 02:22:43,507] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 132500, global step 2118998: learning rate 0.0000
[2019-04-28 02:22:43,828] A3C_AGENT_WORKER-Thread-10 INFO:Local step 132500, global step 2119159: loss 0.1547
[2019-04-28 02:22:43,830] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 132500, global step 2119161: learning rate 0.0000
[2019-04-28 02:22:44,178] A3C_AGENT_WORKER-Thread-14 INFO:Local step 132500, global step 2119341: loss 0.9322
[2019-04-28 02:22:44,181] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 132500, global step 2119342: learning rate 0.0000
[2019-04-28 02:22:44,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:44,325] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:44,357] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:44,358] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:44,374] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run11
[2019-04-28 02:22:44,423] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run11
[2019-04-28 02:22:44,870] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:44,872] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:44,895] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run11
[2019-04-28 02:22:46,138] A3C_AGENT_WORKER-Thread-12 INFO:Local step 132500, global step 2120153: loss 1.9731
[2019-04-28 02:22:46,143] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 132500, global step 2120154: learning rate 0.0000
[2019-04-28 02:22:46,291] A3C_AGENT_WORKER-Thread-13 INFO:Local step 132500, global step 2120231: loss 0.2195
[2019-04-28 02:22:46,294] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 132500, global step 2120231: learning rate 0.0000
[2019-04-28 02:22:46,655] A3C_AGENT_WORKER-Thread-21 INFO:Local step 132500, global step 2120424: loss 4.1923
[2019-04-28 02:22:46,657] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 132500, global step 2120424: learning rate 0.0000
[2019-04-28 02:22:46,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:46,658] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:46,689] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run11
[2019-04-28 02:22:48,316] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.0693469e-12 9.9999952e-01 9.4675186e-23 4.8556950e-07 9.4406679e-12], sum to 1.0000
[2019-04-28 02:22:48,322] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3959
[2019-04-28 02:22:48,326] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.8, 95.0, 1.0, 2.0, 0.7321840579104695, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1095165.905318714, 1095165.905318714, 237395.6173608549], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 126000.0000, 
sim time next is 126600.0000, 
raw observation next is [22.8, 95.16666666666667, 1.0, 2.0, 0.9119344527621884, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1363298.675074063, 1363298.675074063, 286022.8737582292], 
processed observation next is [1.0, 0.4782608695652174, 0.2796208530805688, 0.9516666666666667, 1.0, 1.0, 0.8938969310387812, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37869407640946195, 0.37869407640946195, 0.42689981157944656], 
reward next is 0.5731, 
noisyNet noise sample is [array([-0.36628014], dtype=float32), 1.7475628]. 
=============================================
[2019-04-28 02:22:48,514] A3C_AGENT_WORKER-Thread-16 INFO:Local step 132500, global step 2121355: loss 0.0814
[2019-04-28 02:22:48,516] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 132500, global step 2121356: learning rate 0.0000
[2019-04-28 02:22:50,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:50,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:50,856] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run11
[2019-04-28 02:22:51,288] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133000, global step 2122677: loss 0.0034
[2019-04-28 02:22:51,290] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133000, global step 2122677: learning rate 0.0000
[2019-04-28 02:22:51,596] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:51,597] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:51,629] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run11
[2019-04-28 02:22:52,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:52,409] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:52,433] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run11
[2019-04-28 02:22:52,634] A3C_AGENT_WORKER-Thread-19 INFO:Local step 132500, global step 2123247: loss 0.0218
[2019-04-28 02:22:52,638] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 132500, global step 2123248: learning rate 0.0000
[2019-04-28 02:22:53,352] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133000, global step 2123598: loss 0.0191
[2019-04-28 02:22:53,354] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133000, global step 2123598: learning rate 0.0000
[2019-04-28 02:22:53,440] A3C_AGENT_WORKER-Thread-3 INFO:Local step 132500, global step 2123637: loss 7.9867
[2019-04-28 02:22:53,443] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 132500, global step 2123639: learning rate 0.0000
[2019-04-28 02:22:53,527] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.5568871e-13 9.9979299e-01 1.5592507e-22 2.0698634e-04 9.6578384e-11], sum to 1.0000
[2019-04-28 02:22:53,533] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0055
[2019-04-28 02:22:53,540] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 85.0, 1.0, 2.0, 0.3107733800305792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 501288.572051924, 501288.572051924, 167191.282826826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 13800.0000, 
sim time next is 14400.0000, 
raw observation next is [21.2, 85.0, 1.0, 2.0, 0.3098742515674188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 499538.4435189567, 499538.4435189574, 167065.2828317584], 
processed observation next is [1.0, 0.17391304347826086, 0.20379146919431282, 0.85, 1.0, 1.0, 0.16852319465954074, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13876067875526577, 0.13876067875526596, 0.24935116840560956], 
reward next is 0.7506, 
noisyNet noise sample is [array([-0.6919588], dtype=float32), 0.1466399]. 
=============================================
[2019-04-28 02:22:53,792] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:53,792] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:53,828] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run11
[2019-04-28 02:22:54,197] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:22:54,199] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:54,219] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run11
[2019-04-28 02:22:54,365] A3C_AGENT_WORKER-Thread-22 INFO:Local step 132500, global step 2124047: loss 5.0934
[2019-04-28 02:22:54,366] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 132500, global step 2124048: learning rate 0.0000
[2019-04-28 02:22:54,934] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133000, global step 2124299: loss 0.0016
[2019-04-28 02:22:54,935] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133000, global step 2124300: learning rate 0.0000
[2019-04-28 02:22:55,646] A3C_AGENT_WORKER-Thread-17 INFO:Local step 132500, global step 2124653: loss 2.4458
[2019-04-28 02:22:55,647] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 132500, global step 2124653: learning rate 0.0000
[2019-04-28 02:22:56,058] A3C_AGENT_WORKER-Thread-15 INFO:Local step 132500, global step 2124881: loss 0.3708
[2019-04-28 02:22:56,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 132500, global step 2124881: learning rate 0.0000
[2019-04-28 02:22:56,305] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-28 02:22:56,306] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:22:56,307] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:56,307] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:22:56,309] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:56,310] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:22:56,311] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:22:56,312] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:56,312] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:56,314] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:22:56,315] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:22:56,332] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run86
[2019-04-28 02:22:56,363] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run86
[2019-04-28 02:22:56,364] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run86
[2019-04-28 02:22:56,437] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run86
[2019-04-28 02:22:56,464] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run86
[2019-04-28 02:23:10,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10099604]
[2019-04-28 02:23:10,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.6, 72.33333333333333, 1.0, 2.0, 0.2630574634704714, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 432440.6115371984, 432440.6115371984, 162223.5975661894]
[2019-04-28 02:23:10,063] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:23:10,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0988171e-13 9.9982399e-01 9.9423280e-24 1.7603638e-04 9.3697272e-10], sampled 0.8956149327161913
[2019-04-28 02:23:10,160] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10099604]
[2019-04-28 02:23:10,161] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [20.85, 76.66666666666667, 1.0, 2.0, 0.2517965152325217, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 414630.3420875883, 414630.3420875883, 161073.7574260815]
[2019-04-28 02:23:10,162] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:23:10,165] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3310998e-13 9.9983704e-01 6.7823145e-24 1.6293638e-04 7.9024498e-10], sampled 0.45414563499116734
[2019-04-28 02:23:25,113] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10099604]
[2019-04-28 02:23:25,116] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.87731577166667, 88.79622776166666, 1.0, 2.0, 0.2913877388198523, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 474132.3442445168, 474132.3442445168, 165182.850297352]
[2019-04-28 02:23:25,116] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:23:25,120] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.5248082e-13 9.9982673e-01 7.4277557e-24 1.7332354e-04 8.7293239e-10], sampled 0.2000510572994989
[2019-04-28 02:24:11,262] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10099604]
[2019-04-28 02:24:11,262] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.26932564, 80.08346711, 1.0, 2.0, 0.3295101718810783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 525060.2834689014, 525060.2834689014, 168987.8634816813]
[2019-04-28 02:24:11,263] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:24:11,266] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.4469988e-13 9.9982822e-01 7.1124883e-24 1.7176551e-04 8.5576557e-10], sampled 0.009390155614943718
[2019-04-28 02:24:18,618] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10099604]
[2019-04-28 02:24:18,619] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [34.03333333333333, 64.16666666666667, 1.0, 2.0, 0.7878516310555281, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1101108.754388537, 1101108.754388537, 240957.9258264571]
[2019-04-28 02:24:18,621] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:24:18,622] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2007009e-12 9.9978703e-01 7.9861558e-23 2.1295434e-04 1.7480654e-09], sampled 0.28120674815723135
[2019-04-28 02:24:24,816] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7996.6495 3007751160.0815 1766.0000
[2019-04-28 02:24:24,903] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8496.7789 2842491947.2566 1131.0000
[2019-04-28 02:24:24,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8660.8152 2779117188.9940 931.0000
[2019-04-28 02:24:25,156] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7883.3253 3164109857.4416 1778.0000
[2019-04-28 02:24:25,204] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.6227 2927220960.6340 1337.0000
[2019-04-28 02:24:26,217] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2125000, evaluation results [2125000.0, 7883.32531379043, 3164109857.4415865, 1778.0, 8255.622686253282, 2927220960.6340084, 1337.0, 8660.81515993254, 2779117188.994003, 931.0, 7996.649548676481, 3007751160.0815425, 1766.0, 8496.778948090465, 2842491947.2566366, 1131.0]
[2019-04-28 02:24:28,359] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133000, global step 2125865: loss 0.0003
[2019-04-28 02:24:28,360] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133000, global step 2125865: learning rate 0.0000
[2019-04-28 02:24:29,301] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133000, global step 2126246: loss 0.0001
[2019-04-28 02:24:29,303] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133000, global step 2126246: learning rate 0.0000
[2019-04-28 02:24:29,448] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133000, global step 2126309: loss 0.0134
[2019-04-28 02:24:29,450] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133000, global step 2126310: learning rate 0.0000
[2019-04-28 02:24:30,667] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133000, global step 2126821: loss 0.0001
[2019-04-28 02:24:30,668] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133000, global step 2126821: learning rate 0.0000
[2019-04-28 02:24:32,701] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133000, global step 2127672: loss 0.0515
[2019-04-28 02:24:32,709] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133000, global step 2127672: learning rate 0.0000
[2019-04-28 02:24:33,081] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133000, global step 2127828: loss 0.0360
[2019-04-28 02:24:33,085] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133000, global step 2127830: learning rate 0.0000
[2019-04-28 02:24:33,258] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.944839e-13 9.999831e-01 1.854706e-25 1.698314e-05 8.191030e-10], sum to 1.0000
[2019-04-28 02:24:33,266] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.0973
[2019-04-28 02:24:33,272] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 78.83333333333334, 1.0, 2.0, 0.3013051075397071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 479424.4883262644, 479424.4883262644, 165574.4798935821], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 319800.0000, 
sim time next is 320400.0000, 
raw observation next is [22.6, 79.0, 1.0, 2.0, 0.2993775856819734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 476788.9494197393, 476788.9494197399, 165393.446471702], 
processed observation next is [0.0, 0.7391304347826086, 0.27014218009478685, 0.79, 1.0, 1.0, 0.15587660925538965, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13244137483881646, 0.13244137483881663, 0.2468558902562716], 
reward next is 0.7531, 
noisyNet noise sample is [array([-0.40288323], dtype=float32), -0.43479007]. 
=============================================
[2019-04-28 02:24:33,466] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5718340e-13 9.9991822e-01 2.8987814e-24 8.1739141e-05 3.0302350e-09], sum to 1.0000
[2019-04-28 02:24:33,472] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0861
[2019-04-28 02:24:33,476] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.4, 96.0, 1.0, 2.0, 0.3819850059386324, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 576836.0541452381, 576836.0541452381, 172637.409321775], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 157800.0000, 
sim time next is 158400.0000, 
raw observation next is [22.4, 96.0, 1.0, 2.0, 0.3803188587064071, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 574320.0518680543, 574320.0518680537, 172414.0207110183], 
processed observation next is [1.0, 0.8695652173913043, 0.2606635071090047, 0.96, 1.0, 1.0, 0.2533962153089242, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1595333477411262, 0.15953334774112604, 0.25733435927017656], 
reward next is 0.7427, 
noisyNet noise sample is [array([-0.52427465], dtype=float32), 0.77051944]. 
=============================================
[2019-04-28 02:24:33,583] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.6287057e-12 9.9996066e-01 1.3754978e-23 3.9376293e-05 1.1002671e-08], sum to 1.0000
[2019-04-28 02:24:33,591] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3524
[2019-04-28 02:24:33,598] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.25, 79.0, 1.0, 2.0, 0.3317843451468782, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 541376.6389771572, 541376.6389771566, 170069.3708773235], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 376200.0000, 
sim time next is 376800.0000, 
raw observation next is [21.33333333333333, 78.66666666666667, 1.0, 2.0, 0.3587676915180267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585056.6533695918, 585056.6533695924, 173609.863459545], 
processed observation next is [1.0, 0.34782608695652173, 0.21011058451816728, 0.7866666666666667, 1.0, 1.0, 0.2274309536361767, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16251573704710884, 0.162515737047109, 0.25911919919335075], 
reward next is 0.7409, 
noisyNet noise sample is [array([0.27922592], dtype=float32), -1.2870618]. 
=============================================
[2019-04-28 02:24:33,845] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133000, global step 2128116: loss 0.0009
[2019-04-28 02:24:33,847] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133000, global step 2128117: learning rate 0.0000
[2019-04-28 02:24:34,261] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2710410e-12 9.9988651e-01 3.5608702e-23 1.1350204e-04 6.0679954e-09], sum to 1.0000
[2019-04-28 02:24:34,284] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9912
[2019-04-28 02:24:34,298] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.6, 96.0, 1.0, 2.0, 0.3176503916619723, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 503813.350235659, 503813.3502356585, 167337.6746773538], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 169200.0000, 
sim time next is 169800.0000, 
raw observation next is [20.56666666666667, 96.0, 1.0, 2.0, 0.3148618225490976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 500013.017037639, 500013.0170376384, 167063.0082945245], 
processed observation next is [1.0, 1.0, 0.17377567140600336, 0.96, 1.0, 1.0, 0.17453231632421398, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1388925047326775, 0.13889250473267734, 0.2493477735739172], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.03182887], dtype=float32), -1.2277682]. 
=============================================
[2019-04-28 02:24:36,796] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133000, global step 2129297: loss 0.0200
[2019-04-28 02:24:36,798] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133000, global step 2129297: learning rate 0.0000
[2019-04-28 02:24:38,514] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.5128912e-15 9.9999142e-01 5.9000552e-24 8.6190948e-06 1.1927477e-10], sum to 1.0000
[2019-04-28 02:24:38,521] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6200
[2019-04-28 02:24:38,525] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.53333333333333, 80.33333333333333, 1.0, 2.0, 0.2291182250388971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 380366.530738741, 380366.5307387405, 158678.2271407442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 597000.0000, 
sim time next is 597600.0000, 
raw observation next is [19.4, 81.0, 1.0, 2.0, 0.2292047561545382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 380660.749794824, 380660.749794824, 158663.3649295302], 
processed observation next is [1.0, 0.9565217391304348, 0.11848341232227487, 0.81, 1.0, 1.0, 0.0713310315114918, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10573909716522889, 0.10573909716522889, 0.2368109924321346], 
reward next is 0.7632, 
noisyNet noise sample is [array([-0.3379633], dtype=float32), -1.2893541]. 
=============================================
[2019-04-28 02:24:39,382] A3C_AGENT_WORKER-Thread-18 INFO:Local step 133500, global step 2130310: loss 0.0166
[2019-04-28 02:24:39,384] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 133500, global step 2130310: learning rate 0.0000
[2019-04-28 02:24:41,558] A3C_AGENT_WORKER-Thread-20 INFO:Local step 133500, global step 2131219: loss 0.0275
[2019-04-28 02:24:41,558] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133000, global step 2131219: loss 0.0105
[2019-04-28 02:24:41,559] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 133500, global step 2131220: learning rate 0.0000
[2019-04-28 02:24:41,560] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133000, global step 2131220: learning rate 0.0000
[2019-04-28 02:24:41,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.6699102e-14 9.9999964e-01 1.8344806e-23 3.2257128e-07 2.0619118e-09], sum to 1.0000
[2019-04-28 02:24:41,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3305
[2019-04-28 02:24:41,602] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.56666666666667, 91.0, 1.0, 2.0, 0.2856054866037849, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459242.3688511135, 459242.3688511135, 164219.7473754493], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 254400.0000, 
sim time next is 255000.0000, 
raw observation next is [20.53333333333333, 91.0, 1.0, 2.0, 0.2845031306979766, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457763.0814410559, 457763.0814410559, 164120.3348613558], 
processed observation next is [0.0, 0.9565217391304348, 0.17219589257503945, 0.91, 1.0, 1.0, 0.13795557915418868, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12715641151140442, 0.12715641151140442, 0.2449557236736654], 
reward next is 0.7550, 
noisyNet noise sample is [array([0.20963973], dtype=float32), -1.6845644]. 
=============================================
[2019-04-28 02:24:41,665] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[80.798454]
 [80.76352 ]
 [80.7307  ]
 [80.70762 ]
 [80.70231 ]], R is [[80.77368927]
 [80.72084808]
 [80.668396  ]
 [80.61628723]
 [80.56446075]].
[2019-04-28 02:24:42,525] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133000, global step 2131607: loss 0.0054
[2019-04-28 02:24:42,526] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133000, global step 2131607: learning rate 0.0000
[2019-04-28 02:24:43,552] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133000, global step 2132039: loss 0.0001
[2019-04-28 02:24:43,557] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133000, global step 2132039: learning rate 0.0000
[2019-04-28 02:24:43,907] A3C_AGENT_WORKER-Thread-2 INFO:Local step 133500, global step 2132193: loss 0.0587
[2019-04-28 02:24:43,913] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 133500, global step 2132193: learning rate 0.0000
[2019-04-28 02:24:45,386] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133000, global step 2132816: loss 0.0032
[2019-04-28 02:24:45,393] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133000, global step 2132820: learning rate 0.0000
[2019-04-28 02:24:45,949] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133000, global step 2133061: loss 0.0636
[2019-04-28 02:24:45,956] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133000, global step 2133062: learning rate 0.0000
[2019-04-28 02:24:47,927] A3C_AGENT_WORKER-Thread-11 INFO:Local step 133500, global step 2133829: loss 0.0213
[2019-04-28 02:24:47,928] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 133500, global step 2133829: learning rate 0.0000
[2019-04-28 02:24:48,889] A3C_AGENT_WORKER-Thread-10 INFO:Local step 133500, global step 2134217: loss 0.0464
[2019-04-28 02:24:48,891] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 133500, global step 2134218: learning rate 0.0000
[2019-04-28 02:24:49,211] A3C_AGENT_WORKER-Thread-9 INFO:Local step 133500, global step 2134350: loss 0.0097
[2019-04-28 02:24:49,216] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 133500, global step 2134353: learning rate 0.0000
[2019-04-28 02:24:49,539] A3C_AGENT_WORKER-Thread-14 INFO:Local step 133500, global step 2134483: loss 0.0374
[2019-04-28 02:24:49,540] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 133500, global step 2134483: learning rate 0.0000
[2019-04-28 02:24:51,670] A3C_AGENT_WORKER-Thread-12 INFO:Local step 133500, global step 2135383: loss 0.1093
[2019-04-28 02:24:51,671] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 133500, global step 2135383: learning rate 0.0000
[2019-04-28 02:24:52,325] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.2845319e-12 9.9999928e-01 1.2272586e-24 7.3456459e-07 5.2403889e-12], sum to 1.0000
[2019-04-28 02:24:52,331] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2213
[2019-04-28 02:24:52,335] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.91666666666667, 80.83333333333333, 1.0, 2.0, 0.2640749210196531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 431159.913625452, 431159.9136254514, 162295.4495051271], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 414600.0000, 
sim time next is 415200.0000, 
raw observation next is [20.83333333333334, 80.66666666666667, 1.0, 2.0, 0.2606400843770779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 426190.2617188799, 426190.2617188799, 161959.4254899148], 
processed observation next is [1.0, 0.8260869565217391, 0.1864139020537128, 0.8066666666666668, 1.0, 1.0, 0.10920492093623844, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11838618381079997, 0.11838618381079997, 0.241730485805843], 
reward next is 0.7583, 
noisyNet noise sample is [array([0.3963881], dtype=float32), -0.18285611]. 
=============================================
[2019-04-28 02:24:52,372] A3C_AGENT_WORKER-Thread-13 INFO:Local step 133500, global step 2135697: loss 0.0164
[2019-04-28 02:24:52,374] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 133500, global step 2135697: learning rate 0.0000
[2019-04-28 02:24:53,224] A3C_AGENT_WORKER-Thread-21 INFO:Local step 133500, global step 2136059: loss 0.0154
[2019-04-28 02:24:53,225] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 133500, global step 2136059: learning rate 0.0000
[2019-04-28 02:24:55,922] A3C_AGENT_WORKER-Thread-16 INFO:Local step 133500, global step 2137168: loss 0.0101
[2019-04-28 02:24:55,927] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 133500, global step 2137169: learning rate 0.0000
[2019-04-28 02:24:58,945] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134000, global step 2138439: loss 0.0038
[2019-04-28 02:24:58,946] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134000, global step 2138439: learning rate 0.0000
[2019-04-28 02:24:59,461] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.8353224e-16 9.9999988e-01 4.1382869e-27 9.8381328e-08 1.0612615e-10], sum to 1.0000
[2019-04-28 02:24:59,470] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.5761
[2019-04-28 02:24:59,475] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.26666666666667, 56.00000000000001, 1.0, 2.0, 0.2417796091616441, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 397264.0672467303, 397264.0672467309, 160112.5659771853], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 667200.0000, 
sim time next is 667800.0000, 
raw observation next is [24.05, 57.0, 1.0, 2.0, 0.2372704020837894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 390053.9359032724, 390053.9359032724, 159683.1448555862], 
processed observation next is [1.0, 0.7391304347826086, 0.3388625592417062, 0.57, 1.0, 1.0, 0.08104867720938483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.10834831552868679, 0.10834831552868679, 0.238333052023263], 
reward next is 0.7617, 
noisyNet noise sample is [array([1.4948238], dtype=float32), -0.10432331]. 
=============================================
[2019-04-28 02:25:00,357] A3C_AGENT_WORKER-Thread-19 INFO:Local step 133500, global step 2139043: loss 0.0438
[2019-04-28 02:25:00,360] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 133500, global step 2139043: learning rate 0.0000
[2019-04-28 02:25:00,478] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.13428635e-13 9.99989867e-01 4.72751492e-22 1.01676515e-05
 1.13889051e-10], sum to 1.0000
[2019-04-28 02:25:00,483] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3695
[2019-04-28 02:25:00,491] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 70.5, 1.0, 2.0, 0.2499026127298319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411276.6103205053, 411276.6103205059, 160891.6280815469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [21.6, 71.0, 1.0, 2.0, 0.2496570161819618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 411226.8252311549, 411226.8252311542, 160859.4074592386], 
processed observation next is [1.0, 0.8260869565217391, 0.22274881516587688, 0.71, 1.0, 1.0, 0.095972308652966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1142296736753208, 0.11422967367532061, 0.24008866784960986], 
reward next is 0.7599, 
noisyNet noise sample is [array([-1.9303], dtype=float32), 0.049418237]. 
=============================================
[2019-04-28 02:25:00,530] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[75.47596 ]
 [75.471825]
 [75.44072 ]
 [75.4458  ]
 [75.47051 ]], R is [[75.47910309]
 [75.48417664]
 [75.48905182]
 [75.49354553]
 [75.49769592]].
[2019-04-28 02:25:00,988] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134000, global step 2139320: loss 0.0493
[2019-04-28 02:25:00,997] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134000, global step 2139321: learning rate 0.0000
[2019-04-28 02:25:01,235] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2150607e-14 9.9999988e-01 5.0055776e-24 1.5207969e-07 1.4001333e-10], sum to 1.0000
[2019-04-28 02:25:01,240] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6534
[2019-04-28 02:25:01,246] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.75, 70.5, 1.0, 2.0, 0.2499026127298319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 411276.6103205053, 411276.6103205059, 160891.6280815469], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 587400.0000, 
sim time next is 588000.0000, 
raw observation next is [21.6, 71.0, 1.0, 2.0, 0.2496570161819618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 411226.8252311549, 411226.8252311542, 160859.4074592386], 
processed observation next is [1.0, 0.8260869565217391, 0.22274881516587688, 0.71, 1.0, 1.0, 0.095972308652966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1142296736753208, 0.11422967367532061, 0.24008866784960986], 
reward next is 0.7599, 
noisyNet noise sample is [array([-0.02008101], dtype=float32), -0.13647276]. 
=============================================
[2019-04-28 02:25:01,259] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[74.60506 ]
 [74.608185]
 [74.57808 ]
 [74.582504]
 [74.597336]], R is [[74.61357117]
 [74.62730408]
 [74.6407547 ]
 [74.6537323 ]
 [74.66628265]].
[2019-04-28 02:25:02,121] A3C_AGENT_WORKER-Thread-3 INFO:Local step 133500, global step 2139787: loss 0.0465
[2019-04-28 02:25:02,126] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 133500, global step 2139787: learning rate 0.0000
[2019-04-28 02:25:02,435] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134000, global step 2139935: loss 0.0039
[2019-04-28 02:25:02,436] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134000, global step 2139935: learning rate 0.0000
[2019-04-28 02:25:02,545] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.9255296e-13 9.9999964e-01 2.4065914e-23 3.2744799e-07 2.7937199e-11], sum to 1.0000
[2019-04-28 02:25:02,550] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.2748
[2019-04-28 02:25:02,557] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.41666666666667, 91.83333333333333, 1.0, 2.0, 0.2596210836681977, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 424958.8554000512, 424958.8554000505, 161864.003403954], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 784200.0000, 
sim time next is 784800.0000, 
raw observation next is [19.4, 92.0, 1.0, 2.0, 0.2599698969719779, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 425513.1070046628, 425513.1070046628, 161899.20165011], 
processed observation next is [0.0, 0.08695652173913043, 0.11848341232227487, 0.92, 1.0, 1.0, 0.10839746623129869, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.118198085279073, 0.118198085279073, 0.2416405994777761], 
reward next is 0.7584, 
noisyNet noise sample is [array([1.3670055], dtype=float32), -1.1653962]. 
=============================================
[2019-04-28 02:25:02,788] A3C_AGENT_WORKER-Thread-22 INFO:Local step 133500, global step 2140084: loss 0.0185
[2019-04-28 02:25:02,790] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 133500, global step 2140085: learning rate 0.0000
[2019-04-28 02:25:04,681] A3C_AGENT_WORKER-Thread-15 INFO:Local step 133500, global step 2140886: loss 0.0236
[2019-04-28 02:25:04,682] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 133500, global step 2140886: learning rate 0.0000
[2019-04-28 02:25:04,742] A3C_AGENT_WORKER-Thread-17 INFO:Local step 133500, global step 2140911: loss 0.0402
[2019-04-28 02:25:04,743] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 133500, global step 2140911: learning rate 0.0000
[2019-04-28 02:25:06,178] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [5.2239783e-15 1.0000000e+00 6.1735376e-27 2.5306498e-08 1.6914761e-09], sum to 1.0000
[2019-04-28 02:25:06,189] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.6127
[2019-04-28 02:25:06,197] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 86.5, 1.0, 2.0, 0.277683325061373, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 448518.5441800884, 448518.5441800891, 163500.584891153], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 882600.0000, 
sim time next is 883200.0000, 
raw observation next is [21.0, 86.0, 1.0, 2.0, 0.278570810106858, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 449665.4145380659, 449665.4145380653, 163577.5260373691], 
processed observation next is [0.0, 0.21739130434782608, 0.19431279620853087, 0.86, 1.0, 1.0, 0.1308082049480217, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1249070595939072, 0.12490705959390704, 0.24414556124980463], 
reward next is 0.7559, 
noisyNet noise sample is [array([0.53227496], dtype=float32), 0.06969528]. 
=============================================
[2019-04-28 02:25:07,098] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134000, global step 2141925: loss 0.0074
[2019-04-28 02:25:07,100] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134000, global step 2141926: learning rate 0.0000
[2019-04-28 02:25:08,110] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134000, global step 2142389: loss 0.0008
[2019-04-28 02:25:08,113] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134000, global step 2142389: loss 0.0073
[2019-04-28 02:25:08,114] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134000, global step 2142389: learning rate 0.0000
[2019-04-28 02:25:08,115] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134000, global step 2142389: learning rate 0.0000
[2019-04-28 02:25:08,305] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134000, global step 2142477: loss 0.0503
[2019-04-28 02:25:08,318] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134000, global step 2142478: learning rate 0.0000
[2019-04-28 02:25:10,484] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134000, global step 2143373: loss 0.0416
[2019-04-28 02:25:10,486] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134000, global step 2143374: learning rate 0.0000
[2019-04-28 02:25:11,292] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134000, global step 2143724: loss 0.0449
[2019-04-28 02:25:11,305] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134000, global step 2143727: learning rate 0.0000
[2019-04-28 02:25:11,847] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134000, global step 2143972: loss 0.0089
[2019-04-28 02:25:11,860] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134000, global step 2143975: learning rate 0.0000
[2019-04-28 02:25:14,170] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134000, global step 2144974: loss 0.0119
[2019-04-28 02:25:14,173] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134000, global step 2144974: learning rate 0.0000
[2019-04-28 02:25:17,934] A3C_AGENT_WORKER-Thread-18 INFO:Local step 134500, global step 2146550: loss 0.5216
[2019-04-28 02:25:17,938] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 134500, global step 2146551: learning rate 0.0000
[2019-04-28 02:25:18,281] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [3.8269664e-13 9.9999988e-01 5.8264967e-24 8.5349093e-08 1.5351095e-12], sum to 1.0000
[2019-04-28 02:25:18,306] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.0804
[2019-04-28 02:25:18,312] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.7, 67.83333333333334, 1.0, 2.0, 0.8288934001619661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1277220.535519017, 1277220.535519017, 266958.9426805781], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1091400.0000, 
sim time next is 1092000.0000, 
raw observation next is [25.7, 67.66666666666667, 1.0, 2.0, 0.7245263114899639, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117098.341229629, 1117098.341229629, 239467.6401872277], 
processed observation next is [1.0, 0.6521739130434783, 0.4170616113744076, 0.6766666666666667, 1.0, 1.0, 0.6681039897469445, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31030509478600804, 0.31030509478600804, 0.3574143883391458], 
reward next is 0.6426, 
noisyNet noise sample is [array([-1.6976837], dtype=float32), 1.2149241]. 
=============================================
[2019-04-28 02:25:18,337] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[77.10858 ]
 [77.06835 ]
 [76.968025]
 [76.91556 ]
 [76.831245]], R is [[77.12682343]
 [76.95710754]
 [76.84869385]
 [76.74861908]
 [76.65029907]].
[2019-04-28 02:25:19,367] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134000, global step 2147156: loss 0.0032
[2019-04-28 02:25:19,370] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134000, global step 2147158: learning rate 0.0000
[2019-04-28 02:25:19,931] A3C_AGENT_WORKER-Thread-20 INFO:Local step 134500, global step 2147394: loss 0.2389
[2019-04-28 02:25:19,937] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 134500, global step 2147395: learning rate 0.0000
[2019-04-28 02:25:20,961] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134000, global step 2147827: loss 0.0409
[2019-04-28 02:25:20,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134000, global step 2147827: learning rate 0.0000
[2019-04-28 02:25:21,012] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134000, global step 2147838: loss 0.0259
[2019-04-28 02:25:21,014] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134000, global step 2147838: learning rate 0.0000
[2019-04-28 02:25:21,409] A3C_AGENT_WORKER-Thread-2 INFO:Local step 134500, global step 2148016: loss 0.3524
[2019-04-28 02:25:21,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 134500, global step 2148016: learning rate 0.0000
[2019-04-28 02:25:23,329] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134000, global step 2148852: loss 0.0034
[2019-04-28 02:25:23,330] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134000, global step 2148852: learning rate 0.0000
[2019-04-28 02:25:23,355] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134000, global step 2148861: loss 0.0053
[2019-04-28 02:25:23,360] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134000, global step 2148861: learning rate 0.0000
[2019-04-28 02:25:25,805] A3C_AGENT_WORKER-Thread-11 INFO:Local step 134500, global step 2149890: loss 0.8580
[2019-04-28 02:25:25,807] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 134500, global step 2149892: learning rate 0.0000
[2019-04-28 02:25:26,095] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 02:25:26,097] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:25:26,097] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:25:26,097] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:25:26,100] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:25:26,101] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:25:26,101] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:25:26,101] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:25:26,102] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:25:26,101] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:25:26,103] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:25:26,127] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run87
[2019-04-28 02:25:26,163] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run87
[2019-04-28 02:25:26,191] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run87
[2019-04-28 02:25:26,242] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run87
[2019-04-28 02:25:26,279] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run87
[2019-04-28 02:25:30,046] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:25:30,047] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.73333333333333, 92.66666666666667, 1.0, 2.0, 0.3048412396151696, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 486642.5017893348, 486642.5017893342, 166115.4785106497]
[2019-04-28 02:25:30,048] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:25:30,050] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4104989e-14 1.0000000e+00 4.4684726e-24 6.5140515e-09 2.9241655e-12], sampled 0.19904773197068693
[2019-04-28 02:25:44,964] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:25:44,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.70876489, 96.02688005499999, 1.0, 2.0, 0.286114173960455, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 463226.8299291026, 463226.8299291033, 164480.5973059341]
[2019-04-28 02:25:44,966] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:25:44,968] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.5189449e-14 1.0000000e+00 4.8217206e-24 6.6906067e-09 3.0363680e-12], sampled 0.38686540392873836
[2019-04-28 02:25:53,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:25:53,647] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.5, 85.0, 1.0, 2.0, 0.4635365679839554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683044.8440060922, 683044.8440060916, 182524.5029754804]
[2019-04-28 02:25:53,649] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:25:53,650] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.1642650e-14 1.0000000e+00 7.1476028e-24 7.6805104e-09 3.6897221e-12], sampled 0.021698079081390764
[2019-04-28 02:26:04,318] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:26:04,320] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.37556611666666, 98.80646499500001, 1.0, 2.0, 0.4815070038011401, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 672823.043464592, 672823.0434645914, 180749.541933054]
[2019-04-28 02:26:04,321] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:26:04,325] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0227624e-14 1.0000000e+00 6.6041258e-24 7.4704625e-09 3.5483170e-12], sampled 0.365340374150102
[2019-04-28 02:26:21,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:26:21,202] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5501475598523213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 768771.0967108719, 768771.0967108713, 191825.7660821197]
[2019-04-28 02:26:21,203] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:26:21,206] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8883161e-14 1.0000000e+00 2.9304735e-24 5.6178631e-09 2.3732568e-12], sampled 0.7966918331118482
[2019-04-28 02:26:29,973] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:26:29,975] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.33936635, 85.06032149666667, 1.0, 2.0, 0.6813368997191968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1005253.61416699, 1005253.61416699, 223772.8879295269]
[2019-04-28 02:26:29,975] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:26:29,979] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.6165725e-14 1.0000000e+00 9.0044836e-24 8.3287706e-09 4.1364408e-12], sampled 0.8274814934196403
[2019-04-28 02:26:40,125] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:26:40,127] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.271649865, 97.777818465, 1.0, 2.0, 0.7653356719352108, 1.0, 2.0, 0.7653356719352108, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 2140277.94987548, 2140277.94987548, 403826.9306264833]
[2019-04-28 02:26:40,129] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:26:40,133] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.9232300e-13 1.0000000e+00 1.6137023e-22 2.2920016e-08 1.7259381e-11], sampled 0.4875565741355311
[2019-04-28 02:26:40,135] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2140277.94987548 W.
[2019-04-28 02:27:00,767] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.10739396]
[2019-04-28 02:27:00,769] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.37314737666667, 96.90234538333334, 1.0, 2.0, 0.5049606912617308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705606.413508781, 705606.413508781, 184375.3146784634]
[2019-04-28 02:27:00,770] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:27:00,772] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.7227249e-14 1.0000000e+00 5.5143132e-24 7.0126318e-09 3.2450639e-12], sampled 0.624669323941528
[2019-04-28 02:27:02,333] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:27:03,083] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:27:03,134] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:27:03,204] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:27:03,361] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:27:04,374] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2150000, evaluation results [2150000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:27:04,978] A3C_AGENT_WORKER-Thread-9 INFO:Local step 134500, global step 2150256: loss 0.4163
[2019-04-28 02:27:04,980] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 134500, global step 2150256: learning rate 0.0000
[2019-04-28 02:27:05,485] A3C_AGENT_WORKER-Thread-14 INFO:Local step 134500, global step 2150458: loss 1.2345
[2019-04-28 02:27:05,487] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 134500, global step 2150459: learning rate 0.0000
[2019-04-28 02:27:05,738] A3C_AGENT_WORKER-Thread-10 INFO:Local step 134500, global step 2150572: loss 0.2321
[2019-04-28 02:27:05,740] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 134500, global step 2150572: learning rate 0.0000
[2019-04-28 02:27:07,754] A3C_AGENT_WORKER-Thread-12 INFO:Local step 134500, global step 2151386: loss 1.8283
[2019-04-28 02:27:07,756] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 134500, global step 2151386: learning rate 0.0000
[2019-04-28 02:27:08,975] A3C_AGENT_WORKER-Thread-13 INFO:Local step 134500, global step 2151890: loss 4.2063
[2019-04-28 02:27:08,979] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 134500, global step 2151890: learning rate 0.0000
[2019-04-28 02:27:09,491] A3C_AGENT_WORKER-Thread-21 INFO:Local step 134500, global step 2152078: loss 1.6987
[2019-04-28 02:27:09,492] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 134500, global step 2152078: learning rate 0.0000
[2019-04-28 02:27:11,775] A3C_AGENT_WORKER-Thread-16 INFO:Local step 134500, global step 2153025: loss 5.2728
[2019-04-28 02:27:11,778] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 134500, global step 2153025: learning rate 0.0000
[2019-04-28 02:27:14,363] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [5.3442369e-13 1.0000000e+00 2.0645058e-24 3.0476173e-09 1.0800810e-11], sum to 1.0000
[2019-04-28 02:27:14,398] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3191
[2019-04-28 02:27:14,402] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 96.0, 1.0, 2.0, 0.3572979927717415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 548995.5839576753, 548995.5839576758, 170522.2986247193], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1468200.0000, 
sim time next is 1468800.0000, 
raw observation next is [21.8, 96.0, 1.0, 2.0, 0.3566330248468683, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548512.7026637949, 548512.7026637949, 170497.4604648544], 
processed observation next is [0.0, 0.0, 0.23222748815165886, 0.96, 1.0, 1.0, 0.22485906608056425, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1523646396288319, 0.1523646396288319, 0.25447382158933496], 
reward next is 0.7455, 
noisyNet noise sample is [array([-0.5067703], dtype=float32), 0.23388466]. 
=============================================
[2019-04-28 02:27:14,845] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135000, global step 2154253: loss 0.1040
[2019-04-28 02:27:14,848] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135000, global step 2154254: learning rate 0.0000
[2019-04-28 02:27:15,640] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.2191934e-12 9.9999917e-01 4.1650594e-22 7.9987404e-07 9.0764743e-11], sum to 1.0000
[2019-04-28 02:27:15,646] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8752
[2019-04-28 02:27:15,651] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 95.0, 1.0, 2.0, 0.7450749485453285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1106612.822111481, 1106612.82211148, 239564.1965244612], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1329600.0000, 
sim time next is 1330200.0000, 
raw observation next is [23.0, 95.0, 1.0, 2.0, 0.7423227785716151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1102562.798345055, 1102562.798345055, 238896.4301853346], 
processed observation next is [1.0, 0.391304347826087, 0.28909952606635075, 0.95, 1.0, 1.0, 0.6895455163513435, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3062674439847375, 0.3062674439847375, 0.35656183609751435], 
reward next is 0.6434, 
noisyNet noise sample is [array([-0.49957085], dtype=float32), -0.13845484]. 
=============================================
[2019-04-28 02:27:16,902] A3C_AGENT_WORKER-Thread-19 INFO:Local step 134500, global step 2155091: loss 2.8695
[2019-04-28 02:27:16,903] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 134500, global step 2155091: learning rate 0.0000
[2019-04-28 02:27:17,291] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135000, global step 2155264: loss 0.0088
[2019-04-28 02:27:17,292] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135000, global step 2155264: learning rate 0.0000
[2019-04-28 02:27:17,390] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.32951457e-13 1.00000000e+00 1.28063025e-23 9.13442832e-09
 5.15205283e-14], sum to 1.0000
[2019-04-28 02:27:17,398] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.8443
[2019-04-28 02:27:17,404] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 87.33333333333333, 1.0, 2.0, 0.3444935536151093, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 533157.7505699028, 533157.7505699035, 169330.8309994396], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1413600.0000, 
sim time next is 1414200.0000, 
raw observation next is [22.85, 86.66666666666667, 1.0, 2.0, 0.3457964186680915, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 534249.9698801828, 534249.9698801828, 169392.7170558443], 
processed observation next is [0.0, 0.34782608695652173, 0.28199052132701435, 0.8666666666666667, 1.0, 1.0, 0.21180291405794158, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14840276941116187, 0.14840276941116187, 0.25282495082961837], 
reward next is 0.7472, 
noisyNet noise sample is [array([1.139492], dtype=float32), -0.20638435]. 
=============================================
[2019-04-28 02:27:18,648] A3C_AGENT_WORKER-Thread-3 INFO:Local step 134500, global step 2155813: loss 2.8839
[2019-04-28 02:27:18,651] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 134500, global step 2155815: learning rate 0.0000
[2019-04-28 02:27:18,845] A3C_AGENT_WORKER-Thread-22 INFO:Local step 134500, global step 2155904: loss 3.7465
[2019-04-28 02:27:18,848] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 134500, global step 2155906: learning rate 0.0000
[2019-04-28 02:27:19,307] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135000, global step 2156087: loss 0.0002
[2019-04-28 02:27:19,310] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135000, global step 2156087: learning rate 0.0000
[2019-04-28 02:27:21,057] A3C_AGENT_WORKER-Thread-15 INFO:Local step 134500, global step 2156773: loss 1.0772
[2019-04-28 02:27:21,058] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 134500, global step 2156773: learning rate 0.0000
[2019-04-28 02:27:21,174] A3C_AGENT_WORKER-Thread-17 INFO:Local step 134500, global step 2156830: loss 1.7164
[2019-04-28 02:27:21,177] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 134500, global step 2156830: learning rate 0.0000
[2019-04-28 02:27:21,464] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [2.3304407e-16 1.0000000e+00 2.4026637e-25 4.1022902e-10 3.2798905e-13], sum to 1.0000
[2019-04-28 02:27:21,475] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6263
[2019-04-28 02:27:21,480] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.93333333333334, 91.33333333333334, 1.0, 2.0, 0.6037842607230055, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 962711.6791604629, 962711.6791604629, 214989.1359678286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1354800.0000, 
sim time next is 1355400.0000, 
raw observation next is [20.9, 91.5, 1.0, 2.0, 0.5898153121947395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 940732.0345272601, 940732.0345272601, 212064.7947096643], 
processed observation next is [1.0, 0.6956521739130435, 0.1895734597156398, 0.915, 1.0, 1.0, 0.5058015809575174, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26131445403535003, 0.26131445403535003, 0.3165146189696482], 
reward next is 0.6835, 
noisyNet noise sample is [array([0.35233486], dtype=float32), -0.35489726]. 
=============================================
[2019-04-28 02:27:23,853] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135000, global step 2157922: loss 0.0079
[2019-04-28 02:27:23,855] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135000, global step 2157922: learning rate 0.0000
[2019-04-28 02:27:24,908] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135000, global step 2158360: loss 0.0229
[2019-04-28 02:27:24,911] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135000, global step 2158360: learning rate 0.0000
[2019-04-28 02:27:25,018] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135000, global step 2158407: loss 0.0140
[2019-04-28 02:27:25,019] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135000, global step 2158407: learning rate 0.0000
[2019-04-28 02:27:25,578] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135000, global step 2158626: loss 0.0792
[2019-04-28 02:27:25,579] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135000, global step 2158626: learning rate 0.0000
[2019-04-28 02:27:27,123] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135000, global step 2159252: loss 0.0649
[2019-04-28 02:27:27,126] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135000, global step 2159253: learning rate 0.0000
[2019-04-28 02:27:28,373] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135000, global step 2159740: loss 0.0024
[2019-04-28 02:27:28,377] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135000, global step 2159744: learning rate 0.0000
[2019-04-28 02:27:29,425] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135000, global step 2160159: loss 0.0045
[2019-04-28 02:27:29,428] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135000, global step 2160159: learning rate 0.0000
[2019-04-28 02:27:31,569] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135000, global step 2161026: loss 0.0738
[2019-04-28 02:27:31,570] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135000, global step 2161026: learning rate 0.0000
[2019-04-28 02:27:33,364] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.7671871e-14 1.0000000e+00 6.0969034e-23 4.0493696e-08 5.9415840e-13], sum to 1.0000
[2019-04-28 02:27:33,375] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1557
[2019-04-28 02:27:33,386] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.28333333333333, 98.0, 1.0, 2.0, 0.3052578858737106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485545.4604064184, 485545.460406419, 166012.4568929832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1392600.0000, 
sim time next is 1393200.0000, 
raw observation next is [20.3, 98.0, 1.0, 2.0, 0.3055627625221013, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 485848.896974797, 485848.8969747977, 166031.5411313304], 
processed observation next is [0.0, 0.13043478260869565, 0.16113744075829392, 0.98, 1.0, 1.0, 0.16332862954470034, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1349580269374436, 0.1349580269374438, 0.24780827034526925], 
reward next is 0.7522, 
noisyNet noise sample is [array([-1.0907302], dtype=float32), 0.20033349]. 
=============================================
[2019-04-28 02:27:34,604] A3C_AGENT_WORKER-Thread-18 INFO:Local step 135500, global step 2162222: loss 0.0041
[2019-04-28 02:27:34,606] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 135500, global step 2162222: learning rate 0.0000
[2019-04-28 02:27:34,842] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [5.1773198e-14 9.9999988e-01 1.3870492e-23 1.2485360e-07 2.2728085e-11], sum to 1.0000
[2019-04-28 02:27:34,844] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7963
[2019-04-28 02:27:34,853] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 69.16666666666666, 1.0, 2.0, 0.4350579333445968, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 624688.8363887838, 624688.8363887845, 176211.0381997831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1443000.0000, 
sim time next is 1443600.0000, 
raw observation next is [27.6, 69.0, 1.0, 2.0, 0.4307955358048171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 620818.23157765, 620818.2315776505, 175895.6247755926], 
processed observation next is [0.0, 0.7391304347826086, 0.5071090047393366, 0.69, 1.0, 1.0, 0.3142114889214664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17244950877156942, 0.1724495087715696, 0.2625307832471531], 
reward next is 0.7375, 
noisyNet noise sample is [array([1.0821509], dtype=float32), -0.5776545]. 
=============================================
[2019-04-28 02:27:35,365] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [2.7509454e-13 9.9999976e-01 1.7259074e-23 2.9178543e-07 2.5160958e-11], sum to 1.0000
[2019-04-28 02:27:35,372] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3471
[2019-04-28 02:27:35,378] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 88.16666666666667, 1.0, 2.0, 0.3777673281179953, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 570617.3028294251, 570617.3028294244, 172091.4384280974], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1421400.0000, 
sim time next is 1422000.0000, 
raw observation next is [23.2, 88.0, 1.0, 2.0, 0.3689522825108486, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 560048.3335572479, 560048.3335572473, 171256.9946980918], 
processed observation next is [0.0, 0.4782608695652174, 0.29857819905213273, 0.88, 1.0, 1.0, 0.2397015451937935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15556898154367996, 0.15556898154367982, 0.25560745477327135], 
reward next is 0.7444, 
noisyNet noise sample is [array([-0.16545437], dtype=float32), 1.5099146]. 
=============================================
[2019-04-28 02:27:35,406] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[77.17749 ]
 [77.153984]
 [77.13286 ]
 [77.11665 ]
 [77.09912 ]], R is [[77.18247223]
 [77.15379333]
 [77.12470245]
 [77.0953064 ]
 [77.0657959 ]].
[2019-04-28 02:27:36,423] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135000, global step 2162962: loss 0.0620
[2019-04-28 02:27:36,426] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135000, global step 2162962: learning rate 0.0000
[2019-04-28 02:27:36,499] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1467164e-14 1.0000000e+00 7.2730159e-26 7.8918410e-09 7.4131743e-13], sum to 1.0000
[2019-04-28 02:27:36,509] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9368
[2019-04-28 02:27:36,526] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.66666666666667, 97.83333333333334, 1.0, 2.0, 0.3187335954136062, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 502412.6620609707, 502412.6620609707, 167173.1726237352], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1482600.0000, 
sim time next is 1483200.0000, 
raw observation next is [20.6, 98.0, 1.0, 2.0, 0.3167240878302783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499821.5339507469, 499821.5339507469, 166990.9866928654], 
processed observation next is [0.0, 0.17391304347826086, 0.17535545023696694, 0.98, 1.0, 1.0, 0.1767760094340702, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13883931498631857, 0.13883931498631857, 0.24924027864606776], 
reward next is 0.7508, 
noisyNet noise sample is [array([-0.04423045], dtype=float32), -0.5418686]. 
=============================================
[2019-04-28 02:27:37,790] A3C_AGENT_WORKER-Thread-20 INFO:Local step 135500, global step 2163531: loss 0.0148
[2019-04-28 02:27:37,793] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 135500, global step 2163531: learning rate 0.0000
[2019-04-28 02:27:38,480] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135000, global step 2163814: loss 0.0007
[2019-04-28 02:27:38,482] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135000, global step 2163815: learning rate 0.0000
[2019-04-28 02:27:38,924] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1776868e-13 9.9999988e-01 1.0586511e-22 1.6204477e-07 1.6701115e-10], sum to 1.0000
[2019-04-28 02:27:38,929] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.3433
[2019-04-28 02:27:38,933] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.3, 99.0, 1.0, 2.0, 0.4281217305497677, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 616870.4574160738, 616870.4574160744, 175508.2533789157], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1654800.0000, 
sim time next is 1655400.0000, 
raw observation next is [23.3, 99.0, 1.0, 2.0, 0.4283203187018617, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 617158.4046353626, 617158.404635362, 175536.2886674038], 
processed observation next is [1.0, 0.13043478260869565, 0.3033175355450238, 0.99, 1.0, 1.0, 0.3112292996407972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1714328901764896, 0.17143289017648944, 0.2619944606976176], 
reward next is 0.7380, 
noisyNet noise sample is [array([-1.1374717], dtype=float32), -1.2903126]. 
=============================================
[2019-04-28 02:27:39,256] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135000, global step 2164104: loss 0.0268
[2019-04-28 02:27:39,256] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135000, global step 2164104: learning rate 0.0000
[2019-04-28 02:27:39,285] A3C_AGENT_WORKER-Thread-2 INFO:Local step 135500, global step 2164110: loss 0.0207
[2019-04-28 02:27:39,286] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 135500, global step 2164110: learning rate 0.0000
[2019-04-28 02:27:40,463] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135000, global step 2164606: loss 0.0314
[2019-04-28 02:27:40,464] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135000, global step 2164606: learning rate 0.0000
[2019-04-28 02:27:41,490] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135000, global step 2164992: loss 0.0129
[2019-04-28 02:27:41,491] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135000, global step 2164992: learning rate 0.0000
[2019-04-28 02:27:43,786] A3C_AGENT_WORKER-Thread-11 INFO:Local step 135500, global step 2165946: loss 0.0090
[2019-04-28 02:27:43,789] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 135500, global step 2165946: learning rate 0.0000
[2019-04-28 02:27:44,963] A3C_AGENT_WORKER-Thread-14 INFO:Local step 135500, global step 2166439: loss 0.0071
[2019-04-28 02:27:44,968] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 135500, global step 2166439: learning rate 0.0000
[2019-04-28 02:27:45,002] A3C_AGENT_WORKER-Thread-10 INFO:Local step 135500, global step 2166455: loss 0.0145
[2019-04-28 02:27:45,002] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 135500, global step 2166455: learning rate 0.0000
[2019-04-28 02:27:45,089] A3C_AGENT_WORKER-Thread-9 INFO:Local step 135500, global step 2166500: loss 0.0204
[2019-04-28 02:27:45,095] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 135500, global step 2166500: learning rate 0.0000
[2019-04-28 02:27:45,580] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.5589457e-12 1.0000000e+00 7.2614354e-24 3.5708172e-08 2.8268074e-12], sum to 1.0000
[2019-04-28 02:27:45,605] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.4777
[2019-04-28 02:27:45,609] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.53333333333333, 85.0, 1.0, 2.0, 0.7318827464191792, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1114832.101468987, 1114832.101468988, 239745.0294733926], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1588800.0000, 
sim time next is 1589400.0000, 
raw observation next is [23.55, 85.0, 1.0, 2.0, 0.7709525610057512, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1173762.643976374, 1173762.643976374, 249564.6657224942], 
processed observation next is [1.0, 0.391304347826087, 0.3151658767772513, 0.85, 1.0, 1.0, 0.724039230127411, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.32604517888232615, 0.32604517888232615, 0.3724845757052152], 
reward next is 0.6275, 
noisyNet noise sample is [array([-0.824618], dtype=float32), -0.15975009]. 
=============================================
[2019-04-28 02:27:46,672] A3C_AGENT_WORKER-Thread-12 INFO:Local step 135500, global step 2167136: loss 0.0231
[2019-04-28 02:27:46,675] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 135500, global step 2167136: learning rate 0.0000
[2019-04-28 02:27:47,190] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4048205e-13 1.0000000e+00 1.7259681e-22 5.2142486e-09 1.9563561e-11], sum to 1.0000
[2019-04-28 02:27:47,198] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9021
[2019-04-28 02:27:47,204] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.13333333333333, 75.33333333333333, 1.0, 2.0, 0.8720888546057197, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1228875.472310606, 1228875.472310606, 263832.6591274828], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1947000.0000, 
sim time next is 1947600.0000, 
raw observation next is [27.2, 75.0, 1.0, 2.0, 0.8617270084472232, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1213619.04219943, 1213619.042199431, 260964.5555358225], 
processed observation next is [1.0, 0.5652173913043478, 0.4881516587677725, 0.75, 1.0, 1.0, 0.8334060342737628, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.33711640061095277, 0.33711640061095305, 0.3894993366206306], 
reward next is 0.6105, 
noisyNet noise sample is [array([0.5450884], dtype=float32), 0.14126208]. 
=============================================
[2019-04-28 02:27:47,750] A3C_AGENT_WORKER-Thread-13 INFO:Local step 135500, global step 2167575: loss 0.0060
[2019-04-28 02:27:47,755] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 135500, global step 2167576: learning rate 0.0000
[2019-04-28 02:27:49,095] A3C_AGENT_WORKER-Thread-21 INFO:Local step 135500, global step 2168124: loss 0.0014
[2019-04-28 02:27:49,098] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 135500, global step 2168124: learning rate 0.0000
[2019-04-28 02:27:50,073] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.2561532e-12 1.0000000e+00 2.7025782e-21 1.6671567e-08 1.4609391e-12], sum to 1.0000
[2019-04-28 02:27:50,088] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2273
[2019-04-28 02:27:50,092] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.43333333333333, 90.0, 1.0, 2.0, 0.9086910339732761, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1270096.036179465, 1270096.036179465, 272336.1166628831], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1678800.0000, 
sim time next is 1679400.0000, 
raw observation next is [25.55, 89.5, 1.0, 2.0, 0.9582083846393582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1339351.106769004, 1339351.106769004, 286447.7388908998], 
processed observation next is [1.0, 0.43478260869565216, 0.40995260663507116, 0.895, 1.0, 1.0, 0.9496486561919978, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.37204197410250106, 0.37204197410250106, 0.42753393864313405], 
reward next is 0.5725, 
noisyNet noise sample is [array([-1.9019973], dtype=float32), -1.4772209]. 
=============================================
[2019-04-28 02:27:51,237] A3C_AGENT_WORKER-Thread-16 INFO:Local step 135500, global step 2169016: loss 0.0974
[2019-04-28 02:27:51,238] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 135500, global step 2169017: learning rate 0.0000
[2019-04-28 02:27:52,151] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8560163e-11 9.9999714e-01 1.2778428e-19 2.8786114e-06 5.7842935e-09], sum to 1.0000
[2019-04-28 02:27:52,158] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4667
[2019-04-28 02:27:52,167] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1756405.504503039 W.
[2019-04-28 02:27:52,171] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [27.05, 84.0, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 7.337521059127193, 6.9112, 168.9110588598789, 1756405.504503039, 1453962.070154995, 311348.8603486515], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 1866600.0000, 
sim time next is 1867200.0000, 
raw observation next is [27.03333333333333, 84.33333333333334, 1.0, 2.0, 0.6107991630605679, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 6.915180456977593, 6.9112, 168.9125635528272, 1707804.789231151, 1704980.922623924, 367654.9032134871], 
processed observation next is [1.0, 0.6086956521739131, 0.48025276461295413, 0.8433333333333334, 1.0, 1.0, 0.531083328988636, 0.0, 1.0, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.0003980456977592972, 0.0, 0.8294380155505713, 0.4743902192308753, 0.4736058118399789, 0.5487386615126674], 
reward next is 0.4314, 
noisyNet noise sample is [array([-1.085937], dtype=float32), 0.879999]. 
=============================================
[2019-04-28 02:27:52,211] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.1311374e-12 9.9999976e-01 9.4493098e-21 2.3633559e-07 2.7195652e-09], sum to 1.0000
[2019-04-28 02:27:52,217] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1452
[2019-04-28 02:27:52,220] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 82.83333333333334, 1.0, 2.0, 0.5465004769424989, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 784310.8514507177, 784310.8514507182, 193833.0144228241], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1930200.0000, 
sim time next is 1930800.0000, 
raw observation next is [25.6, 82.66666666666667, 1.0, 2.0, 0.8095534257718462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1160609.468294198, 1160609.468294198, 250365.9228827418], 
processed observation next is [1.0, 0.34782608695652173, 0.4123222748815167, 0.8266666666666667, 1.0, 1.0, 0.7705462961106581, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3223915189706105, 0.3223915189706105, 0.37368048191454], 
reward next is 0.6263, 
noisyNet noise sample is [array([-0.50045156], dtype=float32), -0.06621508]. 
=============================================
[2019-04-28 02:27:52,976] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136000, global step 2169983: loss 0.0002
[2019-04-28 02:27:52,978] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136000, global step 2169983: learning rate 0.0000
[2019-04-28 02:27:53,788] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.6299047e-15 1.0000000e+00 3.1291998e-23 2.8922322e-08 3.3751021e-10], sum to 1.0000
[2019-04-28 02:27:53,796] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-28 02:27:53,800] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.7, 85.5, 1.0, 2.0, 0.5047805089321843, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 705354.5522846185, 705354.5522846185, 184348.6271707236], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2046600.0000, 
sim time next is 2047200.0000, 
raw observation next is [26.63333333333333, 85.66666666666666, 1.0, 2.0, 0.5032134804873083, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 703164.1418835776, 703164.1418835782, 184101.2471757254], 
processed observation next is [0.0, 0.6956521739130435, 0.46129541864139006, 0.8566666666666666, 1.0, 1.0, 0.40146202468350395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1953233727454382, 0.19532337274543837, 0.27477798085929167], 
reward next is 0.7252, 
noisyNet noise sample is [array([0.2865408], dtype=float32), 0.5552237]. 
=============================================
[2019-04-28 02:27:54,794] A3C_AGENT_WORKER-Thread-19 INFO:Local step 135500, global step 2170887: loss 0.0009
[2019-04-28 02:27:54,797] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 135500, global step 2170887: learning rate 0.0000
[2019-04-28 02:27:56,377] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136000, global step 2171651: loss 0.0343
[2019-04-28 02:27:56,379] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136000, global step 2171651: learning rate 0.0000
[2019-04-28 02:27:57,114] A3C_AGENT_WORKER-Thread-3 INFO:Local step 135500, global step 2171998: loss 0.0433
[2019-04-28 02:27:57,118] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 135500, global step 2172000: learning rate 0.0000
[2019-04-28 02:27:57,373] A3C_AGENT_WORKER-Thread-22 INFO:Local step 135500, global step 2172125: loss 0.0007
[2019-04-28 02:27:57,374] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 135500, global step 2172125: learning rate 0.0000
[2019-04-28 02:27:57,395] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136000, global step 2172133: loss 0.0076
[2019-04-28 02:27:57,402] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136000, global step 2172135: learning rate 0.0000
[2019-04-28 02:27:58,418] A3C_AGENT_WORKER-Thread-15 INFO:Local step 135500, global step 2172623: loss 0.0889
[2019-04-28 02:27:58,420] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 135500, global step 2172624: learning rate 0.0000
[2019-04-28 02:27:59,018] A3C_AGENT_WORKER-Thread-17 INFO:Local step 135500, global step 2172889: loss 0.0474
[2019-04-28 02:27:59,031] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 135500, global step 2172892: learning rate 0.0000
[2019-04-28 02:28:01,234] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136000, global step 2173934: loss 0.0002
[2019-04-28 02:28:01,237] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136000, global step 2173934: learning rate 0.0000
[2019-04-28 02:28:02,210] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136000, global step 2174401: loss 0.0009
[2019-04-28 02:28:02,213] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136000, global step 2174402: learning rate 0.0000
[2019-04-28 02:28:02,246] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136000, global step 2174414: loss 0.0027
[2019-04-28 02:28:02,250] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136000, global step 2174415: learning rate 0.0000
[2019-04-28 02:28:02,655] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136000, global step 2174607: loss 0.0249
[2019-04-28 02:28:02,658] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136000, global step 2174608: learning rate 0.0000
[2019-04-28 02:28:03,503] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-28 02:28:03,505] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:28:03,508] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:28:03,509] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:28:03,512] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:28:03,512] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:28:03,514] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:28:03,511] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:28:03,512] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:28:03,517] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:28:03,518] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:28:03,537] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run88
[2019-04-28 02:28:03,574] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run88
[2019-04-28 02:28:03,574] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run88
[2019-04-28 02:28:03,644] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run88
[2019-04-28 02:28:03,678] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run88
[2019-04-28 02:28:07,282] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:07,283] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [20.03333333333333, 94.83333333333334, 1.0, 2.0, 0.2842843622234943, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457962.4109121758, 457962.4109121758, 164134.9558894765]
[2019-04-28 02:28:07,285] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:28:07,289] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.4569658e-14 1.0000000e+00 3.1504582e-24 4.9513114e-09 1.8708038e-10], sampled 0.937476088324131
[2019-04-28 02:28:27,165] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:27,166] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.60324728666667, 96.4763229, 1.0, 2.0, 0.4905050515693808, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 685400.3204768242, 685400.3204768236, 182121.3751060241]
[2019-04-28 02:28:27,167] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:28:27,169] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.8168904e-14 1.0000000e+00 3.7499059e-24 5.2655293e-09 2.0108347e-10], sampled 0.5595802417838502
[2019-04-28 02:28:33,084] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:33,085] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.1, 92.0, 1.0, 2.0, 0.3915005669984053, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 587213.2836973852, 587213.2836973852, 173454.0543112863]
[2019-04-28 02:28:33,086] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:28:33,088] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.9367391e-14 1.0000000e+00 5.8960332e-24 6.1783769e-09 2.4254418e-10], sampled 0.3994122405687198
[2019-04-28 02:28:33,234] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:33,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.64881268333333, 92.55701820666667, 1.0, 2.0, 0.4948294817721328, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 694879.9217437122, 694879.9217437122, 183231.0059113313]
[2019-04-28 02:28:33,237] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:28:33,239] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0643440e-14 1.0000000e+00 2.5480314e-24 4.5933946e-09 1.7136448e-10], sampled 0.09191560557822798
[2019-04-28 02:28:40,657] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:40,658] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.7, 83.0, 1.0, 2.0, 0.7860098485909752, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1098533.330628257, 1098533.330628258, 240505.3354368378]
[2019-04-28 02:28:40,660] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:28:40,663] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.6170007e-14 1.0000000e+00 1.2646246e-23 8.0907414e-09 3.3267838e-10], sampled 0.08084437592835336
[2019-04-28 02:28:59,329] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:28:59,329] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [37.5, 45.0, 1.0, 2.0, 0.8490387934155987, 1.0, 1.0, 0.8490387934155987, 0.0, 1.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2374641.273434682, 2374641.273434682, 444170.1128538491]
[2019-04-28 02:28:59,331] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:28:59,334] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.5290359e-13 1.0000000e+00 4.1355204e-22 2.7744280e-08 1.4101658e-09], sampled 0.010286679844347502
[2019-04-28 02:28:59,336] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2374641.273434682 W.
[2019-04-28 02:29:02,529] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11167686]
[2019-04-28 02:29:02,530] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 84.0, 1.0, 2.0, 0.5364795520307343, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 749664.8095138671, 749664.8095138664, 189509.7604375454]
[2019-04-28 02:29:02,531] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:29:02,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.7478087e-14 1.0000000e+00 3.6314431e-24 5.2061297e-09 1.9842905e-10], sampled 0.2413596234449792
[2019-04-28 02:29:32,945] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:29:33,078] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:29:33,117] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:29:33,260] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:29:33,262] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:29:34,274] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2175000, evaluation results [2175000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:29:34,870] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136000, global step 2175289: loss 0.0003
[2019-04-28 02:29:34,872] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136000, global step 2175290: learning rate 0.0000
[2019-04-28 02:29:35,852] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136000, global step 2175771: loss 0.0013
[2019-04-28 02:29:35,856] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136000, global step 2175771: learning rate 0.0000
[2019-04-28 02:29:36,455] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136000, global step 2176064: loss 0.0048
[2019-04-28 02:29:36,460] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136000, global step 2176064: learning rate 0.0000
[2019-04-28 02:29:36,951] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.5517787e-11 1.0000000e+00 1.0727004e-22 1.0196413e-08 4.9756582e-10], sum to 1.0000
[2019-04-28 02:29:36,958] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1240
[2019-04-28 02:29:36,966] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.93333333333333, 97.33333333333334, 1.0, 2.0, 0.4583683932920895, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 648304.8621784303, 648304.8621784303, 178340.272852537], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2089200.0000, 
sim time next is 2089800.0000, 
raw observation next is [23.9, 97.5, 1.0, 2.0, 0.4569873957359317, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 646711.1113267598, 646711.1113267604, 178184.7499755667], 
processed observation next is [0.0, 0.17391304347826086, 0.33175355450236965, 0.975, 1.0, 1.0, 0.3457679466697972, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1796419753685444, 0.17964197536854454, 0.26594738802323387], 
reward next is 0.7341, 
noisyNet noise sample is [array([0.2793905], dtype=float32), 0.009866894]. 
=============================================
[2019-04-28 02:29:37,271] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [6.1718651e-11 9.9999928e-01 7.2274608e-18 6.0960178e-07 1.0238408e-07], sum to 1.0000
[2019-04-28 02:29:37,279] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.2521
[2019-04-28 02:29:37,284] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.45, 78.83333333333333, 1.0, 2.0, 0.6725429737793114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 939880.9882351824, 939880.988235183, 215027.8458054866], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2271000.0000, 
sim time next is 2271600.0000, 
raw observation next is [27.6, 78.0, 1.0, 2.0, 0.6397260890291292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 893999.9828443662, 893999.9828443662, 208370.7297452536], 
processed observation next is [1.0, 0.30434782608695654, 0.5071090047393366, 0.78, 1.0, 1.0, 0.5659350470230472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2483333285678795, 0.2483333285678795, 0.3110010891720203], 
reward next is 0.6890, 
noisyNet noise sample is [array([0.21272343], dtype=float32), -0.97119206]. 
=============================================
[2019-04-28 02:29:38,116] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136000, global step 2176876: loss 0.0008
[2019-04-28 02:29:38,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136000, global step 2176877: learning rate 0.0000
[2019-04-28 02:29:39,287] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.1681928e-11 9.9999750e-01 3.0060066e-18 2.5331087e-06 1.3878133e-08], sum to 1.0000
[2019-04-28 02:29:39,297] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.0661
[2019-04-28 02:29:39,300] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.55, 94.00000000000001, 1.0, 2.0, 0.5093806351527911, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 711784.6877669791, 711784.6877669791, 185078.8189309198], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2164200.0000, 
sim time next is 2164800.0000, 
raw observation next is [25.5, 94.0, 1.0, 2.0, 0.5079720506626362, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 709815.7400262054, 709815.7400262054, 184854.4702663285], 
processed observation next is [1.0, 0.043478260869565216, 0.40758293838862564, 0.94, 1.0, 1.0, 0.4071952417622122, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1971710388961682, 0.1971710388961682, 0.275902194427356], 
reward next is 0.7241, 
noisyNet noise sample is [array([0.45443386], dtype=float32), -0.79033476]. 
=============================================
[2019-04-28 02:29:39,490] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.2051306e-15 1.0000000e+00 6.7102717e-25 2.2143603e-08 6.4971820e-11], sum to 1.0000
[2019-04-28 02:29:39,499] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.0728
[2019-04-28 02:29:39,503] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.6, 94.0, 1.0, 2.0, 0.5037625457317685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 703931.6310894974, 703931.6310894967, 184188.0953561375], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2016000.0000, 
sim time next is 2016600.0000, 
raw observation next is [25.6, 94.00000000000001, 1.0, 2.0, 0.5044118205129783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704839.1949613115, 704839.1949613115, 184290.5711192366], 
processed observation next is [0.0, 0.34782608695652173, 0.4123222748815167, 0.9400000000000002, 1.0, 1.0, 0.4029058078469618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19578866526703098, 0.19578866526703098, 0.2750605539093084], 
reward next is 0.7249, 
noisyNet noise sample is [array([1.1495787], dtype=float32), -0.72753114]. 
=============================================
[2019-04-28 02:29:39,787] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.2032486e-10 9.9999166e-01 7.4507433e-18 8.3846762e-06 4.4201709e-08], sum to 1.0000
[2019-04-28 02:29:39,799] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8074
[2019-04-28 02:29:39,803] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 95.66666666666667, 1.0, 2.0, 0.6136397413985974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 857530.2976508441, 857530.2976508441, 203305.9632519845], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2173200.0000, 
sim time next is 2173800.0000, 
raw observation next is [24.83333333333334, 95.83333333333333, 1.0, 2.0, 0.6042654707323429, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 844425.0241070074, 844425.0241070081, 201535.2731236075], 
processed observation next is [1.0, 0.13043478260869565, 0.3759873617693526, 0.9583333333333333, 1.0, 1.0, 0.523211410520895, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.23456250669639095, 0.23456250669639114, 0.30079891510986195], 
reward next is 0.6992, 
noisyNet noise sample is [array([1.9577955], dtype=float32), 0.23132461]. 
=============================================
[2019-04-28 02:29:40,708] A3C_AGENT_WORKER-Thread-18 INFO:Local step 136500, global step 2178110: loss 0.0166
[2019-04-28 02:29:40,710] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 136500, global step 2178110: learning rate 0.0000
[2019-04-28 02:29:42,154] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136000, global step 2178821: loss 0.0127
[2019-04-28 02:29:42,156] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136000, global step 2178822: learning rate 0.0000
[2019-04-28 02:29:44,262] A3C_AGENT_WORKER-Thread-20 INFO:Local step 136500, global step 2179830: loss 0.0099
[2019-04-28 02:29:44,263] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 136500, global step 2179830: learning rate 0.0000
[2019-04-28 02:29:44,586] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136000, global step 2179990: loss 0.0005
[2019-04-28 02:29:44,589] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136000, global step 2179992: learning rate 0.0000
[2019-04-28 02:29:44,630] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136000, global step 2180018: loss 0.0005
[2019-04-28 02:29:44,633] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136000, global step 2180018: learning rate 0.0000
[2019-04-28 02:29:45,194] A3C_AGENT_WORKER-Thread-2 INFO:Local step 136500, global step 2180290: loss 0.0095
[2019-04-28 02:29:45,198] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 136500, global step 2180295: learning rate 0.0000
[2019-04-28 02:29:45,803] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136000, global step 2180573: loss 0.0055
[2019-04-28 02:29:45,804] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136000, global step 2180573: learning rate 0.0000
[2019-04-28 02:29:46,206] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136000, global step 2180762: loss 0.0341
[2019-04-28 02:29:46,208] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136000, global step 2180763: learning rate 0.0000
[2019-04-28 02:29:48,715] A3C_AGENT_WORKER-Thread-11 INFO:Local step 136500, global step 2181981: loss 0.1709
[2019-04-28 02:29:48,717] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 136500, global step 2181981: learning rate 0.0000
[2019-04-28 02:29:49,637] A3C_AGENT_WORKER-Thread-9 INFO:Local step 136500, global step 2182437: loss 0.0309
[2019-04-28 02:29:49,642] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 136500, global step 2182439: learning rate 0.0000
[2019-04-28 02:29:49,678] A3C_AGENT_WORKER-Thread-14 INFO:Local step 136500, global step 2182451: loss 0.0114
[2019-04-28 02:29:49,681] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 136500, global step 2182451: learning rate 0.0000
[2019-04-28 02:29:50,100] A3C_AGENT_WORKER-Thread-10 INFO:Local step 136500, global step 2182659: loss 0.1848
[2019-04-28 02:29:50,100] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 136500, global step 2182659: learning rate 0.0000
[2019-04-28 02:29:51,391] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.0382969e-11 9.8001820e-01 1.2564230e-19 1.9981828e-02 2.8274533e-08], sum to 1.0000
[2019-04-28 02:29:51,397] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.4848
[2019-04-28 02:29:51,403] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1721899.189899638 W.
[2019-04-28 02:29:51,409] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.5, 67.0, 1.0, 2.0, 0.4105605174770054, 1.0, 2.0, 0.4105605174770054, 1.0, 2.0, 0.7130078444196738, 6.911199999999999, 6.9112, 170.5573041426782, 1721899.189899638, 1721899.189899639, 358811.4497126309], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 2206800.0000, 
sim time next is 2207400.0000, 
raw observation next is [31.6, 66.66666666666667, 1.0, 2.0, 0.4521792652551813, 1.0, 2.0, 0.4521792652551813, 1.0, 2.0, 0.7852858457801558, 6.9112, 6.9112, 170.5573041426782, 1896603.547320803, 1896603.547320803, 383652.7471810937], 
processed observation next is [1.0, 0.5652173913043478, 0.6966824644549764, 0.6666666666666667, 1.0, 1.0, 0.33997501837973654, 1.0, 1.0, 0.33997501837973654, 1.0, 1.0, 0.7381534704636046, 0.0, 0.0, 0.8375144448122397, 0.526834318700223, 0.526834318700223, 0.5726160405687966], 
reward next is 0.4274, 
noisyNet noise sample is [array([1.2542621], dtype=float32), -0.3010088]. 
=============================================
[2019-04-28 02:29:51,443] A3C_AGENT_WORKER-Thread-12 INFO:Local step 136500, global step 2183301: loss 0.0888
[2019-04-28 02:29:51,445] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 136500, global step 2183301: learning rate 0.0000
[2019-04-28 02:29:52,544] A3C_AGENT_WORKER-Thread-13 INFO:Local step 136500, global step 2183827: loss 0.1429
[2019-04-28 02:29:52,547] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 136500, global step 2183827: learning rate 0.0000
[2019-04-28 02:29:52,803] A3C_AGENT_WORKER-Thread-21 INFO:Local step 136500, global step 2183948: loss 0.3280
[2019-04-28 02:29:52,812] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 136500, global step 2183949: learning rate 0.0000
[2019-04-28 02:29:54,740] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.5401048e-11 9.9743783e-01 3.4207537e-21 2.5621441e-03 4.0040233e-09], sum to 1.0000
[2019-04-28 02:29:54,747] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3895
[2019-04-28 02:29:54,752] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.86666666666667, 87.66666666666667, 1.0, 2.0, 0.5300133249570241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 740625.8949625496, 740625.8949625501, 188431.7795252927], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2582400.0000, 
sim time next is 2583000.0000, 
raw observation next is [26.75, 88.0, 1.0, 2.0, 0.527408824582229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736985.1759527229, 736985.1759527235, 188001.5331280662], 
processed observation next is [1.0, 0.9130434782608695, 0.4668246445497631, 0.88, 1.0, 1.0, 0.43061304166533615, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20471810443131191, 0.20471810443131208, 0.2805993031762182], 
reward next is 0.7194, 
noisyNet noise sample is [array([-2.1045299], dtype=float32), 0.70809495]. 
=============================================
[2019-04-28 02:29:54,773] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[67.3093 ]
 [67.32582]
 [67.46404]
 [67.13215]
 [67.37117]], R is [[67.4882431 ]
 [67.53211975]
 [67.57515717]
 [67.61740112]
 [67.65919495]].
[2019-04-28 02:29:55,016] A3C_AGENT_WORKER-Thread-16 INFO:Local step 136500, global step 2185006: loss 0.0251
[2019-04-28 02:29:55,019] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 136500, global step 2185007: learning rate 0.0000
[2019-04-28 02:29:55,892] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [3.0433355e-14 9.9903190e-01 3.4331646e-23 9.6813374e-04 2.9065324e-13], sum to 1.0000
[2019-04-28 02:29:55,899] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-28 02:29:55,904] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.66666666666667, 1.0, 2.0, 0.4425105747072061, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 633717.9702613762, 633717.9702613757, 177062.6729520806], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2690400.0000, 
sim time next is 2691000.0000, 
raw observation next is [24.0, 95.0, 1.0, 2.0, 0.4449135388437176, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 636019.2870487001, 636019.2870487001, 177263.3433444242], 
processed observation next is [0.0, 0.13043478260869565, 0.3364928909952607, 0.95, 1.0, 1.0, 0.3312211311370092, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17667202418019448, 0.17667202418019448, 0.26457215424540925], 
reward next is 0.7354, 
noisyNet noise sample is [array([0.44179338], dtype=float32), -2.2758489]. 
=============================================
[2019-04-28 02:29:55,921] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[74.4527  ]
 [74.43321 ]
 [74.365295]
 [74.31538 ]
 [74.37297 ]], R is [[74.46431732]
 [74.45539856]
 [74.44670105]
 [74.4381485 ]
 [74.42957306]].
[2019-04-28 02:29:57,040] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137000, global step 2185989: loss 0.0017
[2019-04-28 02:29:57,045] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137000, global step 2185991: learning rate 0.0000
[2019-04-28 02:29:58,859] A3C_AGENT_WORKER-Thread-19 INFO:Local step 136500, global step 2186864: loss 0.0790
[2019-04-28 02:29:58,861] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 136500, global step 2186865: learning rate 0.0000
[2019-04-28 02:30:00,462] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137000, global step 2187638: loss -2.4514
[2019-04-28 02:30:00,464] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137000, global step 2187638: learning rate 0.0000
[2019-04-28 02:30:00,852] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.4295132e-11 9.8567569e-01 2.6268617e-21 1.4322490e-02 1.8095199e-06], sum to 1.0000
[2019-04-28 02:30:00,858] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.1895
[2019-04-28 02:30:00,863] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.16666666666667, 88.16666666666667, 1.0, 2.0, 0.461440988732043, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 652397.9849566373, 652397.9849566367, 178758.6166126817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2617800.0000, 
sim time next is 2618400.0000, 
raw observation next is [25.33333333333334, 87.33333333333334, 1.0, 2.0, 0.4638925871243377, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 654358.2785045952, 654358.2785045946, 178926.8233017709], 
processed observation next is [0.0, 0.30434782608695654, 0.3996840442338076, 0.8733333333333334, 1.0, 1.0, 0.3540874543666719, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18176618847349865, 0.18176618847349849, 0.2670549601518969], 
reward next is 0.7329, 
noisyNet noise sample is [array([-1.1740433], dtype=float32), 0.628311]. 
=============================================
[2019-04-28 02:30:01,214] A3C_AGENT_WORKER-Thread-22 INFO:Local step 136500, global step 2188000: loss 0.0044
[2019-04-28 02:30:01,215] A3C_AGENT_WORKER-Thread-3 INFO:Local step 136500, global step 2188000: loss 0.0053
[2019-04-28 02:30:01,217] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 136500, global step 2188000: learning rate 0.0000
[2019-04-28 02:30:01,218] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 136500, global step 2188000: learning rate 0.0000
[2019-04-28 02:30:01,694] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137000, global step 2188240: loss 0.0048
[2019-04-28 02:30:01,696] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137000, global step 2188241: learning rate 0.0000
[2019-04-28 02:30:02,560] A3C_AGENT_WORKER-Thread-15 INFO:Local step 136500, global step 2188663: loss 0.0406
[2019-04-28 02:30:02,563] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 136500, global step 2188664: learning rate 0.0000
[2019-04-28 02:30:03,063] A3C_AGENT_WORKER-Thread-17 INFO:Local step 136500, global step 2188905: loss 0.0058
[2019-04-28 02:30:03,067] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 136500, global step 2188907: learning rate 0.0000
[2019-04-28 02:30:05,010] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137000, global step 2189854: loss 0.0171
[2019-04-28 02:30:05,012] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137000, global step 2189854: learning rate 0.0000
[2019-04-28 02:30:05,755] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [5.2971776e-12 9.9949396e-01 7.6725438e-23 5.0602033e-04 2.1154278e-08], sum to 1.0000
[2019-04-28 02:30:05,762] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.8072
[2019-04-28 02:30:05,766] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.03333333333333, 92.0, 1.0, 2.0, 0.4335841862253737, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 628352.3139503202, 628352.3139503202, 176732.0910779584], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2605200.0000, 
sim time next is 2605800.0000, 
raw observation next is [24.01666666666667, 92.0, 1.0, 2.0, 0.4326088736147158, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 627353.6987812736, 627353.6987812736, 176645.1972888044], 
processed observation next is [0.0, 0.13043478260869565, 0.33728278041074267, 0.92, 1.0, 1.0, 0.3163962332707419, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17426491632813154, 0.17426491632813154, 0.26364954819224534], 
reward next is 0.7364, 
noisyNet noise sample is [array([-1.1472], dtype=float32), 0.52695817]. 
=============================================
[2019-04-28 02:30:05,790] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137000, global step 2190235: loss 0.0113
[2019-04-28 02:30:05,791] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137000, global step 2190236: learning rate 0.0000
[2019-04-28 02:30:05,849] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1486149e-13 9.9998307e-01 2.7734232e-23 1.6906512e-05 1.7615548e-11], sum to 1.0000
[2019-04-28 02:30:05,856] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3715
[2019-04-28 02:30:05,859] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.91666666666667, 93.0, 1.0, 2.0, 0.5527946400132785, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 772471.4470041594, 772471.4470041594, 192281.2788737733], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2494200.0000, 
sim time next is 2494800.0000, 
raw observation next is [26.9, 93.0, 1.0, 2.0, 0.5517149727380124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 770962.1798130178, 770962.1798130178, 192095.428203302], 
processed observation next is [1.0, 0.9130434782608695, 0.4739336492890995, 0.93, 1.0, 1.0, 0.45989755751567757, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21415616105917162, 0.21415616105917162, 0.2867095943332866], 
reward next is 0.7133, 
noisyNet noise sample is [array([0.94327027], dtype=float32), -0.7537381]. 
=============================================
[2019-04-28 02:30:06,119] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137000, global step 2190396: loss 0.0128
[2019-04-28 02:30:06,120] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137000, global step 2190396: learning rate 0.0000
[2019-04-28 02:30:06,636] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137000, global step 2190637: loss 0.0045
[2019-04-28 02:30:06,639] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137000, global step 2190640: learning rate 0.0000
[2019-04-28 02:30:08,115] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137000, global step 2191356: loss 0.0001
[2019-04-28 02:30:08,119] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137000, global step 2191356: learning rate 0.0000
[2019-04-28 02:30:09,289] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137000, global step 2191932: loss 0.0177
[2019-04-28 02:30:09,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137000, global step 2191932: learning rate 0.0000
[2019-04-28 02:30:09,457] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137000, global step 2192010: loss 0.0010
[2019-04-28 02:30:09,463] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137000, global step 2192012: learning rate 0.0000
[2019-04-28 02:30:11,479] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137000, global step 2192980: loss 0.0005
[2019-04-28 02:30:11,481] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137000, global step 2192980: learning rate 0.0000
[2019-04-28 02:30:13,147] A3C_AGENT_WORKER-Thread-18 INFO:Local step 137500, global step 2193789: loss 0.0036
[2019-04-28 02:30:13,149] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 137500, global step 2193789: learning rate 0.0000
[2019-04-28 02:30:13,970] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [6.0783358e-11 9.9907851e-01 1.8028041e-19 9.2144235e-04 2.5420166e-08], sum to 1.0000
[2019-04-28 02:30:13,981] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.1129
[2019-04-28 02:30:13,985] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.0, 94.0, 1.0, 2.0, 0.314803195304462, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 497805.83781817, 497805.8378181707, 166861.4792520773], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2954400.0000, 
sim time next is 2955000.0000, 
raw observation next is [21.0, 94.0, 1.0, 2.0, 0.3160220621309831, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499731.7658908868, 499731.7658908868, 167005.2323440549], 
processed observation next is [1.0, 0.17391304347826086, 0.19431279620853087, 0.94, 1.0, 1.0, 0.17593019533853385, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1388143794141352, 0.1388143794141352, 0.24926154081202226], 
reward next is 0.7507, 
noisyNet noise sample is [array([0.14000058], dtype=float32), 1.5466825]. 
=============================================
[2019-04-28 02:30:14,012] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[69.32955 ]
 [69.28668 ]
 [69.303055]
 [69.322556]
 [69.20929 ]], R is [[69.37018585]
 [69.42743683]
 [69.48472595]
 [69.54161072]
 [69.59621429]].
[2019-04-28 02:30:14,602] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.6682090e-13 9.9998701e-01 3.6077060e-22 1.3001560e-05 2.4357403e-09], sum to 1.0000
[2019-04-28 02:30:14,614] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5575
[2019-04-28 02:30:14,620] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3496026625079432, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 538542.6937369745, 538542.6937369745, 169696.5313190107], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2871600.0000, 
sim time next is 2872200.0000, 
raw observation next is [22.0, 94.0, 1.0, 2.0, 0.3426318834560549, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 527812.8876769763, 527812.8876769756, 168824.2942452934], 
processed observation next is [1.0, 0.21739130434782608, 0.2417061611374408, 0.94, 1.0, 1.0, 0.20799022103139142, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1466146910213823, 0.1466146910213821, 0.2519765585750648], 
reward next is 0.7480, 
noisyNet noise sample is [array([0.7487981], dtype=float32), 1.3337018]. 
=============================================
[2019-04-28 02:30:15,517] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [5.1379240e-13 9.9996936e-01 4.0400553e-24 3.0652169e-05 9.1451041e-11], sum to 1.0000
[2019-04-28 02:30:15,527] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2844
[2019-04-28 02:30:15,533] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 80.66666666666667, 1.0, 2.0, 0.4938176322457861, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 690030.6117701499, 690030.6117701493, 182632.9445954108], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2644800.0000, 
sim time next is 2645400.0000, 
raw observation next is [27.0, 79.83333333333334, 1.0, 2.0, 0.4894840688501706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 683973.2053727263, 683973.2053727269, 181964.649621705], 
processed observation next is [0.0, 0.6086956521739131, 0.4786729857819906, 0.7983333333333335, 1.0, 1.0, 0.3849205648797236, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18999255704797954, 0.18999255704797968, 0.2715890292861269], 
reward next is 0.7284, 
noisyNet noise sample is [array([0.27813303], dtype=float32), -0.38087878]. 
=============================================
[2019-04-28 02:30:15,734] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137000, global step 2195033: loss 0.0325
[2019-04-28 02:30:15,739] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137000, global step 2195033: learning rate 0.0000
[2019-04-28 02:30:17,189] A3C_AGENT_WORKER-Thread-20 INFO:Local step 137500, global step 2195747: loss 0.0415
[2019-04-28 02:30:17,190] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 137500, global step 2195747: learning rate 0.0000
[2019-04-28 02:30:17,791] A3C_AGENT_WORKER-Thread-2 INFO:Local step 137500, global step 2196041: loss 0.0039
[2019-04-28 02:30:17,791] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 137500, global step 2196041: learning rate 0.0000
[2019-04-28 02:30:17,961] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137000, global step 2196121: loss 0.0017
[2019-04-28 02:30:17,963] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137000, global step 2196121: learning rate 0.0000
[2019-04-28 02:30:18,022] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137000, global step 2196152: loss 0.0074
[2019-04-28 02:30:18,026] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137000, global step 2196154: learning rate 0.0000
[2019-04-28 02:30:18,471] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [8.5220574e-11 9.9963522e-01 2.3298513e-21 3.6397699e-04 8.0197555e-07], sum to 1.0000
[2019-04-28 02:30:18,488] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.1069
[2019-04-28 02:30:18,493] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.0, 94.00000000000001, 1.0, 2.0, 0.4415283653261661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 634208.0951602483, 634208.0951602483, 177162.4779117545], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2686200.0000, 
sim time next is 2686800.0000, 
raw observation next is [24.0, 94.0, 1.0, 2.0, 0.442320514599603, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 635328.231742431, 635328.231742431, 177274.088662323], 
processed observation next is [0.0, 0.08695652173913043, 0.3364928909952607, 0.94, 1.0, 1.0, 0.3280970055416903, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.17648006437289748, 0.17648006437289748, 0.2645881920333179], 
reward next is 0.7354, 
noisyNet noise sample is [array([-0.65462947], dtype=float32), 0.22357064]. 
=============================================
[2019-04-28 02:30:19,163] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137000, global step 2196702: loss 0.0233
[2019-04-28 02:30:19,166] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137000, global step 2196703: learning rate 0.0000
[2019-04-28 02:30:19,936] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137000, global step 2197072: loss 0.0019
[2019-04-28 02:30:19,939] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137000, global step 2197073: learning rate 0.0000
[2019-04-28 02:30:21,469] A3C_AGENT_WORKER-Thread-11 INFO:Local step 137500, global step 2197813: loss 0.0049
[2019-04-28 02:30:21,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 137500, global step 2197813: learning rate 0.0000
[2019-04-28 02:30:22,433] A3C_AGENT_WORKER-Thread-14 INFO:Local step 137500, global step 2198271: loss 0.0099
[2019-04-28 02:30:22,436] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 137500, global step 2198272: learning rate 0.0000
[2019-04-28 02:30:22,534] A3C_AGENT_WORKER-Thread-9 INFO:Local step 137500, global step 2198323: loss 0.0065
[2019-04-28 02:30:22,535] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 137500, global step 2198323: learning rate 0.0000
[2019-04-28 02:30:22,795] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.0717573e-12 9.9996436e-01 5.9807986e-22 3.5609730e-05 7.7314249e-10], sum to 1.0000
[2019-04-28 02:30:22,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.9690
[2019-04-28 02:30:22,813] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 88.0, 1.0, 2.0, 0.553290195321537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 869356.8270014918, 869356.8270014912, 203553.8672014843], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2980200.0000, 
sim time next is 2980800.0000, 
raw observation next is [22.0, 88.0, 1.0, 2.0, 0.5808879828875657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 912708.653975482, 912708.653975482, 209038.4272906284], 
processed observation next is [1.0, 0.5217391304347826, 0.2417061611374408, 0.88, 1.0, 1.0, 0.4950457625151394, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2535301816598561, 0.2535301816598561, 0.31199765267257973], 
reward next is 0.6880, 
noisyNet noise sample is [array([-0.68042505], dtype=float32), -2.3434122]. 
=============================================
[2019-04-28 02:30:22,919] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.5269346e-14 9.9999440e-01 4.7650850e-23 5.5975875e-06 9.9731688e-11], sum to 1.0000
[2019-04-28 02:30:22,925] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7493
[2019-04-28 02:30:22,929] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3843210724165375, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 578860.9898432888, 578860.9898432888, 172773.2748123322], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3105600.0000, 
sim time next is 3106200.0000, 
raw observation next is [22.0, 100.0, 1.0, 2.0, 0.3849212062235661, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 579764.8643923954, 579764.8643923948, 172854.1836680601], 
processed observation next is [1.0, 0.9565217391304348, 0.2417061611374408, 1.0, 1.0, 1.0, 0.2589412123175496, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16104579566455426, 0.1610457956645541, 0.2579913189075524], 
reward next is 0.7420, 
noisyNet noise sample is [array([1.1448542], dtype=float32), 1.9191314]. 
=============================================
[2019-04-28 02:30:23,075] A3C_AGENT_WORKER-Thread-10 INFO:Local step 137500, global step 2198583: loss 0.0008
[2019-04-28 02:30:23,080] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 137500, global step 2198583: learning rate 0.0000
[2019-04-28 02:30:24,652] A3C_AGENT_WORKER-Thread-12 INFO:Local step 137500, global step 2199347: loss 0.0183
[2019-04-28 02:30:24,654] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 137500, global step 2199347: learning rate 0.0000
[2019-04-28 02:30:25,838] A3C_AGENT_WORKER-Thread-21 INFO:Local step 137500, global step 2199925: loss 0.0010
[2019-04-28 02:30:25,841] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 137500, global step 2199925: learning rate 0.0000
[2019-04-28 02:30:25,975] A3C_AGENT_WORKER-Thread-13 INFO:Local step 137500, global step 2199988: loss 0.0030
[2019-04-28 02:30:25,976] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 137500, global step 2199988: learning rate 0.0000
[2019-04-28 02:30:25,997] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 02:30:26,002] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:30:26,006] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:30:26,006] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:30:26,007] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:30:26,008] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:30:26,008] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:30:26,009] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:30:26,009] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:30:26,011] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:30:26,012] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:30:26,020] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run89
[2019-04-28 02:30:26,050] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run89
[2019-04-28 02:30:26,074] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run89
[2019-04-28 02:30:26,099] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run89
[2019-04-28 02:30:26,136] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run89
[2019-04-28 02:30:53,877] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11549971]
[2019-04-28 02:30:53,881] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.5, 93.0, 1.0, 2.0, 0.7620022220373106, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1064963.264275522, 1064963.264275522, 234797.8168785044]
[2019-04-28 02:30:53,881] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:30:53,883] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7559195e-13 9.9999952e-01 2.4124185e-23 4.7519967e-07 9.4763565e-11], sampled 0.6470042387539485
[2019-04-28 02:31:16,703] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11549971]
[2019-04-28 02:31:16,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [32.0, 71.0, 1.0, 2.0, 0.6030353442904537, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 842705.3136804615, 842705.3136804615, 201313.362568434]
[2019-04-28 02:31:16,704] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:31:16,707] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3366436e-13 9.9999964e-01 1.4914619e-23 4.0996474e-07 7.5758753e-11], sampled 0.3670514050646052
[2019-04-28 02:31:32,024] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11549971]
[2019-04-28 02:31:32,026] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [33.8, 53.0, 1.0, 2.0, 0.5415173487612382, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756707.0241387336, 756707.0241387343, 190356.8480943336]
[2019-04-28 02:31:32,027] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:31:32,029] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3526704e-13 9.9999964e-01 1.5133664e-23 4.1661659e-07 7.6963373e-11], sampled 0.8043165338606182
[2019-04-28 02:31:51,929] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11549971]
[2019-04-28 02:31:51,932] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.53333333333333, 85.66666666666667, 1.0, 2.0, 0.5722948888604447, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799731.2195721186, 799731.2195721186, 195698.1135647662]
[2019-04-28 02:31:51,933] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:31:51,935] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.0567853e-13 9.9999952e-01 3.5855640e-23 4.3455412e-07 9.7319576e-11], sampled 0.5701634417846312
[2019-04-28 02:31:55,024] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11549971]
[2019-04-28 02:31:55,024] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.26106240833333, 96.11548840833333, 1.0, 2.0, 0.505834386358515, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706827.6768244195, 706827.6768244195, 184513.0554092159]
[2019-04-28 02:31:55,025] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:31:55,028] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.1594018e-13 9.9999964e-01 1.3286608e-23 3.1025724e-07 5.9923018e-11], sampled 0.2228495304254653
[2019-04-28 02:31:58,134] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:31:58,225] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:31:58,280] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:31:58,458] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:31:58,670] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:31:59,683] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2200000, evaluation results [2200000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:32:01,306] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2611316e-16 1.0000000e+00 4.3527780e-29 2.7652630e-11 1.8678540e-15], sum to 1.0000
[2019-04-28 02:32:01,317] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.3522
[2019-04-28 02:32:01,323] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 94.0, 1.0, 2.0, 0.4874377884848182, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 681112.9495931175, 681112.9495931175, 181651.1779037188], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3195000.0000, 
sim time next is 3195600.0000, 
raw observation next is [25.0, 94.0, 1.0, 2.0, 0.4862582826163963, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 679464.2599147927, 679464.259914792, 181471.0371673381], 
processed observation next is [1.0, 1.0, 0.38388625592417064, 0.94, 1.0, 1.0, 0.38103407544144136, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18874007219855354, 0.18874007219855335, 0.2708522942796091], 
reward next is 0.7291, 
noisyNet noise sample is [array([1.8786321], dtype=float32), 1.490846]. 
=============================================
[2019-04-28 02:32:02,116] A3C_AGENT_WORKER-Thread-16 INFO:Local step 137500, global step 2200999: loss 0.0051
[2019-04-28 02:32:02,117] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 137500, global step 2200999: learning rate 0.0000
[2019-04-28 02:32:04,437] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138000, global step 2201956: loss 0.0001
[2019-04-28 02:32:04,439] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138000, global step 2201956: learning rate 0.0000
[2019-04-28 02:32:07,340] A3C_AGENT_WORKER-Thread-19 INFO:Local step 137500, global step 2203131: loss 0.0206
[2019-04-28 02:32:07,340] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 137500, global step 2203131: learning rate 0.0000
[2019-04-28 02:32:08,951] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138000, global step 2203805: loss 0.0001
[2019-04-28 02:32:08,959] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138000, global step 2203805: learning rate 0.0000
[2019-04-28 02:32:09,491] A3C_AGENT_WORKER-Thread-22 INFO:Local step 137500, global step 2204034: loss 0.0009
[2019-04-28 02:32:09,523] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 137500, global step 2204042: learning rate 0.0000
[2019-04-28 02:32:09,652] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138000, global step 2204088: loss 0.0018
[2019-04-28 02:32:09,653] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138000, global step 2204088: learning rate 0.0000
[2019-04-28 02:32:09,957] A3C_AGENT_WORKER-Thread-3 INFO:Local step 137500, global step 2204211: loss 0.0289
[2019-04-28 02:32:09,958] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 137500, global step 2204211: learning rate 0.0000
[2019-04-28 02:32:10,008] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.3063804e-16 1.0000000e+00 2.9994726e-26 2.0747881e-10 5.8247942e-14], sum to 1.0000
[2019-04-28 02:32:10,015] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4453
[2019-04-28 02:32:10,021] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.0, 100.0, 1.0, 2.0, 0.3044939415586104, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484889.0046768906, 484889.0046768906, 165973.4332609786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3012000.0000, 
sim time next is 3012600.0000, 
raw observation next is [20.0, 100.0, 1.0, 2.0, 0.3046781921487323, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485182.2398720158, 485182.2398720158, 165994.6212026615], 
processed observation next is [1.0, 0.8695652173913043, 0.1469194312796209, 1.0, 1.0, 1.0, 0.1622628821069064, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13477284440889328, 0.13477284440889328, 0.24775316597412167], 
reward next is 0.7522, 
noisyNet noise sample is [array([-0.52076304], dtype=float32), -1.6341102]. 
=============================================
[2019-04-28 02:32:11,212] A3C_AGENT_WORKER-Thread-15 INFO:Local step 137500, global step 2204748: loss 0.0009
[2019-04-28 02:32:11,217] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 137500, global step 2204748: learning rate 0.0000
[2019-04-28 02:32:11,635] A3C_AGENT_WORKER-Thread-17 INFO:Local step 137500, global step 2204927: loss 0.0080
[2019-04-28 02:32:11,638] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 137500, global step 2204928: learning rate 0.0000
[2019-04-28 02:32:13,460] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138000, global step 2205658: loss 0.0143
[2019-04-28 02:32:13,462] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138000, global step 2205658: learning rate 0.0000
[2019-04-28 02:32:14,633] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138000, global step 2206136: loss 0.0012
[2019-04-28 02:32:14,635] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138000, global step 2206136: learning rate 0.0000
[2019-04-28 02:32:14,835] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138000, global step 2206223: loss 0.0132
[2019-04-28 02:32:14,842] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138000, global step 2206223: learning rate 0.0000
[2019-04-28 02:32:15,824] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138000, global step 2206630: loss 0.0080
[2019-04-28 02:32:15,825] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138000, global step 2206630: learning rate 0.0000
[2019-04-28 02:32:17,869] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138000, global step 2207457: loss 0.0050
[2019-04-28 02:32:17,870] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138000, global step 2207457: learning rate 0.0000
[2019-04-28 02:32:19,070] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138000, global step 2207922: loss 0.0051
[2019-04-28 02:32:19,071] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138000, global step 2207923: learning rate 0.0000
[2019-04-28 02:32:19,368] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138000, global step 2208042: loss 0.0011
[2019-04-28 02:32:19,371] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138000, global step 2208042: learning rate 0.0000
[2019-04-28 02:32:21,402] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138000, global step 2208873: loss 0.0001
[2019-04-28 02:32:21,405] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138000, global step 2208874: learning rate 0.0000
[2019-04-28 02:32:21,994] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [1.5853785e-14 9.9999964e-01 1.1109376e-24 3.7340814e-07 2.5192662e-10], sum to 1.0000
[2019-04-28 02:32:21,998] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.5007
[2019-04-28 02:32:22,004] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 74.83333333333334, 1.0, 2.0, 0.5192834623208975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 725627.1618463501, 725627.1618463501, 186672.8798474257], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3438600.0000, 
sim time next is 3439200.0000, 
raw observation next is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.5178026954821553, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 723557.2892918305, 723557.2892918298, 186432.7597208667], 
processed observation next is [1.0, 0.8260869565217391, 0.5576619273301741, 0.7566666666666667, 1.0, 1.0, 0.419039392147175, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20098813591439735, 0.20098813591439715, 0.27825785032965183], 
reward next is 0.7217, 
noisyNet noise sample is [array([0.30210793], dtype=float32), 1.2367141]. 
=============================================
[2019-04-28 02:32:23,486] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.7645429e-12 1.0000000e+00 2.3044117e-19 2.3367881e-12 2.8006553e-12], sum to 1.0000
[2019-04-28 02:32:23,496] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6692
[2019-04-28 02:32:23,496] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 1848552.030041358 W.
[2019-04-28 02:32:23,510] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [26.2, 92.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 7.467326388646963, 6.9112, 168.90939050433, 1848552.030041358, 1454025.15877248, 311349.6983082551], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3378000.0000, 
sim time next is 3378600.0000, 
raw observation next is [26.15, 93.0, 1.0, 2.0, 0.345853763119632, 1.0, 1.0, 0.345853763119632, 1.0, 1.0, 0.5938074778789731, 6.9112, 6.9112, 170.5573041426782, 1450334.115792167, 1450334.115792167, 324137.710339089], 
processed observation next is [1.0, 0.08695652173913043, 0.43838862559241704, 0.93, 1.0, 1.0, 0.21187200375859278, 1.0, 0.5, 0.21187200375859278, 1.0, 0.5, 0.5046432657060648, 0.0, 0.0, 0.8375144448122397, 0.40287058772004636, 0.40287058772004636, 0.4837876273717746], 
reward next is 0.5162, 
noisyNet noise sample is [array([0.6243652], dtype=float32), -0.54773426]. 
=============================================
[2019-04-28 02:32:24,188] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [2.0979590e-13 9.9999905e-01 1.0126249e-22 9.5817325e-07 4.9501048e-10], sum to 1.0000
[2019-04-28 02:32:24,191] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6296
[2019-04-28 02:32:24,196] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.83333333333333, 71.5, 1.0, 2.0, 0.5325943095267229, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 744233.7545501533, 744233.7545501539, 188860.9576867862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3235800.0000, 
sim time next is 3236400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5304637583312806, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 741255.5376612237, 741255.537661223, 188507.1194313451], 
processed observation next is [0.0, 0.4782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.43429368473648255, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20590431601700657, 0.20590431601700637, 0.28135390959902257], 
reward next is 0.7186, 
noisyNet noise sample is [array([1.4793855], dtype=float32), -0.21232755]. 
=============================================
[2019-04-28 02:32:24,464] A3C_AGENT_WORKER-Thread-18 INFO:Local step 138500, global step 2210142: loss 0.1303
[2019-04-28 02:32:24,466] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 138500, global step 2210142: learning rate 0.0000
[2019-04-28 02:32:25,847] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.1748755e-13 9.9999702e-01 6.2152824e-24 3.2233103e-08 2.9876151e-06], sum to 1.0000
[2019-04-28 02:32:25,853] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6543
[2019-04-28 02:32:25,872] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.421087259009552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 616551.6015760098, 616551.6015760098, 175763.073442786], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3304800.0000, 
sim time next is 3305400.0000, 
raw observation next is [25.16666666666667, 83.16666666666667, 1.0, 2.0, 0.4268892088796643, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 622013.9468405113, 622013.9468405107, 176205.7505025073], 
processed observation next is [0.0, 0.2608695652173913, 0.39178515007898923, 0.8316666666666667, 1.0, 1.0, 0.30950507093935453, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17278165190014202, 0.17278165190014186, 0.2629936574664288], 
reward next is 0.7370, 
noisyNet noise sample is [array([0.7225226], dtype=float32), -0.9821153]. 
=============================================
[2019-04-28 02:32:26,347] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138000, global step 2210924: loss 0.0514
[2019-04-28 02:32:26,349] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138000, global step 2210924: learning rate 0.0000
[2019-04-28 02:32:28,156] A3C_AGENT_WORKER-Thread-20 INFO:Local step 138500, global step 2211678: loss 0.1684
[2019-04-28 02:32:28,158] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 138500, global step 2211679: learning rate 0.0000
[2019-04-28 02:32:28,938] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138000, global step 2211997: loss 0.0027
[2019-04-28 02:32:28,945] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138000, global step 2211999: learning rate 0.0000
[2019-04-28 02:32:29,502] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138000, global step 2212229: loss 0.0188
[2019-04-28 02:32:29,507] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138000, global step 2212229: learning rate 0.0000
[2019-04-28 02:32:29,662] A3C_AGENT_WORKER-Thread-2 INFO:Local step 138500, global step 2212290: loss 0.0027
[2019-04-28 02:32:29,664] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 138500, global step 2212291: learning rate 0.0000
[2019-04-28 02:32:30,638] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138000, global step 2212706: loss 0.0007
[2019-04-28 02:32:30,642] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138000, global step 2212706: learning rate 0.0000
[2019-04-28 02:32:31,202] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138000, global step 2212897: loss 0.0005
[2019-04-28 02:32:31,204] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138000, global step 2212898: learning rate 0.0000
[2019-04-28 02:32:32,788] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0037785e-11 9.9998510e-01 9.9244290e-21 1.4739331e-05 1.4994947e-07], sum to 1.0000
[2019-04-28 02:32:32,794] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.7558
[2019-04-28 02:32:32,798] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 89.0, 1.0, 2.0, 0.5368056235281804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750120.615587785, 750120.6155877844, 189563.6615732112], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3363000.0000, 
sim time next is 3363600.0000, 
raw observation next is [27.0, 89.0, 1.0, 2.0, 0.5380746063124562, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 751894.4923981369, 751894.4923981369, 189776.4799464616], 
processed observation next is [0.0, 0.9565217391304348, 0.4786729857819906, 0.89, 1.0, 1.0, 0.44346338109934474, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2088595812217047, 0.2088595812217047, 0.28324847753203225], 
reward next is 0.7168, 
noisyNet noise sample is [array([-0.4546119], dtype=float32), -1.0809155]. 
=============================================
[2019-04-28 02:32:33,137] A3C_AGENT_WORKER-Thread-11 INFO:Local step 138500, global step 2213708: loss 0.0331
[2019-04-28 02:32:33,140] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 138500, global step 2213709: learning rate 0.0000
[2019-04-28 02:32:34,017] A3C_AGENT_WORKER-Thread-9 INFO:Local step 138500, global step 2214069: loss 0.0174
[2019-04-28 02:32:34,019] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 138500, global step 2214070: learning rate 0.0000
[2019-04-28 02:32:34,303] A3C_AGENT_WORKER-Thread-14 INFO:Local step 138500, global step 2214190: loss 0.6084
[2019-04-28 02:32:34,304] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 138500, global step 2214190: learning rate 0.0000
[2019-04-28 02:32:35,475] A3C_AGENT_WORKER-Thread-10 INFO:Local step 138500, global step 2214681: loss 0.5589
[2019-04-28 02:32:35,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 138500, global step 2214681: learning rate 0.0000
[2019-04-28 02:32:35,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2329458e-08 9.1809857e-01 1.6954254e-15 8.1892073e-02 9.3391673e-06], sum to 1.0000
[2019-04-28 02:32:35,967] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0055
[2019-04-28 02:32:35,971] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.16666666666667, 78.16666666666667, 1.0, 2.0, 0.6200542443010056, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 891944.0295834214, 891944.0295834214, 207828.5426787276], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3732600.0000, 
sim time next is 3733200.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.5711560854166213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822849.6763475658, 822849.6763475658, 198649.3753765672], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 1.0, 1.0, 0.4833205848393028, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2285693545409905, 0.2285693545409905, 0.2964916050396525], 
reward next is 0.7035, 
noisyNet noise sample is [array([0.55474174], dtype=float32), -0.47438154]. 
=============================================
[2019-04-28 02:32:37,864] A3C_AGENT_WORKER-Thread-12 INFO:Local step 138500, global step 2215653: loss -4.2982
[2019-04-28 02:32:37,867] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 138500, global step 2215654: learning rate 0.0000
[2019-04-28 02:32:38,547] A3C_AGENT_WORKER-Thread-21 INFO:Local step 138500, global step 2215935: loss 0.1046
[2019-04-28 02:32:38,548] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 138500, global step 2215935: learning rate 0.0000
[2019-04-28 02:32:38,959] A3C_AGENT_WORKER-Thread-13 INFO:Local step 138500, global step 2216112: loss 0.0478
[2019-04-28 02:32:38,961] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 138500, global step 2216112: learning rate 0.0000
[2019-04-28 02:32:40,803] A3C_AGENT_WORKER-Thread-16 INFO:Local step 138500, global step 2216871: loss 0.0037
[2019-04-28 02:32:40,828] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 138500, global step 2216871: learning rate 0.0000
[2019-04-28 02:32:43,104] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139000, global step 2217786: loss 0.1820
[2019-04-28 02:32:43,106] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139000, global step 2217786: learning rate 0.0000
[2019-04-28 02:32:46,408] A3C_AGENT_WORKER-Thread-19 INFO:Local step 138500, global step 2219091: loss 0.0624
[2019-04-28 02:32:46,413] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 138500, global step 2219092: learning rate 0.0000
[2019-04-28 02:32:47,521] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139000, global step 2219568: loss 0.0051
[2019-04-28 02:32:47,523] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139000, global step 2219568: learning rate 0.0000
[2019-04-28 02:32:48,435] A3C_AGENT_WORKER-Thread-22 INFO:Local step 138500, global step 2219969: loss 0.0611
[2019-04-28 02:32:48,437] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 138500, global step 2219969: learning rate 0.0000
[2019-04-28 02:32:48,961] A3C_AGENT_WORKER-Thread-3 INFO:Local step 138500, global step 2220195: loss 0.1301
[2019-04-28 02:32:48,963] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 138500, global step 2220195: learning rate 0.0000
[2019-04-28 02:32:49,309] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139000, global step 2220333: loss 0.0155
[2019-04-28 02:32:49,313] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139000, global step 2220333: learning rate 0.0000
[2019-04-28 02:32:49,876] A3C_AGENT_WORKER-Thread-15 INFO:Local step 138500, global step 2220587: loss 0.0791
[2019-04-28 02:32:49,878] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 138500, global step 2220587: learning rate 0.0000
[2019-04-28 02:32:50,783] A3C_AGENT_WORKER-Thread-17 INFO:Local step 138500, global step 2220957: loss 0.4927
[2019-04-28 02:32:50,788] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 138500, global step 2220957: learning rate 0.0000
[2019-04-28 02:32:52,460] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139000, global step 2221643: loss 0.0012
[2019-04-28 02:32:52,463] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139000, global step 2221643: learning rate 0.0000
[2019-04-28 02:32:53,310] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139000, global step 2221988: loss 0.0042
[2019-04-28 02:32:53,312] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139000, global step 2221989: learning rate 0.0000
[2019-04-28 02:32:53,514] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139000, global step 2222088: loss 0.0110
[2019-04-28 02:32:53,515] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139000, global step 2222089: learning rate 0.0000
[2019-04-28 02:32:55,132] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139000, global step 2222750: loss 0.0154
[2019-04-28 02:32:55,136] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139000, global step 2222753: learning rate 0.0000
[2019-04-28 02:32:56,844] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139000, global step 2223456: loss 0.0006
[2019-04-28 02:32:56,845] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139000, global step 2223456: learning rate 0.0000
[2019-04-28 02:32:56,975] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [9.0179437e-13 9.9393058e-01 4.0269697e-21 6.0694744e-03 7.0776551e-10], sum to 1.0000
[2019-04-28 02:32:56,983] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4973
[2019-04-28 02:32:56,993] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [33.83333333333334, 67.66666666666667, 1.0, 2.0, 0.4506427648672363, 1.0, 2.0, 0.4506427648672363, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1259731.865550774, 1259731.865550774, 285659.5252184608], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4122600.0000, 
sim time next is 4123200.0000, 
raw observation next is [33.66666666666667, 68.33333333333334, 1.0, 2.0, 0.6151952061523852, 0.0, 1.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104269, 859704.8601787516, 859704.8601787522, 203613.6954476416], 
processed observation next is [1.0, 0.7391304347826086, 0.7946287519747238, 0.6833333333333335, 1.0, 1.0, 0.5363797664486568, 0.0, 0.5, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451522826, 0.23880690560520879, 0.23880690560520895, 0.3039010379815546], 
reward next is 0.6961, 
noisyNet noise sample is [array([-0.02805285], dtype=float32), -0.74101365]. 
=============================================
[2019-04-28 02:32:57,793] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139000, global step 2223836: loss 0.0003
[2019-04-28 02:32:57,796] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139000, global step 2223837: learning rate 0.0000
[2019-04-28 02:32:58,829] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139000, global step 2224270: loss 0.0006
[2019-04-28 02:32:58,829] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139000, global step 2224270: learning rate 0.0000
[2019-04-28 02:33:00,328] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139000, global step 2224874: loss 0.0001
[2019-04-28 02:33:00,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139000, global step 2224874: learning rate 0.0000
[2019-04-28 02:33:00,624] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-28 02:33:00,625] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:33:00,626] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:33:00,628] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:33:00,630] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:33:00,630] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:33:00,631] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:33:00,631] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:33:00,636] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:33:00,639] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:33:00,642] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:33:00,664] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run90
[2019-04-28 02:33:00,692] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run90
[2019-04-28 02:33:00,732] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run90
[2019-04-28 02:33:00,733] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run90
[2019-04-28 02:33:00,819] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run90
[2019-04-28 02:33:08,500] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:33:08,501] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.3, 60.5, 1.0, 2.0, 0.3123279032490586, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 499776.8196360314, 499776.8196360314, 167085.2176852555]
[2019-04-28 02:33:08,503] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:33:08,507] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.2542706e-11 9.8763090e-01 1.3513912e-20 1.2369144e-02 2.2016977e-09], sampled 0.2918972368256889
[2019-04-28 02:33:19,217] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:33:19,218] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [19.82271339833333, 88.91010858333334, 1.0, 2.0, 0.2703314381058237, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 442591.4183281321, 442591.4183281314, 162974.4721814109]
[2019-04-28 02:33:19,219] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:33:19,222] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.7298485e-12 9.9000603e-01 1.4628393e-21 9.9940011e-03 8.3423113e-10], sampled 0.4917124995665335
[2019-04-28 02:33:25,654] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:33:25,656] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.47221483, 96.60473121999999, 1.0, 2.0, 0.5040332173702372, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 704309.9789725766, 704309.9789725766, 184228.956668275]
[2019-04-28 02:33:25,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:33:25,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [4.2498713e-12 9.8977506e-01 1.8587984e-21 1.0224890e-02 9.2605479e-10], sampled 0.907895093221413
[2019-04-28 02:33:35,016] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:33:35,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [27.26666666666667, 84.0, 1.0, 2.0, 0.6757635608304595, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 944383.7702033484, 944383.7702033484, 215699.7630741714]
[2019-04-28 02:33:35,017] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:33:35,019] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.20858679e-11 9.87716854e-01 1.26493574e-20 1.22831585e-02
 2.13843498e-09], sampled 0.6576010461739917
[2019-04-28 02:34:08,253] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:08,254] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.1, 82.0, 1.0, 2.0, 0.9125302919196611, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1275465.468849645, 1275465.468849645, 273413.1812436061]
[2019-04-28 02:34:08,255] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:34:08,267] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.6116079e-10 9.7891164e-01 3.5512133e-18 2.1088365e-02 2.5054753e-08], sampled 0.3270942559785025
[2019-04-28 02:34:08,359] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:08,360] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.85, 48.5, 1.0, 2.0, 1.004340731796297, 1.0, 2.0, 1.004340731796297, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2809463.197298845, 2809463.197298845, 531611.8778311424]
[2019-04-28 02:34:08,361] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:34:08,364] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.1299958e-10 9.8177737e-01 7.6492286e-19 1.8222634e-02 1.2823681e-08], sampled 0.47613353096939426
[2019-04-28 02:34:08,365] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2809463.197298845 W.
[2019-04-28 02:34:08,380] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:08,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [35.5, 49.0, 1.0, 2.0, 1.014806756256303, 1.0, 2.0, 1.014806756256303, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2838773.290595083, 2838773.290595083, 537993.7985099856]
[2019-04-28 02:34:08,381] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:34:08,383] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [9.6634513e-11 9.8226821e-01 5.7408439e-19 1.7731806e-02 1.1316536e-08], sampled 0.7139593197271044
[2019-04-28 02:34:08,384] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2838773.290595083 W.
[2019-04-28 02:34:09,685] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:09,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.48333333333333, 92.83333333333333, 1.0, 2.0, 0.6187017188763936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 864607.0327015099, 864607.0327015093, 204281.4723642755]
[2019-04-28 02:34:09,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:34:09,689] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.6361088e-12 9.8962605e-01 2.1807502e-21 1.0373911e-02 9.9270492e-10], sampled 0.9474991451013006
[2019-04-28 02:34:17,434] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:17,435] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [26.017364265, 75.294714725, 1.0, 2.0, 0.4500376263098461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 654347.6918507, 654347.6918507005, 179386.2495835071]
[2019-04-28 02:34:17,436] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:34:17,438] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.1977992e-12 9.8907822e-01 3.7112634e-21 1.0921755e-02 1.2521344e-09], sampled 0.25371919420896905
[2019-04-28 02:34:33,130] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11858278]
[2019-04-28 02:34:33,131] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.3, 86.33333333333334, 1.0, 2.0, 0.4027598588474285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 594720.212489671, 594720.2124896715, 173864.3419595282]
[2019-04-28 02:34:33,132] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:34:33,135] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.1436290e-12 9.8879880e-01 4.8132759e-21 1.1201160e-02 1.4029068e-09], sampled 0.5851170293187569
[2019-04-28 02:34:36,472] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7876.4145 3013596271.2026 1734.0000
[2019-04-28 02:34:36,593] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8373.6935 2848542820.7079 1115.0000
[2019-04-28 02:34:36,788] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8542.7816 2785522135.2959 909.0000
[2019-04-28 02:34:37,270] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8156.8407 2932381365.5114 1286.0000
[2019-04-28 02:34:37,315] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7791.2646 3169602739.6665 1765.0000
[2019-04-28 02:34:38,327] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2225000, evaluation results [2225000.0, 7791.264570288303, 3169602739.666505, 1765.0, 8156.840685691186, 2932381365.5113626, 1286.0, 8542.781553975077, 2785522135.2959485, 909.0, 7876.414531302319, 3013596271.2026196, 1734.0, 8373.693535391942, 2848542820.70792, 1115.0]
[2019-04-28 02:34:39,773] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [5.5736767e-14 9.9902642e-01 7.3489872e-23 9.7359577e-04 2.7348616e-12], sum to 1.0000
[2019-04-28 02:34:39,780] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.1691
[2019-04-28 02:34:39,783] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [35.0, 59.33333333333333, 1.0, 2.0, 0.6260126598148094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 874827.9421240434, 874827.9421240427, 205692.8584708141], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3852600.0000, 
sim time next is 3853200.0000, 
raw observation next is [35.0, 58.66666666666667, 1.0, 2.0, 0.6194142991095503, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 865603.2363984286, 865603.2363984279, 204419.268257242], 
processed observation next is [0.0, 0.6086956521739131, 0.8578199052132701, 0.5866666666666667, 1.0, 1.0, 0.5414630109753618, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24044534344400795, 0.24044534344400775, 0.30510338545857013], 
reward next is 0.6949, 
noisyNet noise sample is [array([0.9879364], dtype=float32), -0.07478212]. 
=============================================
[2019-04-28 02:34:40,471] A3C_AGENT_WORKER-Thread-18 INFO:Local step 139500, global step 2225858: loss 0.4238
[2019-04-28 02:34:40,471] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 139500, global step 2225858: learning rate 0.0000
[2019-04-28 02:34:43,587] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139000, global step 2227121: loss 0.0000
[2019-04-28 02:34:43,588] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139000, global step 2227121: learning rate 0.0000
[2019-04-28 02:34:44,896] A3C_AGENT_WORKER-Thread-20 INFO:Local step 139500, global step 2227654: loss 9.2486
[2019-04-28 02:34:44,897] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 139500, global step 2227654: learning rate 0.0000
[2019-04-28 02:34:45,422] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139000, global step 2227861: loss 0.0015
[2019-04-28 02:34:45,425] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139000, global step 2227861: learning rate 0.0000
[2019-04-28 02:34:46,443] A3C_AGENT_WORKER-Thread-2 INFO:Local step 139500, global step 2228275: loss 0.4695
[2019-04-28 02:34:46,451] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 139500, global step 2228278: learning rate 0.0000
[2019-04-28 02:34:46,501] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139000, global step 2228296: loss 0.0232
[2019-04-28 02:34:46,504] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139000, global step 2228297: learning rate 0.0000
[2019-04-28 02:34:47,252] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139000, global step 2228608: loss 0.0092
[2019-04-28 02:34:47,254] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139000, global step 2228609: learning rate 0.0000
[2019-04-28 02:34:48,192] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139000, global step 2228999: loss 0.0006
[2019-04-28 02:34:48,193] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139000, global step 2228999: learning rate 0.0000
[2019-04-28 02:34:50,182] A3C_AGENT_WORKER-Thread-11 INFO:Local step 139500, global step 2229737: loss 16.1089
[2019-04-28 02:34:50,186] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 139500, global step 2229737: learning rate 0.0000
[2019-04-28 02:34:50,669] A3C_AGENT_WORKER-Thread-14 INFO:Local step 139500, global step 2229956: loss 7.2889
[2019-04-28 02:34:50,670] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 139500, global step 2229956: learning rate 0.0000
[2019-04-28 02:34:50,882] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.3575946e-12 9.9053228e-01 1.7810586e-20 9.4668437e-03 8.2073291e-07], sum to 1.0000
[2019-04-28 02:34:50,890] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.9040
[2019-04-28 02:34:50,895] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 3076991.334378713 W.
[2019-04-28 02:34:50,899] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [34.16666666666667, 67.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.144980561153981, 6.9112, 170.5573041426782, 3076991.334378713, 2909524.808838894, 552435.9629005373], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4121400.0000, 
sim time next is 4122000.0000, 
raw observation next is [34.0, 67.0, 1.0, 2.0, 0.7360313275872613, 1.0, 2.0, 0.6886057033078933, 1.0, 1.0, 1.03, 7.005100573364645, 6.9112, 170.5573041426782, 2889468.987109413, 2822204.186777206, 532952.7082493951], 
processed observation next is [1.0, 0.7391304347826086, 0.8104265402843602, 0.67, 1.0, 1.0, 0.6819654549244112, 1.0, 1.0, 0.6248261485637269, 1.0, 0.5, 1.0365853658536586, 0.00939005733646452, 0.0, 0.8375144448122397, 0.8026302741970591, 0.7839456074381128, 0.7954518033573061], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.85309523], dtype=float32), 1.0395606]. 
=============================================
[2019-04-28 02:34:50,929] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[61.17857 ]
 [61.133186]
 [61.005955]
 [61.087036]
 [61.29794 ]], R is [[60.70821762]
 [60.10113525]
 [59.50012589]
 [58.90512466]
 [58.31607437]].
[2019-04-28 02:34:51,559] A3C_AGENT_WORKER-Thread-9 INFO:Local step 139500, global step 2230355: loss 1.0710
[2019-04-28 02:34:51,561] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 139500, global step 2230355: learning rate 0.0000
[2019-04-28 02:34:52,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [1.3373694e-11 9.9986613e-01 3.5196656e-21 1.3389974e-04 7.3684357e-11], sum to 1.0000
[2019-04-28 02:34:52,293] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9764
[2019-04-28 02:34:52,296] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.33333333333334, 77.66666666666667, 1.0, 2.0, 0.6299860219547007, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 880382.8599504848, 880382.8599504848, 206465.6320131386], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4222200.0000, 
sim time next is 4222800.0000, 
raw observation next is [31.0, 79.0, 1.0, 2.0, 0.6253095733433646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 873845.0023829468, 873845.0023829468, 205556.1514635404], 
processed observation next is [1.0, 0.9130434782608695, 0.6682464454976303, 0.79, 1.0, 1.0, 0.5485657510161018, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.24273472288415188, 0.24273472288415188, 0.30680022606498564], 
reward next is 0.6932, 
noisyNet noise sample is [array([-0.6212197], dtype=float32), -0.1414923]. 
=============================================
[2019-04-28 02:34:53,193] A3C_AGENT_WORKER-Thread-10 INFO:Local step 139500, global step 2230866: loss 2.5171
[2019-04-28 02:34:53,194] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 139500, global step 2230866: learning rate 0.0000
[2019-04-28 02:34:53,600] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [6.4087483e-19 1.0000000e+00 1.4151822e-25 2.1510719e-19 1.9283599e-22], sum to 1.0000
[2019-04-28 02:34:53,601] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0526
[2019-04-28 02:34:53,616] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5952278981837773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831790.6091750228, 831790.6091750228, 199860.3863433832], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4238400.0000, 
sim time next is 4239000.0000, 
raw observation next is [30.5, 77.0, 1.0, 2.0, 0.593500489155033, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829375.7297710113, 829375.7297710113, 199541.4305315053], 
processed observation next is [1.0, 0.043478260869565216, 0.6445497630331753, 0.77, 1.0, 1.0, 0.5102415531988349, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23038214715861424, 0.23038214715861424, 0.29782303064403776], 
reward next is 0.7022, 
noisyNet noise sample is [array([-1.1181555], dtype=float32), 1.2627838]. 
=============================================
[2019-04-28 02:34:53,743] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[38.256428]
 [38.529472]
 [38.745872]
 [38.898132]
 [39.211704]], R is [[38.43730545]
 [38.75463486]
 [39.06784058]
 [39.37807465]
 [39.68513107]].
[2019-04-28 02:34:55,177] A3C_AGENT_WORKER-Thread-12 INFO:Local step 139500, global step 2231455: loss 0.9684
[2019-04-28 02:34:55,180] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 139500, global step 2231455: learning rate 0.0000
[2019-04-28 02:34:56,971] A3C_AGENT_WORKER-Thread-21 INFO:Local step 139500, global step 2231971: loss 8.4652
[2019-04-28 02:34:56,993] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 139500, global step 2231971: learning rate 0.0000
[2019-04-28 02:34:58,827] A3C_AGENT_WORKER-Thread-13 INFO:Local step 139500, global step 2232508: loss 0.8936
[2019-04-28 02:34:58,830] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 139500, global step 2232508: learning rate 0.0000
[2019-04-28 02:34:59,740] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.6993013e-10 9.9871182e-01 3.3639490e-16 1.2881493e-03 1.0162240e-09], sum to 1.0000
[2019-04-28 02:34:59,743] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9053
[2019-04-28 02:34:59,749] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2797434.829656771 W.
[2019-04-28 02:34:59,800] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [32.0, 71.0, 1.0, 2.0, 1.000045580919229, 1.0, 2.0, 1.000045580919229, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2797434.829656771, 2797434.829656771, 529017.7512043099], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4100400.0000, 
sim time next is 4101000.0000, 
raw observation next is [32.16666666666667, 71.0, 1.0, 2.0, 0.6100821486168124, 1.0, 2.0, 0.6100821486168124, 1.0, 1.0, 1.03, 6.944375320616209, 6.9112, 170.5573041426782, 2559637.545788134, 2535872.714144982, 491664.2367392123], 
processed observation next is [1.0, 0.4782608695652174, 0.7235387045813588, 0.71, 1.0, 1.0, 0.5302194561648342, 1.0, 1.0, 0.5302194561648342, 1.0, 0.5, 1.0365853658536586, 0.003317532061620909, 0.0, 0.8375144448122397, 0.7110104293855928, 0.704409087262495, 0.7338272190137497], 
reward next is 0.1003, 
noisyNet noise sample is [array([-0.01692028], dtype=float32), 0.1955899]. 
=============================================
[2019-04-28 02:34:59,973] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[48.69522 ]
 [47.989666]
 [47.40951 ]
 [46.998425]
 [46.819454]], R is [[48.57138824]
 [48.2960968 ]
 [48.02659225]
 [47.54632568]
 [47.07086182]].
[2019-04-28 02:35:00,264] A3C_AGENT_WORKER-Thread-16 INFO:Local step 139500, global step 2232939: loss 0.3225
[2019-04-28 02:35:00,268] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 139500, global step 2232939: learning rate 0.0000
[2019-04-28 02:35:03,293] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140000, global step 2233839: loss 0.0014
[2019-04-28 02:35:03,295] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140000, global step 2233839: learning rate 0.0000
[2019-04-28 02:35:07,473] A3C_AGENT_WORKER-Thread-19 INFO:Local step 139500, global step 2235031: loss 0.1444
[2019-04-28 02:35:07,474] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 139500, global step 2235031: learning rate 0.0000
[2019-04-28 02:35:09,074] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140000, global step 2235488: loss 0.2859
[2019-04-28 02:35:09,076] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140000, global step 2235488: learning rate 0.0000
[2019-04-28 02:35:09,814] A3C_AGENT_WORKER-Thread-22 INFO:Local step 139500, global step 2235699: loss 0.4978
[2019-04-28 02:35:09,818] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 139500, global step 2235700: learning rate 0.0000
[2019-04-28 02:35:11,501] A3C_AGENT_WORKER-Thread-3 INFO:Local step 139500, global step 2236177: loss 0.9185
[2019-04-28 02:35:11,503] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 139500, global step 2236177: learning rate 0.0000
[2019-04-28 02:35:11,885] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140000, global step 2236276: loss 0.0102
[2019-04-28 02:35:11,894] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140000, global step 2236278: learning rate 0.0000
[2019-04-28 02:35:13,235] A3C_AGENT_WORKER-Thread-15 INFO:Local step 139500, global step 2236646: loss 1.3164
[2019-04-28 02:35:13,241] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 139500, global step 2236646: learning rate 0.0000
[2019-04-28 02:35:14,166] A3C_AGENT_WORKER-Thread-17 INFO:Local step 139500, global step 2236895: loss 0.1873
[2019-04-28 02:35:14,172] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 139500, global step 2236895: learning rate 0.0000
[2019-04-28 02:35:17,624] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140000, global step 2237830: loss 0.0047
[2019-04-28 02:35:17,631] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140000, global step 2237830: learning rate 0.0000
[2019-04-28 02:35:18,242] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140000, global step 2237983: loss 0.0093
[2019-04-28 02:35:18,244] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140000, global step 2237983: learning rate 0.0000
[2019-04-28 02:35:19,012] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140000, global step 2238185: loss 0.0036
[2019-04-28 02:35:19,018] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140000, global step 2238185: learning rate 0.0000
[2019-04-28 02:35:19,999] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [5.0218063e-08 9.0880775e-01 2.6664337e-13 9.1192238e-02 2.9228090e-08], sum to 1.0000
[2019-04-28 02:35:20,001] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3977
[2019-04-28 02:35:20,002] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2940406.621947623 W.
[2019-04-28 02:35:20,079] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 75.0, 1.0, 2.0, 0.7602812571448048, 1.0, 2.0, 0.700730668086665, 1.0, 1.0, 1.03, 7.005102485758526, 6.9112, 170.5573041426782, 2940406.621947623, 2873140.451689887, 541046.8124876386], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4266000.0000, 
sim time next is 4266600.0000, 
raw observation next is [33.16666666666666, 74.33333333333333, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.022698123182956, 6.9112, 170.5573041426782, 2989293.428805295, 2909422.787715671, 553079.1362987462], 
processed observation next is [1.0, 0.391304347826087, 0.7709320695102682, 0.7433333333333333, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 0.5, -0.2195121951219512, 0.011149812318295638, 0.0, 0.8375144448122397, 0.8303592857792486, 0.8081729965876864, 0.8254912482070839], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.2946554], dtype=float32), -0.20260864]. 
=============================================
[2019-04-28 02:35:20,490] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.4328278e-10 9.9077916e-01 6.4347366e-17 9.2208637e-03 8.9790266e-11], sum to 1.0000
[2019-04-28 02:35:20,496] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6929
[2019-04-28 02:35:20,499] A3C_AGENT_WORKER-Thread-20 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 3174204.878357431 W.
[2019-04-28 02:35:20,524] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.66666666666666, 61.0, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 1.0, 0.0, 7.280531211538338, 6.9112, 170.5573041426782, 3174204.878357431, 2909637.908099373, 551693.6912816834], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4628400.0000, 
sim time next is 4629000.0000, 
raw observation next is [34.83333333333334, 60.5, 1.0, 2.0, 1.04, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 7.658306354232366, 6.9112, 170.5573041426782, 3445135.830306521, 2909953.158426425, 549487.095449984], 
processed observation next is [1.0, 0.5652173913043478, 0.8499210110584523, 0.605, 1.0, 1.0, 1.0481927710843375, 1.0, 1.0, 1.0481927710843375, 0.0, 1.0, -0.2195121951219512, 0.07471063542323657, 0.0, 0.8375144448122397, 0.9569821750851447, 0.808320321785118, 0.8201299932089313], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06616486], dtype=float32), 0.8613452]. 
=============================================
[2019-04-28 02:35:20,706] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[49.636185]
 [49.216858]
 [49.00039 ]
 [48.95177 ]
 [48.366776]], R is [[49.36831665]
 [48.87463379]
 [48.38588715]
 [48.10454559]
 [47.62350082]].
[2019-04-28 02:35:22,176] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140000, global step 2239043: loss 0.0212
[2019-04-28 02:35:22,177] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140000, global step 2239043: learning rate 0.0000
[2019-04-28 02:35:23,733] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140000, global step 2239444: loss 0.0022
[2019-04-28 02:35:23,744] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140000, global step 2239444: learning rate 0.0000
[2019-04-28 02:35:24,774] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.5697402e-13 1.0000000e+00 6.4428943e-19 7.1997124e-11 3.7050619e-15], sum to 1.0000
[2019-04-28 02:35:24,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2706
[2019-04-28 02:35:24,778] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.66666666666666, 85.66666666666666, 1.0, 2.0, 0.5049403936174067, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 705578.0411922628, 705578.0411922622, 184373.8777602233], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4668600.0000, 
sim time next is 4669200.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5069772357225876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 708425.1698864554, 708425.169886456, 184696.6834716284], 
processed observation next is [1.0, 0.043478260869565216, 0.4786729857819906, 0.84, 1.0, 1.0, 0.4059966695452862, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1967847694129043, 0.19678476941290443, 0.2756666917486991], 
reward next is 0.7243, 
noisyNet noise sample is [array([-0.8185562], dtype=float32), -0.81818765]. 
=============================================
[2019-04-28 02:35:26,518] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140000, global step 2240180: loss 0.0102
[2019-04-28 02:35:26,521] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140000, global step 2240180: learning rate 0.0000
[2019-04-28 02:35:26,911] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140000, global step 2240282: loss 0.0118
[2019-04-28 02:35:26,918] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140000, global step 2240282: learning rate 0.0000
[2019-04-28 02:35:29,024] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140000, global step 2240846: loss 0.0466
[2019-04-28 02:35:29,025] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140000, global step 2240846: learning rate 0.0000
[2019-04-28 02:35:31,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [4.0278721e-14 9.9841022e-01 2.4465271e-20 1.5896978e-03 3.6421360e-14], sum to 1.0000
[2019-04-28 02:35:31,987] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.3801
[2019-04-28 02:35:32,006] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 63.66666666666666, 1.0, 2.0, 0.5504719465408118, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 769224.5557942092, 769224.5557942085, 191882.5537764546], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4387800.0000, 
sim time next is 4388400.0000, 
raw observation next is [32.0, 63.0, 1.0, 2.0, 0.5483205362909488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 766217.1101081015, 766217.1101081009, 191513.7958283018], 
processed observation next is [1.0, 0.8260869565217391, 0.7156398104265403, 0.63, 1.0, 1.0, 0.45580787504933584, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2128380861411393, 0.21283808614113914, 0.28584148631089823], 
reward next is 0.7142, 
noisyNet noise sample is [array([0.01800967], dtype=float32), 0.36026618]. 
=============================================
[2019-04-28 02:35:32,770] A3C_AGENT_WORKER-Thread-18 INFO:Local step 140500, global step 2241893: loss 13.5202
[2019-04-28 02:35:32,772] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 140500, global step 2241893: learning rate 0.0000
[2019-04-28 02:35:35,571] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140000, global step 2242703: loss 0.0053
[2019-04-28 02:35:35,578] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140000, global step 2242703: learning rate 0.0000
[2019-04-28 02:35:36,222] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [9.8382291e-10 9.7583437e-01 1.7519655e-16 2.4165610e-02 3.5148609e-10], sum to 1.0000
[2019-04-28 02:35:36,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.3728
[2019-04-28 02:35:36,223] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2020435.601988785 W.
[2019-04-28 02:35:36,261] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [29.66666666666667, 76.33333333333334, 1.0, 2.0, 0.7225123601655592, 1.0, 2.0, 0.7225123601655592, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2020435.601988785, 2020435.601988785, 383913.642407808], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4696800.0000, 
sim time next is 4697400.0000, 
raw observation next is [29.83333333333333, 75.66666666666666, 1.0, 2.0, 0.5016094462878004, 1.0, 2.0, 0.5016094462878004, 1.0, 1.0, 0.8711297234231539, 6.9112, 6.9112, 170.5573041426782, 2104135.225775614, 2104135.225775614, 416260.5540634607], 
processed observation next is [1.0, 0.34782608695652173, 0.6129541864139019, 0.7566666666666666, 1.0, 1.0, 0.3995294533587956, 1.0, 1.0, 0.3995294533587956, 1.0, 0.5, 0.8428411261257974, 0.0, 0.0, 0.8375144448122397, 0.5844820071598927, 0.5844820071598927, 0.6212844090499413], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2211039], dtype=float32), 1.5183716]. 
=============================================
[2019-04-28 02:35:37,697] A3C_AGENT_WORKER-Thread-20 INFO:Local step 140500, global step 2243309: loss 3.3780
[2019-04-28 02:35:37,700] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 140500, global step 2243309: learning rate 0.0000
[2019-04-28 02:35:39,289] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140000, global step 2243716: loss 0.0165
[2019-04-28 02:35:39,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140000, global step 2243716: learning rate 0.0000
[2019-04-28 02:35:40,908] A3C_AGENT_WORKER-Thread-2 INFO:Local step 140500, global step 2244209: loss -4.2527
[2019-04-28 02:35:40,912] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 140500, global step 2244209: learning rate 0.0000
[2019-04-28 02:35:41,197] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140000, global step 2244289: loss 0.7650
[2019-04-28 02:35:41,200] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140000, global step 2244289: learning rate 0.0000
[2019-04-28 02:35:41,647] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1679067e-13 9.4463378e-01 2.2647098e-23 5.5366188e-02 8.4500930e-15], sum to 1.0000
[2019-04-28 02:35:41,648] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.2053
[2019-04-28 02:35:41,675] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 88.16666666666667, 1.0, 2.0, 0.5057382097944894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706693.2397994349, 706693.2397994349, 184499.3885446456], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4504200.0000, 
sim time next is 4504800.0000, 
raw observation next is [26.0, 87.33333333333334, 1.0, 2.0, 0.5031500513758868, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 703075.4800365015, 703075.4800365015, 184090.356017219], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.8733333333333334, 1.0, 1.0, 0.4013856040673334, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19529874445458376, 0.19529874445458376, 0.2747617253988343], 
reward next is 0.7252, 
noisyNet noise sample is [array([-0.4830139], dtype=float32), -1.456828]. 
=============================================
[2019-04-28 02:35:42,661] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140000, global step 2244711: loss 0.0266
[2019-04-28 02:35:42,662] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140000, global step 2244711: learning rate 0.0000
[2019-04-28 02:35:43,105] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140000, global step 2244836: loss 0.0272
[2019-04-28 02:35:43,107] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140000, global step 2244836: learning rate 0.0000
[2019-04-28 02:35:47,129] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [1.3367761e-11 9.6834105e-01 1.1305489e-20 3.1658974e-02 1.1911498e-12], sum to 1.0000
[2019-04-28 02:35:47,130] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4134
[2019-04-28 02:35:47,130] A3C_AGENT_WORKER-Thread-21 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2396557.801678048 W.
[2019-04-28 02:35:47,147] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.16666666666667, 69.5, 1.0, 2.0, 0.8568747094019938, 1.0, 2.0, 0.8568747094019938, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2396557.801678048, 2396557.801678048, 448511.5567830689], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 4726200.0000, 
sim time next is 4726800.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5662035831981788, 1.0, 2.0, 0.5662035831981788, 1.0, 1.0, 0.9833083776289804, 6.9112, 6.9112, 170.5573041426782, 2375367.318639155, 2375367.318639155, 463945.6600682849], 
processed observation next is [1.0, 0.7391304347826086, 0.6682464454976303, 0.7, 1.0, 1.0, 0.4773537146966009, 1.0, 1.0, 0.4773537146966009, 1.0, 0.5, 0.9796443629621712, 0.0, 0.0, 0.8375144448122397, 0.6598242551775431, 0.6598242551775431, 0.6924562090571417], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.1591212], dtype=float32), 0.801132]. 
=============================================
[2019-04-28 02:35:47,374] A3C_AGENT_WORKER-Thread-11 INFO:Local step 140500, global step 2246024: loss -0.3103
[2019-04-28 02:35:47,378] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 140500, global step 2246024: learning rate 0.0000
[2019-04-28 02:35:47,814] A3C_AGENT_WORKER-Thread-14 INFO:Local step 140500, global step 2246148: loss -14.1887
[2019-04-28 02:35:47,815] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 140500, global step 2246148: learning rate 0.0000
[2019-04-28 02:35:48,355] A3C_AGENT_WORKER-Thread-9 INFO:Local step 140500, global step 2246295: loss -7.4916
[2019-04-28 02:35:48,356] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 140500, global step 2246295: learning rate 0.0000
[2019-04-28 02:35:48,698] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.5487997e-11 9.9999928e-01 6.9276350e-17 7.1298501e-07 4.9517037e-13], sum to 1.0000
[2019-04-28 02:35:48,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8598
[2019-04-28 02:35:48,699] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1948215.447494898 W.
[2019-04-28 02:35:48,724] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [28.0, 86.5, 1.0, 2.0, 0.6967097299423636, 1.0, 2.0, 0.6967097299423636, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1948215.447494898, 1948215.447494898, 372722.1456067696], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 4588200.0000, 
sim time next is 4588800.0000, 
raw observation next is [28.0, 85.66666666666667, 1.0, 2.0, 0.6683488387162189, 0.0, 1.0, 0.0, 1.0, 1.0, 1.03, 6.99795341528479, 6.9112, 168.9124453704888, 1830838.542117563, 1769292.869733582, 378244.9595247827], 
processed observation next is [1.0, 0.08695652173913043, 0.5260663507109005, 0.8566666666666667, 1.0, 1.0, 0.6004202876099022, 0.0, 0.5, -0.20481927710843376, 1.0, 0.5, 1.0365853658536586, 0.008675341528479041, 0.0, 0.8294374352211856, 0.5085662616993231, 0.49147024159266167, 0.564544715708631], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.5696331], dtype=float32), 1.9276314]. 
=============================================
[2019-04-28 02:35:49,200] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [4.4066244e-09 9.8492581e-01 1.4107341e-14 1.5074157e-02 6.3763730e-11], sum to 1.0000
[2019-04-28 02:35:49,200] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4421
[2019-04-28 02:35:49,201] A3C_AGENT_WORKER-Thread-22 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2740437.303656983 W.
[2019-04-28 02:35:49,252] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6650759893328878, 1.0, 2.0, 0.6531280341807064, 1.0, 1.0, 1.03, 7.005094978591839, 6.9112, 170.5573041426782, 2740437.303656983, 2673176.511088117, 510525.9302339208], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4613400.0000, 
sim time next is 4614000.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.969412093162126, 1.0, 2.0, 0.969412093162126, 0.0, 1.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2711650.549084825, 2711650.549084825, 510786.261133157], 
processed observation next is [1.0, 0.391304347826087, 0.7156398104265403, 0.75, 1.0, 1.0, 0.9631471001953325, 1.0, 1.0, 0.9631471001953325, 0.0, 0.5, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.7532362636346736, 0.7532362636346736, 0.762367553930085], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.2230016], dtype=float32), 1.9300163]. 
=============================================
[2019-04-28 02:35:49,378] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[44.75172 ]
 [44.54    ]
 [44.352646]
 [43.925114]
 [42.901875]], R is [[44.83755493]
 [44.38917923]
 [44.19143677]
 [44.00823212]
 [43.79052353]].
[2019-04-28 02:35:49,997] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.9674284e-15 1.0000000e+00 3.3868938e-23 1.0197706e-10 2.3464415e-18], sum to 1.0000
[2019-04-28 02:35:49,998] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5742
[2019-04-28 02:35:50,024] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 89.0, 1.0, 2.0, 0.5747839928138172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 803210.8361674407, 803210.8361674407, 196142.0513626629], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4584600.0000, 
sim time next is 4585200.0000, 
raw observation next is [28.0, 89.0, 1.0, 2.0, 0.575662974505974, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804439.6020684422, 804439.6020684422, 196299.2984193361], 
processed observation next is [1.0, 0.043478260869565216, 0.5260663507109005, 0.89, 1.0, 1.0, 0.4887505716939446, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2234554450190117, 0.2234554450190117, 0.2929840274915464], 
reward next is 0.7070, 
noisyNet noise sample is [array([0.9162839], dtype=float32), -1.3740528]. 
=============================================
[2019-04-28 02:35:50,672] A3C_AGENT_WORKER-Thread-10 INFO:Local step 140500, global step 2246952: loss -11.3380
[2019-04-28 02:35:50,673] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 140500, global step 2246952: learning rate 0.0000
[2019-04-28 02:35:52,209] A3C_AGENT_WORKER-Thread-12 INFO:Local step 140500, global step 2247387: loss 3.3542
[2019-04-28 02:35:52,216] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 140500, global step 2247387: learning rate 0.0000
[2019-04-28 02:35:54,822] A3C_AGENT_WORKER-Thread-13 INFO:Local step 140500, global step 2248114: loss 1.3430
[2019-04-28 02:35:54,851] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 140500, global step 2248114: learning rate 0.0000
[2019-04-28 02:35:55,675] A3C_AGENT_WORKER-Thread-21 INFO:Local step 140500, global step 2248361: loss -5.1711
[2019-04-28 02:35:55,677] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 140500, global step 2248361: learning rate 0.0000
[2019-04-28 02:35:57,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.3123738e-14 9.9794394e-01 8.3096216e-23 2.0561062e-03 1.4417944e-15], sum to 1.0000
[2019-04-28 02:35:57,024] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.4869
[2019-04-28 02:35:57,084] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4901598713567438, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 684917.8322771245, 684917.8322771239, 182068.4723652152], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4911600.0000, 
sim time next is 4912200.0000, 
raw observation next is [27.5, 76.5, 1.0, 2.0, 0.4890598876174903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 683380.2914506076, 683380.2914506069, 181899.5067941759], 
processed observation next is [1.0, 0.8695652173913043, 0.5023696682464456, 0.765, 1.0, 1.0, 0.3844095031536027, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1898278587362799, 0.1898278587362797, 0.27149180118533717], 
reward next is 0.7285, 
noisyNet noise sample is [array([-0.9091003], dtype=float32), 0.7696333]. 
=============================================
[2019-04-28 02:35:58,588] A3C_AGENT_WORKER-Thread-16 INFO:Local step 140500, global step 2249146: loss 31.5064
[2019-04-28 02:35:58,590] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 140500, global step 2249146: learning rate 0.0000
[2019-04-28 02:36:01,238] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141000, global step 2249862: loss 0.2243
[2019-04-28 02:36:01,241] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141000, global step 2249862: learning rate 0.0000
[2019-04-28 02:36:01,783] A3C_AGENT_WORKER-Thread-12 INFO:Evaluating...
[2019-04-28 02:36:01,784] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:36:01,793] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:36:01,793] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:36:01,800] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:36:01,803] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:36:01,806] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:36:01,807] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run91
[2019-04-28 02:36:01,850] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:36:01,851] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:36:01,859] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run91
[2019-04-28 02:36:01,895] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run91
[2019-04-28 02:36:01,938] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run91
[2019-04-28 02:36:01,980] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:36:01,981] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:36:01,983] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run91
[2019-04-28 02:36:08,104] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:08,105] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [24.2, 44.5, 1.0, 2.0, 0.2144654637466976, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 358282.4353086074, 358282.4353086068, 156704.0575697172]
[2019-04-28 02:36:08,106] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:36:08,109] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [4.0458622e-13 9.8603743e-01 4.5691248e-22 1.3962509e-02 1.3963702e-14], sampled 0.8219369194506099
[2019-04-28 02:36:09,407] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:09,408] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [19.46666666666667, 89.0, 1.0, 2.0, 0.2531046859685488, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 415992.6476052666, 415992.647605266, 161216.3246881483]
[2019-04-28 02:36:09,412] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:36:09,414] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.6147165e-14 9.9995017e-01 8.3498964e-23 4.9876515e-05 3.9672832e-16], sampled 0.9583037643178415
[2019-04-28 02:36:37,860] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:37,861] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.83333333333333, 89.0, 1.0, 2.0, 0.7652901427489769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1136342.606158289, 1136342.60615829, 244539.8227294159]
[2019-04-28 02:36:37,862] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:36:37,865] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.8789580e-13 9.7624660e-01 5.0782468e-22 2.3753386e-02 1.8753054e-14], sampled 0.5141161786203701
[2019-04-28 02:36:41,696] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:41,697] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.21666666666667, 72.16666666666666, 1.0, 2.0, 0.5637086360816485, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787728.2456213266, 787728.2456213266, 194179.0597974107]
[2019-04-28 02:36:41,699] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:36:41,701] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.4696772e-14 9.9992359e-01 3.4403212e-23 7.6455122e-05 2.8006332e-16], sampled 0.4775364762662915
[2019-04-28 02:36:49,532] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:49,533] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [33.50209184333333, 80.15952813666667, 1.0, 2.0, 0.9286864594157318, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1298061.136371394, 1298061.136371394, 277964.3060848091]
[2019-04-28 02:36:49,534] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:36:49,536] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.0218102e-12 9.7224063e-01 1.2620007e-20 2.7759386e-02 1.4348725e-13], sampled 0.9271541749475773
[2019-04-28 02:36:52,070] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:52,072] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [36.9, 47.0, 1.0, 2.0, 0.6926576513634495, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 968004.1264376969, 968004.1264376969, 219272.6969877908]
[2019-04-28 02:36:52,073] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:36:52,076] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [6.6914481e-13 9.7208619e-01 8.3632428e-22 2.7913820e-02 2.7323336e-14], sampled 0.7969808395601543
[2019-04-28 02:36:54,925] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:36:54,926] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 79.0, 1.0, 2.0, 0.5883744232739123, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 822209.6359954597, 822209.6359954597, 198592.6564126737]
[2019-04-28 02:36:54,928] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:36:54,933] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.2857501e-13 9.6558094e-01 5.0002265e-22 3.4419015e-02 2.1853963e-14], sampled 0.3927076282694162
[2019-04-28 02:37:03,309] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:37:03,310] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.96666666666667, 92.66666666666667, 1.0, 2.0, 0.6368243200540762, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 889943.1398798657, 889943.1398798657, 207806.7735287169]
[2019-04-28 02:37:03,311] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:37:03,313] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.23149270e-14 9.99886155e-01 4.63400214e-23 1.13813745e-04
 4.01772820e-16], sampled 0.7960176754078647
[2019-04-28 02:37:06,621] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.11895169]
[2019-04-28 02:37:06,624] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [28.14990268333333, 82.77705396, 1.0, 2.0, 0.5426903036421901, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 758346.6761961309, 758346.6761961315, 190555.2100832486]
[2019-04-28 02:37:06,624] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:37:06,627] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.02783674e-13 9.99875069e-01 3.59343056e-22 1.24887389e-04
 1.45887206e-15], sampled 0.8011119014044089
[2019-04-28 02:37:27,815] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8127.3706 2932115356.3979 1201.0000
[2019-04-28 02:37:27,982] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7868.4399 3011944287.9820 1604.0000
[2019-04-28 02:37:28,071] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7728.9845 3169306992.0336 1630.0000
[2019-04-28 02:37:28,115] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8489.6412 2787589733.1356 848.0000
[2019-04-28 02:37:28,197] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8316.0513 2849747314.7014 1033.0000
[2019-04-28 02:37:29,212] A3C_AGENT_WORKER-Thread-12 INFO:Global step: 2250000, evaluation results [2250000.0, 7728.984499908721, 3169306992.0336127, 1630.0, 8127.370600371488, 2932115356.397874, 1201.0, 8489.641157412641, 2787589733.1356087, 848.0, 7868.439860785699, 3011944287.9820366, 1604.0, 8316.051296334417, 2849747314.7013655, 1033.0]
[2019-04-28 02:37:30,376] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.1281888e-13 9.9842227e-01 7.1985546e-24 1.5777594e-03 1.4164428e-15], sum to 1.0000
[2019-04-28 02:37:30,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0185
[2019-04-28 02:37:30,389] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.16666666666666, 70.0, 1.0, 2.0, 0.5160341835910508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 721085.2009765356, 721085.2009765356, 186148.2280716166], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4729800.0000, 
sim time next is 4730400.0000, 
raw observation next is [30.0, 70.0, 1.0, 2.0, 0.5176365907795337, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 723325.102033454, 723325.1020334547, 186406.7668423981], 
processed observation next is [1.0, 0.782608695652174, 0.6208530805687204, 0.7, 1.0, 1.0, 0.4188392659994382, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20092363945373723, 0.20092363945373742, 0.27821905498865385], 
reward next is 0.7218, 
noisyNet noise sample is [array([-1.8051838], dtype=float32), 1.5614139]. 
=============================================
[2019-04-28 02:37:30,821] A3C_AGENT_WORKER-Thread-19 INFO:Local step 140500, global step 2250772: loss -0.0881
[2019-04-28 02:37:30,823] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 140500, global step 2250772: learning rate 0.0000
[2019-04-28 02:37:31,523] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141000, global step 2251108: loss 0.0035
[2019-04-28 02:37:31,524] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141000, global step 2251109: learning rate 0.0000
[2019-04-28 02:37:33,148] A3C_AGENT_WORKER-Thread-22 INFO:Local step 140500, global step 2251879: loss -58.3218
[2019-04-28 02:37:33,150] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 140500, global step 2251879: learning rate 0.0000
[2019-04-28 02:37:33,410] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141000, global step 2252006: loss 0.0017
[2019-04-28 02:37:33,411] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141000, global step 2252006: learning rate 0.0000
[2019-04-28 02:37:33,857] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.2816843e-18 1.0000000e+00 2.7479253e-28 6.8191647e-11 1.7133101e-19], sum to 1.0000
[2019-04-28 02:37:33,863] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.3761
[2019-04-28 02:37:33,868] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 74.0, 1.0, 2.0, 0.4917798306321272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 687182.1911867269, 687182.1911867263, 182317.988432442], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4825200.0000, 
sim time next is 4825800.0000, 
raw observation next is [28.0, 74.0, 1.0, 2.0, 0.4921104465037565, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 687644.3225295888, 687644.3225295888, 182368.9809710474], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.74, 1.0, 1.0, 0.38808487530573077, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19101231181377468, 0.19101231181377468, 0.272192508912011], 
reward next is 0.7278, 
noisyNet noise sample is [array([0.911388], dtype=float32), -0.91685224]. 
=============================================
[2019-04-28 02:37:34,185] A3C_AGENT_WORKER-Thread-3 INFO:Local step 140500, global step 2252383: loss 7.9913
[2019-04-28 02:37:34,188] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 140500, global step 2252384: learning rate 0.0000
[2019-04-28 02:37:35,001] A3C_AGENT_WORKER-Thread-17 INFO:Local step 140500, global step 2252769: loss 12.5425
[2019-04-28 02:37:35,003] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 140500, global step 2252769: learning rate 0.0000
[2019-04-28 02:37:35,007] A3C_AGENT_WORKER-Thread-15 INFO:Local step 140500, global step 2252772: loss 27.4448
[2019-04-28 02:37:35,010] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 140500, global step 2252772: learning rate 0.0000
[2019-04-28 02:37:36,781] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.7722765e-15 1.0000000e+00 8.5559309e-24 2.3199768e-10 7.6967347e-19], sum to 1.0000
[2019-04-28 02:37:36,787] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.9335
[2019-04-28 02:37:36,793] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.5, 81.5, 1.0, 2.0, 0.7578257801204309, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1059123.418745315, 1059123.418745315, 233820.490975015], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5207400.0000, 
sim time next is 5208000.0000, 
raw observation next is [27.66666666666666, 80.66666666666667, 1.0, 2.0, 0.7683279187139259, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1073808.43622509, 1073808.43622509, 236283.6146859817], 
processed observation next is [1.0, 0.2608695652173913, 0.5102685624012636, 0.8066666666666668, 1.0, 1.0, 0.7208770104987059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2982801211736361, 0.2982801211736361, 0.35266211147161447], 
reward next is 0.6473, 
noisyNet noise sample is [array([1.723977], dtype=float32), 1.6254115]. 
=============================================
[2019-04-28 02:37:36,809] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[51.83953 ]
 [52.121624]
 [52.401016]
 [52.530785]
 [52.750687]], R is [[51.08685684]
 [51.22700119]
 [51.36667633]
 [51.50063705]
 [51.61657715]].
[2019-04-28 02:37:37,020] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141000, global step 2253744: loss -6.2659
[2019-04-28 02:37:37,023] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141000, global step 2253744: learning rate 0.0000
[2019-04-28 02:37:37,466] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141000, global step 2253960: loss 0.0031
[2019-04-28 02:37:37,470] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141000, global step 2253960: learning rate 0.0000
[2019-04-28 02:37:38,027] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141000, global step 2254228: loss 0.0046
[2019-04-28 02:37:38,030] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141000, global step 2254228: learning rate 0.0000
[2019-04-28 02:37:39,418] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141000, global step 2254909: loss 0.0128
[2019-04-28 02:37:39,421] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141000, global step 2254909: learning rate 0.0000
[2019-04-28 02:37:40,512] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141000, global step 2255446: loss 0.0028
[2019-04-28 02:37:40,518] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141000, global step 2255448: learning rate 0.0000
[2019-04-28 02:37:42,156] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141000, global step 2256247: loss 0.0025
[2019-04-28 02:37:42,160] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141000, global step 2256248: learning rate 0.0000
[2019-04-28 02:37:42,502] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141000, global step 2256419: loss 0.0003
[2019-04-28 02:37:42,506] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141000, global step 2256421: learning rate 0.0000
[2019-04-28 02:37:44,266] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141000, global step 2257279: loss 0.0205
[2019-04-28 02:37:44,270] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141000, global step 2257280: learning rate 0.0000
[2019-04-28 02:37:45,562] A3C_AGENT_WORKER-Thread-18 INFO:Local step 141500, global step 2257916: loss -36.6673
[2019-04-28 02:37:45,565] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 141500, global step 2257916: learning rate 0.0000
[2019-04-28 02:37:47,303] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.6189482e-15 9.9999273e-01 1.4413184e-24 7.3254537e-06 4.0733761e-16], sum to 1.0000
[2019-04-28 02:37:47,305] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.6802
[2019-04-28 02:37:47,314] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 78.16666666666667, 1.0, 2.0, 0.5220866714079402, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 729545.6058289391, 729545.6058289384, 187128.7836591793], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5169000.0000, 
sim time next is 5169600.0000, 
raw observation next is [28.0, 79.0, 1.0, 2.0, 0.5212672859450231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 728400.2327483637, 728400.232748363, 186995.1039861079], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.79, 1.0, 1.0, 0.42321359752412424, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2023333979856566, 0.20233339798565642, 0.27909717012851926], 
reward next is 0.7209, 
noisyNet noise sample is [array([-0.84711117], dtype=float32), -0.33036834]. 
=============================================
[2019-04-28 02:37:47,581] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141000, global step 2258894: loss 0.0081
[2019-04-28 02:37:47,583] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141000, global step 2258894: learning rate 0.0000
[2019-04-28 02:37:48,304] A3C_AGENT_WORKER-Thread-20 INFO:Local step 141500, global step 2259253: loss -8.4267
[2019-04-28 02:37:48,310] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 141500, global step 2259253: learning rate 0.0000
[2019-04-28 02:37:49,290] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.5475228e-14 9.9860471e-01 9.4362620e-23 1.3953183e-03 8.0239754e-18], sum to 1.0000
[2019-04-28 02:37:49,292] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.4480
[2019-04-28 02:37:49,298] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 84.0, 1.0, 2.0, 0.4830693989562231, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675067.2344215836, 675067.2344215829, 180993.0059718229], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5109000.0000, 
sim time next is 5109600.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.4824779490548618, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 674300.589662005, 674300.5896620057, 180911.1572169932], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.37647945669260463, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18730571935055693, 0.18730571935055712, 0.27001665256267643], 
reward next is 0.7300, 
noisyNet noise sample is [array([0.79632974], dtype=float32), 1.946254]. 
=============================================
[2019-04-28 02:37:49,417] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141000, global step 2259786: loss 0.0025
[2019-04-28 02:37:49,418] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141000, global step 2259786: learning rate 0.0000
[2019-04-28 02:37:49,831] A3C_AGENT_WORKER-Thread-2 INFO:Local step 141500, global step 2259994: loss -24.6732
[2019-04-28 02:37:49,832] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 141500, global step 2259994: learning rate 0.0000
[2019-04-28 02:37:50,521] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141000, global step 2260327: loss 0.0222
[2019-04-28 02:37:50,524] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141000, global step 2260327: learning rate 0.0000
[2019-04-28 02:37:51,380] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141000, global step 2260740: loss 0.0440
[2019-04-28 02:37:51,384] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141000, global step 2260740: learning rate 0.0000
[2019-04-28 02:37:51,424] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.2842589e-15 9.9998164e-01 9.4837544e-24 1.8404660e-05 1.9843784e-18], sum to 1.0000
[2019-04-28 02:37:51,433] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.4913
[2019-04-28 02:37:51,440] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 83.16666666666666, 1.0, 2.0, 0.5090934949786691, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 711383.3171582368, 711383.3171582373, 185032.8877888426], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5183400.0000, 
sim time next is 5184000.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.5120795447716742, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 715557.288129317, 715557.288129317, 185510.2060561532], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.41214402984539056, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19876591336925473, 0.19876591336925473, 0.27688090456142267], 
reward next is 0.7231, 
noisyNet noise sample is [array([-1.1613545], dtype=float32), -0.40902403]. 
=============================================
[2019-04-28 02:37:51,467] A3C_AGENT_WORKER-Thread-19 DEBUG:Value prediction is [[69.23502 ]
 [69.29353 ]
 [69.37379 ]
 [69.43955 ]
 [69.496284]], R is [[61.01063919]
 [61.12436295]
 [61.23776245]
 [61.35095596]
 [61.46383286]].
[2019-04-28 02:37:51,572] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141000, global step 2260837: loss 0.0199
[2019-04-28 02:37:51,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141000, global step 2260837: learning rate 0.0000
[2019-04-28 02:37:53,650] A3C_AGENT_WORKER-Thread-11 INFO:Local step 141500, global step 2261836: loss -35.9930
[2019-04-28 02:37:53,654] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 141500, global step 2261837: learning rate 0.0000
[2019-04-28 02:37:53,738] A3C_AGENT_WORKER-Thread-14 INFO:Local step 141500, global step 2261881: loss -25.3258
[2019-04-28 02:37:53,740] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 141500, global step 2261882: learning rate 0.0000
[2019-04-28 02:37:54,471] A3C_AGENT_WORKER-Thread-9 INFO:Local step 141500, global step 2262233: loss -28.8843
[2019-04-28 02:37:54,473] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 141500, global step 2262233: learning rate 0.0000
[2019-04-28 02:37:55,971] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [1.5084269e-10 3.3291861e-02 5.9718395e-18 9.6670812e-01 9.6172560e-13], sum to 1.0000
[2019-04-28 02:37:55,980] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5345
[2019-04-28 02:37:55,983] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.93333333333333, 92.33333333333333, 1.0, 2.0, 0.5527456217652795, 1.0, 2.0, 0.5527456217652795, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1545357.084976244, 1545357.084976244, 317463.645158049], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5452800.0000, 
sim time next is 5453400.0000, 
raw observation next is [27.91666666666666, 92.16666666666667, 1.0, 2.0, 0.5318864473531667, 1.0, 2.0, 0.5318864473531667, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 1486998.894866235, 1486998.894866234, 310465.6795163539], 
processed observation next is [1.0, 0.08695652173913043, 0.5221169036334911, 0.9216666666666667, 1.0, 1.0, 0.43600776789538154, 1.0, 1.0, 0.43600776789538154, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.4130552485739542, 0.41305524857395387, 0.4633816112184386], 
reward next is 0.5366, 
noisyNet noise sample is [array([-2.167375], dtype=float32), -0.7514895]. 
=============================================
[2019-04-28 02:37:56,152] A3C_AGENT_WORKER-Thread-10 INFO:Local step 141500, global step 2263059: loss -25.2503
[2019-04-28 02:37:56,155] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 141500, global step 2263060: learning rate 0.0000
[2019-04-28 02:37:56,974] A3C_AGENT_WORKER-Thread-12 INFO:Local step 141500, global step 2263470: loss -15.7731
[2019-04-28 02:37:56,976] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 141500, global step 2263471: learning rate 0.0000
[2019-04-28 02:37:58,601] A3C_AGENT_WORKER-Thread-13 INFO:Local step 141500, global step 2264254: loss -21.2795
[2019-04-28 02:37:58,601] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 141500, global step 2264254: learning rate 0.0000
[2019-04-28 02:37:58,991] A3C_AGENT_WORKER-Thread-21 INFO:Local step 141500, global step 2264443: loss -31.9498
[2019-04-28 02:37:58,993] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 141500, global step 2264444: learning rate 0.0000
[2019-04-28 02:38:00,593] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.8092116e-09 6.2332948e-04 5.6051366e-17 9.9937671e-01 1.0441525e-11], sum to 1.0000
[2019-04-28 02:38:00,600] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.0323
[2019-04-28 02:38:00,615] A3C_AGENT_WORKER-Thread-16 DEBUG:Action function: raw action 3 has been changed to 4 for the demand 3416665.849773243 W.
[2019-04-28 02:38:00,623] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [38.0, 54.0, 1.0, 2.0, 0.9869666051855691, 1.0, 2.0, 0.8140733421070473, 1.0, 1.0, 1.03, 7.005120370123514, 6.9112, 170.5573041426782, 3416665.849773243, 3349386.868217377, 627310.2915674832], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 5410800.0000, 
sim time next is 5411400.0000, 
raw observation next is [37.5, 53.83333333333333, 1.0, 2.0, 0.8589123711890123, 1.0, 2.0, 0.7500462251087687, 1.0, 2.0, 1.03, 7.005110265595649, 6.9112, 170.5573041426782, 3147605.362066561, 3080333.618795031, 576222.691235153], 
processed observation next is [1.0, 0.6521739130434783, 0.976303317535545, 0.5383333333333333, 1.0, 1.0, 0.8300149050470028, 1.0, 1.0, 0.6988508736250225, 1.0, 1.0, 1.0365853658536586, 0.009391026559564874, 0.0, 0.8375144448122397, 0.874334822796267, 0.8556482274430642, 0.8600338675151538], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.4356773], dtype=float32), 0.16300732]. 
=============================================
[2019-04-28 02:38:00,655] A3C_AGENT_WORKER-Thread-16 INFO:Local step 141500, global step 2265260: loss -20.5461
[2019-04-28 02:38:00,656] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 141500, global step 2265260: learning rate 0.0000
[2019-04-28 02:38:00,995] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [1.1944609e-09 5.6531634e-03 5.5621843e-16 9.9434680e-01 3.7589207e-12], sum to 1.0000
[2019-04-28 02:38:01,003] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.6029
[2019-04-28 02:38:01,009] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.56666666666666, 84.66666666666667, 1.0, 2.0, 0.3017957332785292, 1.0, 2.0, 0.3017957332785292, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 843479.5404031101, 843479.5404031101, 250236.6272057566], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5359200.0000, 
sim time next is 5359800.0000, 
raw observation next is [29.53333333333333, 84.83333333333334, 1.0, 2.0, 0.301559657940448, 1.0, 2.0, 0.301559657940448, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 842819.4815009598, 842819.4815009598, 250190.7412920052], 
processed observation next is [1.0, 0.0, 0.598736176935229, 0.8483333333333334, 1.0, 1.0, 0.15850561197644333, 1.0, 1.0, 0.15850561197644333, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2341165226391555, 0.2341165226391555, 0.3734190168537391], 
reward next is 0.6266, 
noisyNet noise sample is [array([0.7185948], dtype=float32), -0.085579365]. 
=============================================
[2019-04-28 02:38:01,914] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142000, global step 2265863: loss 0.0707
[2019-04-28 02:38:01,915] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142000, global step 2265863: learning rate 0.0000
[2019-04-28 02:38:04,104] A3C_AGENT_WORKER-Thread-19 INFO:Local step 141500, global step 2266931: loss -30.2903
[2019-04-28 02:38:04,105] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 141500, global step 2266931: learning rate 0.0000
[2019-04-28 02:38:04,349] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142000, global step 2267044: loss 0.0192
[2019-04-28 02:38:04,351] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142000, global step 2267046: learning rate 0.0000
[2019-04-28 02:38:05,712] A3C_AGENT_WORKER-Thread-22 INFO:Local step 141500, global step 2267707: loss -30.6629
[2019-04-28 02:38:05,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 141500, global step 2267708: learning rate 0.0000
[2019-04-28 02:38:06,164] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142000, global step 2267928: loss 0.0453
[2019-04-28 02:38:06,170] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142000, global step 2267928: learning rate 0.0000
[2019-04-28 02:38:06,731] A3C_AGENT_WORKER-Thread-3 INFO:Local step 141500, global step 2268191: loss -20.0575
[2019-04-28 02:38:06,732] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 141500, global step 2268191: learning rate 0.0000
[2019-04-28 02:38:06,997] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.19090568e-10 5.80151216e-04 1.12547005e-17 9.99419928e-01
 1.56506901e-13], sum to 1.0000
[2019-04-28 02:38:07,003] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3117
[2019-04-28 02:38:07,011] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.25, 92.66666666666666, 1.0, 2.0, 0.4938335915264657, 1.0, 2.0, 0.4938335915264657, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1380545.733072057, 1380545.733072057, 298346.812115305], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5802600.0000, 
sim time next is 5803200.0000, 
raw observation next is [26.2, 93.0, 1.0, 2.0, 0.4853472229390219, 1.0, 2.0, 0.4853472229390219, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1356806.492278372, 1356806.492278372, 295762.617505196], 
processed observation next is [1.0, 0.17391304347826086, 0.44075829383886256, 0.93, 1.0, 1.0, 0.3799364131795444, 1.0, 1.0, 0.3799364131795444, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.37689069229954775, 0.37689069229954775, 0.4414367425450687], 
reward next is 0.5586, 
noisyNet noise sample is [array([-0.27760234], dtype=float32), -1.029448]. 
=============================================
[2019-04-28 02:38:07,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.5271855e-12 6.7490349e-05 1.0050697e-20 9.9993253e-01 1.2349632e-16], sum to 1.0000
[2019-04-28 02:38:07,346] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6380
[2019-04-28 02:38:07,351] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.88333333333333, 75.66666666666667, 1.0, 2.0, 0.291182604916576, 1.0, 2.0, 0.291182604916576, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 813805.9829854352, 813805.9829854352, 248207.3456549059], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5422200.0000, 
sim time next is 5422800.0000, 
raw observation next is [30.86666666666667, 76.33333333333334, 1.0, 2.0, 0.2937328488507177, 1.0, 2.0, 0.2937328488507177, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 820936.2099935074, 820936.2099935074, 248689.3377826392], 
processed observation next is [1.0, 0.782608695652174, 0.6619273301737759, 0.7633333333333334, 1.0, 1.0, 0.1490757215068888, 1.0, 1.0, 0.1490757215068888, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2280378361093076, 0.2280378361093076, 0.37117811609349133], 
reward next is 0.6288, 
noisyNet noise sample is [array([-0.0432447], dtype=float32), -0.14543109]. 
=============================================
[2019-04-28 02:38:08,110] A3C_AGENT_WORKER-Thread-15 INFO:Local step 141500, global step 2268853: loss -32.8058
[2019-04-28 02:38:08,112] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 141500, global step 2268853: learning rate 0.0000
[2019-04-28 02:38:08,128] A3C_AGENT_WORKER-Thread-17 INFO:Local step 141500, global step 2268860: loss -32.5791
[2019-04-28 02:38:08,130] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 141500, global step 2268860: learning rate 0.0000
[2019-04-28 02:38:08,842] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1667195e-10 1.2585381e-03 2.1877092e-16 9.9874145e-01 1.5617695e-11], sum to 1.0000
[2019-04-28 02:38:08,849] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.5413
[2019-04-28 02:38:08,854] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.9, 81.0, 1.0, 2.0, 0.5251410543446003, 1.0, 2.0, 0.5251410543446003, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1468127.834201951, 1468127.834201951, 308259.5597940517], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5468400.0000, 
sim time next is 5469000.0000, 
raw observation next is [30.08333333333334, 80.16666666666667, 1.0, 2.0, 0.5138572525904204, 1.0, 2.0, 0.5138572525904204, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1436560.763593794, 1436560.763593794, 304625.7112723808], 
processed observation next is [1.0, 0.30434782608695654, 0.6248025276461299, 0.8016666666666667, 1.0, 1.0, 0.41428584649448236, 1.0, 1.0, 0.41428584649448236, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3990446565538317, 0.3990446565538317, 0.45466524070504594], 
reward next is 0.5453, 
noisyNet noise sample is [array([1.8909944], dtype=float32), -0.21721317]. 
=============================================
[2019-04-28 02:38:08,869] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[46.98699 ]
 [46.730618]
 [46.916603]
 [47.04779 ]
 [47.19687 ]], R is [[47.17907715]
 [47.2471962 ]
 [47.30957413]
 [47.37381363]
 [47.44400024]].
[2019-04-28 02:38:09,082] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.11004781e-12 1.03827275e-04 7.84966209e-21 9.99896169e-01
 4.60342554e-16], sum to 1.0000
[2019-04-28 02:38:09,088] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3203
[2019-04-28 02:38:09,091] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [32.4, 61.5, 1.0, 2.0, 0.2761996886709062, 1.0, 2.0, 0.2761996886709062, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 771916.2048132498, 771916.2048132491, 245448.8489144172], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5761800.0000, 
sim time next is 5762400.0000, 
raw observation next is [32.23333333333333, 62.66666666666666, 1.0, 2.0, 0.2774059183702279, 1.0, 2.0, 0.2774059183702279, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 775288.5647997874, 775288.5647997874, 245665.8993292996], 
processed observation next is [0.0, 0.6956521739130435, 0.7266982622432857, 0.6266666666666666, 1.0, 1.0, 0.12940472092798538, 1.0, 1.0, 0.12940472092798538, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2153579346666076, 0.2153579346666076, 0.3666655213870143], 
reward next is 0.6333, 
noisyNet noise sample is [array([-0.39000154], dtype=float32), 1.232438]. 
=============================================
[2019-04-28 02:38:10,155] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142000, global step 2269837: loss 0.0113
[2019-04-28 02:38:10,160] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142000, global step 2269838: learning rate 0.0000
[2019-04-28 02:38:10,206] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142000, global step 2269865: loss 0.0325
[2019-04-28 02:38:10,210] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142000, global step 2269865: learning rate 0.0000
[2019-04-28 02:38:10,905] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142000, global step 2270202: loss 0.0037
[2019-04-28 02:38:10,906] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142000, global step 2270202: learning rate 0.0000
[2019-04-28 02:38:12,408] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142000, global step 2270924: loss 0.0127
[2019-04-28 02:38:12,411] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142000, global step 2270924: learning rate 0.0000
[2019-04-28 02:38:12,592] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.2094315e-13 7.4215241e-05 4.8942672e-22 9.9992573e-01 1.4180556e-16], sum to 1.0000
[2019-04-28 02:38:12,611] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1171
[2019-04-28 02:38:12,617] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.8, 53.0, 1.0, 2.0, 0.2654293822577881, 1.0, 2.0, 0.2654293822577881, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 741805.204009137, 741805.2040091376, 243549.8639904655], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5755800.0000, 
sim time next is 5756400.0000, 
raw observation next is [34.0, 53.0, 1.0, 2.0, 0.2680874309153953, 1.0, 2.0, 0.2680874309153953, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 749236.3444053718, 749236.3444053718, 244012.6565066332], 
processed observation next is [0.0, 0.6521739130434783, 0.8104265402843602, 0.53, 1.0, 1.0, 0.11817762760890996, 1.0, 1.0, 0.11817762760890996, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20812120677926996, 0.20812120677926996, 0.3641979947860197], 
reward next is 0.6358, 
noisyNet noise sample is [array([0.72574145], dtype=float32), 0.73514324]. 
=============================================
[2019-04-28 02:38:13,614] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142000, global step 2271507: loss 0.0181
[2019-04-28 02:38:13,617] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142000, global step 2271507: learning rate 0.0000
[2019-04-28 02:38:15,193] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142000, global step 2272276: loss 0.0232
[2019-04-28 02:38:15,196] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142000, global step 2272276: learning rate 0.0000
[2019-04-28 02:38:15,610] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142000, global step 2272479: loss 0.0109
[2019-04-28 02:38:15,612] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142000, global step 2272480: learning rate 0.0000
[2019-04-28 02:38:16,058] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.0154696e-12 5.0334165e-05 7.9214661e-21 9.9994969e-01 1.4584802e-15], sum to 1.0000
[2019-04-28 02:38:16,067] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.6052
[2019-04-28 02:38:16,073] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.65, 91.00000000000001, 1.0, 2.0, 0.2668246878695507, 1.0, 2.0, 0.2668246878695507, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 745706.0712122779, 745706.0712122779, 243789.5772218539], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5965800.0000, 
sim time next is 5966400.0000, 
raw observation next is [26.6, 91.0, 1.0, 2.0, 0.2658764573683047, 1.0, 2.0, 0.2658764573683047, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 743055.0939730533, 743055.0939730533, 243624.7931530253], 
processed observation next is [1.0, 0.043478260869565216, 0.4597156398104266, 0.91, 1.0, 1.0, 0.11551380405819839, 1.0, 1.0, 0.11551380405819839, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20640419277029257, 0.20640419277029257, 0.36361909425824673], 
reward next is 0.6364, 
noisyNet noise sample is [array([-0.05388137], dtype=float32), -0.4200449]. 
=============================================
[2019-04-28 02:38:17,221] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142000, global step 2273267: loss 0.0751
[2019-04-28 02:38:17,233] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142000, global step 2273269: learning rate 0.0000
[2019-04-28 02:38:18,473] A3C_AGENT_WORKER-Thread-18 INFO:Local step 142500, global step 2273869: loss 0.8307
[2019-04-28 02:38:18,476] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 142500, global step 2273869: learning rate 0.0000
[2019-04-28 02:38:19,421] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [2.0916487e-11 6.7471563e-05 3.3407250e-18 9.9993253e-01 2.8940825e-13], sum to 1.0000
[2019-04-28 02:38:19,432] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.1601
[2019-04-28 02:38:19,437] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.03333333333333, 88.33333333333334, 1.0, 2.0, 0.2697676693034611, 1.0, 2.0, 0.2697676693034611, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 753933.834769953, 753933.834769953, 244303.2934293333], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5790000.0000, 
sim time next is 5790600.0000, 
raw observation next is [27.0, 88.5, 1.0, 2.0, 0.2695521358337792, 1.0, 2.0, 0.2695521358337792, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 753331.2605536943, 753331.2605536943, 244265.4321148291], 
processed observation next is [1.0, 0.0, 0.4786729857819906, 0.885, 1.0, 1.0, 0.11994233232985443, 1.0, 1.0, 0.11994233232985443, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.2092586834871373, 0.2092586834871373, 0.3645752718131778], 
reward next is 0.6354, 
noisyNet noise sample is [array([-0.1524289], dtype=float32), 0.6488867]. 
=============================================
[2019-04-28 02:38:20,594] A3C_AGENT_WORKER-Thread-20 INFO:Local step 142500, global step 2274899: loss 0.1480
[2019-04-28 02:38:20,597] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 142500, global step 2274899: learning rate 0.0000
[2019-04-28 02:38:20,803] A3C_AGENT_WORKER-Thread-11 INFO:Evaluating...
[2019-04-28 02:38:20,806] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:38:20,808] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:38:20,809] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:38:20,811] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:38:20,811] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:38:20,814] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:38:20,813] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:38:20,815] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:38:20,816] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:38:20,815] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:38:20,830] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run92
[2019-04-28 02:38:20,863] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run92
[2019-04-28 02:38:20,865] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run92
[2019-04-28 02:38:20,928] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run92
[2019-04-28 02:38:20,929] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run92
[2019-04-28 02:38:35,049] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:38:35,050] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [21.21666666666667, 92.0, 1.0, 2.0, 0.1721702338204535, 1.0, 2.0, 0.1721702338204535, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 540639.5416848232, 540639.5416848232, 240579.7415702429]
[2019-04-28 02:38:35,051] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:38:35,053] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.5962346e-13 8.0004120e-06 6.0801612e-21 9.9999201e-01 9.1271474e-16], sampled 0.6532268356759736
[2019-04-28 02:38:36,395] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:38:36,396] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [22.65, 62.5, 1.0, 2.0, 0.17, 1.0, 2.0, 0.17, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 169.0403247858759, 388118.4201777965, 388118.4201777959, 210631.1491672755]
[2019-04-28 02:38:36,398] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:38:36,399] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [6.7761356e-13 1.1832346e-05 2.8641232e-20 9.9998820e-01 2.9033469e-15], sampled 0.22933864826385064
[2019-04-28 02:40:07,710] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:40:07,710] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.0, 78.16666666666667, 1.0, 2.0, 0.2555420630242202, 1.0, 2.0, 0.2555420630242202, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 714163.5519712016, 714163.5519712016, 241863.3560149085]
[2019-04-28 02:40:07,711] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:40:07,712] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.4853099e-13 7.8627936e-06 5.6629904e-21 9.9999213e-01 8.6608817e-16], sampled 0.1470691373070001
[2019-04-28 02:40:10,220] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:40:10,221] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [38.45, 39.5, 1.0, 2.0, 0.7574035316347129, 1.0, 2.0, 0.7574035316347129, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 2118120.624180559, 2118120.624180559, 399383.2918375853]
[2019-04-28 02:40:10,221] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:40:10,223] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.7642584e-12 2.8293889e-05 9.0866163e-19 9.9997175e-01 3.8317209e-14], sampled 0.584286326604541
[2019-04-28 02:40:14,367] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:40:14,368] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.342849945, 86.80359025499999, 1.0, 2.0, 0.2416261487514677, 1.0, 2.0, 0.2416261487514677, 0.0, 2.0, 0.0, 6.9112, 6.9112, 171.5212843490159, 675259.3019910225, 675259.3019910225, 239782.6994884695]
[2019-04-28 02:40:14,369] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:40:14,371] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.3978573e-13 7.7492095e-06 5.3443978e-21 9.9999225e-01 8.2952885e-16], sampled 0.8775758050969235
[2019-04-28 02:40:38,061] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12102659]
[2019-04-28 02:40:38,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [25.9, 94.33333333333334, 1.0, 2.0, 0.2667353488142529, 1.0, 2.0, 0.2667353488142529, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 745452.8505099762, 745452.8505099762, 244257.0347619248]
[2019-04-28 02:40:38,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:40:38,065] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [2.3362253e-13 7.6672468e-06 5.1254825e-21 9.9999237e-01 8.0381685e-16], sampled 0.05723831580660155
[2019-04-28 02:40:48,098] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 02:40:48,480] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 02:40:48,878] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 02:40:48,956] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6857.2441 3474993395.0658 8.0000
[2019-04-28 02:40:49,106] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 02:40:50,126] A3C_AGENT_WORKER-Thread-11 INFO:Global step: 2275000, evaluation results [2275000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6857.244101597982, 3474993395.065798, 8.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 02:40:50,254] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142000, global step 2275058: loss 0.0136
[2019-04-28 02:40:50,258] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142000, global step 2275059: learning rate 0.0000
[2019-04-28 02:40:51,696] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142000, global step 2275655: loss 0.0340
[2019-04-28 02:40:51,700] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142000, global step 2275656: learning rate 0.0000
[2019-04-28 02:40:52,183] A3C_AGENT_WORKER-Thread-2 INFO:Local step 142500, global step 2275849: loss 0.5754
[2019-04-28 02:40:52,183] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 142500, global step 2275849: learning rate 0.0000
[2019-04-28 02:40:53,085] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142000, global step 2276200: loss 0.0269
[2019-04-28 02:40:53,086] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142000, global step 2276201: learning rate 0.0000
[2019-04-28 02:40:54,716] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142000, global step 2276910: loss 0.0402
[2019-04-28 02:40:54,719] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142000, global step 2276910: learning rate 0.0000
[2019-04-28 02:40:54,737] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142000, global step 2276917: loss 0.0109
[2019-04-28 02:40:54,740] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142000, global step 2276917: learning rate 0.0000
[2019-04-28 02:40:55,942] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.4108987e-11 3.6287434e-05 2.8174274e-18 9.9996376e-01 2.8051913e-14], sum to 1.0000
[2019-04-28 02:40:55,947] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.4221
[2019-04-28 02:40:55,953] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.05, 88.33333333333334, 1.0, 2.0, 0.350900765796348, 1.0, 2.0, 0.350900765796348, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 980784.4322995131, 980784.4322995131, 260467.3236272577], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6070200.0000, 
sim time next is 6070800.0000, 
raw observation next is [27.2, 87.66666666666667, 1.0, 2.0, 0.3401168544625736, 1.0, 2.0, 0.3401168544625736, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 950629.5239421419, 950629.5239421419, 258096.5041557487], 
processed observation next is [1.0, 0.2608695652173913, 0.4881516587677725, 0.8766666666666667, 1.0, 1.0, 0.20496006561755856, 1.0, 1.0, 0.20496006561755856, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.26406375665059495, 0.26406375665059495, 0.3852186629190279], 
reward next is 0.6148, 
noisyNet noise sample is [array([-0.24405944], dtype=float32), 1.5413266]. 
=============================================
[2019-04-28 02:40:56,775] A3C_AGENT_WORKER-Thread-14 INFO:Local step 142500, global step 2277773: loss 0.4669
[2019-04-28 02:40:56,777] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 142500, global step 2277773: learning rate 0.0000
[2019-04-28 02:40:56,895] A3C_AGENT_WORKER-Thread-11 INFO:Local step 142500, global step 2277821: loss 0.2586
[2019-04-28 02:40:56,896] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 142500, global step 2277821: learning rate 0.0000
[2019-04-28 02:40:58,135] A3C_AGENT_WORKER-Thread-9 INFO:Local step 142500, global step 2278345: loss 0.0463
[2019-04-28 02:40:58,138] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 142500, global step 2278347: learning rate 0.0000
[2019-04-28 02:40:59,387] A3C_AGENT_WORKER-Thread-10 INFO:Local step 142500, global step 2278875: loss 0.0303
[2019-04-28 02:40:59,389] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 142500, global step 2278875: learning rate 0.0000
[2019-04-28 02:41:01,128] A3C_AGENT_WORKER-Thread-12 INFO:Local step 142500, global step 2279618: loss 0.0330
[2019-04-28 02:41:01,130] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 142500, global step 2279618: learning rate 0.0000
[2019-04-28 02:41:02,549] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.4471349e-12 9.5493291e-05 5.5799872e-19 9.9990451e-01 4.1114383e-14], sum to 1.0000
[2019-04-28 02:41:02,551] A3C_AGENT_WORKER-Thread-13 INFO:Local step 142500, global step 2280191: loss 0.0470
[2019-04-28 02:41:02,554] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 142500, global step 2280192: learning rate 0.0000
[2019-04-28 02:41:02,555] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6019
[2019-04-28 02:41:02,562] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.6, 75.0, 1.0, 2.0, 0.8797299077778999, 1.0, 2.0, 0.8797299077778999, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2460543.505182408, 2460543.505182408, 460557.4050096393], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5997600.0000, 
sim time next is 5998200.0000, 
raw observation next is [30.76666666666667, 74.33333333333333, 1.0, 2.0, 0.790098867097114, 1.0, 2.0, 0.790098867097114, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2209629.630646077, 2209629.630646078, 415086.4065488809], 
processed observation next is [1.0, 0.43478260869565216, 0.6571879936808849, 0.7433333333333333, 1.0, 1.0, 0.7471070687917035, 1.0, 1.0, 0.7471070687917035, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6137860085127992, 0.6137860085127994, 0.6195319500729566], 
reward next is 0.3805, 
noisyNet noise sample is [array([0.61646986], dtype=float32), -1.3723725]. 
=============================================
[2019-04-28 02:41:03,437] A3C_AGENT_WORKER-Thread-21 INFO:Local step 142500, global step 2280535: loss 0.5228
[2019-04-28 02:41:03,441] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 142500, global step 2280536: learning rate 0.0000
[2019-04-28 02:41:05,201] A3C_AGENT_WORKER-Thread-16 INFO:Local step 142500, global step 2281289: loss 0.1560
[2019-04-28 02:41:05,215] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 142500, global step 2281290: learning rate 0.0000
[2019-04-28 02:41:06,773] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143000, global step 2281964: loss 0.6744
[2019-04-28 02:41:06,774] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143000, global step 2281964: learning rate 0.0000
[2019-04-28 02:41:07,805] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [7.0686404e-12 6.7553709e-05 1.2938784e-18 9.9993241e-01 1.4101376e-13], sum to 1.0000
[2019-04-28 02:41:07,812] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.0183
[2019-04-28 02:41:07,819] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.88333333333333, 86.5, 1.0, 2.0, 0.5468847745148994, 1.0, 2.0, 0.5468847745148994, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1528959.737381761, 1528959.737381761, 315465.1757973576], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6163800.0000, 
sim time next is 6164400.0000, 
raw observation next is [27.96666666666667, 86.0, 1.0, 2.0, 0.7390542525490109, 1.0, 2.0, 0.7390542525490109, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2066738.017676522, 2066738.017676523, 391292.7926834461], 
processed observation next is [1.0, 0.34782608695652173, 0.524486571879937, 0.86, 1.0, 1.0, 0.6856075331915794, 1.0, 1.0, 0.6856075331915794, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.5740938937990339, 0.5740938937990342, 0.5840190935573822], 
reward next is 0.4160, 
noisyNet noise sample is [array([-1.361367], dtype=float32), -2.3885317]. 
=============================================
[2019-04-28 02:41:08,675] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143000, global step 2282751: loss 0.5873
[2019-04-28 02:41:08,676] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143000, global step 2282751: learning rate 0.0000
[2019-04-28 02:41:09,552] A3C_AGENT_WORKER-Thread-19 INFO:Local step 142500, global step 2283129: loss 0.0604
[2019-04-28 02:41:09,555] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 142500, global step 2283129: learning rate 0.0000
[2019-04-28 02:41:10,710] A3C_AGENT_WORKER-Thread-22 INFO:Local step 142500, global step 2283632: loss 0.0340
[2019-04-28 02:41:10,714] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 142500, global step 2283632: learning rate 0.0000
[2019-04-28 02:41:11,179] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143000, global step 2283835: loss 0.0541
[2019-04-28 02:41:11,181] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143000, global step 2283837: learning rate 0.0000
[2019-04-28 02:41:12,367] A3C_AGENT_WORKER-Thread-3 INFO:Local step 142500, global step 2284329: loss 0.0609
[2019-04-28 02:41:12,369] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 142500, global step 2284332: learning rate 0.0000
[2019-04-28 02:41:13,351] A3C_AGENT_WORKER-Thread-15 INFO:Local step 142500, global step 2284749: loss 0.0246
[2019-04-28 02:41:13,356] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 142500, global step 2284751: learning rate 0.0000
[2019-04-28 02:41:13,705] A3C_AGENT_WORKER-Thread-17 INFO:Local step 142500, global step 2284891: loss 0.0256
[2019-04-28 02:41:13,708] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 142500, global step 2284891: learning rate 0.0000
[2019-04-28 02:41:15,260] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143000, global step 2285582: loss 0.5570
[2019-04-28 02:41:15,266] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143000, global step 2285584: learning rate 0.0000
[2019-04-28 02:41:15,937] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143000, global step 2285870: loss 1.0431
[2019-04-28 02:41:15,939] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143000, global step 2285871: learning rate 0.0000
[2019-04-28 02:41:16,508] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143000, global step 2286110: loss 1.7008
[2019-04-28 02:41:16,512] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143000, global step 2286110: learning rate 0.0000
[2019-04-28 02:41:18,532] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143000, global step 2286950: loss 0.1162
[2019-04-28 02:41:18,533] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143000, global step 2286950: learning rate 0.0000
[2019-04-28 02:41:20,067] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143000, global step 2287562: loss 0.1034
[2019-04-28 02:41:20,069] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143000, global step 2287562: learning rate 0.0000
[2019-04-28 02:41:22,008] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143000, global step 2288376: loss 0.6220
[2019-04-28 02:41:22,014] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143000, global step 2288377: learning rate 0.0000
[2019-04-28 02:41:22,923] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143000, global step 2288762: loss 0.1988
[2019-04-28 02:41:22,925] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143000, global step 2288762: learning rate 0.0000
[2019-04-28 02:41:24,225] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143000, global step 2289310: loss 1.5353
[2019-04-28 02:41:24,227] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143000, global step 2289310: learning rate 0.0000
[2019-04-28 02:41:24,333] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.43037036e-12 1.26175102e-04 1.15743554e-20 9.99873757e-01
 1.64630493e-14], sum to 1.0000
[2019-04-28 02:41:24,343] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.2167
[2019-04-28 02:41:24,349] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.8, 75.66666666666666, 1.0, 2.0, 0.2459184692762786, 1.0, 2.0, 0.2459184692762786, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 687259.8726487079, 687259.8726487074, 240279.1783470644], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6554400.0000, 
sim time next is 6555000.0000, 
raw observation next is [27.75, 76.33333333333334, 1.0, 2.0, 0.24669999168808, 1.0, 2.0, 0.24669999168808, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 689444.6683320049, 689444.6683320049, 240405.8421069874], 
processed observation next is [1.0, 0.8695652173913043, 0.514218009478673, 0.7633333333333334, 1.0, 1.0, 0.0924096285398554, 1.0, 1.0, 0.0924096285398554, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19151240787000137, 0.19151240787000137, 0.3588146897119215], 
reward next is 0.6412, 
noisyNet noise sample is [array([1.4051074], dtype=float32), 1.3985056]. 
=============================================
[2019-04-28 02:41:24,373] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[61.045914]
 [61.045147]
 [61.0866  ]
 [61.19272 ]
 [61.208466]], R is [[60.98781204]
 [61.01930618]
 [61.05073166]
 [61.08209991]
 [61.11344147]].
[2019-04-28 02:41:25,571] A3C_AGENT_WORKER-Thread-18 INFO:Local step 143500, global step 2289877: loss 0.4625
[2019-04-28 02:41:25,574] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 143500, global step 2289878: learning rate 0.0000
[2019-04-28 02:41:28,049] A3C_AGENT_WORKER-Thread-20 INFO:Local step 143500, global step 2290876: loss 0.2196
[2019-04-28 02:41:28,050] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 143500, global step 2290876: learning rate 0.0000
[2019-04-28 02:41:28,713] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143000, global step 2291143: loss 0.2159
[2019-04-28 02:41:28,715] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143000, global step 2291145: learning rate 0.0000
[2019-04-28 02:41:29,749] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143000, global step 2291572: loss 0.0200
[2019-04-28 02:41:29,750] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143000, global step 2291572: learning rate 0.0000
[2019-04-28 02:41:30,623] A3C_AGENT_WORKER-Thread-2 INFO:Local step 143500, global step 2291909: loss 0.0019
[2019-04-28 02:41:30,626] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 143500, global step 2291910: learning rate 0.0000
[2019-04-28 02:41:30,757] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.0264736e-13 2.5303965e-05 1.8713447e-22 9.9997473e-01 2.3709738e-17], sum to 1.0000
[2019-04-28 02:41:30,761] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.3646
[2019-04-28 02:41:30,781] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.66666666666666, 83.66666666666667, 1.0, 2.0, 0.266994607704776, 1.0, 2.0, 0.266994607704776, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 746181.1184413127, 746181.1184413127, 243818.7582814854], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6295200.0000, 
sim time next is 6295800.0000, 
raw observation next is [27.63333333333334, 83.83333333333333, 1.0, 2.0, 0.2672311666853239, 1.0, 2.0, 0.2672311666853239, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 746842.4699992153, 746842.4699992153, 243859.8233798229], 
processed observation next is [0.0, 0.8695652173913043, 0.5086887835703005, 0.8383333333333333, 1.0, 1.0, 0.11714598395822153, 1.0, 1.0, 0.11714598395822153, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20745624166644872, 0.20745624166644872, 0.3639698856415267], 
reward next is 0.6360, 
noisyNet noise sample is [array([-2.1779213], dtype=float32), 1.2056444]. 
=============================================
[2019-04-28 02:41:31,741] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143000, global step 2292372: loss 0.0287
[2019-04-28 02:41:31,742] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143000, global step 2292372: learning rate 0.0000
[2019-04-28 02:41:32,513] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143000, global step 2292705: loss 0.0289
[2019-04-28 02:41:32,515] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143000, global step 2292705: learning rate 0.0000
[2019-04-28 02:41:32,945] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143000, global step 2292887: loss 0.0209
[2019-04-28 02:41:32,949] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143000, global step 2292888: learning rate 0.0000
[2019-04-28 02:41:35,094] A3C_AGENT_WORKER-Thread-11 INFO:Local step 143500, global step 2293740: loss 0.0282
[2019-04-28 02:41:35,095] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 143500, global step 2293741: learning rate 0.0000
[2019-04-28 02:41:35,425] A3C_AGENT_WORKER-Thread-14 INFO:Local step 143500, global step 2293884: loss 1.2396
[2019-04-28 02:41:35,430] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 143500, global step 2293884: learning rate 0.0000
[2019-04-28 02:41:35,571] A3C_AGENT_WORKER-Thread-9 INFO:Local step 143500, global step 2293935: loss 2.4730
[2019-04-28 02:41:35,575] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 143500, global step 2293938: learning rate 0.0000
[2019-04-28 02:41:37,331] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.2401294e-12 6.6228749e-05 2.2062999e-20 9.9993372e-01 2.1716855e-15], sum to 1.0000
[2019-04-28 02:41:37,342] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0631
[2019-04-28 02:41:37,351] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.1, 72.5, 1.0, 2.0, 0.241791031560604, 1.0, 2.0, 0.241791031560604, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 675721.4314183367, 675721.4314183367, 239616.7084541579], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6551400.0000, 
sim time next is 6552000.0000, 
raw observation next is [28.0, 73.0, 1.0, 2.0, 0.2416508105621007, 1.0, 2.0, 0.2416508105621007, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 675329.4393728541, 675329.4393728541, 239594.328397282], 
processed observation next is [1.0, 0.8695652173913043, 0.5260663507109005, 0.73, 1.0, 1.0, 0.08632627778566347, 1.0, 1.0, 0.08632627778566347, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1875915109369039, 0.1875915109369039, 0.3576034752198239], 
reward next is 0.6424, 
noisyNet noise sample is [array([-1.1818339], dtype=float32), -0.21455769]. 
=============================================
[2019-04-28 02:41:37,370] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[55.771305]
 [55.80701 ]
 [55.886353]
 [55.992382]
 [56.165546]], R is [[55.74997711]
 [55.83484268]
 [55.9187088 ]
 [56.00143814]
 [56.08305359]].
[2019-04-28 02:41:38,054] A3C_AGENT_WORKER-Thread-10 INFO:Local step 143500, global step 2294972: loss 0.0944
[2019-04-28 02:41:38,059] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 143500, global step 2294972: learning rate 0.0000
[2019-04-28 02:41:39,197] A3C_AGENT_WORKER-Thread-12 INFO:Local step 143500, global step 2295465: loss 0.2988
[2019-04-28 02:41:39,199] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 143500, global step 2295465: learning rate 0.0000
[2019-04-28 02:41:41,315] A3C_AGENT_WORKER-Thread-13 INFO:Local step 143500, global step 2296353: loss 0.0363
[2019-04-28 02:41:41,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 143500, global step 2296353: learning rate 0.0000
[2019-04-28 02:41:42,746] A3C_AGENT_WORKER-Thread-21 INFO:Local step 143500, global step 2296947: loss 0.2791
[2019-04-28 02:41:42,748] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 143500, global step 2296947: learning rate 0.0000
[2019-04-28 02:41:43,063] A3C_AGENT_WORKER-Thread-16 INFO:Local step 143500, global step 2297084: loss 0.1332
[2019-04-28 02:41:43,068] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 143500, global step 2297084: learning rate 0.0000
[2019-04-28 02:41:44,692] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144000, global step 2297777: loss 0.1717
[2019-04-28 02:41:44,701] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144000, global step 2297777: learning rate 0.0000
[2019-04-28 02:41:47,180] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144000, global step 2298793: loss 0.1924
[2019-04-28 02:41:47,182] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144000, global step 2298795: learning rate 0.0000
[2019-04-28 02:41:48,165] A3C_AGENT_WORKER-Thread-19 INFO:Local step 143500, global step 2299195: loss 0.0385
[2019-04-28 02:41:48,176] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 143500, global step 2299196: learning rate 0.0000
[2019-04-28 02:41:48,956] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3125033e-11 3.2451662e-04 9.5587348e-19 9.9967551e-01 1.2239092e-13], sum to 1.0000
[2019-04-28 02:41:48,973] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.8559
[2019-04-28 02:41:48,988] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.73333333333333, 68.33333333333334, 1.0, 2.0, 0.200162529282492, 1.0, 2.0, 0.200162529282492, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 591362.4466847162, 591362.4466847162, 238865.7179394322], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6727200.0000, 
sim time next is 6727800.0000, 
raw observation next is [26.61666666666667, 68.66666666666666, 1.0, 2.0, 0.1991250949733658, 1.0, 2.0, 0.1991250949733658, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 589459.4663624963, 589459.4663624956, 238902.441233534], 
processed observation next is [1.0, 0.8695652173913043, 0.4605055292259086, 0.6866666666666665, 1.0, 1.0, 0.035090475871525056, 1.0, 1.0, 0.035090475871525056, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.16373874065624897, 0.16373874065624877, 0.3565708078112448], 
reward next is 0.6434, 
noisyNet noise sample is [array([-0.02286706], dtype=float32), -0.6682172]. 
=============================================
[2019-04-28 02:41:49,333] A3C_AGENT_WORKER-Thread-22 INFO:Local step 143500, global step 2299693: loss 0.0133
[2019-04-28 02:41:49,335] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 143500, global step 2299693: learning rate 0.0000
[2019-04-28 02:41:49,817] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144000, global step 2299893: loss 0.5667
[2019-04-28 02:41:49,818] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144000, global step 2299893: learning rate 0.0000
[2019-04-28 02:41:50,086] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-28 02:41:50,087] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:41:50,091] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:41:50,092] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:41:50,093] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:41:50,093] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:41:50,094] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:41:50,095] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:41:50,096] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:41:50,096] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:41:50,097] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:41:50,117] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run93
[2019-04-28 02:41:50,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run93
[2019-04-28 02:41:50,180] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run93
[2019-04-28 02:41:50,182] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run93
[2019-04-28 02:41:50,259] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run93
[2019-04-28 02:42:11,959] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:42:11,961] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.86666666666667, 94.0, 1.0, 2.0, 0.2420707117128533, 1.0, 2.0, 0.2420707117128533, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 676503.2861770364, 676503.2861770364, 239660.9347727337]
[2019-04-28 02:42:11,962] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:42:11,963] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.7676229e-13 2.3590563e-05 2.2257007e-21 9.9997640e-01 6.2268552e-16], sampled 0.4442502649662067
[2019-04-28 02:42:26,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:42:26,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.7, 76.0, 1.0, 2.0, 0.6057975847635987, 1.0, 2.0, 0.6057975847635987, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1693808.110288155, 1693808.110288155, 336119.8378761395]
[2019-04-28 02:42:26,002] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:42:26,004] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [5.2894023e-13 3.8669805e-05 1.2764616e-20 9.9996138e-01 2.2450477e-15], sampled 0.34289109980391275
[2019-04-28 02:42:54,392] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:42:54,394] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [31.0, 70.0, 1.0, 2.0, 0.8767756502306645, 1.0, 2.0, 0.8767756502306645, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2452272.543864268, 2452272.543864268, 458980.1345968838]
[2019-04-28 02:42:54,395] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:42:54,398] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.7296767e-12 6.4104614e-05 1.9196229e-19 9.9993587e-01 1.6724237e-14], sampled 0.20408453904325163
[2019-04-28 02:42:56,507] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:42:56,508] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [37.45, 54.0, 1.0, 2.0, 0.9977793311215304, 1.0, 2.0, 0.9977793311215304, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2791088.350560504, 2791088.350560504, 527660.9920588472]
[2019-04-28 02:42:56,508] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:42:56,509] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [4.6736621e-12 9.6598189e-05 4.2145144e-19 9.9990344e-01 2.9249342e-14], sampled 0.7275806568842073
[2019-04-28 02:43:07,063] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:43:07,065] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [32.408045, 77.92838189, 1.0, 2.0, 0.3252198638260679, 1.0, 2.0, 0.3252198638260679, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 908969.5243549754, 908969.5243549754, 255444.3001923861]
[2019-04-28 02:43:07,066] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:43:07,068] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [7.0312056e-14 1.6893729e-05 4.9572711e-22 9.9998307e-01 2.0535693e-16], sampled 0.34971280993276754
[2019-04-28 02:43:13,361] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:43:13,362] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.83055825, 77.192753075, 1.0, 2.0, 0.2106301426850927, 1.0, 2.0, 0.2106301426850927, 0.0, 2.0, 0.0, 6.9112, 6.9112, 172.86236624, 613491.638034651, 613491.638034651, 239507.7463688767]
[2019-04-28 02:43:13,364] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:43:13,368] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.6660962e-13 2.8007686e-05 4.3045243e-21 9.9997199e-01 1.0108489e-15], sampled 0.8767163228491491
[2019-04-28 02:43:16,481] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:43:16,483] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [28.7, 81.5, 1.0, 2.0, 0.54876316246259, 1.0, 2.0, 0.54876316246259, 0.0, 2.0, 0.0, 6.9112, 6.9112, 169.0403247858759, 1534224.876360442, 1534224.876360442, 315814.4853143144]
[2019-04-28 02:43:16,486] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:43:16,491] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [4.1563129e-13 3.2373664e-05 8.9400663e-21 9.9996758e-01 1.7339295e-15], sampled 0.9139877988250718
[2019-04-28 02:43:24,815] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.12505652]
[2019-04-28 02:43:24,817] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.41666666666666, 68.33333333333334, 1.0, 2.0, 0.7841102901285427, 1.0, 2.0, 0.7841102901285427, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 2192864.535806442, 2192864.535806441, 412206.9694389836]
[2019-04-28 02:43:24,817] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:43:24,821] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.3875740e-12 5.0920997e-05 6.3361751e-20 9.9994910e-01 7.3682176e-15], sampled 0.07065416130291358
[2019-04-28 02:43:25,795] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 6701.0599 3429708688.0380 33.0000
[2019-04-28 02:43:26,182] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 6503.2956 3684592754.8713 228.0000
[2019-04-28 02:43:26,372] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 6794.7133 3393663420.6697 9.0000
[2019-04-28 02:43:26,420] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 6856.3269 3475024263.0896 9.0000
[2019-04-28 02:43:26,530] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 6798.7741 3511243238.1342 0.0000
[2019-04-28 02:43:27,546] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2300000, evaluation results [2300000.0, 6503.295605200641, 3684592754.8713183, 228.0, 6856.326911409196, 3475024263.089649, 9.0, 6794.7133353702575, 3393663420.6696663, 9.0, 6798.774095523391, 3511243238.1342273, 0.0, 6701.059852294064, 3429708688.0380054, 33.0]
[2019-04-28 02:43:28,691] A3C_AGENT_WORKER-Thread-15 INFO:Local step 143500, global step 2300504: loss 1.3284
[2019-04-28 02:43:28,694] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 143500, global step 2300504: learning rate 0.0000
[2019-04-28 02:43:28,795] A3C_AGENT_WORKER-Thread-3 INFO:Local step 143500, global step 2300547: loss 1.6388
[2019-04-28 02:43:28,796] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 143500, global step 2300547: learning rate 0.0000
[2019-04-28 02:43:29,770] A3C_AGENT_WORKER-Thread-17 INFO:Local step 143500, global step 2300967: loss 0.1337
[2019-04-28 02:43:29,777] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 143500, global step 2300968: learning rate 0.0000
[2019-04-28 02:43:31,302] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144000, global step 2301637: loss 0.2846
[2019-04-28 02:43:31,304] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144000, global step 2301637: learning rate 0.0000
[2019-04-28 02:43:31,868] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144000, global step 2301883: loss 0.0635
[2019-04-28 02:43:31,871] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144000, global step 2301884: learning rate 0.0000
[2019-04-28 02:43:31,937] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144000, global step 2301911: loss 0.0377
[2019-04-28 02:43:31,940] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144000, global step 2301911: learning rate 0.0000
[2019-04-28 02:43:33,563] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144000, global step 2302620: loss 0.0149
[2019-04-28 02:43:33,565] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144000, global step 2302620: learning rate 0.0000
[2019-04-28 02:43:35,618] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144000, global step 2303538: loss 0.2000
[2019-04-28 02:43:35,619] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144000, global step 2303538: learning rate 0.0000
[2019-04-28 02:43:37,999] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144000, global step 2304592: loss 0.8282
[2019-04-28 02:43:38,003] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144000, global step 2304592: learning rate 0.0000
[2019-04-28 02:43:39,086] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144000, global step 2305076: loss 0.5248
[2019-04-28 02:43:39,091] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144000, global step 2305077: learning rate 0.0000
[2019-04-28 02:43:39,212] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144000, global step 2305132: loss 0.6497
[2019-04-28 02:43:39,218] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144000, global step 2305132: learning rate 0.0000
[2019-04-28 02:43:41,064] A3C_AGENT_WORKER-Thread-18 INFO:Local step 144500, global step 2305938: loss -27.0773
[2019-04-28 02:43:41,072] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 144500, global step 2305939: learning rate 0.0000
[2019-04-28 02:43:42,895] A3C_AGENT_WORKER-Thread-20 INFO:Local step 144500, global step 2306778: loss 32.3153
[2019-04-28 02:43:42,900] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 144500, global step 2306779: learning rate 0.0000
[2019-04-28 02:43:43,449] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144000, global step 2307040: loss 0.9112
[2019-04-28 02:43:43,451] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144000, global step 2307040: learning rate 0.0000
[2019-04-28 02:43:44,519] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144000, global step 2307449: loss 0.8108
[2019-04-28 02:43:44,520] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144000, global step 2307449: learning rate 0.0000
[2019-04-28 02:43:45,400] A3C_AGENT_WORKER-Thread-2 INFO:Local step 144500, global step 2307842: loss -14.0553
[2019-04-28 02:43:45,401] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 144500, global step 2307842: learning rate 0.0000
[2019-04-28 02:43:47,071] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144000, global step 2308619: loss 0.7904
[2019-04-28 02:43:47,075] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144000, global step 2308621: learning rate 0.0000
[2019-04-28 02:43:47,263] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144000, global step 2308715: loss 0.5426
[2019-04-28 02:43:47,265] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144000, global step 2308716: learning rate 0.0000
[2019-04-28 02:43:47,777] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144000, global step 2308966: loss 0.2542
[2019-04-28 02:43:47,781] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144000, global step 2308968: learning rate 0.0000
[2019-04-28 02:43:49,189] A3C_AGENT_WORKER-Thread-11 INFO:Local step 144500, global step 2309659: loss 2.8182
[2019-04-28 02:43:49,191] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 144500, global step 2309660: learning rate 0.0000
[2019-04-28 02:43:49,783] A3C_AGENT_WORKER-Thread-9 INFO:Local step 144500, global step 2309948: loss -40.4780
[2019-04-28 02:43:49,785] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 144500, global step 2309948: learning rate 0.0000
[2019-04-28 02:43:49,791] A3C_AGENT_WORKER-Thread-14 INFO:Local step 144500, global step 2309952: loss -29.7281
[2019-04-28 02:43:49,794] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 144500, global step 2309952: learning rate 0.0000
[2019-04-28 02:43:51,284] A3C_AGENT_WORKER-Thread-10 INFO:Local step 144500, global step 2310689: loss -0.1236
[2019-04-28 02:43:51,288] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 144500, global step 2310689: learning rate 0.0000
[2019-04-28 02:43:53,093] A3C_AGENT_WORKER-Thread-12 INFO:Local step 144500, global step 2311583: loss -95.8641
[2019-04-28 02:43:53,094] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 144500, global step 2311583: learning rate 0.0000
[2019-04-28 02:43:55,201] A3C_AGENT_WORKER-Thread-13 INFO:Local step 144500, global step 2312623: loss -54.8670
[2019-04-28 02:43:55,205] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 144500, global step 2312623: learning rate 0.0000
[2019-04-28 02:43:56,434] A3C_AGENT_WORKER-Thread-21 INFO:Local step 144500, global step 2313245: loss -136.1181
[2019-04-28 02:43:56,435] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 144500, global step 2313245: learning rate 0.0000
[2019-04-28 02:43:56,452] A3C_AGENT_WORKER-Thread-16 INFO:Local step 144500, global step 2313255: loss -86.8515
[2019-04-28 02:43:56,453] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 144500, global step 2313255: learning rate 0.0000
[2019-04-28 02:43:57,469] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145000, global step 2313773: loss 0.0111
[2019-04-28 02:43:57,472] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145000, global step 2313775: learning rate 0.0000
[2019-04-28 02:43:59,030] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145000, global step 2314594: loss 0.0020
[2019-04-28 02:43:59,034] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145000, global step 2314595: learning rate 0.0000
[2019-04-28 02:44:00,132] A3C_AGENT_WORKER-Thread-19 INFO:Local step 144500, global step 2315166: loss -71.2847
[2019-04-28 02:44:00,133] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 144500, global step 2315166: learning rate 0.0000
[2019-04-28 02:44:00,749] A3C_AGENT_WORKER-Thread-22 INFO:Local step 144500, global step 2315496: loss -44.5370
[2019-04-28 02:44:00,755] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 144500, global step 2315496: learning rate 0.0000
[2019-04-28 02:44:00,975] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145000, global step 2315615: loss 0.0052
[2019-04-28 02:44:00,978] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145000, global step 2315615: learning rate 0.0000
[2019-04-28 02:44:03,097] A3C_AGENT_WORKER-Thread-15 INFO:Local step 144500, global step 2316726: loss -90.4573
[2019-04-28 02:44:03,099] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 144500, global step 2316726: learning rate 0.0000
[2019-04-28 02:44:03,138] A3C_AGENT_WORKER-Thread-3 INFO:Local step 144500, global step 2316743: loss -31.6201
[2019-04-28 02:44:03,141] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 144500, global step 2316745: learning rate 0.0000
[2019-04-28 02:44:03,943] A3C_AGENT_WORKER-Thread-17 INFO:Local step 144500, global step 2317129: loss -17.8927
[2019-04-28 02:44:03,944] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 144500, global step 2317129: learning rate 0.0000
[2019-04-28 02:44:04,767] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145000, global step 2317507: loss 0.0273
[2019-04-28 02:44:04,770] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145000, global step 2317508: learning rate 0.0000
[2019-04-28 02:44:05,515] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145000, global step 2317868: loss 0.0003
[2019-04-28 02:44:05,519] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145000, global step 2317868: learning rate 0.0000
[2019-04-28 02:44:05,551] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145000, global step 2317878: loss 0.0002
[2019-04-28 02:44:05,552] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145000, global step 2317878: learning rate 0.0000
[2019-04-28 02:44:06,465] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.5606368e-16 9.9999201e-01 4.2263156e-28 7.9442098e-06 1.0043165e-21], sum to 1.0000
[2019-04-28 02:44:06,472] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.9326
[2019-04-28 02:44:06,477] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333333, 92.16666666666667, 1.0, 2.0, 0.3961341365651395, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 588624.7158582052, 588624.7158582046, 173415.9105741535], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7527000.0000, 
sim time next is 7527600.0000, 
raw observation next is [23.3, 92.0, 1.0, 2.0, 0.3942261005116172, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 586920.5135751214, 586920.513575122, 173294.9515845607], 
processed observation next is [0.0, 0.13043478260869565, 0.3033175355450238, 0.92, 1.0, 1.0, 0.27015192832724966, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1630334759930893, 0.16303347599308946, 0.2586491814694936], 
reward next is 0.7414, 
noisyNet noise sample is [array([0.5773109], dtype=float32), -0.59780884]. 
=============================================
[2019-04-28 02:44:07,073] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145000, global step 2318611: loss 0.0010
[2019-04-28 02:44:07,079] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145000, global step 2318612: learning rate 0.0000
[2019-04-28 02:44:07,371] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [5.8720088e-16 9.9991071e-01 2.6970484e-25 8.9294568e-05 8.1244218e-21], sum to 1.0000
[2019-04-28 02:44:07,377] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.3408
[2019-04-28 02:44:07,381] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.63333333333333, 91.33333333333334, 1.0, 2.0, 0.5852324403612141, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831151.5802689774, 831151.5802689774, 199743.1143115084], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7630800.0000, 
sim time next is 7631400.0000, 
raw observation next is [24.71666666666667, 91.16666666666667, 1.0, 2.0, 0.6029276688474788, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 854008.1614414913, 854008.1614414913, 202772.713782027], 
processed observation next is [1.0, 0.30434782608695654, 0.3704581358609796, 0.9116666666666667, 1.0, 1.0, 0.5215996010210588, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23722448928930315, 0.23722448928930315, 0.30264584146571194], 
reward next is 0.6974, 
noisyNet noise sample is [array([-0.2557726], dtype=float32), 1.3686824]. 
=============================================
[2019-04-28 02:44:07,596] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [4.3507670e-16 9.9996233e-01 4.7178117e-25 3.7728048e-05 5.2697449e-19], sum to 1.0000
[2019-04-28 02:44:07,600] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7023
[2019-04-28 02:44:07,607] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 77.0, 1.0, 2.0, 0.4157413201553971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 606491.3773013739, 606491.3773013739, 174739.69708899], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7486200.0000, 
sim time next is 7486800.0000, 
raw observation next is [26.06666666666667, 76.66666666666667, 1.0, 2.0, 0.4157555384071969, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 606176.3761989558, 606176.3761989564, 174699.6892515032], 
processed observation next is [0.0, 0.6521739130434783, 0.4344391785150081, 0.7666666666666667, 1.0, 1.0, 0.2960910101291529, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16838232672193218, 0.16838232672193232, 0.26074580485298987], 
reward next is 0.7393, 
noisyNet noise sample is [array([-0.07285554], dtype=float32), -0.2723108]. 
=============================================
[2019-04-28 02:44:08,746] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1335965e-13 9.9997854e-01 4.7631536e-24 2.1498823e-05 3.4434434e-20], sum to 1.0000
[2019-04-28 02:44:08,751] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1779
[2019-04-28 02:44:08,759] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666667, 82.0, 1.0, 2.0, 0.3771318744214461, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 571243.9689907632, 571243.9689907638, 172194.9392325905], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7357800.0000, 
sim time next is 7358400.0000, 
raw observation next is [24.1, 83.0, 1.0, 2.0, 0.3776804476273991, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 570742.1987594216, 570742.1987594216, 172110.4719952524], 
processed observation next is [1.0, 0.17391304347826086, 0.3412322274881518, 0.83, 1.0, 1.0, 0.25021740677999893, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15853949965539488, 0.15853949965539488, 0.25688130148545135], 
reward next is 0.7431, 
noisyNet noise sample is [array([0.59553486], dtype=float32), -0.49507228]. 
=============================================
[2019-04-28 02:44:08,974] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145000, global step 2319543: loss 0.0003
[2019-04-28 02:44:08,979] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145000, global step 2319544: learning rate 0.0000
[2019-04-28 02:44:09,433] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.7800369e-19 9.9999988e-01 6.0117128e-28 1.6363093e-07 7.4268074e-23], sum to 1.0000
[2019-04-28 02:44:09,439] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2682
[2019-04-28 02:44:09,445] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.8, 67.5, 1.0, 2.0, 0.3844436291327872, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 575146.5214300234, 575146.5214300241, 172319.7016298247], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7323000.0000, 
sim time next is 7323600.0000, 
raw observation next is [26.7, 68.0, 1.0, 2.0, 0.390849191565308, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 584974.975979596, 584974.9759795954, 173212.8141027009], 
processed observation next is [1.0, 0.782608695652174, 0.46445497630331756, 0.68, 1.0, 1.0, 0.2660833633316964, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1624930488832211, 0.16249304888322097, 0.2585265882129864], 
reward next is 0.7415, 
noisyNet noise sample is [array([0.39547995], dtype=float32), -1.3101499]. 
=============================================
[2019-04-28 02:44:10,921] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145000, global step 2320469: loss 0.0010
[2019-04-28 02:44:10,923] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145000, global step 2320469: learning rate 0.0000
[2019-04-28 02:44:12,427] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145000, global step 2321205: loss 0.0206
[2019-04-28 02:44:12,428] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145000, global step 2321205: learning rate 0.0000
[2019-04-28 02:44:12,652] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145000, global step 2321314: loss 0.0005
[2019-04-28 02:44:12,654] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145000, global step 2321316: learning rate 0.0000
[2019-04-28 02:44:13,689] A3C_AGENT_WORKER-Thread-18 INFO:Local step 145500, global step 2321813: loss 4.7428
[2019-04-28 02:44:13,697] A3C_AGENT_WORKER-Thread-18 DEBUG:Local step 145500, global step 2321815: learning rate 0.0000
[2019-04-28 02:44:15,650] A3C_AGENT_WORKER-Thread-20 INFO:Local step 145500, global step 2322772: loss -7.7982
[2019-04-28 02:44:15,650] A3C_AGENT_WORKER-Thread-20 DEBUG:Local step 145500, global step 2322772: learning rate 0.0000
[2019-04-28 02:44:15,711] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.8149876e-14 9.9957234e-01 3.1337137e-23 4.2769281e-04 4.8151473e-20], sum to 1.0000
[2019-04-28 02:44:15,717] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8878
[2019-04-28 02:44:15,723] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 60.5, 1.0, 2.0, 0.4363663616191771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 624022.2027819117, 624022.2027819117, 176073.0586350189], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7569000.0000, 
sim time next is 7569600.0000, 
raw observation next is [29.66666666666667, 61.0, 1.0, 2.0, 0.4466220104644879, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 632240.670983596, 632240.6709835954, 176714.2738547961], 
processed observation next is [0.0, 0.6086956521739131, 0.6050552922590839, 0.61, 1.0, 1.0, 0.3332795306801059, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17562240860655445, 0.1756224086065543, 0.2637526475444718], 
reward next is 0.7362, 
noisyNet noise sample is [array([-0.44481584], dtype=float32), 0.48431417]. 
=============================================
[2019-04-28 02:44:16,087] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145000, global step 2322985: loss 0.0042
[2019-04-28 02:44:16,092] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145000, global step 2322985: learning rate 0.0000
[2019-04-28 02:44:17,329] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145000, global step 2323578: loss -2.1385
[2019-04-28 02:44:17,336] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145000, global step 2323578: learning rate 0.0000
[2019-04-28 02:44:17,460] A3C_AGENT_WORKER-Thread-2 INFO:Local step 145500, global step 2323643: loss -20.3484
[2019-04-28 02:44:17,463] A3C_AGENT_WORKER-Thread-2 DEBUG:Local step 145500, global step 2323644: learning rate 0.0000
[2019-04-28 02:44:19,499] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145000, global step 2324632: loss 0.0365
[2019-04-28 02:44:19,505] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145000, global step 2324633: learning rate 0.0000
[2019-04-28 02:44:19,659] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145000, global step 2324707: loss 0.0002
[2019-04-28 02:44:19,663] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145000, global step 2324708: learning rate 0.0000
[2019-04-28 02:44:20,301] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145000, global step 2324965: loss 0.0015
[2019-04-28 02:44:20,303] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145000, global step 2324965: learning rate 0.0000
[2019-04-28 02:44:20,401] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-28 02:44:20,401] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:44:20,402] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:44:20,403] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:44:20,403] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:44:20,404] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:44:20,404] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:44:20,405] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:44:20,404] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:44:20,404] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:44:20,412] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:44:20,427] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run94
[2019-04-28 02:44:20,427] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run94
[2019-04-28 02:44:20,483] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run94
[2019-04-28 02:44:20,519] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run94
[2019-04-28 02:44:20,547] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run94
[2019-04-28 02:44:26,175] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:44:26,177] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [21.7, 90.5, 1.0, 2.0, 0.3222229192268474, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 506377.0591154601, 506377.0591154601, 167438.5517887819]
[2019-04-28 02:44:26,179] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:44:26,181] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.8948113e-14 9.8148030e-01 2.4305755e-24 1.8519765e-02 1.2642649e-17], sampled 0.47309608800605274
[2019-04-28 02:44:26,385] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:44:26,386] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [19.83333333333333, 82.0, 1.0, 2.0, 0.2286532391269225, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 377902.9934132327, 377902.9934132327, 158809.3642383329]
[2019-04-28 02:44:26,387] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:44:26,389] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.83841674e-14 9.81637299e-01 2.31066076e-24 1.83627605e-02
 1.21709825e-17], sampled 0.2934579211284234
[2019-04-28 02:45:12,516] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:45:12,517] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.16666666666666, 69.33333333333334, 1.0, 2.0, 0.7555433881963625, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.986518466856122, 6.9112, 168.9124472240604, 1952856.501474254, 1899423.14993136, 397873.050354858]
[2019-04-28 02:45:12,521] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:45:12,523] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.2239536e-10 8.5226750e-01 6.8704484e-18 1.4773257e-01 7.6497202e-13], sampled 0.6814371071475279
[2019-04-28 02:45:12,523] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1952856.501474254 W.
[2019-04-28 02:45:23,841] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:45:23,841] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.03333333333333, 72.66666666666667, 1.0, 2.0, 0.7036532342804697, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.994651533557244, 6.9112, 168.9123910330958, 1880240.975846684, 1821037.783958846, 385874.9265584563]
[2019-04-28 02:45:23,843] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:45:23,846] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [2.4476783e-13 9.8627555e-01 2.6434517e-22 1.3724443e-02 2.8781306e-16], sampled 0.8194794614531975
[2019-04-28 02:45:23,847] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1880240.975846684 W.
[2019-04-28 02:45:37,227] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:45:37,228] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.791659075, 80.94032079666667, 1.0, 2.0, 0.5376038402482325, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 850192.1349129323, 850192.1349129323, 201037.3679582981]
[2019-04-28 02:45:37,229] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:45:37,232] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [3.2982287e-14 9.8543859e-01 7.1869946e-24 1.4561476e-02 2.4535016e-17], sampled 0.4970553940202712
[2019-04-28 02:45:42,016] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1290197]
[2019-04-28 02:45:42,018] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [24.26106240833333, 96.11548840833333, 1.0, 2.0, 0.5009722119721582, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 700031.2787997511, 700031.2787997518, 183746.6394942146]
[2019-04-28 02:45:42,019] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:45:42,022] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.2121293e-14 9.9113888e-01 1.4717693e-24 8.8610714e-03 6.8508393e-18], sampled 0.22391607449745343
[2019-04-28 02:45:44,726] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8331.1011 2851760011.1333 1078.0000
[2019-04-28 02:45:44,801] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7743.9892 3170452285.7550 1697.0000
[2019-04-28 02:45:44,835] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8470.7246 2788590693.1321 862.0000
[2019-04-28 02:45:44,852] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7849.8507 3013630722.6457 1669.0000
[2019-04-28 02:45:45,077] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8095.3123 2935266487.7069 1244.0000
[2019-04-28 02:45:46,093] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2325000, evaluation results [2325000.0, 7743.989155902607, 3170452285.754976, 1697.0, 8095.31230789854, 2935266487.7068796, 1244.0, 8470.724610908193, 2788590693.132075, 862.0, 7849.850654784798, 3013630722.6456594, 1669.0, 8331.101086116156, 2851760011.1332583, 1078.0]
[2019-04-28 02:45:46,448] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [3.0304240e-14 9.9550724e-01 3.8756709e-24 4.4927844e-03 8.2737738e-19], sum to 1.0000
[2019-04-28 02:45:46,455] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.2422
[2019-04-28 02:45:46,458] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.5, 62.0, 1.0, 2.0, 0.457885054305628, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 643735.1257590854, 643735.1257590854, 177769.4383347258], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7572600.0000, 
sim time next is 7573200.0000, 
raw observation next is [29.33333333333334, 62.0, 1.0, 2.0, 0.4513166778290517, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 638710.9203464325, 638710.9203464318, 177365.3130836729], 
processed observation next is [0.0, 0.6521739130434783, 0.5892575039494474, 0.62, 1.0, 1.0, 0.3389357564205442, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17741970009623126, 0.17741970009623106, 0.26472434788607896], 
reward next is 0.7353, 
noisyNet noise sample is [array([0.11445907], dtype=float32), -0.05507382]. 
=============================================
[2019-04-28 02:45:47,109] A3C_AGENT_WORKER-Thread-11 INFO:Local step 145500, global step 2325508: loss -9.4426
[2019-04-28 02:45:47,111] A3C_AGENT_WORKER-Thread-11 DEBUG:Local step 145500, global step 2325508: learning rate 0.0000
[2019-04-28 02:45:47,998] A3C_AGENT_WORKER-Thread-14 INFO:Local step 145500, global step 2325948: loss -11.9746
[2019-04-28 02:45:48,001] A3C_AGENT_WORKER-Thread-14 DEBUG:Local step 145500, global step 2325949: learning rate 0.0000
[2019-04-28 02:45:48,080] A3C_AGENT_WORKER-Thread-9 INFO:Local step 145500, global step 2325993: loss 1.8868
[2019-04-28 02:45:48,085] A3C_AGENT_WORKER-Thread-9 DEBUG:Local step 145500, global step 2325995: learning rate 0.0000
[2019-04-28 02:45:48,246] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:48,247] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:48,296] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-18_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res13/Eplus-env-sub_run12
[2019-04-28 02:45:49,290] A3C_AGENT_WORKER-Thread-10 INFO:Local step 145500, global step 2326538: loss -8.2829
[2019-04-28 02:45:49,291] A3C_AGENT_WORKER-Thread-10 DEBUG:Local step 145500, global step 2326538: learning rate 0.0000
[2019-04-28 02:45:50,012] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:50,013] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:50,033] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-20_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res15/Eplus-env-sub_run12
[2019-04-28 02:45:50,891] A3C_AGENT_WORKER-Thread-12 INFO:Local step 145500, global step 2327316: loss -16.8390
[2019-04-28 02:45:50,896] A3C_AGENT_WORKER-Thread-12 DEBUG:Local step 145500, global step 2327316: learning rate 0.0000
[2019-04-28 02:45:51,772] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:51,773] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:51,826] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-2_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res2/Eplus-env-sub_run12
[2019-04-28 02:45:52,691] A3C_AGENT_WORKER-Thread-13 INFO:Local step 145500, global step 2328182: loss -10.6714
[2019-04-28 02:45:52,693] A3C_AGENT_WORKER-Thread-13 DEBUG:Local step 145500, global step 2328182: learning rate 0.0000
[2019-04-28 02:45:54,295] A3C_AGENT_WORKER-Thread-16 INFO:Local step 145500, global step 2328981: loss 0.1980
[2019-04-28 02:45:54,298] A3C_AGENT_WORKER-Thread-16 DEBUG:Local step 145500, global step 2328981: learning rate 0.0000
[2019-04-28 02:45:54,368] A3C_AGENT_WORKER-Thread-21 INFO:Local step 145500, global step 2329020: loss 0.4632
[2019-04-28 02:45:54,374] A3C_AGENT_WORKER-Thread-21 DEBUG:Local step 145500, global step 2329022: learning rate 0.0000
[2019-04-28 02:45:55,145] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:55,146] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:55,181] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-11_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res6/Eplus-env-sub_run12
[2019-04-28 02:45:56,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:56,141] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:56,176] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-9_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res4/Eplus-env-sub_run12
[2019-04-28 02:45:56,349] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:56,350] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:56,362] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-14_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res9/Eplus-env-sub_run12
[2019-04-28 02:45:57,417] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:57,418] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:57,466] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-10_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res5/Eplus-env-sub_run12
[2019-04-28 02:45:57,638] A3C_AGENT_WORKER-Thread-19 INFO:Local step 145500, global step 2330473: loss -148.9536
[2019-04-28 02:45:57,639] A3C_AGENT_WORKER-Thread-19 DEBUG:Local step 145500, global step 2330473: learning rate 0.0000
[2019-04-28 02:45:58,574] A3C_AGENT_WORKER-Thread-22 INFO:Local step 145500, global step 2330955: loss -92.9691
[2019-04-28 02:45:58,579] A3C_AGENT_WORKER-Thread-22 DEBUG:Local step 145500, global step 2330957: learning rate 0.0000
[2019-04-28 02:45:59,085] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:45:59,086] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:45:59,120] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-12_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res7/Eplus-env-sub_run12
[2019-04-28 02:46:00,482] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4166041e-19 1.0000000e+00 7.3245167e-28 6.4575674e-09 1.5735145e-23], sum to 1.0000
[2019-04-28 02:46:00,489] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1358
[2019-04-28 02:46:00,498] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.5, 96.0, 1.0, 2.0, 0.3826110538410947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 575872.5079116046, 575872.5079116046, 172493.9529415882], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 151200.0000, 
sim time next is 151800.0000, 
raw observation next is [22.48333333333333, 96.0, 1.0, 2.0, 0.385925887861531, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581081.8251097152, 581081.8251097158, 172966.4456628128], 
processed observation next is [1.0, 0.782608695652174, 0.26461295418641384, 0.96, 1.0, 1.0, 0.2601516721223265, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.161411618086032, 0.16141161808603216, 0.25815887412360117], 
reward next is 0.7418, 
noisyNet noise sample is [array([-0.6068917], dtype=float32), -0.6872901]. 
=============================================
[2019-04-28 02:46:00,563] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:00,563] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:00,575] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-13_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res8/Eplus-env-sub_run12
[2019-04-28 02:46:00,687] A3C_AGENT_WORKER-Thread-3 INFO:Local step 145500, global step 2331978: loss -46.3883
[2019-04-28 02:46:00,688] A3C_AGENT_WORKER-Thread-3 DEBUG:Local step 145500, global step 2331978: learning rate 0.0000
[2019-04-28 02:46:00,969] A3C_AGENT_WORKER-Thread-15 INFO:Local step 145500, global step 2332112: loss -3.1283
[2019-04-28 02:46:00,971] A3C_AGENT_WORKER-Thread-15 DEBUG:Local step 145500, global step 2332113: learning rate 0.0000
[2019-04-28 02:46:01,262] A3C_AGENT_WORKER-Thread-17 INFO:Local step 145500, global step 2332251: loss -91.4405
[2019-04-28 02:46:01,265] A3C_AGENT_WORKER-Thread-17 DEBUG:Local step 145500, global step 2332252: learning rate 0.0000
[2019-04-28 02:46:01,944] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [1.1173205e-11 9.9967766e-01 7.0438033e-19 3.2235938e-04 8.3401430e-15], sum to 1.0000
[2019-04-28 02:46:01,950] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9243
[2019-04-28 02:46:01,958] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.21666666666667, 89.83333333333333, 1.0, 2.0, 0.6508600437936357, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 909566.0474464631, 909566.0474464631, 210594.4513989143], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 7877400.0000, 
sim time next is 7878000.0000, 
raw observation next is [26.23333333333333, 89.66666666666667, 1.0, 2.0, 0.6033241219465292, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 843109.0231053925, 843109.0231053925, 201360.2444601352], 
processed observation next is [1.0, 0.17391304347826086, 0.44233807266982617, 0.8966666666666667, 1.0, 1.0, 0.522077255357264, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23419695086260903, 0.23419695086260903, 0.30053767829870925], 
reward next is 0.6995, 
noisyNet noise sample is [array([-0.69842404], dtype=float32), 0.018776111]. 
=============================================
[2019-04-28 02:46:01,971] A3C_AGENT_WORKER-Thread-22 DEBUG:Value prediction is [[53.43776 ]
 [53.355083]
 [53.570984]
 [53.84513 ]
 [54.028828]], R is [[53.68400574]
 [53.8328476 ]
 [53.9523735 ]
 [54.06529999]
 [54.17318726]].
[2019-04-28 02:46:02,486] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [6.1611637e-13 9.7997642e-01 1.5906099e-23 2.0023610e-02 5.7471086e-17], sum to 1.0000
[2019-04-28 02:46:02,494] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.5806
[2019-04-28 02:46:02,503] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2155174.450893761 W.
[2019-04-28 02:46:02,506] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.26666666666667, 67.66666666666667, 1.0, 2.0, 0.7706468582700565, 1.0, 1.0, 0.7706468582700565, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2155174.450893761, 2155174.450893761, 405830.0651128915], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 7836000.0000, 
sim time next is 7836600.0000, 
raw observation next is [30.08333333333333, 68.83333333333333, 1.0, 2.0, 0.7741630551948054, 1.0, 2.0, 0.7741630551948054, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2165017.709241207, 2165017.709241207, 407484.9017681458], 
processed observation next is [1.0, 0.6956521739130435, 0.6248025276461293, 0.6883333333333332, 1.0, 1.0, 0.7279072954154282, 1.0, 1.0, 0.7279072954154282, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6013938081225576, 0.6013938081225576, 0.6081864205494714], 
reward next is 0.3918, 
noisyNet noise sample is [array([0.6619778], dtype=float32), 2.49774]. 
=============================================
[2019-04-28 02:46:02,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:02,509] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:02,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:02,555] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:02,569] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-16_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res11/Eplus-env-sub_run12
[2019-04-28 02:46:02,640] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-21_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res16/Eplus-env-sub_run12
[2019-04-28 02:46:04,469] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.5958028e-18 9.9999988e-01 4.0443379e-26 1.6147182e-07 8.5638130e-21], sum to 1.0000
[2019-04-28 02:46:04,473] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8384
[2019-04-28 02:46:04,478] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.86666666666666, 91.0, 1.0, 2.0, 0.3670398073843508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 557268.6903578637, 557268.6903578637, 171022.0574552652], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 111000.0000, 
sim time next is 111600.0000, 
raw observation next is [22.9, 91.0, 1.0, 2.0, 0.3677547410208171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 557765.5716413929, 557765.5716413935, 171045.9753533538], 
processed observation next is [1.0, 0.30434782608695654, 0.2843601895734597, 0.91, 1.0, 1.0, 0.2382587241214664, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.15493488101149802, 0.1549348810114982, 0.2552925005273937], 
reward next is 0.7447, 
noisyNet noise sample is [array([-1.8378036], dtype=float32), 0.4158748]. 
=============================================
[2019-04-28 02:46:05,487] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:05,488] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:05,576] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-19_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res14/Eplus-env-sub_run12
[2019-04-28 02:46:06,252] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [3.8830443e-13 9.9863297e-01 5.0539034e-22 1.3670230e-03 1.6072714e-16], sum to 1.0000
[2019-04-28 02:46:06,266] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.3681
[2019-04-28 02:46:06,274] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.06666666666667, 70.66666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 1.0, 0.0, 10.93604248980016, 6.9112, 168.8836264532655, 4310371.048014656, 1455505.06657596, 302803.5508140426], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 7910400.0000, 
sim time next is 7911000.0000, 
raw observation next is [30.1, 70.5, 1.0, 2.0, 0.7549365243814358, 1.0, 1.0, 0.6980583017049804, 1.0, 1.0, 1.03, 7.004475128152777, 6.9112, 170.5573041426782, 2929179.687337085, 2862362.918851623, 539267.2639561858], 
processed observation next is [1.0, 0.5652173913043478, 0.6255924170616115, 0.705, 1.0, 1.0, 0.7047428004595612, 1.0, 0.5, 0.6362148213313017, 1.0, 0.5, 1.0365853658536586, 0.009327512815277661, 0.0, 0.8375144448122397, 0.8136610242603014, 0.7951008107921175, 0.8048765133674415], 
reward next is 0.0000, 
noisyNet noise sample is [array([1.6491922], dtype=float32), 0.32787198]. 
=============================================
[2019-04-28 02:46:06,290] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[63.764946]
 [64.0976  ]
 [63.104736]
 [62.18807 ]
 [61.88222 ]], R is [[64.33614349]
 [63.69278336]
 [63.52288437]
 [62.88765717]
 [62.25878143]].
[2019-04-28 02:46:06,718] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:06,719] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:06,741] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-22_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res17/Eplus-env-sub_run12
[2019-04-28 02:46:08,946] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:08,947] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:08,954] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-3_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res3/Eplus-env-sub_run12
[2019-04-28 02:46:09,330] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:09,331] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:09,343] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-15_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res10/Eplus-env-sub_run12
[2019-04-28 02:46:09,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Last EnergyPlus process has been closed. 
[2019-04-28 02:46:09,498] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:09,562] EPLUS_ENV_Part3-NA-Shg-Train-v1_Thread-17_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res12/Eplus-env-sub_run12
[2019-04-28 02:46:11,203] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [3.4351764e-18 1.0000000e+00 2.7176713e-29 8.3042320e-09 1.4194981e-23], sum to 1.0000
[2019-04-28 02:46:11,213] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.6640
[2019-04-28 02:46:11,217] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.03333333333333, 89.66666666666667, 1.0, 2.0, 0.2571424099729964, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 418631.7409646709, 418631.7409646703, 161548.2531185804], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 362400.0000, 
sim time next is 363000.0000, 
raw observation next is [20.01666666666667, 89.83333333333333, 1.0, 2.0, 0.2575832082847994, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 419326.6400830902, 419326.6400830902, 161591.6598176466], 
processed observation next is [1.0, 0.17391304347826086, 0.14770932069510287, 0.8983333333333333, 1.0, 1.0, 0.1055219376925294, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.11647962224530284, 0.11647962224530284, 0.241181581817383], 
reward next is 0.7588, 
noisyNet noise sample is [array([0.13059507], dtype=float32), 0.40398893]. 
=============================================
[2019-04-28 02:46:11,240] A3C_AGENT_WORKER-Thread-2 DEBUG:Value prediction is [[74.99091 ]
 [74.790215]
 [74.95569 ]
 [75.167946]
 [75.40654 ]], R is [[74.97463989]
 [74.98377228]
 [74.99289703]
 [75.00218201]
 [75.01132965]].
[2019-04-28 02:46:12,602] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [2.0809118e-17 9.9999619e-01 2.2677500e-27 3.7612692e-06 1.1574845e-20], sum to 1.0000
[2019-04-28 02:46:12,609] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.2220
[2019-04-28 02:46:12,618] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 78.0, 1.0, 2.0, 0.3066500209044473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 485678.6350541453, 485678.6350541453, 165984.1924993817], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 316800.0000, 
sim time next is 317400.0000, 
raw observation next is [22.93333333333333, 78.16666666666667, 1.0, 2.0, 0.3057860900630685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 484764.9369811529, 484764.9369811529, 165926.998357975], 
processed observation next is [0.0, 0.6956521739130435, 0.28593996840442326, 0.7816666666666667, 1.0, 1.0, 0.16359769887116687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13465692693920914, 0.13465692693920914, 0.24765223635518657], 
reward next is 0.7523, 
noisyNet noise sample is [array([0.18585795], dtype=float32), 0.4755956]. 
=============================================
[2019-04-28 02:46:14,465] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6409702e-15 9.9998355e-01 5.5519745e-27 1.6452439e-05 3.8803865e-19], sum to 1.0000
[2019-04-28 02:46:14,473] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.0544
[2019-04-28 02:46:14,479] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 89.33333333333334, 1.0, 2.0, 0.2961518835978992, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 473780.3036619611, 473780.3036619617, 165208.6597920846], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 246000.0000, 
sim time next is 246600.0000, 
raw observation next is [21.0, 89.5, 1.0, 2.0, 0.2948496151294944, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 471822.5963958799, 471822.5963958799, 165072.4058873631], 
processed observation next is [0.0, 0.8695652173913043, 0.19431279620853087, 0.895, 1.0, 1.0, 0.15042122304758362, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.13106183233218888, 0.13106183233218888, 0.24637672520501955], 
reward next is 0.7536, 
noisyNet noise sample is [array([0.5012964], dtype=float32), 1.1449897]. 
=============================================
[2019-04-28 02:46:17,828] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [5.1391282e-17 9.9995589e-01 1.4617458e-25 4.4123208e-05 9.4558951e-20], sum to 1.0000
[2019-04-28 02:46:17,834] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.6093
[2019-04-28 02:46:17,839] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.66666666666667, 73.0, 1.0, 2.0, 0.301360185706277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 487300.2156215706, 487300.2156215701, 166165.7320035826], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 391200.0000, 
sim time next is 391800.0000, 
raw observation next is [22.68333333333333, 73.0, 1.0, 2.0, 0.3120328401531444, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 504413.3474573876, 504413.3474573876, 167408.9133559373], 
processed observation next is [1.0, 0.5217391304347826, 0.27409162717219576, 0.73, 1.0, 1.0, 0.17112390379896916, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.14011481873816323, 0.14011481873816323, 0.24986404978498103], 
reward next is 0.7501, 
noisyNet noise sample is [array([0.13329339], dtype=float32), -0.5204052]. 
=============================================
[2019-04-28 02:46:24,224] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [3.6464917e-18 9.9999726e-01 5.4240525e-29 2.7076787e-06 1.1676465e-19], sum to 1.0000
[2019-04-28 02:46:24,233] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8717
[2019-04-28 02:46:24,236] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.5, 91.5, 1.0, 2.0, 0.2839297291492424, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 456765.0287974051, 456765.0287974051, 164052.4937628196], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 257400.0000, 
sim time next is 258000.0000, 
raw observation next is [20.5, 91.66666666666666, 1.0, 2.0, 0.2846809848806243, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 457797.0968169583, 457797.0968169583, 164121.7406051502], 
processed observation next is [0.0, 1.0, 0.1706161137440759, 0.9166666666666665, 1.0, 1.0, 0.138169861301957, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12716586022693285, 0.12716586022693285, 0.24495782179873166], 
reward next is 0.7550, 
noisyNet noise sample is [array([-0.57251316], dtype=float32), 0.34052518]. 
=============================================
[2019-04-28 02:46:24,249] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.312225]
 [78.28885 ]
 [78.26299 ]
 [78.27684 ]
 [78.27204 ]], R is [[78.29212189]
 [78.26434326]
 [78.23691559]
 [78.2097702 ]
 [78.18286896]].
[2019-04-28 02:46:28,148] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.1440158e-16 9.9995482e-01 3.1409424e-25 4.5234778e-05 2.8595889e-17], sum to 1.0000
[2019-04-28 02:46:28,158] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.7485
[2019-04-28 02:46:28,163] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.65, 53.83333333333333, 1.0, 2.0, 0.5499152841677151, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 903703.1610937818, 903703.1610937818, 205279.6561704139], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 492600.0000, 
sim time next is 493200.0000, 
raw observation next is [24.6, 54.0, 1.0, 2.0, 0.5626927310481771, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 924898.0743983721, 924898.0743983714, 207834.4372550697], 
processed observation next is [1.0, 0.7391304347826086, 0.36492890995260674, 0.54, 1.0, 1.0, 0.47312377234720127, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2569161317773256, 0.25691613177732536, 0.310200652619507], 
reward next is 0.6898, 
noisyNet noise sample is [array([-0.47030896], dtype=float32), 0.10247133]. 
=============================================
[2019-04-28 02:46:28,651] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.2923074e-16 9.9997926e-01 1.8750946e-25 2.0763635e-05 1.2877543e-18], sum to 1.0000
[2019-04-28 02:46:28,657] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6765
[2019-04-28 02:46:28,663] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.63333333333333, 93.0, 1.0, 2.0, 0.2241964888763261, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 373352.8194498153, 373352.8194498153, 158004.704600148], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 700800.0000, 
sim time next is 701400.0000, 
raw observation next is [17.61666666666667, 93.0, 1.0, 2.0, 0.2231216326482891, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 371605.0903524174, 371605.090352418, 157897.7036187377], 
processed observation next is [1.0, 0.08695652173913043, 0.03396524486571906, 0.93, 1.0, 1.0, 0.06400196704613144, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.10322363620900482, 0.103223636209005, 0.23566821435632493], 
reward next is 0.7643, 
noisyNet noise sample is [array([0.19603354], dtype=float32), 1.0155023]. 
=============================================
[2019-04-28 02:46:30,845] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.0972453e-15 9.9922383e-01 2.5182734e-24 7.7615760e-04 1.2387756e-17], sum to 1.0000
[2019-04-28 02:46:30,852] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.5805
[2019-04-28 02:46:30,864] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.11666666666667, 91.83333333333333, 1.0, 2.0, 0.206211874541012, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 344595.6360731431, 344595.6360731431, 155942.4043292709], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 615000.0000, 
sim time next is 615600.0000, 
raw observation next is [17.1, 92.0, 1.0, 2.0, 0.2060497328044147, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 344322.7328201954, 344322.7328201961, 155930.3209037432], 
processed observation next is [1.0, 0.13043478260869565, 0.009478672985782125, 0.92, 1.0, 1.0, 0.04343341301736708, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.09564520356116538, 0.09564520356116557, 0.23273182224439284], 
reward next is 0.7673, 
noisyNet noise sample is [array([-0.5972105], dtype=float32), -1.4304345]. 
=============================================
[2019-04-28 02:46:32,520] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.6206833e-14 9.9998403e-01 2.3175582e-23 1.5943810e-05 9.2602985e-18], sum to 1.0000
[2019-04-28 02:46:32,526] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.4289
[2019-04-28 02:46:32,534] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [17.16666666666667, 91.33333333333334, 1.0, 2.0, 0.2182630173528393, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 364737.7274437897, 364737.7274437897, 156963.1787796891], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 613200.0000, 
sim time next is 613800.0000, 
raw observation next is [17.15, 91.5, 1.0, 2.0, 0.2087223887586454, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 348792.9853543976, 348792.9853543976, 156150.0011927825], 
processed observation next is [1.0, 0.08695652173913043, 0.011848341232227487, 0.915, 1.0, 1.0, 0.046653480432102885, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.09688694037622156, 0.09688694037622156, 0.23305970327280973], 
reward next is 0.7669, 
noisyNet noise sample is [array([-0.1024618], dtype=float32), 0.41501293]. 
=============================================
[2019-04-28 02:46:40,049] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [8.0522145e-18 9.9997294e-01 2.0487484e-27 2.7034526e-05 5.6166727e-20], sum to 1.0000
[2019-04-28 02:46:40,060] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.6323
[2019-04-28 02:46:40,064] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.7, 63.66666666666667, 1.0, 2.0, 0.2452484764347657, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 404157.9458174463, 404157.9458174463, 160423.4053713964], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 498000.0000, 
sim time next is 498600.0000, 
raw observation next is [22.45, 65.0, 1.0, 2.0, 0.2424079751973434, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 399628.108440928, 399628.1084409273, 160144.5748213432], 
processed observation next is [1.0, 0.782608695652174, 0.26303317535545023, 0.65, 1.0, 1.0, 0.08723852433414865, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11100780790025777, 0.11100780790025758, 0.23902175346469134], 
reward next is 0.7610, 
noisyNet noise sample is [array([1.0051215], dtype=float32), 1.1960676]. 
=============================================
[2019-04-28 02:46:40,300] A3C_AGENT_WORKER-Thread-15 INFO:Evaluating...
[2019-04-28 02:46:40,305] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:46:40,324] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:46:40,326] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:40,327] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:46:40,330] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:40,332] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:46:40,334] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:40,336] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:40,330] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:46:40,338] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:46:40,350] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run95
[2019-04-28 02:46:40,384] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run95
[2019-04-28 02:46:40,385] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run95
[2019-04-28 02:46:40,422] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run95
[2019-04-28 02:46:40,507] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run95
[2019-04-28 02:46:48,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1340746]
[2019-04-28 02:46:48,588] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.53524594666667, 63.59337137000001, 1.0, 2.0, 0.3401688193917846, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 532979.2917192398, 532979.2917192392, 169480.8104979667]
[2019-04-28 02:46:48,589] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:46:48,591] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.2075068e-14 9.9993205e-01 1.4191202e-23 6.7907269e-05 2.5036820e-17], sampled 0.6321379454637678
[2019-04-28 02:47:40,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1340746]
[2019-04-28 02:47:40,375] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.0, 84.0, 1.0, 2.0, 0.5100935444767921, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712781.2081546375, 712781.2081546369, 185192.638322905]
[2019-04-28 02:47:40,376] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:47:40,378] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.4864723e-14 9.9993992e-01 7.2881392e-24 6.0111073e-05 1.5411389e-17], sampled 0.4305682902011885
[2019-04-28 02:47:52,162] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1340746]
[2019-04-28 02:47:52,163] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [28.2, 89.0, 1.0, 2.0, 0.5685113411793915, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9873161900551203, 6.911199999999999, 6.9112, 168.9125949764211, 1589478.784773544, 1589478.784773544, 347817.9667359103]
[2019-04-28 02:47:52,164] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:47:52,165] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [5.6406594e-12 9.9962234e-01 1.6043688e-19 3.7759362e-04 2.2417679e-14], sampled 0.9247728973711893
[2019-04-28 02:48:01,653] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1340746]
[2019-04-28 02:48:01,653] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [27.33453021666666, 76.95542913166668, 1.0, 2.0, 0.478393296233541, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 668470.8048940826, 668470.8048940826, 180280.1111722963]
[2019-04-28 02:48:01,654] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:48:01,656] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.5894114e-14 9.9993861e-01 8.1595780e-24 6.1360974e-05 1.6732330e-17], sampled 0.4655128211609053
[2019-04-28 02:48:15,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1340746]
[2019-04-28 02:48:15,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.2233848, 72.94842202, 1.0, 2.0, 0.6745863845362154, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.995859509561233, 6.9112, 168.9124093310793, 1839566.748390028, 1779506.57311237, 379627.873862627]
[2019-04-28 02:48:15,571] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:48:15,573] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.83756740e-14 9.99899387e-01 1.19729383e-22 1.00562655e-04
 1.18367375e-16], sampled 0.31661507707354897
[2019-04-28 02:48:15,574] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1839566.748390028 W.
[2019-04-28 02:48:16,032] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8493.7703 2842631099.5208 1131.0000
[2019-04-28 02:48:16,302] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:48:16,408] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:48:16,454] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8254.3666 2927217796.0829 1337.0000
[2019-04-28 02:48:16,507] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.0825 3164065548.2349 1778.0000
[2019-04-28 02:48:17,523] A3C_AGENT_WORKER-Thread-15 INFO:Global step: 2350000, evaluation results [2350000.0, 7884.082509737438, 3164065548.234936, 1778.0, 8254.366556485142, 2927217796.082931, 1337.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8493.770328695708, 2842631099.520829, 1131.0]
[2019-04-28 02:48:31,284] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [5.8037705e-18 9.9999869e-01 3.4996782e-26 1.2757686e-06 4.3583273e-19], sum to 1.0000
[2019-04-28 02:48:31,291] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3781
[2019-04-28 02:48:31,319] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.85, 87.5, 1.0, 2.0, 0.2801559704741876, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 451697.7751915869, 451697.7751915863, 163713.5292860979], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 880200.0000, 
sim time next is 880800.0000, 
raw observation next is [20.83333333333334, 87.33333333333334, 1.0, 2.0, 0.2793482335134556, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 450704.8353969005, 450704.8353969005, 163647.0533938554], 
processed observation next is [0.0, 0.17391304347826086, 0.1864139020537128, 0.8733333333333334, 1.0, 1.0, 0.1317448596547658, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.12519578761025013, 0.12519578761025013, 0.24424933342366478], 
reward next is 0.7558, 
noisyNet noise sample is [array([0.38383147], dtype=float32), 0.074396156]. 
=============================================
[2019-04-28 02:48:32,434] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [4.1309035e-14 9.9991274e-01 5.6206525e-24 8.7304630e-05 6.1147399e-18], sum to 1.0000
[2019-04-28 02:48:32,449] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1233
[2019-04-28 02:48:32,454] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.91666666666667, 88.16666666666667, 1.0, 2.0, 0.2863654259169712, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 460412.6558931374, 460412.6558931368, 164299.2700965568], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 877800.0000, 
sim time next is 878400.0000, 
raw observation next is [20.9, 88.0, 1.0, 2.0, 0.2858749732194508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 459951.6349758428, 459951.6349758428, 164268.9991846066], 
processed observation next is [0.0, 0.17391304347826086, 0.1895734597156398, 0.88, 1.0, 1.0, 0.13960840146921785, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1277643430488452, 0.1277643430488452, 0.24517761072329342], 
reward next is 0.7548, 
noisyNet noise sample is [array([-1.6378049], dtype=float32), 0.5713884]. 
=============================================
[2019-04-28 02:48:35,913] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [4.0519854e-16 9.9999940e-01 2.0676630e-26 5.8450553e-07 2.9567226e-18], sum to 1.0000
[2019-04-28 02:48:35,918] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7188
[2019-04-28 02:48:35,924] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [19.53333333333333, 88.66666666666666, 1.0, 2.0, 0.2538946702498717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 417037.6318998868, 417037.6318998868, 161296.6966982902], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 776400.0000, 
sim time next is 777000.0000, 
raw observation next is [19.51666666666667, 88.83333333333334, 1.0, 2.0, 0.2536828856595335, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 416672.4523089746, 416672.4523089739, 161275.6469874782], 
processed observation next is [1.0, 1.0, 0.1240126382306479, 0.8883333333333334, 1.0, 1.0, 0.10082275380666687, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.11574234786360404, 0.11574234786360385, 0.24070992087683313], 
reward next is 0.7593, 
noisyNet noise sample is [array([1.1132606], dtype=float32), -0.43083578]. 
=============================================
[2019-04-28 02:48:35,961] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[76.67092 ]
 [76.646645]
 [76.620155]
 [76.59277 ]
 [76.57033 ]], R is [[76.68548584]
 [76.67789459]
 [76.67030334]
 [76.66266632]
 [76.65496826]].
[2019-04-28 02:48:47,982] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.5134405e-15 9.9993014e-01 5.8204228e-25 6.9836926e-05 1.1634194e-20], sum to 1.0000
[2019-04-28 02:48:47,991] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.5545
[2019-04-28 02:48:47,996] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.83333333333334, 92.66666666666667, 1.0, 2.0, 0.354959699653924, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 551870.001663338, 551870.001663338, 170931.3663231286], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1222800.0000, 
sim time next is 1223400.0000, 
raw observation next is [21.81666666666667, 92.83333333333333, 1.0, 2.0, 0.3491319941215574, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 542759.697251745, 542759.697251745, 170177.2541444965], 
processed observation next is [1.0, 0.13043478260869565, 0.2330173775671408, 0.9283333333333332, 1.0, 1.0, 0.21582167966452695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.15076658256992917, 0.15076658256992917, 0.2539959017082037], 
reward next is 0.7460, 
noisyNet noise sample is [array([0.40535805], dtype=float32), -0.61394984]. 
=============================================
[2019-04-28 02:48:51,236] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [1.2898687e-15 9.9977213e-01 2.9733932e-22 2.2792550e-04 9.3848841e-19], sum to 1.0000
[2019-04-28 02:48:51,243] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.1768
[2019-04-28 02:48:51,246] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.03333333333333, 94.83333333333333, 1.0, 2.0, 0.3246202013418588, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 511649.2636062397, 511649.2636062391, 167874.5646775391], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1371000.0000, 
sim time next is 1371600.0000, 
raw observation next is [21.0, 95.0, 1.0, 2.0, 0.3227843354490936, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 508922.2850250268, 508922.2850250274, 167669.4591381972], 
processed observation next is [1.0, 0.9130434782608695, 0.19431279620853087, 0.95, 1.0, 1.0, 0.18407751258926935, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14136730139584078, 0.14136730139584094, 0.2502529240868615], 
reward next is 0.7497, 
noisyNet noise sample is [array([-0.22458415], dtype=float32), -0.16344056]. 
=============================================
[2019-04-28 02:48:54,598] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.0919233e-14 9.9959797e-01 1.0300364e-20 4.0201587e-04 9.1289082e-17], sum to 1.0000
[2019-04-28 02:48:54,606] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.6578
[2019-04-28 02:48:54,611] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.05, 93.5, 1.0, 2.0, 0.2889763351293487, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 466575.7557576234, 466575.7557576234, 164720.1972861002], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1135800.0000, 
sim time next is 1136400.0000, 
raw observation next is [20.0, 93.66666666666667, 1.0, 2.0, 0.2836976881972058, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 458312.5188377349, 458312.5188377356, 164155.6497084287], 
processed observation next is [1.0, 0.13043478260869565, 0.1469194312796209, 0.9366666666666668, 1.0, 1.0, 0.1369851665026576, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1273090330104819, 0.1273090330104821, 0.24500843240063988], 
reward next is 0.7550, 
noisyNet noise sample is [array([0.00602631], dtype=float32), 0.6920801]. 
=============================================
[2019-04-28 02:48:57,480] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [4.7241373e-16 9.9998164e-01 1.4739292e-24 1.8378159e-05 5.4545883e-16], sum to 1.0000
[2019-04-28 02:48:57,496] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.4066
[2019-04-28 02:48:57,511] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [20.9, 89.5, 1.0, 2.0, 0.2928451233047513, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 469547.5094624624, 469547.5094624631, 164921.6273135842], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1125000.0000, 
sim time next is 1125600.0000, 
raw observation next is [20.86666666666667, 89.66666666666667, 1.0, 2.0, 0.292297394789205, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 468788.9812294227, 468788.9812294221, 164869.7039335522], 
processed observation next is [1.0, 0.0, 0.18799368088467638, 0.8966666666666667, 1.0, 1.0, 0.14734625878217472, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.13021916145261742, 0.13021916145261725, 0.24607418497545105], 
reward next is 0.7539, 
noisyNet noise sample is [array([-0.4782661], dtype=float32), 1.5878413]. 
=============================================
[2019-04-28 02:49:08,206] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [8.3572519e-17 9.9998844e-01 3.8332674e-26 1.1590375e-05 1.4202165e-18], sum to 1.0000
[2019-04-28 02:49:08,219] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.1659
[2019-04-28 02:49:08,222] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.2, 78.5, 1.0, 2.0, 0.3560926938630097, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 546592.6715774988, 546592.6715774994, 170305.2003777306], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1539000.0000, 
sim time next is 1539600.0000, 
raw observation next is [24.0, 80.0, 1.0, 2.0, 0.3576428750531925, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 548685.4184535787, 548685.4184535787, 170471.7032823074], 
processed observation next is [0.0, 0.8260869565217391, 0.3364928909952607, 0.8, 1.0, 1.0, 0.2260757530761355, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1524126162371052, 0.1524126162371052, 0.25443537803329463], 
reward next is 0.7456, 
noisyNet noise sample is [array([-0.40813258], dtype=float32), -0.2883411]. 
=============================================
[2019-04-28 02:49:11,324] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.1183464e-15 9.9999869e-01 4.5130387e-25 1.2908512e-06 1.1175235e-19], sum to 1.0000
[2019-04-28 02:49:11,338] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.6288
[2019-04-28 02:49:11,343] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.26666666666667, 92.33333333333334, 1.0, 2.0, 0.6524538832873579, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1001912.57165928, 1001912.571659281, 222064.0386547018], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1340400.0000, 
sim time next is 1341000.0000, 
raw observation next is [22.2, 92.0, 1.0, 2.0, 0.5936243180710554, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 914529.9528314535, 914529.9528314535, 209841.8331625114], 
processed observation next is [1.0, 0.5217391304347826, 0.2511848341232228, 0.92, 1.0, 1.0, 0.5103907446639222, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2540360980087371, 0.2540360980087371, 0.31319676591419615], 
reward next is 0.6868, 
noisyNet noise sample is [array([0.5778313], dtype=float32), -0.37023968]. 
=============================================
[2019-04-28 02:49:11,361] A3C_AGENT_WORKER-Thread-3 DEBUG:Value prediction is [[78.59017 ]
 [78.260185]
 [78.38198 ]
 [78.30378 ]
 [78.18661 ]], R is [[78.53340912]
 [78.41664124]
 [78.2241745 ]
 [78.06937408]
 [77.93610382]].
[2019-04-28 02:49:17,888] A3C_AGENT_WORKER-Thread-3 INFO:Evaluating...
[2019-04-28 02:49:17,890] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:49:17,891] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:49:17,891] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:49:17,892] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:49:17,892] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:49:17,892] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:49:17,893] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:49:17,894] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:49:17,893] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:49:17,895] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:49:17,935] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run96
[2019-04-28 02:49:17,973] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run96
[2019-04-28 02:49:18,003] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run96
[2019-04-28 02:49:18,042] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run96
[2019-04-28 02:49:18,074] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run96
[2019-04-28 02:49:19,737] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:49:19,737] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.91666666666667, 62.5, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 8.51007511185068, 6.9112, 168.9041546335589, 2679815.757138647, 1545577.830785188, 324484.2008179007]
[2019-04-28 02:49:19,738] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:49:19,740] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [5.2601674e-14 9.9996793e-01 6.9658198e-23 3.2074997e-05 1.1245295e-16], sampled 0.9637158637927831
[2019-04-28 02:49:19,740] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 0, 1, 0] for the demand 2679815.757138647 W.
[2019-04-28 02:49:35,651] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:49:35,652] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [21.42887504333333, 88.72644867, 1.0, 2.0, 0.3163566134192195, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 503906.2057953756, 503906.2057953763, 167373.6149454799]
[2019-04-28 02:49:35,653] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:49:35,655] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.4740964e-14 9.9997926e-01 8.2469673e-24 2.0705185e-05 2.4091306e-17], sampled 0.3374110031697748
[2019-04-28 02:49:48,238] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:49:48,239] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.21666666666667, 76.33333333333333, 1.0, 2.0, 0.5072403198911464, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 736731.3820587979, 736731.3820587985, 188270.2356517988]
[2019-04-28 02:49:48,240] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:49:48,241] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [1.16362405e-14 9.99980927e-01 5.54462405e-24 1.90936880e-05
 1.80883108e-17], sampled 0.21508076774533724
[2019-04-28 02:50:39,623] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:50:39,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [28.60582436333333, 87.8766887, 1.0, 2.0, 0.7662671052000724, 0.0, 2.0, 0.0, 1.0, 1.0, 1.03, 7.005979679978065, 6.9112, 168.9118558141123, 1967863.989031517, 1900624.460149554, 399670.9380656253]
[2019-04-28 02:50:39,624] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:50:39,632] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [9.6339278e-13 9.9991286e-01 9.1514335e-21 8.7134453e-05 3.8078595e-15], sampled 0.3973956812644568
[2019-04-28 02:50:39,633] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1967863.989031517 W.
[2019-04-28 02:50:44,049] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:50:44,054] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [29.21666666666667, 83.83333333333333, 1.0, 2.0, 0.5910851689919094, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 825999.1751060665, 825999.1751060665, 199096.6864875829]
[2019-04-28 02:50:44,057] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:50:44,063] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.8619684e-15 9.9998331e-01 2.8723094e-24 1.6685328e-05 1.1251040e-17], sampled 0.716967295119043
[2019-04-28 02:50:50,203] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:50:50,204] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.53333333333333, 77.66666666666667, 1.0, 2.0, 0.8947656710261188, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1250620.808167445, 1250620.808167444, 268504.0415527661]
[2019-04-28 02:50:50,205] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:50:50,210] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.9958595e-14 9.9997699e-01 1.3707850e-23 2.2983832e-05 3.4773813e-17], sampled 0.725876159162416
[2019-04-28 02:51:19,163] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.13975129]
[2019-04-28 02:51:19,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [24.45, 94.5, 1.0, 2.0, 0.5988745510912139, 0.0, 2.0, 0.0, 1.0, 1.0, 0.9974587795230102, 6.911200000000001, 6.9112, 168.9126980703873, 1674437.069039251, 1674437.06903925, 357290.6134396274]
[2019-04-28 02:51:19,165] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:51:19,168] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.2308337e-14 9.9997616e-01 1.6520685e-23 2.3879427e-05 3.9796013e-17], sampled 0.3366127232017151
[2019-04-28 02:51:19,170] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1674437.069039251 W.
[2019-04-28 02:51:26,620] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:51:26,637] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:51:26,721] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:51:26,727] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:51:26,765] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:51:27,782] A3C_AGENT_WORKER-Thread-3 INFO:Global step: 2375000, evaluation results [2375000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:51:28,195] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7768022e-14 9.9998176e-01 2.1005026e-23 1.8184344e-05 1.2522358e-15], sum to 1.0000
[2019-04-28 02:51:28,203] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9996
[2019-04-28 02:51:28,207] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.26666666666667, 70.66666666666667, 1.0, 2.0, 0.4270259918218965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 617269.5286702916, 617269.5286702916, 175605.094778121], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1433400.0000, 
sim time next is 1434000.0000, 
raw observation next is [27.33333333333334, 70.33333333333334, 1.0, 2.0, 0.4272811100127285, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 617414.2181239652, 617414.2181239659, 175612.5894926342], 
processed observation next is [0.0, 0.6086956521739131, 0.4944707740916275, 0.7033333333333335, 1.0, 1.0, 0.30997724097919094, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17150394947887923, 0.17150394947887942, 0.2621083425263197], 
reward next is 0.7379, 
noisyNet noise sample is [array([1.0301999], dtype=float32), -0.49750137]. 
=============================================
[2019-04-28 02:51:28,222] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[70.63898]
 [70.65493]
 [70.66576]
 [70.68325]
 [70.69778]], R is [[70.67883301]
 [70.70994568]
 [70.74073792]
 [70.77114105]
 [70.8011322 ]].
[2019-04-28 02:51:28,320] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.0167587e-15 9.9999821e-01 5.2596001e-23 1.7452088e-06 1.0092864e-18], sum to 1.0000
[2019-04-28 02:51:28,331] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.1769
[2019-04-28 02:51:28,335] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 90.66666666666667, 1.0, 2.0, 0.3877274178202514, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581494.2746672592, 581494.2746672592, 172934.328901482], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1453800.0000, 
sim time next is 1454400.0000, 
raw observation next is [23.0, 92.0, 1.0, 2.0, 0.3844789086674804, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 578128.2771486635, 578128.2771486642, 172678.2718035859], 
processed observation next is [0.0, 0.8695652173913043, 0.28909952606635075, 0.92, 1.0, 1.0, 0.25840832369575956, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.16059118809685097, 0.16059118809685116, 0.2577287638859491], 
reward next is 0.7423, 
noisyNet noise sample is [array([1.9482857], dtype=float32), -0.6406729]. 
=============================================
[2019-04-28 02:51:30,177] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.3256043e-13 9.9750769e-01 1.4165006e-23 2.4923140e-03 1.5830679e-16], sum to 1.0000
[2019-04-28 02:51:30,186] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.3331
[2019-04-28 02:51:30,206] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.25, 99.0, 1.0, 2.0, 0.4681029145885947, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 675971.529121182, 675971.5291211813, 181536.7750607812], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1650600.0000, 
sim time next is 1651200.0000, 
raw observation next is [23.26666666666667, 99.0, 1.0, 2.0, 0.472636319865294, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 682080.1074267359, 682080.1074267353, 182177.0432744259], 
processed observation next is [1.0, 0.08695652173913043, 0.3017377567140602, 0.99, 1.0, 1.0, 0.36462207212686015, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18946669650742665, 0.18946669650742648, 0.2719060347379491], 
reward next is 0.7281, 
noisyNet noise sample is [array([-1.2864203], dtype=float32), -0.5611995]. 
=============================================
[2019-04-28 02:51:34,404] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.3365259e-11 9.9864489e-01 6.1975221e-20 1.3550796e-03 2.9950377e-14], sum to 1.0000
[2019-04-28 02:51:34,414] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1302
[2019-04-28 02:51:34,420] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.86666666666667, 89.0, 1.0, 2.0, 0.9425105176243491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1340210.611618378, 1340210.611618378, 285355.522334118], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1759200.0000, 
sim time next is 1759800.0000, 
raw observation next is [24.78333333333333, 89.0, 1.0, 2.0, 0.9460413016240383, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1349931.384088655, 1349931.384088655, 287099.2215068976], 
processed observation next is [1.0, 0.34782608695652173, 0.37361769352290675, 0.89, 1.0, 1.0, 0.9349895200289619, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3749809400246264, 0.3749809400246264, 0.4285063007565636], 
reward next is 0.5715, 
noisyNet noise sample is [array([0.0359393], dtype=float32), 2.7076766]. 
=============================================
[2019-04-28 02:51:38,147] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.9951501e-15 9.9999583e-01 1.1456742e-23 4.2153215e-06 7.4600137e-19], sum to 1.0000
[2019-04-28 02:51:38,156] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.9944
[2019-04-28 02:51:38,165] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.06666666666667, 91.16666666666667, 1.0, 2.0, 0.4754312487179817, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 664330.5683320911, 664330.5683320905, 179836.0029839871], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2063400.0000, 
sim time next is 2064000.0000, 
raw observation next is [25.03333333333334, 91.33333333333334, 1.0, 2.0, 0.4750164030414903, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 663750.7142235433, 663750.7142235427, 179774.0483028352], 
processed observation next is [0.0, 0.9130434782608695, 0.3854660347551346, 0.9133333333333334, 1.0, 1.0, 0.367489642218663, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1843751983954287, 0.18437519839542854, 0.2683194750788585], 
reward next is 0.7317, 
noisyNet noise sample is [array([0.8818878], dtype=float32), -0.9835316]. 
=============================================
[2019-04-28 02:51:38,184] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[72.58449]
 [72.60166]
 [72.59539]
 [72.60023]
 [72.61482]], R is [[72.59148407]
 [72.59716034]
 [72.60268402]
 [72.608078  ]
 [72.61336517]].
[2019-04-28 02:51:38,520] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [7.2517498e-15 9.9999201e-01 5.1771764e-23 7.9958118e-06 7.5881405e-18], sum to 1.0000
[2019-04-28 02:51:38,536] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.8418
[2019-04-28 02:51:38,543] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.18333333333333, 96.16666666666666, 1.0, 2.0, 0.4160070435122561, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 610601.1554898728, 610601.1554898728, 175238.9585569047], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1631400.0000, 
sim time next is 1632000.0000, 
raw observation next is [23.16666666666667, 96.33333333333333, 1.0, 2.0, 0.4164400034122114, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 611138.6189723732, 611138.6189723732, 175287.062005289], 
processed observation next is [1.0, 0.9130434782608695, 0.2969984202211693, 0.9633333333333333, 1.0, 1.0, 0.29691566676170045, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16976072749232587, 0.16976072749232587, 0.26162248060490895], 
reward next is 0.7384, 
noisyNet noise sample is [array([0.8025025], dtype=float32), 0.12524559]. 
=============================================
[2019-04-28 02:51:38,566] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[71.15471 ]
 [71.19882 ]
 [71.160675]
 [71.14423 ]
 [71.121315]], R is [[71.22708893]
 [71.25326538]
 [71.2791214 ]
 [71.30456543]
 [71.32978058]].
[2019-04-28 02:51:42,882] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4410298e-13 9.9995089e-01 6.9030624e-23 4.9106609e-05 4.3383350e-17], sum to 1.0000
[2019-04-28 02:51:42,892] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.6337
[2019-04-28 02:51:42,896] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.0, 74.83333333333333, 1.0, 2.0, 0.5548215783842794, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 775304.9105216585, 775304.9105216579, 192631.8159201287], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2116200.0000, 
sim time next is 2116800.0000, 
raw observation next is [30.0, 75.0, 1.0, 2.0, 0.5565474575879465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 777717.5284410662, 777717.5284410662, 192930.6573023678], 
processed observation next is [0.0, 0.5217391304347826, 0.6208530805687204, 0.75, 1.0, 1.0, 0.46571982841921267, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21603264678918505, 0.21603264678918505, 0.28795620492890717], 
reward next is 0.7120, 
noisyNet noise sample is [array([0.84384423], dtype=float32), 0.68867624]. 
=============================================
[2019-04-28 02:51:50,755] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.7497807e-10 9.8098022e-01 2.3801053e-17 1.9019807e-02 4.0129098e-12], sum to 1.0000
[2019-04-28 02:51:50,767] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.2957
[2019-04-28 02:51:50,774] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.4, 85.16666666666667, 1.0, 2.0, 0.886336154583685, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1238831.934994465, 1238831.934994465, 266203.9479862264], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 1860600.0000, 
sim time next is 1861200.0000, 
raw observation next is [26.5, 85.0, 1.0, 2.0, 0.7470040219412213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1043991.693378904, 1043991.693378904, 231315.5883646876], 
processed observation next is [1.0, 0.5652173913043478, 0.4549763033175356, 0.85, 1.0, 1.0, 0.695185568603881, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2899976926052511, 0.2899976926052511, 0.3452471468129666], 
reward next is 0.6548, 
noisyNet noise sample is [array([0.7275497], dtype=float32), 0.3354721]. 
=============================================
[2019-04-28 02:51:54,522] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.20171356e-11 9.96375144e-01 7.29368573e-19 3.62485298e-03
 1.39104266e-11], sum to 1.0000
[2019-04-28 02:51:54,531] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.1139
[2019-04-28 02:51:54,534] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.1, 94.66666666666666, 1.0, 2.0, 0.8384846329673163, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1171912.958737343, 1171912.958737343, 253573.8587463729], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2169600.0000, 
sim time next is 2170200.0000, 
raw observation next is [25.05, 94.83333333333333, 1.0, 2.0, 0.7996795206832504, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1117648.222401079, 1117648.222401079, 243829.3396713943], 
processed observation next is [1.0, 0.08695652173913043, 0.3862559241706162, 0.9483333333333333, 1.0, 1.0, 0.7586500249195788, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.31045783955585526, 0.31045783955585526, 0.3639243875692452], 
reward next is 0.6361, 
noisyNet noise sample is [array([-2.6005676], dtype=float32), -0.05680086]. 
=============================================
[2019-04-28 02:51:57,502] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8742301e-10 9.9826449e-01 8.4584558e-19 1.7355724e-03 4.6444957e-12], sum to 1.0000
[2019-04-28 02:51:57,508] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.5609
[2019-04-28 02:51:57,514] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.6, 96.66666666666666, 1.0, 2.0, 0.5738524123772334, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 801908.541795593, 801908.541795593, 195969.5670910848], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2176800.0000, 
sim time next is 2177400.0000, 
raw observation next is [24.55, 96.83333333333334, 1.0, 2.0, 0.5637536282232145, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 787791.14109876, 787791.14109876, 194182.0596976558], 
processed observation next is [1.0, 0.17391304347826086, 0.3625592417061612, 0.9683333333333334, 1.0, 1.0, 0.47440196171471627, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21883087252743333, 0.21883087252743333, 0.28982396969799373], 
reward next is 0.7102, 
noisyNet noise sample is [array([0.32022613], dtype=float32), 1.5039222]. 
=============================================
[2019-04-28 02:51:57,991] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [9.0824693e-14 9.9991715e-01 4.8053285e-24 8.2795610e-05 2.7460709e-15], sum to 1.0000
[2019-04-28 02:51:57,999] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.5353
[2019-04-28 02:51:58,007] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.33333333333334, 95.66666666666667, 1.0, 2.0, 0.4675708701703993, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655868.527327139, 655868.5273271385, 178997.5540942362], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2079600.0000, 
sim time next is 2080200.0000, 
raw observation next is [24.31666666666667, 95.83333333333333, 1.0, 2.0, 0.467735033285475, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 655960.7706544079, 655960.7706544073, 179003.9712951069], 
processed observation next is [0.0, 0.043478260869565216, 0.3515007898894157, 0.9583333333333333, 1.0, 1.0, 0.35871690757286145, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.18221132518178, 0.18221132518177982, 0.2671701064106073], 
reward next is 0.7328, 
noisyNet noise sample is [array([-0.5118365], dtype=float32), -0.13711242]. 
=============================================
[2019-04-28 02:52:14,573] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [3.0222085e-11 9.9999833e-01 2.3344661e-17 1.6728412e-06 3.4340742e-12], sum to 1.0000
[2019-04-28 02:52:14,589] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.9308
[2019-04-28 02:52:14,598] A3C_AGENT_WORKER-Thread-17 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1692144.105965784 W.
[2019-04-28 02:52:14,606] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [31.95, 63.5, 1.0, 2.0, 0.4034714700060221, 1.0, 2.0, 0.4034714700060221, 1.0, 1.0, 0.6998402450672884, 6.911199999999999, 6.9112, 170.5573041426782, 1692144.105965784, 1692144.105965785, 354698.1943202058], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 2291400.0000, 
sim time next is 2292000.0000, 
raw observation next is [31.93333333333333, 63.66666666666667, 1.0, 2.0, 0.8042139578714953, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.000135321076685, 6.9112, 168.9123931976613, 2020971.031610588, 1957877.464334555, 409101.2242577212], 
processed observation next is [1.0, 0.5217391304347826, 0.7124802527646128, 0.6366666666666667, 1.0, 1.0, 0.7641132022548136, 0.0, 0.5, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.008893532107668457, 0.0, 0.8294371790287177, 0.5613808421140523, 0.5438548512040431, 0.6105988421757033], 
reward next is 0.0000, 
noisyNet noise sample is [array([0.06259757], dtype=float32), 1.109998]. 
=============================================
[2019-04-28 02:52:14,621] A3C_AGENT_WORKER-Thread-17 DEBUG:Value prediction is [[46.614147]
 [46.314816]
 [45.99536 ]
 [45.657024]
 [46.010796]], R is [[46.38881683]
 [46.3955307 ]
 [46.38960266]
 [46.23286819]
 [46.10116196]].
[2019-04-28 02:52:17,988] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.2297593e-14 9.9996555e-01 3.8600403e-22 3.4477031e-05 3.8547546e-16], sum to 1.0000
[2019-04-28 02:52:17,997] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.9532
[2019-04-28 02:52:18,004] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.7, 87.0, 1.0, 2.0, 0.5415113386926087, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 756698.6227808386, 756698.6227808392, 190356.336347664], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2485800.0000, 
sim time next is 2486400.0000, 
raw observation next is [27.63333333333333, 87.66666666666666, 1.0, 2.0, 0.5447932851181566, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 761286.4016643214, 761286.401664322, 190912.3790330255], 
processed observation next is [1.0, 0.782608695652174, 0.5086887835703, 0.8766666666666666, 1.0, 1.0, 0.4515581748411525, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21146844490675595, 0.21146844490675612, 0.28494384930302313], 
reward next is 0.7151, 
noisyNet noise sample is [array([0.28180793], dtype=float32), -1.7798219]. 
=============================================
[2019-04-28 02:52:19,689] A3C_AGENT_WORKER-Thread-22 INFO:Evaluating...
[2019-04-28 02:52:19,691] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:52:19,692] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:52:19,693] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:52:19,693] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:52:19,693] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:52:19,695] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:52:19,694] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:52:19,697] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:52:19,697] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:52:19,698] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:52:19,715] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run97
[2019-04-28 02:52:19,750] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run97
[2019-04-28 02:52:19,778] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run97
[2019-04-28 02:52:19,804] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run97
[2019-04-28 02:52:19,835] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run97
[2019-04-28 02:52:30,565] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:52:30,566] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.29440299833333, 84.49339263666667, 1.0, 2.0, 0.3591795883946213, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 550980.9317923187, 550980.9317923181, 170662.452715896]
[2019-04-28 02:52:30,568] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:52:30,569] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [7.2204468e-15 9.9998820e-01 3.6137395e-24 1.1787139e-05 2.8487542e-16], sampled 0.5617445710342132
[2019-04-28 02:52:42,625] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:52:42,627] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [27.26666666666667, 82.83333333333334, 1.0, 2.0, 0.5107027243003914, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 713632.7338979434, 713632.7338979434, 185290.1323114938]
[2019-04-28 02:52:42,628] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:52:42,630] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [6.24670663e-15 9.99988794e-01 2.83666394e-24 1.12131975e-05
 2.42638332e-16], sampled 0.6334393883078059
[2019-04-28 02:52:53,294] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:52:53,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [25.62337378333334, 94.032324975, 1.0, 2.0, 0.5059302098567885, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 706961.6203457896, 706961.6203457896, 184530.76899438]
[2019-04-28 02:52:53,297] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:52:53,302] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9651950e-14 9.9998331e-01 1.9201936e-23 1.6728969e-05 8.6420601e-16], sampled 0.01671372401450466
[2019-04-28 02:53:05,930] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:53:05,931] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.6, 52.0, 1.0, 2.0, 0.8692157567255635, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005985851569115, 6.9112, 168.9123159737736, 2111947.960862636, 2044703.870480276, 425671.8674426244]
[2019-04-28 02:53:05,931] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:53:05,933] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [7.852431e-14 9.999726e-01 1.926932e-22 2.740719e-05 4.019028e-15], sampled 0.05761306654837539
[2019-04-28 02:53:05,934] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2111947.960862636 W.
[2019-04-28 02:53:07,146] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:53:07,147] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.63333333333333, 64.33333333333333, 1.0, 2.0, 0.5582860981568593, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780147.991097974, 780147.991097974, 193231.6953364487]
[2019-04-28 02:53:07,148] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:53:07,149] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.3926284e-14 9.9998510e-01 1.0785431e-23 1.4882371e-05 5.9014596e-16], sampled 0.31123777114915185
[2019-04-28 02:53:12,145] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:53:12,146] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [30.15612916, 76.48971603, 1.0, 2.0, 0.5863210312002938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 819339.0660543737, 819339.0660543737, 198224.7046815622]
[2019-04-28 02:53:12,148] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 02:53:12,151] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [1.9028771e-14 9.9998343e-01 1.8196820e-23 1.6541751e-05 8.3385216e-16], sampled 0.4271476084691246
[2019-04-28 02:53:16,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14206453]
[2019-04-28 02:53:16,182] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [29.5, 79.0, 1.0, 2.0, 0.576154017769004, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805126.0529546161, 805126.0529546161, 196387.1308349516]
[2019-04-28 02:53:16,183] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:53:16,184] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5304478e-14 9.9998462e-01 1.2614300e-23 1.5406575e-05 6.5547581e-16], sampled 0.027548007999309454
[2019-04-28 02:53:43,534] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:53:43,988] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7999.0502 3007584666.7691 1766.0000
[2019-04-28 02:53:44,027] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:53:44,091] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:53:44,200] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.9164 3164002230.9074 1778.0000
[2019-04-28 02:53:45,219] A3C_AGENT_WORKER-Thread-22 INFO:Global step: 2400000, evaluation results [2400000.0, 7884.916440885027, 3164002230.907359, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 7999.050242421542, 3007584666.769063, 1766.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:53:47,067] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.25979920e-14 9.99880075e-01 1.04232804e-22 1.19867218e-04
 3.92551914e-15], sum to 1.0000
[2019-04-28 02:53:47,075] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.0877
[2019-04-28 02:53:47,082] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.5648009271982863, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 789255.1837983464, 789255.1837983464, 194373.0434766914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2398800.0000, 
sim time next is 2399400.0000, 
raw observation next is [32.05, 67.0, 1.0, 2.0, 0.5658571838627734, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 790731.7506965585, 790731.7506965585, 194558.9448403707], 
processed observation next is [1.0, 0.782608695652174, 0.7180094786729857, 0.67, 1.0, 1.0, 0.47693636609972695, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2196477085268218, 0.2196477085268218, 0.2903864848363742], 
reward next is 0.7096, 
noisyNet noise sample is [array([-0.3517594], dtype=float32), 0.027368372]. 
=============================================
[2019-04-28 02:53:55,245] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [5.5890077e-14 9.9999893e-01 1.4041094e-24 1.1163976e-06 1.2518603e-16], sum to 1.0000
[2019-04-28 02:53:55,264] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.4574
[2019-04-28 02:53:55,275] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.33333333333334, 98.0, 1.0, 2.0, 0.4309244411845149, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 623563.9284298284, 623563.9284298284, 176236.7602489321], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2683200.0000, 
sim time next is 2683800.0000, 
raw observation next is [23.5, 97.0, 1.0, 2.0, 0.4332689551429648, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 625832.8628156359, 625832.8628156359, 176427.988253219], 
processed observation next is [0.0, 0.043478260869565216, 0.31279620853080575, 0.97, 1.0, 1.0, 0.3171915122204395, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1738424618932322, 0.1738424618932322, 0.2633253556018194], 
reward next is 0.7367, 
noisyNet noise sample is [array([0.864796], dtype=float32), 0.23382892]. 
=============================================
[2019-04-28 02:53:57,315] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [5.3676420e-16 9.9999976e-01 3.0334498e-27 2.9628768e-07 5.5801174e-18], sum to 1.0000
[2019-04-28 02:53:57,324] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7457
[2019-04-28 02:53:57,337] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 94.0, 1.0, 2.0, 0.3484596300850965, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 536791.4933470496, 536791.4933470496, 169553.3087885754], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2775600.0000, 
sim time next is 2776200.0000, 
raw observation next is [21.83333333333334, 95.0, 1.0, 2.0, 0.3493758065039241, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 538680.3763129507, 538680.37631295, 169722.2388710988], 
processed observation next is [1.0, 0.13043478260869565, 0.23380726698262277, 0.95, 1.0, 1.0, 0.21611542952280008, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.14963343786470854, 0.14963343786470834, 0.2533167744344758], 
reward next is 0.7467, 
noisyNet noise sample is [array([-0.7290484], dtype=float32), 0.33113867]. 
=============================================
[2019-04-28 02:54:03,875] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [6.3847595e-14 9.9999332e-01 4.2953090e-24 6.6897942e-06 2.8092832e-15], sum to 1.0000
[2019-04-28 02:54:03,881] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.1798
[2019-04-28 02:54:03,885] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [2.5898905e-15 9.9996686e-01 5.8501091e-25 3.3175544e-05 3.9005876e-15], sum to 1.0000
[2019-04-28 02:54:03,895] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 100.0, 1.0, 2.0, 0.3859984006418039, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 581386.5162420993, 581386.5162421, 172999.6175667803], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2728800.0000, 
sim time next is 2729400.0000, 
raw observation next is [22.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3877364187725297, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 583346.5155750271, 583346.5155750271, 173156.6542452343], 
processed observation next is [0.0, 0.6086956521739131, 0.24960505529225935, 0.9900000000000001, 1.0, 1.0, 0.2623330346656984, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16204069877084085, 0.16204069877084085, 0.25844276753020046], 
reward next is 0.7416, 
noisyNet noise sample is [array([0.04695128], dtype=float32), 1.5867243]. 
=============================================
[2019-04-28 02:54:03,904] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.6319
[2019-04-28 02:54:03,910] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [24.16666666666666, 88.0, 1.0, 2.0, 0.5453369986837747, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 802816.9274189383, 802816.9274189383, 196120.2684870559], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2812200.0000, 
sim time next is 2812800.0000, 
raw observation next is [24.33333333333333, 87.0, 1.0, 2.0, 0.6523596692798883, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 959223.0942226064, 959223.0942226064, 217033.8834302443], 
processed observation next is [1.0, 0.5652173913043478, 0.35229067930489716, 0.87, 1.0, 1.0, 0.5811562280480582, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.26645085950627956, 0.26645085950627956, 0.3239311692988721], 
reward next is 0.6761, 
noisyNet noise sample is [array([-0.69717115], dtype=float32), -0.20534267]. 
=============================================
[2019-04-28 02:54:15,521] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [2.1103435e-19 9.9999988e-01 5.2543240e-27 1.4386633e-07 2.9597821e-18], sum to 1.0000
[2019-04-28 02:54:15,530] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.3973
[2019-04-28 02:54:15,540] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [23.0, 100.0, 1.0, 2.0, 0.4296902682384305, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 623687.5206354492, 623687.5206354499, 176302.0155777762], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3091200.0000, 
sim time next is 3091800.0000, 
raw observation next is [23.0, 100.0, 1.0, 2.0, 0.4293043876365769, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 623128.9560877865, 623128.9560877858, 176247.5925242753], 
processed observation next is [1.0, 0.782608695652174, 0.28909952606635075, 1.0, 1.0, 1.0, 0.3124149248633457, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1730913766910518, 0.1730913766910516, 0.263056108245187], 
reward next is 0.7369, 
noisyNet noise sample is [array([0.6518858], dtype=float32), -0.5019858]. 
=============================================
[2019-04-28 02:54:17,470] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [8.9902764e-16 1.0000000e+00 5.4511034e-26 4.1074934e-08 2.0753536e-18], sum to 1.0000
[2019-04-28 02:54:17,477] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.9810
[2019-04-28 02:54:17,480] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.0, 98.0, 1.0, 2.0, 0.3852086289591706, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 583957.1103113506, 583957.1103113499, 173337.6641927693], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3122400.0000, 
sim time next is 3123000.0000, 
raw observation next is [22.0, 97.0, 1.0, 2.0, 0.3820660893101436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581410.119889785, 581410.119889785, 173170.2989534034], 
processed observation next is [1.0, 0.13043478260869565, 0.2417061611374408, 0.97, 1.0, 1.0, 0.25550131242185975, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16150281108049583, 0.16150281108049583, 0.25846313276627375], 
reward next is 0.7415, 
noisyNet noise sample is [array([1.3665694], dtype=float32), -0.44625074]. 
=============================================
[2019-04-28 02:54:17,494] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[73.6802 ]
 [73.69496]
 [73.85876]
 [73.9437 ]
 [73.95017]], R is [[73.68452454]
 [73.68896484]
 [73.68465424]
 [73.69087982]
 [73.6965332 ]].
[2019-04-28 02:54:18,509] A3C_AGENT_WORKER-Thread-14 DEBUG:Policy network output: [6.8832684e-15 9.9999738e-01 7.5869502e-22 2.5708359e-06 2.0290642e-14], sum to 1.0000
[2019-04-28 02:54:18,515] A3C_AGENT_WORKER-Thread-14 DEBUG:Softmax action selection sampled number: 0.2437
[2019-04-28 02:54:18,519] A3C_AGENT_WORKER-Thread-14 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 81.5, 1.0, 2.0, 0.6146705821470689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874129.0050966056, 874129.0050966056, 205474.3056008784], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3155400.0000, 
sim time next is 3156000.0000, 
raw observation next is [26.0, 82.33333333333334, 1.0, 2.0, 0.5879840693374689, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831913.991455402, 831913.991455402, 199849.2379311827], 
processed observation next is [1.0, 0.5217391304347826, 0.4312796208530806, 0.8233333333333335, 1.0, 1.0, 0.5035952642620107, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23108721984872277, 0.23108721984872277, 0.29828244467340703], 
reward next is 0.7017, 
noisyNet noise sample is [array([0.06413388], dtype=float32), 1.4029087]. 
=============================================
[2019-04-28 02:54:18,590] A3C_AGENT_WORKER-Thread-14 DEBUG:Value prediction is [[66.93996 ]
 [66.24168 ]
 [65.073555]
 [65.612526]
 [65.54627 ]], R is [[67.46437836]
 [67.48305511]
 [67.47935486]
 [67.39854431]
 [67.33786011]].
[2019-04-28 02:54:19,064] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.9249011e-16 9.9999917e-01 7.9667847e-25 8.3435594e-07 3.6801090e-17], sum to 1.0000
[2019-04-28 02:54:19,085] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.8212
[2019-04-28 02:54:19,090] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [21.16666666666667, 99.00000000000001, 1.0, 2.0, 0.3597844510134063, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 558262.3869942317, 558262.3869942317, 171442.0311372862], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 2967000.0000, 
sim time next is 2967600.0000, 
raw observation next is [21.33333333333334, 98.0, 1.0, 2.0, 0.4793927355192894, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 742797.372358481, 742797.3723584805, 189103.1106037358], 
processed observation next is [1.0, 0.34782608695652173, 0.2101105845181678, 0.98, 1.0, 1.0, 0.37276233195095104, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2063326034329114, 0.20633260343291124, 0.28224344866229223], 
reward next is 0.7178, 
noisyNet noise sample is [array([-1.1975427], dtype=float32), -0.3477924]. 
=============================================
[2019-04-28 02:54:36,240] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4105642e-16 1.0000000e+00 2.1377898e-24 4.5214130e-09 2.6801012e-16], sum to 1.0000
[2019-04-28 02:54:36,241] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.7014
[2019-04-28 02:54:36,254] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333333, 77.33333333333333, 1.0, 2.0, 0.4831848330666783, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 675168.2657133715, 675168.2657133715, 181003.4236145625], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3310800.0000, 
sim time next is 3311400.0000, 
raw observation next is [27.66666666666667, 75.66666666666667, 1.0, 2.0, 0.4854969919839591, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 678400.1444755488, 678400.1444755488, 181354.9769944343], 
processed observation next is [0.0, 0.30434782608695654, 0.5102685624012641, 0.7566666666666667, 1.0, 1.0, 0.38011685781199894, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.1884444845765413, 0.1884444845765413, 0.2706790701409467], 
reward next is 0.7293, 
noisyNet noise sample is [array([-0.29953748], dtype=float32), -0.43166676]. 
=============================================
[2019-04-28 02:54:42,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [2.8455962e-15 1.0000000e+00 1.5012740e-24 3.1850589e-10 5.5838877e-18], sum to 1.0000
[2019-04-28 02:54:42,938] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0728
[2019-04-28 02:54:43,007] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [22.83333333333334, 95.0, 1.0, 2.0, 0.3907757072072625, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 584246.9615002999, 584246.9615002999, 173127.884081503], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3109800.0000, 
sim time next is 3110400.0000, 
raw observation next is [23.0, 94.0, 1.0, 2.0, 0.3896586440802436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 581716.8524090784, 581716.8524090784, 172871.7893211565], 
processed observation next is [1.0, 0.0, 0.28909952606635075, 0.94, 1.0, 1.0, 0.26464896877137783, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.16158801455807734, 0.16158801455807734, 0.25801759600172613], 
reward next is 0.7420, 
noisyNet noise sample is [array([0.19176728], dtype=float32), 0.4405002]. 
=============================================
[2019-04-28 02:54:48,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.6258489e-16 1.0000000e+00 4.2855460e-26 1.3111664e-09 4.3861049e-18], sum to 1.0000
[2019-04-28 02:54:48,212] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2283
[2019-04-28 02:54:48,328] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [25.0, 83.0, 1.0, 2.0, 0.4242200039282436, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 621057.2932498611, 621057.2932498611, 176193.8867669823], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3298200.0000, 
sim time next is 3298800.0000, 
raw observation next is [25.0, 83.0, 1.0, 2.0, 0.4232603203675319, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 619726.5050434443, 619726.5050434449, 176067.713518332], 
processed observation next is [0.0, 0.17391304347826086, 0.38388625592417064, 0.83, 1.0, 1.0, 0.3051329161054601, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.17214625140095677, 0.17214625140095693, 0.2627876321169134], 
reward next is 0.7372, 
noisyNet noise sample is [array([-0.33392158], dtype=float32), -1.4425701]. 
=============================================
[2019-04-28 02:55:17,227] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.2191139e-08 9.3711579e-01 8.2789271e-14 6.2884018e-02 2.6650849e-07], sum to 1.0000
[2019-04-28 02:55:17,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4245
[2019-04-28 02:55:17,228] A3C_AGENT_WORKER-Thread-18 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 1929064.018490267 W.
[2019-04-28 02:55:17,239] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [30.0, 66.5, 1.0, 2.0, 0.7385419670979674, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 6.977709114706963, 6.9112, 168.9125601458725, 1929064.018490267, 1881880.274169875, 394218.6415091728], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3749400.0000, 
sim time next is 3750000.0000, 
raw observation next is [30.33333333333333, 65.33333333333334, 1.0, 2.0, 0.6814584680365471, 1.0, 1.0, 0.6814584680365471, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1905530.286451344, 1905530.286451344, 366282.9609977244], 
processed observation next is [1.0, 0.391304347826087, 0.6366508688783569, 0.6533333333333334, 1.0, 1.0, 0.6162150217307796, 1.0, 0.5, 0.6162150217307796, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.5293139684587066, 0.5293139684587066, 0.5466909865637677], 
reward next is 0.0000, 
noisyNet noise sample is [array([-0.93677986], dtype=float32), -1.8943932]. 
=============================================
[2019-04-28 02:55:17,371] A3C_AGENT_WORKER-Thread-18 DEBUG:Value prediction is [[45.360947]
 [44.962353]
 [45.01376 ]
 [44.640892]
 [44.0937  ]], R is [[45.07391357]
 [44.62317657]
 [44.17694473]
 [43.73517609]
 [43.73271179]].
[2019-04-28 02:55:18,356] A3C_AGENT_WORKER-Thread-9 INFO:Evaluating...
[2019-04-28 02:55:18,393] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:55:18,453] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:55:18,455] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run98
[2019-04-28 02:55:18,452] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:55:18,492] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:55:18,501] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:55:18,501] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:55:18,504] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run98
[2019-04-28 02:55:18,555] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run98
[2019-04-28 02:55:18,599] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:55:18,600] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:55:18,603] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run98
[2019-04-28 02:55:18,651] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:55:18,652] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:55:18,654] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run98
[2019-04-28 02:56:14,180] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14647318]
[2019-04-28 02:56:14,181] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [31.16117712, 72.16938039, 1.0, 2.0, 0.5992827853155971, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 837459.2727743108, 837459.2727743114, 200611.6313811164]
[2019-04-28 02:56:14,182] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:56:14,183] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [2.60851713e-12 9.95170891e-01 5.86992041e-21 4.82912129e-03
 1.37508034e-11], sampled 0.8805720999971063
[2019-04-28 02:56:25,963] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14647318]
[2019-04-28 02:56:25,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [30.66666666666667, 76.33333333333334, 1.0, 2.0, 0.5952278981837773, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 831790.6091750228, 831790.6091750228, 199860.3863433832]
[2019-04-28 02:56:25,964] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:56:25,966] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.9980963e-11 9.9165195e-01 7.1729530e-19 8.3479909e-03 1.8319961e-10], sampled 0.18513137570815053
[2019-04-28 02:57:00,059] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14647318]
[2019-04-28 02:57:00,061] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [30.3, 78.0, 1.0, 2.0, 0.6074718615358187, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 848907.5550145715, 848907.5550145721, 202145.319267265]
[2019-04-28 02:57:00,062] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:57:00,066] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.5739593e-12 9.9485427e-01 1.0215178e-20 5.1457249e-03 1.8535383e-11], sampled 0.04131757394696289
[2019-04-28 02:57:02,292] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.14647318]
[2019-04-28 02:57:02,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [22.76666666666667, 82.66666666666667, 1.0, 2.0, 0.557431832983214, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 875193.2792397122, 875193.2792397122, 204298.9784368594]
[2019-04-28 02:57:02,293] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:57:02,295] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.3691368e-12 9.9526393e-01 4.9563838e-21 4.7360025e-03 1.2554157e-11], sampled 0.2373127767569464
[2019-04-28 02:57:09,083] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8453.8594 2844562011.8168 1120.0000
[2019-04-28 02:57:09,127] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8619.7678 2781440425.9755 912.0000
[2019-04-28 02:57:09,212] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 7946.7412 3009629681.0431 1728.0000
[2019-04-28 02:57:09,268] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8208.0692 2930070120.7250 1311.0000
[2019-04-28 02:57:09,533] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7845.1461 3165917482.0657 1748.0000
[2019-04-28 02:57:10,547] A3C_AGENT_WORKER-Thread-9 INFO:Global step: 2425000, evaluation results [2425000.0, 7845.146115202247, 3165917482.0656633, 1748.0, 8208.069195847484, 2930070120.7250037, 1311.0, 8619.767769277723, 2781440425.975503, 912.0, 7946.741228100458, 3009629681.043097, 1728.0, 8453.85941388751, 2844562011.8168306, 1120.0]
[2019-04-28 02:57:10,789] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [7.3431408e-09 9.9089843e-01 5.9757226e-15 9.1016097e-03 1.2826404e-08], sum to 1.0000
[2019-04-28 02:57:10,799] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.9543
[2019-04-28 02:57:10,801] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.9, 89.66666666666667, 1.0, 2.0, 0.5371120742056108, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 750548.9944536419, 750548.9944536426, 189614.9975068734], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3370800.0000, 
sim time next is 3371400.0000, 
raw observation next is [26.85, 90.0, 1.0, 2.0, 0.5371561813274274, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 750610.650584857, 750610.6505848564, 189622.3825395429], 
processed observation next is [1.0, 0.0, 0.4715639810426541, 0.9, 1.0, 1.0, 0.44235684497280403, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.20850295849579362, 0.20850295849579345, 0.2830184814023028], 
reward next is 0.7170, 
noisyNet noise sample is [array([1.0578103], dtype=float32), 0.10285058]. 
=============================================
[2019-04-28 02:57:15,521] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [8.4891899e-11 9.9992704e-01 2.8439487e-20 7.2932606e-05 1.8538861e-08], sum to 1.0000
[2019-04-28 02:57:15,523] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0337
[2019-04-28 02:57:15,525] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.0, 79.0, 1.0, 2.0, 0.8621373551190564, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.9129565104278, 1204990.083021733, 1204990.083021734, 259731.3493923704], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3639000.0000, 
sim time next is 3639600.0000, 
raw observation next is [27.0, 79.0, 1.0, 2.0, 0.8066290140806415, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1127366.130540363, 1127366.130540363, 245540.9838231572], 
processed observation next is [1.0, 0.13043478260869565, 0.4786729857819906, 0.79, 1.0, 1.0, 0.7670229085308933, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.3131572584834342, 0.3131572584834342, 0.3664790803330705], 
reward next is 0.6335, 
noisyNet noise sample is [array([0.2647734], dtype=float32), 0.8191457]. 
=============================================
[2019-04-28 02:57:17,323] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [1.6043192e-07 9.4790226e-01 1.6833766e-13 5.2097149e-02 4.7921304e-07], sum to 1.0000
[2019-04-28 02:57:17,343] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.4311
[2019-04-28 02:57:17,353] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 4 for the demand 2237559.026595407 W.
[2019-04-28 02:57:17,358] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 4, 
current raw observation is [31.0, 69.33333333333333, 1.0, 2.0, 0.80007650949797, 1.0, 2.0, 0.80007650949797, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2237559.026595407, 2237559.026595407, 419910.3104982687], 
current ob forecast is [], 
actual action is [0, 0, 0, 0, 1], 
sim time this is 3581400.0000, 
sim time next is 3582000.0000, 
raw observation next is [31.0, 70.0, 1.0, 2.0, 0.5234349144643313, 1.0, 2.0, 0.5234349144643313, 1.0, 1.0, 0.9090333438531679, 6.9112, 6.9112, 170.5573041426782, 2195781.837107954, 2195781.837107954, 431730.1661884889], 
processed observation next is [1.0, 0.4782608695652174, 0.6682464454976303, 0.7, 1.0, 1.0, 0.42582519814979675, 1.0, 1.0, 0.42582519814979675, 1.0, 0.5, 0.8890650534794731, 0.0, 0.0, 0.8375144448122397, 0.609939399196654, 0.609939399196654, 0.6443733823708789], 
reward next is 0.3556, 
noisyNet noise sample is [array([-0.53214496], dtype=float32), -1.0988901]. 
=============================================
[2019-04-28 02:57:17,387] A3C_AGENT_WORKER-Thread-13 DEBUG:Value prediction is [[40.1477  ]
 [39.75197 ]
 [39.581768]
 [39.136917]
 [39.099026]], R is [[40.89173508]
 [40.85608673]
 [40.82114029]
 [40.41292953]
 [40.00880051]].
[2019-04-28 02:57:17,931] A3C_AGENT_WORKER-Thread-13 DEBUG:Policy network output: [4.3357056e-07 7.3497128e-01 2.8363612e-13 2.6502147e-01 6.7751639e-06], sum to 1.0000
[2019-04-28 02:57:17,937] A3C_AGENT_WORKER-Thread-13 DEBUG:Softmax action selection sampled number: 0.5774
[2019-04-28 02:57:17,952] A3C_AGENT_WORKER-Thread-13 DEBUG:Action function: raw action 1 has been changed to 2 for the demand 1946025.71842409 W.
[2019-04-28 02:57:17,955] A3C_AGENT_WORKER-Thread-13 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 2, 
current raw observation is [32.16666666666667, 66.33333333333334, 1.0, 2.0, 0.7506623463013739, 0.0, 1.0, 0.0, 1.0, 2.0, 1.03, 7.005978403657625, 6.9112, 168.9123932093295, 1946025.71842409, 1878786.881081339, 396010.8798811268], 
current ob forecast is [], 
actual action is [0, 0, 1, 0, 0], 
sim time this is 3589800.0000, 
sim time next is 3590400.0000, 
raw observation next is [32.33333333333334, 65.66666666666667, 1.0, 2.0, 0.9983185822625532, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.005993270615242, 6.9112, 168.9123159563189, 2292648.583011871, 2225399.229329754, 462779.1838311087], 
processed observation next is [1.0, 0.5652173913043478, 0.7314375987361774, 0.6566666666666667, 1.0, 1.0, 0.997974195497052, 0.0, 1.0, -0.20481927710843376, 1.0, 1.0, 1.0365853658536586, 0.009479327061524235, 0.0, 0.8294367997383654, 0.6368468286144087, 0.6181664525915984, 0.6907151997479234], 
reward next is 0.0000, 
noisyNet noise sample is [array([-1.7558762], dtype=float32), -0.6812412]. 
=============================================
[2019-04-28 02:57:19,283] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [5.2315221e-13 1.0000000e+00 9.9927827e-18 4.8616791e-09 1.0263792e-11], sum to 1.0000
[2019-04-28 02:57:19,290] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.4082
[2019-04-28 02:57:19,293] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.83333333333334, 70.66666666666667, 1.0, 2.0, 0.766064290039546, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1070643.213574274, 1070643.213574274, 235748.9150830416], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3743400.0000, 
sim time next is 3744000.0000, 
raw observation next is [29.0, 70.0, 1.0, 2.0, 0.8019746525445421, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 1120857.637960667, 1120857.637960668, 244393.6951459325], 
processed observation next is [1.0, 0.34782608695652173, 0.5734597156398105, 0.7, 1.0, 1.0, 0.7614152440295688, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.31134934387796304, 0.3113493438779633, 0.36476670917303355], 
reward next is 0.6352, 
noisyNet noise sample is [array([-0.4879739], dtype=float32), -0.71595067]. 
=============================================
[2019-04-28 02:57:19,324] A3C_AGENT_WORKER-Thread-11 DEBUG:Value prediction is [[42.885994]
 [42.979626]
 [43.098705]
 [43.165142]
 [43.258545]], R is [[43.05340195]
 [43.27100372]
 [43.48637009]
 [43.69194031]
 [43.9014473 ]].
[2019-04-28 02:57:23,036] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.2296870e-10 9.6253014e-01 8.0249523e-17 3.7469864e-02 2.2232698e-09], sum to 1.0000
[2019-04-28 02:57:23,038] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.5149
[2019-04-28 02:57:23,041] A3C_AGENT_WORKER-Thread-3 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2718960.018455696 W.
[2019-04-28 02:57:23,048] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.0, 63.0, 1.0, 2.0, 0.6548497608910332, 1.0, 2.0, 0.6480149199597791, 1.0, 2.0, 1.03, 7.005094172371729, 6.9112, 170.5573041426782, 2718960.018455696, 2651699.80341509, 507446.5904389943], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 3505800.0000, 
sim time next is 3506400.0000, 
raw observation next is [33.0, 63.0, 1.0, 2.0, 0.9911486680529739, 1.0, 2.0, 0.9911486680529739, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2772519.812810805, 2772519.812810805, 523660.8634520231], 
processed observation next is [1.0, 0.6086956521739131, 0.7630331753554502, 0.63, 1.0, 1.0, 0.9893357446421371, 1.0, 1.0, 0.9893357446421371, 0.0, 0.5, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7701443924474459, 0.7701443924474459, 0.7815833782866017], 
reward next is 0.2184, 
noisyNet noise sample is [array([-0.80348], dtype=float32), 0.004690814]. 
=============================================
[2019-04-28 02:57:26,797] A3C_AGENT_WORKER-Thread-2 DEBUG:Policy network output: [2.2319618e-13 9.9933892e-01 2.2626015e-23 6.6102709e-04 1.7679136e-11], sum to 1.0000
[2019-04-28 02:57:26,804] A3C_AGENT_WORKER-Thread-2 DEBUG:Softmax action selection sampled number: 0.7490
[2019-04-28 02:57:26,813] A3C_AGENT_WORKER-Thread-2 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [34.5, 58.0, 1.0, 2.0, 0.5935372414575378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829427.1086153643, 829427.1086153643, 199548.8762243844], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3933000.0000, 
sim time next is 3933600.0000, 
raw observation next is [34.66666666666666, 57.33333333333333, 1.0, 2.0, 0.5933124401680284, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 829112.8418008387, 829112.8418008387, 199507.4815980341], 
processed observation next is [0.0, 0.5217391304347826, 0.842022116903633, 0.5733333333333333, 1.0, 1.0, 0.5100149881542511, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.23030912272245518, 0.23030912272245518, 0.29777236059408074], 
reward next is 0.7022, 
noisyNet noise sample is [array([-0.06868047], dtype=float32), 0.6455386]. 
=============================================
[2019-04-28 02:57:28,628] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [4.1201721e-12 9.9999225e-01 2.6776369e-17 7.7226805e-06 7.7456735e-11], sum to 1.0000
[2019-04-28 02:57:28,636] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.2553
[2019-04-28 02:57:28,644] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.00000000000001, 1.0, 2.0, 0.5126395231903313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738920.2227204571, 738920.2227204578, 188467.3204370738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.4853506647827253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699630.447884891, 699630.447884891, 184058.717490553], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 1.0, 1.0, 0.3799405599791872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19434179107913638, 0.19434179107913638, 0.2747145037172433], 
reward next is 0.7253, 
noisyNet noise sample is [array([0.00521194], dtype=float32), 0.96048373]. 
=============================================
[2019-04-28 02:57:31,368] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [7.4372047e-14 9.9722868e-01 5.5551863e-21 2.7712567e-03 3.0378219e-12], sum to 1.0000
[2019-04-28 02:57:31,375] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8826
[2019-04-28 02:57:31,377] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.0, 84.0, 1.0, 2.0, 0.5829586620148277, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 814638.6088747102, 814638.6088747096, 197613.973473719], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3885600.0000, 
sim time next is 3886200.0000, 
raw observation next is [29.0, 84.0, 1.0, 2.0, 0.5821198693267234, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 813466.0136017958, 813466.0136017958, 197462.0988921315], 
processed observation next is [0.0, 1.0, 0.5734597156398105, 0.84, 1.0, 1.0, 0.49652996304424507, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22596278155605437, 0.22596278155605437, 0.2947195505852709], 
reward next is 0.7053, 
noisyNet noise sample is [array([-1.1810112], dtype=float32), -0.61341393]. 
=============================================
[2019-04-28 02:57:39,954] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [1.7783264e-14 1.0000000e+00 3.8263820e-21 1.0475145e-09 3.5737413e-15], sum to 1.0000
[2019-04-28 02:57:39,962] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.7869
[2019-04-28 02:57:39,967] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.0, 79.00000000000001, 1.0, 2.0, 0.5126395231903313, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 738920.2227204571, 738920.2227204578, 188467.3204370738], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3733800.0000, 
sim time next is 3734400.0000, 
raw observation next is [26.0, 79.0, 1.0, 2.0, 0.4853506647827253, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 699630.447884891, 699630.447884891, 184058.717490553], 
processed observation next is [1.0, 0.21739130434782608, 0.4312796208530806, 0.79, 1.0, 1.0, 0.3799405599791872, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19434179107913638, 0.19434179107913638, 0.2747145037172433], 
reward next is 0.7253, 
noisyNet noise sample is [array([-0.3290552], dtype=float32), -0.3022001]. 
=============================================
[2019-04-28 02:57:40,686] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [7.1903536e-14 9.9995005e-01 1.0627532e-23 4.9914768e-05 5.8834708e-13], sum to 1.0000
[2019-04-28 02:57:40,701] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.8184
[2019-04-28 02:57:40,706] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.16666666666667, 83.16666666666667, 1.0, 2.0, 0.5860418472139938, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 818948.7772540991, 818948.7772540991, 198174.1133729149], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3913800.0000, 
sim time next is 3914400.0000, 
raw observation next is [29.33333333333334, 82.33333333333334, 1.0, 2.0, 0.5868537278236341, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 820083.7564639845, 820083.7564639839, 198322.1489552396], 
processed observation next is [0.0, 0.30434782608695654, 0.5892575039494474, 0.8233333333333335, 1.0, 1.0, 0.5022334070164266, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2278010434622179, 0.22780104346221774, 0.29600320739588], 
reward next is 0.7040, 
noisyNet noise sample is [array([-0.23315749], dtype=float32), 0.61517054]. 
=============================================
[2019-04-28 02:57:43,779] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [7.8866867e-17 9.9999917e-01 2.8870830e-26 8.0020970e-07 2.8232200e-17], sum to 1.0000
[2019-04-28 02:57:43,798] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.9074
[2019-04-28 02:57:43,803] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.83333333333334, 84.83333333333333, 1.0, 2.0, 0.5423248753624772, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 757835.8504527296, 757835.8504527302, 190493.3553043394], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 3826200.0000, 
sim time next is 3826800.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5434979214518272, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 759475.6317513906, 759475.6317513912, 190692.0933461548], 
processed observation next is [0.0, 0.30434782608695654, 0.5260663507109005, 0.84, 1.0, 1.0, 0.44999749572509296, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21096545326427518, 0.21096545326427535, 0.28461506469575343], 
reward next is 0.7154, 
noisyNet noise sample is [array([-0.46144718], dtype=float32), 0.5409646]. 
=============================================
[2019-04-28 02:57:53,085] A3C_AGENT_WORKER-Thread-11 DEBUG:Policy network output: [7.2435031e-09 5.3505999e-01 1.3026620e-14 4.6494001e-01 1.3073105e-08], sum to 1.0000
[2019-04-28 02:57:53,093] A3C_AGENT_WORKER-Thread-11 DEBUG:Softmax action selection sampled number: 0.1096
[2019-04-28 02:57:53,103] A3C_AGENT_WORKER-Thread-11 DEBUG:Action function: raw action 1 has been changed to 3 for the demand 2867824.521592008 W.
[2019-04-28 02:57:53,107] A3C_AGENT_WORKER-Thread-11 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [33.33333333333334, 73.66666666666667, 1.0, 2.0, 1.025180104052181, 1.0, 2.0, 1.025180104052181, 0.0, 1.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2867824.521592008, 2867824.521592008, 544404.0638533755], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4267200.0000, 
sim time next is 4267800.0000, 
raw observation next is [33.5, 73.0, 1.0, 2.0, 1.019343298056913, 1.0, 2.0, 1.019343298056913, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2851478.086005942, 2851478.086005942, 540801.0350225009], 
processed observation next is [1.0, 0.391304347826087, 0.7867298578199052, 0.73, 1.0, 1.0, 1.023305178381823, 1.0, 1.0, 1.023305178381823, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7920772461127616, 0.7920772461127616, 0.8071657239141804], 
reward next is 0.1928, 
noisyNet noise sample is [array([-0.98368], dtype=float32), -0.46305177]. 
=============================================
[2019-04-28 02:57:54,951] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [6.7103159e-13 9.9381316e-01 1.5080499e-21 6.1868420e-03 2.1138962e-13], sum to 1.0000
[2019-04-28 02:57:54,955] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.7365
[2019-04-28 02:57:54,977] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.0, 75.0, 1.0, 2.0, 0.6357747105557006, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 888475.7281487472, 888475.7281487467, 207600.5647921237], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4443600.0000, 
sim time next is 4444200.0000, 
raw observation next is [32.0, 75.0, 1.0, 2.0, 0.632968016525192, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 884551.8256887555, 884551.8256887548, 207049.2942383701], 
processed observation next is [0.0, 0.43478260869565216, 0.7156398104265403, 0.75, 1.0, 1.0, 0.5577927909942072, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24570884046909874, 0.24570884046909855, 0.3090287973707016], 
reward next is 0.6910, 
noisyNet noise sample is [array([0.253609], dtype=float32), 0.55095154]. 
=============================================
[2019-04-28 02:58:00,238] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [3.3015741e-15 9.9998713e-01 1.3335949e-23 1.2923274e-05 8.2849954e-17], sum to 1.0000
[2019-04-28 02:58:00,245] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0672
[2019-04-28 02:58:00,254] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.33333333333334, 80.66666666666667, 1.0, 2.0, 0.5108780687742124, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 713877.8346131871, 713877.8346131865, 185317.563013489], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4522800.0000, 
sim time next is 4523400.0000, 
raw observation next is [27.5, 79.0, 1.0, 2.0, 0.5084769099822527, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 710521.4421212187, 710521.4421212187, 184934.4049114291], 
processed observation next is [0.0, 0.34782608695652173, 0.5023696682464456, 0.79, 1.0, 1.0, 0.4078035060027141, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19736706725589406, 0.19736706725589406, 0.2760214998678046], 
reward next is 0.7240, 
noisyNet noise sample is [array([-0.36663485], dtype=float32), 0.1060275]. 
=============================================
[2019-04-28 02:58:06,325] A3C_AGENT_WORKER-Thread-15 DEBUG:Policy network output: [2.9621806e-12 9.9999046e-01 2.2210196e-18 9.5188652e-06 2.9853819e-14], sum to 1.0000
[2019-04-28 02:58:06,335] A3C_AGENT_WORKER-Thread-15 DEBUG:Softmax action selection sampled number: 0.9297
[2019-04-28 02:58:06,338] A3C_AGENT_WORKER-Thread-15 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.16666666666667, 88.16666666666667, 1.0, 2.0, 0.5721029547719283, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 799462.9078126133, 799462.9078126133, 195663.9561112169], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4151400.0000, 
sim time next is 4152000.0000, 
raw observation next is [28.33333333333334, 87.33333333333334, 1.0, 2.0, 0.5729372617275736, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 800629.2168491923, 800629.2168491923, 195812.6602071171], 
processed observation next is [1.0, 0.043478260869565216, 0.5418641390205374, 0.8733333333333334, 1.0, 1.0, 0.485466580394667, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2223970046803312, 0.2223970046803312, 0.2922577018016673], 
reward next is 0.7077, 
noisyNet noise sample is [array([-0.5254766], dtype=float32), 0.39010814]. 
=============================================
[2019-04-28 02:58:06,359] A3C_AGENT_WORKER-Thread-15 DEBUG:Value prediction is [[51.90291 ]
 [52.277912]
 [52.310276]
 [52.478233]
 [51.82793 ]], R is [[51.79228973]
 [51.98233414]
 [52.17053223]
 [52.35641479]
 [52.53999329]].
[2019-04-28 02:58:10,903] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 02:58:10,904] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 02:58:10,905] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:58:10,907] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 02:58:10,908] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:58:10,909] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 02:58:10,909] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:58:10,910] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 02:58:10,911] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:58:10,911] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 02:58:10,912] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 02:58:10,919] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run99
[2019-04-28 02:58:10,983] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run99
[2019-04-28 02:58:11,016] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run99
[2019-04-28 02:58:11,053] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run99
[2019-04-28 02:58:11,085] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run99
[2019-04-28 02:58:16,606] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:58:16,606] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [22.24417301, 84.32769902, 1.0, 2.0, 0.3206718545718471, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 505155.2580692103, 505155.2580692109, 167373.3839649543]
[2019-04-28 02:58:16,608] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:58:16,611] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [3.8809581e-16 9.9999583e-01 6.0810073e-25 4.1655126e-06 4.9046947e-18], sampled 0.4980641237665485
[2019-04-28 02:58:34,664] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:58:34,666] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.06666666666667, 87.0, 1.0, 2.0, 0.4928728000745627, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 688709.9329947385, 688709.9329947378, 182486.8250618642]
[2019-04-28 02:58:34,667] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:58:34,669] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [2.9762104e-16 9.9999881e-01 6.2512903e-25 1.1857838e-06 3.0541984e-18], sampled 0.20637394612535853
[2019-04-28 02:58:38,754] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:58:38,755] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [28.1, 83.0, 1.0, 2.0, 0.5417498416376717, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 757032.0214540289, 757032.0214540289, 190396.1012867672]
[2019-04-28 02:58:38,756] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 02:58:38,760] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [7.3310347e-16 9.9999833e-01 2.5698146e-24 1.6609316e-06 8.4763513e-18], sampled 0.9916094020249578
[2019-04-28 02:58:47,189] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:58:47,191] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [26.1, 90.33333333333334, 1.0, 2.0, 0.5223952225316251, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 729976.9124963433, 729976.9124963438, 187178.5314789752]
[2019-04-28 02:58:47,192] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 02:58:47,195] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [7.0605993e-16 9.9999785e-01 2.1968289e-24 2.1318131e-06 8.4721451e-18], sampled 0.9556393709896235
[2019-04-28 02:59:15,723] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:59:15,724] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [30.75, 72.5, 1.0, 2.0, 0.8694643847951419, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1215236.768973236, 1215236.768973236, 261679.1169083352]
[2019-04-28 02:59:15,725] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:59:15,727] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.3507671e-14 9.9999082e-01 9.3265301e-22 9.1352122e-06 6.7428545e-16], sampled 0.7039925487051318
[2019-04-28 02:59:16,685] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:59:16,685] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [36.58333333333334, 55.16666666666667, 1.0, 2.0, 1.04, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 7.17753350479221, 6.9112, 168.9113661276118, 2472796.303652641, 2283851.927864611, 475536.1483866071]
[2019-04-28 02:59:16,686] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 02:59:16,688] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.2555945e-09 9.7880667e-01 3.1607236e-15 2.1193337e-02 1.8567095e-10], sampled 0.4942377495135114
[2019-04-28 02:59:16,689] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 2472796.303652641 W.
[2019-04-28 02:59:17,962] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.1486955]
[2019-04-28 02:59:17,963] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [30.33157615, 67.932368265, 1.0, 2.0, 0.5468723011964015, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 764192.6342689104, 764192.6342689104, 191264.8321116404]
[2019-04-28 02:59:17,965] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 02:59:17,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [6.3931273e-16 9.9999559e-01 1.4060450e-24 4.3901464e-06 8.4973725e-18], sampled 0.9446811369202585
[2019-04-28 02:59:41,030] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8661.4676 2779124528.0117 933.0000
[2019-04-28 02:59:41,179] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8497.6119 2842427357.4275 1131.0000
[2019-04-28 02:59:41,271] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8255.1610 2927253227.7716 1338.0000
[2019-04-28 02:59:41,343] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8000.7306 3007394173.3379 1762.0000
[2019-04-28 02:59:41,709] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7884.8083 3164061769.0605 1778.0000
[2019-04-28 02:59:42,721] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2450000, evaluation results [2450000.0, 7884.808315653347, 3164061769.060528, 1778.0, 8255.161016606155, 2927253227.7715816, 1338.0, 8661.467580702742, 2779124528.011676, 933.0, 8000.730617815953, 3007394173.337876, 1762.0, 8497.61193292354, 2842427357.427519, 1131.0]
[2019-04-28 02:59:43,789] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.1921225e-15 9.9998486e-01 2.2895725e-23 1.5121258e-05 1.8319423e-16], sum to 1.0000
[2019-04-28 02:59:43,802] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1929
[2019-04-28 02:59:43,807] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.0, 84.0, 1.0, 2.0, 0.551201893325089, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 770244.9467074851, 770244.9467074844, 192006.7182166297], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 4479600.0000, 
sim time next is 4480200.0000, 
raw observation next is [28.0, 84.0, 1.0, 2.0, 0.5509894483882853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 769947.97032294, 769947.97032294, 191970.2304227675], 
processed observation next is [0.0, 0.8695652173913043, 0.5260663507109005, 0.84, 1.0, 1.0, 0.45902343179311483, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21387443620081667, 0.21387443620081667, 0.28652273197427985], 
reward next is 0.7135, 
noisyNet noise sample is [array([0.5166852], dtype=float32), -1.4489666]. 
=============================================
[2019-04-28 02:59:59,477] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.0311826e-13 8.7675371e-04 2.3908774e-19 9.9912328e-01 4.3131758e-14], sum to 1.0000
[2019-04-28 02:59:59,487] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3142
[2019-04-28 02:59:59,491] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.08333333333334, 65.83333333333333, 1.0, 2.0, 0.8271980781844092, 1.0, 2.0, 0.8271980781844092, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2313479.616929636, 2313479.616929636, 433326.9394127805], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4805400.0000, 
sim time next is 4806000.0000, 
raw observation next is [31.0, 66.0, 1.0, 2.0, 0.8247735857502984, 1.0, 2.0, 0.8247735857502984, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2306692.620560755, 2306692.620560755, 432109.3417878894], 
processed observation next is [1.0, 0.6521739130434783, 0.6682464454976303, 0.66, 1.0, 1.0, 0.7888838382533715, 1.0, 1.0, 0.7888838382533715, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.6407479501557652, 0.6407479501557652, 0.6449393161013275], 
reward next is 0.3551, 
noisyNet noise sample is [array([0.750125], dtype=float32), 0.44728604]. 
=============================================
[2019-04-28 02:59:59,512] A3C_AGENT_WORKER-Thread-10 DEBUG:Value prediction is [[58.928406]
 [58.719707]
 [58.576134]
 [58.352165]
 [58.154594]], R is [[58.9616127 ]
 [58.72524261]
 [58.49455643]
 [58.27639008]
 [58.06105042]].
[2019-04-28 03:00:01,766] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.9471566e-12 5.9856154e-04 2.1162258e-22 9.9940145e-01 9.8574423e-16], sum to 1.0000
[2019-04-28 03:00:01,775] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0387
[2019-04-28 03:00:01,780] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.66666666666667, 78.0, 1.0, 2.0, 0.266786584993405, 1.0, 2.0, 0.266786584993405, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 745599.5465043172, 745599.5465043179, 243783.4372842282], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5041200.0000, 
sim time next is 5041800.0000, 
raw observation next is [29.0, 75.0, 1.0, 2.0, 0.264943976531443, 1.0, 2.0, 0.264943976531443, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 740448.154613625, 740448.154613625, 243463.3338896162], 
processed observation next is [0.0, 0.34782608695652173, 0.5734597156398105, 0.75, 1.0, 1.0, 0.11439033317041328, 1.0, 1.0, 0.11439033317041328, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20568004294822917, 0.20568004294822917, 0.36337811028300926], 
reward next is 0.6366, 
noisyNet noise sample is [array([-0.84574187], dtype=float32), 0.520466]. 
=============================================
[2019-04-28 03:00:14,328] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.5173776e-15 9.0056092e-06 1.0889142e-22 9.9999094e-01 1.0561762e-16], sum to 1.0000
[2019-04-28 03:00:14,341] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.4485
[2019-04-28 03:00:14,345] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.41666666666667, 84.0, 1.0, 2.0, 0.2475913844628414, 1.0, 2.0, 0.2475913844628414, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 691936.6190459993, 691936.6190459993, 240549.4709564297], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5011800.0000, 
sim time next is 5012400.0000, 
raw observation next is [26.33333333333334, 84.0, 1.0, 2.0, 0.2459080922587706, 1.0, 2.0, 0.2459080922587706, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 687230.863065209, 687230.863065209, 240276.4614007644], 
processed observation next is [0.0, 0.0, 0.44707740916271754, 0.84, 1.0, 1.0, 0.0914555328418923, 1.0, 1.0, 0.0914555328418923, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.19089746196255805, 0.19089746196255805, 0.35862158418024537], 
reward next is 0.6414, 
noisyNet noise sample is [array([0.5545907], dtype=float32), -0.23214656]. 
=============================================
[2019-04-28 03:00:14,456] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [6.06802369e-13 2.13608742e-04 1.06487355e-20 9.99786437e-01
 6.56512240e-15], sum to 1.0000
[2019-04-28 03:00:14,462] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.7120
[2019-04-28 03:00:14,466] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.16666666666666, 84.0, 1.0, 2.0, 0.2426520754244077, 1.0, 2.0, 0.2426520754244077, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 678128.5087420675, 678128.5087420675, 239753.0633517793], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5013600.0000, 
sim time next is 5014200.0000, 
raw observation next is [26.08333333333334, 84.0, 1.0, 2.0, 0.2411636773014341, 1.0, 2.0, 0.2411636773014341, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 673967.6447115398, 673967.6447115398, 239515.8358587179], 
processed observation next is [0.0, 0.0, 0.43522906793049004, 0.84, 1.0, 1.0, 0.08573937024269167, 1.0, 1.0, 0.08573937024269167, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.1872132346420944, 0.1872132346420944, 0.3574863221771909], 
reward next is 0.6425, 
noisyNet noise sample is [array([0.36613417], dtype=float32), 1.9887089]. 
=============================================
[2019-04-28 03:00:16,145] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [1.4331639e-10 4.6955489e-04 4.2624610e-17 9.9953043e-01 1.1550331e-10], sum to 1.0000
[2019-04-28 03:00:16,151] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.3270
[2019-04-28 03:00:16,156] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [34.73333333333333, 58.5, 1.0, 2.0, 0.9367145533434215, 1.0, 2.0, 0.9367145533434215, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2620092.712923418, 2620092.712923418, 491925.77355222], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5307000.0000, 
sim time next is 5307600.0000, 
raw observation next is [35.06666666666666, 57.00000000000001, 1.0, 2.0, 0.9456550524379114, 1.0, 2.0, 0.9456550524379114, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2645126.73576595, 2645126.73576595, 497019.3736604199], 
processed observation next is [1.0, 0.43478260869565216, 0.8609794628751973, 0.5700000000000001, 1.0, 1.0, 0.9345241595637487, 1.0, 1.0, 0.9345241595637487, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.7347574266016528, 0.7347574266016528, 0.7418199606871939], 
reward next is 0.2582, 
noisyNet noise sample is [array([-0.6072617], dtype=float32), 0.7897309]. 
=============================================
[2019-04-28 03:00:16,991] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [1.28690825e-11 6.20188133e-04 3.74701981e-18 9.99379754e-01
 3.36153279e-13], sum to 1.0000
[2019-04-28 03:00:16,992] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.8998
[2019-04-28 03:00:16,998] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 85.66666666666667, 1.0, 2.0, 0.3191507284976911, 1.0, 2.0, 0.3191507284976911, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 892004.650520181, 892004.650520181, 253678.2548281637], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 4861200.0000, 
sim time next is 4861800.0000, 
raw observation next is [27.0, 86.5, 1.0, 2.0, 0.3233807674704581, 1.0, 2.0, 0.3233807674704581, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 903832.3047598478, 903832.3047598478, 254549.0108810325], 
processed observation next is [1.0, 0.2608695652173913, 0.4786729857819906, 0.865, 1.0, 1.0, 0.18479610538609406, 1.0, 1.0, 0.18479610538609406, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.25106452909995775, 0.25106452909995775, 0.37992389683736194], 
reward next is 0.6201, 
noisyNet noise sample is [array([-2.0876157], dtype=float32), -0.17154178]. 
=============================================
[2019-04-28 03:00:17,815] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [2.7411306e-13 9.5431533e-06 3.7265994e-23 9.9999046e-01 9.4309207e-15], sum to 1.0000
[2019-04-28 03:00:17,817] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.7763
[2019-04-28 03:00:17,826] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [27.0, 84.0, 1.0, 2.0, 0.2586166207372088, 1.0, 2.0, 0.2586166207372088, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 722758.9151743836, 722758.9151743843, 242379.0509865449], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5095200.0000, 
sim time next is 5095800.0000, 
raw observation next is [27.0, 84.0, 1.0, 2.0, 0.2583293259086977, 1.0, 2.0, 0.2583293259086977, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 721955.7387185008, 721955.7387185001, 242330.5650477719], 
processed observation next is [0.0, 1.0, 0.4786729857819906, 0.84, 1.0, 1.0, 0.10642087458879243, 1.0, 1.0, 0.10642087458879243, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20054326075513912, 0.20054326075513892, 0.36168741051906256], 
reward next is 0.6383, 
noisyNet noise sample is [array([-0.76580566], dtype=float32), 1.9018332]. 
=============================================
[2019-04-28 03:00:19,782] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.08316086e-13 8.87807983e-05 3.54126715e-20 9.99911189e-01
 6.39176483e-14], sum to 1.0000
[2019-04-28 03:00:19,788] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.2305
[2019-04-28 03:00:19,797] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.0, 84.83333333333333, 1.0, 2.0, 0.244164843652535, 1.0, 2.0, 0.244164843652535, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 682357.5167114162, 682357.5167114162, 239994.7168913605], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5107800.0000, 
sim time next is 5108400.0000, 
raw observation next is [26.0, 84.0, 1.0, 2.0, 0.2423369695588135, 1.0, 2.0, 0.2423369695588135, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 677247.6189812424, 677247.6189812424, 239701.827886218], 
processed observation next is [0.0, 0.13043478260869565, 0.4312796208530806, 0.84, 1.0, 1.0, 0.08715297537206446, 1.0, 1.0, 0.08715297537206446, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.18812433860590067, 0.18812433860590067, 0.35776392221823583], 
reward next is 0.6422, 
noisyNet noise sample is [array([1.3302857], dtype=float32), 1.1483827]. 
=============================================
[2019-04-28 03:00:20,878] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [1.31967996e-14 3.89880297e-05 6.42008018e-24 9.99961019e-01
 4.66996528e-16], sum to 1.0000
[2019-04-28 03:00:20,887] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.3111
[2019-04-28 03:00:20,895] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [28.66666666666667, 75.66666666666667, 1.0, 2.0, 0.2611385053354923, 1.0, 2.0, 0.2611385053354923, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 729809.25353146, 729809.25353146, 242809.2677138018], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5125800.0000, 
sim time next is 5126400.0000, 
raw observation next is [29.0, 74.0, 1.0, 2.0, 0.2619841822426482, 1.0, 2.0, 0.2619841822426482, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 732173.4911763048, 732173.4911763048, 242953.8992438482], 
processed observation next is [0.0, 0.34782608695652173, 0.5734597156398105, 0.74, 1.0, 1.0, 0.11082431595499781, 1.0, 1.0, 0.11082431595499781, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20338152532675133, 0.20338152532675133, 0.36261776006544505], 
reward next is 0.6374, 
noisyNet noise sample is [array([1.1168388], dtype=float32), 0.58459306]. 
=============================================
[2019-04-28 03:00:28,305] A3C_AGENT_WORKER-Thread-18 DEBUG:Policy network output: [4.9994006e-22 1.0000000e+00 1.9359132e-31 9.7824830e-16 1.7568038e-29], sum to 1.0000
[2019-04-28 03:00:28,312] A3C_AGENT_WORKER-Thread-21 DEBUG:Policy network output: [3.7030905e-08 9.7410522e-02 9.2680468e-14 9.0258950e-01 2.1071481e-10], sum to 1.0000
[2019-04-28 03:00:28,313] A3C_AGENT_WORKER-Thread-18 DEBUG:Softmax action selection sampled number: 0.4098
[2019-04-28 03:00:28,316] A3C_AGENT_WORKER-Thread-18 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.6, 87.0, 1.0, 2.0, 0.550308261473757, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 768995.7409539418, 768995.7409539425, 191853.4611998173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5526000.0000, 
sim time next is 5526600.0000, 
raw observation next is [27.53333333333334, 87.5, 1.0, 2.0, 0.5495289017941459, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 767906.2771858142, 767906.2771858149, 191719.8930627896], 
processed observation next is [1.0, 1.0, 0.5039494470774094, 0.875, 1.0, 1.0, 0.45726373710138063, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21330729921828173, 0.21330729921828193, 0.2861490941235666], 
reward next is 0.7139, 
noisyNet noise sample is [array([-0.5942201], dtype=float32), -1.4028468]. 
=============================================
[2019-04-28 03:00:28,322] A3C_AGENT_WORKER-Thread-21 DEBUG:Softmax action selection sampled number: 0.5985
[2019-04-28 03:00:28,325] A3C_AGENT_WORKER-Thread-21 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [31.0, 70.0, 1.0, 2.0, 0.8767756502306645, 1.0, 2.0, 0.8767756502306645, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 2452272.543864268, 2452272.543864268, 458980.1382759276], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5223600.0000, 
sim time next is 5224200.0000, 
raw observation next is [31.16666666666667, 69.5, 1.0, 2.0, 0.8995498551386762, 1.0, 2.0, 0.8995498551386762, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 170.5573041426782, 2516034.334230164, 2516034.334230165, 471246.599116688], 
processed observation next is [1.0, 0.4782608695652174, 0.6761453396524489, 0.695, 1.0, 1.0, 0.8789757290827425, 1.0, 1.0, 0.8789757290827425, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8375144448122397, 0.6988984261750456, 0.6988984261750458, 0.7033531330099821], 
reward next is 0.2966, 
noisyNet noise sample is [array([0.85990936], dtype=float32), -0.15678068]. 
=============================================
[2019-04-28 03:00:28,948] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.9263905e-17 1.0000000e+00 2.6516163e-24 3.1356926e-09 3.7746631e-24], sum to 1.0000
[2019-04-28 03:00:28,958] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.3463
[2019-04-28 03:00:28,962] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [32.5, 72.0, 1.0, 2.0, 0.623467966627987, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 871270.3754062502, 871270.3754062497, 205200.5711246113], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5340000.0000, 
sim time next is 5340600.0000, 
raw observation next is [32.25, 73.5, 1.0, 2.0, 0.6236263006865378, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 871491.7314012394, 871491.7314012387, 205231.2299993737], 
processed observation next is [1.0, 0.8260869565217391, 0.7274881516587678, 0.735, 1.0, 1.0, 0.5465377116705274, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.24208103650034427, 0.2420810365003441, 0.3063152686557816], 
reward next is 0.6937, 
noisyNet noise sample is [array([0.8485268], dtype=float32), 0.13757983]. 
=============================================
[2019-04-28 03:00:32,879] A3C_AGENT_WORKER-Thread-9 DEBUG:Policy network output: [5.8022205e-11 9.5810508e-04 1.9016137e-15 9.9904186e-01 5.7098190e-13], sum to 1.0000
[2019-04-28 03:00:32,887] A3C_AGENT_WORKER-Thread-9 DEBUG:Softmax action selection sampled number: 0.4398
[2019-04-28 03:00:32,891] A3C_AGENT_WORKER-Thread-9 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [29.1, 88.0, 1.0, 2.0, 0.5893125185769645, 1.0, 2.0, 0.5893125185769645, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1647668.798498359, 1647668.798498359, 330351.2856758143], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5366400.0000, 
sim time next is 5367000.0000, 
raw observation next is [29.05, 88.5, 1.0, 2.0, 0.5657501965547845, 1.0, 2.0, 0.5657501965547845, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1581741.88992358, 1581741.88992358, 321959.6226002443], 
processed observation next is [1.0, 0.08695652173913043, 0.575829383886256, 0.885, 1.0, 1.0, 0.47680746572865607, 1.0, 1.0, 0.47680746572865607, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.43937274720099445, 0.43937274720099445, 0.4805367501496184], 
reward next is 0.5195, 
noisyNet noise sample is [array([1.1526026], dtype=float32), -1.3019383]. 
=============================================
[2019-04-28 03:00:32,915] A3C_AGENT_WORKER-Thread-9 DEBUG:Value prediction is [[48.918423]
 [48.096455]
 [47.323265]
 [45.430363]
 [48.973522]], R is [[49.42326355]
 [49.43597031]
 [48.94161224]
 [48.45219803]
 [47.96767807]].
[2019-04-28 03:00:34,185] A3C_AGENT_WORKER-Thread-13 INFO:Evaluating...
[2019-04-28 03:00:34,186] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 03:00:34,187] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:00:34,190] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 03:00:34,192] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 03:00:34,192] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:00:34,193] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:00:34,195] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 03:00:34,196] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 03:00:34,196] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:00:34,198] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:00:34,221] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run100
[2019-04-28 03:00:34,243] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run100
[2019-04-28 03:00:34,244] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run100
[2019-04-28 03:00:34,312] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run100
[2019-04-28 03:00:34,338] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run100
[2019-04-28 03:00:37,776] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:37,777] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [18.67444578166667, 93.83983068333333, 1.0, 2.0, 0.2610141955902481, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 429469.0327443865, 429469.0327443871, 162008.7653707584]
[2019-04-28 03:00:37,777] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 03:00:37,781] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.2616753e-14 9.9786502e-01 8.3403070e-23 2.1350516e-03 7.7525962e-19], sampled 0.6406173386328934
[2019-04-28 03:00:39,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:39,164] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.33333333333333, 78.66666666666667, 1.0, 2.0, 0.3587676915180267, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 585056.6533695918, 585056.6533695924, 173609.863459545]
[2019-04-28 03:00:39,166] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 03:00:39,168] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.5721204e-14 9.9833989e-01 1.3056920e-22 1.6601464e-03 1.0232850e-18], sampled 0.06578669853979302
[2019-04-28 03:00:46,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:46,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [21.98333333333333, 97.83333333333334, 1.0, 2.0, 0.3643513155949491, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 553924.9319534462, 553924.9319534457, 170759.8298645259]
[2019-04-28 03:00:46,181] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 03:00:46,184] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [8.9455848e-16 9.9999845e-01 1.2479122e-23 1.5606850e-06 2.0465582e-20], sampled 0.6929738805603711
[2019-04-28 03:00:47,657] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:47,658] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [24.58691132, 75.92411452, 1.0, 2.0, 0.3853991716361244, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 593573.5851112083, 593573.5851112077, 174425.6515113471]
[2019-04-28 03:00:47,659] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 03:00:47,661] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.3091809e-14 9.9903691e-01 1.1535928e-22 9.6306455e-04 7.8976558e-19], sampled 0.05326148549334564
[2019-04-28 03:00:53,922] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:53,923] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation this: [25.15000532, 92.795798215, 1.0, 2.0, 0.5961906851606508, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 833136.566338875, 833136.5663388744, 200030.7442504216]
[2019-04-28 03:00:53,924] A3C_EVAL-Part3-NA-Shg-Test-v2 DEBUG:Observation forecast: []
[2019-04-28 03:00:53,927] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Softmax [1.7373354e-14 9.9934703e-01 2.1004440e-22 6.5299543e-04 1.1267362e-18], sampled 0.4552460134366316
[2019-04-28 03:00:54,898] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:00:54,899] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [23.56666666666667, 84.16666666666666, 1.0, 2.0, 0.5724182636522096, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 874111.7582375564, 874111.7582375564, 204754.9232676098]
[2019-04-28 03:00:54,900] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 03:00:54,902] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [1.5151592e-14 9.9913090e-01 1.5214951e-22 8.6910481e-04 9.5219821e-19], sampled 0.9705201403438793
[2019-04-28 03:01:07,915] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:01:07,916] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation this: [31.1, 67.0, 1.0, 2.0, 0.7010901947734844, 0.0, 2.0, 0.0, 1.0, 2.0, 1.03, 6.99468035191749, 6.9112, 168.9123929643308, 1876654.365161014, 1817430.727927885, 385317.0526934578]
[2019-04-28 03:01:07,917] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Observation forecast: []
[2019-04-28 03:01:07,919] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Softmax [3.9964432e-10 9.1969955e-01 7.7521249e-16 8.0300473e-02 6.8990636e-13], sampled 0.8793919607546333
[2019-04-28 03:01:07,921] A3C_EVAL-Part3-NA-Shg-Test-v1 DEBUG:Action function: raw action [0, 1, 0, 0, 0] has been changed to [0, 0, 1, 0, 0] for the demand 1876654.365161014 W.
[2019-04-28 03:01:10,731] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15043606]
[2019-04-28 03:01:10,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation this: [31.65, 57.33333333333333, 1.0, 2.0, 0.5182058851510473, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 724120.8828410414, 724120.8828410414, 186496.9682183577]
[2019-04-28 03:01:10,732] A3C_EVAL-Part3-NA-Shg-Test-v3 DEBUG:Observation forecast: []
[2019-04-28 03:01:10,734] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Softmax [3.00852826e-15 9.99976277e-01 3.44725494e-23 2.36874985e-05
 1.01561694e-19], sampled 0.7475650408713541
[2019-04-28 03:02:02,158] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8308.7849 2921447984.6439 1151.0000
[2019-04-28 03:02:02,316] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8690.4642 2774842792.7302 785.0000
[2019-04-28 03:02:02,485] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8099.6010 2995094737.1554 1400.0000
[2019-04-28 03:02:02,558] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8552.6387 2836055645.2647 936.0000
[2019-04-28 03:02:02,681] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 7972.7183 3152215361.6117 1440.0000
[2019-04-28 03:02:03,704] A3C_AGENT_WORKER-Thread-13 INFO:Global step: 2475000, evaluation results [2475000.0, 7972.718281288885, 3152215361.6116896, 1440.0, 8308.784855246711, 2921447984.643874, 1151.0, 8690.464184559109, 2774842792.7301593, 785.0, 8099.600990301597, 2995094737.1553717, 1400.0, 8552.63865703459, 2836055645.264676, 936.0]
[2019-04-28 03:02:05,831] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [1.4943487e-14 9.9981922e-01 4.0986669e-23 1.8082315e-04 6.0035077e-18], sum to 1.0000
[2019-04-28 03:02:05,839] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.0701
[2019-04-28 03:02:05,844] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [31.75, 65.33333333333334, 1.0, 2.0, 0.5579412373779853, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 779665.906399961, 779665.9063999616, 193172.5295775914], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5658600.0000, 
sim time next is 5659200.0000, 
raw observation next is [31.8, 65.0, 1.0, 2.0, 0.5583092521566168, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 780180.358338228, 780180.358338228, 193236.4637166237], 
processed observation next is [0.0, 0.5217391304347826, 0.7061611374407584, 0.65, 1.0, 1.0, 0.4678424724778515, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21671676620506333, 0.21671676620506333, 0.2884126324128712], 
reward next is 0.7116, 
noisyNet noise sample is [array([-0.7367814], dtype=float32), -0.54911655]. 
=============================================
[2019-04-28 03:02:09,207] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [1.4293476e-18 1.0000000e+00 2.0600274e-26 6.5040204e-09 2.6068660e-23], sum to 1.0000
[2019-04-28 03:02:09,214] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.8993
[2019-04-28 03:02:09,219] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [30.6, 74.66666666666667, 1.0, 2.0, 0.5730722300147552, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 800817.8942086841, 800817.8942086835, 195837.3381750356], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5514000.0000, 
sim time next is 5514600.0000, 
raw observation next is [30.35, 76.33333333333333, 1.0, 2.0, 0.576406859247662, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 805479.5113056763, 805479.5113056763, 196433.5117151019], 
processed observation next is [1.0, 0.8260869565217391, 0.637440758293839, 0.7633333333333333, 1.0, 1.0, 0.4896468183706771, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.2237443086960212, 0.2237443086960212, 0.29318434584343567], 
reward next is 0.7068, 
noisyNet noise sample is [array([0.05574535], dtype=float32), 0.29960874]. 
=============================================
[2019-04-28 03:02:11,023] A3C_AGENT_WORKER-Thread-19 DEBUG:Policy network output: [3.1988884e-11 9.7745711e-01 1.0982369e-18 2.2542968e-02 1.4284289e-14], sum to 1.0000
[2019-04-28 03:02:11,024] A3C_AGENT_WORKER-Thread-19 DEBUG:Softmax action selection sampled number: 0.8445
[2019-04-28 03:02:11,031] A3C_AGENT_WORKER-Thread-19 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.1, 88.66666666666666, 1.0, 2.0, 0.8738278257637558, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1221338.989856808, 1221338.989856808, 262845.3602332624], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5377200.0000, 
sim time next is 5377800.0000, 
raw observation next is [29.3, 87.33333333333333, 1.0, 2.0, 0.8827643314146119, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 1233836.697391118, 1233836.697391118, 265246.1361687466], 
processed observation next is [1.0, 0.21739130434782608, 0.5876777251184835, 0.8733333333333333, 1.0, 1.0, 0.8587522065236287, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.3427324159419772, 0.3427324159419772, 0.39588975547574123], 
reward next is 0.6041, 
noisyNet noise sample is [array([0.84590226], dtype=float32), 0.18646942]. 
=============================================
[2019-04-28 03:02:12,224] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [3.2074810e-14 9.9892104e-01 4.4637024e-24 1.0789972e-03 1.1570216e-19], sum to 1.0000
[2019-04-28 03:02:12,232] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.8963
[2019-04-28 03:02:12,236] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [29.43333333333333, 80.5, 1.0, 2.0, 0.5779775225271246, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 807675.214012238, 807675.214012238, 196714.8250113312], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5518200.0000, 
sim time next is 5518800.0000, 
raw observation next is [29.3, 81.0, 1.0, 2.0, 0.5759460982098975, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 804835.393041449, 804835.393041449, 196350.2243232327], 
processed observation next is [1.0, 0.9130434782608695, 0.5876777251184835, 0.81, 1.0, 1.0, 0.48909168459023794, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.22356538695595807, 0.22356538695595807, 0.2930600363033324], 
reward next is 0.7069, 
noisyNet noise sample is [array([0.9444802], dtype=float32), 0.52598137]. 
=============================================
[2019-04-28 03:02:15,677] A3C_AGENT_WORKER-Thread-10 DEBUG:Policy network output: [3.2241454e-13 4.0188473e-02 7.8687454e-21 9.5981157e-01 2.6806141e-17], sum to 1.0000
[2019-04-28 03:02:15,685] A3C_AGENT_WORKER-Thread-10 DEBUG:Softmax action selection sampled number: 0.0745
[2019-04-28 03:02:15,692] A3C_AGENT_WORKER-Thread-10 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.25, 91.16666666666667, 1.0, 2.0, 0.262257749521958, 1.0, 2.0, 0.262257749521958, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 732938.2974493527, 732938.2974493527, 242999.7816191695], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 5616600.0000, 
sim time next is 5617200.0000, 
raw observation next is [26.2, 91.33333333333334, 1.0, 2.0, 0.2615103980902825, 1.0, 2.0, 0.2615103980902825, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 730848.9441040494, 730848.9441040494, 242871.9559094807], 
processed observation next is [0.0, 0.0, 0.44075829383886256, 0.9133333333333334, 1.0, 1.0, 0.11025349167503913, 1.0, 1.0, 0.11025349167503913, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.20301359558445817, 0.20301359558445817, 0.3624954565813145], 
reward next is 0.6375, 
noisyNet noise sample is [array([1.0529186], dtype=float32), 0.39045438]. 
=============================================
[2019-04-28 03:02:30,862] A3C_AGENT_WORKER-Thread-16 DEBUG:Policy network output: [3.0084644e-21 1.0000000e+00 8.6086228e-28 9.6565221e-11 6.0438455e-25], sum to 1.0000
[2019-04-28 03:02:30,875] A3C_AGENT_WORKER-Thread-16 DEBUG:Softmax action selection sampled number: 0.1070
[2019-04-28 03:02:30,880] A3C_AGENT_WORKER-Thread-16 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [27.63333333333333, 85.33333333333333, 1.0, 2.0, 0.5470873843352874, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 764493.2969627016, 764493.296962701, 191301.9922141657], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5780400.0000, 
sim time next is 5781000.0000, 
raw observation next is [27.56666666666667, 85.66666666666667, 1.0, 2.0, 0.5461227528156555, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 763144.8483131959, 763144.8483131966, 191137.5523005457], 
processed observation next is [0.0, 0.9130434782608695, 0.505529225908373, 0.8566666666666667, 1.0, 1.0, 0.45315994315139213, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, -8.881784197001253e-17, 0.0, 0.8294399451523027, 0.21198468008699886, 0.21198468008699906, 0.28527992880678466], 
reward next is 0.7147, 
noisyNet noise sample is [array([0.37846854], dtype=float32), 0.26137573]. 
=============================================
[2019-04-28 03:02:30,903] A3C_AGENT_WORKER-Thread-16 DEBUG:Value prediction is [[71.93706 ]
 [71.96968 ]
 [71.987045]
 [72.02933 ]
 [72.084305]], R is [[71.90579224]
 [71.9012146 ]
 [71.89676666]
 [71.89246368]
 [71.88807678]].
[2019-04-28 03:02:32,234] A3C_AGENT_WORKER-Thread-3 DEBUG:Policy network output: [9.9997353e-23 1.0000000e+00 4.1377401e-32 5.1419797e-18 2.7427176e-30], sum to 1.0000
[2019-04-28 03:02:32,248] A3C_AGENT_WORKER-Thread-3 DEBUG:Softmax action selection sampled number: 0.0249
[2019-04-28 03:02:32,251] A3C_AGENT_WORKER-Thread-3 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.9, 81.5, 1.0, 2.0, 0.5558250749686544, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 776707.7043472845, 776707.7043472845, 192805.4338136173], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5599800.0000, 
sim time next is 5600400.0000, 
raw observation next is [28.63333333333333, 83.66666666666667, 1.0, 2.0, 0.5594036030144646, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 781710.165004735, 781710.165004735, 193427.1474426003], 
processed observation next is [1.0, 0.8260869565217391, 0.55608214849921, 0.8366666666666667, 1.0, 1.0, 0.4691609674873067, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.21714171250131528, 0.21714171250131528, 0.28869723498895566], 
reward next is 0.7113, 
noisyNet noise sample is [array([0.72578686], dtype=float32), -0.9942333]. 
=============================================
[2019-04-28 03:02:32,348] A3C_AGENT_WORKER-Thread-22 DEBUG:Policy network output: [8.8748137e-20 1.0000000e+00 1.1615453e-27 8.8192995e-12 1.4536367e-24], sum to 1.0000
[2019-04-28 03:02:32,357] A3C_AGENT_WORKER-Thread-22 DEBUG:Softmax action selection sampled number: 0.7429
[2019-04-28 03:02:32,361] A3C_AGENT_WORKER-Thread-22 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.6, 86.0, 1.0, 2.0, 0.5047848553754679, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 705360.6277993605, 705360.6277993611, 184349.2608425079], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5637600.0000, 
sim time next is 5638200.0000, 
raw observation next is [26.75, 85.16666666666667, 1.0, 2.0, 0.5062960044907171, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 707472.9337201746, 707472.9337201746, 184588.4830706276], 
processed observation next is [0.0, 0.2608695652173913, 0.4668246445497631, 0.8516666666666667, 1.0, 1.0, 0.4051759090249603, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8294399451523027, 0.19652025936671516, 0.19652025936671516, 0.27550519861287703], 
reward next is 0.7245, 
noisyNet noise sample is [array([0.23865], dtype=float32), -0.71245104]. 
=============================================
[2019-04-28 03:02:36,738] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [4.8894198e-11 7.6729030e-04 1.9110360e-18 9.9923277e-01 3.1947179e-14], sum to 1.0000
[2019-04-28 03:02:36,746] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5310
[2019-04-28 03:02:36,749] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.65, 92.0, 1.0, 2.0, 0.4955462184655135, 1.0, 2.0, 0.4955462184655135, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1385336.59313485, 1385336.59313485, 298875.7006349745], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6143400.0000, 
sim time next is 6144000.0000, 
raw observation next is [26.63333333333333, 92.0, 1.0, 2.0, 0.4180442384047067, 1.0, 2.0, 0.4180442384047067, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 1168555.914764529, 1168555.914764529, 276768.5221516437], 
processed observation next is [1.0, 0.08695652173913043, 0.46129541864139006, 0.92, 1.0, 1.0, 0.29884848000567077, 1.0, 1.0, 0.29884848000567077, 0.0, 1.0, -0.2195121951219512, 0.0, 0.0, 0.8375144448122397, 0.3245988652123692, 0.3245988652123692, 0.4130873464949906], 
reward next is 0.5869, 
noisyNet noise sample is [array([0.4903547], dtype=float32), -1.1995779]. 
=============================================
[2019-04-28 03:02:36,767] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[53.979572]
 [53.94505 ]
 [54.33936 ]
 [54.47712 ]
 [54.45893 ]], R is [[54.16879272]
 [54.18102264]
 [54.16524506]
 [54.10221481]
 [54.19620895]].
[2019-04-28 03:02:37,678] A3C_AGENT_WORKER-Thread-17 DEBUG:Policy network output: [1.7267708e-15 9.9999988e-01 2.2349297e-24 1.2699319e-07 2.6965653e-20], sum to 1.0000
[2019-04-28 03:02:37,691] A3C_AGENT_WORKER-Thread-17 DEBUG:Softmax action selection sampled number: 0.2459
[2019-04-28 03:02:37,699] A3C_AGENT_WORKER-Thread-17 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [26.36666666666667, 87.33333333333333, 1.0, 2.0, 0.5096567978102957, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 712170.713913877, 712170.713913877, 185122.6269993627], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 5710200.0000, 
sim time next is 5710800.0000, 
raw observation next is [26.33333333333334, 87.66666666666667, 1.0, 2.0, 0.5097269183795204, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 712268.7299873319, 712268.7299873313, 185133.8424928221], 
processed observation next is [0.0, 0.08695652173913043, 0.44707740916271754, 0.8766666666666667, 1.0, 1.0, 0.4093095402162897, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.1978524249964811, 0.19785242499648092, 0.2763191678997345], 
reward next is 0.7237, 
noisyNet noise sample is [array([-1.9852093], dtype=float32), 0.53957695]. 
=============================================
[2019-04-28 03:02:45,658] A3C_AGENT_WORKER-Thread-12 DEBUG:Policy network output: [1.8550153e-23 1.0000000e+00 4.1422432e-31 2.0418448e-19 4.9135054e-30], sum to 1.0000
[2019-04-28 03:02:45,664] A3C_AGENT_WORKER-Thread-12 DEBUG:Softmax action selection sampled number: 0.1478
[2019-04-28 03:02:45,673] A3C_AGENT_WORKER-Thread-12 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 1, 
current raw observation is [28.06666666666667, 83.0, 1.0, 2.0, 0.5385921739529465, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752617.9870947594, 752617.9870947587, 189863.9064238334], 
current ob forecast is [], 
actual action is [0, 1, 0, 0, 0], 
sim time this is 6033000.0000, 
sim time next is 6033600.0000, 
raw observation next is [27.9, 84.0, 1.0, 2.0, 0.5386286325349633, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 752668.951641782, 752668.9516417814, 189869.9950581023], 
processed observation next is [1.0, 0.8695652173913043, 0.5213270142180094, 0.84, 1.0, 1.0, 0.4441308825722449, 0.0, 1.0, -0.20481927710843376, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8294399451523027, 0.2090747087893839, 0.20907470878938372, 0.28338805232552583], 
reward next is 0.7166, 
noisyNet noise sample is [array([-0.0179456], dtype=float32), -0.9737085]. 
=============================================
[2019-04-28 03:02:58,526] A3C_AGENT_WORKER-Thread-20 DEBUG:Policy network output: [2.9086009e-12 3.4407824e-03 1.5447341e-20 9.9655920e-01 7.5835879e-17], sum to 1.0000
[2019-04-28 03:02:58,533] A3C_AGENT_WORKER-Thread-20 DEBUG:Softmax action selection sampled number: 0.5452
[2019-04-28 03:02:58,537] A3C_AGENT_WORKER-Thread-20 DEBUG:TRAINING DEBUG INFO ======>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>Current H regulation is 0.0000, 
Environment debug: raw action idx is 3, 
current raw observation is [26.55, 90.5, 1.0, 2.0, 0.2642645620419592, 1.0, 2.0, 0.2642645620419592, 0.0, 2.0, 0.0, 6.9112, 6.9112, 170.5573041426782, 738548.71756013, 738548.71756013, 243345.6070388289], 
current ob forecast is [], 
actual action is [0, 0, 0, 1, 0], 
sim time this is 6485400.0000, 
sim time next is 6486000.0000, 
raw observation next is [26.5, 90.66666666666667, 1.0, 2.0, 0.2637995617799449, 1.0, 2.0, 0.2637995617799449, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 170.5573041426782, 737248.7196591428, 737248.7196591421, 243265.4144249201], 
processed observation next is [1.0, 0.043478260869565216, 0.4549763033175356, 0.9066666666666667, 1.0, 1.0, 0.11301152021680108, 1.0, 1.0, 0.11301152021680108, 0.0, 1.0, -0.2195121951219512, 8.881784197001253e-17, 0.0, 0.8375144448122397, 0.20479131101642856, 0.20479131101642836, 0.3630827080968957], 
reward next is 0.6369, 
noisyNet noise sample is [array([0.43004844], dtype=float32), -0.07930797]. 
=============================================
[2019-04-28 03:02:58,579] A3C_AGENT_WORKER-Thread-20 DEBUG:Value prediction is [[58.844597]
 [59.134136]
 [59.413956]
 [59.61988 ]
 [59.548267]], R is [[58.65205765]
 [58.70233536]
 [58.75198364]
 [58.80100632]
 [58.84944916]].
[2019-04-28 03:03:04,961] A3C_AGENT_WORKER-Thread-10 INFO:Evaluating...
[2019-04-28 03:03:04,962] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation job starts!
[2019-04-28 03:03:04,963] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:03:04,965] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation job starts!
[2019-04-28 03:03:04,966] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation job starts!
[2019-04-28 03:03:04,968] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:03:04,968] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation job starts!
[2019-04-28 03:03:04,968] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:03:04,969] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:03:04,970] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation job starts!
[2019-04-28 03:03:04,971] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:Creating EnergyPlus simulation environment...
[2019-04-28 03:03:04,998] EPLUS_ENV_Part3-NA-Shg-Test-v2_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v2-res1/Eplus-env-sub_run101
[2019-04-28 03:03:05,028] EPLUS_ENV_Part3-NA-Shg-Train-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Train-v1-res1/Eplus-env-sub_run101
[2019-04-28 03:03:05,064] EPLUS_ENV_Part3-NA-Shg-Test-v3_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v3-res1/Eplus-env-sub_run101
[2019-04-28 03:03:05,093] EPLUS_ENV_Part3-NA-Shg-Test-v1_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v1-res1/Eplus-env-sub_run101
[2019-04-28 03:03:05,127] EPLUS_ENV_Part3-NA-Shg-Test-v4_MainThread_ROOT INFO:EnergyPlus working directory is in /home/zhiangz/Documents/HVAC-RL-Control/src/runs/rl_parametric_part3_shg_na/40/Eplus-env-Part3-NA-Shg-Test-v4-res1/Eplus-env-sub_run101
[2019-04-28 03:03:23,231] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15365717]
[2019-04-28 03:03:23,233] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [23.62534986666667, 89.62624635333334, 1.0, 2.0, 0.3956311696366838, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.9112, 6.9112, 168.912956510431, 590916.5662461667, 590916.5662461667, 173719.1756014882]
[2019-04-28 03:03:23,234] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 03:03:23,237] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.8185998e-16 9.9999726e-01 1.0892814e-24 2.7339845e-06 9.6610111e-22], sampled 0.46259946431790155
[2019-04-28 03:04:17,919] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15365717]
[2019-04-28 03:04:17,920] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [25.7, 95.0, 1.0, 2.0, 0.8046205679501416, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911200000000001, 6.9112, 168.912956510431, 1124557.586466148, 1124557.586466147, 245046.9223346355]
[2019-04-28 03:04:17,920] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 03:04:17,923] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.33568308e-15 9.99995112e-01 1.20243386e-23 4.86172803e-06
 7.99938340e-21], sampled 0.5296668096576154
[2019-04-28 03:04:21,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15365717]
[2019-04-28 03:04:21,020] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [26.6, 92.0, 1.0, 2.0, 0.7153428128753893, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 999722.0550100794, 999722.0550100801, 224192.0248112398]
[2019-04-28 03:04:21,024] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 03:04:21,026] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [1.8762875e-15 9.9999213e-01 1.7784533e-23 7.8328785e-06 1.2500502e-20], sampled 0.31305374071724734
[2019-04-28 03:04:30,405] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15365717]
[2019-04-28 03:04:30,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation this: [23.96666666666667, 89.66666666666666, 1.0, 2.0, 0.4177167231459037, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 613994.5689308227, 613994.5689308232, 175586.1054938332]
[2019-04-28 03:04:30,407] A3C_EVAL-Part3-NA-Shg-Train-v1 DEBUG:Observation forecast: []
[2019-04-28 03:04:30,416] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Softmax [3.1995762e-16 9.9999392e-01 9.9705264e-25 6.0380771e-06 1.1100307e-21], sampled 0.1328624189975367
[2019-04-28 03:04:33,664] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:NoisyNet noise sample: [array([-0.00045975], dtype=float32), 0.15365717]
[2019-04-28 03:04:33,665] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation this: [22.57435623, 96.45403449, 1.0, 2.0, 0.3992357731702181, 0.0, 2.0, 0.0, 0.0, 2.0, 0.0, 6.911199999999999, 6.9112, 168.912956510431, 598219.0969484875, 598219.0969484881, 174443.7436781928]
[2019-04-28 03:04:33,666] A3C_EVAL-Part3-NA-Shg-Test-v4 DEBUG:Observation forecast: []
[2019-04-28 03:04:33,668] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Softmax [2.0272798e-16 9.9999988e-01 1.8452744e-24 1.6200813e-07 6.9992617e-22], sampled 0.2561379208591974
[2019-04-28 03:04:41,731] A3C_EVAL-Part3-NA-Shg-Test-v1 INFO:Evaluation: average rewards by now are 8300.3525 2922234073.9731 1198.0000
[2019-04-28 03:04:41,874] A3C_EVAL-Part3-NA-Shg-Test-v2 INFO:Evaluation: average rewards by now are 8701.5288 2774632201.5206 813.0000
[2019-04-28 03:04:41,959] A3C_EVAL-Part3-NA-Shg-Test-v4 INFO:Evaluation: average rewards by now are 8565.3289 2835045041.2449 949.0000
[2019-04-28 03:04:41,991] A3C_EVAL-Part3-NA-Shg-Test-v3 INFO:Evaluation: average rewards by now are 8151.6041 2991965308.9642 1362.0000
[2019-04-28 03:04:42,147] A3C_EVAL-Part3-NA-Shg-Train-v1 INFO:Evaluation: average rewards by now are 8012.6441 3150538019.5620 1418.0000
[2019-04-28 03:04:43,164] A3C_AGENT_WORKER-Thread-10 INFO:Global step: 2500000, evaluation results [2500000.0, 8012.644093349104, 3150538019.5619564, 1418.0, 8300.352462522016, 2922234073.973076, 1198.0, 8701.528816276972, 2774632201.520619, 813.0, 8151.604114326148, 2991965308.964185, 1362.0, 8565.328935620528, 2835045041.244943, 949.0]
